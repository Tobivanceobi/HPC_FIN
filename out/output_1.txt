no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  1
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.89118
[1mStep[0m  [10/106], [94mLoss[0m : 11.21385
[1mStep[0m  [20/106], [94mLoss[0m : 10.43318
[1mStep[0m  [30/106], [94mLoss[0m : 10.81615
[1mStep[0m  [40/106], [94mLoss[0m : 10.37814
[1mStep[0m  [50/106], [94mLoss[0m : 10.24901
[1mStep[0m  [60/106], [94mLoss[0m : 10.78469
[1mStep[0m  [70/106], [94mLoss[0m : 10.27305
[1mStep[0m  [80/106], [94mLoss[0m : 9.47082
[1mStep[0m  [90/106], [94mLoss[0m : 9.89597
[1mStep[0m  [100/106], [94mLoss[0m : 9.44889

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.274, [92mTest[0m: 10.895, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.69148
[1mStep[0m  [10/106], [94mLoss[0m : 9.36653
[1mStep[0m  [20/106], [94mLoss[0m : 9.07950
[1mStep[0m  [30/106], [94mLoss[0m : 8.79206
[1mStep[0m  [40/106], [94mLoss[0m : 8.68458
[1mStep[0m  [50/106], [94mLoss[0m : 8.44076
[1mStep[0m  [60/106], [94mLoss[0m : 8.42128
[1mStep[0m  [70/106], [94mLoss[0m : 8.39549
[1mStep[0m  [80/106], [94mLoss[0m : 7.56756
[1mStep[0m  [90/106], [94mLoss[0m : 6.89409
[1mStep[0m  [100/106], [94mLoss[0m : 7.75559

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.585, [92mTest[0m: 9.932, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.18896
[1mStep[0m  [10/106], [94mLoss[0m : 7.59998
[1mStep[0m  [20/106], [94mLoss[0m : 7.09263
[1mStep[0m  [30/106], [94mLoss[0m : 7.38884
[1mStep[0m  [40/106], [94mLoss[0m : 7.50878
[1mStep[0m  [50/106], [94mLoss[0m : 7.00427
[1mStep[0m  [60/106], [94mLoss[0m : 6.93249
[1mStep[0m  [70/106], [94mLoss[0m : 6.42019
[1mStep[0m  [80/106], [94mLoss[0m : 6.61420
[1mStep[0m  [90/106], [94mLoss[0m : 6.31828
[1mStep[0m  [100/106], [94mLoss[0m : 6.28711

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.885, [92mTest[0m: 8.755, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.49013
[1mStep[0m  [10/106], [94mLoss[0m : 5.79435
[1mStep[0m  [20/106], [94mLoss[0m : 5.36982
[1mStep[0m  [30/106], [94mLoss[0m : 5.86400
[1mStep[0m  [40/106], [94mLoss[0m : 5.85141
[1mStep[0m  [50/106], [94mLoss[0m : 5.70522
[1mStep[0m  [60/106], [94mLoss[0m : 5.02850
[1mStep[0m  [70/106], [94mLoss[0m : 4.65091
[1mStep[0m  [80/106], [94mLoss[0m : 5.61386
[1mStep[0m  [90/106], [94mLoss[0m : 4.65119
[1mStep[0m  [100/106], [94mLoss[0m : 4.90433

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.311, [92mTest[0m: 7.445, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.54690
[1mStep[0m  [10/106], [94mLoss[0m : 4.53477
[1mStep[0m  [20/106], [94mLoss[0m : 3.91824
[1mStep[0m  [30/106], [94mLoss[0m : 4.68288
[1mStep[0m  [40/106], [94mLoss[0m : 3.90596
[1mStep[0m  [50/106], [94mLoss[0m : 4.43630
[1mStep[0m  [60/106], [94mLoss[0m : 3.65035
[1mStep[0m  [70/106], [94mLoss[0m : 3.62836
[1mStep[0m  [80/106], [94mLoss[0m : 3.46994
[1mStep[0m  [90/106], [94mLoss[0m : 3.43109
[1mStep[0m  [100/106], [94mLoss[0m : 3.39738

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.054, [92mTest[0m: 5.954, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.92159
[1mStep[0m  [10/106], [94mLoss[0m : 3.89594
[1mStep[0m  [20/106], [94mLoss[0m : 3.72587
[1mStep[0m  [30/106], [94mLoss[0m : 3.24594
[1mStep[0m  [40/106], [94mLoss[0m : 3.20152
[1mStep[0m  [50/106], [94mLoss[0m : 3.31159
[1mStep[0m  [60/106], [94mLoss[0m : 3.21575
[1mStep[0m  [70/106], [94mLoss[0m : 3.38463
[1mStep[0m  [80/106], [94mLoss[0m : 3.27578
[1mStep[0m  [90/106], [94mLoss[0m : 3.22553
[1mStep[0m  [100/106], [94mLoss[0m : 2.83984

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.299, [92mTest[0m: 4.666, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.09446
[1mStep[0m  [10/106], [94mLoss[0m : 3.14461
[1mStep[0m  [20/106], [94mLoss[0m : 2.92141
[1mStep[0m  [30/106], [94mLoss[0m : 3.30032
[1mStep[0m  [40/106], [94mLoss[0m : 3.17872
[1mStep[0m  [50/106], [94mLoss[0m : 2.96423
[1mStep[0m  [60/106], [94mLoss[0m : 3.51535
[1mStep[0m  [70/106], [94mLoss[0m : 2.82319
[1mStep[0m  [80/106], [94mLoss[0m : 3.02339
[1mStep[0m  [90/106], [94mLoss[0m : 3.01908
[1mStep[0m  [100/106], [94mLoss[0m : 3.11705

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.961, [92mTest[0m: 3.856, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61477
[1mStep[0m  [10/106], [94mLoss[0m : 3.13331
[1mStep[0m  [20/106], [94mLoss[0m : 2.76613
[1mStep[0m  [30/106], [94mLoss[0m : 2.89413
[1mStep[0m  [40/106], [94mLoss[0m : 2.60558
[1mStep[0m  [50/106], [94mLoss[0m : 2.92689
[1mStep[0m  [60/106], [94mLoss[0m : 2.63303
[1mStep[0m  [70/106], [94mLoss[0m : 2.55475
[1mStep[0m  [80/106], [94mLoss[0m : 2.83530
[1mStep[0m  [90/106], [94mLoss[0m : 2.85639
[1mStep[0m  [100/106], [94mLoss[0m : 2.69128

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.813, [92mTest[0m: 3.531, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.85617
[1mStep[0m  [10/106], [94mLoss[0m : 2.66845
[1mStep[0m  [20/106], [94mLoss[0m : 2.60083
[1mStep[0m  [30/106], [94mLoss[0m : 2.65776
[1mStep[0m  [40/106], [94mLoss[0m : 2.61878
[1mStep[0m  [50/106], [94mLoss[0m : 2.37075
[1mStep[0m  [60/106], [94mLoss[0m : 2.77071
[1mStep[0m  [70/106], [94mLoss[0m : 2.62157
[1mStep[0m  [80/106], [94mLoss[0m : 2.70963
[1mStep[0m  [90/106], [94mLoss[0m : 2.80829
[1mStep[0m  [100/106], [94mLoss[0m : 2.83690

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.763, [92mTest[0m: 3.258, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73926
[1mStep[0m  [10/106], [94mLoss[0m : 2.52208
[1mStep[0m  [20/106], [94mLoss[0m : 2.68542
[1mStep[0m  [30/106], [94mLoss[0m : 2.61309
[1mStep[0m  [40/106], [94mLoss[0m : 2.72203
[1mStep[0m  [50/106], [94mLoss[0m : 2.63119
[1mStep[0m  [60/106], [94mLoss[0m : 2.26212
[1mStep[0m  [70/106], [94mLoss[0m : 2.53519
[1mStep[0m  [80/106], [94mLoss[0m : 2.74521
[1mStep[0m  [90/106], [94mLoss[0m : 2.74551
[1mStep[0m  [100/106], [94mLoss[0m : 3.05161

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.722, [92mTest[0m: 3.114, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94062
[1mStep[0m  [10/106], [94mLoss[0m : 2.61229
[1mStep[0m  [20/106], [94mLoss[0m : 2.79455
[1mStep[0m  [30/106], [94mLoss[0m : 2.91879
[1mStep[0m  [40/106], [94mLoss[0m : 2.91044
[1mStep[0m  [50/106], [94mLoss[0m : 2.43498
[1mStep[0m  [60/106], [94mLoss[0m : 2.86633
[1mStep[0m  [70/106], [94mLoss[0m : 2.59755
[1mStep[0m  [80/106], [94mLoss[0m : 2.52754
[1mStep[0m  [90/106], [94mLoss[0m : 2.54820
[1mStep[0m  [100/106], [94mLoss[0m : 2.42068

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.979, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35095
[1mStep[0m  [10/106], [94mLoss[0m : 2.67054
[1mStep[0m  [20/106], [94mLoss[0m : 2.59335
[1mStep[0m  [30/106], [94mLoss[0m : 2.80874
[1mStep[0m  [40/106], [94mLoss[0m : 2.56394
[1mStep[0m  [50/106], [94mLoss[0m : 2.72682
[1mStep[0m  [60/106], [94mLoss[0m : 2.50350
[1mStep[0m  [70/106], [94mLoss[0m : 2.48599
[1mStep[0m  [80/106], [94mLoss[0m : 2.83514
[1mStep[0m  [90/106], [94mLoss[0m : 2.88197
[1mStep[0m  [100/106], [94mLoss[0m : 2.36620

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.971, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51848
[1mStep[0m  [10/106], [94mLoss[0m : 2.52573
[1mStep[0m  [20/106], [94mLoss[0m : 2.65677
[1mStep[0m  [30/106], [94mLoss[0m : 2.76893
[1mStep[0m  [40/106], [94mLoss[0m : 2.62508
[1mStep[0m  [50/106], [94mLoss[0m : 2.90242
[1mStep[0m  [60/106], [94mLoss[0m : 2.65163
[1mStep[0m  [70/106], [94mLoss[0m : 2.69173
[1mStep[0m  [80/106], [94mLoss[0m : 2.55610
[1mStep[0m  [90/106], [94mLoss[0m : 2.57895
[1mStep[0m  [100/106], [94mLoss[0m : 2.37289

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.918, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76908
[1mStep[0m  [10/106], [94mLoss[0m : 2.66424
[1mStep[0m  [20/106], [94mLoss[0m : 2.49566
[1mStep[0m  [30/106], [94mLoss[0m : 2.98157
[1mStep[0m  [40/106], [94mLoss[0m : 2.78309
[1mStep[0m  [50/106], [94mLoss[0m : 2.68789
[1mStep[0m  [60/106], [94mLoss[0m : 2.59427
[1mStep[0m  [70/106], [94mLoss[0m : 3.08742
[1mStep[0m  [80/106], [94mLoss[0m : 2.52260
[1mStep[0m  [90/106], [94mLoss[0m : 2.34066
[1mStep[0m  [100/106], [94mLoss[0m : 2.34445

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.880, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27411
[1mStep[0m  [10/106], [94mLoss[0m : 2.81105
[1mStep[0m  [20/106], [94mLoss[0m : 3.20354
[1mStep[0m  [30/106], [94mLoss[0m : 2.68510
[1mStep[0m  [40/106], [94mLoss[0m : 2.82587
[1mStep[0m  [50/106], [94mLoss[0m : 2.46646
[1mStep[0m  [60/106], [94mLoss[0m : 2.56148
[1mStep[0m  [70/106], [94mLoss[0m : 2.71048
[1mStep[0m  [80/106], [94mLoss[0m : 2.86986
[1mStep[0m  [90/106], [94mLoss[0m : 2.67958
[1mStep[0m  [100/106], [94mLoss[0m : 2.47320

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.861, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.93536
[1mStep[0m  [10/106], [94mLoss[0m : 2.44336
[1mStep[0m  [20/106], [94mLoss[0m : 2.58142
[1mStep[0m  [30/106], [94mLoss[0m : 2.67854
[1mStep[0m  [40/106], [94mLoss[0m : 2.66785
[1mStep[0m  [50/106], [94mLoss[0m : 2.61193
[1mStep[0m  [60/106], [94mLoss[0m : 2.81264
[1mStep[0m  [70/106], [94mLoss[0m : 2.41722
[1mStep[0m  [80/106], [94mLoss[0m : 2.47971
[1mStep[0m  [90/106], [94mLoss[0m : 2.41191
[1mStep[0m  [100/106], [94mLoss[0m : 2.32906

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.832, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69787
[1mStep[0m  [10/106], [94mLoss[0m : 2.71774
[1mStep[0m  [20/106], [94mLoss[0m : 2.54012
[1mStep[0m  [30/106], [94mLoss[0m : 2.84246
[1mStep[0m  [40/106], [94mLoss[0m : 2.83216
[1mStep[0m  [50/106], [94mLoss[0m : 2.49492
[1mStep[0m  [60/106], [94mLoss[0m : 2.60856
[1mStep[0m  [70/106], [94mLoss[0m : 2.69939
[1mStep[0m  [80/106], [94mLoss[0m : 2.95522
[1mStep[0m  [90/106], [94mLoss[0m : 2.78652
[1mStep[0m  [100/106], [94mLoss[0m : 2.70611

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.847, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67407
[1mStep[0m  [10/106], [94mLoss[0m : 2.59549
[1mStep[0m  [20/106], [94mLoss[0m : 2.60522
[1mStep[0m  [30/106], [94mLoss[0m : 2.75998
[1mStep[0m  [40/106], [94mLoss[0m : 2.54583
[1mStep[0m  [50/106], [94mLoss[0m : 2.72744
[1mStep[0m  [60/106], [94mLoss[0m : 2.64186
[1mStep[0m  [70/106], [94mLoss[0m : 2.75167
[1mStep[0m  [80/106], [94mLoss[0m : 2.89400
[1mStep[0m  [90/106], [94mLoss[0m : 2.75752
[1mStep[0m  [100/106], [94mLoss[0m : 2.64918

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.798, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.87067
[1mStep[0m  [10/106], [94mLoss[0m : 3.11084
[1mStep[0m  [20/106], [94mLoss[0m : 2.75934
[1mStep[0m  [30/106], [94mLoss[0m : 2.60998
[1mStep[0m  [40/106], [94mLoss[0m : 2.80221
[1mStep[0m  [50/106], [94mLoss[0m : 2.44314
[1mStep[0m  [60/106], [94mLoss[0m : 2.43552
[1mStep[0m  [70/106], [94mLoss[0m : 2.80577
[1mStep[0m  [80/106], [94mLoss[0m : 2.54977
[1mStep[0m  [90/106], [94mLoss[0m : 2.75334
[1mStep[0m  [100/106], [94mLoss[0m : 2.95267

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58210
[1mStep[0m  [10/106], [94mLoss[0m : 2.72386
[1mStep[0m  [20/106], [94mLoss[0m : 2.64551
[1mStep[0m  [30/106], [94mLoss[0m : 2.57226
[1mStep[0m  [40/106], [94mLoss[0m : 2.86992
[1mStep[0m  [50/106], [94mLoss[0m : 2.71688
[1mStep[0m  [60/106], [94mLoss[0m : 2.40806
[1mStep[0m  [70/106], [94mLoss[0m : 2.93376
[1mStep[0m  [80/106], [94mLoss[0m : 2.56424
[1mStep[0m  [90/106], [94mLoss[0m : 2.82270
[1mStep[0m  [100/106], [94mLoss[0m : 2.62340

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.787, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46953
[1mStep[0m  [10/106], [94mLoss[0m : 2.98262
[1mStep[0m  [20/106], [94mLoss[0m : 2.75624
[1mStep[0m  [30/106], [94mLoss[0m : 2.71228
[1mStep[0m  [40/106], [94mLoss[0m : 2.76179
[1mStep[0m  [50/106], [94mLoss[0m : 2.63193
[1mStep[0m  [60/106], [94mLoss[0m : 2.56540
[1mStep[0m  [70/106], [94mLoss[0m : 2.67058
[1mStep[0m  [80/106], [94mLoss[0m : 2.42108
[1mStep[0m  [90/106], [94mLoss[0m : 2.86518
[1mStep[0m  [100/106], [94mLoss[0m : 2.70686

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.812, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80516
[1mStep[0m  [10/106], [94mLoss[0m : 2.45571
[1mStep[0m  [20/106], [94mLoss[0m : 2.94650
[1mStep[0m  [30/106], [94mLoss[0m : 2.69634
[1mStep[0m  [40/106], [94mLoss[0m : 2.55426
[1mStep[0m  [50/106], [94mLoss[0m : 2.61969
[1mStep[0m  [60/106], [94mLoss[0m : 2.90107
[1mStep[0m  [70/106], [94mLoss[0m : 2.62203
[1mStep[0m  [80/106], [94mLoss[0m : 2.77444
[1mStep[0m  [90/106], [94mLoss[0m : 2.60165
[1mStep[0m  [100/106], [94mLoss[0m : 2.62572

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.786, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75040
[1mStep[0m  [10/106], [94mLoss[0m : 2.92147
[1mStep[0m  [20/106], [94mLoss[0m : 2.48534
[1mStep[0m  [30/106], [94mLoss[0m : 2.28908
[1mStep[0m  [40/106], [94mLoss[0m : 2.59086
[1mStep[0m  [50/106], [94mLoss[0m : 2.75793
[1mStep[0m  [60/106], [94mLoss[0m : 2.68674
[1mStep[0m  [70/106], [94mLoss[0m : 2.77837
[1mStep[0m  [80/106], [94mLoss[0m : 2.66744
[1mStep[0m  [90/106], [94mLoss[0m : 2.22086
[1mStep[0m  [100/106], [94mLoss[0m : 2.88947

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.773, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.00970
[1mStep[0m  [10/106], [94mLoss[0m : 2.78714
[1mStep[0m  [20/106], [94mLoss[0m : 2.69375
[1mStep[0m  [30/106], [94mLoss[0m : 2.64680
[1mStep[0m  [40/106], [94mLoss[0m : 2.95188
[1mStep[0m  [50/106], [94mLoss[0m : 2.54808
[1mStep[0m  [60/106], [94mLoss[0m : 2.59883
[1mStep[0m  [70/106], [94mLoss[0m : 2.65189
[1mStep[0m  [80/106], [94mLoss[0m : 2.67376
[1mStep[0m  [90/106], [94mLoss[0m : 2.41922
[1mStep[0m  [100/106], [94mLoss[0m : 2.71129

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.731, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.97214
[1mStep[0m  [10/106], [94mLoss[0m : 2.28458
[1mStep[0m  [20/106], [94mLoss[0m : 2.55603
[1mStep[0m  [30/106], [94mLoss[0m : 2.70198
[1mStep[0m  [40/106], [94mLoss[0m : 2.11664
[1mStep[0m  [50/106], [94mLoss[0m : 2.15468
[1mStep[0m  [60/106], [94mLoss[0m : 2.94860
[1mStep[0m  [70/106], [94mLoss[0m : 2.67288
[1mStep[0m  [80/106], [94mLoss[0m : 2.51666
[1mStep[0m  [90/106], [94mLoss[0m : 2.76217
[1mStep[0m  [100/106], [94mLoss[0m : 2.45113

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.755, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69385
[1mStep[0m  [10/106], [94mLoss[0m : 2.55123
[1mStep[0m  [20/106], [94mLoss[0m : 2.76647
[1mStep[0m  [30/106], [94mLoss[0m : 2.49925
[1mStep[0m  [40/106], [94mLoss[0m : 2.55450
[1mStep[0m  [50/106], [94mLoss[0m : 2.73872
[1mStep[0m  [60/106], [94mLoss[0m : 2.32837
[1mStep[0m  [70/106], [94mLoss[0m : 3.06308
[1mStep[0m  [80/106], [94mLoss[0m : 2.39790
[1mStep[0m  [90/106], [94mLoss[0m : 2.49378
[1mStep[0m  [100/106], [94mLoss[0m : 2.68484

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.731, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.85840
[1mStep[0m  [10/106], [94mLoss[0m : 2.71889
[1mStep[0m  [20/106], [94mLoss[0m : 2.62535
[1mStep[0m  [30/106], [94mLoss[0m : 2.76366
[1mStep[0m  [40/106], [94mLoss[0m : 2.51340
[1mStep[0m  [50/106], [94mLoss[0m : 2.61030
[1mStep[0m  [60/106], [94mLoss[0m : 2.37749
[1mStep[0m  [70/106], [94mLoss[0m : 2.30933
[1mStep[0m  [80/106], [94mLoss[0m : 2.45926
[1mStep[0m  [90/106], [94mLoss[0m : 2.59501
[1mStep[0m  [100/106], [94mLoss[0m : 2.77869

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.731, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71339
[1mStep[0m  [10/106], [94mLoss[0m : 2.76405
[1mStep[0m  [20/106], [94mLoss[0m : 2.98453
[1mStep[0m  [30/106], [94mLoss[0m : 2.43410
[1mStep[0m  [40/106], [94mLoss[0m : 2.63085
[1mStep[0m  [50/106], [94mLoss[0m : 2.56882
[1mStep[0m  [60/106], [94mLoss[0m : 2.30370
[1mStep[0m  [70/106], [94mLoss[0m : 2.55362
[1mStep[0m  [80/106], [94mLoss[0m : 2.65331
[1mStep[0m  [90/106], [94mLoss[0m : 2.77133
[1mStep[0m  [100/106], [94mLoss[0m : 2.42092

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.726, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76392
[1mStep[0m  [10/106], [94mLoss[0m : 2.64295
[1mStep[0m  [20/106], [94mLoss[0m : 2.38560
[1mStep[0m  [30/106], [94mLoss[0m : 2.62790
[1mStep[0m  [40/106], [94mLoss[0m : 3.07001
[1mStep[0m  [50/106], [94mLoss[0m : 3.07446
[1mStep[0m  [60/106], [94mLoss[0m : 2.46329
[1mStep[0m  [70/106], [94mLoss[0m : 2.45969
[1mStep[0m  [80/106], [94mLoss[0m : 2.49334
[1mStep[0m  [90/106], [94mLoss[0m : 2.70389
[1mStep[0m  [100/106], [94mLoss[0m : 2.64429

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.743, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.87425
[1mStep[0m  [10/106], [94mLoss[0m : 2.70251
[1mStep[0m  [20/106], [94mLoss[0m : 2.40510
[1mStep[0m  [30/106], [94mLoss[0m : 2.72157
[1mStep[0m  [40/106], [94mLoss[0m : 2.63081
[1mStep[0m  [50/106], [94mLoss[0m : 2.43944
[1mStep[0m  [60/106], [94mLoss[0m : 2.80048
[1mStep[0m  [70/106], [94mLoss[0m : 2.59100
[1mStep[0m  [80/106], [94mLoss[0m : 2.53840
[1mStep[0m  [90/106], [94mLoss[0m : 2.57047
[1mStep[0m  [100/106], [94mLoss[0m : 2.43348

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.690, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.697
====================================

Phase 1 - Evaluation MAE:  2.6969641379590303
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.58558
[1mStep[0m  [10/106], [94mLoss[0m : 2.70123
[1mStep[0m  [20/106], [94mLoss[0m : 2.51420
[1mStep[0m  [30/106], [94mLoss[0m : 2.67514
[1mStep[0m  [40/106], [94mLoss[0m : 2.46848
[1mStep[0m  [50/106], [94mLoss[0m : 2.63273
[1mStep[0m  [60/106], [94mLoss[0m : 2.42460
[1mStep[0m  [70/106], [94mLoss[0m : 2.60109
[1mStep[0m  [80/106], [94mLoss[0m : 3.01184
[1mStep[0m  [90/106], [94mLoss[0m : 2.45290
[1mStep[0m  [100/106], [94mLoss[0m : 2.98749

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.701, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56345
[1mStep[0m  [10/106], [94mLoss[0m : 2.70564
[1mStep[0m  [20/106], [94mLoss[0m : 2.36745
[1mStep[0m  [30/106], [94mLoss[0m : 2.64836
[1mStep[0m  [40/106], [94mLoss[0m : 2.56206
[1mStep[0m  [50/106], [94mLoss[0m : 2.43515
[1mStep[0m  [60/106], [94mLoss[0m : 2.57261
[1mStep[0m  [70/106], [94mLoss[0m : 2.65545
[1mStep[0m  [80/106], [94mLoss[0m : 2.71095
[1mStep[0m  [90/106], [94mLoss[0m : 2.77219
[1mStep[0m  [100/106], [94mLoss[0m : 2.73227

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52245
[1mStep[0m  [10/106], [94mLoss[0m : 2.40017
[1mStep[0m  [20/106], [94mLoss[0m : 2.84473
[1mStep[0m  [30/106], [94mLoss[0m : 2.26756
[1mStep[0m  [40/106], [94mLoss[0m : 2.39298
[1mStep[0m  [50/106], [94mLoss[0m : 2.16459
[1mStep[0m  [60/106], [94mLoss[0m : 3.03524
[1mStep[0m  [70/106], [94mLoss[0m : 2.37407
[1mStep[0m  [80/106], [94mLoss[0m : 2.65060
[1mStep[0m  [90/106], [94mLoss[0m : 2.48888
[1mStep[0m  [100/106], [94mLoss[0m : 2.63737

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.715, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.82578
[1mStep[0m  [10/106], [94mLoss[0m : 2.54534
[1mStep[0m  [20/106], [94mLoss[0m : 2.40465
[1mStep[0m  [30/106], [94mLoss[0m : 2.80775
[1mStep[0m  [40/106], [94mLoss[0m : 2.88509
[1mStep[0m  [50/106], [94mLoss[0m : 2.46326
[1mStep[0m  [60/106], [94mLoss[0m : 2.99859
[1mStep[0m  [70/106], [94mLoss[0m : 2.68608
[1mStep[0m  [80/106], [94mLoss[0m : 2.78967
[1mStep[0m  [90/106], [94mLoss[0m : 2.49600
[1mStep[0m  [100/106], [94mLoss[0m : 2.79403

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.696, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61190
[1mStep[0m  [10/106], [94mLoss[0m : 2.41132
[1mStep[0m  [20/106], [94mLoss[0m : 2.75183
[1mStep[0m  [30/106], [94mLoss[0m : 2.47300
[1mStep[0m  [40/106], [94mLoss[0m : 2.79543
[1mStep[0m  [50/106], [94mLoss[0m : 2.54704
[1mStep[0m  [60/106], [94mLoss[0m : 2.68500
[1mStep[0m  [70/106], [94mLoss[0m : 2.71477
[1mStep[0m  [80/106], [94mLoss[0m : 2.46101
[1mStep[0m  [90/106], [94mLoss[0m : 2.38918
[1mStep[0m  [100/106], [94mLoss[0m : 2.69266

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.678, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80240
[1mStep[0m  [10/106], [94mLoss[0m : 2.65406
[1mStep[0m  [20/106], [94mLoss[0m : 2.49950
[1mStep[0m  [30/106], [94mLoss[0m : 2.63210
[1mStep[0m  [40/106], [94mLoss[0m : 2.53585
[1mStep[0m  [50/106], [94mLoss[0m : 2.64545
[1mStep[0m  [60/106], [94mLoss[0m : 2.71555
[1mStep[0m  [70/106], [94mLoss[0m : 2.32329
[1mStep[0m  [80/106], [94mLoss[0m : 2.55466
[1mStep[0m  [90/106], [94mLoss[0m : 2.77269
[1mStep[0m  [100/106], [94mLoss[0m : 2.73974

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.698, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75918
[1mStep[0m  [10/106], [94mLoss[0m : 2.68773
[1mStep[0m  [20/106], [94mLoss[0m : 2.63340
[1mStep[0m  [30/106], [94mLoss[0m : 2.93565
[1mStep[0m  [40/106], [94mLoss[0m : 2.47302
[1mStep[0m  [50/106], [94mLoss[0m : 2.58525
[1mStep[0m  [60/106], [94mLoss[0m : 3.05813
[1mStep[0m  [70/106], [94mLoss[0m : 2.53735
[1mStep[0m  [80/106], [94mLoss[0m : 2.66519
[1mStep[0m  [90/106], [94mLoss[0m : 2.63836
[1mStep[0m  [100/106], [94mLoss[0m : 2.55055

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.694, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62380
[1mStep[0m  [10/106], [94mLoss[0m : 2.60639
[1mStep[0m  [20/106], [94mLoss[0m : 2.52276
[1mStep[0m  [30/106], [94mLoss[0m : 2.89481
[1mStep[0m  [40/106], [94mLoss[0m : 2.45833
[1mStep[0m  [50/106], [94mLoss[0m : 2.44298
[1mStep[0m  [60/106], [94mLoss[0m : 2.71266
[1mStep[0m  [70/106], [94mLoss[0m : 2.41896
[1mStep[0m  [80/106], [94mLoss[0m : 2.29349
[1mStep[0m  [90/106], [94mLoss[0m : 2.66198
[1mStep[0m  [100/106], [94mLoss[0m : 2.73110

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51204
[1mStep[0m  [10/106], [94mLoss[0m : 2.57034
[1mStep[0m  [20/106], [94mLoss[0m : 2.52458
[1mStep[0m  [30/106], [94mLoss[0m : 2.55340
[1mStep[0m  [40/106], [94mLoss[0m : 2.70229
[1mStep[0m  [50/106], [94mLoss[0m : 2.54391
[1mStep[0m  [60/106], [94mLoss[0m : 2.60679
[1mStep[0m  [70/106], [94mLoss[0m : 2.71529
[1mStep[0m  [80/106], [94mLoss[0m : 2.31939
[1mStep[0m  [90/106], [94mLoss[0m : 2.50279
[1mStep[0m  [100/106], [94mLoss[0m : 2.39642

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.661, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34000
[1mStep[0m  [10/106], [94mLoss[0m : 2.47730
[1mStep[0m  [20/106], [94mLoss[0m : 2.82118
[1mStep[0m  [30/106], [94mLoss[0m : 2.51946
[1mStep[0m  [40/106], [94mLoss[0m : 2.71579
[1mStep[0m  [50/106], [94mLoss[0m : 2.26085
[1mStep[0m  [60/106], [94mLoss[0m : 2.74575
[1mStep[0m  [70/106], [94mLoss[0m : 2.76274
[1mStep[0m  [80/106], [94mLoss[0m : 2.44819
[1mStep[0m  [90/106], [94mLoss[0m : 2.76774
[1mStep[0m  [100/106], [94mLoss[0m : 2.42500

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63993
[1mStep[0m  [10/106], [94mLoss[0m : 2.40836
[1mStep[0m  [20/106], [94mLoss[0m : 2.41027
[1mStep[0m  [30/106], [94mLoss[0m : 2.30341
[1mStep[0m  [40/106], [94mLoss[0m : 2.64968
[1mStep[0m  [50/106], [94mLoss[0m : 2.52858
[1mStep[0m  [60/106], [94mLoss[0m : 2.80436
[1mStep[0m  [70/106], [94mLoss[0m : 2.55215
[1mStep[0m  [80/106], [94mLoss[0m : 2.73116
[1mStep[0m  [90/106], [94mLoss[0m : 2.56674
[1mStep[0m  [100/106], [94mLoss[0m : 2.66721

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58340
[1mStep[0m  [10/106], [94mLoss[0m : 2.55249
[1mStep[0m  [20/106], [94mLoss[0m : 2.70552
[1mStep[0m  [30/106], [94mLoss[0m : 2.36366
[1mStep[0m  [40/106], [94mLoss[0m : 2.34153
[1mStep[0m  [50/106], [94mLoss[0m : 2.45239
[1mStep[0m  [60/106], [94mLoss[0m : 2.44623
[1mStep[0m  [70/106], [94mLoss[0m : 2.28879
[1mStep[0m  [80/106], [94mLoss[0m : 2.08330
[1mStep[0m  [90/106], [94mLoss[0m : 2.94006
[1mStep[0m  [100/106], [94mLoss[0m : 2.63532

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.632, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58248
[1mStep[0m  [10/106], [94mLoss[0m : 2.57194
[1mStep[0m  [20/106], [94mLoss[0m : 2.80178
[1mStep[0m  [30/106], [94mLoss[0m : 2.40780
[1mStep[0m  [40/106], [94mLoss[0m : 2.51522
[1mStep[0m  [50/106], [94mLoss[0m : 2.68495
[1mStep[0m  [60/106], [94mLoss[0m : 2.09542
[1mStep[0m  [70/106], [94mLoss[0m : 2.62348
[1mStep[0m  [80/106], [94mLoss[0m : 2.39748
[1mStep[0m  [90/106], [94mLoss[0m : 2.79357
[1mStep[0m  [100/106], [94mLoss[0m : 2.52305

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.591, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67553
[1mStep[0m  [10/106], [94mLoss[0m : 2.46251
[1mStep[0m  [20/106], [94mLoss[0m : 2.53996
[1mStep[0m  [30/106], [94mLoss[0m : 2.66039
[1mStep[0m  [40/106], [94mLoss[0m : 2.84519
[1mStep[0m  [50/106], [94mLoss[0m : 2.49908
[1mStep[0m  [60/106], [94mLoss[0m : 2.69926
[1mStep[0m  [70/106], [94mLoss[0m : 2.61330
[1mStep[0m  [80/106], [94mLoss[0m : 2.62485
[1mStep[0m  [90/106], [94mLoss[0m : 2.59004
[1mStep[0m  [100/106], [94mLoss[0m : 2.56517

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.592, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66084
[1mStep[0m  [10/106], [94mLoss[0m : 2.83241
[1mStep[0m  [20/106], [94mLoss[0m : 2.51383
[1mStep[0m  [30/106], [94mLoss[0m : 2.76651
[1mStep[0m  [40/106], [94mLoss[0m : 2.57895
[1mStep[0m  [50/106], [94mLoss[0m : 2.57564
[1mStep[0m  [60/106], [94mLoss[0m : 2.51498
[1mStep[0m  [70/106], [94mLoss[0m : 2.76700
[1mStep[0m  [80/106], [94mLoss[0m : 2.68907
[1mStep[0m  [90/106], [94mLoss[0m : 2.70940
[1mStep[0m  [100/106], [94mLoss[0m : 2.88103

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.613, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43472
[1mStep[0m  [10/106], [94mLoss[0m : 2.51576
[1mStep[0m  [20/106], [94mLoss[0m : 2.51008
[1mStep[0m  [30/106], [94mLoss[0m : 2.41572
[1mStep[0m  [40/106], [94mLoss[0m : 2.92929
[1mStep[0m  [50/106], [94mLoss[0m : 2.60554
[1mStep[0m  [60/106], [94mLoss[0m : 2.20949
[1mStep[0m  [70/106], [94mLoss[0m : 2.36969
[1mStep[0m  [80/106], [94mLoss[0m : 2.28728
[1mStep[0m  [90/106], [94mLoss[0m : 2.56568
[1mStep[0m  [100/106], [94mLoss[0m : 2.49821

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.595, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.85153
[1mStep[0m  [10/106], [94mLoss[0m : 2.51871
[1mStep[0m  [20/106], [94mLoss[0m : 2.53142
[1mStep[0m  [30/106], [94mLoss[0m : 2.48478
[1mStep[0m  [40/106], [94mLoss[0m : 2.31805
[1mStep[0m  [50/106], [94mLoss[0m : 2.45093
[1mStep[0m  [60/106], [94mLoss[0m : 2.69112
[1mStep[0m  [70/106], [94mLoss[0m : 2.80173
[1mStep[0m  [80/106], [94mLoss[0m : 2.39982
[1mStep[0m  [90/106], [94mLoss[0m : 2.44226
[1mStep[0m  [100/106], [94mLoss[0m : 2.45525

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.593, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35123
[1mStep[0m  [10/106], [94mLoss[0m : 2.39760
[1mStep[0m  [20/106], [94mLoss[0m : 2.65814
[1mStep[0m  [30/106], [94mLoss[0m : 2.46575
[1mStep[0m  [40/106], [94mLoss[0m : 2.58400
[1mStep[0m  [50/106], [94mLoss[0m : 2.61093
[1mStep[0m  [60/106], [94mLoss[0m : 2.63999
[1mStep[0m  [70/106], [94mLoss[0m : 2.43338
[1mStep[0m  [80/106], [94mLoss[0m : 2.58807
[1mStep[0m  [90/106], [94mLoss[0m : 2.45847
[1mStep[0m  [100/106], [94mLoss[0m : 2.65489

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.594, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38575
[1mStep[0m  [10/106], [94mLoss[0m : 2.63993
[1mStep[0m  [20/106], [94mLoss[0m : 2.65724
[1mStep[0m  [30/106], [94mLoss[0m : 2.27982
[1mStep[0m  [40/106], [94mLoss[0m : 2.79781
[1mStep[0m  [50/106], [94mLoss[0m : 2.35300
[1mStep[0m  [60/106], [94mLoss[0m : 2.76544
[1mStep[0m  [70/106], [94mLoss[0m : 2.46669
[1mStep[0m  [80/106], [94mLoss[0m : 2.54802
[1mStep[0m  [90/106], [94mLoss[0m : 2.75432
[1mStep[0m  [100/106], [94mLoss[0m : 2.45762

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.578, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71441
[1mStep[0m  [10/106], [94mLoss[0m : 2.38218
[1mStep[0m  [20/106], [94mLoss[0m : 2.31738
[1mStep[0m  [30/106], [94mLoss[0m : 2.47085
[1mStep[0m  [40/106], [94mLoss[0m : 2.38287
[1mStep[0m  [50/106], [94mLoss[0m : 2.30205
[1mStep[0m  [60/106], [94mLoss[0m : 2.27051
[1mStep[0m  [70/106], [94mLoss[0m : 2.75409
[1mStep[0m  [80/106], [94mLoss[0m : 2.04474
[1mStep[0m  [90/106], [94mLoss[0m : 2.57587
[1mStep[0m  [100/106], [94mLoss[0m : 2.63249

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.571, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75646
[1mStep[0m  [10/106], [94mLoss[0m : 2.55071
[1mStep[0m  [20/106], [94mLoss[0m : 2.76139
[1mStep[0m  [30/106], [94mLoss[0m : 2.33854
[1mStep[0m  [40/106], [94mLoss[0m : 2.63374
[1mStep[0m  [50/106], [94mLoss[0m : 2.58962
[1mStep[0m  [60/106], [94mLoss[0m : 2.19868
[1mStep[0m  [70/106], [94mLoss[0m : 2.67572
[1mStep[0m  [80/106], [94mLoss[0m : 2.23855
[1mStep[0m  [90/106], [94mLoss[0m : 2.71895
[1mStep[0m  [100/106], [94mLoss[0m : 2.22188

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.591, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41176
[1mStep[0m  [10/106], [94mLoss[0m : 2.45917
[1mStep[0m  [20/106], [94mLoss[0m : 2.52835
[1mStep[0m  [30/106], [94mLoss[0m : 2.40256
[1mStep[0m  [40/106], [94mLoss[0m : 2.29693
[1mStep[0m  [50/106], [94mLoss[0m : 2.56457
[1mStep[0m  [60/106], [94mLoss[0m : 2.68222
[1mStep[0m  [70/106], [94mLoss[0m : 2.52158
[1mStep[0m  [80/106], [94mLoss[0m : 2.22373
[1mStep[0m  [90/106], [94mLoss[0m : 2.30802
[1mStep[0m  [100/106], [94mLoss[0m : 2.77004

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.577, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53905
[1mStep[0m  [10/106], [94mLoss[0m : 2.51720
[1mStep[0m  [20/106], [94mLoss[0m : 2.58512
[1mStep[0m  [30/106], [94mLoss[0m : 2.39939
[1mStep[0m  [40/106], [94mLoss[0m : 2.52973
[1mStep[0m  [50/106], [94mLoss[0m : 2.77186
[1mStep[0m  [60/106], [94mLoss[0m : 2.29667
[1mStep[0m  [70/106], [94mLoss[0m : 2.33565
[1mStep[0m  [80/106], [94mLoss[0m : 2.52187
[1mStep[0m  [90/106], [94mLoss[0m : 2.78215
[1mStep[0m  [100/106], [94mLoss[0m : 2.69609

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.586, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.81718
[1mStep[0m  [10/106], [94mLoss[0m : 2.42952
[1mStep[0m  [20/106], [94mLoss[0m : 2.48613
[1mStep[0m  [30/106], [94mLoss[0m : 2.42199
[1mStep[0m  [40/106], [94mLoss[0m : 2.38540
[1mStep[0m  [50/106], [94mLoss[0m : 2.43212
[1mStep[0m  [60/106], [94mLoss[0m : 2.48914
[1mStep[0m  [70/106], [94mLoss[0m : 2.53887
[1mStep[0m  [80/106], [94mLoss[0m : 2.41518
[1mStep[0m  [90/106], [94mLoss[0m : 2.34819
[1mStep[0m  [100/106], [94mLoss[0m : 2.88094

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.578, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46072
[1mStep[0m  [10/106], [94mLoss[0m : 2.51573
[1mStep[0m  [20/106], [94mLoss[0m : 2.71359
[1mStep[0m  [30/106], [94mLoss[0m : 2.59627
[1mStep[0m  [40/106], [94mLoss[0m : 2.31867
[1mStep[0m  [50/106], [94mLoss[0m : 2.37686
[1mStep[0m  [60/106], [94mLoss[0m : 2.31010
[1mStep[0m  [70/106], [94mLoss[0m : 2.38880
[1mStep[0m  [80/106], [94mLoss[0m : 2.71158
[1mStep[0m  [90/106], [94mLoss[0m : 2.56543
[1mStep[0m  [100/106], [94mLoss[0m : 2.43266

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.567, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47279
[1mStep[0m  [10/106], [94mLoss[0m : 2.50753
[1mStep[0m  [20/106], [94mLoss[0m : 2.61780
[1mStep[0m  [30/106], [94mLoss[0m : 2.39128
[1mStep[0m  [40/106], [94mLoss[0m : 2.48810
[1mStep[0m  [50/106], [94mLoss[0m : 2.45177
[1mStep[0m  [60/106], [94mLoss[0m : 2.43346
[1mStep[0m  [70/106], [94mLoss[0m : 2.13458
[1mStep[0m  [80/106], [94mLoss[0m : 2.35453
[1mStep[0m  [90/106], [94mLoss[0m : 2.58926
[1mStep[0m  [100/106], [94mLoss[0m : 2.26288

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.571, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18143
[1mStep[0m  [10/106], [94mLoss[0m : 2.35935
[1mStep[0m  [20/106], [94mLoss[0m : 2.38118
[1mStep[0m  [30/106], [94mLoss[0m : 2.66301
[1mStep[0m  [40/106], [94mLoss[0m : 2.16766
[1mStep[0m  [50/106], [94mLoss[0m : 2.76061
[1mStep[0m  [60/106], [94mLoss[0m : 2.48481
[1mStep[0m  [70/106], [94mLoss[0m : 2.32185
[1mStep[0m  [80/106], [94mLoss[0m : 2.34430
[1mStep[0m  [90/106], [94mLoss[0m : 2.50434
[1mStep[0m  [100/106], [94mLoss[0m : 2.45918

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.578, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24929
[1mStep[0m  [10/106], [94mLoss[0m : 2.37205
[1mStep[0m  [20/106], [94mLoss[0m : 2.48930
[1mStep[0m  [30/106], [94mLoss[0m : 2.42437
[1mStep[0m  [40/106], [94mLoss[0m : 3.00131
[1mStep[0m  [50/106], [94mLoss[0m : 2.38079
[1mStep[0m  [60/106], [94mLoss[0m : 2.50081
[1mStep[0m  [70/106], [94mLoss[0m : 2.52081
[1mStep[0m  [80/106], [94mLoss[0m : 2.47260
[1mStep[0m  [90/106], [94mLoss[0m : 2.50172
[1mStep[0m  [100/106], [94mLoss[0m : 2.50803

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.568, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39071
[1mStep[0m  [10/106], [94mLoss[0m : 2.52113
[1mStep[0m  [20/106], [94mLoss[0m : 2.07540
[1mStep[0m  [30/106], [94mLoss[0m : 2.43745
[1mStep[0m  [40/106], [94mLoss[0m : 2.56391
[1mStep[0m  [50/106], [94mLoss[0m : 2.37796
[1mStep[0m  [60/106], [94mLoss[0m : 2.59314
[1mStep[0m  [70/106], [94mLoss[0m : 2.40908
[1mStep[0m  [80/106], [94mLoss[0m : 2.71874
[1mStep[0m  [90/106], [94mLoss[0m : 2.66488
[1mStep[0m  [100/106], [94mLoss[0m : 2.44623

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.562, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45403
[1mStep[0m  [10/106], [94mLoss[0m : 2.34579
[1mStep[0m  [20/106], [94mLoss[0m : 2.57050
[1mStep[0m  [30/106], [94mLoss[0m : 2.82684
[1mStep[0m  [40/106], [94mLoss[0m : 2.43955
[1mStep[0m  [50/106], [94mLoss[0m : 2.52419
[1mStep[0m  [60/106], [94mLoss[0m : 2.26440
[1mStep[0m  [70/106], [94mLoss[0m : 2.62008
[1mStep[0m  [80/106], [94mLoss[0m : 2.31553
[1mStep[0m  [90/106], [94mLoss[0m : 2.54185
[1mStep[0m  [100/106], [94mLoss[0m : 2.71656

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.554, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.553
====================================

Phase 2 - Evaluation MAE:  2.5531210179598824
MAE score P1        2.696964
MAE score P2        2.553121
loss                  2.4497
learning_rate         0.0001
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.94884
[1mStep[0m  [10/106], [94mLoss[0m : 11.21121
[1mStep[0m  [20/106], [94mLoss[0m : 10.43537
[1mStep[0m  [30/106], [94mLoss[0m : 11.07576
[1mStep[0m  [40/106], [94mLoss[0m : 10.90175
[1mStep[0m  [50/106], [94mLoss[0m : 10.54224
[1mStep[0m  [60/106], [94mLoss[0m : 10.29558
[1mStep[0m  [70/106], [94mLoss[0m : 10.79013
[1mStep[0m  [80/106], [94mLoss[0m : 10.49055
[1mStep[0m  [90/106], [94mLoss[0m : 10.45541
[1mStep[0m  [100/106], [94mLoss[0m : 10.47318

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.763, [92mTest[0m: 10.959, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.44597
[1mStep[0m  [10/106], [94mLoss[0m : 10.88738
[1mStep[0m  [20/106], [94mLoss[0m : 10.50516
[1mStep[0m  [30/106], [94mLoss[0m : 10.74522
[1mStep[0m  [40/106], [94mLoss[0m : 10.48679
[1mStep[0m  [50/106], [94mLoss[0m : 10.44523
[1mStep[0m  [60/106], [94mLoss[0m : 10.44427
[1mStep[0m  [70/106], [94mLoss[0m : 10.17850
[1mStep[0m  [80/106], [94mLoss[0m : 10.30166
[1mStep[0m  [90/106], [94mLoss[0m : 10.41454
[1mStep[0m  [100/106], [94mLoss[0m : 10.01588

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.312, [92mTest[0m: 10.646, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.92207
[1mStep[0m  [10/106], [94mLoss[0m : 10.08974
[1mStep[0m  [20/106], [94mLoss[0m : 9.33247
[1mStep[0m  [30/106], [94mLoss[0m : 10.12418
[1mStep[0m  [40/106], [94mLoss[0m : 9.79649
[1mStep[0m  [50/106], [94mLoss[0m : 9.84468
[1mStep[0m  [60/106], [94mLoss[0m : 9.83334
[1mStep[0m  [70/106], [94mLoss[0m : 9.64296
[1mStep[0m  [80/106], [94mLoss[0m : 9.83599
[1mStep[0m  [90/106], [94mLoss[0m : 9.58337
[1mStep[0m  [100/106], [94mLoss[0m : 9.64688

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.848, [92mTest[0m: 10.264, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.53110
[1mStep[0m  [10/106], [94mLoss[0m : 9.96866
[1mStep[0m  [20/106], [94mLoss[0m : 9.41124
[1mStep[0m  [30/106], [94mLoss[0m : 9.08572
[1mStep[0m  [40/106], [94mLoss[0m : 9.63771
[1mStep[0m  [50/106], [94mLoss[0m : 9.55006
[1mStep[0m  [60/106], [94mLoss[0m : 9.53277
[1mStep[0m  [70/106], [94mLoss[0m : 8.68525
[1mStep[0m  [80/106], [94mLoss[0m : 9.07346
[1mStep[0m  [90/106], [94mLoss[0m : 8.76394
[1mStep[0m  [100/106], [94mLoss[0m : 9.26713

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.391, [92mTest[0m: 9.876, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.28597
[1mStep[0m  [10/106], [94mLoss[0m : 8.59094
[1mStep[0m  [20/106], [94mLoss[0m : 9.40941
[1mStep[0m  [30/106], [94mLoss[0m : 8.86337
[1mStep[0m  [40/106], [94mLoss[0m : 8.96748
[1mStep[0m  [50/106], [94mLoss[0m : 8.91396
[1mStep[0m  [60/106], [94mLoss[0m : 8.23700
[1mStep[0m  [70/106], [94mLoss[0m : 9.27245
[1mStep[0m  [80/106], [94mLoss[0m : 8.99911
[1mStep[0m  [90/106], [94mLoss[0m : 8.68604
[1mStep[0m  [100/106], [94mLoss[0m : 8.56020

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.939, [92mTest[0m: 9.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.60979
[1mStep[0m  [10/106], [94mLoss[0m : 8.96044
[1mStep[0m  [20/106], [94mLoss[0m : 7.92044
[1mStep[0m  [30/106], [94mLoss[0m : 8.69226
[1mStep[0m  [40/106], [94mLoss[0m : 8.26106
[1mStep[0m  [50/106], [94mLoss[0m : 8.12280
[1mStep[0m  [60/106], [94mLoss[0m : 8.07125
[1mStep[0m  [70/106], [94mLoss[0m : 8.50972
[1mStep[0m  [80/106], [94mLoss[0m : 8.14142
[1mStep[0m  [90/106], [94mLoss[0m : 8.98413
[1mStep[0m  [100/106], [94mLoss[0m : 8.53027

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.490, [92mTest[0m: 9.162, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.93264
[1mStep[0m  [10/106], [94mLoss[0m : 8.54775
[1mStep[0m  [20/106], [94mLoss[0m : 7.84302
[1mStep[0m  [30/106], [94mLoss[0m : 7.83108
[1mStep[0m  [40/106], [94mLoss[0m : 8.27160
[1mStep[0m  [50/106], [94mLoss[0m : 8.04123
[1mStep[0m  [60/106], [94mLoss[0m : 7.97325
[1mStep[0m  [70/106], [94mLoss[0m : 8.26687
[1mStep[0m  [80/106], [94mLoss[0m : 8.22460
[1mStep[0m  [90/106], [94mLoss[0m : 8.38651
[1mStep[0m  [100/106], [94mLoss[0m : 7.71673

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.051, [92mTest[0m: 8.777, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.47756
[1mStep[0m  [10/106], [94mLoss[0m : 7.89310
[1mStep[0m  [20/106], [94mLoss[0m : 7.80840
[1mStep[0m  [30/106], [94mLoss[0m : 7.84352
[1mStep[0m  [40/106], [94mLoss[0m : 8.19119
[1mStep[0m  [50/106], [94mLoss[0m : 7.70437
[1mStep[0m  [60/106], [94mLoss[0m : 7.54633
[1mStep[0m  [70/106], [94mLoss[0m : 7.84889
[1mStep[0m  [80/106], [94mLoss[0m : 8.26115
[1mStep[0m  [90/106], [94mLoss[0m : 7.31135
[1mStep[0m  [100/106], [94mLoss[0m : 7.28504

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.629, [92mTest[0m: 8.394, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.31592
[1mStep[0m  [10/106], [94mLoss[0m : 7.30405
[1mStep[0m  [20/106], [94mLoss[0m : 7.63223
[1mStep[0m  [30/106], [94mLoss[0m : 7.40869
[1mStep[0m  [40/106], [94mLoss[0m : 7.32176
[1mStep[0m  [50/106], [94mLoss[0m : 7.76166
[1mStep[0m  [60/106], [94mLoss[0m : 7.28704
[1mStep[0m  [70/106], [94mLoss[0m : 6.86783
[1mStep[0m  [80/106], [94mLoss[0m : 7.35917
[1mStep[0m  [90/106], [94mLoss[0m : 6.47224
[1mStep[0m  [100/106], [94mLoss[0m : 6.63921

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.225, [92mTest[0m: 8.031, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.30810
[1mStep[0m  [10/106], [94mLoss[0m : 6.71547
[1mStep[0m  [20/106], [94mLoss[0m : 6.75541
[1mStep[0m  [30/106], [94mLoss[0m : 7.22091
[1mStep[0m  [40/106], [94mLoss[0m : 7.22204
[1mStep[0m  [50/106], [94mLoss[0m : 6.94871
[1mStep[0m  [60/106], [94mLoss[0m : 6.99984
[1mStep[0m  [70/106], [94mLoss[0m : 6.59132
[1mStep[0m  [80/106], [94mLoss[0m : 7.00957
[1mStep[0m  [90/106], [94mLoss[0m : 7.17696
[1mStep[0m  [100/106], [94mLoss[0m : 6.61551

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.833, [92mTest[0m: 7.722, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.41549
[1mStep[0m  [10/106], [94mLoss[0m : 6.78330
[1mStep[0m  [20/106], [94mLoss[0m : 6.19173
[1mStep[0m  [30/106], [94mLoss[0m : 6.21589
[1mStep[0m  [40/106], [94mLoss[0m : 6.37658
[1mStep[0m  [50/106], [94mLoss[0m : 6.19120
[1mStep[0m  [60/106], [94mLoss[0m : 6.76017
[1mStep[0m  [70/106], [94mLoss[0m : 5.95441
[1mStep[0m  [80/106], [94mLoss[0m : 6.43116
[1mStep[0m  [90/106], [94mLoss[0m : 6.81597
[1mStep[0m  [100/106], [94mLoss[0m : 6.82867

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.474, [92mTest[0m: 7.353, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.92036
[1mStep[0m  [10/106], [94mLoss[0m : 6.33952
[1mStep[0m  [20/106], [94mLoss[0m : 6.63645
[1mStep[0m  [30/106], [94mLoss[0m : 6.20978
[1mStep[0m  [40/106], [94mLoss[0m : 5.80013
[1mStep[0m  [50/106], [94mLoss[0m : 6.18119
[1mStep[0m  [60/106], [94mLoss[0m : 5.69610
[1mStep[0m  [70/106], [94mLoss[0m : 6.07239
[1mStep[0m  [80/106], [94mLoss[0m : 5.86448
[1mStep[0m  [90/106], [94mLoss[0m : 6.02629
[1mStep[0m  [100/106], [94mLoss[0m : 5.80718

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.127, [92mTest[0m: 7.023, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.96650
[1mStep[0m  [10/106], [94mLoss[0m : 5.71095
[1mStep[0m  [20/106], [94mLoss[0m : 5.74073
[1mStep[0m  [30/106], [94mLoss[0m : 5.65922
[1mStep[0m  [40/106], [94mLoss[0m : 6.38225
[1mStep[0m  [50/106], [94mLoss[0m : 5.45528
[1mStep[0m  [60/106], [94mLoss[0m : 6.04159
[1mStep[0m  [70/106], [94mLoss[0m : 5.41530
[1mStep[0m  [80/106], [94mLoss[0m : 5.52502
[1mStep[0m  [90/106], [94mLoss[0m : 6.09694
[1mStep[0m  [100/106], [94mLoss[0m : 5.76669

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.811, [92mTest[0m: 6.744, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.60198
[1mStep[0m  [10/106], [94mLoss[0m : 5.64669
[1mStep[0m  [20/106], [94mLoss[0m : 5.74155
[1mStep[0m  [30/106], [94mLoss[0m : 5.61582
[1mStep[0m  [40/106], [94mLoss[0m : 5.48738
[1mStep[0m  [50/106], [94mLoss[0m : 5.75324
[1mStep[0m  [60/106], [94mLoss[0m : 5.67662
[1mStep[0m  [70/106], [94mLoss[0m : 5.68555
[1mStep[0m  [80/106], [94mLoss[0m : 5.67003
[1mStep[0m  [90/106], [94mLoss[0m : 5.79048
[1mStep[0m  [100/106], [94mLoss[0m : 5.63531

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.503, [92mTest[0m: 6.441, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.19019
[1mStep[0m  [10/106], [94mLoss[0m : 4.95072
[1mStep[0m  [20/106], [94mLoss[0m : 5.42235
[1mStep[0m  [30/106], [94mLoss[0m : 5.20797
[1mStep[0m  [40/106], [94mLoss[0m : 5.51086
[1mStep[0m  [50/106], [94mLoss[0m : 5.20363
[1mStep[0m  [60/106], [94mLoss[0m : 5.33058
[1mStep[0m  [70/106], [94mLoss[0m : 5.54715
[1mStep[0m  [80/106], [94mLoss[0m : 4.84241
[1mStep[0m  [90/106], [94mLoss[0m : 4.84693
[1mStep[0m  [100/106], [94mLoss[0m : 5.38950

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.211, [92mTest[0m: 6.184, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.14690
[1mStep[0m  [10/106], [94mLoss[0m : 5.10743
[1mStep[0m  [20/106], [94mLoss[0m : 5.58214
[1mStep[0m  [30/106], [94mLoss[0m : 4.91959
[1mStep[0m  [40/106], [94mLoss[0m : 4.49176
[1mStep[0m  [50/106], [94mLoss[0m : 5.12717
[1mStep[0m  [60/106], [94mLoss[0m : 5.20118
[1mStep[0m  [70/106], [94mLoss[0m : 5.00460
[1mStep[0m  [80/106], [94mLoss[0m : 5.27302
[1mStep[0m  [90/106], [94mLoss[0m : 4.69372
[1mStep[0m  [100/106], [94mLoss[0m : 4.28768

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.945, [92mTest[0m: 5.823, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.56223
[1mStep[0m  [10/106], [94mLoss[0m : 4.77412
[1mStep[0m  [20/106], [94mLoss[0m : 5.08676
[1mStep[0m  [30/106], [94mLoss[0m : 4.76964
[1mStep[0m  [40/106], [94mLoss[0m : 4.66697
[1mStep[0m  [50/106], [94mLoss[0m : 4.19018
[1mStep[0m  [60/106], [94mLoss[0m : 4.35910
[1mStep[0m  [70/106], [94mLoss[0m : 4.70329
[1mStep[0m  [80/106], [94mLoss[0m : 4.26662
[1mStep[0m  [90/106], [94mLoss[0m : 4.35973
[1mStep[0m  [100/106], [94mLoss[0m : 4.20567

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.682, [92mTest[0m: 5.534, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.51412
[1mStep[0m  [10/106], [94mLoss[0m : 4.35888
[1mStep[0m  [20/106], [94mLoss[0m : 4.56387
[1mStep[0m  [30/106], [94mLoss[0m : 3.85871
[1mStep[0m  [40/106], [94mLoss[0m : 4.34441
[1mStep[0m  [50/106], [94mLoss[0m : 4.52145
[1mStep[0m  [60/106], [94mLoss[0m : 4.34467
[1mStep[0m  [70/106], [94mLoss[0m : 4.32847
[1mStep[0m  [80/106], [94mLoss[0m : 4.76731
[1mStep[0m  [90/106], [94mLoss[0m : 3.67100
[1mStep[0m  [100/106], [94mLoss[0m : 4.33955

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.428, [92mTest[0m: 5.292, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.28650
[1mStep[0m  [10/106], [94mLoss[0m : 4.03083
[1mStep[0m  [20/106], [94mLoss[0m : 4.12352
[1mStep[0m  [30/106], [94mLoss[0m : 4.85695
[1mStep[0m  [40/106], [94mLoss[0m : 4.26967
[1mStep[0m  [50/106], [94mLoss[0m : 3.75879
[1mStep[0m  [60/106], [94mLoss[0m : 4.13465
[1mStep[0m  [70/106], [94mLoss[0m : 4.18671
[1mStep[0m  [80/106], [94mLoss[0m : 3.92936
[1mStep[0m  [90/106], [94mLoss[0m : 4.72933
[1mStep[0m  [100/106], [94mLoss[0m : 3.82277

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.192, [92mTest[0m: 5.017, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.91986
[1mStep[0m  [10/106], [94mLoss[0m : 3.98687
[1mStep[0m  [20/106], [94mLoss[0m : 4.25970
[1mStep[0m  [30/106], [94mLoss[0m : 4.00954
[1mStep[0m  [40/106], [94mLoss[0m : 4.07258
[1mStep[0m  [50/106], [94mLoss[0m : 3.49364
[1mStep[0m  [60/106], [94mLoss[0m : 4.22436
[1mStep[0m  [70/106], [94mLoss[0m : 4.03690
[1mStep[0m  [80/106], [94mLoss[0m : 4.76850
[1mStep[0m  [90/106], [94mLoss[0m : 4.41564
[1mStep[0m  [100/106], [94mLoss[0m : 3.89771

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.999, [92mTest[0m: 4.757, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.01475
[1mStep[0m  [10/106], [94mLoss[0m : 4.06828
[1mStep[0m  [20/106], [94mLoss[0m : 4.29003
[1mStep[0m  [30/106], [94mLoss[0m : 4.07304
[1mStep[0m  [40/106], [94mLoss[0m : 3.73002
[1mStep[0m  [50/106], [94mLoss[0m : 3.51789
[1mStep[0m  [60/106], [94mLoss[0m : 3.88579
[1mStep[0m  [70/106], [94mLoss[0m : 3.73810
[1mStep[0m  [80/106], [94mLoss[0m : 3.90527
[1mStep[0m  [90/106], [94mLoss[0m : 3.85751
[1mStep[0m  [100/106], [94mLoss[0m : 3.45659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.801, [92mTest[0m: 4.438, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.90683
[1mStep[0m  [10/106], [94mLoss[0m : 3.74875
[1mStep[0m  [20/106], [94mLoss[0m : 3.07750
[1mStep[0m  [30/106], [94mLoss[0m : 3.68528
[1mStep[0m  [40/106], [94mLoss[0m : 3.44915
[1mStep[0m  [50/106], [94mLoss[0m : 3.57130
[1mStep[0m  [60/106], [94mLoss[0m : 3.45255
[1mStep[0m  [70/106], [94mLoss[0m : 3.43813
[1mStep[0m  [80/106], [94mLoss[0m : 3.59739
[1mStep[0m  [90/106], [94mLoss[0m : 4.09003
[1mStep[0m  [100/106], [94mLoss[0m : 3.81327

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.638, [92mTest[0m: 4.269, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.18142
[1mStep[0m  [10/106], [94mLoss[0m : 3.42866
[1mStep[0m  [20/106], [94mLoss[0m : 4.17935
[1mStep[0m  [30/106], [94mLoss[0m : 3.46949
[1mStep[0m  [40/106], [94mLoss[0m : 3.57718
[1mStep[0m  [50/106], [94mLoss[0m : 3.94125
[1mStep[0m  [60/106], [94mLoss[0m : 3.85333
[1mStep[0m  [70/106], [94mLoss[0m : 3.41302
[1mStep[0m  [80/106], [94mLoss[0m : 3.35577
[1mStep[0m  [90/106], [94mLoss[0m : 3.75360
[1mStep[0m  [100/106], [94mLoss[0m : 2.88279

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.512, [92mTest[0m: 4.114, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.32057
[1mStep[0m  [10/106], [94mLoss[0m : 3.76701
[1mStep[0m  [20/106], [94mLoss[0m : 3.66011
[1mStep[0m  [30/106], [94mLoss[0m : 3.25833
[1mStep[0m  [40/106], [94mLoss[0m : 3.82290
[1mStep[0m  [50/106], [94mLoss[0m : 3.23745
[1mStep[0m  [60/106], [94mLoss[0m : 3.40018
[1mStep[0m  [70/106], [94mLoss[0m : 3.46643
[1mStep[0m  [80/106], [94mLoss[0m : 2.98045
[1mStep[0m  [90/106], [94mLoss[0m : 2.87964
[1mStep[0m  [100/106], [94mLoss[0m : 3.42598

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.399, [92mTest[0m: 3.958, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.39647
[1mStep[0m  [10/106], [94mLoss[0m : 3.16802
[1mStep[0m  [20/106], [94mLoss[0m : 3.66213
[1mStep[0m  [30/106], [94mLoss[0m : 3.48463
[1mStep[0m  [40/106], [94mLoss[0m : 2.79144
[1mStep[0m  [50/106], [94mLoss[0m : 2.83033
[1mStep[0m  [60/106], [94mLoss[0m : 3.39997
[1mStep[0m  [70/106], [94mLoss[0m : 3.30040
[1mStep[0m  [80/106], [94mLoss[0m : 3.17533
[1mStep[0m  [90/106], [94mLoss[0m : 3.43949
[1mStep[0m  [100/106], [94mLoss[0m : 3.77137

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.302, [92mTest[0m: 3.801, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.51772
[1mStep[0m  [10/106], [94mLoss[0m : 3.27006
[1mStep[0m  [20/106], [94mLoss[0m : 3.79451
[1mStep[0m  [30/106], [94mLoss[0m : 2.76771
[1mStep[0m  [40/106], [94mLoss[0m : 3.13581
[1mStep[0m  [50/106], [94mLoss[0m : 3.52277
[1mStep[0m  [60/106], [94mLoss[0m : 2.82427
[1mStep[0m  [70/106], [94mLoss[0m : 3.01892
[1mStep[0m  [80/106], [94mLoss[0m : 3.39843
[1mStep[0m  [90/106], [94mLoss[0m : 2.93635
[1mStep[0m  [100/106], [94mLoss[0m : 2.93949

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.199, [92mTest[0m: 3.700, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.02691
[1mStep[0m  [10/106], [94mLoss[0m : 2.93083
[1mStep[0m  [20/106], [94mLoss[0m : 3.60557
[1mStep[0m  [30/106], [94mLoss[0m : 2.80496
[1mStep[0m  [40/106], [94mLoss[0m : 2.41264
[1mStep[0m  [50/106], [94mLoss[0m : 3.28315
[1mStep[0m  [60/106], [94mLoss[0m : 3.32481
[1mStep[0m  [70/106], [94mLoss[0m : 3.33416
[1mStep[0m  [80/106], [94mLoss[0m : 3.04141
[1mStep[0m  [90/106], [94mLoss[0m : 2.98837
[1mStep[0m  [100/106], [94mLoss[0m : 3.28484

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.103, [92mTest[0m: 3.523, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.24212
[1mStep[0m  [10/106], [94mLoss[0m : 3.24726
[1mStep[0m  [20/106], [94mLoss[0m : 2.95232
[1mStep[0m  [30/106], [94mLoss[0m : 3.00190
[1mStep[0m  [40/106], [94mLoss[0m : 3.15890
[1mStep[0m  [50/106], [94mLoss[0m : 3.35500
[1mStep[0m  [60/106], [94mLoss[0m : 3.20057
[1mStep[0m  [70/106], [94mLoss[0m : 3.34593
[1mStep[0m  [80/106], [94mLoss[0m : 2.95294
[1mStep[0m  [90/106], [94mLoss[0m : 2.94377
[1mStep[0m  [100/106], [94mLoss[0m : 2.79825

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.046, [92mTest[0m: 3.467, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39805
[1mStep[0m  [10/106], [94mLoss[0m : 3.05136
[1mStep[0m  [20/106], [94mLoss[0m : 2.69837
[1mStep[0m  [30/106], [94mLoss[0m : 2.84037
[1mStep[0m  [40/106], [94mLoss[0m : 2.80961
[1mStep[0m  [50/106], [94mLoss[0m : 3.19475
[1mStep[0m  [60/106], [94mLoss[0m : 2.87277
[1mStep[0m  [70/106], [94mLoss[0m : 2.65673
[1mStep[0m  [80/106], [94mLoss[0m : 3.16804
[1mStep[0m  [90/106], [94mLoss[0m : 2.78874
[1mStep[0m  [100/106], [94mLoss[0m : 3.03360

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.994, [92mTest[0m: 3.349, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.47038
[1mStep[0m  [10/106], [94mLoss[0m : 3.11745
[1mStep[0m  [20/106], [94mLoss[0m : 2.94773
[1mStep[0m  [30/106], [94mLoss[0m : 2.74310
[1mStep[0m  [40/106], [94mLoss[0m : 2.56615
[1mStep[0m  [50/106], [94mLoss[0m : 3.10141
[1mStep[0m  [60/106], [94mLoss[0m : 3.14232
[1mStep[0m  [70/106], [94mLoss[0m : 2.85827
[1mStep[0m  [80/106], [94mLoss[0m : 2.68154
[1mStep[0m  [90/106], [94mLoss[0m : 3.29286
[1mStep[0m  [100/106], [94mLoss[0m : 3.25954

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.948, [92mTest[0m: 3.286, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.258
====================================

Phase 1 - Evaluation MAE:  3.2579202022192613
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 3.02825
[1mStep[0m  [10/106], [94mLoss[0m : 2.63881
[1mStep[0m  [20/106], [94mLoss[0m : 2.87189
[1mStep[0m  [30/106], [94mLoss[0m : 3.01203
[1mStep[0m  [40/106], [94mLoss[0m : 2.71630
[1mStep[0m  [50/106], [94mLoss[0m : 2.77613
[1mStep[0m  [60/106], [94mLoss[0m : 2.68265
[1mStep[0m  [70/106], [94mLoss[0m : 2.82602
[1mStep[0m  [80/106], [94mLoss[0m : 3.00224
[1mStep[0m  [90/106], [94mLoss[0m : 2.87896
[1mStep[0m  [100/106], [94mLoss[0m : 2.66934

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.875, [92mTest[0m: 3.255, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67585
[1mStep[0m  [10/106], [94mLoss[0m : 2.90761
[1mStep[0m  [20/106], [94mLoss[0m : 3.28317
[1mStep[0m  [30/106], [94mLoss[0m : 2.83710
[1mStep[0m  [40/106], [94mLoss[0m : 2.62364
[1mStep[0m  [50/106], [94mLoss[0m : 3.33451
[1mStep[0m  [60/106], [94mLoss[0m : 2.70024
[1mStep[0m  [70/106], [94mLoss[0m : 2.92488
[1mStep[0m  [80/106], [94mLoss[0m : 2.58804
[1mStep[0m  [90/106], [94mLoss[0m : 2.58627
[1mStep[0m  [100/106], [94mLoss[0m : 2.69753

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.833, [92mTest[0m: 3.115, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69530
[1mStep[0m  [10/106], [94mLoss[0m : 2.95315
[1mStep[0m  [20/106], [94mLoss[0m : 3.25461
[1mStep[0m  [30/106], [94mLoss[0m : 2.66564
[1mStep[0m  [40/106], [94mLoss[0m : 2.70365
[1mStep[0m  [50/106], [94mLoss[0m : 2.86810
[1mStep[0m  [60/106], [94mLoss[0m : 2.83584
[1mStep[0m  [70/106], [94mLoss[0m : 2.91780
[1mStep[0m  [80/106], [94mLoss[0m : 2.67626
[1mStep[0m  [90/106], [94mLoss[0m : 2.72325
[1mStep[0m  [100/106], [94mLoss[0m : 2.84582

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.790, [92mTest[0m: 3.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80408
[1mStep[0m  [10/106], [94mLoss[0m : 2.72452
[1mStep[0m  [20/106], [94mLoss[0m : 2.89213
[1mStep[0m  [30/106], [94mLoss[0m : 2.29705
[1mStep[0m  [40/106], [94mLoss[0m : 2.63318
[1mStep[0m  [50/106], [94mLoss[0m : 2.42874
[1mStep[0m  [60/106], [94mLoss[0m : 2.80919
[1mStep[0m  [70/106], [94mLoss[0m : 2.86283
[1mStep[0m  [80/106], [94mLoss[0m : 2.73513
[1mStep[0m  [90/106], [94mLoss[0m : 2.63437
[1mStep[0m  [100/106], [94mLoss[0m : 3.12250

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.773, [92mTest[0m: 2.872, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48911
[1mStep[0m  [10/106], [94mLoss[0m : 2.32008
[1mStep[0m  [20/106], [94mLoss[0m : 2.87785
[1mStep[0m  [30/106], [94mLoss[0m : 2.65893
[1mStep[0m  [40/106], [94mLoss[0m : 2.66778
[1mStep[0m  [50/106], [94mLoss[0m : 2.71121
[1mStep[0m  [60/106], [94mLoss[0m : 2.81923
[1mStep[0m  [70/106], [94mLoss[0m : 2.59838
[1mStep[0m  [80/106], [94mLoss[0m : 2.57200
[1mStep[0m  [90/106], [94mLoss[0m : 2.90004
[1mStep[0m  [100/106], [94mLoss[0m : 2.52053

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.737, [92mTest[0m: 2.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62987
[1mStep[0m  [10/106], [94mLoss[0m : 2.72837
[1mStep[0m  [20/106], [94mLoss[0m : 2.71456
[1mStep[0m  [30/106], [94mLoss[0m : 2.67812
[1mStep[0m  [40/106], [94mLoss[0m : 2.69744
[1mStep[0m  [50/106], [94mLoss[0m : 2.82438
[1mStep[0m  [60/106], [94mLoss[0m : 2.34165
[1mStep[0m  [70/106], [94mLoss[0m : 2.61380
[1mStep[0m  [80/106], [94mLoss[0m : 2.81306
[1mStep[0m  [90/106], [94mLoss[0m : 2.81762
[1mStep[0m  [100/106], [94mLoss[0m : 2.49858

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.700, [92mTest[0m: 2.798, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44649
[1mStep[0m  [10/106], [94mLoss[0m : 2.45769
[1mStep[0m  [20/106], [94mLoss[0m : 3.14972
[1mStep[0m  [30/106], [94mLoss[0m : 2.66716
[1mStep[0m  [40/106], [94mLoss[0m : 2.79647
[1mStep[0m  [50/106], [94mLoss[0m : 2.26426
[1mStep[0m  [60/106], [94mLoss[0m : 2.89561
[1mStep[0m  [70/106], [94mLoss[0m : 2.68461
[1mStep[0m  [80/106], [94mLoss[0m : 2.63799
[1mStep[0m  [90/106], [94mLoss[0m : 2.69850
[1mStep[0m  [100/106], [94mLoss[0m : 2.76128

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.780, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72067
[1mStep[0m  [10/106], [94mLoss[0m : 2.67893
[1mStep[0m  [20/106], [94mLoss[0m : 2.76357
[1mStep[0m  [30/106], [94mLoss[0m : 2.66019
[1mStep[0m  [40/106], [94mLoss[0m : 3.01764
[1mStep[0m  [50/106], [94mLoss[0m : 2.60471
[1mStep[0m  [60/106], [94mLoss[0m : 2.49035
[1mStep[0m  [70/106], [94mLoss[0m : 2.69328
[1mStep[0m  [80/106], [94mLoss[0m : 2.59362
[1mStep[0m  [90/106], [94mLoss[0m : 2.52754
[1mStep[0m  [100/106], [94mLoss[0m : 2.77495

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.790, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68767
[1mStep[0m  [10/106], [94mLoss[0m : 2.77010
[1mStep[0m  [20/106], [94mLoss[0m : 2.76854
[1mStep[0m  [30/106], [94mLoss[0m : 2.64761
[1mStep[0m  [40/106], [94mLoss[0m : 2.25687
[1mStep[0m  [50/106], [94mLoss[0m : 2.45503
[1mStep[0m  [60/106], [94mLoss[0m : 2.42699
[1mStep[0m  [70/106], [94mLoss[0m : 2.45142
[1mStep[0m  [80/106], [94mLoss[0m : 2.33164
[1mStep[0m  [90/106], [94mLoss[0m : 3.01779
[1mStep[0m  [100/106], [94mLoss[0m : 2.87239

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.763, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57283
[1mStep[0m  [10/106], [94mLoss[0m : 2.39794
[1mStep[0m  [20/106], [94mLoss[0m : 2.81550
[1mStep[0m  [30/106], [94mLoss[0m : 2.88016
[1mStep[0m  [40/106], [94mLoss[0m : 2.45163
[1mStep[0m  [50/106], [94mLoss[0m : 2.59164
[1mStep[0m  [60/106], [94mLoss[0m : 2.94610
[1mStep[0m  [70/106], [94mLoss[0m : 2.80455
[1mStep[0m  [80/106], [94mLoss[0m : 2.71181
[1mStep[0m  [90/106], [94mLoss[0m : 2.55497
[1mStep[0m  [100/106], [94mLoss[0m : 3.06737

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70626
[1mStep[0m  [10/106], [94mLoss[0m : 2.89698
[1mStep[0m  [20/106], [94mLoss[0m : 2.86227
[1mStep[0m  [30/106], [94mLoss[0m : 2.64080
[1mStep[0m  [40/106], [94mLoss[0m : 2.46344
[1mStep[0m  [50/106], [94mLoss[0m : 2.65844
[1mStep[0m  [60/106], [94mLoss[0m : 2.24241
[1mStep[0m  [70/106], [94mLoss[0m : 2.63315
[1mStep[0m  [80/106], [94mLoss[0m : 2.42619
[1mStep[0m  [90/106], [94mLoss[0m : 3.13861
[1mStep[0m  [100/106], [94mLoss[0m : 2.33373

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.779, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73804
[1mStep[0m  [10/106], [94mLoss[0m : 2.48083
[1mStep[0m  [20/106], [94mLoss[0m : 2.51294
[1mStep[0m  [30/106], [94mLoss[0m : 2.58453
[1mStep[0m  [40/106], [94mLoss[0m : 3.22850
[1mStep[0m  [50/106], [94mLoss[0m : 2.14415
[1mStep[0m  [60/106], [94mLoss[0m : 2.52486
[1mStep[0m  [70/106], [94mLoss[0m : 2.50706
[1mStep[0m  [80/106], [94mLoss[0m : 2.75162
[1mStep[0m  [90/106], [94mLoss[0m : 2.42130
[1mStep[0m  [100/106], [94mLoss[0m : 2.61289

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.781, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56353
[1mStep[0m  [10/106], [94mLoss[0m : 2.35000
[1mStep[0m  [20/106], [94mLoss[0m : 2.46004
[1mStep[0m  [30/106], [94mLoss[0m : 2.56261
[1mStep[0m  [40/106], [94mLoss[0m : 2.40315
[1mStep[0m  [50/106], [94mLoss[0m : 2.70961
[1mStep[0m  [60/106], [94mLoss[0m : 2.43930
[1mStep[0m  [70/106], [94mLoss[0m : 2.89496
[1mStep[0m  [80/106], [94mLoss[0m : 2.61452
[1mStep[0m  [90/106], [94mLoss[0m : 2.41091
[1mStep[0m  [100/106], [94mLoss[0m : 2.64067

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76727
[1mStep[0m  [10/106], [94mLoss[0m : 2.50625
[1mStep[0m  [20/106], [94mLoss[0m : 2.66147
[1mStep[0m  [30/106], [94mLoss[0m : 2.55774
[1mStep[0m  [40/106], [94mLoss[0m : 2.51737
[1mStep[0m  [50/106], [94mLoss[0m : 2.69570
[1mStep[0m  [60/106], [94mLoss[0m : 2.95682
[1mStep[0m  [70/106], [94mLoss[0m : 2.71672
[1mStep[0m  [80/106], [94mLoss[0m : 2.37251
[1mStep[0m  [90/106], [94mLoss[0m : 2.36770
[1mStep[0m  [100/106], [94mLoss[0m : 2.84511

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69776
[1mStep[0m  [10/106], [94mLoss[0m : 2.91221
[1mStep[0m  [20/106], [94mLoss[0m : 2.58435
[1mStep[0m  [30/106], [94mLoss[0m : 2.72139
[1mStep[0m  [40/106], [94mLoss[0m : 2.72539
[1mStep[0m  [50/106], [94mLoss[0m : 2.75431
[1mStep[0m  [60/106], [94mLoss[0m : 2.78764
[1mStep[0m  [70/106], [94mLoss[0m : 2.64447
[1mStep[0m  [80/106], [94mLoss[0m : 2.78726
[1mStep[0m  [90/106], [94mLoss[0m : 2.84563
[1mStep[0m  [100/106], [94mLoss[0m : 2.59335

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.783, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67021
[1mStep[0m  [10/106], [94mLoss[0m : 2.81528
[1mStep[0m  [20/106], [94mLoss[0m : 2.43241
[1mStep[0m  [30/106], [94mLoss[0m : 2.35330
[1mStep[0m  [40/106], [94mLoss[0m : 2.60508
[1mStep[0m  [50/106], [94mLoss[0m : 2.47407
[1mStep[0m  [60/106], [94mLoss[0m : 2.98921
[1mStep[0m  [70/106], [94mLoss[0m : 2.49226
[1mStep[0m  [80/106], [94mLoss[0m : 2.80344
[1mStep[0m  [90/106], [94mLoss[0m : 2.51507
[1mStep[0m  [100/106], [94mLoss[0m : 2.74510

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69612
[1mStep[0m  [10/106], [94mLoss[0m : 2.67779
[1mStep[0m  [20/106], [94mLoss[0m : 2.64086
[1mStep[0m  [30/106], [94mLoss[0m : 2.69933
[1mStep[0m  [40/106], [94mLoss[0m : 2.47871
[1mStep[0m  [50/106], [94mLoss[0m : 2.93096
[1mStep[0m  [60/106], [94mLoss[0m : 2.59004
[1mStep[0m  [70/106], [94mLoss[0m : 2.27999
[1mStep[0m  [80/106], [94mLoss[0m : 2.49585
[1mStep[0m  [90/106], [94mLoss[0m : 2.30378
[1mStep[0m  [100/106], [94mLoss[0m : 2.42353

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77446
[1mStep[0m  [10/106], [94mLoss[0m : 2.61705
[1mStep[0m  [20/106], [94mLoss[0m : 2.58792
[1mStep[0m  [30/106], [94mLoss[0m : 2.60716
[1mStep[0m  [40/106], [94mLoss[0m : 2.57796
[1mStep[0m  [50/106], [94mLoss[0m : 2.50556
[1mStep[0m  [60/106], [94mLoss[0m : 2.72712
[1mStep[0m  [70/106], [94mLoss[0m : 2.42202
[1mStep[0m  [80/106], [94mLoss[0m : 2.49418
[1mStep[0m  [90/106], [94mLoss[0m : 2.95223
[1mStep[0m  [100/106], [94mLoss[0m : 2.75608

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.767, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43746
[1mStep[0m  [10/106], [94mLoss[0m : 2.57585
[1mStep[0m  [20/106], [94mLoss[0m : 2.43132
[1mStep[0m  [30/106], [94mLoss[0m : 2.88422
[1mStep[0m  [40/106], [94mLoss[0m : 2.74576
[1mStep[0m  [50/106], [94mLoss[0m : 2.70542
[1mStep[0m  [60/106], [94mLoss[0m : 2.47063
[1mStep[0m  [70/106], [94mLoss[0m : 2.82197
[1mStep[0m  [80/106], [94mLoss[0m : 2.51545
[1mStep[0m  [90/106], [94mLoss[0m : 2.58118
[1mStep[0m  [100/106], [94mLoss[0m : 2.59022

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.16450
[1mStep[0m  [10/106], [94mLoss[0m : 2.55409
[1mStep[0m  [20/106], [94mLoss[0m : 2.83899
[1mStep[0m  [30/106], [94mLoss[0m : 2.62077
[1mStep[0m  [40/106], [94mLoss[0m : 2.88469
[1mStep[0m  [50/106], [94mLoss[0m : 2.59725
[1mStep[0m  [60/106], [94mLoss[0m : 2.36610
[1mStep[0m  [70/106], [94mLoss[0m : 3.10743
[1mStep[0m  [80/106], [94mLoss[0m : 2.47991
[1mStep[0m  [90/106], [94mLoss[0m : 2.48289
[1mStep[0m  [100/106], [94mLoss[0m : 2.67639

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.821, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34457
[1mStep[0m  [10/106], [94mLoss[0m : 2.26993
[1mStep[0m  [20/106], [94mLoss[0m : 2.87226
[1mStep[0m  [30/106], [94mLoss[0m : 2.47071
[1mStep[0m  [40/106], [94mLoss[0m : 2.49667
[1mStep[0m  [50/106], [94mLoss[0m : 2.61633
[1mStep[0m  [60/106], [94mLoss[0m : 2.51872
[1mStep[0m  [70/106], [94mLoss[0m : 2.47729
[1mStep[0m  [80/106], [94mLoss[0m : 2.25007
[1mStep[0m  [90/106], [94mLoss[0m : 2.32506
[1mStep[0m  [100/106], [94mLoss[0m : 3.01981

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.818, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76648
[1mStep[0m  [10/106], [94mLoss[0m : 2.65211
[1mStep[0m  [20/106], [94mLoss[0m : 2.48324
[1mStep[0m  [30/106], [94mLoss[0m : 2.47213
[1mStep[0m  [40/106], [94mLoss[0m : 2.74001
[1mStep[0m  [50/106], [94mLoss[0m : 2.78163
[1mStep[0m  [60/106], [94mLoss[0m : 2.56329
[1mStep[0m  [70/106], [94mLoss[0m : 2.49394
[1mStep[0m  [80/106], [94mLoss[0m : 2.58048
[1mStep[0m  [90/106], [94mLoss[0m : 2.79724
[1mStep[0m  [100/106], [94mLoss[0m : 2.40670

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.815, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.99430
[1mStep[0m  [10/106], [94mLoss[0m : 2.73588
[1mStep[0m  [20/106], [94mLoss[0m : 2.65939
[1mStep[0m  [30/106], [94mLoss[0m : 2.40761
[1mStep[0m  [40/106], [94mLoss[0m : 2.45086
[1mStep[0m  [50/106], [94mLoss[0m : 2.64638
[1mStep[0m  [60/106], [94mLoss[0m : 3.05236
[1mStep[0m  [70/106], [94mLoss[0m : 2.58238
[1mStep[0m  [80/106], [94mLoss[0m : 2.81702
[1mStep[0m  [90/106], [94mLoss[0m : 2.60220
[1mStep[0m  [100/106], [94mLoss[0m : 2.55889

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.824, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53879
[1mStep[0m  [10/106], [94mLoss[0m : 2.76579
[1mStep[0m  [20/106], [94mLoss[0m : 2.36014
[1mStep[0m  [30/106], [94mLoss[0m : 2.58649
[1mStep[0m  [40/106], [94mLoss[0m : 2.60849
[1mStep[0m  [50/106], [94mLoss[0m : 2.67754
[1mStep[0m  [60/106], [94mLoss[0m : 2.52349
[1mStep[0m  [70/106], [94mLoss[0m : 3.03191
[1mStep[0m  [80/106], [94mLoss[0m : 2.49990
[1mStep[0m  [90/106], [94mLoss[0m : 2.57816
[1mStep[0m  [100/106], [94mLoss[0m : 2.46309

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.843, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15840
[1mStep[0m  [10/106], [94mLoss[0m : 2.44427
[1mStep[0m  [20/106], [94mLoss[0m : 2.53107
[1mStep[0m  [30/106], [94mLoss[0m : 2.56027
[1mStep[0m  [40/106], [94mLoss[0m : 2.58104
[1mStep[0m  [50/106], [94mLoss[0m : 2.88272
[1mStep[0m  [60/106], [94mLoss[0m : 2.24709
[1mStep[0m  [70/106], [94mLoss[0m : 2.61543
[1mStep[0m  [80/106], [94mLoss[0m : 2.34589
[1mStep[0m  [90/106], [94mLoss[0m : 2.54924
[1mStep[0m  [100/106], [94mLoss[0m : 2.61250

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.835, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52245
[1mStep[0m  [10/106], [94mLoss[0m : 2.34633
[1mStep[0m  [20/106], [94mLoss[0m : 2.71539
[1mStep[0m  [30/106], [94mLoss[0m : 2.56669
[1mStep[0m  [40/106], [94mLoss[0m : 2.43818
[1mStep[0m  [50/106], [94mLoss[0m : 2.53148
[1mStep[0m  [60/106], [94mLoss[0m : 2.39860
[1mStep[0m  [70/106], [94mLoss[0m : 2.70389
[1mStep[0m  [80/106], [94mLoss[0m : 2.79749
[1mStep[0m  [90/106], [94mLoss[0m : 2.84680
[1mStep[0m  [100/106], [94mLoss[0m : 2.53859

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.787, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61439
[1mStep[0m  [10/106], [94mLoss[0m : 2.29362
[1mStep[0m  [20/106], [94mLoss[0m : 2.72494
[1mStep[0m  [30/106], [94mLoss[0m : 2.51750
[1mStep[0m  [40/106], [94mLoss[0m : 2.72096
[1mStep[0m  [50/106], [94mLoss[0m : 2.53113
[1mStep[0m  [60/106], [94mLoss[0m : 2.46592
[1mStep[0m  [70/106], [94mLoss[0m : 2.70543
[1mStep[0m  [80/106], [94mLoss[0m : 2.74043
[1mStep[0m  [90/106], [94mLoss[0m : 2.44098
[1mStep[0m  [100/106], [94mLoss[0m : 2.72710

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.819, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45185
[1mStep[0m  [10/106], [94mLoss[0m : 2.67957
[1mStep[0m  [20/106], [94mLoss[0m : 2.27118
[1mStep[0m  [30/106], [94mLoss[0m : 2.57542
[1mStep[0m  [40/106], [94mLoss[0m : 3.13140
[1mStep[0m  [50/106], [94mLoss[0m : 2.18582
[1mStep[0m  [60/106], [94mLoss[0m : 2.41805
[1mStep[0m  [70/106], [94mLoss[0m : 2.34859
[1mStep[0m  [80/106], [94mLoss[0m : 2.63864
[1mStep[0m  [90/106], [94mLoss[0m : 2.91494
[1mStep[0m  [100/106], [94mLoss[0m : 2.52927

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.771, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31182
[1mStep[0m  [10/106], [94mLoss[0m : 2.38534
[1mStep[0m  [20/106], [94mLoss[0m : 2.43792
[1mStep[0m  [30/106], [94mLoss[0m : 2.58605
[1mStep[0m  [40/106], [94mLoss[0m : 2.59710
[1mStep[0m  [50/106], [94mLoss[0m : 2.55496
[1mStep[0m  [60/106], [94mLoss[0m : 3.02049
[1mStep[0m  [70/106], [94mLoss[0m : 2.45469
[1mStep[0m  [80/106], [94mLoss[0m : 2.40205
[1mStep[0m  [90/106], [94mLoss[0m : 2.56404
[1mStep[0m  [100/106], [94mLoss[0m : 2.40047

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.757, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46820
[1mStep[0m  [10/106], [94mLoss[0m : 2.39997
[1mStep[0m  [20/106], [94mLoss[0m : 2.21639
[1mStep[0m  [30/106], [94mLoss[0m : 2.56822
[1mStep[0m  [40/106], [94mLoss[0m : 2.58020
[1mStep[0m  [50/106], [94mLoss[0m : 2.60791
[1mStep[0m  [60/106], [94mLoss[0m : 2.48119
[1mStep[0m  [70/106], [94mLoss[0m : 2.13117
[1mStep[0m  [80/106], [94mLoss[0m : 2.72797
[1mStep[0m  [90/106], [94mLoss[0m : 2.63622
[1mStep[0m  [100/106], [94mLoss[0m : 2.34956

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.765, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.798
====================================

Phase 2 - Evaluation MAE:  2.7982463206884995
MAE score P1       3.25792
MAE score P2      2.798246
loss              2.544293
learning_rate       0.0001
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 10.56515
[1mStep[0m  [10/106], [94mLoss[0m : 10.80178
[1mStep[0m  [20/106], [94mLoss[0m : 10.09003
[1mStep[0m  [30/106], [94mLoss[0m : 10.87333
[1mStep[0m  [40/106], [94mLoss[0m : 10.21516
[1mStep[0m  [50/106], [94mLoss[0m : 10.48955
[1mStep[0m  [60/106], [94mLoss[0m : 10.13633
[1mStep[0m  [70/106], [94mLoss[0m : 10.41449
[1mStep[0m  [80/106], [94mLoss[0m : 10.31637
[1mStep[0m  [90/106], [94mLoss[0m : 10.21982
[1mStep[0m  [100/106], [94mLoss[0m : 10.00061

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.352, [92mTest[0m: 10.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.35354
[1mStep[0m  [10/106], [94mLoss[0m : 9.84980
[1mStep[0m  [20/106], [94mLoss[0m : 10.05806
[1mStep[0m  [30/106], [94mLoss[0m : 10.59824
[1mStep[0m  [40/106], [94mLoss[0m : 9.92901
[1mStep[0m  [50/106], [94mLoss[0m : 10.20689
[1mStep[0m  [60/106], [94mLoss[0m : 9.49681
[1mStep[0m  [70/106], [94mLoss[0m : 10.04933
[1mStep[0m  [80/106], [94mLoss[0m : 9.44317
[1mStep[0m  [90/106], [94mLoss[0m : 10.04239
[1mStep[0m  [100/106], [94mLoss[0m : 10.07618

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.007, [92mTest[0m: 10.346, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.88492
[1mStep[0m  [10/106], [94mLoss[0m : 9.88470
[1mStep[0m  [20/106], [94mLoss[0m : 9.96948
[1mStep[0m  [30/106], [94mLoss[0m : 9.63014
[1mStep[0m  [40/106], [94mLoss[0m : 9.90285
[1mStep[0m  [50/106], [94mLoss[0m : 9.53650
[1mStep[0m  [60/106], [94mLoss[0m : 9.63722
[1mStep[0m  [70/106], [94mLoss[0m : 9.32123
[1mStep[0m  [80/106], [94mLoss[0m : 9.79987
[1mStep[0m  [90/106], [94mLoss[0m : 9.96855
[1mStep[0m  [100/106], [94mLoss[0m : 9.48413

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.667, [92mTest[0m: 10.063, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.44903
[1mStep[0m  [10/106], [94mLoss[0m : 9.07739
[1mStep[0m  [20/106], [94mLoss[0m : 9.16529
[1mStep[0m  [30/106], [94mLoss[0m : 9.27220
[1mStep[0m  [40/106], [94mLoss[0m : 9.35028
[1mStep[0m  [50/106], [94mLoss[0m : 9.18443
[1mStep[0m  [60/106], [94mLoss[0m : 9.03302
[1mStep[0m  [70/106], [94mLoss[0m : 8.83415
[1mStep[0m  [80/106], [94mLoss[0m : 9.20805
[1mStep[0m  [90/106], [94mLoss[0m : 9.26991
[1mStep[0m  [100/106], [94mLoss[0m : 8.97807

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.322, [92mTest[0m: 9.779, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.33185
[1mStep[0m  [10/106], [94mLoss[0m : 9.24669
[1mStep[0m  [20/106], [94mLoss[0m : 9.28900
[1mStep[0m  [30/106], [94mLoss[0m : 9.38081
[1mStep[0m  [40/106], [94mLoss[0m : 8.84001
[1mStep[0m  [50/106], [94mLoss[0m : 8.99961
[1mStep[0m  [60/106], [94mLoss[0m : 9.20674
[1mStep[0m  [70/106], [94mLoss[0m : 8.56527
[1mStep[0m  [80/106], [94mLoss[0m : 8.90713
[1mStep[0m  [90/106], [94mLoss[0m : 9.00926
[1mStep[0m  [100/106], [94mLoss[0m : 8.96036

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.981, [92mTest[0m: 9.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.08520
[1mStep[0m  [10/106], [94mLoss[0m : 9.03591
[1mStep[0m  [20/106], [94mLoss[0m : 8.84724
[1mStep[0m  [30/106], [94mLoss[0m : 8.28134
[1mStep[0m  [40/106], [94mLoss[0m : 9.26422
[1mStep[0m  [50/106], [94mLoss[0m : 8.67302
[1mStep[0m  [60/106], [94mLoss[0m : 8.32943
[1mStep[0m  [70/106], [94mLoss[0m : 8.29694
[1mStep[0m  [80/106], [94mLoss[0m : 8.04940
[1mStep[0m  [90/106], [94mLoss[0m : 8.35819
[1mStep[0m  [100/106], [94mLoss[0m : 8.71232

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.633, [92mTest[0m: 9.235, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.98734
[1mStep[0m  [10/106], [94mLoss[0m : 7.75674
[1mStep[0m  [20/106], [94mLoss[0m : 8.70726
[1mStep[0m  [30/106], [94mLoss[0m : 7.90977
[1mStep[0m  [40/106], [94mLoss[0m : 8.40468
[1mStep[0m  [50/106], [94mLoss[0m : 8.27525
[1mStep[0m  [60/106], [94mLoss[0m : 7.69756
[1mStep[0m  [70/106], [94mLoss[0m : 8.05573
[1mStep[0m  [80/106], [94mLoss[0m : 8.33375
[1mStep[0m  [90/106], [94mLoss[0m : 8.11650
[1mStep[0m  [100/106], [94mLoss[0m : 7.75443

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.288, [92mTest[0m: 8.919, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.21970
[1mStep[0m  [10/106], [94mLoss[0m : 8.62173
[1mStep[0m  [20/106], [94mLoss[0m : 7.57174
[1mStep[0m  [30/106], [94mLoss[0m : 8.63064
[1mStep[0m  [40/106], [94mLoss[0m : 7.80186
[1mStep[0m  [50/106], [94mLoss[0m : 8.30486
[1mStep[0m  [60/106], [94mLoss[0m : 7.33167
[1mStep[0m  [70/106], [94mLoss[0m : 8.48414
[1mStep[0m  [80/106], [94mLoss[0m : 8.03604
[1mStep[0m  [90/106], [94mLoss[0m : 8.31116
[1mStep[0m  [100/106], [94mLoss[0m : 7.54775

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.946, [92mTest[0m: 8.613, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.46101
[1mStep[0m  [10/106], [94mLoss[0m : 7.84261
[1mStep[0m  [20/106], [94mLoss[0m : 8.32938
[1mStep[0m  [30/106], [94mLoss[0m : 7.56005
[1mStep[0m  [40/106], [94mLoss[0m : 7.73999
[1mStep[0m  [50/106], [94mLoss[0m : 7.87475
[1mStep[0m  [60/106], [94mLoss[0m : 8.19493
[1mStep[0m  [70/106], [94mLoss[0m : 7.66017
[1mStep[0m  [80/106], [94mLoss[0m : 7.46332
[1mStep[0m  [90/106], [94mLoss[0m : 7.80684
[1mStep[0m  [100/106], [94mLoss[0m : 7.82496

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.624, [92mTest[0m: 8.371, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.31660
[1mStep[0m  [10/106], [94mLoss[0m : 7.89200
[1mStep[0m  [20/106], [94mLoss[0m : 7.91498
[1mStep[0m  [30/106], [94mLoss[0m : 7.40413
[1mStep[0m  [40/106], [94mLoss[0m : 7.28182
[1mStep[0m  [50/106], [94mLoss[0m : 7.48132
[1mStep[0m  [60/106], [94mLoss[0m : 7.01197
[1mStep[0m  [70/106], [94mLoss[0m : 7.25464
[1mStep[0m  [80/106], [94mLoss[0m : 7.19511
[1mStep[0m  [90/106], [94mLoss[0m : 7.69530
[1mStep[0m  [100/106], [94mLoss[0m : 7.16540

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.286, [92mTest[0m: 8.044, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.28107
[1mStep[0m  [10/106], [94mLoss[0m : 7.13497
[1mStep[0m  [20/106], [94mLoss[0m : 7.14745
[1mStep[0m  [30/106], [94mLoss[0m : 7.00205
[1mStep[0m  [40/106], [94mLoss[0m : 6.95794
[1mStep[0m  [50/106], [94mLoss[0m : 6.61136
[1mStep[0m  [60/106], [94mLoss[0m : 7.03866
[1mStep[0m  [70/106], [94mLoss[0m : 6.94575
[1mStep[0m  [80/106], [94mLoss[0m : 6.70164
[1mStep[0m  [90/106], [94mLoss[0m : 6.76890
[1mStep[0m  [100/106], [94mLoss[0m : 6.99455

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.957, [92mTest[0m: 7.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.79554
[1mStep[0m  [10/106], [94mLoss[0m : 6.48408
[1mStep[0m  [20/106], [94mLoss[0m : 6.56496
[1mStep[0m  [30/106], [94mLoss[0m : 6.75698
[1mStep[0m  [40/106], [94mLoss[0m : 7.09201
[1mStep[0m  [50/106], [94mLoss[0m : 6.38646
[1mStep[0m  [60/106], [94mLoss[0m : 6.97215
[1mStep[0m  [70/106], [94mLoss[0m : 6.91383
[1mStep[0m  [80/106], [94mLoss[0m : 6.58878
[1mStep[0m  [90/106], [94mLoss[0m : 6.81559
[1mStep[0m  [100/106], [94mLoss[0m : 7.53466

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.662, [92mTest[0m: 7.446, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.49890
[1mStep[0m  [10/106], [94mLoss[0m : 6.49309
[1mStep[0m  [20/106], [94mLoss[0m : 6.12947
[1mStep[0m  [30/106], [94mLoss[0m : 6.51327
[1mStep[0m  [40/106], [94mLoss[0m : 6.85561
[1mStep[0m  [50/106], [94mLoss[0m : 5.80730
[1mStep[0m  [60/106], [94mLoss[0m : 6.27364
[1mStep[0m  [70/106], [94mLoss[0m : 6.19953
[1mStep[0m  [80/106], [94mLoss[0m : 6.17480
[1mStep[0m  [90/106], [94mLoss[0m : 6.37352
[1mStep[0m  [100/106], [94mLoss[0m : 6.30542

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.338, [92mTest[0m: 7.168, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.20377
[1mStep[0m  [10/106], [94mLoss[0m : 6.24380
[1mStep[0m  [20/106], [94mLoss[0m : 5.94253
[1mStep[0m  [30/106], [94mLoss[0m : 5.86371
[1mStep[0m  [40/106], [94mLoss[0m : 5.98072
[1mStep[0m  [50/106], [94mLoss[0m : 5.76683
[1mStep[0m  [60/106], [94mLoss[0m : 5.78131
[1mStep[0m  [70/106], [94mLoss[0m : 6.31184
[1mStep[0m  [80/106], [94mLoss[0m : 5.91096
[1mStep[0m  [90/106], [94mLoss[0m : 6.07609
[1mStep[0m  [100/106], [94mLoss[0m : 5.69742

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.051, [92mTest[0m: 6.900, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.81606
[1mStep[0m  [10/106], [94mLoss[0m : 5.68779
[1mStep[0m  [20/106], [94mLoss[0m : 6.07672
[1mStep[0m  [30/106], [94mLoss[0m : 5.82160
[1mStep[0m  [40/106], [94mLoss[0m : 5.73680
[1mStep[0m  [50/106], [94mLoss[0m : 5.61877
[1mStep[0m  [60/106], [94mLoss[0m : 5.31658
[1mStep[0m  [70/106], [94mLoss[0m : 5.48086
[1mStep[0m  [80/106], [94mLoss[0m : 5.68654
[1mStep[0m  [90/106], [94mLoss[0m : 5.97210
[1mStep[0m  [100/106], [94mLoss[0m : 5.48568

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.719, [92mTest[0m: 6.625, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.43985
[1mStep[0m  [10/106], [94mLoss[0m : 5.61083
[1mStep[0m  [20/106], [94mLoss[0m : 6.09667
[1mStep[0m  [30/106], [94mLoss[0m : 5.44445
[1mStep[0m  [40/106], [94mLoss[0m : 5.49206
[1mStep[0m  [50/106], [94mLoss[0m : 4.81629
[1mStep[0m  [60/106], [94mLoss[0m : 5.50218
[1mStep[0m  [70/106], [94mLoss[0m : 5.35001
[1mStep[0m  [80/106], [94mLoss[0m : 5.68555
[1mStep[0m  [90/106], [94mLoss[0m : 5.50854
[1mStep[0m  [100/106], [94mLoss[0m : 5.85149

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.435, [92mTest[0m: 6.273, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.99720
[1mStep[0m  [10/106], [94mLoss[0m : 5.64469
[1mStep[0m  [20/106], [94mLoss[0m : 5.05704
[1mStep[0m  [30/106], [94mLoss[0m : 5.03228
[1mStep[0m  [40/106], [94mLoss[0m : 5.21915
[1mStep[0m  [50/106], [94mLoss[0m : 5.01476
[1mStep[0m  [60/106], [94mLoss[0m : 5.02015
[1mStep[0m  [70/106], [94mLoss[0m : 5.48378
[1mStep[0m  [80/106], [94mLoss[0m : 5.00352
[1mStep[0m  [90/106], [94mLoss[0m : 5.66106
[1mStep[0m  [100/106], [94mLoss[0m : 4.98612

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.160, [92mTest[0m: 5.991, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.84371
[1mStep[0m  [10/106], [94mLoss[0m : 5.03625
[1mStep[0m  [20/106], [94mLoss[0m : 4.35036
[1mStep[0m  [30/106], [94mLoss[0m : 5.17287
[1mStep[0m  [40/106], [94mLoss[0m : 4.67519
[1mStep[0m  [50/106], [94mLoss[0m : 5.21437
[1mStep[0m  [60/106], [94mLoss[0m : 5.26110
[1mStep[0m  [70/106], [94mLoss[0m : 5.24298
[1mStep[0m  [80/106], [94mLoss[0m : 4.79280
[1mStep[0m  [90/106], [94mLoss[0m : 4.64282
[1mStep[0m  [100/106], [94mLoss[0m : 4.92028

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.894, [92mTest[0m: 5.694, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.41404
[1mStep[0m  [10/106], [94mLoss[0m : 4.38186
[1mStep[0m  [20/106], [94mLoss[0m : 4.83908
[1mStep[0m  [30/106], [94mLoss[0m : 4.14582
[1mStep[0m  [40/106], [94mLoss[0m : 4.55160
[1mStep[0m  [50/106], [94mLoss[0m : 4.46258
[1mStep[0m  [60/106], [94mLoss[0m : 4.14817
[1mStep[0m  [70/106], [94mLoss[0m : 4.67259
[1mStep[0m  [80/106], [94mLoss[0m : 4.61386
[1mStep[0m  [90/106], [94mLoss[0m : 4.66284
[1mStep[0m  [100/106], [94mLoss[0m : 4.47684

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.644, [92mTest[0m: 5.390, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.11983
[1mStep[0m  [10/106], [94mLoss[0m : 4.17334
[1mStep[0m  [20/106], [94mLoss[0m : 4.55587
[1mStep[0m  [30/106], [94mLoss[0m : 3.90062
[1mStep[0m  [40/106], [94mLoss[0m : 4.48148
[1mStep[0m  [50/106], [94mLoss[0m : 3.88688
[1mStep[0m  [60/106], [94mLoss[0m : 4.28745
[1mStep[0m  [70/106], [94mLoss[0m : 3.78087
[1mStep[0m  [80/106], [94mLoss[0m : 4.26579
[1mStep[0m  [90/106], [94mLoss[0m : 4.46307
[1mStep[0m  [100/106], [94mLoss[0m : 3.94966

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.427, [92mTest[0m: 5.101, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.23321
[1mStep[0m  [10/106], [94mLoss[0m : 4.78352
[1mStep[0m  [20/106], [94mLoss[0m : 4.31096
[1mStep[0m  [30/106], [94mLoss[0m : 4.49867
[1mStep[0m  [40/106], [94mLoss[0m : 3.67119
[1mStep[0m  [50/106], [94mLoss[0m : 4.50058
[1mStep[0m  [60/106], [94mLoss[0m : 4.36971
[1mStep[0m  [70/106], [94mLoss[0m : 4.30195
[1mStep[0m  [80/106], [94mLoss[0m : 4.14746
[1mStep[0m  [90/106], [94mLoss[0m : 4.11802
[1mStep[0m  [100/106], [94mLoss[0m : 4.49129

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.193, [92mTest[0m: 4.892, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.38645
[1mStep[0m  [10/106], [94mLoss[0m : 3.99963
[1mStep[0m  [20/106], [94mLoss[0m : 4.87522
[1mStep[0m  [30/106], [94mLoss[0m : 3.80145
[1mStep[0m  [40/106], [94mLoss[0m : 3.78080
[1mStep[0m  [50/106], [94mLoss[0m : 3.79591
[1mStep[0m  [60/106], [94mLoss[0m : 3.72983
[1mStep[0m  [70/106], [94mLoss[0m : 3.85126
[1mStep[0m  [80/106], [94mLoss[0m : 3.60790
[1mStep[0m  [90/106], [94mLoss[0m : 3.80241
[1mStep[0m  [100/106], [94mLoss[0m : 3.96434

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.007, [92mTest[0m: 4.602, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.85499
[1mStep[0m  [10/106], [94mLoss[0m : 3.75417
[1mStep[0m  [20/106], [94mLoss[0m : 3.30408
[1mStep[0m  [30/106], [94mLoss[0m : 4.13077
[1mStep[0m  [40/106], [94mLoss[0m : 3.97005
[1mStep[0m  [50/106], [94mLoss[0m : 3.73622
[1mStep[0m  [60/106], [94mLoss[0m : 3.83918
[1mStep[0m  [70/106], [94mLoss[0m : 3.42527
[1mStep[0m  [80/106], [94mLoss[0m : 3.79836
[1mStep[0m  [90/106], [94mLoss[0m : 3.41960
[1mStep[0m  [100/106], [94mLoss[0m : 3.43544

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.852, [92mTest[0m: 4.388, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.80080
[1mStep[0m  [10/106], [94mLoss[0m : 3.32074
[1mStep[0m  [20/106], [94mLoss[0m : 3.65881
[1mStep[0m  [30/106], [94mLoss[0m : 3.63974
[1mStep[0m  [40/106], [94mLoss[0m : 3.81827
[1mStep[0m  [50/106], [94mLoss[0m : 3.64436
[1mStep[0m  [60/106], [94mLoss[0m : 3.65949
[1mStep[0m  [70/106], [94mLoss[0m : 4.14001
[1mStep[0m  [80/106], [94mLoss[0m : 3.54607
[1mStep[0m  [90/106], [94mLoss[0m : 3.40357
[1mStep[0m  [100/106], [94mLoss[0m : 3.51472

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.704, [92mTest[0m: 4.207, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.04176
[1mStep[0m  [10/106], [94mLoss[0m : 4.16735
[1mStep[0m  [20/106], [94mLoss[0m : 3.81049
[1mStep[0m  [30/106], [94mLoss[0m : 3.38840
[1mStep[0m  [40/106], [94mLoss[0m : 3.81938
[1mStep[0m  [50/106], [94mLoss[0m : 3.87076
[1mStep[0m  [60/106], [94mLoss[0m : 3.44743
[1mStep[0m  [70/106], [94mLoss[0m : 3.33992
[1mStep[0m  [80/106], [94mLoss[0m : 3.82576
[1mStep[0m  [90/106], [94mLoss[0m : 3.86455
[1mStep[0m  [100/106], [94mLoss[0m : 3.80630

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.554, [92mTest[0m: 4.082, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.60830
[1mStep[0m  [10/106], [94mLoss[0m : 3.50246
[1mStep[0m  [20/106], [94mLoss[0m : 3.77527
[1mStep[0m  [30/106], [94mLoss[0m : 3.71959
[1mStep[0m  [40/106], [94mLoss[0m : 3.60718
[1mStep[0m  [50/106], [94mLoss[0m : 2.94563
[1mStep[0m  [60/106], [94mLoss[0m : 3.91838
[1mStep[0m  [70/106], [94mLoss[0m : 3.78238
[1mStep[0m  [80/106], [94mLoss[0m : 3.84508
[1mStep[0m  [90/106], [94mLoss[0m : 3.44159
[1mStep[0m  [100/106], [94mLoss[0m : 3.30526

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.468, [92mTest[0m: 3.917, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.28200
[1mStep[0m  [10/106], [94mLoss[0m : 3.10923
[1mStep[0m  [20/106], [94mLoss[0m : 3.40937
[1mStep[0m  [30/106], [94mLoss[0m : 3.85579
[1mStep[0m  [40/106], [94mLoss[0m : 3.56613
[1mStep[0m  [50/106], [94mLoss[0m : 4.01072
[1mStep[0m  [60/106], [94mLoss[0m : 3.31293
[1mStep[0m  [70/106], [94mLoss[0m : 3.25108
[1mStep[0m  [80/106], [94mLoss[0m : 3.08567
[1mStep[0m  [90/106], [94mLoss[0m : 3.40440
[1mStep[0m  [100/106], [94mLoss[0m : 3.12606

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.372, [92mTest[0m: 3.745, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.20585
[1mStep[0m  [10/106], [94mLoss[0m : 3.29366
[1mStep[0m  [20/106], [94mLoss[0m : 3.02773
[1mStep[0m  [30/106], [94mLoss[0m : 3.31606
[1mStep[0m  [40/106], [94mLoss[0m : 3.49818
[1mStep[0m  [50/106], [94mLoss[0m : 3.20283
[1mStep[0m  [60/106], [94mLoss[0m : 3.38648
[1mStep[0m  [70/106], [94mLoss[0m : 3.20681
[1mStep[0m  [80/106], [94mLoss[0m : 3.60793
[1mStep[0m  [90/106], [94mLoss[0m : 3.27056
[1mStep[0m  [100/106], [94mLoss[0m : 3.55507

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.281, [92mTest[0m: 3.626, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.38075
[1mStep[0m  [10/106], [94mLoss[0m : 3.36781
[1mStep[0m  [20/106], [94mLoss[0m : 3.18619
[1mStep[0m  [30/106], [94mLoss[0m : 3.13261
[1mStep[0m  [40/106], [94mLoss[0m : 2.78564
[1mStep[0m  [50/106], [94mLoss[0m : 3.13823
[1mStep[0m  [60/106], [94mLoss[0m : 3.15560
[1mStep[0m  [70/106], [94mLoss[0m : 3.12086
[1mStep[0m  [80/106], [94mLoss[0m : 3.52640
[1mStep[0m  [90/106], [94mLoss[0m : 3.38979
[1mStep[0m  [100/106], [94mLoss[0m : 3.07097

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.205, [92mTest[0m: 3.557, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.34775
[1mStep[0m  [10/106], [94mLoss[0m : 2.78889
[1mStep[0m  [20/106], [94mLoss[0m : 2.99542
[1mStep[0m  [30/106], [94mLoss[0m : 2.99688
[1mStep[0m  [40/106], [94mLoss[0m : 3.22281
[1mStep[0m  [50/106], [94mLoss[0m : 3.37233
[1mStep[0m  [60/106], [94mLoss[0m : 2.71787
[1mStep[0m  [70/106], [94mLoss[0m : 2.87746
[1mStep[0m  [80/106], [94mLoss[0m : 2.70102
[1mStep[0m  [90/106], [94mLoss[0m : 2.96149
[1mStep[0m  [100/106], [94mLoss[0m : 3.40546

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.138, [92mTest[0m: 3.413, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.341
====================================

Phase 1 - Evaluation MAE:  3.3406134326502963
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 3.00127
[1mStep[0m  [10/106], [94mLoss[0m : 2.65830
[1mStep[0m  [20/106], [94mLoss[0m : 3.52328
[1mStep[0m  [30/106], [94mLoss[0m : 3.05586
[1mStep[0m  [40/106], [94mLoss[0m : 2.73560
[1mStep[0m  [50/106], [94mLoss[0m : 3.39921
[1mStep[0m  [60/106], [94mLoss[0m : 2.98137
[1mStep[0m  [70/106], [94mLoss[0m : 3.04437
[1mStep[0m  [80/106], [94mLoss[0m : 2.95472
[1mStep[0m  [90/106], [94mLoss[0m : 3.18254
[1mStep[0m  [100/106], [94mLoss[0m : 3.21779

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.068, [92mTest[0m: 3.349, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.11630
[1mStep[0m  [10/106], [94mLoss[0m : 2.89250
[1mStep[0m  [20/106], [94mLoss[0m : 3.02239
[1mStep[0m  [30/106], [94mLoss[0m : 3.15780
[1mStep[0m  [40/106], [94mLoss[0m : 3.37644
[1mStep[0m  [50/106], [94mLoss[0m : 2.78998
[1mStep[0m  [60/106], [94mLoss[0m : 3.21303
[1mStep[0m  [70/106], [94mLoss[0m : 2.81372
[1mStep[0m  [80/106], [94mLoss[0m : 2.94872
[1mStep[0m  [90/106], [94mLoss[0m : 2.91316
[1mStep[0m  [100/106], [94mLoss[0m : 2.67472

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.012, [92mTest[0m: 3.137, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.07902
[1mStep[0m  [10/106], [94mLoss[0m : 3.05796
[1mStep[0m  [20/106], [94mLoss[0m : 2.80177
[1mStep[0m  [30/106], [94mLoss[0m : 3.09501
[1mStep[0m  [40/106], [94mLoss[0m : 2.91369
[1mStep[0m  [50/106], [94mLoss[0m : 2.52480
[1mStep[0m  [60/106], [94mLoss[0m : 3.50006
[1mStep[0m  [70/106], [94mLoss[0m : 3.00490
[1mStep[0m  [80/106], [94mLoss[0m : 2.95231
[1mStep[0m  [90/106], [94mLoss[0m : 2.60035
[1mStep[0m  [100/106], [94mLoss[0m : 2.61628

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.937, [92mTest[0m: 2.892, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.33386
[1mStep[0m  [10/106], [94mLoss[0m : 2.86940
[1mStep[0m  [20/106], [94mLoss[0m : 3.16846
[1mStep[0m  [30/106], [94mLoss[0m : 2.85716
[1mStep[0m  [40/106], [94mLoss[0m : 3.07520
[1mStep[0m  [50/106], [94mLoss[0m : 2.56317
[1mStep[0m  [60/106], [94mLoss[0m : 2.77584
[1mStep[0m  [70/106], [94mLoss[0m : 2.95725
[1mStep[0m  [80/106], [94mLoss[0m : 3.36798
[1mStep[0m  [90/106], [94mLoss[0m : 2.73401
[1mStep[0m  [100/106], [94mLoss[0m : 2.46977

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.910, [92mTest[0m: 2.890, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.91168
[1mStep[0m  [10/106], [94mLoss[0m : 2.80896
[1mStep[0m  [20/106], [94mLoss[0m : 3.10623
[1mStep[0m  [30/106], [94mLoss[0m : 2.58899
[1mStep[0m  [40/106], [94mLoss[0m : 2.54673
[1mStep[0m  [50/106], [94mLoss[0m : 3.14173
[1mStep[0m  [60/106], [94mLoss[0m : 2.99508
[1mStep[0m  [70/106], [94mLoss[0m : 2.42423
[1mStep[0m  [80/106], [94mLoss[0m : 2.38427
[1mStep[0m  [90/106], [94mLoss[0m : 3.11038
[1mStep[0m  [100/106], [94mLoss[0m : 2.56892

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.882, [92mTest[0m: 2.753, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.83583
[1mStep[0m  [10/106], [94mLoss[0m : 2.92380
[1mStep[0m  [20/106], [94mLoss[0m : 3.15229
[1mStep[0m  [30/106], [94mLoss[0m : 2.73388
[1mStep[0m  [40/106], [94mLoss[0m : 3.26341
[1mStep[0m  [50/106], [94mLoss[0m : 3.15042
[1mStep[0m  [60/106], [94mLoss[0m : 3.06816
[1mStep[0m  [70/106], [94mLoss[0m : 2.61049
[1mStep[0m  [80/106], [94mLoss[0m : 2.85429
[1mStep[0m  [90/106], [94mLoss[0m : 3.27247
[1mStep[0m  [100/106], [94mLoss[0m : 2.89272

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.857, [92mTest[0m: 2.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.97600
[1mStep[0m  [10/106], [94mLoss[0m : 2.88968
[1mStep[0m  [20/106], [94mLoss[0m : 2.86954
[1mStep[0m  [30/106], [94mLoss[0m : 2.85045
[1mStep[0m  [40/106], [94mLoss[0m : 2.69893
[1mStep[0m  [50/106], [94mLoss[0m : 3.22061
[1mStep[0m  [60/106], [94mLoss[0m : 2.61805
[1mStep[0m  [70/106], [94mLoss[0m : 3.05816
[1mStep[0m  [80/106], [94mLoss[0m : 3.43759
[1mStep[0m  [90/106], [94mLoss[0m : 3.23770
[1mStep[0m  [100/106], [94mLoss[0m : 3.14954

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.840, [92mTest[0m: 2.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.97216
[1mStep[0m  [10/106], [94mLoss[0m : 2.93883
[1mStep[0m  [20/106], [94mLoss[0m : 2.71521
[1mStep[0m  [30/106], [94mLoss[0m : 2.57479
[1mStep[0m  [40/106], [94mLoss[0m : 2.89441
[1mStep[0m  [50/106], [94mLoss[0m : 2.75684
[1mStep[0m  [60/106], [94mLoss[0m : 3.11460
[1mStep[0m  [70/106], [94mLoss[0m : 2.50489
[1mStep[0m  [80/106], [94mLoss[0m : 2.64094
[1mStep[0m  [90/106], [94mLoss[0m : 2.67558
[1mStep[0m  [100/106], [94mLoss[0m : 3.14913

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.802, [92mTest[0m: 2.722, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.82905
[1mStep[0m  [10/106], [94mLoss[0m : 2.62976
[1mStep[0m  [20/106], [94mLoss[0m : 2.50115
[1mStep[0m  [30/106], [94mLoss[0m : 2.95486
[1mStep[0m  [40/106], [94mLoss[0m : 2.73689
[1mStep[0m  [50/106], [94mLoss[0m : 2.69268
[1mStep[0m  [60/106], [94mLoss[0m : 2.52236
[1mStep[0m  [70/106], [94mLoss[0m : 3.01170
[1mStep[0m  [80/106], [94mLoss[0m : 2.52525
[1mStep[0m  [90/106], [94mLoss[0m : 2.93845
[1mStep[0m  [100/106], [94mLoss[0m : 2.68328

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.792, [92mTest[0m: 2.725, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.95649
[1mStep[0m  [10/106], [94mLoss[0m : 3.01105
[1mStep[0m  [20/106], [94mLoss[0m : 2.78767
[1mStep[0m  [30/106], [94mLoss[0m : 2.49627
[1mStep[0m  [40/106], [94mLoss[0m : 3.06027
[1mStep[0m  [50/106], [94mLoss[0m : 2.98249
[1mStep[0m  [60/106], [94mLoss[0m : 2.84341
[1mStep[0m  [70/106], [94mLoss[0m : 2.72307
[1mStep[0m  [80/106], [94mLoss[0m : 2.67295
[1mStep[0m  [90/106], [94mLoss[0m : 3.03812
[1mStep[0m  [100/106], [94mLoss[0m : 2.90907

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.781, [92mTest[0m: 2.735, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.91330
[1mStep[0m  [10/106], [94mLoss[0m : 2.86411
[1mStep[0m  [20/106], [94mLoss[0m : 2.78763
[1mStep[0m  [30/106], [94mLoss[0m : 2.99649
[1mStep[0m  [40/106], [94mLoss[0m : 2.53267
[1mStep[0m  [50/106], [94mLoss[0m : 2.88694
[1mStep[0m  [60/106], [94mLoss[0m : 2.76574
[1mStep[0m  [70/106], [94mLoss[0m : 2.68377
[1mStep[0m  [80/106], [94mLoss[0m : 2.83022
[1mStep[0m  [90/106], [94mLoss[0m : 2.47876
[1mStep[0m  [100/106], [94mLoss[0m : 2.57401

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.762, [92mTest[0m: 2.749, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49451
[1mStep[0m  [10/106], [94mLoss[0m : 2.81246
[1mStep[0m  [20/106], [94mLoss[0m : 2.66033
[1mStep[0m  [30/106], [94mLoss[0m : 2.96264
[1mStep[0m  [40/106], [94mLoss[0m : 2.65328
[1mStep[0m  [50/106], [94mLoss[0m : 3.08524
[1mStep[0m  [60/106], [94mLoss[0m : 2.84475
[1mStep[0m  [70/106], [94mLoss[0m : 2.73195
[1mStep[0m  [80/106], [94mLoss[0m : 2.62285
[1mStep[0m  [90/106], [94mLoss[0m : 2.98748
[1mStep[0m  [100/106], [94mLoss[0m : 2.43594

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.754, [92mTest[0m: 2.765, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.79577
[1mStep[0m  [10/106], [94mLoss[0m : 2.44693
[1mStep[0m  [20/106], [94mLoss[0m : 2.83038
[1mStep[0m  [30/106], [94mLoss[0m : 2.90398
[1mStep[0m  [40/106], [94mLoss[0m : 3.03639
[1mStep[0m  [50/106], [94mLoss[0m : 2.67938
[1mStep[0m  [60/106], [94mLoss[0m : 2.85002
[1mStep[0m  [70/106], [94mLoss[0m : 2.89342
[1mStep[0m  [80/106], [94mLoss[0m : 2.57245
[1mStep[0m  [90/106], [94mLoss[0m : 3.06463
[1mStep[0m  [100/106], [94mLoss[0m : 2.78274

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.04602
[1mStep[0m  [10/106], [94mLoss[0m : 2.60164
[1mStep[0m  [20/106], [94mLoss[0m : 2.48668
[1mStep[0m  [30/106], [94mLoss[0m : 2.64538
[1mStep[0m  [40/106], [94mLoss[0m : 2.61523
[1mStep[0m  [50/106], [94mLoss[0m : 2.60639
[1mStep[0m  [60/106], [94mLoss[0m : 2.75036
[1mStep[0m  [70/106], [94mLoss[0m : 2.70338
[1mStep[0m  [80/106], [94mLoss[0m : 2.96734
[1mStep[0m  [90/106], [94mLoss[0m : 2.89022
[1mStep[0m  [100/106], [94mLoss[0m : 2.97083

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.726, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.86445
[1mStep[0m  [10/106], [94mLoss[0m : 2.79164
[1mStep[0m  [20/106], [94mLoss[0m : 2.94173
[1mStep[0m  [30/106], [94mLoss[0m : 2.83721
[1mStep[0m  [40/106], [94mLoss[0m : 2.69244
[1mStep[0m  [50/106], [94mLoss[0m : 2.40584
[1mStep[0m  [60/106], [94mLoss[0m : 2.99336
[1mStep[0m  [70/106], [94mLoss[0m : 2.80004
[1mStep[0m  [80/106], [94mLoss[0m : 2.41964
[1mStep[0m  [90/106], [94mLoss[0m : 2.80142
[1mStep[0m  [100/106], [94mLoss[0m : 3.16228

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.838, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58098
[1mStep[0m  [10/106], [94mLoss[0m : 2.81013
[1mStep[0m  [20/106], [94mLoss[0m : 2.68273
[1mStep[0m  [30/106], [94mLoss[0m : 2.45623
[1mStep[0m  [40/106], [94mLoss[0m : 2.87340
[1mStep[0m  [50/106], [94mLoss[0m : 3.02649
[1mStep[0m  [60/106], [94mLoss[0m : 2.55169
[1mStep[0m  [70/106], [94mLoss[0m : 2.51013
[1mStep[0m  [80/106], [94mLoss[0m : 2.94393
[1mStep[0m  [90/106], [94mLoss[0m : 2.64047
[1mStep[0m  [100/106], [94mLoss[0m : 3.12380

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.731, [92mTest[0m: 2.915, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26888
[1mStep[0m  [10/106], [94mLoss[0m : 2.64658
[1mStep[0m  [20/106], [94mLoss[0m : 2.65697
[1mStep[0m  [30/106], [94mLoss[0m : 2.79668
[1mStep[0m  [40/106], [94mLoss[0m : 2.34747
[1mStep[0m  [50/106], [94mLoss[0m : 2.60271
[1mStep[0m  [60/106], [94mLoss[0m : 2.65754
[1mStep[0m  [70/106], [94mLoss[0m : 2.75652
[1mStep[0m  [80/106], [94mLoss[0m : 2.68981
[1mStep[0m  [90/106], [94mLoss[0m : 2.64434
[1mStep[0m  [100/106], [94mLoss[0m : 2.62983

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.847, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90141
[1mStep[0m  [10/106], [94mLoss[0m : 2.36427
[1mStep[0m  [20/106], [94mLoss[0m : 2.65230
[1mStep[0m  [30/106], [94mLoss[0m : 2.94170
[1mStep[0m  [40/106], [94mLoss[0m : 3.19135
[1mStep[0m  [50/106], [94mLoss[0m : 3.01591
[1mStep[0m  [60/106], [94mLoss[0m : 2.42563
[1mStep[0m  [70/106], [94mLoss[0m : 2.19628
[1mStep[0m  [80/106], [94mLoss[0m : 2.16502
[1mStep[0m  [90/106], [94mLoss[0m : 2.86211
[1mStep[0m  [100/106], [94mLoss[0m : 2.71110

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.700, [92mTest[0m: 2.843, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37387
[1mStep[0m  [10/106], [94mLoss[0m : 2.64836
[1mStep[0m  [20/106], [94mLoss[0m : 2.76409
[1mStep[0m  [30/106], [94mLoss[0m : 2.88846
[1mStep[0m  [40/106], [94mLoss[0m : 2.55987
[1mStep[0m  [50/106], [94mLoss[0m : 2.59061
[1mStep[0m  [60/106], [94mLoss[0m : 2.64348
[1mStep[0m  [70/106], [94mLoss[0m : 2.65873
[1mStep[0m  [80/106], [94mLoss[0m : 2.93246
[1mStep[0m  [90/106], [94mLoss[0m : 2.65629
[1mStep[0m  [100/106], [94mLoss[0m : 2.52257

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.944, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52782
[1mStep[0m  [10/106], [94mLoss[0m : 2.69338
[1mStep[0m  [20/106], [94mLoss[0m : 2.63717
[1mStep[0m  [30/106], [94mLoss[0m : 2.68492
[1mStep[0m  [40/106], [94mLoss[0m : 2.82464
[1mStep[0m  [50/106], [94mLoss[0m : 2.91314
[1mStep[0m  [60/106], [94mLoss[0m : 2.59614
[1mStep[0m  [70/106], [94mLoss[0m : 2.52385
[1mStep[0m  [80/106], [94mLoss[0m : 2.53297
[1mStep[0m  [90/106], [94mLoss[0m : 2.80108
[1mStep[0m  [100/106], [94mLoss[0m : 2.61403

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.704, [92mTest[0m: 2.897, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52151
[1mStep[0m  [10/106], [94mLoss[0m : 2.69460
[1mStep[0m  [20/106], [94mLoss[0m : 2.92381
[1mStep[0m  [30/106], [94mLoss[0m : 2.85608
[1mStep[0m  [40/106], [94mLoss[0m : 3.02230
[1mStep[0m  [50/106], [94mLoss[0m : 2.61742
[1mStep[0m  [60/106], [94mLoss[0m : 2.66261
[1mStep[0m  [70/106], [94mLoss[0m : 2.82978
[1mStep[0m  [80/106], [94mLoss[0m : 2.54932
[1mStep[0m  [90/106], [94mLoss[0m : 2.62789
[1mStep[0m  [100/106], [94mLoss[0m : 2.84365

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.887, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69767
[1mStep[0m  [10/106], [94mLoss[0m : 2.86494
[1mStep[0m  [20/106], [94mLoss[0m : 2.51527
[1mStep[0m  [30/106], [94mLoss[0m : 2.62814
[1mStep[0m  [40/106], [94mLoss[0m : 2.68240
[1mStep[0m  [50/106], [94mLoss[0m : 2.52361
[1mStep[0m  [60/106], [94mLoss[0m : 2.41927
[1mStep[0m  [70/106], [94mLoss[0m : 2.65448
[1mStep[0m  [80/106], [94mLoss[0m : 3.19449
[1mStep[0m  [90/106], [94mLoss[0m : 2.66848
[1mStep[0m  [100/106], [94mLoss[0m : 2.59600

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.855, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.04576
[1mStep[0m  [10/106], [94mLoss[0m : 2.61058
[1mStep[0m  [20/106], [94mLoss[0m : 2.54036
[1mStep[0m  [30/106], [94mLoss[0m : 2.99946
[1mStep[0m  [40/106], [94mLoss[0m : 2.58925
[1mStep[0m  [50/106], [94mLoss[0m : 2.48992
[1mStep[0m  [60/106], [94mLoss[0m : 2.63845
[1mStep[0m  [70/106], [94mLoss[0m : 2.48053
[1mStep[0m  [80/106], [94mLoss[0m : 2.72623
[1mStep[0m  [90/106], [94mLoss[0m : 2.63634
[1mStep[0m  [100/106], [94mLoss[0m : 2.70150

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.901, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.89209
[1mStep[0m  [10/106], [94mLoss[0m : 3.04094
[1mStep[0m  [20/106], [94mLoss[0m : 2.60353
[1mStep[0m  [30/106], [94mLoss[0m : 2.67429
[1mStep[0m  [40/106], [94mLoss[0m : 2.75441
[1mStep[0m  [50/106], [94mLoss[0m : 2.71870
[1mStep[0m  [60/106], [94mLoss[0m : 2.56942
[1mStep[0m  [70/106], [94mLoss[0m : 2.82584
[1mStep[0m  [80/106], [94mLoss[0m : 2.40519
[1mStep[0m  [90/106], [94mLoss[0m : 2.71480
[1mStep[0m  [100/106], [94mLoss[0m : 2.76130

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.832, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.11656
[1mStep[0m  [10/106], [94mLoss[0m : 2.71638
[1mStep[0m  [20/106], [94mLoss[0m : 2.85999
[1mStep[0m  [30/106], [94mLoss[0m : 2.90443
[1mStep[0m  [40/106], [94mLoss[0m : 2.83313
[1mStep[0m  [50/106], [94mLoss[0m : 2.34976
[1mStep[0m  [60/106], [94mLoss[0m : 3.02971
[1mStep[0m  [70/106], [94mLoss[0m : 2.68014
[1mStep[0m  [80/106], [94mLoss[0m : 2.59672
[1mStep[0m  [90/106], [94mLoss[0m : 2.40421
[1mStep[0m  [100/106], [94mLoss[0m : 2.79845

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.828, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42592
[1mStep[0m  [10/106], [94mLoss[0m : 2.49570
[1mStep[0m  [20/106], [94mLoss[0m : 2.43254
[1mStep[0m  [30/106], [94mLoss[0m : 2.76978
[1mStep[0m  [40/106], [94mLoss[0m : 2.81662
[1mStep[0m  [50/106], [94mLoss[0m : 2.85682
[1mStep[0m  [60/106], [94mLoss[0m : 2.82664
[1mStep[0m  [70/106], [94mLoss[0m : 2.53015
[1mStep[0m  [80/106], [94mLoss[0m : 2.78279
[1mStep[0m  [90/106], [94mLoss[0m : 2.70652
[1mStep[0m  [100/106], [94mLoss[0m : 2.89732

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.882, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.91381
[1mStep[0m  [10/106], [94mLoss[0m : 2.70466
[1mStep[0m  [20/106], [94mLoss[0m : 2.58138
[1mStep[0m  [30/106], [94mLoss[0m : 2.72712
[1mStep[0m  [40/106], [94mLoss[0m : 2.61721
[1mStep[0m  [50/106], [94mLoss[0m : 2.66703
[1mStep[0m  [60/106], [94mLoss[0m : 3.26632
[1mStep[0m  [70/106], [94mLoss[0m : 2.60226
[1mStep[0m  [80/106], [94mLoss[0m : 2.43038
[1mStep[0m  [90/106], [94mLoss[0m : 2.43936
[1mStep[0m  [100/106], [94mLoss[0m : 2.77155

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.888, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50828
[1mStep[0m  [10/106], [94mLoss[0m : 2.80148
[1mStep[0m  [20/106], [94mLoss[0m : 2.77680
[1mStep[0m  [30/106], [94mLoss[0m : 2.79773
[1mStep[0m  [40/106], [94mLoss[0m : 2.66193
[1mStep[0m  [50/106], [94mLoss[0m : 2.57386
[1mStep[0m  [60/106], [94mLoss[0m : 2.78061
[1mStep[0m  [70/106], [94mLoss[0m : 2.70792
[1mStep[0m  [80/106], [94mLoss[0m : 2.86784
[1mStep[0m  [90/106], [94mLoss[0m : 2.61381
[1mStep[0m  [100/106], [94mLoss[0m : 2.73841

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.688, [92mTest[0m: 2.851, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47909
[1mStep[0m  [10/106], [94mLoss[0m : 2.90677
[1mStep[0m  [20/106], [94mLoss[0m : 2.71946
[1mStep[0m  [30/106], [94mLoss[0m : 2.75727
[1mStep[0m  [40/106], [94mLoss[0m : 2.40607
[1mStep[0m  [50/106], [94mLoss[0m : 2.56489
[1mStep[0m  [60/106], [94mLoss[0m : 2.19052
[1mStep[0m  [70/106], [94mLoss[0m : 2.54254
[1mStep[0m  [80/106], [94mLoss[0m : 2.55851
[1mStep[0m  [90/106], [94mLoss[0m : 2.61975
[1mStep[0m  [100/106], [94mLoss[0m : 2.46601

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.853, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67988
[1mStep[0m  [10/106], [94mLoss[0m : 2.86259
[1mStep[0m  [20/106], [94mLoss[0m : 2.57036
[1mStep[0m  [30/106], [94mLoss[0m : 3.07081
[1mStep[0m  [40/106], [94mLoss[0m : 2.64886
[1mStep[0m  [50/106], [94mLoss[0m : 2.61749
[1mStep[0m  [60/106], [94mLoss[0m : 2.54521
[1mStep[0m  [70/106], [94mLoss[0m : 2.76872
[1mStep[0m  [80/106], [94mLoss[0m : 2.86331
[1mStep[0m  [90/106], [94mLoss[0m : 2.41894
[1mStep[0m  [100/106], [94mLoss[0m : 2.99558

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.797, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.813
====================================

Phase 2 - Evaluation MAE:  2.8128563682988004
MAE score P1      3.340613
MAE score P2      2.812856
loss              2.637832
learning_rate       0.0001
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 9.97857
[1mStep[0m  [10/106], [94mLoss[0m : 10.61303
[1mStep[0m  [20/106], [94mLoss[0m : 10.05194
[1mStep[0m  [30/106], [94mLoss[0m : 10.01755
[1mStep[0m  [40/106], [94mLoss[0m : 10.08423
[1mStep[0m  [50/106], [94mLoss[0m : 10.35494
[1mStep[0m  [60/106], [94mLoss[0m : 9.64815
[1mStep[0m  [70/106], [94mLoss[0m : 10.71512
[1mStep[0m  [80/106], [94mLoss[0m : 10.29494
[1mStep[0m  [90/106], [94mLoss[0m : 9.79984
[1mStep[0m  [100/106], [94mLoss[0m : 10.23360

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.089, [92mTest[0m: 10.263, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.05062
[1mStep[0m  [10/106], [94mLoss[0m : 9.78756
[1mStep[0m  [20/106], [94mLoss[0m : 10.18575
[1mStep[0m  [30/106], [94mLoss[0m : 9.34371
[1mStep[0m  [40/106], [94mLoss[0m : 9.70793
[1mStep[0m  [50/106], [94mLoss[0m : 10.24015
[1mStep[0m  [60/106], [94mLoss[0m : 10.00897
[1mStep[0m  [70/106], [94mLoss[0m : 9.92528
[1mStep[0m  [80/106], [94mLoss[0m : 9.86158
[1mStep[0m  [90/106], [94mLoss[0m : 9.69985
[1mStep[0m  [100/106], [94mLoss[0m : 9.40509

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.787, [92mTest[0m: 9.941, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.60980
[1mStep[0m  [10/106], [94mLoss[0m : 9.69545
[1mStep[0m  [20/106], [94mLoss[0m : 9.53793
[1mStep[0m  [30/106], [94mLoss[0m : 9.50647
[1mStep[0m  [40/106], [94mLoss[0m : 9.31936
[1mStep[0m  [50/106], [94mLoss[0m : 9.38760
[1mStep[0m  [60/106], [94mLoss[0m : 9.13756
[1mStep[0m  [70/106], [94mLoss[0m : 9.23205
[1mStep[0m  [80/106], [94mLoss[0m : 10.04328
[1mStep[0m  [90/106], [94mLoss[0m : 9.23194
[1mStep[0m  [100/106], [94mLoss[0m : 9.96340

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.473, [92mTest[0m: 9.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.74680
[1mStep[0m  [10/106], [94mLoss[0m : 9.69955
[1mStep[0m  [20/106], [94mLoss[0m : 10.11134
[1mStep[0m  [30/106], [94mLoss[0m : 9.11375
[1mStep[0m  [40/106], [94mLoss[0m : 8.85628
[1mStep[0m  [50/106], [94mLoss[0m : 9.08109
[1mStep[0m  [60/106], [94mLoss[0m : 9.06793
[1mStep[0m  [70/106], [94mLoss[0m : 8.78869
[1mStep[0m  [80/106], [94mLoss[0m : 9.37188
[1mStep[0m  [90/106], [94mLoss[0m : 9.15381
[1mStep[0m  [100/106], [94mLoss[0m : 8.59965

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.179, [92mTest[0m: 9.326, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.61522
[1mStep[0m  [10/106], [94mLoss[0m : 8.99845
[1mStep[0m  [20/106], [94mLoss[0m : 8.63286
[1mStep[0m  [30/106], [94mLoss[0m : 8.77424
[1mStep[0m  [40/106], [94mLoss[0m : 9.27944
[1mStep[0m  [50/106], [94mLoss[0m : 9.23052
[1mStep[0m  [60/106], [94mLoss[0m : 9.22360
[1mStep[0m  [70/106], [94mLoss[0m : 8.75897
[1mStep[0m  [80/106], [94mLoss[0m : 8.74901
[1mStep[0m  [90/106], [94mLoss[0m : 9.21565
[1mStep[0m  [100/106], [94mLoss[0m : 9.27121

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.866, [92mTest[0m: 9.020, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.68686
[1mStep[0m  [10/106], [94mLoss[0m : 8.75875
[1mStep[0m  [20/106], [94mLoss[0m : 8.73678
[1mStep[0m  [30/106], [94mLoss[0m : 8.34637
[1mStep[0m  [40/106], [94mLoss[0m : 8.96457
[1mStep[0m  [50/106], [94mLoss[0m : 9.22243
[1mStep[0m  [60/106], [94mLoss[0m : 8.34515
[1mStep[0m  [70/106], [94mLoss[0m : 8.36460
[1mStep[0m  [80/106], [94mLoss[0m : 8.99762
[1mStep[0m  [90/106], [94mLoss[0m : 8.23603
[1mStep[0m  [100/106], [94mLoss[0m : 9.11895

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.557, [92mTest[0m: 8.712, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.08639
[1mStep[0m  [10/106], [94mLoss[0m : 8.40615
[1mStep[0m  [20/106], [94mLoss[0m : 8.61671
[1mStep[0m  [30/106], [94mLoss[0m : 8.61892
[1mStep[0m  [40/106], [94mLoss[0m : 7.73084
[1mStep[0m  [50/106], [94mLoss[0m : 8.32916
[1mStep[0m  [60/106], [94mLoss[0m : 8.52157
[1mStep[0m  [70/106], [94mLoss[0m : 8.28360
[1mStep[0m  [80/106], [94mLoss[0m : 7.69570
[1mStep[0m  [90/106], [94mLoss[0m : 8.67309
[1mStep[0m  [100/106], [94mLoss[0m : 8.54468

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.252, [92mTest[0m: 8.409, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.32201
[1mStep[0m  [10/106], [94mLoss[0m : 8.56838
[1mStep[0m  [20/106], [94mLoss[0m : 8.22893
[1mStep[0m  [30/106], [94mLoss[0m : 8.34992
[1mStep[0m  [40/106], [94mLoss[0m : 7.75817
[1mStep[0m  [50/106], [94mLoss[0m : 7.81928
[1mStep[0m  [60/106], [94mLoss[0m : 8.41237
[1mStep[0m  [70/106], [94mLoss[0m : 7.72376
[1mStep[0m  [80/106], [94mLoss[0m : 7.81961
[1mStep[0m  [90/106], [94mLoss[0m : 8.15755
[1mStep[0m  [100/106], [94mLoss[0m : 8.09171

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.946, [92mTest[0m: 8.099, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.02963
[1mStep[0m  [10/106], [94mLoss[0m : 7.29233
[1mStep[0m  [20/106], [94mLoss[0m : 7.90397
[1mStep[0m  [30/106], [94mLoss[0m : 7.51723
[1mStep[0m  [40/106], [94mLoss[0m : 8.12535
[1mStep[0m  [50/106], [94mLoss[0m : 7.34480
[1mStep[0m  [60/106], [94mLoss[0m : 7.37820
[1mStep[0m  [70/106], [94mLoss[0m : 7.19252
[1mStep[0m  [80/106], [94mLoss[0m : 7.79764
[1mStep[0m  [90/106], [94mLoss[0m : 7.51327
[1mStep[0m  [100/106], [94mLoss[0m : 7.78851

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.643, [92mTest[0m: 7.805, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.11770
[1mStep[0m  [10/106], [94mLoss[0m : 7.48932
[1mStep[0m  [20/106], [94mLoss[0m : 7.31126
[1mStep[0m  [30/106], [94mLoss[0m : 6.83376
[1mStep[0m  [40/106], [94mLoss[0m : 7.52991
[1mStep[0m  [50/106], [94mLoss[0m : 7.30268
[1mStep[0m  [60/106], [94mLoss[0m : 7.17078
[1mStep[0m  [70/106], [94mLoss[0m : 7.27322
[1mStep[0m  [80/106], [94mLoss[0m : 7.13398
[1mStep[0m  [90/106], [94mLoss[0m : 7.57860
[1mStep[0m  [100/106], [94mLoss[0m : 7.10670

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.337, [92mTest[0m: 7.498, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.21200
[1mStep[0m  [10/106], [94mLoss[0m : 7.03179
[1mStep[0m  [20/106], [94mLoss[0m : 7.42792
[1mStep[0m  [30/106], [94mLoss[0m : 6.86406
[1mStep[0m  [40/106], [94mLoss[0m : 6.40017
[1mStep[0m  [50/106], [94mLoss[0m : 7.19831
[1mStep[0m  [60/106], [94mLoss[0m : 6.67294
[1mStep[0m  [70/106], [94mLoss[0m : 6.93512
[1mStep[0m  [80/106], [94mLoss[0m : 7.36612
[1mStep[0m  [90/106], [94mLoss[0m : 6.75330
[1mStep[0m  [100/106], [94mLoss[0m : 7.14934

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.027, [92mTest[0m: 7.186, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.89337
[1mStep[0m  [10/106], [94mLoss[0m : 6.46774
[1mStep[0m  [20/106], [94mLoss[0m : 7.11577
[1mStep[0m  [30/106], [94mLoss[0m : 6.28514
[1mStep[0m  [40/106], [94mLoss[0m : 6.91337
[1mStep[0m  [50/106], [94mLoss[0m : 7.09011
[1mStep[0m  [60/106], [94mLoss[0m : 7.00487
[1mStep[0m  [70/106], [94mLoss[0m : 6.07084
[1mStep[0m  [80/106], [94mLoss[0m : 6.36235
[1mStep[0m  [90/106], [94mLoss[0m : 6.91095
[1mStep[0m  [100/106], [94mLoss[0m : 6.53972

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.721, [92mTest[0m: 6.873, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.13583
[1mStep[0m  [10/106], [94mLoss[0m : 6.65087
[1mStep[0m  [20/106], [94mLoss[0m : 6.20278
[1mStep[0m  [30/106], [94mLoss[0m : 6.27198
[1mStep[0m  [40/106], [94mLoss[0m : 6.85070
[1mStep[0m  [50/106], [94mLoss[0m : 6.20909
[1mStep[0m  [60/106], [94mLoss[0m : 6.82389
[1mStep[0m  [70/106], [94mLoss[0m : 6.40143
[1mStep[0m  [80/106], [94mLoss[0m : 6.17812
[1mStep[0m  [90/106], [94mLoss[0m : 5.72342
[1mStep[0m  [100/106], [94mLoss[0m : 5.97907

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.410, [92mTest[0m: 6.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.28702
[1mStep[0m  [10/106], [94mLoss[0m : 6.44029
[1mStep[0m  [20/106], [94mLoss[0m : 6.28865
[1mStep[0m  [30/106], [94mLoss[0m : 6.51598
[1mStep[0m  [40/106], [94mLoss[0m : 5.78932
[1mStep[0m  [50/106], [94mLoss[0m : 6.39949
[1mStep[0m  [60/106], [94mLoss[0m : 6.46148
[1mStep[0m  [70/106], [94mLoss[0m : 6.75652
[1mStep[0m  [80/106], [94mLoss[0m : 5.79576
[1mStep[0m  [90/106], [94mLoss[0m : 6.15605
[1mStep[0m  [100/106], [94mLoss[0m : 5.67192

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.117, [92mTest[0m: 6.263, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.36254
[1mStep[0m  [10/106], [94mLoss[0m : 5.82222
[1mStep[0m  [20/106], [94mLoss[0m : 6.13178
[1mStep[0m  [30/106], [94mLoss[0m : 5.55597
[1mStep[0m  [40/106], [94mLoss[0m : 5.72997
[1mStep[0m  [50/106], [94mLoss[0m : 6.08677
[1mStep[0m  [60/106], [94mLoss[0m : 6.31623
[1mStep[0m  [70/106], [94mLoss[0m : 6.54294
[1mStep[0m  [80/106], [94mLoss[0m : 5.45707
[1mStep[0m  [90/106], [94mLoss[0m : 5.49843
[1mStep[0m  [100/106], [94mLoss[0m : 5.35250

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.815, [92mTest[0m: 5.972, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.58284
[1mStep[0m  [10/106], [94mLoss[0m : 5.37221
[1mStep[0m  [20/106], [94mLoss[0m : 5.52654
[1mStep[0m  [30/106], [94mLoss[0m : 5.65228
[1mStep[0m  [40/106], [94mLoss[0m : 5.55838
[1mStep[0m  [50/106], [94mLoss[0m : 5.77667
[1mStep[0m  [60/106], [94mLoss[0m : 5.66477
[1mStep[0m  [70/106], [94mLoss[0m : 5.00429
[1mStep[0m  [80/106], [94mLoss[0m : 6.20715
[1mStep[0m  [90/106], [94mLoss[0m : 5.46313
[1mStep[0m  [100/106], [94mLoss[0m : 5.75885

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.527, [92mTest[0m: 5.677, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.71785
[1mStep[0m  [10/106], [94mLoss[0m : 5.59305
[1mStep[0m  [20/106], [94mLoss[0m : 5.08570
[1mStep[0m  [30/106], [94mLoss[0m : 5.12162
[1mStep[0m  [40/106], [94mLoss[0m : 5.17183
[1mStep[0m  [50/106], [94mLoss[0m : 5.32237
[1mStep[0m  [60/106], [94mLoss[0m : 5.05237
[1mStep[0m  [70/106], [94mLoss[0m : 5.55823
[1mStep[0m  [80/106], [94mLoss[0m : 5.58004
[1mStep[0m  [90/106], [94mLoss[0m : 5.39051
[1mStep[0m  [100/106], [94mLoss[0m : 5.11841

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.259, [92mTest[0m: 5.385, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.86167
[1mStep[0m  [10/106], [94mLoss[0m : 5.37959
[1mStep[0m  [20/106], [94mLoss[0m : 5.22621
[1mStep[0m  [30/106], [94mLoss[0m : 4.88860
[1mStep[0m  [40/106], [94mLoss[0m : 5.67155
[1mStep[0m  [50/106], [94mLoss[0m : 4.95884
[1mStep[0m  [60/106], [94mLoss[0m : 5.27085
[1mStep[0m  [70/106], [94mLoss[0m : 4.92475
[1mStep[0m  [80/106], [94mLoss[0m : 4.70526
[1mStep[0m  [90/106], [94mLoss[0m : 4.60253
[1mStep[0m  [100/106], [94mLoss[0m : 4.33264

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.005, [92mTest[0m: 5.133, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.81314
[1mStep[0m  [10/106], [94mLoss[0m : 4.85418
[1mStep[0m  [20/106], [94mLoss[0m : 4.74703
[1mStep[0m  [30/106], [94mLoss[0m : 5.06607
[1mStep[0m  [40/106], [94mLoss[0m : 5.13665
[1mStep[0m  [50/106], [94mLoss[0m : 4.59861
[1mStep[0m  [60/106], [94mLoss[0m : 4.79143
[1mStep[0m  [70/106], [94mLoss[0m : 4.96783
[1mStep[0m  [80/106], [94mLoss[0m : 4.54336
[1mStep[0m  [90/106], [94mLoss[0m : 4.42577
[1mStep[0m  [100/106], [94mLoss[0m : 4.63124

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.773, [92mTest[0m: 4.862, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.23334
[1mStep[0m  [10/106], [94mLoss[0m : 4.27684
[1mStep[0m  [20/106], [94mLoss[0m : 4.21997
[1mStep[0m  [30/106], [94mLoss[0m : 4.49179
[1mStep[0m  [40/106], [94mLoss[0m : 4.80882
[1mStep[0m  [50/106], [94mLoss[0m : 4.65842
[1mStep[0m  [60/106], [94mLoss[0m : 4.17108
[1mStep[0m  [70/106], [94mLoss[0m : 4.77428
[1mStep[0m  [80/106], [94mLoss[0m : 4.49712
[1mStep[0m  [90/106], [94mLoss[0m : 4.30211
[1mStep[0m  [100/106], [94mLoss[0m : 4.92547

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.560, [92mTest[0m: 4.642, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.73871
[1mStep[0m  [10/106], [94mLoss[0m : 5.17184
[1mStep[0m  [20/106], [94mLoss[0m : 4.26619
[1mStep[0m  [30/106], [94mLoss[0m : 4.92299
[1mStep[0m  [40/106], [94mLoss[0m : 3.96624
[1mStep[0m  [50/106], [94mLoss[0m : 4.24377
[1mStep[0m  [60/106], [94mLoss[0m : 4.73528
[1mStep[0m  [70/106], [94mLoss[0m : 4.76324
[1mStep[0m  [80/106], [94mLoss[0m : 4.63709
[1mStep[0m  [90/106], [94mLoss[0m : 4.29401
[1mStep[0m  [100/106], [94mLoss[0m : 4.05060

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.380, [92mTest[0m: 4.435, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.58467
[1mStep[0m  [10/106], [94mLoss[0m : 4.39422
[1mStep[0m  [20/106], [94mLoss[0m : 4.06911
[1mStep[0m  [30/106], [94mLoss[0m : 4.40602
[1mStep[0m  [40/106], [94mLoss[0m : 3.75582
[1mStep[0m  [50/106], [94mLoss[0m : 4.03169
[1mStep[0m  [60/106], [94mLoss[0m : 4.39275
[1mStep[0m  [70/106], [94mLoss[0m : 3.96091
[1mStep[0m  [80/106], [94mLoss[0m : 4.65474
[1mStep[0m  [90/106], [94mLoss[0m : 4.21625
[1mStep[0m  [100/106], [94mLoss[0m : 3.71506

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.232, [92mTest[0m: 4.266, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.73546
[1mStep[0m  [10/106], [94mLoss[0m : 4.17405
[1mStep[0m  [20/106], [94mLoss[0m : 4.08867
[1mStep[0m  [30/106], [94mLoss[0m : 4.01443
[1mStep[0m  [40/106], [94mLoss[0m : 3.75080
[1mStep[0m  [50/106], [94mLoss[0m : 4.01011
[1mStep[0m  [60/106], [94mLoss[0m : 4.10898
[1mStep[0m  [70/106], [94mLoss[0m : 4.22036
[1mStep[0m  [80/106], [94mLoss[0m : 3.92032
[1mStep[0m  [90/106], [94mLoss[0m : 3.70868
[1mStep[0m  [100/106], [94mLoss[0m : 3.53585

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.076, [92mTest[0m: 4.134, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.74023
[1mStep[0m  [10/106], [94mLoss[0m : 3.77908
[1mStep[0m  [20/106], [94mLoss[0m : 4.04572
[1mStep[0m  [30/106], [94mLoss[0m : 3.82011
[1mStep[0m  [40/106], [94mLoss[0m : 3.91052
[1mStep[0m  [50/106], [94mLoss[0m : 4.30334
[1mStep[0m  [60/106], [94mLoss[0m : 3.77443
[1mStep[0m  [70/106], [94mLoss[0m : 3.78598
[1mStep[0m  [80/106], [94mLoss[0m : 3.86109
[1mStep[0m  [90/106], [94mLoss[0m : 3.66563
[1mStep[0m  [100/106], [94mLoss[0m : 3.58815

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.952, [92mTest[0m: 3.978, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.88094
[1mStep[0m  [10/106], [94mLoss[0m : 3.87333
[1mStep[0m  [20/106], [94mLoss[0m : 4.37593
[1mStep[0m  [30/106], [94mLoss[0m : 4.03479
[1mStep[0m  [40/106], [94mLoss[0m : 3.47929
[1mStep[0m  [50/106], [94mLoss[0m : 4.38599
[1mStep[0m  [60/106], [94mLoss[0m : 3.82633
[1mStep[0m  [70/106], [94mLoss[0m : 3.76547
[1mStep[0m  [80/106], [94mLoss[0m : 3.68838
[1mStep[0m  [90/106], [94mLoss[0m : 3.97274
[1mStep[0m  [100/106], [94mLoss[0m : 3.36581

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.837, [92mTest[0m: 3.838, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.84802
[1mStep[0m  [10/106], [94mLoss[0m : 3.87608
[1mStep[0m  [20/106], [94mLoss[0m : 3.65370
[1mStep[0m  [30/106], [94mLoss[0m : 3.44725
[1mStep[0m  [40/106], [94mLoss[0m : 3.67550
[1mStep[0m  [50/106], [94mLoss[0m : 3.75593
[1mStep[0m  [60/106], [94mLoss[0m : 3.56573
[1mStep[0m  [70/106], [94mLoss[0m : 3.61838
[1mStep[0m  [80/106], [94mLoss[0m : 3.66053
[1mStep[0m  [90/106], [94mLoss[0m : 3.47853
[1mStep[0m  [100/106], [94mLoss[0m : 3.68672

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.731, [92mTest[0m: 3.723, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.29104
[1mStep[0m  [10/106], [94mLoss[0m : 3.95764
[1mStep[0m  [20/106], [94mLoss[0m : 3.14614
[1mStep[0m  [30/106], [94mLoss[0m : 3.43126
[1mStep[0m  [40/106], [94mLoss[0m : 3.57532
[1mStep[0m  [50/106], [94mLoss[0m : 3.29124
[1mStep[0m  [60/106], [94mLoss[0m : 3.44558
[1mStep[0m  [70/106], [94mLoss[0m : 3.59757
[1mStep[0m  [80/106], [94mLoss[0m : 3.44858
[1mStep[0m  [90/106], [94mLoss[0m : 3.65588
[1mStep[0m  [100/106], [94mLoss[0m : 3.63780

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.629, [92mTest[0m: 3.634, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.36628
[1mStep[0m  [10/106], [94mLoss[0m : 3.61744
[1mStep[0m  [20/106], [94mLoss[0m : 3.52802
[1mStep[0m  [30/106], [94mLoss[0m : 3.46262
[1mStep[0m  [40/106], [94mLoss[0m : 3.25849
[1mStep[0m  [50/106], [94mLoss[0m : 3.45741
[1mStep[0m  [60/106], [94mLoss[0m : 3.24121
[1mStep[0m  [70/106], [94mLoss[0m : 3.69085
[1mStep[0m  [80/106], [94mLoss[0m : 3.54203
[1mStep[0m  [90/106], [94mLoss[0m : 3.44579
[1mStep[0m  [100/106], [94mLoss[0m : 3.41000

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.543, [92mTest[0m: 3.540, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.74217
[1mStep[0m  [10/106], [94mLoss[0m : 3.58292
[1mStep[0m  [20/106], [94mLoss[0m : 3.76448
[1mStep[0m  [30/106], [94mLoss[0m : 3.67959
[1mStep[0m  [40/106], [94mLoss[0m : 3.48753
[1mStep[0m  [50/106], [94mLoss[0m : 3.75698
[1mStep[0m  [60/106], [94mLoss[0m : 3.32512
[1mStep[0m  [70/106], [94mLoss[0m : 3.55560
[1mStep[0m  [80/106], [94mLoss[0m : 3.68359
[1mStep[0m  [90/106], [94mLoss[0m : 3.59781
[1mStep[0m  [100/106], [94mLoss[0m : 3.42336

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.470, [92mTest[0m: 3.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.14172
[1mStep[0m  [10/106], [94mLoss[0m : 3.56097
[1mStep[0m  [20/106], [94mLoss[0m : 3.46337
[1mStep[0m  [30/106], [94mLoss[0m : 3.80827
[1mStep[0m  [40/106], [94mLoss[0m : 3.84633
[1mStep[0m  [50/106], [94mLoss[0m : 3.85890
[1mStep[0m  [60/106], [94mLoss[0m : 3.41865
[1mStep[0m  [70/106], [94mLoss[0m : 3.14422
[1mStep[0m  [80/106], [94mLoss[0m : 3.36174
[1mStep[0m  [90/106], [94mLoss[0m : 3.75320
[1mStep[0m  [100/106], [94mLoss[0m : 3.36532

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.405, [92mTest[0m: 3.377, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.308
====================================

Phase 1 - Evaluation MAE:  3.308370630696135
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 3.34684
[1mStep[0m  [10/106], [94mLoss[0m : 3.28081
[1mStep[0m  [20/106], [94mLoss[0m : 3.41960
[1mStep[0m  [30/106], [94mLoss[0m : 3.58706
[1mStep[0m  [40/106], [94mLoss[0m : 3.88049
[1mStep[0m  [50/106], [94mLoss[0m : 3.18883
[1mStep[0m  [60/106], [94mLoss[0m : 3.30447
[1mStep[0m  [70/106], [94mLoss[0m : 3.31437
[1mStep[0m  [80/106], [94mLoss[0m : 3.21773
[1mStep[0m  [90/106], [94mLoss[0m : 3.61741
[1mStep[0m  [100/106], [94mLoss[0m : 3.19981

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.333, [92mTest[0m: 3.308, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.45932
[1mStep[0m  [10/106], [94mLoss[0m : 3.20423
[1mStep[0m  [20/106], [94mLoss[0m : 3.72603
[1mStep[0m  [30/106], [94mLoss[0m : 3.30042
[1mStep[0m  [40/106], [94mLoss[0m : 3.47477
[1mStep[0m  [50/106], [94mLoss[0m : 3.81744
[1mStep[0m  [60/106], [94mLoss[0m : 3.33926
[1mStep[0m  [70/106], [94mLoss[0m : 3.42432
[1mStep[0m  [80/106], [94mLoss[0m : 2.97659
[1mStep[0m  [90/106], [94mLoss[0m : 3.26907
[1mStep[0m  [100/106], [94mLoss[0m : 3.81764

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.268, [92mTest[0m: 3.228, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.30457
[1mStep[0m  [10/106], [94mLoss[0m : 3.03301
[1mStep[0m  [20/106], [94mLoss[0m : 3.33000
[1mStep[0m  [30/106], [94mLoss[0m : 3.35132
[1mStep[0m  [40/106], [94mLoss[0m : 2.92537
[1mStep[0m  [50/106], [94mLoss[0m : 3.57356
[1mStep[0m  [60/106], [94mLoss[0m : 3.58348
[1mStep[0m  [70/106], [94mLoss[0m : 3.43309
[1mStep[0m  [80/106], [94mLoss[0m : 2.89806
[1mStep[0m  [90/106], [94mLoss[0m : 3.09850
[1mStep[0m  [100/106], [94mLoss[0m : 3.24967

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.210, [92mTest[0m: 3.160, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.02445
[1mStep[0m  [10/106], [94mLoss[0m : 3.40858
[1mStep[0m  [20/106], [94mLoss[0m : 2.76249
[1mStep[0m  [30/106], [94mLoss[0m : 3.26748
[1mStep[0m  [40/106], [94mLoss[0m : 3.45473
[1mStep[0m  [50/106], [94mLoss[0m : 3.14101
[1mStep[0m  [60/106], [94mLoss[0m : 3.12981
[1mStep[0m  [70/106], [94mLoss[0m : 3.10087
[1mStep[0m  [80/106], [94mLoss[0m : 3.18420
[1mStep[0m  [90/106], [94mLoss[0m : 2.97935
[1mStep[0m  [100/106], [94mLoss[0m : 3.16782

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.154, [92mTest[0m: 3.096, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.86200
[1mStep[0m  [10/106], [94mLoss[0m : 3.06847
[1mStep[0m  [20/106], [94mLoss[0m : 3.22173
[1mStep[0m  [30/106], [94mLoss[0m : 3.51105
[1mStep[0m  [40/106], [94mLoss[0m : 3.15881
[1mStep[0m  [50/106], [94mLoss[0m : 2.95244
[1mStep[0m  [60/106], [94mLoss[0m : 2.92766
[1mStep[0m  [70/106], [94mLoss[0m : 3.03160
[1mStep[0m  [80/106], [94mLoss[0m : 3.28274
[1mStep[0m  [90/106], [94mLoss[0m : 3.16607
[1mStep[0m  [100/106], [94mLoss[0m : 2.93357

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.100, [92mTest[0m: 3.041, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77066
[1mStep[0m  [10/106], [94mLoss[0m : 3.49484
[1mStep[0m  [20/106], [94mLoss[0m : 3.48457
[1mStep[0m  [30/106], [94mLoss[0m : 2.61638
[1mStep[0m  [40/106], [94mLoss[0m : 2.99926
[1mStep[0m  [50/106], [94mLoss[0m : 3.10345
[1mStep[0m  [60/106], [94mLoss[0m : 2.82059
[1mStep[0m  [70/106], [94mLoss[0m : 3.28194
[1mStep[0m  [80/106], [94mLoss[0m : 3.12139
[1mStep[0m  [90/106], [94mLoss[0m : 3.03392
[1mStep[0m  [100/106], [94mLoss[0m : 2.74691

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.052, [92mTest[0m: 2.997, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90519
[1mStep[0m  [10/106], [94mLoss[0m : 3.07580
[1mStep[0m  [20/106], [94mLoss[0m : 3.25350
[1mStep[0m  [30/106], [94mLoss[0m : 2.97825
[1mStep[0m  [40/106], [94mLoss[0m : 2.75812
[1mStep[0m  [50/106], [94mLoss[0m : 3.12176
[1mStep[0m  [60/106], [94mLoss[0m : 3.04336
[1mStep[0m  [70/106], [94mLoss[0m : 2.85780
[1mStep[0m  [80/106], [94mLoss[0m : 2.94565
[1mStep[0m  [90/106], [94mLoss[0m : 3.35696
[1mStep[0m  [100/106], [94mLoss[0m : 3.01580

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.013, [92mTest[0m: 2.940, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78853
[1mStep[0m  [10/106], [94mLoss[0m : 2.68634
[1mStep[0m  [20/106], [94mLoss[0m : 2.74584
[1mStep[0m  [30/106], [94mLoss[0m : 3.21634
[1mStep[0m  [40/106], [94mLoss[0m : 2.82905
[1mStep[0m  [50/106], [94mLoss[0m : 2.79139
[1mStep[0m  [60/106], [94mLoss[0m : 3.34112
[1mStep[0m  [70/106], [94mLoss[0m : 3.26689
[1mStep[0m  [80/106], [94mLoss[0m : 3.05325
[1mStep[0m  [90/106], [94mLoss[0m : 3.20566
[1mStep[0m  [100/106], [94mLoss[0m : 3.01129

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.991, [92mTest[0m: 2.908, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77081
[1mStep[0m  [10/106], [94mLoss[0m : 2.80310
[1mStep[0m  [20/106], [94mLoss[0m : 3.18701
[1mStep[0m  [30/106], [94mLoss[0m : 3.02359
[1mStep[0m  [40/106], [94mLoss[0m : 2.99361
[1mStep[0m  [50/106], [94mLoss[0m : 2.93360
[1mStep[0m  [60/106], [94mLoss[0m : 3.20959
[1mStep[0m  [70/106], [94mLoss[0m : 2.90049
[1mStep[0m  [80/106], [94mLoss[0m : 3.53653
[1mStep[0m  [90/106], [94mLoss[0m : 2.91256
[1mStep[0m  [100/106], [94mLoss[0m : 3.05185

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.939, [92mTest[0m: 2.863, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.79502
[1mStep[0m  [10/106], [94mLoss[0m : 2.86392
[1mStep[0m  [20/106], [94mLoss[0m : 2.98493
[1mStep[0m  [30/106], [94mLoss[0m : 2.98644
[1mStep[0m  [40/106], [94mLoss[0m : 3.08836
[1mStep[0m  [50/106], [94mLoss[0m : 2.91439
[1mStep[0m  [60/106], [94mLoss[0m : 2.97532
[1mStep[0m  [70/106], [94mLoss[0m : 2.60986
[1mStep[0m  [80/106], [94mLoss[0m : 2.88462
[1mStep[0m  [90/106], [94mLoss[0m : 3.00598
[1mStep[0m  [100/106], [94mLoss[0m : 2.74822

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.917, [92mTest[0m: 2.822, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.45418
[1mStep[0m  [10/106], [94mLoss[0m : 2.61161
[1mStep[0m  [20/106], [94mLoss[0m : 2.91321
[1mStep[0m  [30/106], [94mLoss[0m : 2.79752
[1mStep[0m  [40/106], [94mLoss[0m : 3.07626
[1mStep[0m  [50/106], [94mLoss[0m : 2.72634
[1mStep[0m  [60/106], [94mLoss[0m : 2.97605
[1mStep[0m  [70/106], [94mLoss[0m : 2.63494
[1mStep[0m  [80/106], [94mLoss[0m : 2.80347
[1mStep[0m  [90/106], [94mLoss[0m : 2.69540
[1mStep[0m  [100/106], [94mLoss[0m : 2.79543

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.898, [92mTest[0m: 2.786, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78757
[1mStep[0m  [10/106], [94mLoss[0m : 2.91745
[1mStep[0m  [20/106], [94mLoss[0m : 3.00137
[1mStep[0m  [30/106], [94mLoss[0m : 3.24858
[1mStep[0m  [40/106], [94mLoss[0m : 2.71498
[1mStep[0m  [50/106], [94mLoss[0m : 2.78038
[1mStep[0m  [60/106], [94mLoss[0m : 2.76860
[1mStep[0m  [70/106], [94mLoss[0m : 2.96870
[1mStep[0m  [80/106], [94mLoss[0m : 2.71049
[1mStep[0m  [90/106], [94mLoss[0m : 2.80652
[1mStep[0m  [100/106], [94mLoss[0m : 2.82514

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.857, [92mTest[0m: 2.764, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.99159
[1mStep[0m  [10/106], [94mLoss[0m : 2.98094
[1mStep[0m  [20/106], [94mLoss[0m : 3.13375
[1mStep[0m  [30/106], [94mLoss[0m : 3.18713
[1mStep[0m  [40/106], [94mLoss[0m : 2.99595
[1mStep[0m  [50/106], [94mLoss[0m : 2.67057
[1mStep[0m  [60/106], [94mLoss[0m : 2.80123
[1mStep[0m  [70/106], [94mLoss[0m : 2.63413
[1mStep[0m  [80/106], [94mLoss[0m : 2.66833
[1mStep[0m  [90/106], [94mLoss[0m : 2.64832
[1mStep[0m  [100/106], [94mLoss[0m : 2.98385

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.834, [92mTest[0m: 2.737, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47108
[1mStep[0m  [10/106], [94mLoss[0m : 2.83120
[1mStep[0m  [20/106], [94mLoss[0m : 2.75684
[1mStep[0m  [30/106], [94mLoss[0m : 2.83277
[1mStep[0m  [40/106], [94mLoss[0m : 2.44871
[1mStep[0m  [50/106], [94mLoss[0m : 2.91420
[1mStep[0m  [60/106], [94mLoss[0m : 2.88865
[1mStep[0m  [70/106], [94mLoss[0m : 2.55431
[1mStep[0m  [80/106], [94mLoss[0m : 2.85145
[1mStep[0m  [90/106], [94mLoss[0m : 2.87863
[1mStep[0m  [100/106], [94mLoss[0m : 2.68712

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.816, [92mTest[0m: 2.704, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63081
[1mStep[0m  [10/106], [94mLoss[0m : 2.70329
[1mStep[0m  [20/106], [94mLoss[0m : 2.47537
[1mStep[0m  [30/106], [94mLoss[0m : 3.04658
[1mStep[0m  [40/106], [94mLoss[0m : 3.06513
[1mStep[0m  [50/106], [94mLoss[0m : 3.22051
[1mStep[0m  [60/106], [94mLoss[0m : 2.87395
[1mStep[0m  [70/106], [94mLoss[0m : 2.82449
[1mStep[0m  [80/106], [94mLoss[0m : 2.61610
[1mStep[0m  [90/106], [94mLoss[0m : 3.09487
[1mStep[0m  [100/106], [94mLoss[0m : 2.90926

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.794, [92mTest[0m: 2.695, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60320
[1mStep[0m  [10/106], [94mLoss[0m : 3.20936
[1mStep[0m  [20/106], [94mLoss[0m : 2.94402
[1mStep[0m  [30/106], [94mLoss[0m : 2.60495
[1mStep[0m  [40/106], [94mLoss[0m : 2.86938
[1mStep[0m  [50/106], [94mLoss[0m : 2.46832
[1mStep[0m  [60/106], [94mLoss[0m : 2.63962
[1mStep[0m  [70/106], [94mLoss[0m : 2.62367
[1mStep[0m  [80/106], [94mLoss[0m : 2.82565
[1mStep[0m  [90/106], [94mLoss[0m : 2.99837
[1mStep[0m  [100/106], [94mLoss[0m : 2.54352

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.772, [92mTest[0m: 2.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67023
[1mStep[0m  [10/106], [94mLoss[0m : 2.79550
[1mStep[0m  [20/106], [94mLoss[0m : 2.83473
[1mStep[0m  [30/106], [94mLoss[0m : 2.87554
[1mStep[0m  [40/106], [94mLoss[0m : 2.88317
[1mStep[0m  [50/106], [94mLoss[0m : 2.57209
[1mStep[0m  [60/106], [94mLoss[0m : 2.54341
[1mStep[0m  [70/106], [94mLoss[0m : 3.21209
[1mStep[0m  [80/106], [94mLoss[0m : 2.45336
[1mStep[0m  [90/106], [94mLoss[0m : 2.86050
[1mStep[0m  [100/106], [94mLoss[0m : 2.91731

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.763, [92mTest[0m: 2.651, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.82501
[1mStep[0m  [10/106], [94mLoss[0m : 3.00921
[1mStep[0m  [20/106], [94mLoss[0m : 2.71557
[1mStep[0m  [30/106], [94mLoss[0m : 2.31139
[1mStep[0m  [40/106], [94mLoss[0m : 2.99623
[1mStep[0m  [50/106], [94mLoss[0m : 2.85614
[1mStep[0m  [60/106], [94mLoss[0m : 2.64015
[1mStep[0m  [70/106], [94mLoss[0m : 2.68490
[1mStep[0m  [80/106], [94mLoss[0m : 2.76941
[1mStep[0m  [90/106], [94mLoss[0m : 2.65617
[1mStep[0m  [100/106], [94mLoss[0m : 3.16380

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.625, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.95358
[1mStep[0m  [10/106], [94mLoss[0m : 2.94368
[1mStep[0m  [20/106], [94mLoss[0m : 2.68773
[1mStep[0m  [30/106], [94mLoss[0m : 2.69679
[1mStep[0m  [40/106], [94mLoss[0m : 2.79658
[1mStep[0m  [50/106], [94mLoss[0m : 2.94568
[1mStep[0m  [60/106], [94mLoss[0m : 2.66526
[1mStep[0m  [70/106], [94mLoss[0m : 2.69860
[1mStep[0m  [80/106], [94mLoss[0m : 2.88483
[1mStep[0m  [90/106], [94mLoss[0m : 2.76993
[1mStep[0m  [100/106], [94mLoss[0m : 2.69439

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.739, [92mTest[0m: 2.613, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44985
[1mStep[0m  [10/106], [94mLoss[0m : 2.77095
[1mStep[0m  [20/106], [94mLoss[0m : 2.55356
[1mStep[0m  [30/106], [94mLoss[0m : 2.69008
[1mStep[0m  [40/106], [94mLoss[0m : 3.19095
[1mStep[0m  [50/106], [94mLoss[0m : 2.92658
[1mStep[0m  [60/106], [94mLoss[0m : 2.58465
[1mStep[0m  [70/106], [94mLoss[0m : 2.89064
[1mStep[0m  [80/106], [94mLoss[0m : 2.69050
[1mStep[0m  [90/106], [94mLoss[0m : 2.74074
[1mStep[0m  [100/106], [94mLoss[0m : 2.63548

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.602, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68410
[1mStep[0m  [10/106], [94mLoss[0m : 2.89741
[1mStep[0m  [20/106], [94mLoss[0m : 2.96305
[1mStep[0m  [30/106], [94mLoss[0m : 2.54471
[1mStep[0m  [40/106], [94mLoss[0m : 2.63600
[1mStep[0m  [50/106], [94mLoss[0m : 2.84050
[1mStep[0m  [60/106], [94mLoss[0m : 2.99863
[1mStep[0m  [70/106], [94mLoss[0m : 2.65576
[1mStep[0m  [80/106], [94mLoss[0m : 2.65842
[1mStep[0m  [90/106], [94mLoss[0m : 2.22397
[1mStep[0m  [100/106], [94mLoss[0m : 2.78023

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.591, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75396
[1mStep[0m  [10/106], [94mLoss[0m : 2.81436
[1mStep[0m  [20/106], [94mLoss[0m : 2.68590
[1mStep[0m  [30/106], [94mLoss[0m : 2.75368
[1mStep[0m  [40/106], [94mLoss[0m : 2.94582
[1mStep[0m  [50/106], [94mLoss[0m : 2.76629
[1mStep[0m  [60/106], [94mLoss[0m : 2.74931
[1mStep[0m  [70/106], [94mLoss[0m : 2.67450
[1mStep[0m  [80/106], [94mLoss[0m : 2.57945
[1mStep[0m  [90/106], [94mLoss[0m : 2.97840
[1mStep[0m  [100/106], [94mLoss[0m : 2.91886

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.697, [92mTest[0m: 2.585, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.79404
[1mStep[0m  [10/106], [94mLoss[0m : 2.35537
[1mStep[0m  [20/106], [94mLoss[0m : 2.51901
[1mStep[0m  [30/106], [94mLoss[0m : 2.54307
[1mStep[0m  [40/106], [94mLoss[0m : 2.74959
[1mStep[0m  [50/106], [94mLoss[0m : 2.71139
[1mStep[0m  [60/106], [94mLoss[0m : 2.78403
[1mStep[0m  [70/106], [94mLoss[0m : 2.40457
[1mStep[0m  [80/106], [94mLoss[0m : 2.70041
[1mStep[0m  [90/106], [94mLoss[0m : 2.87717
[1mStep[0m  [100/106], [94mLoss[0m : 2.60622

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.573, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63305
[1mStep[0m  [10/106], [94mLoss[0m : 2.91614
[1mStep[0m  [20/106], [94mLoss[0m : 3.12574
[1mStep[0m  [30/106], [94mLoss[0m : 2.79851
[1mStep[0m  [40/106], [94mLoss[0m : 2.99158
[1mStep[0m  [50/106], [94mLoss[0m : 2.67889
[1mStep[0m  [60/106], [94mLoss[0m : 2.88030
[1mStep[0m  [70/106], [94mLoss[0m : 2.60390
[1mStep[0m  [80/106], [94mLoss[0m : 2.88415
[1mStep[0m  [90/106], [94mLoss[0m : 2.69186
[1mStep[0m  [100/106], [94mLoss[0m : 2.65326

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.563, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61154
[1mStep[0m  [10/106], [94mLoss[0m : 2.75933
[1mStep[0m  [20/106], [94mLoss[0m : 2.94289
[1mStep[0m  [30/106], [94mLoss[0m : 2.53485
[1mStep[0m  [40/106], [94mLoss[0m : 2.51436
[1mStep[0m  [50/106], [94mLoss[0m : 2.85643
[1mStep[0m  [60/106], [94mLoss[0m : 2.70075
[1mStep[0m  [70/106], [94mLoss[0m : 2.69781
[1mStep[0m  [80/106], [94mLoss[0m : 2.64241
[1mStep[0m  [90/106], [94mLoss[0m : 2.61276
[1mStep[0m  [100/106], [94mLoss[0m : 2.72922

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.95457
[1mStep[0m  [10/106], [94mLoss[0m : 2.72735
[1mStep[0m  [20/106], [94mLoss[0m : 2.80641
[1mStep[0m  [30/106], [94mLoss[0m : 2.83259
[1mStep[0m  [40/106], [94mLoss[0m : 2.56649
[1mStep[0m  [50/106], [94mLoss[0m : 2.54083
[1mStep[0m  [60/106], [94mLoss[0m : 2.95653
[1mStep[0m  [70/106], [94mLoss[0m : 2.60482
[1mStep[0m  [80/106], [94mLoss[0m : 2.91987
[1mStep[0m  [90/106], [94mLoss[0m : 2.52140
[1mStep[0m  [100/106], [94mLoss[0m : 2.74786

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.551, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38154
[1mStep[0m  [10/106], [94mLoss[0m : 2.85823
[1mStep[0m  [20/106], [94mLoss[0m : 2.59001
[1mStep[0m  [30/106], [94mLoss[0m : 2.79902
[1mStep[0m  [40/106], [94mLoss[0m : 2.71857
[1mStep[0m  [50/106], [94mLoss[0m : 2.65725
[1mStep[0m  [60/106], [94mLoss[0m : 2.74408
[1mStep[0m  [70/106], [94mLoss[0m : 2.70734
[1mStep[0m  [80/106], [94mLoss[0m : 2.90944
[1mStep[0m  [90/106], [94mLoss[0m : 2.47722
[1mStep[0m  [100/106], [94mLoss[0m : 2.58316

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.545, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71143
[1mStep[0m  [10/106], [94mLoss[0m : 2.75876
[1mStep[0m  [20/106], [94mLoss[0m : 2.76913
[1mStep[0m  [30/106], [94mLoss[0m : 2.70915
[1mStep[0m  [40/106], [94mLoss[0m : 2.20941
[1mStep[0m  [50/106], [94mLoss[0m : 2.53431
[1mStep[0m  [60/106], [94mLoss[0m : 2.88285
[1mStep[0m  [70/106], [94mLoss[0m : 2.51644
[1mStep[0m  [80/106], [94mLoss[0m : 2.54593
[1mStep[0m  [90/106], [94mLoss[0m : 2.60303
[1mStep[0m  [100/106], [94mLoss[0m : 2.53221

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.534, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44446
[1mStep[0m  [10/106], [94mLoss[0m : 2.47604
[1mStep[0m  [20/106], [94mLoss[0m : 2.78126
[1mStep[0m  [30/106], [94mLoss[0m : 2.73607
[1mStep[0m  [40/106], [94mLoss[0m : 2.98957
[1mStep[0m  [50/106], [94mLoss[0m : 2.71993
[1mStep[0m  [60/106], [94mLoss[0m : 2.58835
[1mStep[0m  [70/106], [94mLoss[0m : 2.48765
[1mStep[0m  [80/106], [94mLoss[0m : 2.66373
[1mStep[0m  [90/106], [94mLoss[0m : 2.63395
[1mStep[0m  [100/106], [94mLoss[0m : 2.62311

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.532, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71390
[1mStep[0m  [10/106], [94mLoss[0m : 2.92440
[1mStep[0m  [20/106], [94mLoss[0m : 2.55657
[1mStep[0m  [30/106], [94mLoss[0m : 2.57422
[1mStep[0m  [40/106], [94mLoss[0m : 2.74958
[1mStep[0m  [50/106], [94mLoss[0m : 2.47617
[1mStep[0m  [60/106], [94mLoss[0m : 2.43571
[1mStep[0m  [70/106], [94mLoss[0m : 2.85527
[1mStep[0m  [80/106], [94mLoss[0m : 2.53271
[1mStep[0m  [90/106], [94mLoss[0m : 2.50148
[1mStep[0m  [100/106], [94mLoss[0m : 2.27678

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.526, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.520
====================================

Phase 2 - Evaluation MAE:  2.52006622080533
MAE score P1        3.308371
MAE score P2        2.520066
loss                2.632589
learning_rate         0.0001
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay            0.01
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 9.88736
[1mStep[0m  [10/106], [94mLoss[0m : 10.47850
[1mStep[0m  [20/106], [94mLoss[0m : 10.90510
[1mStep[0m  [30/106], [94mLoss[0m : 10.69717
[1mStep[0m  [40/106], [94mLoss[0m : 10.02331
[1mStep[0m  [50/106], [94mLoss[0m : 10.22196
[1mStep[0m  [60/106], [94mLoss[0m : 9.95136
[1mStep[0m  [70/106], [94mLoss[0m : 10.04670
[1mStep[0m  [80/106], [94mLoss[0m : 10.54539
[1mStep[0m  [90/106], [94mLoss[0m : 10.06549
[1mStep[0m  [100/106], [94mLoss[0m : 10.21721

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.350, [92mTest[0m: 10.408, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.97321
[1mStep[0m  [10/106], [94mLoss[0m : 10.12970
[1mStep[0m  [20/106], [94mLoss[0m : 10.51546
[1mStep[0m  [30/106], [94mLoss[0m : 10.34025
[1mStep[0m  [40/106], [94mLoss[0m : 10.30842
[1mStep[0m  [50/106], [94mLoss[0m : 10.17815
[1mStep[0m  [60/106], [94mLoss[0m : 10.09154
[1mStep[0m  [70/106], [94mLoss[0m : 9.99494
[1mStep[0m  [80/106], [94mLoss[0m : 10.06809
[1mStep[0m  [90/106], [94mLoss[0m : 10.28052
[1mStep[0m  [100/106], [94mLoss[0m : 9.77139

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.188, [92mTest[0m: 10.261, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.74542
[1mStep[0m  [10/106], [94mLoss[0m : 9.88928
[1mStep[0m  [20/106], [94mLoss[0m : 10.35460
[1mStep[0m  [30/106], [94mLoss[0m : 10.13815
[1mStep[0m  [40/106], [94mLoss[0m : 10.17401
[1mStep[0m  [50/106], [94mLoss[0m : 10.40139
[1mStep[0m  [60/106], [94mLoss[0m : 10.17947
[1mStep[0m  [70/106], [94mLoss[0m : 9.95415
[1mStep[0m  [80/106], [94mLoss[0m : 9.66508
[1mStep[0m  [90/106], [94mLoss[0m : 10.58778
[1mStep[0m  [100/106], [94mLoss[0m : 9.84200

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.025, [92mTest[0m: 10.105, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.77140
[1mStep[0m  [10/106], [94mLoss[0m : 9.83043
[1mStep[0m  [20/106], [94mLoss[0m : 9.66466
[1mStep[0m  [30/106], [94mLoss[0m : 10.04448
[1mStep[0m  [40/106], [94mLoss[0m : 10.07693
[1mStep[0m  [50/106], [94mLoss[0m : 9.94912
[1mStep[0m  [60/106], [94mLoss[0m : 10.02895
[1mStep[0m  [70/106], [94mLoss[0m : 10.18033
[1mStep[0m  [80/106], [94mLoss[0m : 9.94004
[1mStep[0m  [90/106], [94mLoss[0m : 9.97969
[1mStep[0m  [100/106], [94mLoss[0m : 9.46664

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.872, [92mTest[0m: 9.948, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.07050
[1mStep[0m  [10/106], [94mLoss[0m : 9.69629
[1mStep[0m  [20/106], [94mLoss[0m : 9.21774
[1mStep[0m  [30/106], [94mLoss[0m : 9.39525
[1mStep[0m  [40/106], [94mLoss[0m : 9.88351
[1mStep[0m  [50/106], [94mLoss[0m : 9.81346
[1mStep[0m  [60/106], [94mLoss[0m : 9.90524
[1mStep[0m  [70/106], [94mLoss[0m : 10.13039
[1mStep[0m  [80/106], [94mLoss[0m : 9.87408
[1mStep[0m  [90/106], [94mLoss[0m : 9.20529
[1mStep[0m  [100/106], [94mLoss[0m : 9.38186

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.706, [92mTest[0m: 9.790, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.50064
[1mStep[0m  [10/106], [94mLoss[0m : 9.65404
[1mStep[0m  [20/106], [94mLoss[0m : 9.95763
[1mStep[0m  [30/106], [94mLoss[0m : 9.76624
[1mStep[0m  [40/106], [94mLoss[0m : 9.94580
[1mStep[0m  [50/106], [94mLoss[0m : 8.97557
[1mStep[0m  [60/106], [94mLoss[0m : 9.76087
[1mStep[0m  [70/106], [94mLoss[0m : 9.35407
[1mStep[0m  [80/106], [94mLoss[0m : 9.38721
[1mStep[0m  [90/106], [94mLoss[0m : 9.71675
[1mStep[0m  [100/106], [94mLoss[0m : 9.64353

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.549, [92mTest[0m: 9.622, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.89425
[1mStep[0m  [10/106], [94mLoss[0m : 9.75816
[1mStep[0m  [20/106], [94mLoss[0m : 9.57304
[1mStep[0m  [30/106], [94mLoss[0m : 9.78835
[1mStep[0m  [40/106], [94mLoss[0m : 9.68008
[1mStep[0m  [50/106], [94mLoss[0m : 9.39536
[1mStep[0m  [60/106], [94mLoss[0m : 9.41892
[1mStep[0m  [70/106], [94mLoss[0m : 8.90179
[1mStep[0m  [80/106], [94mLoss[0m : 8.69178
[1mStep[0m  [90/106], [94mLoss[0m : 8.89233
[1mStep[0m  [100/106], [94mLoss[0m : 9.38510

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.387, [92mTest[0m: 9.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.93554
[1mStep[0m  [10/106], [94mLoss[0m : 9.51524
[1mStep[0m  [20/106], [94mLoss[0m : 9.47263
[1mStep[0m  [30/106], [94mLoss[0m : 9.49736
[1mStep[0m  [40/106], [94mLoss[0m : 9.19964
[1mStep[0m  [50/106], [94mLoss[0m : 9.25464
[1mStep[0m  [60/106], [94mLoss[0m : 9.13694
[1mStep[0m  [70/106], [94mLoss[0m : 9.53191
[1mStep[0m  [80/106], [94mLoss[0m : 9.67798
[1mStep[0m  [90/106], [94mLoss[0m : 9.15069
[1mStep[0m  [100/106], [94mLoss[0m : 8.73956

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.233, [92mTest[0m: 9.319, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.73429
[1mStep[0m  [10/106], [94mLoss[0m : 8.68538
[1mStep[0m  [20/106], [94mLoss[0m : 9.03729
[1mStep[0m  [30/106], [94mLoss[0m : 8.95317
[1mStep[0m  [40/106], [94mLoss[0m : 9.60748
[1mStep[0m  [50/106], [94mLoss[0m : 9.45505
[1mStep[0m  [60/106], [94mLoss[0m : 9.20173
[1mStep[0m  [70/106], [94mLoss[0m : 8.35813
[1mStep[0m  [80/106], [94mLoss[0m : 9.51006
[1mStep[0m  [90/106], [94mLoss[0m : 8.59538
[1mStep[0m  [100/106], [94mLoss[0m : 8.80577

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.074, [92mTest[0m: 9.143, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.96878
[1mStep[0m  [10/106], [94mLoss[0m : 9.04235
[1mStep[0m  [20/106], [94mLoss[0m : 9.05355
[1mStep[0m  [30/106], [94mLoss[0m : 9.12489
[1mStep[0m  [40/106], [94mLoss[0m : 9.06616
[1mStep[0m  [50/106], [94mLoss[0m : 8.62799
[1mStep[0m  [60/106], [94mLoss[0m : 8.47518
[1mStep[0m  [70/106], [94mLoss[0m : 9.42406
[1mStep[0m  [80/106], [94mLoss[0m : 8.82902
[1mStep[0m  [90/106], [94mLoss[0m : 8.22404
[1mStep[0m  [100/106], [94mLoss[0m : 8.97204

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.914, [92mTest[0m: 8.991, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.70239
[1mStep[0m  [10/106], [94mLoss[0m : 8.55710
[1mStep[0m  [20/106], [94mLoss[0m : 8.77526
[1mStep[0m  [30/106], [94mLoss[0m : 8.51544
[1mStep[0m  [40/106], [94mLoss[0m : 8.27047
[1mStep[0m  [50/106], [94mLoss[0m : 7.95276
[1mStep[0m  [60/106], [94mLoss[0m : 8.48527
[1mStep[0m  [70/106], [94mLoss[0m : 8.77527
[1mStep[0m  [80/106], [94mLoss[0m : 8.40587
[1mStep[0m  [90/106], [94mLoss[0m : 8.47901
[1mStep[0m  [100/106], [94mLoss[0m : 9.33363

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.753, [92mTest[0m: 8.850, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.85534
[1mStep[0m  [10/106], [94mLoss[0m : 8.43714
[1mStep[0m  [20/106], [94mLoss[0m : 8.71570
[1mStep[0m  [30/106], [94mLoss[0m : 8.35134
[1mStep[0m  [40/106], [94mLoss[0m : 8.46898
[1mStep[0m  [50/106], [94mLoss[0m : 8.07236
[1mStep[0m  [60/106], [94mLoss[0m : 8.18662
[1mStep[0m  [70/106], [94mLoss[0m : 9.05122
[1mStep[0m  [80/106], [94mLoss[0m : 8.27841
[1mStep[0m  [90/106], [94mLoss[0m : 8.42652
[1mStep[0m  [100/106], [94mLoss[0m : 8.54465

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.594, [92mTest[0m: 8.681, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.11946
[1mStep[0m  [10/106], [94mLoss[0m : 8.25814
[1mStep[0m  [20/106], [94mLoss[0m : 8.27571
[1mStep[0m  [30/106], [94mLoss[0m : 9.11158
[1mStep[0m  [40/106], [94mLoss[0m : 8.77456
[1mStep[0m  [50/106], [94mLoss[0m : 8.90615
[1mStep[0m  [60/106], [94mLoss[0m : 8.80434
[1mStep[0m  [70/106], [94mLoss[0m : 7.97299
[1mStep[0m  [80/106], [94mLoss[0m : 8.20772
[1mStep[0m  [90/106], [94mLoss[0m : 8.14841
[1mStep[0m  [100/106], [94mLoss[0m : 8.07769

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.426, [92mTest[0m: 8.515, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.31643
[1mStep[0m  [10/106], [94mLoss[0m : 8.31942
[1mStep[0m  [20/106], [94mLoss[0m : 7.78304
[1mStep[0m  [30/106], [94mLoss[0m : 8.10987
[1mStep[0m  [40/106], [94mLoss[0m : 8.52599
[1mStep[0m  [50/106], [94mLoss[0m : 7.73644
[1mStep[0m  [60/106], [94mLoss[0m : 8.36833
[1mStep[0m  [70/106], [94mLoss[0m : 8.63655
[1mStep[0m  [80/106], [94mLoss[0m : 8.54500
[1mStep[0m  [90/106], [94mLoss[0m : 8.27445
[1mStep[0m  [100/106], [94mLoss[0m : 8.31106

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.269, [92mTest[0m: 8.363, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.54609
[1mStep[0m  [10/106], [94mLoss[0m : 8.16035
[1mStep[0m  [20/106], [94mLoss[0m : 8.19851
[1mStep[0m  [30/106], [94mLoss[0m : 8.33128
[1mStep[0m  [40/106], [94mLoss[0m : 8.21749
[1mStep[0m  [50/106], [94mLoss[0m : 8.29331
[1mStep[0m  [60/106], [94mLoss[0m : 7.82140
[1mStep[0m  [70/106], [94mLoss[0m : 8.10983
[1mStep[0m  [80/106], [94mLoss[0m : 8.28078
[1mStep[0m  [90/106], [94mLoss[0m : 8.01049
[1mStep[0m  [100/106], [94mLoss[0m : 7.67326

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.116, [92mTest[0m: 8.195, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.12880
[1mStep[0m  [10/106], [94mLoss[0m : 8.21680
[1mStep[0m  [20/106], [94mLoss[0m : 8.09339
[1mStep[0m  [30/106], [94mLoss[0m : 7.87710
[1mStep[0m  [40/106], [94mLoss[0m : 7.51099
[1mStep[0m  [50/106], [94mLoss[0m : 7.61299
[1mStep[0m  [60/106], [94mLoss[0m : 8.14139
[1mStep[0m  [70/106], [94mLoss[0m : 8.21519
[1mStep[0m  [80/106], [94mLoss[0m : 7.74989
[1mStep[0m  [90/106], [94mLoss[0m : 7.83337
[1mStep[0m  [100/106], [94mLoss[0m : 7.81356

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.961, [92mTest[0m: 8.042, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.90002
[1mStep[0m  [10/106], [94mLoss[0m : 7.61909
[1mStep[0m  [20/106], [94mLoss[0m : 7.75105
[1mStep[0m  [30/106], [94mLoss[0m : 7.68512
[1mStep[0m  [40/106], [94mLoss[0m : 7.93498
[1mStep[0m  [50/106], [94mLoss[0m : 7.82255
[1mStep[0m  [60/106], [94mLoss[0m : 7.64308
[1mStep[0m  [70/106], [94mLoss[0m : 7.64235
[1mStep[0m  [80/106], [94mLoss[0m : 7.88714
[1mStep[0m  [90/106], [94mLoss[0m : 7.71607
[1mStep[0m  [100/106], [94mLoss[0m : 7.77101

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.803, [92mTest[0m: 7.873, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.00390
[1mStep[0m  [10/106], [94mLoss[0m : 7.72570
[1mStep[0m  [20/106], [94mLoss[0m : 7.97784
[1mStep[0m  [30/106], [94mLoss[0m : 7.46584
[1mStep[0m  [40/106], [94mLoss[0m : 7.23633
[1mStep[0m  [50/106], [94mLoss[0m : 7.80819
[1mStep[0m  [60/106], [94mLoss[0m : 7.54599
[1mStep[0m  [70/106], [94mLoss[0m : 7.42883
[1mStep[0m  [80/106], [94mLoss[0m : 8.01141
[1mStep[0m  [90/106], [94mLoss[0m : 6.88348
[1mStep[0m  [100/106], [94mLoss[0m : 8.35957

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.642, [92mTest[0m: 7.716, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.18483
[1mStep[0m  [10/106], [94mLoss[0m : 7.49559
[1mStep[0m  [20/106], [94mLoss[0m : 6.83540
[1mStep[0m  [30/106], [94mLoss[0m : 7.44015
[1mStep[0m  [40/106], [94mLoss[0m : 7.84272
[1mStep[0m  [50/106], [94mLoss[0m : 7.78750
[1mStep[0m  [60/106], [94mLoss[0m : 7.58544
[1mStep[0m  [70/106], [94mLoss[0m : 7.49403
[1mStep[0m  [80/106], [94mLoss[0m : 6.96789
[1mStep[0m  [90/106], [94mLoss[0m : 7.40197
[1mStep[0m  [100/106], [94mLoss[0m : 7.71063

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.485, [92mTest[0m: 7.556, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.71868
[1mStep[0m  [10/106], [94mLoss[0m : 7.51108
[1mStep[0m  [20/106], [94mLoss[0m : 6.83357
[1mStep[0m  [30/106], [94mLoss[0m : 7.08420
[1mStep[0m  [40/106], [94mLoss[0m : 7.14680
[1mStep[0m  [50/106], [94mLoss[0m : 7.70173
[1mStep[0m  [60/106], [94mLoss[0m : 7.14113
[1mStep[0m  [70/106], [94mLoss[0m : 7.81702
[1mStep[0m  [80/106], [94mLoss[0m : 6.77160
[1mStep[0m  [90/106], [94mLoss[0m : 7.45679
[1mStep[0m  [100/106], [94mLoss[0m : 7.42210

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.333, [92mTest[0m: 7.403, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.15185
[1mStep[0m  [10/106], [94mLoss[0m : 6.96768
[1mStep[0m  [20/106], [94mLoss[0m : 7.41402
[1mStep[0m  [30/106], [94mLoss[0m : 6.62072
[1mStep[0m  [40/106], [94mLoss[0m : 7.66948
[1mStep[0m  [50/106], [94mLoss[0m : 7.53851
[1mStep[0m  [60/106], [94mLoss[0m : 7.02210
[1mStep[0m  [70/106], [94mLoss[0m : 6.89743
[1mStep[0m  [80/106], [94mLoss[0m : 7.26005
[1mStep[0m  [90/106], [94mLoss[0m : 6.86471
[1mStep[0m  [100/106], [94mLoss[0m : 7.15687

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.175, [92mTest[0m: 7.242, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.92418
[1mStep[0m  [10/106], [94mLoss[0m : 7.10773
[1mStep[0m  [20/106], [94mLoss[0m : 7.44070
[1mStep[0m  [30/106], [94mLoss[0m : 6.58582
[1mStep[0m  [40/106], [94mLoss[0m : 7.58501
[1mStep[0m  [50/106], [94mLoss[0m : 6.99192
[1mStep[0m  [60/106], [94mLoss[0m : 7.15485
[1mStep[0m  [70/106], [94mLoss[0m : 6.96099
[1mStep[0m  [80/106], [94mLoss[0m : 6.80265
[1mStep[0m  [90/106], [94mLoss[0m : 6.65817
[1mStep[0m  [100/106], [94mLoss[0m : 6.94770

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.027, [92mTest[0m: 7.109, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.41874
[1mStep[0m  [10/106], [94mLoss[0m : 6.94551
[1mStep[0m  [20/106], [94mLoss[0m : 6.87095
[1mStep[0m  [30/106], [94mLoss[0m : 7.56137
[1mStep[0m  [40/106], [94mLoss[0m : 6.70352
[1mStep[0m  [50/106], [94mLoss[0m : 7.25762
[1mStep[0m  [60/106], [94mLoss[0m : 6.63318
[1mStep[0m  [70/106], [94mLoss[0m : 6.48898
[1mStep[0m  [80/106], [94mLoss[0m : 6.81132
[1mStep[0m  [90/106], [94mLoss[0m : 6.79432
[1mStep[0m  [100/106], [94mLoss[0m : 6.53052

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.876, [92mTest[0m: 6.951, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.60740
[1mStep[0m  [10/106], [94mLoss[0m : 6.74006
[1mStep[0m  [20/106], [94mLoss[0m : 6.63628
[1mStep[0m  [30/106], [94mLoss[0m : 6.90496
[1mStep[0m  [40/106], [94mLoss[0m : 6.84605
[1mStep[0m  [50/106], [94mLoss[0m : 6.53275
[1mStep[0m  [60/106], [94mLoss[0m : 6.58576
[1mStep[0m  [70/106], [94mLoss[0m : 6.47785
[1mStep[0m  [80/106], [94mLoss[0m : 6.71453
[1mStep[0m  [90/106], [94mLoss[0m : 6.65107
[1mStep[0m  [100/106], [94mLoss[0m : 6.57341

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.741, [92mTest[0m: 6.816, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.38716
[1mStep[0m  [10/106], [94mLoss[0m : 6.38627
[1mStep[0m  [20/106], [94mLoss[0m : 6.36603
[1mStep[0m  [30/106], [94mLoss[0m : 6.72545
[1mStep[0m  [40/106], [94mLoss[0m : 6.54724
[1mStep[0m  [50/106], [94mLoss[0m : 6.65297
[1mStep[0m  [60/106], [94mLoss[0m : 6.56571
[1mStep[0m  [70/106], [94mLoss[0m : 6.34569
[1mStep[0m  [80/106], [94mLoss[0m : 6.76523
[1mStep[0m  [90/106], [94mLoss[0m : 7.15091
[1mStep[0m  [100/106], [94mLoss[0m : 6.99905

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.604, [92mTest[0m: 6.677, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.93665
[1mStep[0m  [10/106], [94mLoss[0m : 6.87072
[1mStep[0m  [20/106], [94mLoss[0m : 6.27413
[1mStep[0m  [30/106], [94mLoss[0m : 6.38620
[1mStep[0m  [40/106], [94mLoss[0m : 6.41693
[1mStep[0m  [50/106], [94mLoss[0m : 6.62832
[1mStep[0m  [60/106], [94mLoss[0m : 5.94430
[1mStep[0m  [70/106], [94mLoss[0m : 6.27774
[1mStep[0m  [80/106], [94mLoss[0m : 6.07049
[1mStep[0m  [90/106], [94mLoss[0m : 6.39386
[1mStep[0m  [100/106], [94mLoss[0m : 6.30661

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.462, [92mTest[0m: 6.522, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.40674
[1mStep[0m  [10/106], [94mLoss[0m : 6.46226
[1mStep[0m  [20/106], [94mLoss[0m : 6.18846
[1mStep[0m  [30/106], [94mLoss[0m : 6.55143
[1mStep[0m  [40/106], [94mLoss[0m : 6.36424
[1mStep[0m  [50/106], [94mLoss[0m : 6.53641
[1mStep[0m  [60/106], [94mLoss[0m : 6.12829
[1mStep[0m  [70/106], [94mLoss[0m : 6.35346
[1mStep[0m  [80/106], [94mLoss[0m : 6.03787
[1mStep[0m  [90/106], [94mLoss[0m : 6.49932
[1mStep[0m  [100/106], [94mLoss[0m : 6.22365

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.318, [92mTest[0m: 6.373, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.15201
[1mStep[0m  [10/106], [94mLoss[0m : 6.72607
[1mStep[0m  [20/106], [94mLoss[0m : 5.82380
[1mStep[0m  [30/106], [94mLoss[0m : 6.56773
[1mStep[0m  [40/106], [94mLoss[0m : 5.95049
[1mStep[0m  [50/106], [94mLoss[0m : 5.90132
[1mStep[0m  [60/106], [94mLoss[0m : 5.95627
[1mStep[0m  [70/106], [94mLoss[0m : 5.84290
[1mStep[0m  [80/106], [94mLoss[0m : 5.97819
[1mStep[0m  [90/106], [94mLoss[0m : 5.53151
[1mStep[0m  [100/106], [94mLoss[0m : 6.54362

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.168, [92mTest[0m: 6.236, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.30630
[1mStep[0m  [10/106], [94mLoss[0m : 6.04245
[1mStep[0m  [20/106], [94mLoss[0m : 6.01962
[1mStep[0m  [30/106], [94mLoss[0m : 6.85653
[1mStep[0m  [40/106], [94mLoss[0m : 6.15632
[1mStep[0m  [50/106], [94mLoss[0m : 5.79323
[1mStep[0m  [60/106], [94mLoss[0m : 5.73469
[1mStep[0m  [70/106], [94mLoss[0m : 5.91865
[1mStep[0m  [80/106], [94mLoss[0m : 5.87463
[1mStep[0m  [90/106], [94mLoss[0m : 5.91112
[1mStep[0m  [100/106], [94mLoss[0m : 6.18810

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.033, [92mTest[0m: 6.107, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.02878
[1mStep[0m  [10/106], [94mLoss[0m : 6.25852
[1mStep[0m  [20/106], [94mLoss[0m : 6.00194
[1mStep[0m  [30/106], [94mLoss[0m : 6.36345
[1mStep[0m  [40/106], [94mLoss[0m : 5.51326
[1mStep[0m  [50/106], [94mLoss[0m : 5.80800
[1mStep[0m  [60/106], [94mLoss[0m : 6.09569
[1mStep[0m  [70/106], [94mLoss[0m : 6.07858
[1mStep[0m  [80/106], [94mLoss[0m : 6.02741
[1mStep[0m  [90/106], [94mLoss[0m : 5.97246
[1mStep[0m  [100/106], [94mLoss[0m : 6.27337

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 5.904, [92mTest[0m: 5.965, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.813
====================================

Phase 1 - Evaluation MAE:  5.812520486003947
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 5.69455
[1mStep[0m  [10/106], [94mLoss[0m : 5.84596
[1mStep[0m  [20/106], [94mLoss[0m : 5.57992
[1mStep[0m  [30/106], [94mLoss[0m : 5.35791
[1mStep[0m  [40/106], [94mLoss[0m : 5.97695
[1mStep[0m  [50/106], [94mLoss[0m : 5.69916
[1mStep[0m  [60/106], [94mLoss[0m : 6.12150
[1mStep[0m  [70/106], [94mLoss[0m : 5.67295
[1mStep[0m  [80/106], [94mLoss[0m : 5.45356
[1mStep[0m  [90/106], [94mLoss[0m : 6.42412
[1mStep[0m  [100/106], [94mLoss[0m : 5.99947

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.762, [92mTest[0m: 5.814, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.74931
[1mStep[0m  [10/106], [94mLoss[0m : 5.42817
[1mStep[0m  [20/106], [94mLoss[0m : 5.19106
[1mStep[0m  [30/106], [94mLoss[0m : 5.43984
[1mStep[0m  [40/106], [94mLoss[0m : 5.74616
[1mStep[0m  [50/106], [94mLoss[0m : 5.37398
[1mStep[0m  [60/106], [94mLoss[0m : 5.51511
[1mStep[0m  [70/106], [94mLoss[0m : 6.17219
[1mStep[0m  [80/106], [94mLoss[0m : 5.56517
[1mStep[0m  [90/106], [94mLoss[0m : 5.95532
[1mStep[0m  [100/106], [94mLoss[0m : 5.51731

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.606, [92mTest[0m: 5.657, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.20065
[1mStep[0m  [10/106], [94mLoss[0m : 5.90963
[1mStep[0m  [20/106], [94mLoss[0m : 4.90726
[1mStep[0m  [30/106], [94mLoss[0m : 5.43032
[1mStep[0m  [40/106], [94mLoss[0m : 5.99395
[1mStep[0m  [50/106], [94mLoss[0m : 5.37985
[1mStep[0m  [60/106], [94mLoss[0m : 5.39669
[1mStep[0m  [70/106], [94mLoss[0m : 5.94689
[1mStep[0m  [80/106], [94mLoss[0m : 5.33662
[1mStep[0m  [90/106], [94mLoss[0m : 4.99840
[1mStep[0m  [100/106], [94mLoss[0m : 5.60891

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.475, [92mTest[0m: 5.529, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.11831
[1mStep[0m  [10/106], [94mLoss[0m : 5.57002
[1mStep[0m  [20/106], [94mLoss[0m : 5.08545
[1mStep[0m  [30/106], [94mLoss[0m : 6.15943
[1mStep[0m  [40/106], [94mLoss[0m : 5.43150
[1mStep[0m  [50/106], [94mLoss[0m : 5.06401
[1mStep[0m  [60/106], [94mLoss[0m : 5.16029
[1mStep[0m  [70/106], [94mLoss[0m : 5.41216
[1mStep[0m  [80/106], [94mLoss[0m : 5.27332
[1mStep[0m  [90/106], [94mLoss[0m : 4.67156
[1mStep[0m  [100/106], [94mLoss[0m : 5.44555

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.341, [92mTest[0m: 5.391, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.96561
[1mStep[0m  [10/106], [94mLoss[0m : 5.63883
[1mStep[0m  [20/106], [94mLoss[0m : 5.16735
[1mStep[0m  [30/106], [94mLoss[0m : 5.33072
[1mStep[0m  [40/106], [94mLoss[0m : 4.74142
[1mStep[0m  [50/106], [94mLoss[0m : 5.59680
[1mStep[0m  [60/106], [94mLoss[0m : 4.77401
[1mStep[0m  [70/106], [94mLoss[0m : 4.73630
[1mStep[0m  [80/106], [94mLoss[0m : 4.87765
[1mStep[0m  [90/106], [94mLoss[0m : 5.07308
[1mStep[0m  [100/106], [94mLoss[0m : 4.93978

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.201, [92mTest[0m: 5.221, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.54244
[1mStep[0m  [10/106], [94mLoss[0m : 4.74991
[1mStep[0m  [20/106], [94mLoss[0m : 5.29233
[1mStep[0m  [30/106], [94mLoss[0m : 4.52609
[1mStep[0m  [40/106], [94mLoss[0m : 5.31336
[1mStep[0m  [50/106], [94mLoss[0m : 4.67814
[1mStep[0m  [60/106], [94mLoss[0m : 5.29876
[1mStep[0m  [70/106], [94mLoss[0m : 5.48777
[1mStep[0m  [80/106], [94mLoss[0m : 5.78248
[1mStep[0m  [90/106], [94mLoss[0m : 4.88352
[1mStep[0m  [100/106], [94mLoss[0m : 4.82478

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.071, [92mTest[0m: 5.098, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.60196
[1mStep[0m  [10/106], [94mLoss[0m : 5.19349
[1mStep[0m  [20/106], [94mLoss[0m : 4.51189
[1mStep[0m  [30/106], [94mLoss[0m : 4.76061
[1mStep[0m  [40/106], [94mLoss[0m : 4.61885
[1mStep[0m  [50/106], [94mLoss[0m : 5.20138
[1mStep[0m  [60/106], [94mLoss[0m : 5.19948
[1mStep[0m  [70/106], [94mLoss[0m : 4.61149
[1mStep[0m  [80/106], [94mLoss[0m : 5.18730
[1mStep[0m  [90/106], [94mLoss[0m : 4.85463
[1mStep[0m  [100/106], [94mLoss[0m : 5.31972

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.939, [92mTest[0m: 4.962, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.91217
[1mStep[0m  [10/106], [94mLoss[0m : 4.68506
[1mStep[0m  [20/106], [94mLoss[0m : 4.87668
[1mStep[0m  [30/106], [94mLoss[0m : 4.85028
[1mStep[0m  [40/106], [94mLoss[0m : 4.67229
[1mStep[0m  [50/106], [94mLoss[0m : 5.34851
[1mStep[0m  [60/106], [94mLoss[0m : 4.66533
[1mStep[0m  [70/106], [94mLoss[0m : 4.97200
[1mStep[0m  [80/106], [94mLoss[0m : 4.35095
[1mStep[0m  [90/106], [94mLoss[0m : 4.44406
[1mStep[0m  [100/106], [94mLoss[0m : 4.75052

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.826, [92mTest[0m: 4.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.54877
[1mStep[0m  [10/106], [94mLoss[0m : 4.80431
[1mStep[0m  [20/106], [94mLoss[0m : 4.72959
[1mStep[0m  [30/106], [94mLoss[0m : 4.86084
[1mStep[0m  [40/106], [94mLoss[0m : 4.36789
[1mStep[0m  [50/106], [94mLoss[0m : 4.75630
[1mStep[0m  [60/106], [94mLoss[0m : 4.58640
[1mStep[0m  [70/106], [94mLoss[0m : 4.99200
[1mStep[0m  [80/106], [94mLoss[0m : 4.89562
[1mStep[0m  [90/106], [94mLoss[0m : 4.71193
[1mStep[0m  [100/106], [94mLoss[0m : 5.05543

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.716, [92mTest[0m: 4.709, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.42431
[1mStep[0m  [10/106], [94mLoss[0m : 4.55663
[1mStep[0m  [20/106], [94mLoss[0m : 4.61758
[1mStep[0m  [30/106], [94mLoss[0m : 4.91311
[1mStep[0m  [40/106], [94mLoss[0m : 4.26124
[1mStep[0m  [50/106], [94mLoss[0m : 4.44442
[1mStep[0m  [60/106], [94mLoss[0m : 4.02457
[1mStep[0m  [70/106], [94mLoss[0m : 4.66881
[1mStep[0m  [80/106], [94mLoss[0m : 3.99681
[1mStep[0m  [90/106], [94mLoss[0m : 4.43335
[1mStep[0m  [100/106], [94mLoss[0m : 4.61801

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.606, [92mTest[0m: 4.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.89016
[1mStep[0m  [10/106], [94mLoss[0m : 5.06067
[1mStep[0m  [20/106], [94mLoss[0m : 4.52768
[1mStep[0m  [30/106], [94mLoss[0m : 4.26755
[1mStep[0m  [40/106], [94mLoss[0m : 4.51763
[1mStep[0m  [50/106], [94mLoss[0m : 4.62032
[1mStep[0m  [60/106], [94mLoss[0m : 4.60194
[1mStep[0m  [70/106], [94mLoss[0m : 4.32831
[1mStep[0m  [80/106], [94mLoss[0m : 4.44650
[1mStep[0m  [90/106], [94mLoss[0m : 4.49855
[1mStep[0m  [100/106], [94mLoss[0m : 4.05377

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.512, [92mTest[0m: 4.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.95460
[1mStep[0m  [10/106], [94mLoss[0m : 4.15680
[1mStep[0m  [20/106], [94mLoss[0m : 4.22688
[1mStep[0m  [30/106], [94mLoss[0m : 4.50663
[1mStep[0m  [40/106], [94mLoss[0m : 4.90861
[1mStep[0m  [50/106], [94mLoss[0m : 4.46199
[1mStep[0m  [60/106], [94mLoss[0m : 4.61363
[1mStep[0m  [70/106], [94mLoss[0m : 4.15542
[1mStep[0m  [80/106], [94mLoss[0m : 3.87318
[1mStep[0m  [90/106], [94mLoss[0m : 5.05696
[1mStep[0m  [100/106], [94mLoss[0m : 4.50441

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.413, [92mTest[0m: 4.368, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.64443
[1mStep[0m  [10/106], [94mLoss[0m : 4.17499
[1mStep[0m  [20/106], [94mLoss[0m : 4.44489
[1mStep[0m  [30/106], [94mLoss[0m : 4.51724
[1mStep[0m  [40/106], [94mLoss[0m : 4.13340
[1mStep[0m  [50/106], [94mLoss[0m : 4.97424
[1mStep[0m  [60/106], [94mLoss[0m : 3.91054
[1mStep[0m  [70/106], [94mLoss[0m : 3.74205
[1mStep[0m  [80/106], [94mLoss[0m : 4.25259
[1mStep[0m  [90/106], [94mLoss[0m : 3.76557
[1mStep[0m  [100/106], [94mLoss[0m : 3.67411

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.317, [92mTest[0m: 4.279, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.53615
[1mStep[0m  [10/106], [94mLoss[0m : 4.96691
[1mStep[0m  [20/106], [94mLoss[0m : 4.35567
[1mStep[0m  [30/106], [94mLoss[0m : 4.20187
[1mStep[0m  [40/106], [94mLoss[0m : 4.17730
[1mStep[0m  [50/106], [94mLoss[0m : 4.58303
[1mStep[0m  [60/106], [94mLoss[0m : 3.84282
[1mStep[0m  [70/106], [94mLoss[0m : 3.96697
[1mStep[0m  [80/106], [94mLoss[0m : 3.96129
[1mStep[0m  [90/106], [94mLoss[0m : 4.55606
[1mStep[0m  [100/106], [94mLoss[0m : 4.52910

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.228, [92mTest[0m: 4.182, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.28057
[1mStep[0m  [10/106], [94mLoss[0m : 4.34169
[1mStep[0m  [20/106], [94mLoss[0m : 3.78912
[1mStep[0m  [30/106], [94mLoss[0m : 4.88856
[1mStep[0m  [40/106], [94mLoss[0m : 4.15769
[1mStep[0m  [50/106], [94mLoss[0m : 4.21305
[1mStep[0m  [60/106], [94mLoss[0m : 4.03389
[1mStep[0m  [70/106], [94mLoss[0m : 3.75061
[1mStep[0m  [80/106], [94mLoss[0m : 4.06669
[1mStep[0m  [90/106], [94mLoss[0m : 4.22432
[1mStep[0m  [100/106], [94mLoss[0m : 4.24590

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.126, [92mTest[0m: 4.081, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.95506
[1mStep[0m  [10/106], [94mLoss[0m : 3.80833
[1mStep[0m  [20/106], [94mLoss[0m : 3.91835
[1mStep[0m  [30/106], [94mLoss[0m : 4.63907
[1mStep[0m  [40/106], [94mLoss[0m : 4.31395
[1mStep[0m  [50/106], [94mLoss[0m : 3.89582
[1mStep[0m  [60/106], [94mLoss[0m : 4.54197
[1mStep[0m  [70/106], [94mLoss[0m : 4.07832
[1mStep[0m  [80/106], [94mLoss[0m : 4.46219
[1mStep[0m  [90/106], [94mLoss[0m : 3.92577
[1mStep[0m  [100/106], [94mLoss[0m : 3.57087

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.074, [92mTest[0m: 4.016, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.34447
[1mStep[0m  [10/106], [94mLoss[0m : 4.10226
[1mStep[0m  [20/106], [94mLoss[0m : 4.13490
[1mStep[0m  [30/106], [94mLoss[0m : 3.95044
[1mStep[0m  [40/106], [94mLoss[0m : 3.94445
[1mStep[0m  [50/106], [94mLoss[0m : 3.51069
[1mStep[0m  [60/106], [94mLoss[0m : 3.61452
[1mStep[0m  [70/106], [94mLoss[0m : 4.13923
[1mStep[0m  [80/106], [94mLoss[0m : 4.07511
[1mStep[0m  [90/106], [94mLoss[0m : 4.03179
[1mStep[0m  [100/106], [94mLoss[0m : 4.19998

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.010, [92mTest[0m: 3.912, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.37854
[1mStep[0m  [10/106], [94mLoss[0m : 3.64141
[1mStep[0m  [20/106], [94mLoss[0m : 3.62522
[1mStep[0m  [30/106], [94mLoss[0m : 3.56774
[1mStep[0m  [40/106], [94mLoss[0m : 3.77586
[1mStep[0m  [50/106], [94mLoss[0m : 4.09961
[1mStep[0m  [60/106], [94mLoss[0m : 3.53671
[1mStep[0m  [70/106], [94mLoss[0m : 3.71831
[1mStep[0m  [80/106], [94mLoss[0m : 3.39501
[1mStep[0m  [90/106], [94mLoss[0m : 3.73880
[1mStep[0m  [100/106], [94mLoss[0m : 3.33034

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.940, [92mTest[0m: 3.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.71819
[1mStep[0m  [10/106], [94mLoss[0m : 4.05976
[1mStep[0m  [20/106], [94mLoss[0m : 3.11370
[1mStep[0m  [30/106], [94mLoss[0m : 3.68998
[1mStep[0m  [40/106], [94mLoss[0m : 3.81845
[1mStep[0m  [50/106], [94mLoss[0m : 3.89438
[1mStep[0m  [60/106], [94mLoss[0m : 3.77989
[1mStep[0m  [70/106], [94mLoss[0m : 4.34183
[1mStep[0m  [80/106], [94mLoss[0m : 4.14198
[1mStep[0m  [90/106], [94mLoss[0m : 3.67644
[1mStep[0m  [100/106], [94mLoss[0m : 4.06149

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.855, [92mTest[0m: 3.759, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.72314
[1mStep[0m  [10/106], [94mLoss[0m : 3.52404
[1mStep[0m  [20/106], [94mLoss[0m : 3.76056
[1mStep[0m  [30/106], [94mLoss[0m : 4.34833
[1mStep[0m  [40/106], [94mLoss[0m : 3.87097
[1mStep[0m  [50/106], [94mLoss[0m : 3.86760
[1mStep[0m  [60/106], [94mLoss[0m : 3.67360
[1mStep[0m  [70/106], [94mLoss[0m : 3.78781
[1mStep[0m  [80/106], [94mLoss[0m : 3.45984
[1mStep[0m  [90/106], [94mLoss[0m : 3.41759
[1mStep[0m  [100/106], [94mLoss[0m : 3.98332

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.791, [92mTest[0m: 3.689, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.37769
[1mStep[0m  [10/106], [94mLoss[0m : 4.10013
[1mStep[0m  [20/106], [94mLoss[0m : 3.37994
[1mStep[0m  [30/106], [94mLoss[0m : 3.73380
[1mStep[0m  [40/106], [94mLoss[0m : 3.85927
[1mStep[0m  [50/106], [94mLoss[0m : 3.88057
[1mStep[0m  [60/106], [94mLoss[0m : 4.00572
[1mStep[0m  [70/106], [94mLoss[0m : 3.53344
[1mStep[0m  [80/106], [94mLoss[0m : 3.68696
[1mStep[0m  [90/106], [94mLoss[0m : 3.86244
[1mStep[0m  [100/106], [94mLoss[0m : 3.43589

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.739, [92mTest[0m: 3.616, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.77638
[1mStep[0m  [10/106], [94mLoss[0m : 3.61314
[1mStep[0m  [20/106], [94mLoss[0m : 4.13676
[1mStep[0m  [30/106], [94mLoss[0m : 3.72281
[1mStep[0m  [40/106], [94mLoss[0m : 3.42225
[1mStep[0m  [50/106], [94mLoss[0m : 3.87906
[1mStep[0m  [60/106], [94mLoss[0m : 3.85626
[1mStep[0m  [70/106], [94mLoss[0m : 3.08512
[1mStep[0m  [80/106], [94mLoss[0m : 3.57002
[1mStep[0m  [90/106], [94mLoss[0m : 3.67680
[1mStep[0m  [100/106], [94mLoss[0m : 3.44073

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.671, [92mTest[0m: 3.542, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.43427
[1mStep[0m  [10/106], [94mLoss[0m : 3.84983
[1mStep[0m  [20/106], [94mLoss[0m : 3.45980
[1mStep[0m  [30/106], [94mLoss[0m : 3.89159
[1mStep[0m  [40/106], [94mLoss[0m : 4.02976
[1mStep[0m  [50/106], [94mLoss[0m : 3.79380
[1mStep[0m  [60/106], [94mLoss[0m : 3.52589
[1mStep[0m  [70/106], [94mLoss[0m : 3.22713
[1mStep[0m  [80/106], [94mLoss[0m : 3.37853
[1mStep[0m  [90/106], [94mLoss[0m : 3.58914
[1mStep[0m  [100/106], [94mLoss[0m : 3.74786

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.624, [92mTest[0m: 3.498, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.82411
[1mStep[0m  [10/106], [94mLoss[0m : 3.48919
[1mStep[0m  [20/106], [94mLoss[0m : 3.42605
[1mStep[0m  [30/106], [94mLoss[0m : 3.54926
[1mStep[0m  [40/106], [94mLoss[0m : 3.61395
[1mStep[0m  [50/106], [94mLoss[0m : 3.53157
[1mStep[0m  [60/106], [94mLoss[0m : 4.00432
[1mStep[0m  [70/106], [94mLoss[0m : 3.64979
[1mStep[0m  [80/106], [94mLoss[0m : 3.56214
[1mStep[0m  [90/106], [94mLoss[0m : 3.46049
[1mStep[0m  [100/106], [94mLoss[0m : 3.38683

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.577, [92mTest[0m: 3.438, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.33301
[1mStep[0m  [10/106], [94mLoss[0m : 3.54781
[1mStep[0m  [20/106], [94mLoss[0m : 3.74351
[1mStep[0m  [30/106], [94mLoss[0m : 3.27929
[1mStep[0m  [40/106], [94mLoss[0m : 3.62077
[1mStep[0m  [50/106], [94mLoss[0m : 3.59256
[1mStep[0m  [60/106], [94mLoss[0m : 3.29811
[1mStep[0m  [70/106], [94mLoss[0m : 3.57725
[1mStep[0m  [80/106], [94mLoss[0m : 3.64998
[1mStep[0m  [90/106], [94mLoss[0m : 3.48376
[1mStep[0m  [100/106], [94mLoss[0m : 3.74659

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.547, [92mTest[0m: 3.393, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.56296
[1mStep[0m  [10/106], [94mLoss[0m : 3.50030
[1mStep[0m  [20/106], [94mLoss[0m : 3.21679
[1mStep[0m  [30/106], [94mLoss[0m : 3.79566
[1mStep[0m  [40/106], [94mLoss[0m : 3.12561
[1mStep[0m  [50/106], [94mLoss[0m : 3.69939
[1mStep[0m  [60/106], [94mLoss[0m : 3.36286
[1mStep[0m  [70/106], [94mLoss[0m : 3.51094
[1mStep[0m  [80/106], [94mLoss[0m : 3.55097
[1mStep[0m  [90/106], [94mLoss[0m : 3.52620
[1mStep[0m  [100/106], [94mLoss[0m : 3.41723

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.481, [92mTest[0m: 3.335, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.52473
[1mStep[0m  [10/106], [94mLoss[0m : 3.42773
[1mStep[0m  [20/106], [94mLoss[0m : 3.47024
[1mStep[0m  [30/106], [94mLoss[0m : 3.34076
[1mStep[0m  [40/106], [94mLoss[0m : 3.35778
[1mStep[0m  [50/106], [94mLoss[0m : 3.21625
[1mStep[0m  [60/106], [94mLoss[0m : 3.69776
[1mStep[0m  [70/106], [94mLoss[0m : 3.53739
[1mStep[0m  [80/106], [94mLoss[0m : 3.18450
[1mStep[0m  [90/106], [94mLoss[0m : 3.48005
[1mStep[0m  [100/106], [94mLoss[0m : 3.93907

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.449, [92mTest[0m: 3.299, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.24978
[1mStep[0m  [10/106], [94mLoss[0m : 3.32274
[1mStep[0m  [20/106], [94mLoss[0m : 3.17310
[1mStep[0m  [30/106], [94mLoss[0m : 3.15144
[1mStep[0m  [40/106], [94mLoss[0m : 3.03980
[1mStep[0m  [50/106], [94mLoss[0m : 3.37189
[1mStep[0m  [60/106], [94mLoss[0m : 3.00557
[1mStep[0m  [70/106], [94mLoss[0m : 3.19795
[1mStep[0m  [80/106], [94mLoss[0m : 3.82884
[1mStep[0m  [90/106], [94mLoss[0m : 3.24695
[1mStep[0m  [100/106], [94mLoss[0m : 3.25271

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.403, [92mTest[0m: 3.246, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.75758
[1mStep[0m  [10/106], [94mLoss[0m : 3.41681
[1mStep[0m  [20/106], [94mLoss[0m : 3.52184
[1mStep[0m  [30/106], [94mLoss[0m : 3.01352
[1mStep[0m  [40/106], [94mLoss[0m : 3.45721
[1mStep[0m  [50/106], [94mLoss[0m : 3.23282
[1mStep[0m  [60/106], [94mLoss[0m : 3.37956
[1mStep[0m  [70/106], [94mLoss[0m : 3.17542
[1mStep[0m  [80/106], [94mLoss[0m : 3.51722
[1mStep[0m  [90/106], [94mLoss[0m : 3.38724
[1mStep[0m  [100/106], [94mLoss[0m : 2.78815

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.382, [92mTest[0m: 3.217, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.33613
[1mStep[0m  [10/106], [94mLoss[0m : 3.53899
[1mStep[0m  [20/106], [94mLoss[0m : 3.19868
[1mStep[0m  [30/106], [94mLoss[0m : 2.97469
[1mStep[0m  [40/106], [94mLoss[0m : 3.33716
[1mStep[0m  [50/106], [94mLoss[0m : 3.23940
[1mStep[0m  [60/106], [94mLoss[0m : 3.23495
[1mStep[0m  [70/106], [94mLoss[0m : 3.60104
[1mStep[0m  [80/106], [94mLoss[0m : 3.64771
[1mStep[0m  [90/106], [94mLoss[0m : 3.49717
[1mStep[0m  [100/106], [94mLoss[0m : 3.40661

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.324, [92mTest[0m: 3.158, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.143
====================================

Phase 2 - Evaluation MAE:  3.1431133027346627
MAE score P1        5.81252
MAE score P2       3.143113
loss               3.324496
learning_rate        0.0001
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 11.22053
[1mStep[0m  [21/213], [94mLoss[0m : 11.34190
[1mStep[0m  [42/213], [94mLoss[0m : 11.47790
[1mStep[0m  [63/213], [94mLoss[0m : 11.35943
[1mStep[0m  [84/213], [94mLoss[0m : 11.40858
[1mStep[0m  [105/213], [94mLoss[0m : 11.45710
[1mStep[0m  [126/213], [94mLoss[0m : 10.45988
[1mStep[0m  [147/213], [94mLoss[0m : 11.62388
[1mStep[0m  [168/213], [94mLoss[0m : 11.06943
[1mStep[0m  [189/213], [94mLoss[0m : 11.19457
[1mStep[0m  [210/213], [94mLoss[0m : 11.61527

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.324, [92mTest[0m: 11.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.91881
[1mStep[0m  [21/213], [94mLoss[0m : 10.46925
[1mStep[0m  [42/213], [94mLoss[0m : 10.56606
[1mStep[0m  [63/213], [94mLoss[0m : 10.99631
[1mStep[0m  [84/213], [94mLoss[0m : 10.35308
[1mStep[0m  [105/213], [94mLoss[0m : 10.22641
[1mStep[0m  [126/213], [94mLoss[0m : 10.13526
[1mStep[0m  [147/213], [94mLoss[0m : 10.99148
[1mStep[0m  [168/213], [94mLoss[0m : 9.77738
[1mStep[0m  [189/213], [94mLoss[0m : 10.52582
[1mStep[0m  [210/213], [94mLoss[0m : 10.07817

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.744, [92mTest[0m: 11.047, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.96500
[1mStep[0m  [21/213], [94mLoss[0m : 10.06485
[1mStep[0m  [42/213], [94mLoss[0m : 11.10942
[1mStep[0m  [63/213], [94mLoss[0m : 10.10641
[1mStep[0m  [84/213], [94mLoss[0m : 10.45486
[1mStep[0m  [105/213], [94mLoss[0m : 10.06390
[1mStep[0m  [126/213], [94mLoss[0m : 9.51924
[1mStep[0m  [147/213], [94mLoss[0m : 10.32959
[1mStep[0m  [168/213], [94mLoss[0m : 10.77466
[1mStep[0m  [189/213], [94mLoss[0m : 10.60763
[1mStep[0m  [210/213], [94mLoss[0m : 9.83295

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.170, [92mTest[0m: 10.462, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.90707
[1mStep[0m  [21/213], [94mLoss[0m : 10.13178
[1mStep[0m  [42/213], [94mLoss[0m : 9.80448
[1mStep[0m  [63/213], [94mLoss[0m : 10.51211
[1mStep[0m  [84/213], [94mLoss[0m : 9.75660
[1mStep[0m  [105/213], [94mLoss[0m : 9.67176
[1mStep[0m  [126/213], [94mLoss[0m : 9.61031
[1mStep[0m  [147/213], [94mLoss[0m : 8.87742
[1mStep[0m  [168/213], [94mLoss[0m : 9.05165
[1mStep[0m  [189/213], [94mLoss[0m : 9.41227
[1mStep[0m  [210/213], [94mLoss[0m : 9.24283

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.591, [92mTest[0m: 9.886, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.55068
[1mStep[0m  [21/213], [94mLoss[0m : 9.26811
[1mStep[0m  [42/213], [94mLoss[0m : 9.24196
[1mStep[0m  [63/213], [94mLoss[0m : 9.55880
[1mStep[0m  [84/213], [94mLoss[0m : 8.80982
[1mStep[0m  [105/213], [94mLoss[0m : 9.32768
[1mStep[0m  [126/213], [94mLoss[0m : 8.49423
[1mStep[0m  [147/213], [94mLoss[0m : 8.86641
[1mStep[0m  [168/213], [94mLoss[0m : 9.69806
[1mStep[0m  [189/213], [94mLoss[0m : 8.66533
[1mStep[0m  [210/213], [94mLoss[0m : 8.82896

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.015, [92mTest[0m: 9.322, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.31288
[1mStep[0m  [21/213], [94mLoss[0m : 8.27049
[1mStep[0m  [42/213], [94mLoss[0m : 8.10557
[1mStep[0m  [63/213], [94mLoss[0m : 9.00427
[1mStep[0m  [84/213], [94mLoss[0m : 8.80745
[1mStep[0m  [105/213], [94mLoss[0m : 7.72209
[1mStep[0m  [126/213], [94mLoss[0m : 8.15354
[1mStep[0m  [147/213], [94mLoss[0m : 8.29678
[1mStep[0m  [168/213], [94mLoss[0m : 7.83915
[1mStep[0m  [189/213], [94mLoss[0m : 8.66730
[1mStep[0m  [210/213], [94mLoss[0m : 9.03534

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.445, [92mTest[0m: 8.737, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.56406
[1mStep[0m  [21/213], [94mLoss[0m : 8.38725
[1mStep[0m  [42/213], [94mLoss[0m : 7.63753
[1mStep[0m  [63/213], [94mLoss[0m : 7.84024
[1mStep[0m  [84/213], [94mLoss[0m : 8.09725
[1mStep[0m  [105/213], [94mLoss[0m : 8.35182
[1mStep[0m  [126/213], [94mLoss[0m : 8.44120
[1mStep[0m  [147/213], [94mLoss[0m : 8.00778
[1mStep[0m  [168/213], [94mLoss[0m : 7.66450
[1mStep[0m  [189/213], [94mLoss[0m : 6.57099
[1mStep[0m  [210/213], [94mLoss[0m : 7.41533

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.870, [92mTest[0m: 8.155, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.94198
[1mStep[0m  [21/213], [94mLoss[0m : 8.31753
[1mStep[0m  [42/213], [94mLoss[0m : 7.77212
[1mStep[0m  [63/213], [94mLoss[0m : 7.24768
[1mStep[0m  [84/213], [94mLoss[0m : 6.67292
[1mStep[0m  [105/213], [94mLoss[0m : 7.12124
[1mStep[0m  [126/213], [94mLoss[0m : 7.46257
[1mStep[0m  [147/213], [94mLoss[0m : 7.27061
[1mStep[0m  [168/213], [94mLoss[0m : 7.54234
[1mStep[0m  [189/213], [94mLoss[0m : 7.22638
[1mStep[0m  [210/213], [94mLoss[0m : 6.87449

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.280, [92mTest[0m: 7.582, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.75611
[1mStep[0m  [21/213], [94mLoss[0m : 6.88970
[1mStep[0m  [42/213], [94mLoss[0m : 6.72334
[1mStep[0m  [63/213], [94mLoss[0m : 6.64743
[1mStep[0m  [84/213], [94mLoss[0m : 7.08414
[1mStep[0m  [105/213], [94mLoss[0m : 7.00414
[1mStep[0m  [126/213], [94mLoss[0m : 7.72415
[1mStep[0m  [147/213], [94mLoss[0m : 5.79146
[1mStep[0m  [168/213], [94mLoss[0m : 6.55755
[1mStep[0m  [189/213], [94mLoss[0m : 6.38483
[1mStep[0m  [210/213], [94mLoss[0m : 5.78224

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.713, [92mTest[0m: 6.991, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.68209
[1mStep[0m  [21/213], [94mLoss[0m : 5.84448
[1mStep[0m  [42/213], [94mLoss[0m : 6.45297
[1mStep[0m  [63/213], [94mLoss[0m : 5.70684
[1mStep[0m  [84/213], [94mLoss[0m : 6.24923
[1mStep[0m  [105/213], [94mLoss[0m : 5.93395
[1mStep[0m  [126/213], [94mLoss[0m : 6.49384
[1mStep[0m  [147/213], [94mLoss[0m : 6.39277
[1mStep[0m  [168/213], [94mLoss[0m : 6.44537
[1mStep[0m  [189/213], [94mLoss[0m : 6.25515
[1mStep[0m  [210/213], [94mLoss[0m : 6.50363

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.132, [92mTest[0m: 6.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.18843
[1mStep[0m  [21/213], [94mLoss[0m : 5.79190
[1mStep[0m  [42/213], [94mLoss[0m : 6.56384
[1mStep[0m  [63/213], [94mLoss[0m : 6.06464
[1mStep[0m  [84/213], [94mLoss[0m : 4.95711
[1mStep[0m  [105/213], [94mLoss[0m : 5.83681
[1mStep[0m  [126/213], [94mLoss[0m : 5.35770
[1mStep[0m  [147/213], [94mLoss[0m : 5.30306
[1mStep[0m  [168/213], [94mLoss[0m : 5.03135
[1mStep[0m  [189/213], [94mLoss[0m : 5.29121
[1mStep[0m  [210/213], [94mLoss[0m : 6.61949

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.589, [92mTest[0m: 5.860, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.41297
[1mStep[0m  [21/213], [94mLoss[0m : 5.47561
[1mStep[0m  [42/213], [94mLoss[0m : 4.95090
[1mStep[0m  [63/213], [94mLoss[0m : 4.68051
[1mStep[0m  [84/213], [94mLoss[0m : 5.14020
[1mStep[0m  [105/213], [94mLoss[0m : 5.26515
[1mStep[0m  [126/213], [94mLoss[0m : 4.87823
[1mStep[0m  [147/213], [94mLoss[0m : 5.74732
[1mStep[0m  [168/213], [94mLoss[0m : 5.72043
[1mStep[0m  [189/213], [94mLoss[0m : 4.90725
[1mStep[0m  [210/213], [94mLoss[0m : 4.65573

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.100, [92mTest[0m: 5.302, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 4.64226
[1mStep[0m  [21/213], [94mLoss[0m : 5.41016
[1mStep[0m  [42/213], [94mLoss[0m : 4.39754
[1mStep[0m  [63/213], [94mLoss[0m : 4.38581
[1mStep[0m  [84/213], [94mLoss[0m : 4.20707
[1mStep[0m  [105/213], [94mLoss[0m : 4.19657
[1mStep[0m  [126/213], [94mLoss[0m : 5.08196
[1mStep[0m  [147/213], [94mLoss[0m : 4.54609
[1mStep[0m  [168/213], [94mLoss[0m : 4.20555
[1mStep[0m  [189/213], [94mLoss[0m : 5.04655
[1mStep[0m  [210/213], [94mLoss[0m : 4.18367

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.682, [92mTest[0m: 4.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.67989
[1mStep[0m  [21/213], [94mLoss[0m : 5.30262
[1mStep[0m  [42/213], [94mLoss[0m : 3.65219
[1mStep[0m  [63/213], [94mLoss[0m : 4.06088
[1mStep[0m  [84/213], [94mLoss[0m : 4.23780
[1mStep[0m  [105/213], [94mLoss[0m : 4.60483
[1mStep[0m  [126/213], [94mLoss[0m : 3.75506
[1mStep[0m  [147/213], [94mLoss[0m : 3.96558
[1mStep[0m  [168/213], [94mLoss[0m : 4.20397
[1mStep[0m  [189/213], [94mLoss[0m : 3.89807
[1mStep[0m  [210/213], [94mLoss[0m : 4.27344

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.314, [92mTest[0m: 4.401, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.68962
[1mStep[0m  [21/213], [94mLoss[0m : 3.89910
[1mStep[0m  [42/213], [94mLoss[0m : 4.69322
[1mStep[0m  [63/213], [94mLoss[0m : 3.79799
[1mStep[0m  [84/213], [94mLoss[0m : 4.57073
[1mStep[0m  [105/213], [94mLoss[0m : 3.69894
[1mStep[0m  [126/213], [94mLoss[0m : 4.88932
[1mStep[0m  [147/213], [94mLoss[0m : 3.40035
[1mStep[0m  [168/213], [94mLoss[0m : 4.18584
[1mStep[0m  [189/213], [94mLoss[0m : 3.00737
[1mStep[0m  [210/213], [94mLoss[0m : 3.86548

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.022, [92mTest[0m: 4.068, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.81040
[1mStep[0m  [21/213], [94mLoss[0m : 4.21187
[1mStep[0m  [42/213], [94mLoss[0m : 3.63624
[1mStep[0m  [63/213], [94mLoss[0m : 4.25173
[1mStep[0m  [84/213], [94mLoss[0m : 4.18649
[1mStep[0m  [105/213], [94mLoss[0m : 4.91719
[1mStep[0m  [126/213], [94mLoss[0m : 3.10293
[1mStep[0m  [147/213], [94mLoss[0m : 3.58861
[1mStep[0m  [168/213], [94mLoss[0m : 4.15608
[1mStep[0m  [189/213], [94mLoss[0m : 3.70852
[1mStep[0m  [210/213], [94mLoss[0m : 4.56040

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.788, [92mTest[0m: 3.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.30222
[1mStep[0m  [21/213], [94mLoss[0m : 3.54705
[1mStep[0m  [42/213], [94mLoss[0m : 3.80364
[1mStep[0m  [63/213], [94mLoss[0m : 3.53929
[1mStep[0m  [84/213], [94mLoss[0m : 3.35888
[1mStep[0m  [105/213], [94mLoss[0m : 3.81029
[1mStep[0m  [126/213], [94mLoss[0m : 3.59044
[1mStep[0m  [147/213], [94mLoss[0m : 3.54711
[1mStep[0m  [168/213], [94mLoss[0m : 3.60731
[1mStep[0m  [189/213], [94mLoss[0m : 3.85021
[1mStep[0m  [210/213], [94mLoss[0m : 3.73658

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.590, [92mTest[0m: 3.531, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.13222
[1mStep[0m  [21/213], [94mLoss[0m : 3.40595
[1mStep[0m  [42/213], [94mLoss[0m : 4.39470
[1mStep[0m  [63/213], [94mLoss[0m : 3.29423
[1mStep[0m  [84/213], [94mLoss[0m : 3.34713
[1mStep[0m  [105/213], [94mLoss[0m : 3.04759
[1mStep[0m  [126/213], [94mLoss[0m : 3.73376
[1mStep[0m  [147/213], [94mLoss[0m : 3.17398
[1mStep[0m  [168/213], [94mLoss[0m : 3.88856
[1mStep[0m  [189/213], [94mLoss[0m : 3.78638
[1mStep[0m  [210/213], [94mLoss[0m : 3.33544

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.437, [92mTest[0m: 3.358, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.71587
[1mStep[0m  [21/213], [94mLoss[0m : 3.83641
[1mStep[0m  [42/213], [94mLoss[0m : 3.74550
[1mStep[0m  [63/213], [94mLoss[0m : 3.40052
[1mStep[0m  [84/213], [94mLoss[0m : 4.17177
[1mStep[0m  [105/213], [94mLoss[0m : 3.30063
[1mStep[0m  [126/213], [94mLoss[0m : 3.40641
[1mStep[0m  [147/213], [94mLoss[0m : 3.19875
[1mStep[0m  [168/213], [94mLoss[0m : 3.87710
[1mStep[0m  [189/213], [94mLoss[0m : 3.06115
[1mStep[0m  [210/213], [94mLoss[0m : 3.20813

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.291, [92mTest[0m: 3.185, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.43097
[1mStep[0m  [21/213], [94mLoss[0m : 3.68513
[1mStep[0m  [42/213], [94mLoss[0m : 3.10643
[1mStep[0m  [63/213], [94mLoss[0m : 2.91464
[1mStep[0m  [84/213], [94mLoss[0m : 2.96733
[1mStep[0m  [105/213], [94mLoss[0m : 3.63917
[1mStep[0m  [126/213], [94mLoss[0m : 3.07594
[1mStep[0m  [147/213], [94mLoss[0m : 3.17625
[1mStep[0m  [168/213], [94mLoss[0m : 2.95774
[1mStep[0m  [189/213], [94mLoss[0m : 3.43248
[1mStep[0m  [210/213], [94mLoss[0m : 3.17131

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.207, [92mTest[0m: 3.045, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.49179
[1mStep[0m  [21/213], [94mLoss[0m : 2.93429
[1mStep[0m  [42/213], [94mLoss[0m : 3.27462
[1mStep[0m  [63/213], [94mLoss[0m : 3.41357
[1mStep[0m  [84/213], [94mLoss[0m : 2.87098
[1mStep[0m  [105/213], [94mLoss[0m : 3.02662
[1mStep[0m  [126/213], [94mLoss[0m : 3.22379
[1mStep[0m  [147/213], [94mLoss[0m : 3.41427
[1mStep[0m  [168/213], [94mLoss[0m : 3.06085
[1mStep[0m  [189/213], [94mLoss[0m : 3.02175
[1mStep[0m  [210/213], [94mLoss[0m : 2.97230

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.129, [92mTest[0m: 2.961, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.02342
[1mStep[0m  [21/213], [94mLoss[0m : 2.94881
[1mStep[0m  [42/213], [94mLoss[0m : 3.55685
[1mStep[0m  [63/213], [94mLoss[0m : 3.49809
[1mStep[0m  [84/213], [94mLoss[0m : 3.34650
[1mStep[0m  [105/213], [94mLoss[0m : 3.36070
[1mStep[0m  [126/213], [94mLoss[0m : 2.94364
[1mStep[0m  [147/213], [94mLoss[0m : 3.00001
[1mStep[0m  [168/213], [94mLoss[0m : 3.32167
[1mStep[0m  [189/213], [94mLoss[0m : 2.99835
[1mStep[0m  [210/213], [94mLoss[0m : 2.83999

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.059, [92mTest[0m: 2.886, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.23947
[1mStep[0m  [21/213], [94mLoss[0m : 3.01721
[1mStep[0m  [42/213], [94mLoss[0m : 2.99987
[1mStep[0m  [63/213], [94mLoss[0m : 3.16706
[1mStep[0m  [84/213], [94mLoss[0m : 2.91120
[1mStep[0m  [105/213], [94mLoss[0m : 2.74272
[1mStep[0m  [126/213], [94mLoss[0m : 2.78704
[1mStep[0m  [147/213], [94mLoss[0m : 2.98901
[1mStep[0m  [168/213], [94mLoss[0m : 2.59989
[1mStep[0m  [189/213], [94mLoss[0m : 3.11814
[1mStep[0m  [210/213], [94mLoss[0m : 3.61880

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.006, [92mTest[0m: 2.812, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.80759
[1mStep[0m  [21/213], [94mLoss[0m : 2.88484
[1mStep[0m  [42/213], [94mLoss[0m : 3.05152
[1mStep[0m  [63/213], [94mLoss[0m : 3.30682
[1mStep[0m  [84/213], [94mLoss[0m : 2.96393
[1mStep[0m  [105/213], [94mLoss[0m : 2.99207
[1mStep[0m  [126/213], [94mLoss[0m : 2.66473
[1mStep[0m  [147/213], [94mLoss[0m : 3.27012
[1mStep[0m  [168/213], [94mLoss[0m : 2.99997
[1mStep[0m  [189/213], [94mLoss[0m : 2.58718
[1mStep[0m  [210/213], [94mLoss[0m : 3.51941

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.951, [92mTest[0m: 2.741, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45305
[1mStep[0m  [21/213], [94mLoss[0m : 2.68134
[1mStep[0m  [42/213], [94mLoss[0m : 2.79734
[1mStep[0m  [63/213], [94mLoss[0m : 2.61806
[1mStep[0m  [84/213], [94mLoss[0m : 2.84873
[1mStep[0m  [105/213], [94mLoss[0m : 2.66300
[1mStep[0m  [126/213], [94mLoss[0m : 3.06612
[1mStep[0m  [147/213], [94mLoss[0m : 2.82028
[1mStep[0m  [168/213], [94mLoss[0m : 3.33521
[1mStep[0m  [189/213], [94mLoss[0m : 2.36475
[1mStep[0m  [210/213], [94mLoss[0m : 3.16572

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.913, [92mTest[0m: 2.708, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.64225
[1mStep[0m  [21/213], [94mLoss[0m : 2.75800
[1mStep[0m  [42/213], [94mLoss[0m : 3.21115
[1mStep[0m  [63/213], [94mLoss[0m : 2.64573
[1mStep[0m  [84/213], [94mLoss[0m : 2.83675
[1mStep[0m  [105/213], [94mLoss[0m : 3.32055
[1mStep[0m  [126/213], [94mLoss[0m : 2.29781
[1mStep[0m  [147/213], [94mLoss[0m : 2.76421
[1mStep[0m  [168/213], [94mLoss[0m : 2.96289
[1mStep[0m  [189/213], [94mLoss[0m : 2.59382
[1mStep[0m  [210/213], [94mLoss[0m : 2.54063

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.882, [92mTest[0m: 2.674, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.69555
[1mStep[0m  [21/213], [94mLoss[0m : 2.43408
[1mStep[0m  [42/213], [94mLoss[0m : 2.41032
[1mStep[0m  [63/213], [94mLoss[0m : 3.30927
[1mStep[0m  [84/213], [94mLoss[0m : 2.38035
[1mStep[0m  [105/213], [94mLoss[0m : 3.33440
[1mStep[0m  [126/213], [94mLoss[0m : 2.38571
[1mStep[0m  [147/213], [94mLoss[0m : 2.59502
[1mStep[0m  [168/213], [94mLoss[0m : 2.98579
[1mStep[0m  [189/213], [94mLoss[0m : 2.39421
[1mStep[0m  [210/213], [94mLoss[0m : 2.52050

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.840, [92mTest[0m: 2.630, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.05388
[1mStep[0m  [21/213], [94mLoss[0m : 2.65398
[1mStep[0m  [42/213], [94mLoss[0m : 2.47556
[1mStep[0m  [63/213], [94mLoss[0m : 2.65043
[1mStep[0m  [84/213], [94mLoss[0m : 2.92963
[1mStep[0m  [105/213], [94mLoss[0m : 3.53264
[1mStep[0m  [126/213], [94mLoss[0m : 2.65404
[1mStep[0m  [147/213], [94mLoss[0m : 2.49379
[1mStep[0m  [168/213], [94mLoss[0m : 2.89752
[1mStep[0m  [189/213], [94mLoss[0m : 3.35507
[1mStep[0m  [210/213], [94mLoss[0m : 3.21210

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.833, [92mTest[0m: 2.600, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52313
[1mStep[0m  [21/213], [94mLoss[0m : 2.73705
[1mStep[0m  [42/213], [94mLoss[0m : 2.46679
[1mStep[0m  [63/213], [94mLoss[0m : 3.24536
[1mStep[0m  [84/213], [94mLoss[0m : 2.35372
[1mStep[0m  [105/213], [94mLoss[0m : 2.44093
[1mStep[0m  [126/213], [94mLoss[0m : 2.56787
[1mStep[0m  [147/213], [94mLoss[0m : 2.76609
[1mStep[0m  [168/213], [94mLoss[0m : 2.81081
[1mStep[0m  [189/213], [94mLoss[0m : 2.43179
[1mStep[0m  [210/213], [94mLoss[0m : 2.97890

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.819, [92mTest[0m: 2.571, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38608
[1mStep[0m  [21/213], [94mLoss[0m : 2.13624
[1mStep[0m  [42/213], [94mLoss[0m : 2.97727
[1mStep[0m  [63/213], [94mLoss[0m : 2.68555
[1mStep[0m  [84/213], [94mLoss[0m : 2.78761
[1mStep[0m  [105/213], [94mLoss[0m : 2.49125
[1mStep[0m  [126/213], [94mLoss[0m : 2.51766
[1mStep[0m  [147/213], [94mLoss[0m : 3.44113
[1mStep[0m  [168/213], [94mLoss[0m : 2.91213
[1mStep[0m  [189/213], [94mLoss[0m : 2.54524
[1mStep[0m  [210/213], [94mLoss[0m : 2.73701

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.793, [92mTest[0m: 2.567, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.546
====================================

Phase 1 - Evaluation MAE:  2.545601839164518
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 3.35545
[1mStep[0m  [21/213], [94mLoss[0m : 2.69506
[1mStep[0m  [42/213], [94mLoss[0m : 2.14383
[1mStep[0m  [63/213], [94mLoss[0m : 2.32105
[1mStep[0m  [84/213], [94mLoss[0m : 2.84292
[1mStep[0m  [105/213], [94mLoss[0m : 3.09507
[1mStep[0m  [126/213], [94mLoss[0m : 2.65905
[1mStep[0m  [147/213], [94mLoss[0m : 2.59633
[1mStep[0m  [168/213], [94mLoss[0m : 2.81207
[1mStep[0m  [189/213], [94mLoss[0m : 2.33954
[1mStep[0m  [210/213], [94mLoss[0m : 2.48414

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.790, [92mTest[0m: 2.541, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.17037
[1mStep[0m  [21/213], [94mLoss[0m : 2.81679
[1mStep[0m  [42/213], [94mLoss[0m : 2.51215
[1mStep[0m  [63/213], [94mLoss[0m : 3.56598
[1mStep[0m  [84/213], [94mLoss[0m : 2.26119
[1mStep[0m  [105/213], [94mLoss[0m : 2.95141
[1mStep[0m  [126/213], [94mLoss[0m : 2.54041
[1mStep[0m  [147/213], [94mLoss[0m : 2.50226
[1mStep[0m  [168/213], [94mLoss[0m : 2.93859
[1mStep[0m  [189/213], [94mLoss[0m : 2.67173
[1mStep[0m  [210/213], [94mLoss[0m : 2.49354

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.751, [92mTest[0m: 2.530, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.83225
[1mStep[0m  [21/213], [94mLoss[0m : 2.60853
[1mStep[0m  [42/213], [94mLoss[0m : 2.67712
[1mStep[0m  [63/213], [94mLoss[0m : 2.76652
[1mStep[0m  [84/213], [94mLoss[0m : 2.48971
[1mStep[0m  [105/213], [94mLoss[0m : 3.09917
[1mStep[0m  [126/213], [94mLoss[0m : 2.74795
[1mStep[0m  [147/213], [94mLoss[0m : 3.00108
[1mStep[0m  [168/213], [94mLoss[0m : 2.96853
[1mStep[0m  [189/213], [94mLoss[0m : 2.64380
[1mStep[0m  [210/213], [94mLoss[0m : 2.76101

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.754, [92mTest[0m: 2.505, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.60721
[1mStep[0m  [21/213], [94mLoss[0m : 2.67389
[1mStep[0m  [42/213], [94mLoss[0m : 2.49862
[1mStep[0m  [63/213], [94mLoss[0m : 2.40885
[1mStep[0m  [84/213], [94mLoss[0m : 2.57362
[1mStep[0m  [105/213], [94mLoss[0m : 2.86572
[1mStep[0m  [126/213], [94mLoss[0m : 2.23644
[1mStep[0m  [147/213], [94mLoss[0m : 2.48502
[1mStep[0m  [168/213], [94mLoss[0m : 2.20428
[1mStep[0m  [189/213], [94mLoss[0m : 2.98556
[1mStep[0m  [210/213], [94mLoss[0m : 2.79727

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.498, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.71867
[1mStep[0m  [21/213], [94mLoss[0m : 2.40206
[1mStep[0m  [42/213], [94mLoss[0m : 3.15937
[1mStep[0m  [63/213], [94mLoss[0m : 2.81889
[1mStep[0m  [84/213], [94mLoss[0m : 2.80848
[1mStep[0m  [105/213], [94mLoss[0m : 2.64963
[1mStep[0m  [126/213], [94mLoss[0m : 3.43583
[1mStep[0m  [147/213], [94mLoss[0m : 2.31960
[1mStep[0m  [168/213], [94mLoss[0m : 2.70362
[1mStep[0m  [189/213], [94mLoss[0m : 2.82756
[1mStep[0m  [210/213], [94mLoss[0m : 2.64533

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.721, [92mTest[0m: 2.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.87525
[1mStep[0m  [21/213], [94mLoss[0m : 2.46879
[1mStep[0m  [42/213], [94mLoss[0m : 2.99243
[1mStep[0m  [63/213], [94mLoss[0m : 2.63539
[1mStep[0m  [84/213], [94mLoss[0m : 3.11407
[1mStep[0m  [105/213], [94mLoss[0m : 2.31922
[1mStep[0m  [126/213], [94mLoss[0m : 2.73441
[1mStep[0m  [147/213], [94mLoss[0m : 2.88406
[1mStep[0m  [168/213], [94mLoss[0m : 2.95812
[1mStep[0m  [189/213], [94mLoss[0m : 2.39427
[1mStep[0m  [210/213], [94mLoss[0m : 2.43986

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56882
[1mStep[0m  [21/213], [94mLoss[0m : 2.54595
[1mStep[0m  [42/213], [94mLoss[0m : 3.28603
[1mStep[0m  [63/213], [94mLoss[0m : 2.58645
[1mStep[0m  [84/213], [94mLoss[0m : 2.61568
[1mStep[0m  [105/213], [94mLoss[0m : 3.02253
[1mStep[0m  [126/213], [94mLoss[0m : 2.63123
[1mStep[0m  [147/213], [94mLoss[0m : 3.00556
[1mStep[0m  [168/213], [94mLoss[0m : 2.64893
[1mStep[0m  [189/213], [94mLoss[0m : 2.57705
[1mStep[0m  [210/213], [94mLoss[0m : 2.65450

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22216
[1mStep[0m  [21/213], [94mLoss[0m : 2.60669
[1mStep[0m  [42/213], [94mLoss[0m : 2.60484
[1mStep[0m  [63/213], [94mLoss[0m : 2.36485
[1mStep[0m  [84/213], [94mLoss[0m : 3.22160
[1mStep[0m  [105/213], [94mLoss[0m : 2.41388
[1mStep[0m  [126/213], [94mLoss[0m : 2.54435
[1mStep[0m  [147/213], [94mLoss[0m : 1.93804
[1mStep[0m  [168/213], [94mLoss[0m : 3.04981
[1mStep[0m  [189/213], [94mLoss[0m : 2.90113
[1mStep[0m  [210/213], [94mLoss[0m : 2.97389

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.60565
[1mStep[0m  [21/213], [94mLoss[0m : 2.88332
[1mStep[0m  [42/213], [94mLoss[0m : 2.55095
[1mStep[0m  [63/213], [94mLoss[0m : 2.52088
[1mStep[0m  [84/213], [94mLoss[0m : 2.84646
[1mStep[0m  [105/213], [94mLoss[0m : 2.73766
[1mStep[0m  [126/213], [94mLoss[0m : 2.48776
[1mStep[0m  [147/213], [94mLoss[0m : 3.04854
[1mStep[0m  [168/213], [94mLoss[0m : 3.23443
[1mStep[0m  [189/213], [94mLoss[0m : 2.49155
[1mStep[0m  [210/213], [94mLoss[0m : 2.89302

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.02878
[1mStep[0m  [21/213], [94mLoss[0m : 3.06764
[1mStep[0m  [42/213], [94mLoss[0m : 2.48595
[1mStep[0m  [63/213], [94mLoss[0m : 2.38421
[1mStep[0m  [84/213], [94mLoss[0m : 2.56557
[1mStep[0m  [105/213], [94mLoss[0m : 2.75982
[1mStep[0m  [126/213], [94mLoss[0m : 2.47986
[1mStep[0m  [147/213], [94mLoss[0m : 3.12772
[1mStep[0m  [168/213], [94mLoss[0m : 2.57476
[1mStep[0m  [189/213], [94mLoss[0m : 2.72227
[1mStep[0m  [210/213], [94mLoss[0m : 2.63008

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.477, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.75132
[1mStep[0m  [21/213], [94mLoss[0m : 2.10392
[1mStep[0m  [42/213], [94mLoss[0m : 2.46296
[1mStep[0m  [63/213], [94mLoss[0m : 2.41775
[1mStep[0m  [84/213], [94mLoss[0m : 2.85092
[1mStep[0m  [105/213], [94mLoss[0m : 2.23603
[1mStep[0m  [126/213], [94mLoss[0m : 2.82603
[1mStep[0m  [147/213], [94mLoss[0m : 2.82050
[1mStep[0m  [168/213], [94mLoss[0m : 2.71401
[1mStep[0m  [189/213], [94mLoss[0m : 2.90950
[1mStep[0m  [210/213], [94mLoss[0m : 2.88464

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.697, [92mTest[0m: 2.468, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.16374
[1mStep[0m  [21/213], [94mLoss[0m : 2.59150
[1mStep[0m  [42/213], [94mLoss[0m : 2.86326
[1mStep[0m  [63/213], [94mLoss[0m : 3.20874
[1mStep[0m  [84/213], [94mLoss[0m : 2.84675
[1mStep[0m  [105/213], [94mLoss[0m : 2.79761
[1mStep[0m  [126/213], [94mLoss[0m : 2.74818
[1mStep[0m  [147/213], [94mLoss[0m : 2.86540
[1mStep[0m  [168/213], [94mLoss[0m : 2.95845
[1mStep[0m  [189/213], [94mLoss[0m : 2.12466
[1mStep[0m  [210/213], [94mLoss[0m : 2.32004

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.479, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35972
[1mStep[0m  [21/213], [94mLoss[0m : 2.55902
[1mStep[0m  [42/213], [94mLoss[0m : 2.57392
[1mStep[0m  [63/213], [94mLoss[0m : 2.43982
[1mStep[0m  [84/213], [94mLoss[0m : 2.62001
[1mStep[0m  [105/213], [94mLoss[0m : 2.65716
[1mStep[0m  [126/213], [94mLoss[0m : 2.69401
[1mStep[0m  [147/213], [94mLoss[0m : 2.65737
[1mStep[0m  [168/213], [94mLoss[0m : 2.94687
[1mStep[0m  [189/213], [94mLoss[0m : 2.95583
[1mStep[0m  [210/213], [94mLoss[0m : 2.31807

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.468, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58775
[1mStep[0m  [21/213], [94mLoss[0m : 2.42239
[1mStep[0m  [42/213], [94mLoss[0m : 2.64933
[1mStep[0m  [63/213], [94mLoss[0m : 3.07866
[1mStep[0m  [84/213], [94mLoss[0m : 2.68745
[1mStep[0m  [105/213], [94mLoss[0m : 2.61681
[1mStep[0m  [126/213], [94mLoss[0m : 2.77272
[1mStep[0m  [147/213], [94mLoss[0m : 2.59499
[1mStep[0m  [168/213], [94mLoss[0m : 2.86455
[1mStep[0m  [189/213], [94mLoss[0m : 2.64233
[1mStep[0m  [210/213], [94mLoss[0m : 2.81744

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.681, [92mTest[0m: 2.476, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46129
[1mStep[0m  [21/213], [94mLoss[0m : 2.88109
[1mStep[0m  [42/213], [94mLoss[0m : 2.85783
[1mStep[0m  [63/213], [94mLoss[0m : 2.77711
[1mStep[0m  [84/213], [94mLoss[0m : 2.21415
[1mStep[0m  [105/213], [94mLoss[0m : 2.70129
[1mStep[0m  [126/213], [94mLoss[0m : 2.76037
[1mStep[0m  [147/213], [94mLoss[0m : 2.89076
[1mStep[0m  [168/213], [94mLoss[0m : 2.51788
[1mStep[0m  [189/213], [94mLoss[0m : 3.04728
[1mStep[0m  [210/213], [94mLoss[0m : 2.97264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.03990
[1mStep[0m  [21/213], [94mLoss[0m : 2.69244
[1mStep[0m  [42/213], [94mLoss[0m : 2.36267
[1mStep[0m  [63/213], [94mLoss[0m : 3.44590
[1mStep[0m  [84/213], [94mLoss[0m : 2.79634
[1mStep[0m  [105/213], [94mLoss[0m : 2.73595
[1mStep[0m  [126/213], [94mLoss[0m : 2.18389
[1mStep[0m  [147/213], [94mLoss[0m : 2.53131
[1mStep[0m  [168/213], [94mLoss[0m : 2.83695
[1mStep[0m  [189/213], [94mLoss[0m : 2.88427
[1mStep[0m  [210/213], [94mLoss[0m : 2.14883

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.462, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.67978
[1mStep[0m  [21/213], [94mLoss[0m : 2.94130
[1mStep[0m  [42/213], [94mLoss[0m : 2.60405
[1mStep[0m  [63/213], [94mLoss[0m : 2.64812
[1mStep[0m  [84/213], [94mLoss[0m : 3.05009
[1mStep[0m  [105/213], [94mLoss[0m : 2.60391
[1mStep[0m  [126/213], [94mLoss[0m : 2.25929
[1mStep[0m  [147/213], [94mLoss[0m : 2.73220
[1mStep[0m  [168/213], [94mLoss[0m : 2.97546
[1mStep[0m  [189/213], [94mLoss[0m : 2.81173
[1mStep[0m  [210/213], [94mLoss[0m : 2.79165

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.79739
[1mStep[0m  [21/213], [94mLoss[0m : 2.73695
[1mStep[0m  [42/213], [94mLoss[0m : 2.56045
[1mStep[0m  [63/213], [94mLoss[0m : 2.58594
[1mStep[0m  [84/213], [94mLoss[0m : 2.30527
[1mStep[0m  [105/213], [94mLoss[0m : 2.87763
[1mStep[0m  [126/213], [94mLoss[0m : 2.69189
[1mStep[0m  [147/213], [94mLoss[0m : 2.68229
[1mStep[0m  [168/213], [94mLoss[0m : 2.37821
[1mStep[0m  [189/213], [94mLoss[0m : 2.82835
[1mStep[0m  [210/213], [94mLoss[0m : 2.71270

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.681, [92mTest[0m: 2.467, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63807
[1mStep[0m  [21/213], [94mLoss[0m : 3.06756
[1mStep[0m  [42/213], [94mLoss[0m : 2.88544
[1mStep[0m  [63/213], [94mLoss[0m : 2.90799
[1mStep[0m  [84/213], [94mLoss[0m : 2.90353
[1mStep[0m  [105/213], [94mLoss[0m : 2.57400
[1mStep[0m  [126/213], [94mLoss[0m : 2.88460
[1mStep[0m  [147/213], [94mLoss[0m : 3.04396
[1mStep[0m  [168/213], [94mLoss[0m : 2.82856
[1mStep[0m  [189/213], [94mLoss[0m : 2.75757
[1mStep[0m  [210/213], [94mLoss[0m : 2.32257

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.462, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.86858
[1mStep[0m  [21/213], [94mLoss[0m : 2.67146
[1mStep[0m  [42/213], [94mLoss[0m : 2.78666
[1mStep[0m  [63/213], [94mLoss[0m : 2.60327
[1mStep[0m  [84/213], [94mLoss[0m : 2.25090
[1mStep[0m  [105/213], [94mLoss[0m : 2.59728
[1mStep[0m  [126/213], [94mLoss[0m : 2.42791
[1mStep[0m  [147/213], [94mLoss[0m : 2.71597
[1mStep[0m  [168/213], [94mLoss[0m : 2.61763
[1mStep[0m  [189/213], [94mLoss[0m : 2.70734
[1mStep[0m  [210/213], [94mLoss[0m : 2.53560

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.476, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.77285
[1mStep[0m  [21/213], [94mLoss[0m : 2.50631
[1mStep[0m  [42/213], [94mLoss[0m : 2.51130
[1mStep[0m  [63/213], [94mLoss[0m : 2.56777
[1mStep[0m  [84/213], [94mLoss[0m : 2.23677
[1mStep[0m  [105/213], [94mLoss[0m : 2.31844
[1mStep[0m  [126/213], [94mLoss[0m : 2.59896
[1mStep[0m  [147/213], [94mLoss[0m : 2.56683
[1mStep[0m  [168/213], [94mLoss[0m : 2.43738
[1mStep[0m  [189/213], [94mLoss[0m : 2.35848
[1mStep[0m  [210/213], [94mLoss[0m : 2.80377

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.459, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61676
[1mStep[0m  [21/213], [94mLoss[0m : 2.57056
[1mStep[0m  [42/213], [94mLoss[0m : 2.96897
[1mStep[0m  [63/213], [94mLoss[0m : 2.62413
[1mStep[0m  [84/213], [94mLoss[0m : 2.36205
[1mStep[0m  [105/213], [94mLoss[0m : 2.94938
[1mStep[0m  [126/213], [94mLoss[0m : 2.75040
[1mStep[0m  [147/213], [94mLoss[0m : 2.79784
[1mStep[0m  [168/213], [94mLoss[0m : 3.07896
[1mStep[0m  [189/213], [94mLoss[0m : 2.81596
[1mStep[0m  [210/213], [94mLoss[0m : 2.94464

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.452, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56893
[1mStep[0m  [21/213], [94mLoss[0m : 2.85482
[1mStep[0m  [42/213], [94mLoss[0m : 2.66183
[1mStep[0m  [63/213], [94mLoss[0m : 2.79991
[1mStep[0m  [84/213], [94mLoss[0m : 2.91193
[1mStep[0m  [105/213], [94mLoss[0m : 2.80499
[1mStep[0m  [126/213], [94mLoss[0m : 2.81767
[1mStep[0m  [147/213], [94mLoss[0m : 2.79292
[1mStep[0m  [168/213], [94mLoss[0m : 2.97351
[1mStep[0m  [189/213], [94mLoss[0m : 2.30964
[1mStep[0m  [210/213], [94mLoss[0m : 2.32540

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.460, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65779
[1mStep[0m  [21/213], [94mLoss[0m : 2.52739
[1mStep[0m  [42/213], [94mLoss[0m : 2.47457
[1mStep[0m  [63/213], [94mLoss[0m : 2.57114
[1mStep[0m  [84/213], [94mLoss[0m : 2.50701
[1mStep[0m  [105/213], [94mLoss[0m : 1.96938
[1mStep[0m  [126/213], [94mLoss[0m : 2.94410
[1mStep[0m  [147/213], [94mLoss[0m : 2.83023
[1mStep[0m  [168/213], [94mLoss[0m : 2.67652
[1mStep[0m  [189/213], [94mLoss[0m : 2.45395
[1mStep[0m  [210/213], [94mLoss[0m : 2.74879

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.49279
[1mStep[0m  [21/213], [94mLoss[0m : 2.69017
[1mStep[0m  [42/213], [94mLoss[0m : 2.57938
[1mStep[0m  [63/213], [94mLoss[0m : 2.65912
[1mStep[0m  [84/213], [94mLoss[0m : 3.11646
[1mStep[0m  [105/213], [94mLoss[0m : 2.45283
[1mStep[0m  [126/213], [94mLoss[0m : 2.24076
[1mStep[0m  [147/213], [94mLoss[0m : 2.67169
[1mStep[0m  [168/213], [94mLoss[0m : 2.62282
[1mStep[0m  [189/213], [94mLoss[0m : 2.72449
[1mStep[0m  [210/213], [94mLoss[0m : 2.68386

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.467, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14150
[1mStep[0m  [21/213], [94mLoss[0m : 2.86645
[1mStep[0m  [42/213], [94mLoss[0m : 2.58320
[1mStep[0m  [63/213], [94mLoss[0m : 2.90513
[1mStep[0m  [84/213], [94mLoss[0m : 2.36858
[1mStep[0m  [105/213], [94mLoss[0m : 2.83756
[1mStep[0m  [126/213], [94mLoss[0m : 2.53540
[1mStep[0m  [147/213], [94mLoss[0m : 2.80089
[1mStep[0m  [168/213], [94mLoss[0m : 2.99147
[1mStep[0m  [189/213], [94mLoss[0m : 2.80600
[1mStep[0m  [210/213], [94mLoss[0m : 2.40773

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.465, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.93967
[1mStep[0m  [21/213], [94mLoss[0m : 2.61490
[1mStep[0m  [42/213], [94mLoss[0m : 2.75363
[1mStep[0m  [63/213], [94mLoss[0m : 2.73775
[1mStep[0m  [84/213], [94mLoss[0m : 2.84893
[1mStep[0m  [105/213], [94mLoss[0m : 2.49751
[1mStep[0m  [126/213], [94mLoss[0m : 3.14815
[1mStep[0m  [147/213], [94mLoss[0m : 3.00939
[1mStep[0m  [168/213], [94mLoss[0m : 2.31207
[1mStep[0m  [189/213], [94mLoss[0m : 2.69755
[1mStep[0m  [210/213], [94mLoss[0m : 2.95670

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.466, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.69964
[1mStep[0m  [21/213], [94mLoss[0m : 2.73700
[1mStep[0m  [42/213], [94mLoss[0m : 2.64036
[1mStep[0m  [63/213], [94mLoss[0m : 2.38542
[1mStep[0m  [84/213], [94mLoss[0m : 2.89330
[1mStep[0m  [105/213], [94mLoss[0m : 3.02285
[1mStep[0m  [126/213], [94mLoss[0m : 2.41832
[1mStep[0m  [147/213], [94mLoss[0m : 2.88827
[1mStep[0m  [168/213], [94mLoss[0m : 2.51435
[1mStep[0m  [189/213], [94mLoss[0m : 2.25443
[1mStep[0m  [210/213], [94mLoss[0m : 2.74336

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23091
[1mStep[0m  [21/213], [94mLoss[0m : 3.07604
[1mStep[0m  [42/213], [94mLoss[0m : 3.17699
[1mStep[0m  [63/213], [94mLoss[0m : 3.06096
[1mStep[0m  [84/213], [94mLoss[0m : 2.84365
[1mStep[0m  [105/213], [94mLoss[0m : 3.08099
[1mStep[0m  [126/213], [94mLoss[0m : 2.34501
[1mStep[0m  [147/213], [94mLoss[0m : 3.09709
[1mStep[0m  [168/213], [94mLoss[0m : 3.16896
[1mStep[0m  [189/213], [94mLoss[0m : 2.77410
[1mStep[0m  [210/213], [94mLoss[0m : 2.62227

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.466, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35133
[1mStep[0m  [21/213], [94mLoss[0m : 2.95545
[1mStep[0m  [42/213], [94mLoss[0m : 3.37904
[1mStep[0m  [63/213], [94mLoss[0m : 2.47199
[1mStep[0m  [84/213], [94mLoss[0m : 2.65891
[1mStep[0m  [105/213], [94mLoss[0m : 2.39446
[1mStep[0m  [126/213], [94mLoss[0m : 2.47814
[1mStep[0m  [147/213], [94mLoss[0m : 2.48789
[1mStep[0m  [168/213], [94mLoss[0m : 2.68035
[1mStep[0m  [189/213], [94mLoss[0m : 3.33125
[1mStep[0m  [210/213], [94mLoss[0m : 2.61062

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.450
====================================

Phase 2 - Evaluation MAE:  2.4503437393116503
MAE score P1       2.545602
MAE score P2       2.450344
loss               2.641443
learning_rate        0.0001
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay          0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 10.70687
[1mStep[0m  [10/106], [94mLoss[0m : 11.12075
[1mStep[0m  [20/106], [94mLoss[0m : 10.50620
[1mStep[0m  [30/106], [94mLoss[0m : 10.39111
[1mStep[0m  [40/106], [94mLoss[0m : 10.32497
[1mStep[0m  [50/106], [94mLoss[0m : 9.72219
[1mStep[0m  [60/106], [94mLoss[0m : 10.17469
[1mStep[0m  [70/106], [94mLoss[0m : 8.96171
[1mStep[0m  [80/106], [94mLoss[0m : 9.61220
[1mStep[0m  [90/106], [94mLoss[0m : 9.38375
[1mStep[0m  [100/106], [94mLoss[0m : 9.20218

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.022, [92mTest[0m: 10.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.43701
[1mStep[0m  [10/106], [94mLoss[0m : 8.93523
[1mStep[0m  [20/106], [94mLoss[0m : 9.03269
[1mStep[0m  [30/106], [94mLoss[0m : 8.99708
[1mStep[0m  [40/106], [94mLoss[0m : 8.91486
[1mStep[0m  [50/106], [94mLoss[0m : 8.98535
[1mStep[0m  [60/106], [94mLoss[0m : 8.19049
[1mStep[0m  [70/106], [94mLoss[0m : 7.91642
[1mStep[0m  [80/106], [94mLoss[0m : 8.39407
[1mStep[0m  [90/106], [94mLoss[0m : 7.80175
[1mStep[0m  [100/106], [94mLoss[0m : 7.52536

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.409, [92mTest[0m: 9.214, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.83699
[1mStep[0m  [10/106], [94mLoss[0m : 7.85208
[1mStep[0m  [20/106], [94mLoss[0m : 7.37826
[1mStep[0m  [30/106], [94mLoss[0m : 7.33691
[1mStep[0m  [40/106], [94mLoss[0m : 6.78674
[1mStep[0m  [50/106], [94mLoss[0m : 7.25893
[1mStep[0m  [60/106], [94mLoss[0m : 6.72479
[1mStep[0m  [70/106], [94mLoss[0m : 6.55877
[1mStep[0m  [80/106], [94mLoss[0m : 6.38278
[1mStep[0m  [90/106], [94mLoss[0m : 6.35287
[1mStep[0m  [100/106], [94mLoss[0m : 5.67712

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.790, [92mTest[0m: 7.593, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.18501
[1mStep[0m  [10/106], [94mLoss[0m : 5.68453
[1mStep[0m  [20/106], [94mLoss[0m : 5.91073
[1mStep[0m  [30/106], [94mLoss[0m : 6.30525
[1mStep[0m  [40/106], [94mLoss[0m : 5.44412
[1mStep[0m  [50/106], [94mLoss[0m : 4.47150
[1mStep[0m  [60/106], [94mLoss[0m : 5.18093
[1mStep[0m  [70/106], [94mLoss[0m : 4.89959
[1mStep[0m  [80/106], [94mLoss[0m : 5.18033
[1mStep[0m  [90/106], [94mLoss[0m : 4.15249
[1mStep[0m  [100/106], [94mLoss[0m : 4.87776

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.266, [92mTest[0m: 5.989, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.20207
[1mStep[0m  [10/106], [94mLoss[0m : 5.09786
[1mStep[0m  [20/106], [94mLoss[0m : 4.77222
[1mStep[0m  [30/106], [94mLoss[0m : 4.76427
[1mStep[0m  [40/106], [94mLoss[0m : 4.11675
[1mStep[0m  [50/106], [94mLoss[0m : 4.61455
[1mStep[0m  [60/106], [94mLoss[0m : 4.52897
[1mStep[0m  [70/106], [94mLoss[0m : 3.97216
[1mStep[0m  [80/106], [94mLoss[0m : 4.30123
[1mStep[0m  [90/106], [94mLoss[0m : 3.67376
[1mStep[0m  [100/106], [94mLoss[0m : 3.57891

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.148, [92mTest[0m: 4.594, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.86547
[1mStep[0m  [10/106], [94mLoss[0m : 3.45153
[1mStep[0m  [20/106], [94mLoss[0m : 3.79520
[1mStep[0m  [30/106], [94mLoss[0m : 3.99470
[1mStep[0m  [40/106], [94mLoss[0m : 3.52264
[1mStep[0m  [50/106], [94mLoss[0m : 3.62864
[1mStep[0m  [60/106], [94mLoss[0m : 3.09743
[1mStep[0m  [70/106], [94mLoss[0m : 3.07436
[1mStep[0m  [80/106], [94mLoss[0m : 3.80587
[1mStep[0m  [90/106], [94mLoss[0m : 3.33622
[1mStep[0m  [100/106], [94mLoss[0m : 3.00579

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.522, [92mTest[0m: 3.737, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.52283
[1mStep[0m  [10/106], [94mLoss[0m : 3.28079
[1mStep[0m  [20/106], [94mLoss[0m : 3.17772
[1mStep[0m  [30/106], [94mLoss[0m : 3.11419
[1mStep[0m  [40/106], [94mLoss[0m : 3.23466
[1mStep[0m  [50/106], [94mLoss[0m : 3.15044
[1mStep[0m  [60/106], [94mLoss[0m : 3.34476
[1mStep[0m  [70/106], [94mLoss[0m : 3.07115
[1mStep[0m  [80/106], [94mLoss[0m : 2.98324
[1mStep[0m  [90/106], [94mLoss[0m : 3.03305
[1mStep[0m  [100/106], [94mLoss[0m : 3.05107

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.193, [92mTest[0m: 3.289, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.05825
[1mStep[0m  [10/106], [94mLoss[0m : 3.10554
[1mStep[0m  [20/106], [94mLoss[0m : 3.01134
[1mStep[0m  [30/106], [94mLoss[0m : 3.18733
[1mStep[0m  [40/106], [94mLoss[0m : 2.97316
[1mStep[0m  [50/106], [94mLoss[0m : 2.99022
[1mStep[0m  [60/106], [94mLoss[0m : 3.01750
[1mStep[0m  [70/106], [94mLoss[0m : 2.84005
[1mStep[0m  [80/106], [94mLoss[0m : 3.25782
[1mStep[0m  [90/106], [94mLoss[0m : 2.80826
[1mStep[0m  [100/106], [94mLoss[0m : 3.00889

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.033, [92mTest[0m: 3.059, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64908
[1mStep[0m  [10/106], [94mLoss[0m : 3.30956
[1mStep[0m  [20/106], [94mLoss[0m : 3.03837
[1mStep[0m  [30/106], [94mLoss[0m : 2.99752
[1mStep[0m  [40/106], [94mLoss[0m : 2.79470
[1mStep[0m  [50/106], [94mLoss[0m : 3.46743
[1mStep[0m  [60/106], [94mLoss[0m : 2.83281
[1mStep[0m  [70/106], [94mLoss[0m : 2.73979
[1mStep[0m  [80/106], [94mLoss[0m : 2.89126
[1mStep[0m  [90/106], [94mLoss[0m : 2.85760
[1mStep[0m  [100/106], [94mLoss[0m : 2.87152

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.934, [92mTest[0m: 2.932, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90103
[1mStep[0m  [10/106], [94mLoss[0m : 2.88273
[1mStep[0m  [20/106], [94mLoss[0m : 2.76637
[1mStep[0m  [30/106], [94mLoss[0m : 2.88659
[1mStep[0m  [40/106], [94mLoss[0m : 2.74282
[1mStep[0m  [50/106], [94mLoss[0m : 2.80006
[1mStep[0m  [60/106], [94mLoss[0m : 2.89010
[1mStep[0m  [70/106], [94mLoss[0m : 2.69571
[1mStep[0m  [80/106], [94mLoss[0m : 2.65629
[1mStep[0m  [90/106], [94mLoss[0m : 2.40576
[1mStep[0m  [100/106], [94mLoss[0m : 2.77563

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.877, [92mTest[0m: 2.853, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66830
[1mStep[0m  [10/106], [94mLoss[0m : 2.88096
[1mStep[0m  [20/106], [94mLoss[0m : 2.80709
[1mStep[0m  [30/106], [94mLoss[0m : 2.58869
[1mStep[0m  [40/106], [94mLoss[0m : 3.07045
[1mStep[0m  [50/106], [94mLoss[0m : 2.64977
[1mStep[0m  [60/106], [94mLoss[0m : 2.84234
[1mStep[0m  [70/106], [94mLoss[0m : 2.89363
[1mStep[0m  [80/106], [94mLoss[0m : 2.90551
[1mStep[0m  [90/106], [94mLoss[0m : 2.83862
[1mStep[0m  [100/106], [94mLoss[0m : 2.78272

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.821, [92mTest[0m: 2.804, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90913
[1mStep[0m  [10/106], [94mLoss[0m : 2.69605
[1mStep[0m  [20/106], [94mLoss[0m : 2.98344
[1mStep[0m  [30/106], [94mLoss[0m : 2.80407
[1mStep[0m  [40/106], [94mLoss[0m : 2.81158
[1mStep[0m  [50/106], [94mLoss[0m : 2.45331
[1mStep[0m  [60/106], [94mLoss[0m : 2.61236
[1mStep[0m  [70/106], [94mLoss[0m : 2.91909
[1mStep[0m  [80/106], [94mLoss[0m : 2.91470
[1mStep[0m  [90/106], [94mLoss[0m : 2.86929
[1mStep[0m  [100/106], [94mLoss[0m : 2.66311

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.797, [92mTest[0m: 2.760, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50627
[1mStep[0m  [10/106], [94mLoss[0m : 3.23448
[1mStep[0m  [20/106], [94mLoss[0m : 3.13188
[1mStep[0m  [30/106], [94mLoss[0m : 2.71857
[1mStep[0m  [40/106], [94mLoss[0m : 2.44348
[1mStep[0m  [50/106], [94mLoss[0m : 2.76764
[1mStep[0m  [60/106], [94mLoss[0m : 2.81614
[1mStep[0m  [70/106], [94mLoss[0m : 2.91798
[1mStep[0m  [80/106], [94mLoss[0m : 2.64972
[1mStep[0m  [90/106], [94mLoss[0m : 3.05448
[1mStep[0m  [100/106], [94mLoss[0m : 2.59275

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.767, [92mTest[0m: 2.716, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.79528
[1mStep[0m  [10/106], [94mLoss[0m : 2.89234
[1mStep[0m  [20/106], [94mLoss[0m : 3.14610
[1mStep[0m  [30/106], [94mLoss[0m : 2.70862
[1mStep[0m  [40/106], [94mLoss[0m : 2.84332
[1mStep[0m  [50/106], [94mLoss[0m : 3.17301
[1mStep[0m  [60/106], [94mLoss[0m : 2.97668
[1mStep[0m  [70/106], [94mLoss[0m : 2.73982
[1mStep[0m  [80/106], [94mLoss[0m : 2.54475
[1mStep[0m  [90/106], [94mLoss[0m : 2.52267
[1mStep[0m  [100/106], [94mLoss[0m : 2.88611

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.736, [92mTest[0m: 2.694, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94940
[1mStep[0m  [10/106], [94mLoss[0m : 2.81946
[1mStep[0m  [20/106], [94mLoss[0m : 2.76569
[1mStep[0m  [30/106], [94mLoss[0m : 2.69045
[1mStep[0m  [40/106], [94mLoss[0m : 2.58448
[1mStep[0m  [50/106], [94mLoss[0m : 2.95794
[1mStep[0m  [60/106], [94mLoss[0m : 2.76561
[1mStep[0m  [70/106], [94mLoss[0m : 2.51755
[1mStep[0m  [80/106], [94mLoss[0m : 2.68866
[1mStep[0m  [90/106], [94mLoss[0m : 2.35511
[1mStep[0m  [100/106], [94mLoss[0m : 2.65582

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50487
[1mStep[0m  [10/106], [94mLoss[0m : 2.86125
[1mStep[0m  [20/106], [94mLoss[0m : 2.69951
[1mStep[0m  [30/106], [94mLoss[0m : 3.15240
[1mStep[0m  [40/106], [94mLoss[0m : 2.49398
[1mStep[0m  [50/106], [94mLoss[0m : 2.66764
[1mStep[0m  [60/106], [94mLoss[0m : 2.86109
[1mStep[0m  [70/106], [94mLoss[0m : 2.56078
[1mStep[0m  [80/106], [94mLoss[0m : 2.86108
[1mStep[0m  [90/106], [94mLoss[0m : 2.68416
[1mStep[0m  [100/106], [94mLoss[0m : 2.42183

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.652, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94379
[1mStep[0m  [10/106], [94mLoss[0m : 2.51399
[1mStep[0m  [20/106], [94mLoss[0m : 2.52713
[1mStep[0m  [30/106], [94mLoss[0m : 2.70290
[1mStep[0m  [40/106], [94mLoss[0m : 2.77589
[1mStep[0m  [50/106], [94mLoss[0m : 2.31989
[1mStep[0m  [60/106], [94mLoss[0m : 2.29827
[1mStep[0m  [70/106], [94mLoss[0m : 2.70276
[1mStep[0m  [80/106], [94mLoss[0m : 2.66612
[1mStep[0m  [90/106], [94mLoss[0m : 2.72229
[1mStep[0m  [100/106], [94mLoss[0m : 2.57846

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84654
[1mStep[0m  [10/106], [94mLoss[0m : 2.55002
[1mStep[0m  [20/106], [94mLoss[0m : 2.38734
[1mStep[0m  [30/106], [94mLoss[0m : 2.65870
[1mStep[0m  [40/106], [94mLoss[0m : 2.61784
[1mStep[0m  [50/106], [94mLoss[0m : 2.83426
[1mStep[0m  [60/106], [94mLoss[0m : 2.91411
[1mStep[0m  [70/106], [94mLoss[0m : 2.93655
[1mStep[0m  [80/106], [94mLoss[0m : 2.59989
[1mStep[0m  [90/106], [94mLoss[0m : 2.41337
[1mStep[0m  [100/106], [94mLoss[0m : 2.68910

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29260
[1mStep[0m  [10/106], [94mLoss[0m : 3.02068
[1mStep[0m  [20/106], [94mLoss[0m : 2.51011
[1mStep[0m  [30/106], [94mLoss[0m : 2.69105
[1mStep[0m  [40/106], [94mLoss[0m : 2.76948
[1mStep[0m  [50/106], [94mLoss[0m : 2.60803
[1mStep[0m  [60/106], [94mLoss[0m : 3.05630
[1mStep[0m  [70/106], [94mLoss[0m : 2.71271
[1mStep[0m  [80/106], [94mLoss[0m : 2.24596
[1mStep[0m  [90/106], [94mLoss[0m : 2.79425
[1mStep[0m  [100/106], [94mLoss[0m : 2.66208

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.607, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70528
[1mStep[0m  [10/106], [94mLoss[0m : 2.62074
[1mStep[0m  [20/106], [94mLoss[0m : 2.58825
[1mStep[0m  [30/106], [94mLoss[0m : 2.58435
[1mStep[0m  [40/106], [94mLoss[0m : 2.83996
[1mStep[0m  [50/106], [94mLoss[0m : 2.54507
[1mStep[0m  [60/106], [94mLoss[0m : 2.62217
[1mStep[0m  [70/106], [94mLoss[0m : 2.29124
[1mStep[0m  [80/106], [94mLoss[0m : 2.54494
[1mStep[0m  [90/106], [94mLoss[0m : 2.65809
[1mStep[0m  [100/106], [94mLoss[0m : 2.94023

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.600, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.88137
[1mStep[0m  [10/106], [94mLoss[0m : 2.40515
[1mStep[0m  [20/106], [94mLoss[0m : 2.67056
[1mStep[0m  [30/106], [94mLoss[0m : 2.54483
[1mStep[0m  [40/106], [94mLoss[0m : 2.58840
[1mStep[0m  [50/106], [94mLoss[0m : 2.53492
[1mStep[0m  [60/106], [94mLoss[0m : 2.27740
[1mStep[0m  [70/106], [94mLoss[0m : 2.53621
[1mStep[0m  [80/106], [94mLoss[0m : 2.63358
[1mStep[0m  [90/106], [94mLoss[0m : 2.87669
[1mStep[0m  [100/106], [94mLoss[0m : 2.58012

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39252
[1mStep[0m  [10/106], [94mLoss[0m : 2.64224
[1mStep[0m  [20/106], [94mLoss[0m : 2.51348
[1mStep[0m  [30/106], [94mLoss[0m : 2.51179
[1mStep[0m  [40/106], [94mLoss[0m : 2.88593
[1mStep[0m  [50/106], [94mLoss[0m : 2.29293
[1mStep[0m  [60/106], [94mLoss[0m : 2.77907
[1mStep[0m  [70/106], [94mLoss[0m : 2.69741
[1mStep[0m  [80/106], [94mLoss[0m : 2.41573
[1mStep[0m  [90/106], [94mLoss[0m : 2.34985
[1mStep[0m  [100/106], [94mLoss[0m : 2.34684

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.584, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67444
[1mStep[0m  [10/106], [94mLoss[0m : 2.81850
[1mStep[0m  [20/106], [94mLoss[0m : 2.35292
[1mStep[0m  [30/106], [94mLoss[0m : 2.36373
[1mStep[0m  [40/106], [94mLoss[0m : 2.48160
[1mStep[0m  [50/106], [94mLoss[0m : 2.61377
[1mStep[0m  [60/106], [94mLoss[0m : 2.78828
[1mStep[0m  [70/106], [94mLoss[0m : 2.57895
[1mStep[0m  [80/106], [94mLoss[0m : 2.48221
[1mStep[0m  [90/106], [94mLoss[0m : 2.45507
[1mStep[0m  [100/106], [94mLoss[0m : 2.35033

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75166
[1mStep[0m  [10/106], [94mLoss[0m : 2.37605
[1mStep[0m  [20/106], [94mLoss[0m : 2.43579
[1mStep[0m  [30/106], [94mLoss[0m : 2.64784
[1mStep[0m  [40/106], [94mLoss[0m : 2.70480
[1mStep[0m  [50/106], [94mLoss[0m : 2.57496
[1mStep[0m  [60/106], [94mLoss[0m : 2.31132
[1mStep[0m  [70/106], [94mLoss[0m : 2.46518
[1mStep[0m  [80/106], [94mLoss[0m : 2.81155
[1mStep[0m  [90/106], [94mLoss[0m : 2.48380
[1mStep[0m  [100/106], [94mLoss[0m : 2.41749

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.570, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74099
[1mStep[0m  [10/106], [94mLoss[0m : 2.36879
[1mStep[0m  [20/106], [94mLoss[0m : 2.61491
[1mStep[0m  [30/106], [94mLoss[0m : 2.68127
[1mStep[0m  [40/106], [94mLoss[0m : 2.70241
[1mStep[0m  [50/106], [94mLoss[0m : 2.38294
[1mStep[0m  [60/106], [94mLoss[0m : 2.61298
[1mStep[0m  [70/106], [94mLoss[0m : 2.54394
[1mStep[0m  [80/106], [94mLoss[0m : 2.44903
[1mStep[0m  [90/106], [94mLoss[0m : 2.54745
[1mStep[0m  [100/106], [94mLoss[0m : 2.56280

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.563, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51492
[1mStep[0m  [10/106], [94mLoss[0m : 2.59311
[1mStep[0m  [20/106], [94mLoss[0m : 2.56462
[1mStep[0m  [30/106], [94mLoss[0m : 2.55399
[1mStep[0m  [40/106], [94mLoss[0m : 2.73481
[1mStep[0m  [50/106], [94mLoss[0m : 2.27234
[1mStep[0m  [60/106], [94mLoss[0m : 2.45325
[1mStep[0m  [70/106], [94mLoss[0m : 2.60711
[1mStep[0m  [80/106], [94mLoss[0m : 2.51401
[1mStep[0m  [90/106], [94mLoss[0m : 2.65734
[1mStep[0m  [100/106], [94mLoss[0m : 2.96087

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.559, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56867
[1mStep[0m  [10/106], [94mLoss[0m : 2.66218
[1mStep[0m  [20/106], [94mLoss[0m : 2.57422
[1mStep[0m  [30/106], [94mLoss[0m : 2.47111
[1mStep[0m  [40/106], [94mLoss[0m : 2.57031
[1mStep[0m  [50/106], [94mLoss[0m : 2.74061
[1mStep[0m  [60/106], [94mLoss[0m : 2.91050
[1mStep[0m  [70/106], [94mLoss[0m : 2.38611
[1mStep[0m  [80/106], [94mLoss[0m : 2.42420
[1mStep[0m  [90/106], [94mLoss[0m : 2.51632
[1mStep[0m  [100/106], [94mLoss[0m : 2.94902

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.555, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40198
[1mStep[0m  [10/106], [94mLoss[0m : 2.52672
[1mStep[0m  [20/106], [94mLoss[0m : 2.34093
[1mStep[0m  [30/106], [94mLoss[0m : 2.71287
[1mStep[0m  [40/106], [94mLoss[0m : 2.78945
[1mStep[0m  [50/106], [94mLoss[0m : 2.76309
[1mStep[0m  [60/106], [94mLoss[0m : 2.52134
[1mStep[0m  [70/106], [94mLoss[0m : 2.57763
[1mStep[0m  [80/106], [94mLoss[0m : 2.44941
[1mStep[0m  [90/106], [94mLoss[0m : 2.72194
[1mStep[0m  [100/106], [94mLoss[0m : 2.92121

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.545, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60889
[1mStep[0m  [10/106], [94mLoss[0m : 2.52651
[1mStep[0m  [20/106], [94mLoss[0m : 2.64116
[1mStep[0m  [30/106], [94mLoss[0m : 2.49341
[1mStep[0m  [40/106], [94mLoss[0m : 2.59507
[1mStep[0m  [50/106], [94mLoss[0m : 2.76424
[1mStep[0m  [60/106], [94mLoss[0m : 2.72465
[1mStep[0m  [70/106], [94mLoss[0m : 2.55743
[1mStep[0m  [80/106], [94mLoss[0m : 2.26051
[1mStep[0m  [90/106], [94mLoss[0m : 2.37581
[1mStep[0m  [100/106], [94mLoss[0m : 2.71496

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.544, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24579
[1mStep[0m  [10/106], [94mLoss[0m : 2.53910
[1mStep[0m  [20/106], [94mLoss[0m : 2.45491
[1mStep[0m  [30/106], [94mLoss[0m : 2.77251
[1mStep[0m  [40/106], [94mLoss[0m : 2.31131
[1mStep[0m  [50/106], [94mLoss[0m : 2.59855
[1mStep[0m  [60/106], [94mLoss[0m : 2.41546
[1mStep[0m  [70/106], [94mLoss[0m : 2.64949
[1mStep[0m  [80/106], [94mLoss[0m : 2.33043
[1mStep[0m  [90/106], [94mLoss[0m : 3.02128
[1mStep[0m  [100/106], [94mLoss[0m : 2.93407

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.540, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.536
====================================

Phase 1 - Evaluation MAE:  2.5361134331181363
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.46236
[1mStep[0m  [10/106], [94mLoss[0m : 2.69829
[1mStep[0m  [20/106], [94mLoss[0m : 2.84946
[1mStep[0m  [30/106], [94mLoss[0m : 2.60480
[1mStep[0m  [40/106], [94mLoss[0m : 2.58693
[1mStep[0m  [50/106], [94mLoss[0m : 2.54856
[1mStep[0m  [60/106], [94mLoss[0m : 2.53904
[1mStep[0m  [70/106], [94mLoss[0m : 2.63506
[1mStep[0m  [80/106], [94mLoss[0m : 2.75808
[1mStep[0m  [90/106], [94mLoss[0m : 2.69516
[1mStep[0m  [100/106], [94mLoss[0m : 2.57160

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.534, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51073
[1mStep[0m  [10/106], [94mLoss[0m : 2.71382
[1mStep[0m  [20/106], [94mLoss[0m : 2.46315
[1mStep[0m  [30/106], [94mLoss[0m : 2.73963
[1mStep[0m  [40/106], [94mLoss[0m : 2.37168
[1mStep[0m  [50/106], [94mLoss[0m : 2.38783
[1mStep[0m  [60/106], [94mLoss[0m : 2.71014
[1mStep[0m  [70/106], [94mLoss[0m : 2.23499
[1mStep[0m  [80/106], [94mLoss[0m : 2.55116
[1mStep[0m  [90/106], [94mLoss[0m : 2.53846
[1mStep[0m  [100/106], [94mLoss[0m : 2.51755

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.522, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66646
[1mStep[0m  [10/106], [94mLoss[0m : 2.48063
[1mStep[0m  [20/106], [94mLoss[0m : 2.59189
[1mStep[0m  [30/106], [94mLoss[0m : 2.24665
[1mStep[0m  [40/106], [94mLoss[0m : 2.58464
[1mStep[0m  [50/106], [94mLoss[0m : 2.52271
[1mStep[0m  [60/106], [94mLoss[0m : 2.73035
[1mStep[0m  [70/106], [94mLoss[0m : 2.72281
[1mStep[0m  [80/106], [94mLoss[0m : 2.52076
[1mStep[0m  [90/106], [94mLoss[0m : 2.91976
[1mStep[0m  [100/106], [94mLoss[0m : 2.54851

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.516, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47289
[1mStep[0m  [10/106], [94mLoss[0m : 2.46634
[1mStep[0m  [20/106], [94mLoss[0m : 2.57415
[1mStep[0m  [30/106], [94mLoss[0m : 2.43642
[1mStep[0m  [40/106], [94mLoss[0m : 2.77769
[1mStep[0m  [50/106], [94mLoss[0m : 2.40556
[1mStep[0m  [60/106], [94mLoss[0m : 2.37079
[1mStep[0m  [70/106], [94mLoss[0m : 2.62105
[1mStep[0m  [80/106], [94mLoss[0m : 2.54626
[1mStep[0m  [90/106], [94mLoss[0m : 2.29777
[1mStep[0m  [100/106], [94mLoss[0m : 2.51632

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.504, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34455
[1mStep[0m  [10/106], [94mLoss[0m : 2.69581
[1mStep[0m  [20/106], [94mLoss[0m : 2.87944
[1mStep[0m  [30/106], [94mLoss[0m : 2.45842
[1mStep[0m  [40/106], [94mLoss[0m : 2.46719
[1mStep[0m  [50/106], [94mLoss[0m : 2.38937
[1mStep[0m  [60/106], [94mLoss[0m : 2.37073
[1mStep[0m  [70/106], [94mLoss[0m : 2.62137
[1mStep[0m  [80/106], [94mLoss[0m : 2.50940
[1mStep[0m  [90/106], [94mLoss[0m : 2.79035
[1mStep[0m  [100/106], [94mLoss[0m : 2.62297

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.502, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69713
[1mStep[0m  [10/106], [94mLoss[0m : 2.50977
[1mStep[0m  [20/106], [94mLoss[0m : 2.40244
[1mStep[0m  [30/106], [94mLoss[0m : 2.50329
[1mStep[0m  [40/106], [94mLoss[0m : 2.74695
[1mStep[0m  [50/106], [94mLoss[0m : 2.55524
[1mStep[0m  [60/106], [94mLoss[0m : 2.44395
[1mStep[0m  [70/106], [94mLoss[0m : 2.68157
[1mStep[0m  [80/106], [94mLoss[0m : 2.54213
[1mStep[0m  [90/106], [94mLoss[0m : 2.40131
[1mStep[0m  [100/106], [94mLoss[0m : 2.68582

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.500, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58631
[1mStep[0m  [10/106], [94mLoss[0m : 2.30912
[1mStep[0m  [20/106], [94mLoss[0m : 2.47010
[1mStep[0m  [30/106], [94mLoss[0m : 2.56290
[1mStep[0m  [40/106], [94mLoss[0m : 2.51021
[1mStep[0m  [50/106], [94mLoss[0m : 2.63249
[1mStep[0m  [60/106], [94mLoss[0m : 2.50043
[1mStep[0m  [70/106], [94mLoss[0m : 2.71882
[1mStep[0m  [80/106], [94mLoss[0m : 2.27996
[1mStep[0m  [90/106], [94mLoss[0m : 2.27004
[1mStep[0m  [100/106], [94mLoss[0m : 2.68163

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.499, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55312
[1mStep[0m  [10/106], [94mLoss[0m : 2.18850
[1mStep[0m  [20/106], [94mLoss[0m : 2.43035
[1mStep[0m  [30/106], [94mLoss[0m : 2.36005
[1mStep[0m  [40/106], [94mLoss[0m : 2.32945
[1mStep[0m  [50/106], [94mLoss[0m : 2.64977
[1mStep[0m  [60/106], [94mLoss[0m : 2.56519
[1mStep[0m  [70/106], [94mLoss[0m : 2.62707
[1mStep[0m  [80/106], [94mLoss[0m : 2.46953
[1mStep[0m  [90/106], [94mLoss[0m : 2.48792
[1mStep[0m  [100/106], [94mLoss[0m : 2.53861

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.492, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58152
[1mStep[0m  [10/106], [94mLoss[0m : 2.50743
[1mStep[0m  [20/106], [94mLoss[0m : 2.52851
[1mStep[0m  [30/106], [94mLoss[0m : 2.58885
[1mStep[0m  [40/106], [94mLoss[0m : 2.49066
[1mStep[0m  [50/106], [94mLoss[0m : 2.38241
[1mStep[0m  [60/106], [94mLoss[0m : 2.72987
[1mStep[0m  [70/106], [94mLoss[0m : 2.36320
[1mStep[0m  [80/106], [94mLoss[0m : 2.53780
[1mStep[0m  [90/106], [94mLoss[0m : 2.31854
[1mStep[0m  [100/106], [94mLoss[0m : 2.18498

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.492, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26331
[1mStep[0m  [10/106], [94mLoss[0m : 2.26039
[1mStep[0m  [20/106], [94mLoss[0m : 2.41943
[1mStep[0m  [30/106], [94mLoss[0m : 2.34035
[1mStep[0m  [40/106], [94mLoss[0m : 2.43510
[1mStep[0m  [50/106], [94mLoss[0m : 2.76290
[1mStep[0m  [60/106], [94mLoss[0m : 2.37710
[1mStep[0m  [70/106], [94mLoss[0m : 2.40263
[1mStep[0m  [80/106], [94mLoss[0m : 2.57192
[1mStep[0m  [90/106], [94mLoss[0m : 2.40106
[1mStep[0m  [100/106], [94mLoss[0m : 2.48861

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66281
[1mStep[0m  [10/106], [94mLoss[0m : 2.53000
[1mStep[0m  [20/106], [94mLoss[0m : 2.47700
[1mStep[0m  [30/106], [94mLoss[0m : 2.56846
[1mStep[0m  [40/106], [94mLoss[0m : 2.59988
[1mStep[0m  [50/106], [94mLoss[0m : 2.35908
[1mStep[0m  [60/106], [94mLoss[0m : 2.36644
[1mStep[0m  [70/106], [94mLoss[0m : 2.46644
[1mStep[0m  [80/106], [94mLoss[0m : 2.59204
[1mStep[0m  [90/106], [94mLoss[0m : 2.61893
[1mStep[0m  [100/106], [94mLoss[0m : 2.67427

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45169
[1mStep[0m  [10/106], [94mLoss[0m : 2.62097
[1mStep[0m  [20/106], [94mLoss[0m : 2.65875
[1mStep[0m  [30/106], [94mLoss[0m : 2.87833
[1mStep[0m  [40/106], [94mLoss[0m : 2.38362
[1mStep[0m  [50/106], [94mLoss[0m : 2.42795
[1mStep[0m  [60/106], [94mLoss[0m : 2.48020
[1mStep[0m  [70/106], [94mLoss[0m : 2.54203
[1mStep[0m  [80/106], [94mLoss[0m : 2.48493
[1mStep[0m  [90/106], [94mLoss[0m : 2.21655
[1mStep[0m  [100/106], [94mLoss[0m : 2.51152

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.476, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33674
[1mStep[0m  [10/106], [94mLoss[0m : 2.79081
[1mStep[0m  [20/106], [94mLoss[0m : 2.67590
[1mStep[0m  [30/106], [94mLoss[0m : 2.01215
[1mStep[0m  [40/106], [94mLoss[0m : 2.45040
[1mStep[0m  [50/106], [94mLoss[0m : 2.23890
[1mStep[0m  [60/106], [94mLoss[0m : 2.85361
[1mStep[0m  [70/106], [94mLoss[0m : 2.46946
[1mStep[0m  [80/106], [94mLoss[0m : 2.48546
[1mStep[0m  [90/106], [94mLoss[0m : 2.43177
[1mStep[0m  [100/106], [94mLoss[0m : 2.60099

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.485, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34317
[1mStep[0m  [10/106], [94mLoss[0m : 2.32240
[1mStep[0m  [20/106], [94mLoss[0m : 2.71766
[1mStep[0m  [30/106], [94mLoss[0m : 2.67494
[1mStep[0m  [40/106], [94mLoss[0m : 2.18684
[1mStep[0m  [50/106], [94mLoss[0m : 2.37524
[1mStep[0m  [60/106], [94mLoss[0m : 2.27361
[1mStep[0m  [70/106], [94mLoss[0m : 2.42354
[1mStep[0m  [80/106], [94mLoss[0m : 2.32311
[1mStep[0m  [90/106], [94mLoss[0m : 2.39998
[1mStep[0m  [100/106], [94mLoss[0m : 2.41715

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.483, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51901
[1mStep[0m  [10/106], [94mLoss[0m : 2.37061
[1mStep[0m  [20/106], [94mLoss[0m : 2.61074
[1mStep[0m  [30/106], [94mLoss[0m : 2.43512
[1mStep[0m  [40/106], [94mLoss[0m : 2.69575
[1mStep[0m  [50/106], [94mLoss[0m : 2.16059
[1mStep[0m  [60/106], [94mLoss[0m : 2.61063
[1mStep[0m  [70/106], [94mLoss[0m : 2.39976
[1mStep[0m  [80/106], [94mLoss[0m : 2.40311
[1mStep[0m  [90/106], [94mLoss[0m : 2.58452
[1mStep[0m  [100/106], [94mLoss[0m : 2.53915

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.475, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40425
[1mStep[0m  [10/106], [94mLoss[0m : 2.33093
[1mStep[0m  [20/106], [94mLoss[0m : 2.54129
[1mStep[0m  [30/106], [94mLoss[0m : 2.53488
[1mStep[0m  [40/106], [94mLoss[0m : 2.46188
[1mStep[0m  [50/106], [94mLoss[0m : 2.31271
[1mStep[0m  [60/106], [94mLoss[0m : 2.37361
[1mStep[0m  [70/106], [94mLoss[0m : 2.67942
[1mStep[0m  [80/106], [94mLoss[0m : 2.44733
[1mStep[0m  [90/106], [94mLoss[0m : 2.57937
[1mStep[0m  [100/106], [94mLoss[0m : 2.28341

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.479, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31533
[1mStep[0m  [10/106], [94mLoss[0m : 2.46602
[1mStep[0m  [20/106], [94mLoss[0m : 2.49219
[1mStep[0m  [30/106], [94mLoss[0m : 2.44281
[1mStep[0m  [40/106], [94mLoss[0m : 2.74786
[1mStep[0m  [50/106], [94mLoss[0m : 2.78374
[1mStep[0m  [60/106], [94mLoss[0m : 2.29364
[1mStep[0m  [70/106], [94mLoss[0m : 2.66560
[1mStep[0m  [80/106], [94mLoss[0m : 2.20790
[1mStep[0m  [90/106], [94mLoss[0m : 2.78246
[1mStep[0m  [100/106], [94mLoss[0m : 2.67359

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20446
[1mStep[0m  [10/106], [94mLoss[0m : 2.77339
[1mStep[0m  [20/106], [94mLoss[0m : 2.61992
[1mStep[0m  [30/106], [94mLoss[0m : 2.66856
[1mStep[0m  [40/106], [94mLoss[0m : 2.46726
[1mStep[0m  [50/106], [94mLoss[0m : 2.56528
[1mStep[0m  [60/106], [94mLoss[0m : 2.53342
[1mStep[0m  [70/106], [94mLoss[0m : 2.55739
[1mStep[0m  [80/106], [94mLoss[0m : 2.75883
[1mStep[0m  [90/106], [94mLoss[0m : 2.59142
[1mStep[0m  [100/106], [94mLoss[0m : 2.40556

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45619
[1mStep[0m  [10/106], [94mLoss[0m : 2.41712
[1mStep[0m  [20/106], [94mLoss[0m : 2.58905
[1mStep[0m  [30/106], [94mLoss[0m : 2.43348
[1mStep[0m  [40/106], [94mLoss[0m : 2.48161
[1mStep[0m  [50/106], [94mLoss[0m : 2.27074
[1mStep[0m  [60/106], [94mLoss[0m : 2.15896
[1mStep[0m  [70/106], [94mLoss[0m : 2.46954
[1mStep[0m  [80/106], [94mLoss[0m : 2.67788
[1mStep[0m  [90/106], [94mLoss[0m : 2.23708
[1mStep[0m  [100/106], [94mLoss[0m : 2.28085

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32455
[1mStep[0m  [10/106], [94mLoss[0m : 2.53225
[1mStep[0m  [20/106], [94mLoss[0m : 2.45764
[1mStep[0m  [30/106], [94mLoss[0m : 2.54998
[1mStep[0m  [40/106], [94mLoss[0m : 2.65061
[1mStep[0m  [50/106], [94mLoss[0m : 2.26963
[1mStep[0m  [60/106], [94mLoss[0m : 2.43946
[1mStep[0m  [70/106], [94mLoss[0m : 2.39458
[1mStep[0m  [80/106], [94mLoss[0m : 2.40016
[1mStep[0m  [90/106], [94mLoss[0m : 2.54838
[1mStep[0m  [100/106], [94mLoss[0m : 2.84867

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.468, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18471
[1mStep[0m  [10/106], [94mLoss[0m : 2.82940
[1mStep[0m  [20/106], [94mLoss[0m : 2.66298
[1mStep[0m  [30/106], [94mLoss[0m : 2.42534
[1mStep[0m  [40/106], [94mLoss[0m : 2.43676
[1mStep[0m  [50/106], [94mLoss[0m : 2.44624
[1mStep[0m  [60/106], [94mLoss[0m : 2.44054
[1mStep[0m  [70/106], [94mLoss[0m : 2.68667
[1mStep[0m  [80/106], [94mLoss[0m : 2.37407
[1mStep[0m  [90/106], [94mLoss[0m : 2.61941
[1mStep[0m  [100/106], [94mLoss[0m : 2.09949

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.471, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36280
[1mStep[0m  [10/106], [94mLoss[0m : 2.37245
[1mStep[0m  [20/106], [94mLoss[0m : 2.23100
[1mStep[0m  [30/106], [94mLoss[0m : 2.63941
[1mStep[0m  [40/106], [94mLoss[0m : 2.49441
[1mStep[0m  [50/106], [94mLoss[0m : 2.29480
[1mStep[0m  [60/106], [94mLoss[0m : 2.74989
[1mStep[0m  [70/106], [94mLoss[0m : 2.30708
[1mStep[0m  [80/106], [94mLoss[0m : 2.49375
[1mStep[0m  [90/106], [94mLoss[0m : 2.53607
[1mStep[0m  [100/106], [94mLoss[0m : 2.37947

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.475, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.04169
[1mStep[0m  [10/106], [94mLoss[0m : 2.46162
[1mStep[0m  [20/106], [94mLoss[0m : 2.62840
[1mStep[0m  [30/106], [94mLoss[0m : 2.37853
[1mStep[0m  [40/106], [94mLoss[0m : 2.81583
[1mStep[0m  [50/106], [94mLoss[0m : 2.52172
[1mStep[0m  [60/106], [94mLoss[0m : 2.66744
[1mStep[0m  [70/106], [94mLoss[0m : 2.79954
[1mStep[0m  [80/106], [94mLoss[0m : 2.56793
[1mStep[0m  [90/106], [94mLoss[0m : 2.40224
[1mStep[0m  [100/106], [94mLoss[0m : 2.47317

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.463, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23035
[1mStep[0m  [10/106], [94mLoss[0m : 2.32399
[1mStep[0m  [20/106], [94mLoss[0m : 2.42657
[1mStep[0m  [30/106], [94mLoss[0m : 2.70440
[1mStep[0m  [40/106], [94mLoss[0m : 2.68948
[1mStep[0m  [50/106], [94mLoss[0m : 2.32389
[1mStep[0m  [60/106], [94mLoss[0m : 2.21189
[1mStep[0m  [70/106], [94mLoss[0m : 2.48611
[1mStep[0m  [80/106], [94mLoss[0m : 2.21024
[1mStep[0m  [90/106], [94mLoss[0m : 2.20810
[1mStep[0m  [100/106], [94mLoss[0m : 2.53862

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.465, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70856
[1mStep[0m  [10/106], [94mLoss[0m : 2.41030
[1mStep[0m  [20/106], [94mLoss[0m : 2.32830
[1mStep[0m  [30/106], [94mLoss[0m : 2.53106
[1mStep[0m  [40/106], [94mLoss[0m : 2.24129
[1mStep[0m  [50/106], [94mLoss[0m : 2.29519
[1mStep[0m  [60/106], [94mLoss[0m : 2.21518
[1mStep[0m  [70/106], [94mLoss[0m : 2.46994
[1mStep[0m  [80/106], [94mLoss[0m : 2.46095
[1mStep[0m  [90/106], [94mLoss[0m : 2.81205
[1mStep[0m  [100/106], [94mLoss[0m : 2.64726

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.473, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36404
[1mStep[0m  [10/106], [94mLoss[0m : 2.59533
[1mStep[0m  [20/106], [94mLoss[0m : 2.72717
[1mStep[0m  [30/106], [94mLoss[0m : 2.56929
[1mStep[0m  [40/106], [94mLoss[0m : 2.56003
[1mStep[0m  [50/106], [94mLoss[0m : 2.53833
[1mStep[0m  [60/106], [94mLoss[0m : 2.15146
[1mStep[0m  [70/106], [94mLoss[0m : 2.37105
[1mStep[0m  [80/106], [94mLoss[0m : 2.63716
[1mStep[0m  [90/106], [94mLoss[0m : 2.35877
[1mStep[0m  [100/106], [94mLoss[0m : 2.26457

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.461, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53200
[1mStep[0m  [10/106], [94mLoss[0m : 2.39852
[1mStep[0m  [20/106], [94mLoss[0m : 2.33267
[1mStep[0m  [30/106], [94mLoss[0m : 2.47674
[1mStep[0m  [40/106], [94mLoss[0m : 2.36806
[1mStep[0m  [50/106], [94mLoss[0m : 2.41671
[1mStep[0m  [60/106], [94mLoss[0m : 2.30274
[1mStep[0m  [70/106], [94mLoss[0m : 2.45163
[1mStep[0m  [80/106], [94mLoss[0m : 2.51753
[1mStep[0m  [90/106], [94mLoss[0m : 2.51503
[1mStep[0m  [100/106], [94mLoss[0m : 2.12460

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.474, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27612
[1mStep[0m  [10/106], [94mLoss[0m : 2.32984
[1mStep[0m  [20/106], [94mLoss[0m : 2.61279
[1mStep[0m  [30/106], [94mLoss[0m : 2.38274
[1mStep[0m  [40/106], [94mLoss[0m : 2.23403
[1mStep[0m  [50/106], [94mLoss[0m : 2.14395
[1mStep[0m  [60/106], [94mLoss[0m : 2.62583
[1mStep[0m  [70/106], [94mLoss[0m : 2.27699
[1mStep[0m  [80/106], [94mLoss[0m : 2.57955
[1mStep[0m  [90/106], [94mLoss[0m : 2.46327
[1mStep[0m  [100/106], [94mLoss[0m : 2.57914

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.465, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23312
[1mStep[0m  [10/106], [94mLoss[0m : 2.43248
[1mStep[0m  [20/106], [94mLoss[0m : 2.39652
[1mStep[0m  [30/106], [94mLoss[0m : 2.54415
[1mStep[0m  [40/106], [94mLoss[0m : 2.19488
[1mStep[0m  [50/106], [94mLoss[0m : 2.55915
[1mStep[0m  [60/106], [94mLoss[0m : 2.24656
[1mStep[0m  [70/106], [94mLoss[0m : 2.64359
[1mStep[0m  [80/106], [94mLoss[0m : 2.40634
[1mStep[0m  [90/106], [94mLoss[0m : 2.32467
[1mStep[0m  [100/106], [94mLoss[0m : 2.55149

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.458, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57387
[1mStep[0m  [10/106], [94mLoss[0m : 2.60653
[1mStep[0m  [20/106], [94mLoss[0m : 2.60021
[1mStep[0m  [30/106], [94mLoss[0m : 2.36198
[1mStep[0m  [40/106], [94mLoss[0m : 2.53352
[1mStep[0m  [50/106], [94mLoss[0m : 2.59867
[1mStep[0m  [60/106], [94mLoss[0m : 2.50721
[1mStep[0m  [70/106], [94mLoss[0m : 2.63016
[1mStep[0m  [80/106], [94mLoss[0m : 2.53977
[1mStep[0m  [90/106], [94mLoss[0m : 2.32964
[1mStep[0m  [100/106], [94mLoss[0m : 2.63971

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.471, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.455
====================================

Phase 2 - Evaluation MAE:  2.4551810903369256
MAE score P1      2.536113
MAE score P2      2.455181
loss              2.447809
learning_rate       0.0001
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay         0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 10.19883
[1mStep[0m  [21/213], [94mLoss[0m : 10.30654
[1mStep[0m  [42/213], [94mLoss[0m : 10.38189
[1mStep[0m  [63/213], [94mLoss[0m : 10.90568
[1mStep[0m  [84/213], [94mLoss[0m : 10.33552
[1mStep[0m  [105/213], [94mLoss[0m : 10.80681
[1mStep[0m  [126/213], [94mLoss[0m : 10.98887
[1mStep[0m  [147/213], [94mLoss[0m : 10.04544
[1mStep[0m  [168/213], [94mLoss[0m : 9.96253
[1mStep[0m  [189/213], [94mLoss[0m : 9.92497
[1mStep[0m  [210/213], [94mLoss[0m : 10.04368

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.446, [92mTest[0m: 10.621, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.67597
[1mStep[0m  [21/213], [94mLoss[0m : 9.91968
[1mStep[0m  [42/213], [94mLoss[0m : 10.45609
[1mStep[0m  [63/213], [94mLoss[0m : 11.29488
[1mStep[0m  [84/213], [94mLoss[0m : 10.39630
[1mStep[0m  [105/213], [94mLoss[0m : 10.16117
[1mStep[0m  [126/213], [94mLoss[0m : 10.39656
[1mStep[0m  [147/213], [94mLoss[0m : 9.48318
[1mStep[0m  [168/213], [94mLoss[0m : 9.68791
[1mStep[0m  [189/213], [94mLoss[0m : 10.07035
[1mStep[0m  [210/213], [94mLoss[0m : 9.30771

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.132, [92mTest[0m: 10.291, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.51802
[1mStep[0m  [21/213], [94mLoss[0m : 10.84991
[1mStep[0m  [42/213], [94mLoss[0m : 10.52707
[1mStep[0m  [63/213], [94mLoss[0m : 9.41451
[1mStep[0m  [84/213], [94mLoss[0m : 9.68375
[1mStep[0m  [105/213], [94mLoss[0m : 9.16707
[1mStep[0m  [126/213], [94mLoss[0m : 9.04992
[1mStep[0m  [147/213], [94mLoss[0m : 8.90957
[1mStep[0m  [168/213], [94mLoss[0m : 10.94300
[1mStep[0m  [189/213], [94mLoss[0m : 9.80113
[1mStep[0m  [210/213], [94mLoss[0m : 9.84686

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.805, [92mTest[0m: 9.969, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.54821
[1mStep[0m  [21/213], [94mLoss[0m : 9.68671
[1mStep[0m  [42/213], [94mLoss[0m : 10.03630
[1mStep[0m  [63/213], [94mLoss[0m : 9.87321
[1mStep[0m  [84/213], [94mLoss[0m : 9.46640
[1mStep[0m  [105/213], [94mLoss[0m : 9.52721
[1mStep[0m  [126/213], [94mLoss[0m : 9.28927
[1mStep[0m  [147/213], [94mLoss[0m : 9.24795
[1mStep[0m  [168/213], [94mLoss[0m : 9.22287
[1mStep[0m  [189/213], [94mLoss[0m : 8.89673
[1mStep[0m  [210/213], [94mLoss[0m : 7.98653

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.490, [92mTest[0m: 9.642, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.13313
[1mStep[0m  [21/213], [94mLoss[0m : 8.93739
[1mStep[0m  [42/213], [94mLoss[0m : 8.67191
[1mStep[0m  [63/213], [94mLoss[0m : 9.17122
[1mStep[0m  [84/213], [94mLoss[0m : 8.70394
[1mStep[0m  [105/213], [94mLoss[0m : 9.21081
[1mStep[0m  [126/213], [94mLoss[0m : 8.94762
[1mStep[0m  [147/213], [94mLoss[0m : 8.86585
[1mStep[0m  [168/213], [94mLoss[0m : 9.36881
[1mStep[0m  [189/213], [94mLoss[0m : 9.00205
[1mStep[0m  [210/213], [94mLoss[0m : 8.42157

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.166, [92mTest[0m: 9.322, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.54825
[1mStep[0m  [21/213], [94mLoss[0m : 8.73626
[1mStep[0m  [42/213], [94mLoss[0m : 9.00453
[1mStep[0m  [63/213], [94mLoss[0m : 8.98536
[1mStep[0m  [84/213], [94mLoss[0m : 8.05538
[1mStep[0m  [105/213], [94mLoss[0m : 8.10859
[1mStep[0m  [126/213], [94mLoss[0m : 8.84897
[1mStep[0m  [147/213], [94mLoss[0m : 8.42964
[1mStep[0m  [168/213], [94mLoss[0m : 8.43121
[1mStep[0m  [189/213], [94mLoss[0m : 8.55830
[1mStep[0m  [210/213], [94mLoss[0m : 8.09662

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.847, [92mTest[0m: 9.009, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.55338
[1mStep[0m  [21/213], [94mLoss[0m : 8.51708
[1mStep[0m  [42/213], [94mLoss[0m : 8.21558
[1mStep[0m  [63/213], [94mLoss[0m : 8.54804
[1mStep[0m  [84/213], [94mLoss[0m : 7.99166
[1mStep[0m  [105/213], [94mLoss[0m : 8.97935
[1mStep[0m  [126/213], [94mLoss[0m : 8.59961
[1mStep[0m  [147/213], [94mLoss[0m : 7.70672
[1mStep[0m  [168/213], [94mLoss[0m : 8.82546
[1mStep[0m  [189/213], [94mLoss[0m : 8.67704
[1mStep[0m  [210/213], [94mLoss[0m : 7.86091

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.527, [92mTest[0m: 8.680, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.49581
[1mStep[0m  [21/213], [94mLoss[0m : 8.57591
[1mStep[0m  [42/213], [94mLoss[0m : 7.75107
[1mStep[0m  [63/213], [94mLoss[0m : 8.28623
[1mStep[0m  [84/213], [94mLoss[0m : 8.46058
[1mStep[0m  [105/213], [94mLoss[0m : 8.62378
[1mStep[0m  [126/213], [94mLoss[0m : 8.29036
[1mStep[0m  [147/213], [94mLoss[0m : 8.42151
[1mStep[0m  [168/213], [94mLoss[0m : 8.51541
[1mStep[0m  [189/213], [94mLoss[0m : 7.87388
[1mStep[0m  [210/213], [94mLoss[0m : 8.01882

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.208, [92mTest[0m: 8.367, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.41682
[1mStep[0m  [21/213], [94mLoss[0m : 8.06281
[1mStep[0m  [42/213], [94mLoss[0m : 8.68193
[1mStep[0m  [63/213], [94mLoss[0m : 7.89601
[1mStep[0m  [84/213], [94mLoss[0m : 8.25739
[1mStep[0m  [105/213], [94mLoss[0m : 7.06947
[1mStep[0m  [126/213], [94mLoss[0m : 7.78638
[1mStep[0m  [147/213], [94mLoss[0m : 7.97183
[1mStep[0m  [168/213], [94mLoss[0m : 8.09036
[1mStep[0m  [189/213], [94mLoss[0m : 7.72024
[1mStep[0m  [210/213], [94mLoss[0m : 6.95311

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.895, [92mTest[0m: 8.054, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.69099
[1mStep[0m  [21/213], [94mLoss[0m : 7.89500
[1mStep[0m  [42/213], [94mLoss[0m : 8.11267
[1mStep[0m  [63/213], [94mLoss[0m : 7.56722
[1mStep[0m  [84/213], [94mLoss[0m : 6.69720
[1mStep[0m  [105/213], [94mLoss[0m : 8.07667
[1mStep[0m  [126/213], [94mLoss[0m : 7.38987
[1mStep[0m  [147/213], [94mLoss[0m : 7.36816
[1mStep[0m  [168/213], [94mLoss[0m : 7.58807
[1mStep[0m  [189/213], [94mLoss[0m : 7.64445
[1mStep[0m  [210/213], [94mLoss[0m : 7.47721

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.566, [92mTest[0m: 7.724, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.95856
[1mStep[0m  [21/213], [94mLoss[0m : 7.75158
[1mStep[0m  [42/213], [94mLoss[0m : 6.92587
[1mStep[0m  [63/213], [94mLoss[0m : 7.71703
[1mStep[0m  [84/213], [94mLoss[0m : 6.92825
[1mStep[0m  [105/213], [94mLoss[0m : 6.63845
[1mStep[0m  [126/213], [94mLoss[0m : 6.65739
[1mStep[0m  [147/213], [94mLoss[0m : 7.94386
[1mStep[0m  [168/213], [94mLoss[0m : 8.12577
[1mStep[0m  [189/213], [94mLoss[0m : 7.39042
[1mStep[0m  [210/213], [94mLoss[0m : 6.88393

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.241, [92mTest[0m: 7.388, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.04818
[1mStep[0m  [21/213], [94mLoss[0m : 7.78680
[1mStep[0m  [42/213], [94mLoss[0m : 7.18049
[1mStep[0m  [63/213], [94mLoss[0m : 6.61138
[1mStep[0m  [84/213], [94mLoss[0m : 6.91789
[1mStep[0m  [105/213], [94mLoss[0m : 6.37594
[1mStep[0m  [126/213], [94mLoss[0m : 7.91188
[1mStep[0m  [147/213], [94mLoss[0m : 7.59209
[1mStep[0m  [168/213], [94mLoss[0m : 6.37919
[1mStep[0m  [189/213], [94mLoss[0m : 7.04728
[1mStep[0m  [210/213], [94mLoss[0m : 6.72361

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.928, [92mTest[0m: 7.083, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.38173
[1mStep[0m  [21/213], [94mLoss[0m : 6.24036
[1mStep[0m  [42/213], [94mLoss[0m : 6.77533
[1mStep[0m  [63/213], [94mLoss[0m : 7.31415
[1mStep[0m  [84/213], [94mLoss[0m : 6.53068
[1mStep[0m  [105/213], [94mLoss[0m : 6.92616
[1mStep[0m  [126/213], [94mLoss[0m : 7.04100
[1mStep[0m  [147/213], [94mLoss[0m : 6.21572
[1mStep[0m  [168/213], [94mLoss[0m : 5.92705
[1mStep[0m  [189/213], [94mLoss[0m : 6.49870
[1mStep[0m  [210/213], [94mLoss[0m : 6.30078

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.599, [92mTest[0m: 6.739, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.58195
[1mStep[0m  [21/213], [94mLoss[0m : 6.26625
[1mStep[0m  [42/213], [94mLoss[0m : 6.69345
[1mStep[0m  [63/213], [94mLoss[0m : 6.30464
[1mStep[0m  [84/213], [94mLoss[0m : 6.14349
[1mStep[0m  [105/213], [94mLoss[0m : 5.94087
[1mStep[0m  [126/213], [94mLoss[0m : 6.72484
[1mStep[0m  [147/213], [94mLoss[0m : 6.72280
[1mStep[0m  [168/213], [94mLoss[0m : 6.73722
[1mStep[0m  [189/213], [94mLoss[0m : 5.16094
[1mStep[0m  [210/213], [94mLoss[0m : 6.95715

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.281, [92mTest[0m: 6.430, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.11812
[1mStep[0m  [21/213], [94mLoss[0m : 6.54187
[1mStep[0m  [42/213], [94mLoss[0m : 5.52082
[1mStep[0m  [63/213], [94mLoss[0m : 5.81782
[1mStep[0m  [84/213], [94mLoss[0m : 5.67855
[1mStep[0m  [105/213], [94mLoss[0m : 6.01735
[1mStep[0m  [126/213], [94mLoss[0m : 5.70311
[1mStep[0m  [147/213], [94mLoss[0m : 5.21732
[1mStep[0m  [168/213], [94mLoss[0m : 5.47566
[1mStep[0m  [189/213], [94mLoss[0m : 5.83604
[1mStep[0m  [210/213], [94mLoss[0m : 5.30686

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.955, [92mTest[0m: 6.116, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.73028
[1mStep[0m  [21/213], [94mLoss[0m : 6.27529
[1mStep[0m  [42/213], [94mLoss[0m : 5.12880
[1mStep[0m  [63/213], [94mLoss[0m : 6.10602
[1mStep[0m  [84/213], [94mLoss[0m : 5.75031
[1mStep[0m  [105/213], [94mLoss[0m : 5.92807
[1mStep[0m  [126/213], [94mLoss[0m : 5.78515
[1mStep[0m  [147/213], [94mLoss[0m : 5.15160
[1mStep[0m  [168/213], [94mLoss[0m : 5.73327
[1mStep[0m  [189/213], [94mLoss[0m : 5.79381
[1mStep[0m  [210/213], [94mLoss[0m : 5.69049

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.657, [92mTest[0m: 5.798, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.15745
[1mStep[0m  [21/213], [94mLoss[0m : 5.25485
[1mStep[0m  [42/213], [94mLoss[0m : 4.24854
[1mStep[0m  [63/213], [94mLoss[0m : 5.05454
[1mStep[0m  [84/213], [94mLoss[0m : 5.70008
[1mStep[0m  [105/213], [94mLoss[0m : 5.83937
[1mStep[0m  [126/213], [94mLoss[0m : 5.62059
[1mStep[0m  [147/213], [94mLoss[0m : 4.62481
[1mStep[0m  [168/213], [94mLoss[0m : 5.03644
[1mStep[0m  [189/213], [94mLoss[0m : 5.64584
[1mStep[0m  [210/213], [94mLoss[0m : 5.31827

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.361, [92mTest[0m: 5.479, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.13280
[1mStep[0m  [21/213], [94mLoss[0m : 5.07227
[1mStep[0m  [42/213], [94mLoss[0m : 5.38564
[1mStep[0m  [63/213], [94mLoss[0m : 5.42024
[1mStep[0m  [84/213], [94mLoss[0m : 4.83566
[1mStep[0m  [105/213], [94mLoss[0m : 5.21925
[1mStep[0m  [126/213], [94mLoss[0m : 4.67242
[1mStep[0m  [147/213], [94mLoss[0m : 4.76595
[1mStep[0m  [168/213], [94mLoss[0m : 4.97792
[1mStep[0m  [189/213], [94mLoss[0m : 4.92319
[1mStep[0m  [210/213], [94mLoss[0m : 5.05404

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.084, [92mTest[0m: 5.193, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 4.93060
[1mStep[0m  [21/213], [94mLoss[0m : 4.70762
[1mStep[0m  [42/213], [94mLoss[0m : 4.34400
[1mStep[0m  [63/213], [94mLoss[0m : 5.30245
[1mStep[0m  [84/213], [94mLoss[0m : 4.79857
[1mStep[0m  [105/213], [94mLoss[0m : 5.84025
[1mStep[0m  [126/213], [94mLoss[0m : 5.49109
[1mStep[0m  [147/213], [94mLoss[0m : 5.22805
[1mStep[0m  [168/213], [94mLoss[0m : 4.36363
[1mStep[0m  [189/213], [94mLoss[0m : 5.23059
[1mStep[0m  [210/213], [94mLoss[0m : 4.14079

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.827, [92mTest[0m: 4.907, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.35894
[1mStep[0m  [21/213], [94mLoss[0m : 5.36709
[1mStep[0m  [42/213], [94mLoss[0m : 4.30021
[1mStep[0m  [63/213], [94mLoss[0m : 4.22535
[1mStep[0m  [84/213], [94mLoss[0m : 3.93027
[1mStep[0m  [105/213], [94mLoss[0m : 4.72535
[1mStep[0m  [126/213], [94mLoss[0m : 4.24580
[1mStep[0m  [147/213], [94mLoss[0m : 3.57564
[1mStep[0m  [168/213], [94mLoss[0m : 4.21446
[1mStep[0m  [189/213], [94mLoss[0m : 3.85537
[1mStep[0m  [210/213], [94mLoss[0m : 4.67504

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.581, [92mTest[0m: 4.649, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.16322
[1mStep[0m  [21/213], [94mLoss[0m : 4.25360
[1mStep[0m  [42/213], [94mLoss[0m : 3.95263
[1mStep[0m  [63/213], [94mLoss[0m : 4.01258
[1mStep[0m  [84/213], [94mLoss[0m : 4.06592
[1mStep[0m  [105/213], [94mLoss[0m : 5.00238
[1mStep[0m  [126/213], [94mLoss[0m : 4.24309
[1mStep[0m  [147/213], [94mLoss[0m : 3.99146
[1mStep[0m  [168/213], [94mLoss[0m : 3.90899
[1mStep[0m  [189/213], [94mLoss[0m : 4.10798
[1mStep[0m  [210/213], [94mLoss[0m : 5.43449

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.374, [92mTest[0m: 4.410, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 4.27266
[1mStep[0m  [21/213], [94mLoss[0m : 4.23446
[1mStep[0m  [42/213], [94mLoss[0m : 4.36827
[1mStep[0m  [63/213], [94mLoss[0m : 3.99062
[1mStep[0m  [84/213], [94mLoss[0m : 4.38796
[1mStep[0m  [105/213], [94mLoss[0m : 4.67952
[1mStep[0m  [126/213], [94mLoss[0m : 4.35673
[1mStep[0m  [147/213], [94mLoss[0m : 3.74173
[1mStep[0m  [168/213], [94mLoss[0m : 4.01202
[1mStep[0m  [189/213], [94mLoss[0m : 4.45715
[1mStep[0m  [210/213], [94mLoss[0m : 4.22483

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.202, [92mTest[0m: 4.229, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.64686
[1mStep[0m  [21/213], [94mLoss[0m : 3.93317
[1mStep[0m  [42/213], [94mLoss[0m : 3.80958
[1mStep[0m  [63/213], [94mLoss[0m : 4.28394
[1mStep[0m  [84/213], [94mLoss[0m : 4.26903
[1mStep[0m  [105/213], [94mLoss[0m : 3.75057
[1mStep[0m  [126/213], [94mLoss[0m : 4.15459
[1mStep[0m  [147/213], [94mLoss[0m : 3.82650
[1mStep[0m  [168/213], [94mLoss[0m : 4.11813
[1mStep[0m  [189/213], [94mLoss[0m : 3.81873
[1mStep[0m  [210/213], [94mLoss[0m : 4.01473

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.035, [92mTest[0m: 4.028, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.78899
[1mStep[0m  [21/213], [94mLoss[0m : 3.60300
[1mStep[0m  [42/213], [94mLoss[0m : 3.68661
[1mStep[0m  [63/213], [94mLoss[0m : 3.25633
[1mStep[0m  [84/213], [94mLoss[0m : 3.78356
[1mStep[0m  [105/213], [94mLoss[0m : 4.13342
[1mStep[0m  [126/213], [94mLoss[0m : 4.13777
[1mStep[0m  [147/213], [94mLoss[0m : 3.53936
[1mStep[0m  [168/213], [94mLoss[0m : 3.44995
[1mStep[0m  [189/213], [94mLoss[0m : 3.79347
[1mStep[0m  [210/213], [94mLoss[0m : 3.96577

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.876, [92mTest[0m: 3.881, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 4.53462
[1mStep[0m  [21/213], [94mLoss[0m : 3.42653
[1mStep[0m  [42/213], [94mLoss[0m : 4.01074
[1mStep[0m  [63/213], [94mLoss[0m : 3.82507
[1mStep[0m  [84/213], [94mLoss[0m : 4.28566
[1mStep[0m  [105/213], [94mLoss[0m : 3.91211
[1mStep[0m  [126/213], [94mLoss[0m : 4.02289
[1mStep[0m  [147/213], [94mLoss[0m : 3.88999
[1mStep[0m  [168/213], [94mLoss[0m : 3.65593
[1mStep[0m  [189/213], [94mLoss[0m : 3.67441
[1mStep[0m  [210/213], [94mLoss[0m : 4.06600

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.759, [92mTest[0m: 3.732, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.16565
[1mStep[0m  [21/213], [94mLoss[0m : 3.48562
[1mStep[0m  [42/213], [94mLoss[0m : 3.33106
[1mStep[0m  [63/213], [94mLoss[0m : 3.55464
[1mStep[0m  [84/213], [94mLoss[0m : 3.35765
[1mStep[0m  [105/213], [94mLoss[0m : 3.92380
[1mStep[0m  [126/213], [94mLoss[0m : 4.21219
[1mStep[0m  [147/213], [94mLoss[0m : 3.92102
[1mStep[0m  [168/213], [94mLoss[0m : 3.43213
[1mStep[0m  [189/213], [94mLoss[0m : 3.32546
[1mStep[0m  [210/213], [94mLoss[0m : 3.70679

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.643, [92mTest[0m: 3.590, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.86769
[1mStep[0m  [21/213], [94mLoss[0m : 3.82905
[1mStep[0m  [42/213], [94mLoss[0m : 4.10804
[1mStep[0m  [63/213], [94mLoss[0m : 4.47599
[1mStep[0m  [84/213], [94mLoss[0m : 3.73739
[1mStep[0m  [105/213], [94mLoss[0m : 3.50909
[1mStep[0m  [126/213], [94mLoss[0m : 4.09965
[1mStep[0m  [147/213], [94mLoss[0m : 3.91695
[1mStep[0m  [168/213], [94mLoss[0m : 3.78020
[1mStep[0m  [189/213], [94mLoss[0m : 4.51554
[1mStep[0m  [210/213], [94mLoss[0m : 2.86303

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.520, [92mTest[0m: 3.472, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 4.20465
[1mStep[0m  [21/213], [94mLoss[0m : 3.62657
[1mStep[0m  [42/213], [94mLoss[0m : 3.48339
[1mStep[0m  [63/213], [94mLoss[0m : 3.54472
[1mStep[0m  [84/213], [94mLoss[0m : 3.31015
[1mStep[0m  [105/213], [94mLoss[0m : 3.14587
[1mStep[0m  [126/213], [94mLoss[0m : 2.91799
[1mStep[0m  [147/213], [94mLoss[0m : 3.18905
[1mStep[0m  [168/213], [94mLoss[0m : 3.22930
[1mStep[0m  [189/213], [94mLoss[0m : 3.79735
[1mStep[0m  [210/213], [94mLoss[0m : 4.21848

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.423, [92mTest[0m: 3.341, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.50781
[1mStep[0m  [21/213], [94mLoss[0m : 4.11034
[1mStep[0m  [42/213], [94mLoss[0m : 3.48905
[1mStep[0m  [63/213], [94mLoss[0m : 2.59447
[1mStep[0m  [84/213], [94mLoss[0m : 2.86628
[1mStep[0m  [105/213], [94mLoss[0m : 3.54117
[1mStep[0m  [126/213], [94mLoss[0m : 3.62794
[1mStep[0m  [147/213], [94mLoss[0m : 2.68300
[1mStep[0m  [168/213], [94mLoss[0m : 2.83054
[1mStep[0m  [189/213], [94mLoss[0m : 4.01218
[1mStep[0m  [210/213], [94mLoss[0m : 3.29375

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.342, [92mTest[0m: 3.260, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.14919
[1mStep[0m  [21/213], [94mLoss[0m : 2.92299
[1mStep[0m  [42/213], [94mLoss[0m : 3.59320
[1mStep[0m  [63/213], [94mLoss[0m : 3.43182
[1mStep[0m  [84/213], [94mLoss[0m : 3.81806
[1mStep[0m  [105/213], [94mLoss[0m : 3.64558
[1mStep[0m  [126/213], [94mLoss[0m : 3.40213
[1mStep[0m  [147/213], [94mLoss[0m : 3.29805
[1mStep[0m  [168/213], [94mLoss[0m : 3.05687
[1mStep[0m  [189/213], [94mLoss[0m : 3.78507
[1mStep[0m  [210/213], [94mLoss[0m : 2.87997

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.249, [92mTest[0m: 3.187, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.081
====================================

Phase 1 - Evaluation MAE:  3.0809884161319374
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 3.23596
[1mStep[0m  [21/213], [94mLoss[0m : 3.10528
[1mStep[0m  [42/213], [94mLoss[0m : 3.23903
[1mStep[0m  [63/213], [94mLoss[0m : 2.88152
[1mStep[0m  [84/213], [94mLoss[0m : 3.14659
[1mStep[0m  [105/213], [94mLoss[0m : 3.22904
[1mStep[0m  [126/213], [94mLoss[0m : 3.08073
[1mStep[0m  [147/213], [94mLoss[0m : 2.52218
[1mStep[0m  [168/213], [94mLoss[0m : 2.77934
[1mStep[0m  [189/213], [94mLoss[0m : 3.25195
[1mStep[0m  [210/213], [94mLoss[0m : 3.37905

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.191, [92mTest[0m: 3.077, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.95537
[1mStep[0m  [21/213], [94mLoss[0m : 3.01959
[1mStep[0m  [42/213], [94mLoss[0m : 3.09549
[1mStep[0m  [63/213], [94mLoss[0m : 3.35650
[1mStep[0m  [84/213], [94mLoss[0m : 3.18975
[1mStep[0m  [105/213], [94mLoss[0m : 2.86371
[1mStep[0m  [126/213], [94mLoss[0m : 3.15576
[1mStep[0m  [147/213], [94mLoss[0m : 3.12958
[1mStep[0m  [168/213], [94mLoss[0m : 2.76388
[1mStep[0m  [189/213], [94mLoss[0m : 3.06142
[1mStep[0m  [210/213], [94mLoss[0m : 3.56749

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.095, [92mTest[0m: 3.013, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.90453
[1mStep[0m  [21/213], [94mLoss[0m : 3.09938
[1mStep[0m  [42/213], [94mLoss[0m : 2.92060
[1mStep[0m  [63/213], [94mLoss[0m : 3.14469
[1mStep[0m  [84/213], [94mLoss[0m : 2.92423
[1mStep[0m  [105/213], [94mLoss[0m : 2.82831
[1mStep[0m  [126/213], [94mLoss[0m : 2.58216
[1mStep[0m  [147/213], [94mLoss[0m : 3.25916
[1mStep[0m  [168/213], [94mLoss[0m : 2.90978
[1mStep[0m  [189/213], [94mLoss[0m : 2.18634
[1mStep[0m  [210/213], [94mLoss[0m : 2.88136

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.035, [92mTest[0m: 2.934, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.72104
[1mStep[0m  [21/213], [94mLoss[0m : 3.34634
[1mStep[0m  [42/213], [94mLoss[0m : 3.14618
[1mStep[0m  [63/213], [94mLoss[0m : 3.50821
[1mStep[0m  [84/213], [94mLoss[0m : 2.48134
[1mStep[0m  [105/213], [94mLoss[0m : 2.83176
[1mStep[0m  [126/213], [94mLoss[0m : 2.91469
[1mStep[0m  [147/213], [94mLoss[0m : 2.41626
[1mStep[0m  [168/213], [94mLoss[0m : 2.77817
[1mStep[0m  [189/213], [94mLoss[0m : 2.63845
[1mStep[0m  [210/213], [94mLoss[0m : 3.61846

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.970, [92mTest[0m: 2.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.68710
[1mStep[0m  [21/213], [94mLoss[0m : 2.61668
[1mStep[0m  [42/213], [94mLoss[0m : 2.44469
[1mStep[0m  [63/213], [94mLoss[0m : 3.35653
[1mStep[0m  [84/213], [94mLoss[0m : 2.83321
[1mStep[0m  [105/213], [94mLoss[0m : 2.81771
[1mStep[0m  [126/213], [94mLoss[0m : 2.92556
[1mStep[0m  [147/213], [94mLoss[0m : 2.88292
[1mStep[0m  [168/213], [94mLoss[0m : 2.74646
[1mStep[0m  [189/213], [94mLoss[0m : 3.48081
[1mStep[0m  [210/213], [94mLoss[0m : 3.19250

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.916, [92mTest[0m: 2.814, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08627
[1mStep[0m  [21/213], [94mLoss[0m : 3.02457
[1mStep[0m  [42/213], [94mLoss[0m : 2.42940
[1mStep[0m  [63/213], [94mLoss[0m : 2.39511
[1mStep[0m  [84/213], [94mLoss[0m : 2.91224
[1mStep[0m  [105/213], [94mLoss[0m : 2.95261
[1mStep[0m  [126/213], [94mLoss[0m : 2.96674
[1mStep[0m  [147/213], [94mLoss[0m : 2.31038
[1mStep[0m  [168/213], [94mLoss[0m : 2.73749
[1mStep[0m  [189/213], [94mLoss[0m : 2.58459
[1mStep[0m  [210/213], [94mLoss[0m : 2.47477

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.859, [92mTest[0m: 2.771, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70060
[1mStep[0m  [21/213], [94mLoss[0m : 3.61880
[1mStep[0m  [42/213], [94mLoss[0m : 2.52103
[1mStep[0m  [63/213], [94mLoss[0m : 2.38646
[1mStep[0m  [84/213], [94mLoss[0m : 2.96895
[1mStep[0m  [105/213], [94mLoss[0m : 2.70382
[1mStep[0m  [126/213], [94mLoss[0m : 2.56623
[1mStep[0m  [147/213], [94mLoss[0m : 2.64689
[1mStep[0m  [168/213], [94mLoss[0m : 2.81158
[1mStep[0m  [189/213], [94mLoss[0m : 3.29560
[1mStep[0m  [210/213], [94mLoss[0m : 2.91702

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.826, [92mTest[0m: 2.692, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.75900
[1mStep[0m  [21/213], [94mLoss[0m : 2.91079
[1mStep[0m  [42/213], [94mLoss[0m : 2.40064
[1mStep[0m  [63/213], [94mLoss[0m : 2.87155
[1mStep[0m  [84/213], [94mLoss[0m : 2.33413
[1mStep[0m  [105/213], [94mLoss[0m : 2.48404
[1mStep[0m  [126/213], [94mLoss[0m : 3.67600
[1mStep[0m  [147/213], [94mLoss[0m : 2.53976
[1mStep[0m  [168/213], [94mLoss[0m : 2.69895
[1mStep[0m  [189/213], [94mLoss[0m : 3.03685
[1mStep[0m  [210/213], [94mLoss[0m : 2.88329

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.790, [92mTest[0m: 2.657, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73179
[1mStep[0m  [21/213], [94mLoss[0m : 3.14958
[1mStep[0m  [42/213], [94mLoss[0m : 2.86247
[1mStep[0m  [63/213], [94mLoss[0m : 2.43316
[1mStep[0m  [84/213], [94mLoss[0m : 3.00466
[1mStep[0m  [105/213], [94mLoss[0m : 2.84777
[1mStep[0m  [126/213], [94mLoss[0m : 2.60457
[1mStep[0m  [147/213], [94mLoss[0m : 2.76926
[1mStep[0m  [168/213], [94mLoss[0m : 2.22976
[1mStep[0m  [189/213], [94mLoss[0m : 2.96025
[1mStep[0m  [210/213], [94mLoss[0m : 3.19738

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.621, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.82538
[1mStep[0m  [21/213], [94mLoss[0m : 2.85639
[1mStep[0m  [42/213], [94mLoss[0m : 3.03899
[1mStep[0m  [63/213], [94mLoss[0m : 2.49140
[1mStep[0m  [84/213], [94mLoss[0m : 2.47928
[1mStep[0m  [105/213], [94mLoss[0m : 3.01241
[1mStep[0m  [126/213], [94mLoss[0m : 2.85022
[1mStep[0m  [147/213], [94mLoss[0m : 2.78251
[1mStep[0m  [168/213], [94mLoss[0m : 3.37599
[1mStep[0m  [189/213], [94mLoss[0m : 3.31328
[1mStep[0m  [210/213], [94mLoss[0m : 2.91468

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.718, [92mTest[0m: 2.597, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.04713
[1mStep[0m  [21/213], [94mLoss[0m : 2.82938
[1mStep[0m  [42/213], [94mLoss[0m : 3.12762
[1mStep[0m  [63/213], [94mLoss[0m : 2.83584
[1mStep[0m  [84/213], [94mLoss[0m : 3.05425
[1mStep[0m  [105/213], [94mLoss[0m : 2.52229
[1mStep[0m  [126/213], [94mLoss[0m : 2.76678
[1mStep[0m  [147/213], [94mLoss[0m : 2.92632
[1mStep[0m  [168/213], [94mLoss[0m : 2.54995
[1mStep[0m  [189/213], [94mLoss[0m : 2.09476
[1mStep[0m  [210/213], [94mLoss[0m : 2.62151

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.718, [92mTest[0m: 2.575, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.30153
[1mStep[0m  [21/213], [94mLoss[0m : 2.78872
[1mStep[0m  [42/213], [94mLoss[0m : 2.18428
[1mStep[0m  [63/213], [94mLoss[0m : 2.63861
[1mStep[0m  [84/213], [94mLoss[0m : 2.19573
[1mStep[0m  [105/213], [94mLoss[0m : 2.72617
[1mStep[0m  [126/213], [94mLoss[0m : 2.76464
[1mStep[0m  [147/213], [94mLoss[0m : 2.63449
[1mStep[0m  [168/213], [94mLoss[0m : 2.52658
[1mStep[0m  [189/213], [94mLoss[0m : 2.35594
[1mStep[0m  [210/213], [94mLoss[0m : 3.19381

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.560, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.64307
[1mStep[0m  [21/213], [94mLoss[0m : 2.38808
[1mStep[0m  [42/213], [94mLoss[0m : 2.16797
[1mStep[0m  [63/213], [94mLoss[0m : 2.44193
[1mStep[0m  [84/213], [94mLoss[0m : 2.46942
[1mStep[0m  [105/213], [94mLoss[0m : 2.61886
[1mStep[0m  [126/213], [94mLoss[0m : 2.55616
[1mStep[0m  [147/213], [94mLoss[0m : 2.85143
[1mStep[0m  [168/213], [94mLoss[0m : 3.10025
[1mStep[0m  [189/213], [94mLoss[0m : 2.85223
[1mStep[0m  [210/213], [94mLoss[0m : 2.64250

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.542, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.81375
[1mStep[0m  [21/213], [94mLoss[0m : 2.64796
[1mStep[0m  [42/213], [94mLoss[0m : 2.77345
[1mStep[0m  [63/213], [94mLoss[0m : 2.97646
[1mStep[0m  [84/213], [94mLoss[0m : 2.74636
[1mStep[0m  [105/213], [94mLoss[0m : 2.34840
[1mStep[0m  [126/213], [94mLoss[0m : 2.89153
[1mStep[0m  [147/213], [94mLoss[0m : 2.73342
[1mStep[0m  [168/213], [94mLoss[0m : 2.78760
[1mStep[0m  [189/213], [94mLoss[0m : 1.87877
[1mStep[0m  [210/213], [94mLoss[0m : 2.40094

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.524, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.72995
[1mStep[0m  [21/213], [94mLoss[0m : 2.52825
[1mStep[0m  [42/213], [94mLoss[0m : 3.47615
[1mStep[0m  [63/213], [94mLoss[0m : 2.33751
[1mStep[0m  [84/213], [94mLoss[0m : 2.57396
[1mStep[0m  [105/213], [94mLoss[0m : 2.95111
[1mStep[0m  [126/213], [94mLoss[0m : 2.69289
[1mStep[0m  [147/213], [94mLoss[0m : 2.37618
[1mStep[0m  [168/213], [94mLoss[0m : 2.94296
[1mStep[0m  [189/213], [94mLoss[0m : 2.50911
[1mStep[0m  [210/213], [94mLoss[0m : 2.41513

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.518, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61304
[1mStep[0m  [21/213], [94mLoss[0m : 2.28315
[1mStep[0m  [42/213], [94mLoss[0m : 2.71983
[1mStep[0m  [63/213], [94mLoss[0m : 2.64895
[1mStep[0m  [84/213], [94mLoss[0m : 2.61750
[1mStep[0m  [105/213], [94mLoss[0m : 2.30767
[1mStep[0m  [126/213], [94mLoss[0m : 2.72546
[1mStep[0m  [147/213], [94mLoss[0m : 2.41013
[1mStep[0m  [168/213], [94mLoss[0m : 2.70910
[1mStep[0m  [189/213], [94mLoss[0m : 2.48443
[1mStep[0m  [210/213], [94mLoss[0m : 2.76301

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.494, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33613
[1mStep[0m  [21/213], [94mLoss[0m : 2.31254
[1mStep[0m  [42/213], [94mLoss[0m : 2.18509
[1mStep[0m  [63/213], [94mLoss[0m : 2.48350
[1mStep[0m  [84/213], [94mLoss[0m : 2.55197
[1mStep[0m  [105/213], [94mLoss[0m : 2.31411
[1mStep[0m  [126/213], [94mLoss[0m : 2.86405
[1mStep[0m  [147/213], [94mLoss[0m : 3.11865
[1mStep[0m  [168/213], [94mLoss[0m : 3.12376
[1mStep[0m  [189/213], [94mLoss[0m : 2.70223
[1mStep[0m  [210/213], [94mLoss[0m : 2.32528

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.496, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58227
[1mStep[0m  [21/213], [94mLoss[0m : 2.41247
[1mStep[0m  [42/213], [94mLoss[0m : 2.65108
[1mStep[0m  [63/213], [94mLoss[0m : 2.56911
[1mStep[0m  [84/213], [94mLoss[0m : 2.21440
[1mStep[0m  [105/213], [94mLoss[0m : 2.36125
[1mStep[0m  [126/213], [94mLoss[0m : 2.78394
[1mStep[0m  [147/213], [94mLoss[0m : 2.31015
[1mStep[0m  [168/213], [94mLoss[0m : 2.29299
[1mStep[0m  [189/213], [94mLoss[0m : 2.87926
[1mStep[0m  [210/213], [94mLoss[0m : 2.88050

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.492, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39819
[1mStep[0m  [21/213], [94mLoss[0m : 2.85986
[1mStep[0m  [42/213], [94mLoss[0m : 2.50248
[1mStep[0m  [63/213], [94mLoss[0m : 2.15343
[1mStep[0m  [84/213], [94mLoss[0m : 2.71093
[1mStep[0m  [105/213], [94mLoss[0m : 2.36648
[1mStep[0m  [126/213], [94mLoss[0m : 2.51846
[1mStep[0m  [147/213], [94mLoss[0m : 2.53383
[1mStep[0m  [168/213], [94mLoss[0m : 2.26795
[1mStep[0m  [189/213], [94mLoss[0m : 2.49418
[1mStep[0m  [210/213], [94mLoss[0m : 2.23745

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44896
[1mStep[0m  [21/213], [94mLoss[0m : 2.31398
[1mStep[0m  [42/213], [94mLoss[0m : 2.91757
[1mStep[0m  [63/213], [94mLoss[0m : 2.69637
[1mStep[0m  [84/213], [94mLoss[0m : 2.18398
[1mStep[0m  [105/213], [94mLoss[0m : 2.91172
[1mStep[0m  [126/213], [94mLoss[0m : 2.57436
[1mStep[0m  [147/213], [94mLoss[0m : 3.00211
[1mStep[0m  [168/213], [94mLoss[0m : 2.42796
[1mStep[0m  [189/213], [94mLoss[0m : 2.32796
[1mStep[0m  [210/213], [94mLoss[0m : 2.40283

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.471, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58052
[1mStep[0m  [21/213], [94mLoss[0m : 2.74365
[1mStep[0m  [42/213], [94mLoss[0m : 2.14193
[1mStep[0m  [63/213], [94mLoss[0m : 2.74917
[1mStep[0m  [84/213], [94mLoss[0m : 2.02654
[1mStep[0m  [105/213], [94mLoss[0m : 2.25422
[1mStep[0m  [126/213], [94mLoss[0m : 2.20654
[1mStep[0m  [147/213], [94mLoss[0m : 2.52059
[1mStep[0m  [168/213], [94mLoss[0m : 2.73613
[1mStep[0m  [189/213], [94mLoss[0m : 2.39538
[1mStep[0m  [210/213], [94mLoss[0m : 2.74184

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.469, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17864
[1mStep[0m  [21/213], [94mLoss[0m : 2.22752
[1mStep[0m  [42/213], [94mLoss[0m : 2.23650
[1mStep[0m  [63/213], [94mLoss[0m : 2.77446
[1mStep[0m  [84/213], [94mLoss[0m : 3.14519
[1mStep[0m  [105/213], [94mLoss[0m : 2.44290
[1mStep[0m  [126/213], [94mLoss[0m : 2.40441
[1mStep[0m  [147/213], [94mLoss[0m : 2.43156
[1mStep[0m  [168/213], [94mLoss[0m : 2.53426
[1mStep[0m  [189/213], [94mLoss[0m : 2.36415
[1mStep[0m  [210/213], [94mLoss[0m : 2.47122

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.468, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.68675
[1mStep[0m  [21/213], [94mLoss[0m : 2.55741
[1mStep[0m  [42/213], [94mLoss[0m : 2.69040
[1mStep[0m  [63/213], [94mLoss[0m : 2.14223
[1mStep[0m  [84/213], [94mLoss[0m : 2.30938
[1mStep[0m  [105/213], [94mLoss[0m : 2.37768
[1mStep[0m  [126/213], [94mLoss[0m : 2.73213
[1mStep[0m  [147/213], [94mLoss[0m : 2.72017
[1mStep[0m  [168/213], [94mLoss[0m : 2.40025
[1mStep[0m  [189/213], [94mLoss[0m : 2.97974
[1mStep[0m  [210/213], [94mLoss[0m : 2.60860

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.460, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43275
[1mStep[0m  [21/213], [94mLoss[0m : 2.25704
[1mStep[0m  [42/213], [94mLoss[0m : 2.44877
[1mStep[0m  [63/213], [94mLoss[0m : 2.63635
[1mStep[0m  [84/213], [94mLoss[0m : 2.57212
[1mStep[0m  [105/213], [94mLoss[0m : 2.07888
[1mStep[0m  [126/213], [94mLoss[0m : 2.30455
[1mStep[0m  [147/213], [94mLoss[0m : 2.07809
[1mStep[0m  [168/213], [94mLoss[0m : 2.77389
[1mStep[0m  [189/213], [94mLoss[0m : 2.80944
[1mStep[0m  [210/213], [94mLoss[0m : 2.62010

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.461, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36769
[1mStep[0m  [21/213], [94mLoss[0m : 2.80498
[1mStep[0m  [42/213], [94mLoss[0m : 2.22366
[1mStep[0m  [63/213], [94mLoss[0m : 2.44545
[1mStep[0m  [84/213], [94mLoss[0m : 2.84927
[1mStep[0m  [105/213], [94mLoss[0m : 3.10533
[1mStep[0m  [126/213], [94mLoss[0m : 2.39941
[1mStep[0m  [147/213], [94mLoss[0m : 2.43289
[1mStep[0m  [168/213], [94mLoss[0m : 2.80151
[1mStep[0m  [189/213], [94mLoss[0m : 2.74097
[1mStep[0m  [210/213], [94mLoss[0m : 2.56265

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.37624
[1mStep[0m  [21/213], [94mLoss[0m : 2.53102
[1mStep[0m  [42/213], [94mLoss[0m : 2.51718
[1mStep[0m  [63/213], [94mLoss[0m : 2.56502
[1mStep[0m  [84/213], [94mLoss[0m : 2.63026
[1mStep[0m  [105/213], [94mLoss[0m : 2.64506
[1mStep[0m  [126/213], [94mLoss[0m : 2.58755
[1mStep[0m  [147/213], [94mLoss[0m : 2.48951
[1mStep[0m  [168/213], [94mLoss[0m : 2.77661
[1mStep[0m  [189/213], [94mLoss[0m : 2.34388
[1mStep[0m  [210/213], [94mLoss[0m : 2.25479

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.456, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17719
[1mStep[0m  [21/213], [94mLoss[0m : 2.39627
[1mStep[0m  [42/213], [94mLoss[0m : 2.54631
[1mStep[0m  [63/213], [94mLoss[0m : 2.17857
[1mStep[0m  [84/213], [94mLoss[0m : 2.75108
[1mStep[0m  [105/213], [94mLoss[0m : 2.59368
[1mStep[0m  [126/213], [94mLoss[0m : 2.37537
[1mStep[0m  [147/213], [94mLoss[0m : 2.89409
[1mStep[0m  [168/213], [94mLoss[0m : 3.13443
[1mStep[0m  [189/213], [94mLoss[0m : 2.65421
[1mStep[0m  [210/213], [94mLoss[0m : 2.36829

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.456, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52609
[1mStep[0m  [21/213], [94mLoss[0m : 2.18727
[1mStep[0m  [42/213], [94mLoss[0m : 2.55900
[1mStep[0m  [63/213], [94mLoss[0m : 2.81336
[1mStep[0m  [84/213], [94mLoss[0m : 1.96927
[1mStep[0m  [105/213], [94mLoss[0m : 2.74228
[1mStep[0m  [126/213], [94mLoss[0m : 2.83764
[1mStep[0m  [147/213], [94mLoss[0m : 2.34854
[1mStep[0m  [168/213], [94mLoss[0m : 2.28625
[1mStep[0m  [189/213], [94mLoss[0m : 2.65438
[1mStep[0m  [210/213], [94mLoss[0m : 2.40283

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48502
[1mStep[0m  [21/213], [94mLoss[0m : 2.54534
[1mStep[0m  [42/213], [94mLoss[0m : 2.57424
[1mStep[0m  [63/213], [94mLoss[0m : 2.61100
[1mStep[0m  [84/213], [94mLoss[0m : 3.21248
[1mStep[0m  [105/213], [94mLoss[0m : 2.69897
[1mStep[0m  [126/213], [94mLoss[0m : 2.55698
[1mStep[0m  [147/213], [94mLoss[0m : 2.23894
[1mStep[0m  [168/213], [94mLoss[0m : 2.22478
[1mStep[0m  [189/213], [94mLoss[0m : 2.15458
[1mStep[0m  [210/213], [94mLoss[0m : 2.60263

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.452, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23977
[1mStep[0m  [21/213], [94mLoss[0m : 2.46365
[1mStep[0m  [42/213], [94mLoss[0m : 2.83749
[1mStep[0m  [63/213], [94mLoss[0m : 2.09493
[1mStep[0m  [84/213], [94mLoss[0m : 2.22850
[1mStep[0m  [105/213], [94mLoss[0m : 2.21348
[1mStep[0m  [126/213], [94mLoss[0m : 2.32844
[1mStep[0m  [147/213], [94mLoss[0m : 2.20329
[1mStep[0m  [168/213], [94mLoss[0m : 2.44037
[1mStep[0m  [189/213], [94mLoss[0m : 2.78705
[1mStep[0m  [210/213], [94mLoss[0m : 2.76239

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.450, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.447
====================================

Phase 2 - Evaluation MAE:  2.447464443602652
MAE score P1       3.080988
MAE score P2       2.447464
loss               2.531568
learning_rate        0.0001
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 11.25314
[1mStep[0m  [21/213], [94mLoss[0m : 11.24046
[1mStep[0m  [42/213], [94mLoss[0m : 10.94763
[1mStep[0m  [63/213], [94mLoss[0m : 11.43133
[1mStep[0m  [84/213], [94mLoss[0m : 10.98238
[1mStep[0m  [105/213], [94mLoss[0m : 9.95950
[1mStep[0m  [126/213], [94mLoss[0m : 11.13167
[1mStep[0m  [147/213], [94mLoss[0m : 10.13170
[1mStep[0m  [168/213], [94mLoss[0m : 10.57444
[1mStep[0m  [189/213], [94mLoss[0m : 10.97002
[1mStep[0m  [210/213], [94mLoss[0m : 10.62361

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.793, [92mTest[0m: 10.815, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 11.23470
[1mStep[0m  [21/213], [94mLoss[0m : 10.32932
[1mStep[0m  [42/213], [94mLoss[0m : 10.13075
[1mStep[0m  [63/213], [94mLoss[0m : 10.07718
[1mStep[0m  [84/213], [94mLoss[0m : 11.02781
[1mStep[0m  [105/213], [94mLoss[0m : 9.33776
[1mStep[0m  [126/213], [94mLoss[0m : 12.21002
[1mStep[0m  [147/213], [94mLoss[0m : 10.63602
[1mStep[0m  [168/213], [94mLoss[0m : 10.92020
[1mStep[0m  [189/213], [94mLoss[0m : 10.24718
[1mStep[0m  [210/213], [94mLoss[0m : 10.72409

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.741, [92mTest[0m: 10.751, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.75360
[1mStep[0m  [21/213], [94mLoss[0m : 9.94878
[1mStep[0m  [42/213], [94mLoss[0m : 10.94587
[1mStep[0m  [63/213], [94mLoss[0m : 10.82835
[1mStep[0m  [84/213], [94mLoss[0m : 10.97583
[1mStep[0m  [105/213], [94mLoss[0m : 10.33818
[1mStep[0m  [126/213], [94mLoss[0m : 10.05593
[1mStep[0m  [147/213], [94mLoss[0m : 10.83210
[1mStep[0m  [168/213], [94mLoss[0m : 10.10764
[1mStep[0m  [189/213], [94mLoss[0m : 10.55657
[1mStep[0m  [210/213], [94mLoss[0m : 10.65245

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.689, [92mTest[0m: 10.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.52568
[1mStep[0m  [21/213], [94mLoss[0m : 10.76957
[1mStep[0m  [42/213], [94mLoss[0m : 10.20696
[1mStep[0m  [63/213], [94mLoss[0m : 10.32540
[1mStep[0m  [84/213], [94mLoss[0m : 10.97481
[1mStep[0m  [105/213], [94mLoss[0m : 11.47317
[1mStep[0m  [126/213], [94mLoss[0m : 10.80591
[1mStep[0m  [147/213], [94mLoss[0m : 9.83843
[1mStep[0m  [168/213], [94mLoss[0m : 9.39355
[1mStep[0m  [189/213], [94mLoss[0m : 10.75904
[1mStep[0m  [210/213], [94mLoss[0m : 9.64419

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.642, [92mTest[0m: 10.592, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.89828
[1mStep[0m  [21/213], [94mLoss[0m : 11.70421
[1mStep[0m  [42/213], [94mLoss[0m : 10.57422
[1mStep[0m  [63/213], [94mLoss[0m : 10.80796
[1mStep[0m  [84/213], [94mLoss[0m : 10.16791
[1mStep[0m  [105/213], [94mLoss[0m : 10.59418
[1mStep[0m  [126/213], [94mLoss[0m : 9.91098
[1mStep[0m  [147/213], [94mLoss[0m : 10.92850
[1mStep[0m  [168/213], [94mLoss[0m : 10.68880
[1mStep[0m  [189/213], [94mLoss[0m : 10.25191
[1mStep[0m  [210/213], [94mLoss[0m : 10.22320

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.591, [92mTest[0m: 10.534, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.34728
[1mStep[0m  [21/213], [94mLoss[0m : 10.32508
[1mStep[0m  [42/213], [94mLoss[0m : 10.76963
[1mStep[0m  [63/213], [94mLoss[0m : 10.37218
[1mStep[0m  [84/213], [94mLoss[0m : 10.61855
[1mStep[0m  [105/213], [94mLoss[0m : 11.17832
[1mStep[0m  [126/213], [94mLoss[0m : 10.05840
[1mStep[0m  [147/213], [94mLoss[0m : 10.20778
[1mStep[0m  [168/213], [94mLoss[0m : 10.74845
[1mStep[0m  [189/213], [94mLoss[0m : 10.28329
[1mStep[0m  [210/213], [94mLoss[0m : 11.12223

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.539, [92mTest[0m: 10.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.60051
[1mStep[0m  [21/213], [94mLoss[0m : 10.39944
[1mStep[0m  [42/213], [94mLoss[0m : 11.02667
[1mStep[0m  [63/213], [94mLoss[0m : 10.65435
[1mStep[0m  [84/213], [94mLoss[0m : 11.00680
[1mStep[0m  [105/213], [94mLoss[0m : 10.33617
[1mStep[0m  [126/213], [94mLoss[0m : 11.00238
[1mStep[0m  [147/213], [94mLoss[0m : 9.91292
[1mStep[0m  [168/213], [94mLoss[0m : 10.09725
[1mStep[0m  [189/213], [94mLoss[0m : 10.38614
[1mStep[0m  [210/213], [94mLoss[0m : 10.74091

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.492, [92mTest[0m: 10.452, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.06417
[1mStep[0m  [21/213], [94mLoss[0m : 10.64747
[1mStep[0m  [42/213], [94mLoss[0m : 10.39885
[1mStep[0m  [63/213], [94mLoss[0m : 10.02370
[1mStep[0m  [84/213], [94mLoss[0m : 10.09109
[1mStep[0m  [105/213], [94mLoss[0m : 11.13475
[1mStep[0m  [126/213], [94mLoss[0m : 10.26821
[1mStep[0m  [147/213], [94mLoss[0m : 10.60456
[1mStep[0m  [168/213], [94mLoss[0m : 10.48039
[1mStep[0m  [189/213], [94mLoss[0m : 11.21169
[1mStep[0m  [210/213], [94mLoss[0m : 10.92193

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.437, [92mTest[0m: 10.354, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 11.06456
[1mStep[0m  [21/213], [94mLoss[0m : 10.84677
[1mStep[0m  [42/213], [94mLoss[0m : 10.48945
[1mStep[0m  [63/213], [94mLoss[0m : 9.43431
[1mStep[0m  [84/213], [94mLoss[0m : 10.94006
[1mStep[0m  [105/213], [94mLoss[0m : 10.58303
[1mStep[0m  [126/213], [94mLoss[0m : 10.29090
[1mStep[0m  [147/213], [94mLoss[0m : 11.13152
[1mStep[0m  [168/213], [94mLoss[0m : 11.23124
[1mStep[0m  [189/213], [94mLoss[0m : 10.71996
[1mStep[0m  [210/213], [94mLoss[0m : 9.62298

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.391, [92mTest[0m: 10.302, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.28636
[1mStep[0m  [21/213], [94mLoss[0m : 10.16555
[1mStep[0m  [42/213], [94mLoss[0m : 10.92980
[1mStep[0m  [63/213], [94mLoss[0m : 10.48309
[1mStep[0m  [84/213], [94mLoss[0m : 10.08455
[1mStep[0m  [105/213], [94mLoss[0m : 10.29780
[1mStep[0m  [126/213], [94mLoss[0m : 9.66878
[1mStep[0m  [147/213], [94mLoss[0m : 10.65761
[1mStep[0m  [168/213], [94mLoss[0m : 10.05845
[1mStep[0m  [189/213], [94mLoss[0m : 9.83517
[1mStep[0m  [210/213], [94mLoss[0m : 10.60031

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.341, [92mTest[0m: 10.239, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.09790
[1mStep[0m  [21/213], [94mLoss[0m : 10.93271
[1mStep[0m  [42/213], [94mLoss[0m : 10.14237
[1mStep[0m  [63/213], [94mLoss[0m : 10.21199
[1mStep[0m  [84/213], [94mLoss[0m : 9.49336
[1mStep[0m  [105/213], [94mLoss[0m : 10.36378
[1mStep[0m  [126/213], [94mLoss[0m : 9.26930
[1mStep[0m  [147/213], [94mLoss[0m : 10.47536
[1mStep[0m  [168/213], [94mLoss[0m : 10.32295
[1mStep[0m  [189/213], [94mLoss[0m : 10.25858
[1mStep[0m  [210/213], [94mLoss[0m : 11.46461

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.288, [92mTest[0m: 10.204, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.07389
[1mStep[0m  [21/213], [94mLoss[0m : 10.08894
[1mStep[0m  [42/213], [94mLoss[0m : 11.21667
[1mStep[0m  [63/213], [94mLoss[0m : 10.21416
[1mStep[0m  [84/213], [94mLoss[0m : 9.45843
[1mStep[0m  [105/213], [94mLoss[0m : 11.05956
[1mStep[0m  [126/213], [94mLoss[0m : 10.02067
[1mStep[0m  [147/213], [94mLoss[0m : 10.44039
[1mStep[0m  [168/213], [94mLoss[0m : 9.84009
[1mStep[0m  [189/213], [94mLoss[0m : 10.16371
[1mStep[0m  [210/213], [94mLoss[0m : 10.14103

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.234, [92mTest[0m: 10.087, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.23001
[1mStep[0m  [21/213], [94mLoss[0m : 10.56257
[1mStep[0m  [42/213], [94mLoss[0m : 10.11739
[1mStep[0m  [63/213], [94mLoss[0m : 9.86214
[1mStep[0m  [84/213], [94mLoss[0m : 10.13173
[1mStep[0m  [105/213], [94mLoss[0m : 10.84930
[1mStep[0m  [126/213], [94mLoss[0m : 9.48917
[1mStep[0m  [147/213], [94mLoss[0m : 9.67248
[1mStep[0m  [168/213], [94mLoss[0m : 9.82515
[1mStep[0m  [189/213], [94mLoss[0m : 9.84263
[1mStep[0m  [210/213], [94mLoss[0m : 10.37415

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.182, [92mTest[0m: 10.057, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.59483
[1mStep[0m  [21/213], [94mLoss[0m : 9.73152
[1mStep[0m  [42/213], [94mLoss[0m : 9.71440
[1mStep[0m  [63/213], [94mLoss[0m : 10.45058
[1mStep[0m  [84/213], [94mLoss[0m : 10.45099
[1mStep[0m  [105/213], [94mLoss[0m : 10.52254
[1mStep[0m  [126/213], [94mLoss[0m : 9.80360
[1mStep[0m  [147/213], [94mLoss[0m : 10.35870
[1mStep[0m  [168/213], [94mLoss[0m : 11.20378
[1mStep[0m  [189/213], [94mLoss[0m : 10.74906
[1mStep[0m  [210/213], [94mLoss[0m : 10.00064

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.132, [92mTest[0m: 9.990, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.51266
[1mStep[0m  [21/213], [94mLoss[0m : 9.83303
[1mStep[0m  [42/213], [94mLoss[0m : 9.56643
[1mStep[0m  [63/213], [94mLoss[0m : 9.62102
[1mStep[0m  [84/213], [94mLoss[0m : 10.38533
[1mStep[0m  [105/213], [94mLoss[0m : 10.46980
[1mStep[0m  [126/213], [94mLoss[0m : 10.21375
[1mStep[0m  [147/213], [94mLoss[0m : 9.48744
[1mStep[0m  [168/213], [94mLoss[0m : 10.30502
[1mStep[0m  [189/213], [94mLoss[0m : 9.91586
[1mStep[0m  [210/213], [94mLoss[0m : 10.14783

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.084, [92mTest[0m: 9.903, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.58733
[1mStep[0m  [21/213], [94mLoss[0m : 11.26224
[1mStep[0m  [42/213], [94mLoss[0m : 10.04214
[1mStep[0m  [63/213], [94mLoss[0m : 10.08100
[1mStep[0m  [84/213], [94mLoss[0m : 10.89424
[1mStep[0m  [105/213], [94mLoss[0m : 9.90140
[1mStep[0m  [126/213], [94mLoss[0m : 10.14649
[1mStep[0m  [147/213], [94mLoss[0m : 10.34682
[1mStep[0m  [168/213], [94mLoss[0m : 9.79198
[1mStep[0m  [189/213], [94mLoss[0m : 9.94514
[1mStep[0m  [210/213], [94mLoss[0m : 9.42329

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.028, [92mTest[0m: 9.851, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.62833
[1mStep[0m  [21/213], [94mLoss[0m : 10.09600
[1mStep[0m  [42/213], [94mLoss[0m : 10.22582
[1mStep[0m  [63/213], [94mLoss[0m : 9.85205
[1mStep[0m  [84/213], [94mLoss[0m : 10.55874
[1mStep[0m  [105/213], [94mLoss[0m : 10.09692
[1mStep[0m  [126/213], [94mLoss[0m : 10.26000
[1mStep[0m  [147/213], [94mLoss[0m : 9.66709
[1mStep[0m  [168/213], [94mLoss[0m : 10.10287
[1mStep[0m  [189/213], [94mLoss[0m : 10.14099
[1mStep[0m  [210/213], [94mLoss[0m : 10.68512

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.972, [92mTest[0m: 9.815, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.52409
[1mStep[0m  [21/213], [94mLoss[0m : 9.72060
[1mStep[0m  [42/213], [94mLoss[0m : 10.06142
[1mStep[0m  [63/213], [94mLoss[0m : 9.83896
[1mStep[0m  [84/213], [94mLoss[0m : 9.80390
[1mStep[0m  [105/213], [94mLoss[0m : 9.44094
[1mStep[0m  [126/213], [94mLoss[0m : 9.46688
[1mStep[0m  [147/213], [94mLoss[0m : 9.66456
[1mStep[0m  [168/213], [94mLoss[0m : 9.58779
[1mStep[0m  [189/213], [94mLoss[0m : 10.77232
[1mStep[0m  [210/213], [94mLoss[0m : 10.21677

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.921, [92mTest[0m: 9.760, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.20174
[1mStep[0m  [21/213], [94mLoss[0m : 9.46306
[1mStep[0m  [42/213], [94mLoss[0m : 10.74634
[1mStep[0m  [63/213], [94mLoss[0m : 9.80758
[1mStep[0m  [84/213], [94mLoss[0m : 10.24750
[1mStep[0m  [105/213], [94mLoss[0m : 10.08492
[1mStep[0m  [126/213], [94mLoss[0m : 10.22025
[1mStep[0m  [147/213], [94mLoss[0m : 10.79462
[1mStep[0m  [168/213], [94mLoss[0m : 10.18706
[1mStep[0m  [189/213], [94mLoss[0m : 9.50634
[1mStep[0m  [210/213], [94mLoss[0m : 10.59094

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.871, [92mTest[0m: 9.699, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.07203
[1mStep[0m  [21/213], [94mLoss[0m : 10.05195
[1mStep[0m  [42/213], [94mLoss[0m : 10.44468
[1mStep[0m  [63/213], [94mLoss[0m : 9.52116
[1mStep[0m  [84/213], [94mLoss[0m : 10.01375
[1mStep[0m  [105/213], [94mLoss[0m : 9.65681
[1mStep[0m  [126/213], [94mLoss[0m : 9.52088
[1mStep[0m  [147/213], [94mLoss[0m : 9.77515
[1mStep[0m  [168/213], [94mLoss[0m : 9.66547
[1mStep[0m  [189/213], [94mLoss[0m : 9.97108
[1mStep[0m  [210/213], [94mLoss[0m : 9.81404

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.813, [92mTest[0m: 9.676, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.60073
[1mStep[0m  [21/213], [94mLoss[0m : 10.01987
[1mStep[0m  [42/213], [94mLoss[0m : 10.17257
[1mStep[0m  [63/213], [94mLoss[0m : 9.35071
[1mStep[0m  [84/213], [94mLoss[0m : 10.64397
[1mStep[0m  [105/213], [94mLoss[0m : 9.21680
[1mStep[0m  [126/213], [94mLoss[0m : 9.40512
[1mStep[0m  [147/213], [94mLoss[0m : 10.17566
[1mStep[0m  [168/213], [94mLoss[0m : 9.29298
[1mStep[0m  [189/213], [94mLoss[0m : 9.08667
[1mStep[0m  [210/213], [94mLoss[0m : 9.48719

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.758, [92mTest[0m: 9.605, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.55628
[1mStep[0m  [21/213], [94mLoss[0m : 9.36085
[1mStep[0m  [42/213], [94mLoss[0m : 9.65614
[1mStep[0m  [63/213], [94mLoss[0m : 9.58837
[1mStep[0m  [84/213], [94mLoss[0m : 8.99177
[1mStep[0m  [105/213], [94mLoss[0m : 9.57102
[1mStep[0m  [126/213], [94mLoss[0m : 9.57352
[1mStep[0m  [147/213], [94mLoss[0m : 8.91577
[1mStep[0m  [168/213], [94mLoss[0m : 9.86912
[1mStep[0m  [189/213], [94mLoss[0m : 9.12070
[1mStep[0m  [210/213], [94mLoss[0m : 9.30382

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.711, [92mTest[0m: 9.504, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.72946
[1mStep[0m  [21/213], [94mLoss[0m : 9.43683
[1mStep[0m  [42/213], [94mLoss[0m : 10.72839
[1mStep[0m  [63/213], [94mLoss[0m : 9.87734
[1mStep[0m  [84/213], [94mLoss[0m : 10.14398
[1mStep[0m  [105/213], [94mLoss[0m : 9.45449
[1mStep[0m  [126/213], [94mLoss[0m : 9.48259
[1mStep[0m  [147/213], [94mLoss[0m : 9.27710
[1mStep[0m  [168/213], [94mLoss[0m : 9.87316
[1mStep[0m  [189/213], [94mLoss[0m : 9.26083
[1mStep[0m  [210/213], [94mLoss[0m : 10.40037

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.655, [92mTest[0m: 9.437, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.62578
[1mStep[0m  [21/213], [94mLoss[0m : 9.65820
[1mStep[0m  [42/213], [94mLoss[0m : 9.27375
[1mStep[0m  [63/213], [94mLoss[0m : 8.73795
[1mStep[0m  [84/213], [94mLoss[0m : 9.77978
[1mStep[0m  [105/213], [94mLoss[0m : 9.76481
[1mStep[0m  [126/213], [94mLoss[0m : 10.31413
[1mStep[0m  [147/213], [94mLoss[0m : 10.30674
[1mStep[0m  [168/213], [94mLoss[0m : 9.30383
[1mStep[0m  [189/213], [94mLoss[0m : 9.84688
[1mStep[0m  [210/213], [94mLoss[0m : 9.63503

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.599, [92mTest[0m: 9.382, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.24065
[1mStep[0m  [21/213], [94mLoss[0m : 10.10565
[1mStep[0m  [42/213], [94mLoss[0m : 10.39849
[1mStep[0m  [63/213], [94mLoss[0m : 9.76870
[1mStep[0m  [84/213], [94mLoss[0m : 8.75109
[1mStep[0m  [105/213], [94mLoss[0m : 8.87178
[1mStep[0m  [126/213], [94mLoss[0m : 9.45538
[1mStep[0m  [147/213], [94mLoss[0m : 10.23152
[1mStep[0m  [168/213], [94mLoss[0m : 10.10697
[1mStep[0m  [189/213], [94mLoss[0m : 10.19259
[1mStep[0m  [210/213], [94mLoss[0m : 9.46355

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.548, [92mTest[0m: 9.347, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.72629
[1mStep[0m  [21/213], [94mLoss[0m : 9.24266
[1mStep[0m  [42/213], [94mLoss[0m : 8.72687
[1mStep[0m  [63/213], [94mLoss[0m : 8.65662
[1mStep[0m  [84/213], [94mLoss[0m : 8.90930
[1mStep[0m  [105/213], [94mLoss[0m : 9.27725
[1mStep[0m  [126/213], [94mLoss[0m : 10.58687
[1mStep[0m  [147/213], [94mLoss[0m : 9.57867
[1mStep[0m  [168/213], [94mLoss[0m : 9.64608
[1mStep[0m  [189/213], [94mLoss[0m : 9.52154
[1mStep[0m  [210/213], [94mLoss[0m : 8.74241

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.495, [92mTest[0m: 9.287, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.63065
[1mStep[0m  [21/213], [94mLoss[0m : 9.00566
[1mStep[0m  [42/213], [94mLoss[0m : 9.16452
[1mStep[0m  [63/213], [94mLoss[0m : 9.24823
[1mStep[0m  [84/213], [94mLoss[0m : 9.34267
[1mStep[0m  [105/213], [94mLoss[0m : 9.16660
[1mStep[0m  [126/213], [94mLoss[0m : 9.53035
[1mStep[0m  [147/213], [94mLoss[0m : 8.74787
[1mStep[0m  [168/213], [94mLoss[0m : 8.70881
[1mStep[0m  [189/213], [94mLoss[0m : 10.48278
[1mStep[0m  [210/213], [94mLoss[0m : 9.14036

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.439, [92mTest[0m: 9.209, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.09711
[1mStep[0m  [21/213], [94mLoss[0m : 9.16416
[1mStep[0m  [42/213], [94mLoss[0m : 9.05594
[1mStep[0m  [63/213], [94mLoss[0m : 9.36289
[1mStep[0m  [84/213], [94mLoss[0m : 9.02025
[1mStep[0m  [105/213], [94mLoss[0m : 8.43500
[1mStep[0m  [126/213], [94mLoss[0m : 10.20390
[1mStep[0m  [147/213], [94mLoss[0m : 9.85453
[1mStep[0m  [168/213], [94mLoss[0m : 9.81120
[1mStep[0m  [189/213], [94mLoss[0m : 10.08456
[1mStep[0m  [210/213], [94mLoss[0m : 9.70288

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.381, [92mTest[0m: 9.148, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.41457
[1mStep[0m  [21/213], [94mLoss[0m : 9.23387
[1mStep[0m  [42/213], [94mLoss[0m : 8.61274
[1mStep[0m  [63/213], [94mLoss[0m : 9.14575
[1mStep[0m  [84/213], [94mLoss[0m : 9.70658
[1mStep[0m  [105/213], [94mLoss[0m : 8.92155
[1mStep[0m  [126/213], [94mLoss[0m : 9.11142
[1mStep[0m  [147/213], [94mLoss[0m : 9.60606
[1mStep[0m  [168/213], [94mLoss[0m : 9.74528
[1mStep[0m  [189/213], [94mLoss[0m : 9.25011
[1mStep[0m  [210/213], [94mLoss[0m : 9.89793

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.325, [92mTest[0m: 8.976, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.66932
[1mStep[0m  [21/213], [94mLoss[0m : 9.46979
[1mStep[0m  [42/213], [94mLoss[0m : 9.58076
[1mStep[0m  [63/213], [94mLoss[0m : 9.23600
[1mStep[0m  [84/213], [94mLoss[0m : 8.61673
[1mStep[0m  [105/213], [94mLoss[0m : 9.29767
[1mStep[0m  [126/213], [94mLoss[0m : 8.45295
[1mStep[0m  [147/213], [94mLoss[0m : 9.57081
[1mStep[0m  [168/213], [94mLoss[0m : 9.72014
[1mStep[0m  [189/213], [94mLoss[0m : 9.36908
[1mStep[0m  [210/213], [94mLoss[0m : 9.58803

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.267, [92mTest[0m: 8.996, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.848
====================================

Phase 1 - Evaluation MAE:  8.847618354941314
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 8.94707
[1mStep[0m  [21/213], [94mLoss[0m : 9.05194
[1mStep[0m  [42/213], [94mLoss[0m : 10.01140
[1mStep[0m  [63/213], [94mLoss[0m : 9.49324
[1mStep[0m  [84/213], [94mLoss[0m : 9.46414
[1mStep[0m  [105/213], [94mLoss[0m : 8.69552
[1mStep[0m  [126/213], [94mLoss[0m : 9.02838
[1mStep[0m  [147/213], [94mLoss[0m : 8.41546
[1mStep[0m  [168/213], [94mLoss[0m : 8.90927
[1mStep[0m  [189/213], [94mLoss[0m : 9.05261
[1mStep[0m  [210/213], [94mLoss[0m : 8.71522

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.200, [92mTest[0m: 8.850, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.96719
[1mStep[0m  [21/213], [94mLoss[0m : 8.83602
[1mStep[0m  [42/213], [94mLoss[0m : 9.04081
[1mStep[0m  [63/213], [94mLoss[0m : 9.64608
[1mStep[0m  [84/213], [94mLoss[0m : 8.45319
[1mStep[0m  [105/213], [94mLoss[0m : 9.53051
[1mStep[0m  [126/213], [94mLoss[0m : 8.99832
[1mStep[0m  [147/213], [94mLoss[0m : 8.59869
[1mStep[0m  [168/213], [94mLoss[0m : 9.95856
[1mStep[0m  [189/213], [94mLoss[0m : 9.11107
[1mStep[0m  [210/213], [94mLoss[0m : 8.69581

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.108, [92mTest[0m: 8.833, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.17489
[1mStep[0m  [21/213], [94mLoss[0m : 9.32394
[1mStep[0m  [42/213], [94mLoss[0m : 9.00050
[1mStep[0m  [63/213], [94mLoss[0m : 8.68419
[1mStep[0m  [84/213], [94mLoss[0m : 9.24764
[1mStep[0m  [105/213], [94mLoss[0m : 9.17630
[1mStep[0m  [126/213], [94mLoss[0m : 8.83646
[1mStep[0m  [147/213], [94mLoss[0m : 8.65105
[1mStep[0m  [168/213], [94mLoss[0m : 9.08612
[1mStep[0m  [189/213], [94mLoss[0m : 8.97652
[1mStep[0m  [210/213], [94mLoss[0m : 9.10537

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.035, [92mTest[0m: 8.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.81122
[1mStep[0m  [21/213], [94mLoss[0m : 8.77783
[1mStep[0m  [42/213], [94mLoss[0m : 9.22478
[1mStep[0m  [63/213], [94mLoss[0m : 8.36884
[1mStep[0m  [84/213], [94mLoss[0m : 9.62787
[1mStep[0m  [105/213], [94mLoss[0m : 8.57895
[1mStep[0m  [126/213], [94mLoss[0m : 8.27341
[1mStep[0m  [147/213], [94mLoss[0m : 9.68979
[1mStep[0m  [168/213], [94mLoss[0m : 9.30663
[1mStep[0m  [189/213], [94mLoss[0m : 9.39945
[1mStep[0m  [210/213], [94mLoss[0m : 7.86016

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.943, [92mTest[0m: 8.575, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.60835
[1mStep[0m  [21/213], [94mLoss[0m : 10.13388
[1mStep[0m  [42/213], [94mLoss[0m : 8.61479
[1mStep[0m  [63/213], [94mLoss[0m : 8.23053
[1mStep[0m  [84/213], [94mLoss[0m : 9.34241
[1mStep[0m  [105/213], [94mLoss[0m : 8.19087
[1mStep[0m  [126/213], [94mLoss[0m : 9.00191
[1mStep[0m  [147/213], [94mLoss[0m : 8.87600
[1mStep[0m  [168/213], [94mLoss[0m : 8.39974
[1mStep[0m  [189/213], [94mLoss[0m : 8.13747
[1mStep[0m  [210/213], [94mLoss[0m : 8.50423

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.862, [92mTest[0m: 8.441, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.97918
[1mStep[0m  [21/213], [94mLoss[0m : 8.48279
[1mStep[0m  [42/213], [94mLoss[0m : 8.50417
[1mStep[0m  [63/213], [94mLoss[0m : 8.27723
[1mStep[0m  [84/213], [94mLoss[0m : 8.35077
[1mStep[0m  [105/213], [94mLoss[0m : 9.38781
[1mStep[0m  [126/213], [94mLoss[0m : 9.69134
[1mStep[0m  [147/213], [94mLoss[0m : 8.79146
[1mStep[0m  [168/213], [94mLoss[0m : 8.11190
[1mStep[0m  [189/213], [94mLoss[0m : 8.95452
[1mStep[0m  [210/213], [94mLoss[0m : 8.43073

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.777, [92mTest[0m: 8.298, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.97751
[1mStep[0m  [21/213], [94mLoss[0m : 8.21005
[1mStep[0m  [42/213], [94mLoss[0m : 8.86525
[1mStep[0m  [63/213], [94mLoss[0m : 8.93503
[1mStep[0m  [84/213], [94mLoss[0m : 8.35855
[1mStep[0m  [105/213], [94mLoss[0m : 8.86885
[1mStep[0m  [126/213], [94mLoss[0m : 8.48321
[1mStep[0m  [147/213], [94mLoss[0m : 8.43975
[1mStep[0m  [168/213], [94mLoss[0m : 8.31950
[1mStep[0m  [189/213], [94mLoss[0m : 8.43526
[1mStep[0m  [210/213], [94mLoss[0m : 9.45276

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.681, [92mTest[0m: 8.304, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.03318
[1mStep[0m  [21/213], [94mLoss[0m : 8.94975
[1mStep[0m  [42/213], [94mLoss[0m : 8.74037
[1mStep[0m  [63/213], [94mLoss[0m : 8.91868
[1mStep[0m  [84/213], [94mLoss[0m : 9.85504
[1mStep[0m  [105/213], [94mLoss[0m : 9.28620
[1mStep[0m  [126/213], [94mLoss[0m : 8.09860
[1mStep[0m  [147/213], [94mLoss[0m : 8.87277
[1mStep[0m  [168/213], [94mLoss[0m : 8.09001
[1mStep[0m  [189/213], [94mLoss[0m : 8.38186
[1mStep[0m  [210/213], [94mLoss[0m : 7.95967

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.588, [92mTest[0m: 8.203, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.27342
[1mStep[0m  [21/213], [94mLoss[0m : 8.52349
[1mStep[0m  [42/213], [94mLoss[0m : 7.72355
[1mStep[0m  [63/213], [94mLoss[0m : 8.14498
[1mStep[0m  [84/213], [94mLoss[0m : 9.05278
[1mStep[0m  [105/213], [94mLoss[0m : 8.79972
[1mStep[0m  [126/213], [94mLoss[0m : 8.65544
[1mStep[0m  [147/213], [94mLoss[0m : 8.32314
[1mStep[0m  [168/213], [94mLoss[0m : 8.08622
[1mStep[0m  [189/213], [94mLoss[0m : 8.45440
[1mStep[0m  [210/213], [94mLoss[0m : 9.05800

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.502, [92mTest[0m: 8.066, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.14342
[1mStep[0m  [21/213], [94mLoss[0m : 7.66727
[1mStep[0m  [42/213], [94mLoss[0m : 8.42217
[1mStep[0m  [63/213], [94mLoss[0m : 8.10691
[1mStep[0m  [84/213], [94mLoss[0m : 9.61341
[1mStep[0m  [105/213], [94mLoss[0m : 8.92329
[1mStep[0m  [126/213], [94mLoss[0m : 8.29361
[1mStep[0m  [147/213], [94mLoss[0m : 8.64828
[1mStep[0m  [168/213], [94mLoss[0m : 8.68278
[1mStep[0m  [189/213], [94mLoss[0m : 7.55742
[1mStep[0m  [210/213], [94mLoss[0m : 8.02288

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.402, [92mTest[0m: 8.044, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.33984
[1mStep[0m  [21/213], [94mLoss[0m : 7.72809
[1mStep[0m  [42/213], [94mLoss[0m : 8.25926
[1mStep[0m  [63/213], [94mLoss[0m : 8.28039
[1mStep[0m  [84/213], [94mLoss[0m : 8.57467
[1mStep[0m  [105/213], [94mLoss[0m : 8.87269
[1mStep[0m  [126/213], [94mLoss[0m : 8.22623
[1mStep[0m  [147/213], [94mLoss[0m : 8.50598
[1mStep[0m  [168/213], [94mLoss[0m : 8.91130
[1mStep[0m  [189/213], [94mLoss[0m : 8.78611
[1mStep[0m  [210/213], [94mLoss[0m : 7.98418

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.286, [92mTest[0m: 7.750, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.18035
[1mStep[0m  [21/213], [94mLoss[0m : 8.44962
[1mStep[0m  [42/213], [94mLoss[0m : 8.32349
[1mStep[0m  [63/213], [94mLoss[0m : 8.09252
[1mStep[0m  [84/213], [94mLoss[0m : 7.86183
[1mStep[0m  [105/213], [94mLoss[0m : 8.39437
[1mStep[0m  [126/213], [94mLoss[0m : 8.41430
[1mStep[0m  [147/213], [94mLoss[0m : 8.23785
[1mStep[0m  [168/213], [94mLoss[0m : 7.63467
[1mStep[0m  [189/213], [94mLoss[0m : 8.53664
[1mStep[0m  [210/213], [94mLoss[0m : 8.04675

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.184, [92mTest[0m: 7.718, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.90871
[1mStep[0m  [21/213], [94mLoss[0m : 9.19362
[1mStep[0m  [42/213], [94mLoss[0m : 7.56022
[1mStep[0m  [63/213], [94mLoss[0m : 7.48651
[1mStep[0m  [84/213], [94mLoss[0m : 7.59932
[1mStep[0m  [105/213], [94mLoss[0m : 7.65410
[1mStep[0m  [126/213], [94mLoss[0m : 7.19642
[1mStep[0m  [147/213], [94mLoss[0m : 7.78854
[1mStep[0m  [168/213], [94mLoss[0m : 8.01308
[1mStep[0m  [189/213], [94mLoss[0m : 7.94642
[1mStep[0m  [210/213], [94mLoss[0m : 8.69029

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.072, [92mTest[0m: 7.535, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.65254
[1mStep[0m  [21/213], [94mLoss[0m : 8.27713
[1mStep[0m  [42/213], [94mLoss[0m : 7.28947
[1mStep[0m  [63/213], [94mLoss[0m : 7.89951
[1mStep[0m  [84/213], [94mLoss[0m : 7.65030
[1mStep[0m  [105/213], [94mLoss[0m : 8.06645
[1mStep[0m  [126/213], [94mLoss[0m : 8.33775
[1mStep[0m  [147/213], [94mLoss[0m : 7.26795
[1mStep[0m  [168/213], [94mLoss[0m : 7.93667
[1mStep[0m  [189/213], [94mLoss[0m : 8.42077
[1mStep[0m  [210/213], [94mLoss[0m : 8.08567

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.964, [92mTest[0m: 7.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.73145
[1mStep[0m  [21/213], [94mLoss[0m : 7.78239
[1mStep[0m  [42/213], [94mLoss[0m : 8.01873
[1mStep[0m  [63/213], [94mLoss[0m : 6.73441
[1mStep[0m  [84/213], [94mLoss[0m : 8.25331
[1mStep[0m  [105/213], [94mLoss[0m : 8.27927
[1mStep[0m  [126/213], [94mLoss[0m : 8.21925
[1mStep[0m  [147/213], [94mLoss[0m : 8.00119
[1mStep[0m  [168/213], [94mLoss[0m : 8.31225
[1mStep[0m  [189/213], [94mLoss[0m : 8.17994
[1mStep[0m  [210/213], [94mLoss[0m : 7.94123

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.838, [92mTest[0m: 7.362, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.09234
[1mStep[0m  [21/213], [94mLoss[0m : 8.53167
[1mStep[0m  [42/213], [94mLoss[0m : 7.88057
[1mStep[0m  [63/213], [94mLoss[0m : 6.91309
[1mStep[0m  [84/213], [94mLoss[0m : 7.35798
[1mStep[0m  [105/213], [94mLoss[0m : 7.96677
[1mStep[0m  [126/213], [94mLoss[0m : 8.24260
[1mStep[0m  [147/213], [94mLoss[0m : 7.28761
[1mStep[0m  [168/213], [94mLoss[0m : 7.75351
[1mStep[0m  [189/213], [94mLoss[0m : 8.13135
[1mStep[0m  [210/213], [94mLoss[0m : 7.78354

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.723, [92mTest[0m: 7.246, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.93220
[1mStep[0m  [21/213], [94mLoss[0m : 7.01272
[1mStep[0m  [42/213], [94mLoss[0m : 7.01930
[1mStep[0m  [63/213], [94mLoss[0m : 6.99198
[1mStep[0m  [84/213], [94mLoss[0m : 7.57369
[1mStep[0m  [105/213], [94mLoss[0m : 7.97164
[1mStep[0m  [126/213], [94mLoss[0m : 7.92969
[1mStep[0m  [147/213], [94mLoss[0m : 7.86010
[1mStep[0m  [168/213], [94mLoss[0m : 6.93115
[1mStep[0m  [189/213], [94mLoss[0m : 7.89088
[1mStep[0m  [210/213], [94mLoss[0m : 7.58935

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.601, [92mTest[0m: 6.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.43866
[1mStep[0m  [21/213], [94mLoss[0m : 7.05749
[1mStep[0m  [42/213], [94mLoss[0m : 7.93597
[1mStep[0m  [63/213], [94mLoss[0m : 7.41404
[1mStep[0m  [84/213], [94mLoss[0m : 6.96178
[1mStep[0m  [105/213], [94mLoss[0m : 7.79295
[1mStep[0m  [126/213], [94mLoss[0m : 7.57838
[1mStep[0m  [147/213], [94mLoss[0m : 7.26432
[1mStep[0m  [168/213], [94mLoss[0m : 7.13759
[1mStep[0m  [189/213], [94mLoss[0m : 7.34917
[1mStep[0m  [210/213], [94mLoss[0m : 7.42503

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.477, [92mTest[0m: 6.875, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.55456
[1mStep[0m  [21/213], [94mLoss[0m : 7.72030
[1mStep[0m  [42/213], [94mLoss[0m : 6.81496
[1mStep[0m  [63/213], [94mLoss[0m : 7.42811
[1mStep[0m  [84/213], [94mLoss[0m : 7.59076
[1mStep[0m  [105/213], [94mLoss[0m : 7.43125
[1mStep[0m  [126/213], [94mLoss[0m : 7.02119
[1mStep[0m  [147/213], [94mLoss[0m : 7.45714
[1mStep[0m  [168/213], [94mLoss[0m : 7.05828
[1mStep[0m  [189/213], [94mLoss[0m : 7.18974
[1mStep[0m  [210/213], [94mLoss[0m : 6.64886

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.351, [92mTest[0m: 6.914, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.94417
[1mStep[0m  [21/213], [94mLoss[0m : 6.69039
[1mStep[0m  [42/213], [94mLoss[0m : 7.51130
[1mStep[0m  [63/213], [94mLoss[0m : 7.12922
[1mStep[0m  [84/213], [94mLoss[0m : 7.54149
[1mStep[0m  [105/213], [94mLoss[0m : 6.30448
[1mStep[0m  [126/213], [94mLoss[0m : 7.48355
[1mStep[0m  [147/213], [94mLoss[0m : 6.84914
[1mStep[0m  [168/213], [94mLoss[0m : 7.07286
[1mStep[0m  [189/213], [94mLoss[0m : 7.07367
[1mStep[0m  [210/213], [94mLoss[0m : 6.88458

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.227, [92mTest[0m: 6.421, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.29931
[1mStep[0m  [21/213], [94mLoss[0m : 7.24122
[1mStep[0m  [42/213], [94mLoss[0m : 7.26669
[1mStep[0m  [63/213], [94mLoss[0m : 7.34609
[1mStep[0m  [84/213], [94mLoss[0m : 6.98627
[1mStep[0m  [105/213], [94mLoss[0m : 7.24214
[1mStep[0m  [126/213], [94mLoss[0m : 7.05926
[1mStep[0m  [147/213], [94mLoss[0m : 6.86109
[1mStep[0m  [168/213], [94mLoss[0m : 7.17911
[1mStep[0m  [189/213], [94mLoss[0m : 7.59612
[1mStep[0m  [210/213], [94mLoss[0m : 7.31471

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.107, [92mTest[0m: 6.601, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.58071
[1mStep[0m  [21/213], [94mLoss[0m : 7.03297
[1mStep[0m  [42/213], [94mLoss[0m : 6.76289
[1mStep[0m  [63/213], [94mLoss[0m : 7.40805
[1mStep[0m  [84/213], [94mLoss[0m : 7.27721
[1mStep[0m  [105/213], [94mLoss[0m : 8.19913
[1mStep[0m  [126/213], [94mLoss[0m : 6.83756
[1mStep[0m  [147/213], [94mLoss[0m : 7.52273
[1mStep[0m  [168/213], [94mLoss[0m : 6.68681
[1mStep[0m  [189/213], [94mLoss[0m : 6.83700
[1mStep[0m  [210/213], [94mLoss[0m : 7.06936

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.990, [92mTest[0m: 6.356, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.41702
[1mStep[0m  [21/213], [94mLoss[0m : 6.64002
[1mStep[0m  [42/213], [94mLoss[0m : 6.46219
[1mStep[0m  [63/213], [94mLoss[0m : 6.79451
[1mStep[0m  [84/213], [94mLoss[0m : 6.46173
[1mStep[0m  [105/213], [94mLoss[0m : 6.58218
[1mStep[0m  [126/213], [94mLoss[0m : 6.92045
[1mStep[0m  [147/213], [94mLoss[0m : 7.13729
[1mStep[0m  [168/213], [94mLoss[0m : 6.36482
[1mStep[0m  [189/213], [94mLoss[0m : 6.67822
[1mStep[0m  [210/213], [94mLoss[0m : 6.22259

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.889, [92mTest[0m: 6.509, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.67150
[1mStep[0m  [21/213], [94mLoss[0m : 7.68524
[1mStep[0m  [42/213], [94mLoss[0m : 7.13294
[1mStep[0m  [63/213], [94mLoss[0m : 6.92665
[1mStep[0m  [84/213], [94mLoss[0m : 6.62793
[1mStep[0m  [105/213], [94mLoss[0m : 6.90202
[1mStep[0m  [126/213], [94mLoss[0m : 6.63417
[1mStep[0m  [147/213], [94mLoss[0m : 7.17295
[1mStep[0m  [168/213], [94mLoss[0m : 6.67989
[1mStep[0m  [189/213], [94mLoss[0m : 7.30569
[1mStep[0m  [210/213], [94mLoss[0m : 6.55021

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.770, [92mTest[0m: 6.012, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.84854
[1mStep[0m  [21/213], [94mLoss[0m : 6.67381
[1mStep[0m  [42/213], [94mLoss[0m : 6.16151
[1mStep[0m  [63/213], [94mLoss[0m : 6.64688
[1mStep[0m  [84/213], [94mLoss[0m : 6.23633
[1mStep[0m  [105/213], [94mLoss[0m : 6.73766
[1mStep[0m  [126/213], [94mLoss[0m : 6.65164
[1mStep[0m  [147/213], [94mLoss[0m : 6.94159
[1mStep[0m  [168/213], [94mLoss[0m : 6.62447
[1mStep[0m  [189/213], [94mLoss[0m : 6.97636
[1mStep[0m  [210/213], [94mLoss[0m : 6.97751

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.662, [92mTest[0m: 5.982, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.48585
[1mStep[0m  [21/213], [94mLoss[0m : 6.99096
[1mStep[0m  [42/213], [94mLoss[0m : 6.65476
[1mStep[0m  [63/213], [94mLoss[0m : 6.84663
[1mStep[0m  [84/213], [94mLoss[0m : 5.73804
[1mStep[0m  [105/213], [94mLoss[0m : 7.37259
[1mStep[0m  [126/213], [94mLoss[0m : 6.97858
[1mStep[0m  [147/213], [94mLoss[0m : 5.86327
[1mStep[0m  [168/213], [94mLoss[0m : 5.96024
[1mStep[0m  [189/213], [94mLoss[0m : 6.15230
[1mStep[0m  [210/213], [94mLoss[0m : 6.19329

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.549, [92mTest[0m: 6.249, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.19734
[1mStep[0m  [21/213], [94mLoss[0m : 6.26861
[1mStep[0m  [42/213], [94mLoss[0m : 6.44045
[1mStep[0m  [63/213], [94mLoss[0m : 6.37031
[1mStep[0m  [84/213], [94mLoss[0m : 6.56294
[1mStep[0m  [105/213], [94mLoss[0m : 6.24938
[1mStep[0m  [126/213], [94mLoss[0m : 6.33274
[1mStep[0m  [147/213], [94mLoss[0m : 7.05165
[1mStep[0m  [168/213], [94mLoss[0m : 5.66866
[1mStep[0m  [189/213], [94mLoss[0m : 5.81233
[1mStep[0m  [210/213], [94mLoss[0m : 5.69926

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.456, [92mTest[0m: 6.039, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.25605
[1mStep[0m  [21/213], [94mLoss[0m : 6.20741
[1mStep[0m  [42/213], [94mLoss[0m : 6.34743
[1mStep[0m  [63/213], [94mLoss[0m : 6.33615
[1mStep[0m  [84/213], [94mLoss[0m : 7.29109
[1mStep[0m  [105/213], [94mLoss[0m : 6.32867
[1mStep[0m  [126/213], [94mLoss[0m : 5.93469
[1mStep[0m  [147/213], [94mLoss[0m : 6.25519
[1mStep[0m  [168/213], [94mLoss[0m : 6.90279
[1mStep[0m  [189/213], [94mLoss[0m : 6.60373
[1mStep[0m  [210/213], [94mLoss[0m : 6.23736

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.358, [92mTest[0m: 5.747, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.95751
[1mStep[0m  [21/213], [94mLoss[0m : 6.36142
[1mStep[0m  [42/213], [94mLoss[0m : 6.14497
[1mStep[0m  [63/213], [94mLoss[0m : 6.65826
[1mStep[0m  [84/213], [94mLoss[0m : 6.19485
[1mStep[0m  [105/213], [94mLoss[0m : 5.72006
[1mStep[0m  [126/213], [94mLoss[0m : 6.85359
[1mStep[0m  [147/213], [94mLoss[0m : 6.36692
[1mStep[0m  [168/213], [94mLoss[0m : 5.71938
[1mStep[0m  [189/213], [94mLoss[0m : 6.62004
[1mStep[0m  [210/213], [94mLoss[0m : 5.70769

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.281, [92mTest[0m: 5.400, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.13341
[1mStep[0m  [21/213], [94mLoss[0m : 5.71005
[1mStep[0m  [42/213], [94mLoss[0m : 5.87537
[1mStep[0m  [63/213], [94mLoss[0m : 6.49512
[1mStep[0m  [84/213], [94mLoss[0m : 5.80098
[1mStep[0m  [105/213], [94mLoss[0m : 5.47125
[1mStep[0m  [126/213], [94mLoss[0m : 6.52266
[1mStep[0m  [147/213], [94mLoss[0m : 6.29855
[1mStep[0m  [168/213], [94mLoss[0m : 6.01535
[1mStep[0m  [189/213], [94mLoss[0m : 6.72733
[1mStep[0m  [210/213], [94mLoss[0m : 6.39193

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.182, [92mTest[0m: 5.365, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.803
====================================

Phase 2 - Evaluation MAE:  6.802831906192708
MAE score P1       8.847618
MAE score P2       6.802832
loss               6.181611
learning_rate        0.0001
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 11.18954
[1mStep[0m  [10/106], [94mLoss[0m : 10.59837
[1mStep[0m  [20/106], [94mLoss[0m : 11.17050
[1mStep[0m  [30/106], [94mLoss[0m : 10.61577
[1mStep[0m  [40/106], [94mLoss[0m : 10.35526
[1mStep[0m  [50/106], [94mLoss[0m : 10.12428
[1mStep[0m  [60/106], [94mLoss[0m : 10.82400
[1mStep[0m  [70/106], [94mLoss[0m : 10.97585
[1mStep[0m  [80/106], [94mLoss[0m : 10.73911
[1mStep[0m  [90/106], [94mLoss[0m : 10.79307
[1mStep[0m  [100/106], [94mLoss[0m : 10.46986

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.749, [92mTest[0m: 10.758, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.36491
[1mStep[0m  [10/106], [94mLoss[0m : 10.50611
[1mStep[0m  [20/106], [94mLoss[0m : 10.93026
[1mStep[0m  [30/106], [94mLoss[0m : 10.45434
[1mStep[0m  [40/106], [94mLoss[0m : 10.47147
[1mStep[0m  [50/106], [94mLoss[0m : 10.49023
[1mStep[0m  [60/106], [94mLoss[0m : 10.49216
[1mStep[0m  [70/106], [94mLoss[0m : 10.34434
[1mStep[0m  [80/106], [94mLoss[0m : 10.10047
[1mStep[0m  [90/106], [94mLoss[0m : 10.84527
[1mStep[0m  [100/106], [94mLoss[0m : 11.03525

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.569, [92mTest[0m: 10.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.11002
[1mStep[0m  [10/106], [94mLoss[0m : 10.70701
[1mStep[0m  [20/106], [94mLoss[0m : 10.75598
[1mStep[0m  [30/106], [94mLoss[0m : 11.01037
[1mStep[0m  [40/106], [94mLoss[0m : 10.88635
[1mStep[0m  [50/106], [94mLoss[0m : 10.47376
[1mStep[0m  [60/106], [94mLoss[0m : 10.95849
[1mStep[0m  [70/106], [94mLoss[0m : 10.55302
[1mStep[0m  [80/106], [94mLoss[0m : 10.45246
[1mStep[0m  [90/106], [94mLoss[0m : 10.27374
[1mStep[0m  [100/106], [94mLoss[0m : 10.49673

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.381, [92mTest[0m: 10.543, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.30789
[1mStep[0m  [10/106], [94mLoss[0m : 10.48910
[1mStep[0m  [20/106], [94mLoss[0m : 10.49029
[1mStep[0m  [30/106], [94mLoss[0m : 10.75828
[1mStep[0m  [40/106], [94mLoss[0m : 10.68063
[1mStep[0m  [50/106], [94mLoss[0m : 10.24936
[1mStep[0m  [60/106], [94mLoss[0m : 10.94972
[1mStep[0m  [70/106], [94mLoss[0m : 10.30316
[1mStep[0m  [80/106], [94mLoss[0m : 9.79583
[1mStep[0m  [90/106], [94mLoss[0m : 9.97803
[1mStep[0m  [100/106], [94mLoss[0m : 9.94346

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.206, [92mTest[0m: 10.428, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.98902
[1mStep[0m  [10/106], [94mLoss[0m : 10.16213
[1mStep[0m  [20/106], [94mLoss[0m : 10.41904
[1mStep[0m  [30/106], [94mLoss[0m : 9.87277
[1mStep[0m  [40/106], [94mLoss[0m : 9.86423
[1mStep[0m  [50/106], [94mLoss[0m : 9.48476
[1mStep[0m  [60/106], [94mLoss[0m : 10.15336
[1mStep[0m  [70/106], [94mLoss[0m : 10.15037
[1mStep[0m  [80/106], [94mLoss[0m : 10.11659
[1mStep[0m  [90/106], [94mLoss[0m : 10.19356
[1mStep[0m  [100/106], [94mLoss[0m : 9.89770

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.026, [92mTest[0m: 10.295, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.77598
[1mStep[0m  [10/106], [94mLoss[0m : 9.70866
[1mStep[0m  [20/106], [94mLoss[0m : 10.09780
[1mStep[0m  [30/106], [94mLoss[0m : 10.35616
[1mStep[0m  [40/106], [94mLoss[0m : 9.73079
[1mStep[0m  [50/106], [94mLoss[0m : 9.64840
[1mStep[0m  [60/106], [94mLoss[0m : 9.84541
[1mStep[0m  [70/106], [94mLoss[0m : 9.71414
[1mStep[0m  [80/106], [94mLoss[0m : 10.17880
[1mStep[0m  [90/106], [94mLoss[0m : 9.88374
[1mStep[0m  [100/106], [94mLoss[0m : 9.78992

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.839, [92mTest[0m: 10.204, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.81559
[1mStep[0m  [10/106], [94mLoss[0m : 9.75093
[1mStep[0m  [20/106], [94mLoss[0m : 9.47679
[1mStep[0m  [30/106], [94mLoss[0m : 10.05810
[1mStep[0m  [40/106], [94mLoss[0m : 10.04593
[1mStep[0m  [50/106], [94mLoss[0m : 9.23259
[1mStep[0m  [60/106], [94mLoss[0m : 9.68494
[1mStep[0m  [70/106], [94mLoss[0m : 9.25609
[1mStep[0m  [80/106], [94mLoss[0m : 9.70111
[1mStep[0m  [90/106], [94mLoss[0m : 9.13130
[1mStep[0m  [100/106], [94mLoss[0m : 10.26004

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.664, [92mTest[0m: 10.086, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.73785
[1mStep[0m  [10/106], [94mLoss[0m : 9.31046
[1mStep[0m  [20/106], [94mLoss[0m : 9.66798
[1mStep[0m  [30/106], [94mLoss[0m : 9.64699
[1mStep[0m  [40/106], [94mLoss[0m : 9.78923
[1mStep[0m  [50/106], [94mLoss[0m : 9.95335
[1mStep[0m  [60/106], [94mLoss[0m : 8.77754
[1mStep[0m  [70/106], [94mLoss[0m : 9.47407
[1mStep[0m  [80/106], [94mLoss[0m : 9.31045
[1mStep[0m  [90/106], [94mLoss[0m : 9.86713
[1mStep[0m  [100/106], [94mLoss[0m : 9.23218

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.464, [92mTest[0m: 9.977, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.69351
[1mStep[0m  [10/106], [94mLoss[0m : 9.08952
[1mStep[0m  [20/106], [94mLoss[0m : 9.41586
[1mStep[0m  [30/106], [94mLoss[0m : 8.86559
[1mStep[0m  [40/106], [94mLoss[0m : 9.16748
[1mStep[0m  [50/106], [94mLoss[0m : 9.46839
[1mStep[0m  [60/106], [94mLoss[0m : 9.25343
[1mStep[0m  [70/106], [94mLoss[0m : 9.20869
[1mStep[0m  [80/106], [94mLoss[0m : 9.09630
[1mStep[0m  [90/106], [94mLoss[0m : 9.47220
[1mStep[0m  [100/106], [94mLoss[0m : 9.27893

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.285, [92mTest[0m: 9.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.86613
[1mStep[0m  [10/106], [94mLoss[0m : 9.58231
[1mStep[0m  [20/106], [94mLoss[0m : 8.85945
[1mStep[0m  [30/106], [94mLoss[0m : 8.86559
[1mStep[0m  [40/106], [94mLoss[0m : 8.82697
[1mStep[0m  [50/106], [94mLoss[0m : 9.50188
[1mStep[0m  [60/106], [94mLoss[0m : 8.83241
[1mStep[0m  [70/106], [94mLoss[0m : 9.28411
[1mStep[0m  [80/106], [94mLoss[0m : 9.02097
[1mStep[0m  [90/106], [94mLoss[0m : 9.55862
[1mStep[0m  [100/106], [94mLoss[0m : 9.17214

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.108, [92mTest[0m: 9.738, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.46720
[1mStep[0m  [10/106], [94mLoss[0m : 9.34092
[1mStep[0m  [20/106], [94mLoss[0m : 8.66273
[1mStep[0m  [30/106], [94mLoss[0m : 8.90109
[1mStep[0m  [40/106], [94mLoss[0m : 8.37753
[1mStep[0m  [50/106], [94mLoss[0m : 9.35938
[1mStep[0m  [60/106], [94mLoss[0m : 9.15624
[1mStep[0m  [70/106], [94mLoss[0m : 8.88024
[1mStep[0m  [80/106], [94mLoss[0m : 9.08701
[1mStep[0m  [90/106], [94mLoss[0m : 9.02536
[1mStep[0m  [100/106], [94mLoss[0m : 9.28431

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.925, [92mTest[0m: 9.610, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.85802
[1mStep[0m  [10/106], [94mLoss[0m : 8.49795
[1mStep[0m  [20/106], [94mLoss[0m : 8.45743
[1mStep[0m  [30/106], [94mLoss[0m : 8.77360
[1mStep[0m  [40/106], [94mLoss[0m : 9.31991
[1mStep[0m  [50/106], [94mLoss[0m : 8.56153
[1mStep[0m  [60/106], [94mLoss[0m : 7.81984
[1mStep[0m  [70/106], [94mLoss[0m : 8.32594
[1mStep[0m  [80/106], [94mLoss[0m : 9.40652
[1mStep[0m  [90/106], [94mLoss[0m : 8.34397
[1mStep[0m  [100/106], [94mLoss[0m : 8.42757

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.727, [92mTest[0m: 9.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.95456
[1mStep[0m  [10/106], [94mLoss[0m : 8.62615
[1mStep[0m  [20/106], [94mLoss[0m : 8.07053
[1mStep[0m  [30/106], [94mLoss[0m : 8.39655
[1mStep[0m  [40/106], [94mLoss[0m : 8.69826
[1mStep[0m  [50/106], [94mLoss[0m : 9.23067
[1mStep[0m  [60/106], [94mLoss[0m : 8.50454
[1mStep[0m  [70/106], [94mLoss[0m : 8.76261
[1mStep[0m  [80/106], [94mLoss[0m : 8.27389
[1mStep[0m  [90/106], [94mLoss[0m : 8.33965
[1mStep[0m  [100/106], [94mLoss[0m : 8.18720

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.551, [92mTest[0m: 9.374, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.11429
[1mStep[0m  [10/106], [94mLoss[0m : 7.92778
[1mStep[0m  [20/106], [94mLoss[0m : 7.85779
[1mStep[0m  [30/106], [94mLoss[0m : 7.99672
[1mStep[0m  [40/106], [94mLoss[0m : 8.41057
[1mStep[0m  [50/106], [94mLoss[0m : 8.69688
[1mStep[0m  [60/106], [94mLoss[0m : 8.11857
[1mStep[0m  [70/106], [94mLoss[0m : 9.11095
[1mStep[0m  [80/106], [94mLoss[0m : 8.04828
[1mStep[0m  [90/106], [94mLoss[0m : 8.81083
[1mStep[0m  [100/106], [94mLoss[0m : 8.49181

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.352, [92mTest[0m: 9.220, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.76237
[1mStep[0m  [10/106], [94mLoss[0m : 7.84842
[1mStep[0m  [20/106], [94mLoss[0m : 8.32943
[1mStep[0m  [30/106], [94mLoss[0m : 8.78696
[1mStep[0m  [40/106], [94mLoss[0m : 8.78987
[1mStep[0m  [50/106], [94mLoss[0m : 8.58234
[1mStep[0m  [60/106], [94mLoss[0m : 8.55978
[1mStep[0m  [70/106], [94mLoss[0m : 7.58379
[1mStep[0m  [80/106], [94mLoss[0m : 8.46690
[1mStep[0m  [90/106], [94mLoss[0m : 7.87742
[1mStep[0m  [100/106], [94mLoss[0m : 8.49991

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.161, [92mTest[0m: 9.108, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.48620
[1mStep[0m  [10/106], [94mLoss[0m : 8.34688
[1mStep[0m  [20/106], [94mLoss[0m : 7.93791
[1mStep[0m  [30/106], [94mLoss[0m : 8.21494
[1mStep[0m  [40/106], [94mLoss[0m : 7.97469
[1mStep[0m  [50/106], [94mLoss[0m : 8.21450
[1mStep[0m  [60/106], [94mLoss[0m : 7.26364
[1mStep[0m  [70/106], [94mLoss[0m : 7.60476
[1mStep[0m  [80/106], [94mLoss[0m : 7.66858
[1mStep[0m  [90/106], [94mLoss[0m : 7.74598
[1mStep[0m  [100/106], [94mLoss[0m : 7.51977

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.963, [92mTest[0m: 8.969, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.95404
[1mStep[0m  [10/106], [94mLoss[0m : 7.20746
[1mStep[0m  [20/106], [94mLoss[0m : 7.94303
[1mStep[0m  [30/106], [94mLoss[0m : 8.00855
[1mStep[0m  [40/106], [94mLoss[0m : 7.45991
[1mStep[0m  [50/106], [94mLoss[0m : 7.82076
[1mStep[0m  [60/106], [94mLoss[0m : 7.13702
[1mStep[0m  [70/106], [94mLoss[0m : 7.71957
[1mStep[0m  [80/106], [94mLoss[0m : 7.10105
[1mStep[0m  [90/106], [94mLoss[0m : 7.75276
[1mStep[0m  [100/106], [94mLoss[0m : 7.32290

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.788, [92mTest[0m: 8.832, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.52029
[1mStep[0m  [10/106], [94mLoss[0m : 7.70516
[1mStep[0m  [20/106], [94mLoss[0m : 7.73216
[1mStep[0m  [30/106], [94mLoss[0m : 7.49261
[1mStep[0m  [40/106], [94mLoss[0m : 7.93968
[1mStep[0m  [50/106], [94mLoss[0m : 7.46044
[1mStep[0m  [60/106], [94mLoss[0m : 7.92393
[1mStep[0m  [70/106], [94mLoss[0m : 7.62968
[1mStep[0m  [80/106], [94mLoss[0m : 7.28507
[1mStep[0m  [90/106], [94mLoss[0m : 7.87815
[1mStep[0m  [100/106], [94mLoss[0m : 7.45850

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.594, [92mTest[0m: 8.719, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.59388
[1mStep[0m  [10/106], [94mLoss[0m : 7.89554
[1mStep[0m  [20/106], [94mLoss[0m : 7.37473
[1mStep[0m  [30/106], [94mLoss[0m : 7.22230
[1mStep[0m  [40/106], [94mLoss[0m : 8.00482
[1mStep[0m  [50/106], [94mLoss[0m : 7.41265
[1mStep[0m  [60/106], [94mLoss[0m : 7.66786
[1mStep[0m  [70/106], [94mLoss[0m : 7.98737
[1mStep[0m  [80/106], [94mLoss[0m : 7.12101
[1mStep[0m  [90/106], [94mLoss[0m : 7.26199
[1mStep[0m  [100/106], [94mLoss[0m : 6.73082

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.384, [92mTest[0m: 8.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.25329
[1mStep[0m  [10/106], [94mLoss[0m : 6.56913
[1mStep[0m  [20/106], [94mLoss[0m : 6.92815
[1mStep[0m  [30/106], [94mLoss[0m : 7.29012
[1mStep[0m  [40/106], [94mLoss[0m : 7.53312
[1mStep[0m  [50/106], [94mLoss[0m : 6.70428
[1mStep[0m  [60/106], [94mLoss[0m : 7.31113
[1mStep[0m  [70/106], [94mLoss[0m : 7.31221
[1mStep[0m  [80/106], [94mLoss[0m : 7.30410
[1mStep[0m  [90/106], [94mLoss[0m : 7.52095
[1mStep[0m  [100/106], [94mLoss[0m : 7.02202

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.208, [92mTest[0m: 8.468, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.07123
[1mStep[0m  [10/106], [94mLoss[0m : 6.36231
[1mStep[0m  [20/106], [94mLoss[0m : 6.83375
[1mStep[0m  [30/106], [94mLoss[0m : 6.70246
[1mStep[0m  [40/106], [94mLoss[0m : 7.63948
[1mStep[0m  [50/106], [94mLoss[0m : 6.22260
[1mStep[0m  [60/106], [94mLoss[0m : 6.84879
[1mStep[0m  [70/106], [94mLoss[0m : 7.01311
[1mStep[0m  [80/106], [94mLoss[0m : 6.46262
[1mStep[0m  [90/106], [94mLoss[0m : 7.25873
[1mStep[0m  [100/106], [94mLoss[0m : 7.15504

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.031, [92mTest[0m: 8.297, [96mlr[0m: 9e-05
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.192
====================================

Phase 1 - Evaluation MAE:  8.191836546052176
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 7.31739
[1mStep[0m  [10/106], [94mLoss[0m : 6.53284
[1mStep[0m  [20/106], [94mLoss[0m : 6.90110
[1mStep[0m  [30/106], [94mLoss[0m : 6.51603
[1mStep[0m  [40/106], [94mLoss[0m : 6.94879
[1mStep[0m  [50/106], [94mLoss[0m : 6.88866
[1mStep[0m  [60/106], [94mLoss[0m : 7.34152
[1mStep[0m  [70/106], [94mLoss[0m : 6.78420
[1mStep[0m  [80/106], [94mLoss[0m : 6.49496
[1mStep[0m  [90/106], [94mLoss[0m : 6.68827
[1mStep[0m  [100/106], [94mLoss[0m : 6.63481

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.839, [92mTest[0m: 8.190, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.977
====================================

Phase 2 - Evaluation MAE:  7.9771944082008215
MAE score P1       8.191837
MAE score P2       7.977194
loss               6.838857
learning_rate        0.0001
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 10.26398
[1mStep[0m  [10/106], [94mLoss[0m : 10.20604
[1mStep[0m  [20/106], [94mLoss[0m : 10.48031
[1mStep[0m  [30/106], [94mLoss[0m : 10.71852
[1mStep[0m  [40/106], [94mLoss[0m : 9.69698
[1mStep[0m  [50/106], [94mLoss[0m : 10.58701
[1mStep[0m  [60/106], [94mLoss[0m : 10.01911
[1mStep[0m  [70/106], [94mLoss[0m : 10.33876
[1mStep[0m  [80/106], [94mLoss[0m : 10.16566
[1mStep[0m  [90/106], [94mLoss[0m : 10.35084
[1mStep[0m  [100/106], [94mLoss[0m : 9.70922

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.229, [92mTest[0m: 10.349, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.24896
[1mStep[0m  [10/106], [94mLoss[0m : 10.23027
[1mStep[0m  [20/106], [94mLoss[0m : 10.22525
[1mStep[0m  [30/106], [94mLoss[0m : 10.04721
[1mStep[0m  [40/106], [94mLoss[0m : 10.05869
[1mStep[0m  [50/106], [94mLoss[0m : 10.23361
[1mStep[0m  [60/106], [94mLoss[0m : 10.21422
[1mStep[0m  [70/106], [94mLoss[0m : 9.77804
[1mStep[0m  [80/106], [94mLoss[0m : 9.61939
[1mStep[0m  [90/106], [94mLoss[0m : 9.77826
[1mStep[0m  [100/106], [94mLoss[0m : 9.21608

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.936, [92mTest[0m: 10.092, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.93298
[1mStep[0m  [10/106], [94mLoss[0m : 9.54073
[1mStep[0m  [20/106], [94mLoss[0m : 9.79409
[1mStep[0m  [30/106], [94mLoss[0m : 10.25927
[1mStep[0m  [40/106], [94mLoss[0m : 9.33871
[1mStep[0m  [50/106], [94mLoss[0m : 9.22395
[1mStep[0m  [60/106], [94mLoss[0m : 9.93325
[1mStep[0m  [70/106], [94mLoss[0m : 9.17183
[1mStep[0m  [80/106], [94mLoss[0m : 9.70175
[1mStep[0m  [90/106], [94mLoss[0m : 9.88542
[1mStep[0m  [100/106], [94mLoss[0m : 9.42653

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.659, [92mTest[0m: 9.801, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.02773
[1mStep[0m  [10/106], [94mLoss[0m : 9.54338
[1mStep[0m  [20/106], [94mLoss[0m : 8.83682
[1mStep[0m  [30/106], [94mLoss[0m : 9.37029
[1mStep[0m  [40/106], [94mLoss[0m : 9.16673
[1mStep[0m  [50/106], [94mLoss[0m : 9.63063
[1mStep[0m  [60/106], [94mLoss[0m : 9.07928
[1mStep[0m  [70/106], [94mLoss[0m : 9.82028
[1mStep[0m  [80/106], [94mLoss[0m : 9.21597
[1mStep[0m  [90/106], [94mLoss[0m : 9.48892
[1mStep[0m  [100/106], [94mLoss[0m : 9.74700

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.368, [92mTest[0m: 9.506, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.32740
[1mStep[0m  [10/106], [94mLoss[0m : 9.14531
[1mStep[0m  [20/106], [94mLoss[0m : 9.28722
[1mStep[0m  [30/106], [94mLoss[0m : 9.57093
[1mStep[0m  [40/106], [94mLoss[0m : 9.19272
[1mStep[0m  [50/106], [94mLoss[0m : 8.98944
[1mStep[0m  [60/106], [94mLoss[0m : 9.17928
[1mStep[0m  [70/106], [94mLoss[0m : 8.64058
[1mStep[0m  [80/106], [94mLoss[0m : 9.65345
[1mStep[0m  [90/106], [94mLoss[0m : 8.80967
[1mStep[0m  [100/106], [94mLoss[0m : 8.63190

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.082, [92mTest[0m: 9.222, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.38420
[1mStep[0m  [10/106], [94mLoss[0m : 9.20323
[1mStep[0m  [20/106], [94mLoss[0m : 8.94769
[1mStep[0m  [30/106], [94mLoss[0m : 8.87144
[1mStep[0m  [40/106], [94mLoss[0m : 9.82017
[1mStep[0m  [50/106], [94mLoss[0m : 8.69308
[1mStep[0m  [60/106], [94mLoss[0m : 8.84777
[1mStep[0m  [70/106], [94mLoss[0m : 8.88353
[1mStep[0m  [80/106], [94mLoss[0m : 9.25375
[1mStep[0m  [90/106], [94mLoss[0m : 8.57862
[1mStep[0m  [100/106], [94mLoss[0m : 8.49487

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.803, [92mTest[0m: 8.942, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.72789
[1mStep[0m  [10/106], [94mLoss[0m : 8.84020
[1mStep[0m  [20/106], [94mLoss[0m : 8.94914
[1mStep[0m  [30/106], [94mLoss[0m : 8.69819
[1mStep[0m  [40/106], [94mLoss[0m : 9.06138
[1mStep[0m  [50/106], [94mLoss[0m : 8.99292
[1mStep[0m  [60/106], [94mLoss[0m : 8.45598
[1mStep[0m  [70/106], [94mLoss[0m : 9.17938
[1mStep[0m  [80/106], [94mLoss[0m : 8.67377
[1mStep[0m  [90/106], [94mLoss[0m : 8.12097
[1mStep[0m  [100/106], [94mLoss[0m : 9.40483

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.513, [92mTest[0m: 8.647, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.33693
[1mStep[0m  [10/106], [94mLoss[0m : 9.04647
[1mStep[0m  [20/106], [94mLoss[0m : 8.08478
[1mStep[0m  [30/106], [94mLoss[0m : 8.58695
[1mStep[0m  [40/106], [94mLoss[0m : 8.06114
[1mStep[0m  [50/106], [94mLoss[0m : 8.63433
[1mStep[0m  [60/106], [94mLoss[0m : 8.28005
[1mStep[0m  [70/106], [94mLoss[0m : 8.55871
[1mStep[0m  [80/106], [94mLoss[0m : 8.28156
[1mStep[0m  [90/106], [94mLoss[0m : 8.49543
[1mStep[0m  [100/106], [94mLoss[0m : 7.56904

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.227, [92mTest[0m: 8.362, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.05670
[1mStep[0m  [10/106], [94mLoss[0m : 8.09197
[1mStep[0m  [20/106], [94mLoss[0m : 7.83569
[1mStep[0m  [30/106], [94mLoss[0m : 7.90950
[1mStep[0m  [40/106], [94mLoss[0m : 8.25669
[1mStep[0m  [50/106], [94mLoss[0m : 7.62091
[1mStep[0m  [60/106], [94mLoss[0m : 7.70906
[1mStep[0m  [70/106], [94mLoss[0m : 7.86829
[1mStep[0m  [80/106], [94mLoss[0m : 7.70249
[1mStep[0m  [90/106], [94mLoss[0m : 8.27228
[1mStep[0m  [100/106], [94mLoss[0m : 8.66631

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.935, [92mTest[0m: 8.069, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.46542
[1mStep[0m  [10/106], [94mLoss[0m : 8.46916
[1mStep[0m  [20/106], [94mLoss[0m : 7.73305
[1mStep[0m  [30/106], [94mLoss[0m : 8.23281
[1mStep[0m  [40/106], [94mLoss[0m : 7.96739
[1mStep[0m  [50/106], [94mLoss[0m : 8.06056
[1mStep[0m  [60/106], [94mLoss[0m : 7.56200
[1mStep[0m  [70/106], [94mLoss[0m : 7.58203
[1mStep[0m  [80/106], [94mLoss[0m : 7.31301
[1mStep[0m  [90/106], [94mLoss[0m : 7.54516
[1mStep[0m  [100/106], [94mLoss[0m : 8.03874

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.647, [92mTest[0m: 7.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.51216
[1mStep[0m  [10/106], [94mLoss[0m : 7.25768
[1mStep[0m  [20/106], [94mLoss[0m : 7.64824
[1mStep[0m  [30/106], [94mLoss[0m : 7.10222
[1mStep[0m  [40/106], [94mLoss[0m : 7.37836
[1mStep[0m  [50/106], [94mLoss[0m : 7.55121
[1mStep[0m  [60/106], [94mLoss[0m : 7.04531
[1mStep[0m  [70/106], [94mLoss[0m : 7.20461
[1mStep[0m  [80/106], [94mLoss[0m : 7.64582
[1mStep[0m  [90/106], [94mLoss[0m : 7.13455
[1mStep[0m  [100/106], [94mLoss[0m : 7.36284

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.363, [92mTest[0m: 7.498, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.44706
[1mStep[0m  [10/106], [94mLoss[0m : 7.34749
[1mStep[0m  [20/106], [94mLoss[0m : 7.34712
[1mStep[0m  [30/106], [94mLoss[0m : 7.04259
[1mStep[0m  [40/106], [94mLoss[0m : 6.49776
[1mStep[0m  [50/106], [94mLoss[0m : 7.83689
[1mStep[0m  [60/106], [94mLoss[0m : 6.58376
[1mStep[0m  [70/106], [94mLoss[0m : 7.13103
[1mStep[0m  [80/106], [94mLoss[0m : 6.94199
[1mStep[0m  [90/106], [94mLoss[0m : 7.00217
[1mStep[0m  [100/106], [94mLoss[0m : 7.04702

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.076, [92mTest[0m: 7.209, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.50960
[1mStep[0m  [10/106], [94mLoss[0m : 6.84623
[1mStep[0m  [20/106], [94mLoss[0m : 6.93546
[1mStep[0m  [30/106], [94mLoss[0m : 6.68157
[1mStep[0m  [40/106], [94mLoss[0m : 7.14888
[1mStep[0m  [50/106], [94mLoss[0m : 6.68573
[1mStep[0m  [60/106], [94mLoss[0m : 6.57331
[1mStep[0m  [70/106], [94mLoss[0m : 6.57799
[1mStep[0m  [80/106], [94mLoss[0m : 6.81970
[1mStep[0m  [90/106], [94mLoss[0m : 6.90541
[1mStep[0m  [100/106], [94mLoss[0m : 6.58588

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.782, [92mTest[0m: 6.915, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.05308
[1mStep[0m  [10/106], [94mLoss[0m : 6.54504
[1mStep[0m  [20/106], [94mLoss[0m : 6.59132
[1mStep[0m  [30/106], [94mLoss[0m : 6.23049
[1mStep[0m  [40/106], [94mLoss[0m : 5.71771
[1mStep[0m  [50/106], [94mLoss[0m : 6.47467
[1mStep[0m  [60/106], [94mLoss[0m : 6.58864
[1mStep[0m  [70/106], [94mLoss[0m : 6.18980
[1mStep[0m  [80/106], [94mLoss[0m : 6.19337
[1mStep[0m  [90/106], [94mLoss[0m : 6.88769
[1mStep[0m  [100/106], [94mLoss[0m : 5.74782

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.500, [92mTest[0m: 6.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.58740
[1mStep[0m  [10/106], [94mLoss[0m : 6.51039
[1mStep[0m  [20/106], [94mLoss[0m : 6.56272
[1mStep[0m  [30/106], [94mLoss[0m : 6.15852
[1mStep[0m  [40/106], [94mLoss[0m : 6.48391
[1mStep[0m  [50/106], [94mLoss[0m : 6.07827
[1mStep[0m  [60/106], [94mLoss[0m : 6.06585
[1mStep[0m  [70/106], [94mLoss[0m : 6.35067
[1mStep[0m  [80/106], [94mLoss[0m : 5.79484
[1mStep[0m  [90/106], [94mLoss[0m : 6.54898
[1mStep[0m  [100/106], [94mLoss[0m : 5.70108

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.222, [92mTest[0m: 6.336, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.50548
[1mStep[0m  [10/106], [94mLoss[0m : 5.96839
[1mStep[0m  [20/106], [94mLoss[0m : 5.37368
[1mStep[0m  [30/106], [94mLoss[0m : 5.98730
[1mStep[0m  [40/106], [94mLoss[0m : 5.64085
[1mStep[0m  [50/106], [94mLoss[0m : 5.90462
[1mStep[0m  [60/106], [94mLoss[0m : 6.20126
[1mStep[0m  [70/106], [94mLoss[0m : 6.11716
[1mStep[0m  [80/106], [94mLoss[0m : 5.97148
[1mStep[0m  [90/106], [94mLoss[0m : 5.40438
[1mStep[0m  [100/106], [94mLoss[0m : 6.32565

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.936, [92mTest[0m: 6.043, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.56661
[1mStep[0m  [10/106], [94mLoss[0m : 6.10492
[1mStep[0m  [20/106], [94mLoss[0m : 5.45266
[1mStep[0m  [30/106], [94mLoss[0m : 5.78148
[1mStep[0m  [40/106], [94mLoss[0m : 5.86270
[1mStep[0m  [50/106], [94mLoss[0m : 5.85307
[1mStep[0m  [60/106], [94mLoss[0m : 5.41881
[1mStep[0m  [70/106], [94mLoss[0m : 5.81130
[1mStep[0m  [80/106], [94mLoss[0m : 5.27779
[1mStep[0m  [90/106], [94mLoss[0m : 5.59313
[1mStep[0m  [100/106], [94mLoss[0m : 5.93842

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.658, [92mTest[0m: 5.773, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.66329
[1mStep[0m  [10/106], [94mLoss[0m : 5.61947
[1mStep[0m  [20/106], [94mLoss[0m : 5.69044
[1mStep[0m  [30/106], [94mLoss[0m : 5.58169
[1mStep[0m  [40/106], [94mLoss[0m : 6.03775
[1mStep[0m  [50/106], [94mLoss[0m : 5.14072
[1mStep[0m  [60/106], [94mLoss[0m : 5.11936
[1mStep[0m  [70/106], [94mLoss[0m : 5.16296
[1mStep[0m  [80/106], [94mLoss[0m : 4.98711
[1mStep[0m  [90/106], [94mLoss[0m : 5.27818
[1mStep[0m  [100/106], [94mLoss[0m : 5.70662

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.391, [92mTest[0m: 5.496, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.69081
[1mStep[0m  [10/106], [94mLoss[0m : 5.71038
[1mStep[0m  [20/106], [94mLoss[0m : 5.16907
[1mStep[0m  [30/106], [94mLoss[0m : 4.78102
[1mStep[0m  [40/106], [94mLoss[0m : 5.46245
[1mStep[0m  [50/106], [94mLoss[0m : 5.09562
[1mStep[0m  [60/106], [94mLoss[0m : 5.22171
[1mStep[0m  [70/106], [94mLoss[0m : 5.35461
[1mStep[0m  [80/106], [94mLoss[0m : 4.51271
[1mStep[0m  [90/106], [94mLoss[0m : 5.38563
[1mStep[0m  [100/106], [94mLoss[0m : 5.09477

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.148, [92mTest[0m: 5.224, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.26218
[1mStep[0m  [10/106], [94mLoss[0m : 5.10923
[1mStep[0m  [20/106], [94mLoss[0m : 5.17217
[1mStep[0m  [30/106], [94mLoss[0m : 4.52094
[1mStep[0m  [40/106], [94mLoss[0m : 5.04671
[1mStep[0m  [50/106], [94mLoss[0m : 4.87685
[1mStep[0m  [60/106], [94mLoss[0m : 4.42734
[1mStep[0m  [70/106], [94mLoss[0m : 5.00867
[1mStep[0m  [80/106], [94mLoss[0m : 5.04962
[1mStep[0m  [90/106], [94mLoss[0m : 4.88805
[1mStep[0m  [100/106], [94mLoss[0m : 5.01747

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.912, [92mTest[0m: 4.979, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.13584
[1mStep[0m  [10/106], [94mLoss[0m : 5.50103
[1mStep[0m  [20/106], [94mLoss[0m : 4.88898
[1mStep[0m  [30/106], [94mLoss[0m : 4.69466
[1mStep[0m  [40/106], [94mLoss[0m : 5.30385
[1mStep[0m  [50/106], [94mLoss[0m : 5.36434
[1mStep[0m  [60/106], [94mLoss[0m : 4.62019
[1mStep[0m  [70/106], [94mLoss[0m : 4.68279
[1mStep[0m  [80/106], [94mLoss[0m : 4.81724
[1mStep[0m  [90/106], [94mLoss[0m : 4.69260
[1mStep[0m  [100/106], [94mLoss[0m : 4.46314

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.704, [92mTest[0m: 4.748, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.37972
[1mStep[0m  [10/106], [94mLoss[0m : 4.76829
[1mStep[0m  [20/106], [94mLoss[0m : 4.36361
[1mStep[0m  [30/106], [94mLoss[0m : 5.08817
[1mStep[0m  [40/106], [94mLoss[0m : 4.74533
[1mStep[0m  [50/106], [94mLoss[0m : 4.33184
[1mStep[0m  [60/106], [94mLoss[0m : 4.53914
[1mStep[0m  [70/106], [94mLoss[0m : 4.13920
[1mStep[0m  [80/106], [94mLoss[0m : 4.29700
[1mStep[0m  [90/106], [94mLoss[0m : 4.21945
[1mStep[0m  [100/106], [94mLoss[0m : 4.43098

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.530, [92mTest[0m: 4.559, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.82960
[1mStep[0m  [10/106], [94mLoss[0m : 4.45133
[1mStep[0m  [20/106], [94mLoss[0m : 4.30724
[1mStep[0m  [30/106], [94mLoss[0m : 4.57981
[1mStep[0m  [40/106], [94mLoss[0m : 4.97121
[1mStep[0m  [50/106], [94mLoss[0m : 4.18709
[1mStep[0m  [60/106], [94mLoss[0m : 4.06118
[1mStep[0m  [70/106], [94mLoss[0m : 4.27584
[1mStep[0m  [80/106], [94mLoss[0m : 4.04268
[1mStep[0m  [90/106], [94mLoss[0m : 4.16950
[1mStep[0m  [100/106], [94mLoss[0m : 4.31281

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.357, [92mTest[0m: 4.364, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.92549
[1mStep[0m  [10/106], [94mLoss[0m : 4.01634
[1mStep[0m  [20/106], [94mLoss[0m : 4.29774
[1mStep[0m  [30/106], [94mLoss[0m : 4.25603
[1mStep[0m  [40/106], [94mLoss[0m : 3.90987
[1mStep[0m  [50/106], [94mLoss[0m : 4.09108
[1mStep[0m  [60/106], [94mLoss[0m : 4.26450
[1mStep[0m  [70/106], [94mLoss[0m : 4.31181
[1mStep[0m  [80/106], [94mLoss[0m : 4.13698
[1mStep[0m  [90/106], [94mLoss[0m : 4.04342
[1mStep[0m  [100/106], [94mLoss[0m : 4.31774

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.194, [92mTest[0m: 4.201, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.73105
[1mStep[0m  [10/106], [94mLoss[0m : 4.31076
[1mStep[0m  [20/106], [94mLoss[0m : 4.25513
[1mStep[0m  [30/106], [94mLoss[0m : 3.64216
[1mStep[0m  [40/106], [94mLoss[0m : 4.08479
[1mStep[0m  [50/106], [94mLoss[0m : 3.80621
[1mStep[0m  [60/106], [94mLoss[0m : 4.09846
[1mStep[0m  [70/106], [94mLoss[0m : 3.78918
[1mStep[0m  [80/106], [94mLoss[0m : 4.16072
[1mStep[0m  [90/106], [94mLoss[0m : 3.53825
[1mStep[0m  [100/106], [94mLoss[0m : 4.09852

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.055, [92mTest[0m: 4.058, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.88610
[1mStep[0m  [10/106], [94mLoss[0m : 4.08279
[1mStep[0m  [20/106], [94mLoss[0m : 4.20023
[1mStep[0m  [30/106], [94mLoss[0m : 4.31225
[1mStep[0m  [40/106], [94mLoss[0m : 3.83775
[1mStep[0m  [50/106], [94mLoss[0m : 3.82949
[1mStep[0m  [60/106], [94mLoss[0m : 3.49177
[1mStep[0m  [70/106], [94mLoss[0m : 4.17052
[1mStep[0m  [80/106], [94mLoss[0m : 3.99240
[1mStep[0m  [90/106], [94mLoss[0m : 3.85189
[1mStep[0m  [100/106], [94mLoss[0m : 3.99122

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.930, [92mTest[0m: 3.898, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.33372
[1mStep[0m  [10/106], [94mLoss[0m : 3.25730
[1mStep[0m  [20/106], [94mLoss[0m : 4.12172
[1mStep[0m  [30/106], [94mLoss[0m : 3.66776
[1mStep[0m  [40/106], [94mLoss[0m : 3.74030
[1mStep[0m  [50/106], [94mLoss[0m : 4.02933
[1mStep[0m  [60/106], [94mLoss[0m : 3.57722
[1mStep[0m  [70/106], [94mLoss[0m : 3.33276
[1mStep[0m  [80/106], [94mLoss[0m : 3.93201
[1mStep[0m  [90/106], [94mLoss[0m : 3.54350
[1mStep[0m  [100/106], [94mLoss[0m : 3.63882

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.808, [92mTest[0m: 3.767, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.74302
[1mStep[0m  [10/106], [94mLoss[0m : 3.59833
[1mStep[0m  [20/106], [94mLoss[0m : 3.55973
[1mStep[0m  [30/106], [94mLoss[0m : 3.81676
[1mStep[0m  [40/106], [94mLoss[0m : 3.89731
[1mStep[0m  [50/106], [94mLoss[0m : 4.14480
[1mStep[0m  [60/106], [94mLoss[0m : 3.98427
[1mStep[0m  [70/106], [94mLoss[0m : 4.28760
[1mStep[0m  [80/106], [94mLoss[0m : 4.10498
[1mStep[0m  [90/106], [94mLoss[0m : 3.91190
[1mStep[0m  [100/106], [94mLoss[0m : 3.40011

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.711, [92mTest[0m: 3.641, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.48078
[1mStep[0m  [10/106], [94mLoss[0m : 3.63181
[1mStep[0m  [20/106], [94mLoss[0m : 3.85943
[1mStep[0m  [30/106], [94mLoss[0m : 3.81301
[1mStep[0m  [40/106], [94mLoss[0m : 3.45514
[1mStep[0m  [50/106], [94mLoss[0m : 3.84330
[1mStep[0m  [60/106], [94mLoss[0m : 3.08400
[1mStep[0m  [70/106], [94mLoss[0m : 3.53738
[1mStep[0m  [80/106], [94mLoss[0m : 3.72637
[1mStep[0m  [90/106], [94mLoss[0m : 3.42112
[1mStep[0m  [100/106], [94mLoss[0m : 3.53435

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.614, [92mTest[0m: 3.533, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.91789
[1mStep[0m  [10/106], [94mLoss[0m : 3.22050
[1mStep[0m  [20/106], [94mLoss[0m : 3.22704
[1mStep[0m  [30/106], [94mLoss[0m : 3.91255
[1mStep[0m  [40/106], [94mLoss[0m : 3.33545
[1mStep[0m  [50/106], [94mLoss[0m : 3.05344
[1mStep[0m  [60/106], [94mLoss[0m : 3.35859
[1mStep[0m  [70/106], [94mLoss[0m : 3.73677
[1mStep[0m  [80/106], [94mLoss[0m : 3.73070
[1mStep[0m  [90/106], [94mLoss[0m : 3.46621
[1mStep[0m  [100/106], [94mLoss[0m : 3.84621

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.534, [92mTest[0m: 3.443, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.348
====================================

Phase 1 - Evaluation MAE:  3.348378158965201
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 3.51805
[1mStep[0m  [10/106], [94mLoss[0m : 3.52192
[1mStep[0m  [20/106], [94mLoss[0m : 3.47173
[1mStep[0m  [30/106], [94mLoss[0m : 3.05302
[1mStep[0m  [40/106], [94mLoss[0m : 3.28126
[1mStep[0m  [50/106], [94mLoss[0m : 3.40112
[1mStep[0m  [60/106], [94mLoss[0m : 3.83276
[1mStep[0m  [70/106], [94mLoss[0m : 3.26692
[1mStep[0m  [80/106], [94mLoss[0m : 3.01221
[1mStep[0m  [90/106], [94mLoss[0m : 3.35778
[1mStep[0m  [100/106], [94mLoss[0m : 3.76795

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.433, [92mTest[0m: 3.343, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.46478
[1mStep[0m  [10/106], [94mLoss[0m : 3.42635
[1mStep[0m  [20/106], [94mLoss[0m : 3.38160
[1mStep[0m  [30/106], [94mLoss[0m : 3.60145
[1mStep[0m  [40/106], [94mLoss[0m : 2.87582
[1mStep[0m  [50/106], [94mLoss[0m : 3.44442
[1mStep[0m  [60/106], [94mLoss[0m : 3.38398
[1mStep[0m  [70/106], [94mLoss[0m : 3.13171
[1mStep[0m  [80/106], [94mLoss[0m : 3.53428
[1mStep[0m  [90/106], [94mLoss[0m : 3.50623
[1mStep[0m  [100/106], [94mLoss[0m : 3.57339

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.360, [92mTest[0m: 3.247, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.61612
[1mStep[0m  [10/106], [94mLoss[0m : 3.23700
[1mStep[0m  [20/106], [94mLoss[0m : 3.44028
[1mStep[0m  [30/106], [94mLoss[0m : 3.24863
[1mStep[0m  [40/106], [94mLoss[0m : 3.16782
[1mStep[0m  [50/106], [94mLoss[0m : 3.07793
[1mStep[0m  [60/106], [94mLoss[0m : 3.31203
[1mStep[0m  [70/106], [94mLoss[0m : 3.42829
[1mStep[0m  [80/106], [94mLoss[0m : 3.11608
[1mStep[0m  [90/106], [94mLoss[0m : 2.56871
[1mStep[0m  [100/106], [94mLoss[0m : 2.97485

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.265, [92mTest[0m: 3.166, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60289
[1mStep[0m  [10/106], [94mLoss[0m : 2.69501
[1mStep[0m  [20/106], [94mLoss[0m : 3.55196
[1mStep[0m  [30/106], [94mLoss[0m : 2.63449
[1mStep[0m  [40/106], [94mLoss[0m : 3.16759
[1mStep[0m  [50/106], [94mLoss[0m : 3.19870
[1mStep[0m  [60/106], [94mLoss[0m : 3.59850
[1mStep[0m  [70/106], [94mLoss[0m : 3.34553
[1mStep[0m  [80/106], [94mLoss[0m : 2.67037
[1mStep[0m  [90/106], [94mLoss[0m : 3.01466
[1mStep[0m  [100/106], [94mLoss[0m : 3.14710

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.196, [92mTest[0m: 3.089, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.69398
[1mStep[0m  [10/106], [94mLoss[0m : 3.36832
[1mStep[0m  [20/106], [94mLoss[0m : 2.98003
[1mStep[0m  [30/106], [94mLoss[0m : 2.79267
[1mStep[0m  [40/106], [94mLoss[0m : 3.20245
[1mStep[0m  [50/106], [94mLoss[0m : 2.96625
[1mStep[0m  [60/106], [94mLoss[0m : 3.44200
[1mStep[0m  [70/106], [94mLoss[0m : 3.26118
[1mStep[0m  [80/106], [94mLoss[0m : 2.53431
[1mStep[0m  [90/106], [94mLoss[0m : 2.80668
[1mStep[0m  [100/106], [94mLoss[0m : 2.94104

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.135, [92mTest[0m: 3.019, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.98288
[1mStep[0m  [10/106], [94mLoss[0m : 3.14673
[1mStep[0m  [20/106], [94mLoss[0m : 2.95385
[1mStep[0m  [30/106], [94mLoss[0m : 2.99002
[1mStep[0m  [40/106], [94mLoss[0m : 2.98052
[1mStep[0m  [50/106], [94mLoss[0m : 2.66448
[1mStep[0m  [60/106], [94mLoss[0m : 3.02150
[1mStep[0m  [70/106], [94mLoss[0m : 3.05522
[1mStep[0m  [80/106], [94mLoss[0m : 3.24066
[1mStep[0m  [90/106], [94mLoss[0m : 2.69585
[1mStep[0m  [100/106], [94mLoss[0m : 2.97139

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.073, [92mTest[0m: 2.949, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58782
[1mStep[0m  [10/106], [94mLoss[0m : 2.82411
[1mStep[0m  [20/106], [94mLoss[0m : 3.05025
[1mStep[0m  [30/106], [94mLoss[0m : 3.20275
[1mStep[0m  [40/106], [94mLoss[0m : 3.25485
[1mStep[0m  [50/106], [94mLoss[0m : 3.49420
[1mStep[0m  [60/106], [94mLoss[0m : 2.73461
[1mStep[0m  [70/106], [94mLoss[0m : 3.03405
[1mStep[0m  [80/106], [94mLoss[0m : 3.26036
[1mStep[0m  [90/106], [94mLoss[0m : 3.39213
[1mStep[0m  [100/106], [94mLoss[0m : 2.97091

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.021, [92mTest[0m: 2.877, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.93631
[1mStep[0m  [10/106], [94mLoss[0m : 2.99551
[1mStep[0m  [20/106], [94mLoss[0m : 2.82995
[1mStep[0m  [30/106], [94mLoss[0m : 3.08663
[1mStep[0m  [40/106], [94mLoss[0m : 3.01047
[1mStep[0m  [50/106], [94mLoss[0m : 2.63867
[1mStep[0m  [60/106], [94mLoss[0m : 3.30736
[1mStep[0m  [70/106], [94mLoss[0m : 2.69391
[1mStep[0m  [80/106], [94mLoss[0m : 3.00810
[1mStep[0m  [90/106], [94mLoss[0m : 2.56026
[1mStep[0m  [100/106], [94mLoss[0m : 3.56697

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.978, [92mTest[0m: 2.831, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94431
[1mStep[0m  [10/106], [94mLoss[0m : 2.97091
[1mStep[0m  [20/106], [94mLoss[0m : 3.36885
[1mStep[0m  [30/106], [94mLoss[0m : 2.83538
[1mStep[0m  [40/106], [94mLoss[0m : 2.89799
[1mStep[0m  [50/106], [94mLoss[0m : 2.72103
[1mStep[0m  [60/106], [94mLoss[0m : 3.47980
[1mStep[0m  [70/106], [94mLoss[0m : 3.06715
[1mStep[0m  [80/106], [94mLoss[0m : 2.88318
[1mStep[0m  [90/106], [94mLoss[0m : 2.59117
[1mStep[0m  [100/106], [94mLoss[0m : 2.80924

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.932, [92mTest[0m: 2.780, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64601
[1mStep[0m  [10/106], [94mLoss[0m : 2.89172
[1mStep[0m  [20/106], [94mLoss[0m : 2.98386
[1mStep[0m  [30/106], [94mLoss[0m : 2.82833
[1mStep[0m  [40/106], [94mLoss[0m : 2.89865
[1mStep[0m  [50/106], [94mLoss[0m : 2.94306
[1mStep[0m  [60/106], [94mLoss[0m : 2.89378
[1mStep[0m  [70/106], [94mLoss[0m : 2.44950
[1mStep[0m  [80/106], [94mLoss[0m : 3.02339
[1mStep[0m  [90/106], [94mLoss[0m : 2.84622
[1mStep[0m  [100/106], [94mLoss[0m : 2.98342

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.901, [92mTest[0m: 2.729, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.20939
[1mStep[0m  [10/106], [94mLoss[0m : 3.23604
[1mStep[0m  [20/106], [94mLoss[0m : 2.84613
[1mStep[0m  [30/106], [94mLoss[0m : 2.52912
[1mStep[0m  [40/106], [94mLoss[0m : 3.19393
[1mStep[0m  [50/106], [94mLoss[0m : 2.78102
[1mStep[0m  [60/106], [94mLoss[0m : 3.15348
[1mStep[0m  [70/106], [94mLoss[0m : 2.96405
[1mStep[0m  [80/106], [94mLoss[0m : 3.29043
[1mStep[0m  [90/106], [94mLoss[0m : 3.02333
[1mStep[0m  [100/106], [94mLoss[0m : 3.09224

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.861, [92mTest[0m: 2.695, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.11034
[1mStep[0m  [10/106], [94mLoss[0m : 3.15875
[1mStep[0m  [20/106], [94mLoss[0m : 2.37847
[1mStep[0m  [30/106], [94mLoss[0m : 2.62920
[1mStep[0m  [40/106], [94mLoss[0m : 2.55654
[1mStep[0m  [50/106], [94mLoss[0m : 2.79421
[1mStep[0m  [60/106], [94mLoss[0m : 2.80271
[1mStep[0m  [70/106], [94mLoss[0m : 2.85111
[1mStep[0m  [80/106], [94mLoss[0m : 2.97051
[1mStep[0m  [90/106], [94mLoss[0m : 2.80084
[1mStep[0m  [100/106], [94mLoss[0m : 2.72496

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.835, [92mTest[0m: 2.667, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60774
[1mStep[0m  [10/106], [94mLoss[0m : 2.62672
[1mStep[0m  [20/106], [94mLoss[0m : 3.14096
[1mStep[0m  [30/106], [94mLoss[0m : 3.03456
[1mStep[0m  [40/106], [94mLoss[0m : 2.89345
[1mStep[0m  [50/106], [94mLoss[0m : 3.20677
[1mStep[0m  [60/106], [94mLoss[0m : 3.05623
[1mStep[0m  [70/106], [94mLoss[0m : 2.68108
[1mStep[0m  [80/106], [94mLoss[0m : 2.33346
[1mStep[0m  [90/106], [94mLoss[0m : 3.00332
[1mStep[0m  [100/106], [94mLoss[0m : 3.12316

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.808, [92mTest[0m: 2.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66608
[1mStep[0m  [10/106], [94mLoss[0m : 2.55987
[1mStep[0m  [20/106], [94mLoss[0m : 3.11109
[1mStep[0m  [30/106], [94mLoss[0m : 3.00809
[1mStep[0m  [40/106], [94mLoss[0m : 2.90038
[1mStep[0m  [50/106], [94mLoss[0m : 2.66894
[1mStep[0m  [60/106], [94mLoss[0m : 2.87466
[1mStep[0m  [70/106], [94mLoss[0m : 2.58557
[1mStep[0m  [80/106], [94mLoss[0m : 2.66324
[1mStep[0m  [90/106], [94mLoss[0m : 3.05381
[1mStep[0m  [100/106], [94mLoss[0m : 2.72279

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.775, [92mTest[0m: 2.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.83476
[1mStep[0m  [10/106], [94mLoss[0m : 2.63571
[1mStep[0m  [20/106], [94mLoss[0m : 2.37140
[1mStep[0m  [30/106], [94mLoss[0m : 2.39984
[1mStep[0m  [40/106], [94mLoss[0m : 2.87745
[1mStep[0m  [50/106], [94mLoss[0m : 2.51930
[1mStep[0m  [60/106], [94mLoss[0m : 2.89602
[1mStep[0m  [70/106], [94mLoss[0m : 2.93142
[1mStep[0m  [80/106], [94mLoss[0m : 2.61161
[1mStep[0m  [90/106], [94mLoss[0m : 2.64889
[1mStep[0m  [100/106], [94mLoss[0m : 2.74276

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.770, [92mTest[0m: 2.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62989
[1mStep[0m  [10/106], [94mLoss[0m : 2.56160
[1mStep[0m  [20/106], [94mLoss[0m : 2.81191
[1mStep[0m  [30/106], [94mLoss[0m : 2.81795
[1mStep[0m  [40/106], [94mLoss[0m : 2.62634
[1mStep[0m  [50/106], [94mLoss[0m : 2.63051
[1mStep[0m  [60/106], [94mLoss[0m : 2.41716
[1mStep[0m  [70/106], [94mLoss[0m : 2.42006
[1mStep[0m  [80/106], [94mLoss[0m : 2.69661
[1mStep[0m  [90/106], [94mLoss[0m : 2.59626
[1mStep[0m  [100/106], [94mLoss[0m : 2.82778

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.738, [92mTest[0m: 2.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90399
[1mStep[0m  [10/106], [94mLoss[0m : 2.83306
[1mStep[0m  [20/106], [94mLoss[0m : 3.00415
[1mStep[0m  [30/106], [94mLoss[0m : 2.86209
[1mStep[0m  [40/106], [94mLoss[0m : 2.25447
[1mStep[0m  [50/106], [94mLoss[0m : 3.04547
[1mStep[0m  [60/106], [94mLoss[0m : 2.41252
[1mStep[0m  [70/106], [94mLoss[0m : 3.13568
[1mStep[0m  [80/106], [94mLoss[0m : 2.64002
[1mStep[0m  [90/106], [94mLoss[0m : 2.45894
[1mStep[0m  [100/106], [94mLoss[0m : 2.50704

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.731, [92mTest[0m: 2.567, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49940
[1mStep[0m  [10/106], [94mLoss[0m : 2.61231
[1mStep[0m  [20/106], [94mLoss[0m : 2.62075
[1mStep[0m  [30/106], [94mLoss[0m : 2.78069
[1mStep[0m  [40/106], [94mLoss[0m : 2.48542
[1mStep[0m  [50/106], [94mLoss[0m : 2.96677
[1mStep[0m  [60/106], [94mLoss[0m : 2.82221
[1mStep[0m  [70/106], [94mLoss[0m : 2.38734
[1mStep[0m  [80/106], [94mLoss[0m : 3.08693
[1mStep[0m  [90/106], [94mLoss[0m : 2.33447
[1mStep[0m  [100/106], [94mLoss[0m : 2.93669

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.542, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39508
[1mStep[0m  [10/106], [94mLoss[0m : 2.85826
[1mStep[0m  [20/106], [94mLoss[0m : 2.63428
[1mStep[0m  [30/106], [94mLoss[0m : 2.63505
[1mStep[0m  [40/106], [94mLoss[0m : 2.76086
[1mStep[0m  [50/106], [94mLoss[0m : 2.71236
[1mStep[0m  [60/106], [94mLoss[0m : 2.68953
[1mStep[0m  [70/106], [94mLoss[0m : 2.65274
[1mStep[0m  [80/106], [94mLoss[0m : 2.82221
[1mStep[0m  [90/106], [94mLoss[0m : 2.48920
[1mStep[0m  [100/106], [94mLoss[0m : 2.69437

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.535, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.83772
[1mStep[0m  [10/106], [94mLoss[0m : 2.59478
[1mStep[0m  [20/106], [94mLoss[0m : 2.59639
[1mStep[0m  [30/106], [94mLoss[0m : 2.73289
[1mStep[0m  [40/106], [94mLoss[0m : 2.96530
[1mStep[0m  [50/106], [94mLoss[0m : 2.64291
[1mStep[0m  [60/106], [94mLoss[0m : 2.85996
[1mStep[0m  [70/106], [94mLoss[0m : 2.75393
[1mStep[0m  [80/106], [94mLoss[0m : 2.60204
[1mStep[0m  [90/106], [94mLoss[0m : 2.63902
[1mStep[0m  [100/106], [94mLoss[0m : 2.65609

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.521, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45534
[1mStep[0m  [10/106], [94mLoss[0m : 2.61562
[1mStep[0m  [20/106], [94mLoss[0m : 2.69488
[1mStep[0m  [30/106], [94mLoss[0m : 2.67009
[1mStep[0m  [40/106], [94mLoss[0m : 2.74043
[1mStep[0m  [50/106], [94mLoss[0m : 2.71300
[1mStep[0m  [60/106], [94mLoss[0m : 2.76137
[1mStep[0m  [70/106], [94mLoss[0m : 3.01398
[1mStep[0m  [80/106], [94mLoss[0m : 2.77294
[1mStep[0m  [90/106], [94mLoss[0m : 2.28884
[1mStep[0m  [100/106], [94mLoss[0m : 2.68164

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84044
[1mStep[0m  [10/106], [94mLoss[0m : 2.15820
[1mStep[0m  [20/106], [94mLoss[0m : 2.69113
[1mStep[0m  [30/106], [94mLoss[0m : 2.89021
[1mStep[0m  [40/106], [94mLoss[0m : 2.72973
[1mStep[0m  [50/106], [94mLoss[0m : 2.83984
[1mStep[0m  [60/106], [94mLoss[0m : 2.89008
[1mStep[0m  [70/106], [94mLoss[0m : 2.56036
[1mStep[0m  [80/106], [94mLoss[0m : 2.81668
[1mStep[0m  [90/106], [94mLoss[0m : 2.49208
[1mStep[0m  [100/106], [94mLoss[0m : 2.43662

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.503, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.08558
[1mStep[0m  [10/106], [94mLoss[0m : 2.73236
[1mStep[0m  [20/106], [94mLoss[0m : 2.50992
[1mStep[0m  [30/106], [94mLoss[0m : 2.66301
[1mStep[0m  [40/106], [94mLoss[0m : 2.74086
[1mStep[0m  [50/106], [94mLoss[0m : 2.22478
[1mStep[0m  [60/106], [94mLoss[0m : 2.41304
[1mStep[0m  [70/106], [94mLoss[0m : 2.78769
[1mStep[0m  [80/106], [94mLoss[0m : 2.82085
[1mStep[0m  [90/106], [94mLoss[0m : 2.59709
[1mStep[0m  [100/106], [94mLoss[0m : 2.60628

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.496, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.82048
[1mStep[0m  [10/106], [94mLoss[0m : 2.54447
[1mStep[0m  [20/106], [94mLoss[0m : 2.77637
[1mStep[0m  [30/106], [94mLoss[0m : 2.78880
[1mStep[0m  [40/106], [94mLoss[0m : 2.40036
[1mStep[0m  [50/106], [94mLoss[0m : 2.82211
[1mStep[0m  [60/106], [94mLoss[0m : 2.82397
[1mStep[0m  [70/106], [94mLoss[0m : 2.59874
[1mStep[0m  [80/106], [94mLoss[0m : 2.63017
[1mStep[0m  [90/106], [94mLoss[0m : 2.38791
[1mStep[0m  [100/106], [94mLoss[0m : 2.06489

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.491, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40566
[1mStep[0m  [10/106], [94mLoss[0m : 3.31868
[1mStep[0m  [20/106], [94mLoss[0m : 2.57894
[1mStep[0m  [30/106], [94mLoss[0m : 2.56573
[1mStep[0m  [40/106], [94mLoss[0m : 2.54463
[1mStep[0m  [50/106], [94mLoss[0m : 2.81571
[1mStep[0m  [60/106], [94mLoss[0m : 2.64844
[1mStep[0m  [70/106], [94mLoss[0m : 2.85065
[1mStep[0m  [80/106], [94mLoss[0m : 2.63190
[1mStep[0m  [90/106], [94mLoss[0m : 2.65081
[1mStep[0m  [100/106], [94mLoss[0m : 2.85529

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.483, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49651
[1mStep[0m  [10/106], [94mLoss[0m : 2.50734
[1mStep[0m  [20/106], [94mLoss[0m : 2.35357
[1mStep[0m  [30/106], [94mLoss[0m : 2.67045
[1mStep[0m  [40/106], [94mLoss[0m : 2.64705
[1mStep[0m  [50/106], [94mLoss[0m : 2.49225
[1mStep[0m  [60/106], [94mLoss[0m : 2.54103
[1mStep[0m  [70/106], [94mLoss[0m : 2.60714
[1mStep[0m  [80/106], [94mLoss[0m : 2.30971
[1mStep[0m  [90/106], [94mLoss[0m : 2.66539
[1mStep[0m  [100/106], [94mLoss[0m : 2.97754

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.479, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.91776
[1mStep[0m  [10/106], [94mLoss[0m : 2.58305
[1mStep[0m  [20/106], [94mLoss[0m : 2.52013
[1mStep[0m  [30/106], [94mLoss[0m : 2.78641
[1mStep[0m  [40/106], [94mLoss[0m : 2.59805
[1mStep[0m  [50/106], [94mLoss[0m : 2.60701
[1mStep[0m  [60/106], [94mLoss[0m : 2.39378
[1mStep[0m  [70/106], [94mLoss[0m : 2.93307
[1mStep[0m  [80/106], [94mLoss[0m : 2.92100
[1mStep[0m  [90/106], [94mLoss[0m : 2.78311
[1mStep[0m  [100/106], [94mLoss[0m : 2.32353

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.476, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.83894
[1mStep[0m  [10/106], [94mLoss[0m : 2.60399
[1mStep[0m  [20/106], [94mLoss[0m : 2.53975
[1mStep[0m  [30/106], [94mLoss[0m : 2.98635
[1mStep[0m  [40/106], [94mLoss[0m : 2.42296
[1mStep[0m  [50/106], [94mLoss[0m : 2.62510
[1mStep[0m  [60/106], [94mLoss[0m : 2.35870
[1mStep[0m  [70/106], [94mLoss[0m : 2.73579
[1mStep[0m  [80/106], [94mLoss[0m : 2.48234
[1mStep[0m  [90/106], [94mLoss[0m : 2.72633
[1mStep[0m  [100/106], [94mLoss[0m : 2.85246

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.471, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.79184
[1mStep[0m  [10/106], [94mLoss[0m : 2.67008
[1mStep[0m  [20/106], [94mLoss[0m : 2.52130
[1mStep[0m  [30/106], [94mLoss[0m : 2.54312
[1mStep[0m  [40/106], [94mLoss[0m : 2.83276
[1mStep[0m  [50/106], [94mLoss[0m : 2.68486
[1mStep[0m  [60/106], [94mLoss[0m : 2.57357
[1mStep[0m  [70/106], [94mLoss[0m : 2.81237
[1mStep[0m  [80/106], [94mLoss[0m : 2.74774
[1mStep[0m  [90/106], [94mLoss[0m : 2.48382
[1mStep[0m  [100/106], [94mLoss[0m : 2.81220

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.470, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65768
[1mStep[0m  [10/106], [94mLoss[0m : 2.62713
[1mStep[0m  [20/106], [94mLoss[0m : 2.74494
[1mStep[0m  [30/106], [94mLoss[0m : 2.71897
[1mStep[0m  [40/106], [94mLoss[0m : 2.62843
[1mStep[0m  [50/106], [94mLoss[0m : 2.55293
[1mStep[0m  [60/106], [94mLoss[0m : 2.37861
[1mStep[0m  [70/106], [94mLoss[0m : 2.69801
[1mStep[0m  [80/106], [94mLoss[0m : 2.96681
[1mStep[0m  [90/106], [94mLoss[0m : 2.32954
[1mStep[0m  [100/106], [94mLoss[0m : 2.79542

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.468, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.466
====================================

Phase 2 - Evaluation MAE:  2.4659393598448553
MAE score P1       3.348378
MAE score P2       2.465939
loss               2.614976
learning_rate        0.0001
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 10, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.57331
[1mStep[0m  [10/106], [94mLoss[0m : 10.77855
[1mStep[0m  [20/106], [94mLoss[0m : 10.26232
[1mStep[0m  [30/106], [94mLoss[0m : 9.80629
[1mStep[0m  [40/106], [94mLoss[0m : 10.09720
[1mStep[0m  [50/106], [94mLoss[0m : 9.82775
[1mStep[0m  [60/106], [94mLoss[0m : 9.65684
[1mStep[0m  [70/106], [94mLoss[0m : 9.77979
[1mStep[0m  [80/106], [94mLoss[0m : 8.99883
[1mStep[0m  [90/106], [94mLoss[0m : 9.41807
[1mStep[0m  [100/106], [94mLoss[0m : 9.26588

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.762, [92mTest[0m: 10.407, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.31886
[1mStep[0m  [10/106], [94mLoss[0m : 9.23799
[1mStep[0m  [20/106], [94mLoss[0m : 7.87814
[1mStep[0m  [30/106], [94mLoss[0m : 8.86795
[1mStep[0m  [40/106], [94mLoss[0m : 8.29286
[1mStep[0m  [50/106], [94mLoss[0m : 8.73662
[1mStep[0m  [60/106], [94mLoss[0m : 8.49304
[1mStep[0m  [70/106], [94mLoss[0m : 8.63585
[1mStep[0m  [80/106], [94mLoss[0m : 8.46496
[1mStep[0m  [90/106], [94mLoss[0m : 8.43416
[1mStep[0m  [100/106], [94mLoss[0m : 7.64000

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.415, [92mTest[0m: 9.062, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.12370
[1mStep[0m  [10/106], [94mLoss[0m : 7.53459
[1mStep[0m  [20/106], [94mLoss[0m : 7.27079
[1mStep[0m  [30/106], [94mLoss[0m : 7.40217
[1mStep[0m  [40/106], [94mLoss[0m : 7.18181
[1mStep[0m  [50/106], [94mLoss[0m : 7.16509
[1mStep[0m  [60/106], [94mLoss[0m : 6.81288
[1mStep[0m  [70/106], [94mLoss[0m : 6.27937
[1mStep[0m  [80/106], [94mLoss[0m : 6.32539
[1mStep[0m  [90/106], [94mLoss[0m : 6.68737
[1mStep[0m  [100/106], [94mLoss[0m : 6.42685

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.062, [92mTest[0m: 7.729, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.29071
[1mStep[0m  [10/106], [94mLoss[0m : 6.13510
[1mStep[0m  [20/106], [94mLoss[0m : 5.85562
[1mStep[0m  [30/106], [94mLoss[0m : 5.94484
[1mStep[0m  [40/106], [94mLoss[0m : 5.92442
[1mStep[0m  [50/106], [94mLoss[0m : 5.92831
[1mStep[0m  [60/106], [94mLoss[0m : 5.71037
[1mStep[0m  [70/106], [94mLoss[0m : 5.50989
[1mStep[0m  [80/106], [94mLoss[0m : 5.62309
[1mStep[0m  [90/106], [94mLoss[0m : 4.89787
[1mStep[0m  [100/106], [94mLoss[0m : 4.93537

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.741, [92mTest[0m: 6.383, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.31021
[1mStep[0m  [10/106], [94mLoss[0m : 4.79958
[1mStep[0m  [20/106], [94mLoss[0m : 5.07674
[1mStep[0m  [30/106], [94mLoss[0m : 5.17649
[1mStep[0m  [40/106], [94mLoss[0m : 4.22624
[1mStep[0m  [50/106], [94mLoss[0m : 4.83384
[1mStep[0m  [60/106], [94mLoss[0m : 4.67010
[1mStep[0m  [70/106], [94mLoss[0m : 4.41493
[1mStep[0m  [80/106], [94mLoss[0m : 4.37170
[1mStep[0m  [90/106], [94mLoss[0m : 4.70358
[1mStep[0m  [100/106], [94mLoss[0m : 4.63008

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.605, [92mTest[0m: 5.094, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.53988
[1mStep[0m  [10/106], [94mLoss[0m : 3.97530
[1mStep[0m  [20/106], [94mLoss[0m : 3.92561
[1mStep[0m  [30/106], [94mLoss[0m : 4.05363
[1mStep[0m  [40/106], [94mLoss[0m : 3.82291
[1mStep[0m  [50/106], [94mLoss[0m : 3.92388
[1mStep[0m  [60/106], [94mLoss[0m : 3.90864
[1mStep[0m  [70/106], [94mLoss[0m : 3.85513
[1mStep[0m  [80/106], [94mLoss[0m : 3.61739
[1mStep[0m  [90/106], [94mLoss[0m : 3.54938
[1mStep[0m  [100/106], [94mLoss[0m : 3.24422

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.835, [92mTest[0m: 4.143, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.53210
[1mStep[0m  [10/106], [94mLoss[0m : 3.55718
[1mStep[0m  [20/106], [94mLoss[0m : 4.03697
[1mStep[0m  [30/106], [94mLoss[0m : 3.15840
[1mStep[0m  [40/106], [94mLoss[0m : 3.24614
[1mStep[0m  [50/106], [94mLoss[0m : 2.96706
[1mStep[0m  [60/106], [94mLoss[0m : 3.70747
[1mStep[0m  [70/106], [94mLoss[0m : 3.26965
[1mStep[0m  [80/106], [94mLoss[0m : 3.64018
[1mStep[0m  [90/106], [94mLoss[0m : 3.29258
[1mStep[0m  [100/106], [94mLoss[0m : 3.09022

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.397, [92mTest[0m: 3.544, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.03264
[1mStep[0m  [10/106], [94mLoss[0m : 3.21340
[1mStep[0m  [20/106], [94mLoss[0m : 3.23996
[1mStep[0m  [30/106], [94mLoss[0m : 2.86317
[1mStep[0m  [40/106], [94mLoss[0m : 3.29121
[1mStep[0m  [50/106], [94mLoss[0m : 3.34878
[1mStep[0m  [60/106], [94mLoss[0m : 2.97248
[1mStep[0m  [70/106], [94mLoss[0m : 3.29721
[1mStep[0m  [80/106], [94mLoss[0m : 3.11170
[1mStep[0m  [90/106], [94mLoss[0m : 3.18135
[1mStep[0m  [100/106], [94mLoss[0m : 2.89904

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.137, [92mTest[0m: 3.202, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.21210
[1mStep[0m  [10/106], [94mLoss[0m : 3.27625
[1mStep[0m  [20/106], [94mLoss[0m : 2.97542
[1mStep[0m  [30/106], [94mLoss[0m : 3.38223
[1mStep[0m  [40/106], [94mLoss[0m : 3.23045
[1mStep[0m  [50/106], [94mLoss[0m : 3.08294
[1mStep[0m  [60/106], [94mLoss[0m : 2.83501
[1mStep[0m  [70/106], [94mLoss[0m : 2.73938
[1mStep[0m  [80/106], [94mLoss[0m : 2.98582
[1mStep[0m  [90/106], [94mLoss[0m : 3.16358
[1mStep[0m  [100/106], [94mLoss[0m : 2.89441

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.992, [92mTest[0m: 3.012, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.32902
[1mStep[0m  [10/106], [94mLoss[0m : 3.18405
[1mStep[0m  [20/106], [94mLoss[0m : 2.82603
[1mStep[0m  [30/106], [94mLoss[0m : 3.08994
[1mStep[0m  [40/106], [94mLoss[0m : 3.05111
[1mStep[0m  [50/106], [94mLoss[0m : 3.00480
[1mStep[0m  [60/106], [94mLoss[0m : 3.09653
[1mStep[0m  [70/106], [94mLoss[0m : 2.91478
[1mStep[0m  [80/106], [94mLoss[0m : 3.15523
[1mStep[0m  [90/106], [94mLoss[0m : 2.77941
[1mStep[0m  [100/106], [94mLoss[0m : 3.12976

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.895, [92mTest[0m: 2.891, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60320
[1mStep[0m  [10/106], [94mLoss[0m : 3.48526
[1mStep[0m  [20/106], [94mLoss[0m : 2.68892
[1mStep[0m  [30/106], [94mLoss[0m : 2.94488
[1mStep[0m  [40/106], [94mLoss[0m : 2.80762
[1mStep[0m  [50/106], [94mLoss[0m : 2.90997
[1mStep[0m  [60/106], [94mLoss[0m : 3.07716
[1mStep[0m  [70/106], [94mLoss[0m : 2.48974
[1mStep[0m  [80/106], [94mLoss[0m : 2.83283
[1mStep[0m  [90/106], [94mLoss[0m : 2.69327
[1mStep[0m  [100/106], [94mLoss[0m : 2.83329

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.827, [92mTest[0m: 2.814, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77293
[1mStep[0m  [10/106], [94mLoss[0m : 2.30657
[1mStep[0m  [20/106], [94mLoss[0m : 2.56696
[1mStep[0m  [30/106], [94mLoss[0m : 2.47772
[1mStep[0m  [40/106], [94mLoss[0m : 2.81998
[1mStep[0m  [50/106], [94mLoss[0m : 2.57996
[1mStep[0m  [60/106], [94mLoss[0m : 2.95558
[1mStep[0m  [70/106], [94mLoss[0m : 2.86579
[1mStep[0m  [80/106], [94mLoss[0m : 2.83598
[1mStep[0m  [90/106], [94mLoss[0m : 3.20576
[1mStep[0m  [100/106], [94mLoss[0m : 2.94368

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.782, [92mTest[0m: 2.750, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.83347
[1mStep[0m  [10/106], [94mLoss[0m : 2.82172
[1mStep[0m  [20/106], [94mLoss[0m : 2.42656
[1mStep[0m  [30/106], [94mLoss[0m : 2.52598
[1mStep[0m  [40/106], [94mLoss[0m : 2.60734
[1mStep[0m  [50/106], [94mLoss[0m : 3.00792
[1mStep[0m  [60/106], [94mLoss[0m : 2.82754
[1mStep[0m  [70/106], [94mLoss[0m : 2.79975
[1mStep[0m  [80/106], [94mLoss[0m : 2.52479
[1mStep[0m  [90/106], [94mLoss[0m : 2.87235
[1mStep[0m  [100/106], [94mLoss[0m : 2.67412

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.748, [92mTest[0m: 2.708, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76733
[1mStep[0m  [10/106], [94mLoss[0m : 2.71331
[1mStep[0m  [20/106], [94mLoss[0m : 2.49583
[1mStep[0m  [30/106], [94mLoss[0m : 2.74479
[1mStep[0m  [40/106], [94mLoss[0m : 2.67954
[1mStep[0m  [50/106], [94mLoss[0m : 2.75526
[1mStep[0m  [60/106], [94mLoss[0m : 2.71428
[1mStep[0m  [70/106], [94mLoss[0m : 2.42061
[1mStep[0m  [80/106], [94mLoss[0m : 2.81918
[1mStep[0m  [90/106], [94mLoss[0m : 2.51116
[1mStep[0m  [100/106], [94mLoss[0m : 2.56823

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.710, [92mTest[0m: 2.678, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45457
[1mStep[0m  [10/106], [94mLoss[0m : 2.70504
[1mStep[0m  [20/106], [94mLoss[0m : 2.54875
[1mStep[0m  [30/106], [94mLoss[0m : 2.93690
[1mStep[0m  [40/106], [94mLoss[0m : 2.45571
[1mStep[0m  [50/106], [94mLoss[0m : 2.49733
[1mStep[0m  [60/106], [94mLoss[0m : 2.90088
[1mStep[0m  [70/106], [94mLoss[0m : 2.68337
[1mStep[0m  [80/106], [94mLoss[0m : 2.84702
[1mStep[0m  [90/106], [94mLoss[0m : 2.81875
[1mStep[0m  [100/106], [94mLoss[0m : 2.82604

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.651, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.01495
[1mStep[0m  [10/106], [94mLoss[0m : 2.56906
[1mStep[0m  [20/106], [94mLoss[0m : 2.84505
[1mStep[0m  [30/106], [94mLoss[0m : 2.80744
[1mStep[0m  [40/106], [94mLoss[0m : 2.68132
[1mStep[0m  [50/106], [94mLoss[0m : 2.61037
[1mStep[0m  [60/106], [94mLoss[0m : 2.68814
[1mStep[0m  [70/106], [94mLoss[0m : 2.81008
[1mStep[0m  [80/106], [94mLoss[0m : 2.79910
[1mStep[0m  [90/106], [94mLoss[0m : 2.57953
[1mStep[0m  [100/106], [94mLoss[0m : 2.45276

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.628, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37574
[1mStep[0m  [10/106], [94mLoss[0m : 2.73455
[1mStep[0m  [20/106], [94mLoss[0m : 2.54476
[1mStep[0m  [30/106], [94mLoss[0m : 2.62223
[1mStep[0m  [40/106], [94mLoss[0m : 2.57848
[1mStep[0m  [50/106], [94mLoss[0m : 2.93047
[1mStep[0m  [60/106], [94mLoss[0m : 2.63962
[1mStep[0m  [70/106], [94mLoss[0m : 2.49259
[1mStep[0m  [80/106], [94mLoss[0m : 2.48606
[1mStep[0m  [90/106], [94mLoss[0m : 2.49083
[1mStep[0m  [100/106], [94mLoss[0m : 2.56419

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.611, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.88331
[1mStep[0m  [10/106], [94mLoss[0m : 2.48625
[1mStep[0m  [20/106], [94mLoss[0m : 2.66489
[1mStep[0m  [30/106], [94mLoss[0m : 2.23745
[1mStep[0m  [40/106], [94mLoss[0m : 2.67029
[1mStep[0m  [50/106], [94mLoss[0m : 2.64851
[1mStep[0m  [60/106], [94mLoss[0m : 2.38353
[1mStep[0m  [70/106], [94mLoss[0m : 2.55088
[1mStep[0m  [80/106], [94mLoss[0m : 2.72370
[1mStep[0m  [90/106], [94mLoss[0m : 2.64409
[1mStep[0m  [100/106], [94mLoss[0m : 2.68597

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34010
[1mStep[0m  [10/106], [94mLoss[0m : 2.79750
[1mStep[0m  [20/106], [94mLoss[0m : 2.57616
[1mStep[0m  [30/106], [94mLoss[0m : 2.67708
[1mStep[0m  [40/106], [94mLoss[0m : 2.36837
[1mStep[0m  [50/106], [94mLoss[0m : 2.87413
[1mStep[0m  [60/106], [94mLoss[0m : 2.49513
[1mStep[0m  [70/106], [94mLoss[0m : 2.47677
[1mStep[0m  [80/106], [94mLoss[0m : 2.39030
[1mStep[0m  [90/106], [94mLoss[0m : 2.54517
[1mStep[0m  [100/106], [94mLoss[0m : 2.66193

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.588, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47307
[1mStep[0m  [10/106], [94mLoss[0m : 2.59642
[1mStep[0m  [20/106], [94mLoss[0m : 2.59972
[1mStep[0m  [30/106], [94mLoss[0m : 2.95161
[1mStep[0m  [40/106], [94mLoss[0m : 2.76410
[1mStep[0m  [50/106], [94mLoss[0m : 2.56944
[1mStep[0m  [60/106], [94mLoss[0m : 2.67583
[1mStep[0m  [70/106], [94mLoss[0m : 2.48462
[1mStep[0m  [80/106], [94mLoss[0m : 2.55057
[1mStep[0m  [90/106], [94mLoss[0m : 2.87614
[1mStep[0m  [100/106], [94mLoss[0m : 2.63208

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.575, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71293
[1mStep[0m  [10/106], [94mLoss[0m : 2.47912
[1mStep[0m  [20/106], [94mLoss[0m : 2.59215
[1mStep[0m  [30/106], [94mLoss[0m : 2.50539
[1mStep[0m  [40/106], [94mLoss[0m : 2.63029
[1mStep[0m  [50/106], [94mLoss[0m : 2.39647
[1mStep[0m  [60/106], [94mLoss[0m : 2.41565
[1mStep[0m  [70/106], [94mLoss[0m : 2.88729
[1mStep[0m  [80/106], [94mLoss[0m : 2.54996
[1mStep[0m  [90/106], [94mLoss[0m : 2.33867
[1mStep[0m  [100/106], [94mLoss[0m : 2.44924

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.572, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.87970
[1mStep[0m  [10/106], [94mLoss[0m : 2.60798
[1mStep[0m  [20/106], [94mLoss[0m : 2.48339
[1mStep[0m  [30/106], [94mLoss[0m : 2.62005
[1mStep[0m  [40/106], [94mLoss[0m : 2.50523
[1mStep[0m  [50/106], [94mLoss[0m : 2.44896
[1mStep[0m  [60/106], [94mLoss[0m : 2.81330
[1mStep[0m  [70/106], [94mLoss[0m : 2.38724
[1mStep[0m  [80/106], [94mLoss[0m : 2.53084
[1mStep[0m  [90/106], [94mLoss[0m : 2.60834
[1mStep[0m  [100/106], [94mLoss[0m : 2.74317

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77227
[1mStep[0m  [10/106], [94mLoss[0m : 2.72824
[1mStep[0m  [20/106], [94mLoss[0m : 2.40320
[1mStep[0m  [30/106], [94mLoss[0m : 2.75880
[1mStep[0m  [40/106], [94mLoss[0m : 2.64500
[1mStep[0m  [50/106], [94mLoss[0m : 2.65586
[1mStep[0m  [60/106], [94mLoss[0m : 2.60021
[1mStep[0m  [70/106], [94mLoss[0m : 2.68325
[1mStep[0m  [80/106], [94mLoss[0m : 2.64937
[1mStep[0m  [90/106], [94mLoss[0m : 2.57789
[1mStep[0m  [100/106], [94mLoss[0m : 2.63883

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.556, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55953
[1mStep[0m  [10/106], [94mLoss[0m : 2.77908
[1mStep[0m  [20/106], [94mLoss[0m : 2.44795
[1mStep[0m  [30/106], [94mLoss[0m : 2.36185
[1mStep[0m  [40/106], [94mLoss[0m : 2.63090
[1mStep[0m  [50/106], [94mLoss[0m : 2.53097
[1mStep[0m  [60/106], [94mLoss[0m : 2.62017
[1mStep[0m  [70/106], [94mLoss[0m : 2.73189
[1mStep[0m  [80/106], [94mLoss[0m : 2.65217
[1mStep[0m  [90/106], [94mLoss[0m : 2.73826
[1mStep[0m  [100/106], [94mLoss[0m : 2.18446

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.548, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49742
[1mStep[0m  [10/106], [94mLoss[0m : 2.63102
[1mStep[0m  [20/106], [94mLoss[0m : 2.65516
[1mStep[0m  [30/106], [94mLoss[0m : 2.54220
[1mStep[0m  [40/106], [94mLoss[0m : 2.63005
[1mStep[0m  [50/106], [94mLoss[0m : 2.66987
[1mStep[0m  [60/106], [94mLoss[0m : 2.53026
[1mStep[0m  [70/106], [94mLoss[0m : 2.55810
[1mStep[0m  [80/106], [94mLoss[0m : 2.73816
[1mStep[0m  [90/106], [94mLoss[0m : 2.44689
[1mStep[0m  [100/106], [94mLoss[0m : 2.72659

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.542, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65210
[1mStep[0m  [10/106], [94mLoss[0m : 2.54268
[1mStep[0m  [20/106], [94mLoss[0m : 2.48983
[1mStep[0m  [30/106], [94mLoss[0m : 2.70825
[1mStep[0m  [40/106], [94mLoss[0m : 2.32114
[1mStep[0m  [50/106], [94mLoss[0m : 2.52947
[1mStep[0m  [60/106], [94mLoss[0m : 2.45795
[1mStep[0m  [70/106], [94mLoss[0m : 2.63630
[1mStep[0m  [80/106], [94mLoss[0m : 2.28761
[1mStep[0m  [90/106], [94mLoss[0m : 2.57478
[1mStep[0m  [100/106], [94mLoss[0m : 2.45522

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.542, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58031
[1mStep[0m  [10/106], [94mLoss[0m : 2.64861
[1mStep[0m  [20/106], [94mLoss[0m : 2.72120
[1mStep[0m  [30/106], [94mLoss[0m : 2.59301
[1mStep[0m  [40/106], [94mLoss[0m : 2.75461
[1mStep[0m  [50/106], [94mLoss[0m : 2.65120
[1mStep[0m  [60/106], [94mLoss[0m : 2.78249
[1mStep[0m  [70/106], [94mLoss[0m : 2.18148
[1mStep[0m  [80/106], [94mLoss[0m : 2.66790
[1mStep[0m  [90/106], [94mLoss[0m : 2.36236
[1mStep[0m  [100/106], [94mLoss[0m : 2.62517

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.537, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69056
[1mStep[0m  [10/106], [94mLoss[0m : 2.84384
[1mStep[0m  [20/106], [94mLoss[0m : 2.47036
[1mStep[0m  [30/106], [94mLoss[0m : 2.56105
[1mStep[0m  [40/106], [94mLoss[0m : 2.48705
[1mStep[0m  [50/106], [94mLoss[0m : 2.55000
[1mStep[0m  [60/106], [94mLoss[0m : 2.88625
[1mStep[0m  [70/106], [94mLoss[0m : 2.29781
[1mStep[0m  [80/106], [94mLoss[0m : 2.66940
[1mStep[0m  [90/106], [94mLoss[0m : 2.53070
[1mStep[0m  [100/106], [94mLoss[0m : 2.59768

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.533, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78528
[1mStep[0m  [10/106], [94mLoss[0m : 2.46380
[1mStep[0m  [20/106], [94mLoss[0m : 3.05382
[1mStep[0m  [30/106], [94mLoss[0m : 2.45642
[1mStep[0m  [40/106], [94mLoss[0m : 2.52945
[1mStep[0m  [50/106], [94mLoss[0m : 2.54478
[1mStep[0m  [60/106], [94mLoss[0m : 2.26993
[1mStep[0m  [70/106], [94mLoss[0m : 2.55035
[1mStep[0m  [80/106], [94mLoss[0m : 2.18122
[1mStep[0m  [90/106], [94mLoss[0m : 2.90141
[1mStep[0m  [100/106], [94mLoss[0m : 2.54169

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.532, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94681
[1mStep[0m  [10/106], [94mLoss[0m : 2.49582
[1mStep[0m  [20/106], [94mLoss[0m : 2.65932
[1mStep[0m  [30/106], [94mLoss[0m : 2.54363
[1mStep[0m  [40/106], [94mLoss[0m : 2.57304
[1mStep[0m  [50/106], [94mLoss[0m : 2.74574
[1mStep[0m  [60/106], [94mLoss[0m : 2.39516
[1mStep[0m  [70/106], [94mLoss[0m : 2.43198
[1mStep[0m  [80/106], [94mLoss[0m : 2.85650
[1mStep[0m  [90/106], [94mLoss[0m : 2.49955
[1mStep[0m  [100/106], [94mLoss[0m : 2.65410

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.522, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.528
====================================

Phase 1 - Evaluation MAE:  2.528340978442498
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.41347
[1mStep[0m  [10/106], [94mLoss[0m : 2.84962
[1mStep[0m  [20/106], [94mLoss[0m : 2.59869
[1mStep[0m  [30/106], [94mLoss[0m : 2.57852
[1mStep[0m  [40/106], [94mLoss[0m : 2.41131
[1mStep[0m  [50/106], [94mLoss[0m : 2.30872
[1mStep[0m  [60/106], [94mLoss[0m : 2.64083
[1mStep[0m  [70/106], [94mLoss[0m : 2.32606
[1mStep[0m  [80/106], [94mLoss[0m : 2.45020
[1mStep[0m  [90/106], [94mLoss[0m : 2.45896
[1mStep[0m  [100/106], [94mLoss[0m : 2.52891

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.530, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68669
[1mStep[0m  [10/106], [94mLoss[0m : 2.74169
[1mStep[0m  [20/106], [94mLoss[0m : 2.33779
[1mStep[0m  [30/106], [94mLoss[0m : 2.52700
[1mStep[0m  [40/106], [94mLoss[0m : 2.42528
[1mStep[0m  [50/106], [94mLoss[0m : 2.40751
[1mStep[0m  [60/106], [94mLoss[0m : 2.50980
[1mStep[0m  [70/106], [94mLoss[0m : 2.23050
[1mStep[0m  [80/106], [94mLoss[0m : 2.55933
[1mStep[0m  [90/106], [94mLoss[0m : 2.68491
[1mStep[0m  [100/106], [94mLoss[0m : 2.21825

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.516, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40096
[1mStep[0m  [10/106], [94mLoss[0m : 2.54373
[1mStep[0m  [20/106], [94mLoss[0m : 2.68446
[1mStep[0m  [30/106], [94mLoss[0m : 2.62030
[1mStep[0m  [40/106], [94mLoss[0m : 2.42968
[1mStep[0m  [50/106], [94mLoss[0m : 2.47631
[1mStep[0m  [60/106], [94mLoss[0m : 2.62169
[1mStep[0m  [70/106], [94mLoss[0m : 2.32238
[1mStep[0m  [80/106], [94mLoss[0m : 2.70644
[1mStep[0m  [90/106], [94mLoss[0m : 2.58150
[1mStep[0m  [100/106], [94mLoss[0m : 2.68813

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.506, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42120
[1mStep[0m  [10/106], [94mLoss[0m : 2.56575
[1mStep[0m  [20/106], [94mLoss[0m : 2.28901
[1mStep[0m  [30/106], [94mLoss[0m : 2.58051
[1mStep[0m  [40/106], [94mLoss[0m : 2.52235
[1mStep[0m  [50/106], [94mLoss[0m : 2.44558
[1mStep[0m  [60/106], [94mLoss[0m : 2.32390
[1mStep[0m  [70/106], [94mLoss[0m : 2.37991
[1mStep[0m  [80/106], [94mLoss[0m : 2.77713
[1mStep[0m  [90/106], [94mLoss[0m : 2.35742
[1mStep[0m  [100/106], [94mLoss[0m : 2.59249

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.500, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61844
[1mStep[0m  [10/106], [94mLoss[0m : 2.52129
[1mStep[0m  [20/106], [94mLoss[0m : 2.75720
[1mStep[0m  [30/106], [94mLoss[0m : 2.62226
[1mStep[0m  [40/106], [94mLoss[0m : 2.82123
[1mStep[0m  [50/106], [94mLoss[0m : 2.86188
[1mStep[0m  [60/106], [94mLoss[0m : 2.40453
[1mStep[0m  [70/106], [94mLoss[0m : 2.61442
[1mStep[0m  [80/106], [94mLoss[0m : 2.50650
[1mStep[0m  [90/106], [94mLoss[0m : 2.46798
[1mStep[0m  [100/106], [94mLoss[0m : 2.34922

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.500, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62247
[1mStep[0m  [10/106], [94mLoss[0m : 2.47079
[1mStep[0m  [20/106], [94mLoss[0m : 2.28872
[1mStep[0m  [30/106], [94mLoss[0m : 2.69586
[1mStep[0m  [40/106], [94mLoss[0m : 2.32861
[1mStep[0m  [50/106], [94mLoss[0m : 2.59646
[1mStep[0m  [60/106], [94mLoss[0m : 2.10193
[1mStep[0m  [70/106], [94mLoss[0m : 2.30749
[1mStep[0m  [80/106], [94mLoss[0m : 2.80891
[1mStep[0m  [90/106], [94mLoss[0m : 2.54406
[1mStep[0m  [100/106], [94mLoss[0m : 2.34453

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31396
[1mStep[0m  [10/106], [94mLoss[0m : 2.52890
[1mStep[0m  [20/106], [94mLoss[0m : 2.62612
[1mStep[0m  [30/106], [94mLoss[0m : 2.44378
[1mStep[0m  [40/106], [94mLoss[0m : 2.39644
[1mStep[0m  [50/106], [94mLoss[0m : 2.46265
[1mStep[0m  [60/106], [94mLoss[0m : 2.42081
[1mStep[0m  [70/106], [94mLoss[0m : 2.48257
[1mStep[0m  [80/106], [94mLoss[0m : 2.50671
[1mStep[0m  [90/106], [94mLoss[0m : 2.65352
[1mStep[0m  [100/106], [94mLoss[0m : 2.44788

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24725
[1mStep[0m  [10/106], [94mLoss[0m : 2.61224
[1mStep[0m  [20/106], [94mLoss[0m : 2.71401
[1mStep[0m  [30/106], [94mLoss[0m : 2.40353
[1mStep[0m  [40/106], [94mLoss[0m : 2.87788
[1mStep[0m  [50/106], [94mLoss[0m : 2.45824
[1mStep[0m  [60/106], [94mLoss[0m : 2.14620
[1mStep[0m  [70/106], [94mLoss[0m : 2.58206
[1mStep[0m  [80/106], [94mLoss[0m : 2.63664
[1mStep[0m  [90/106], [94mLoss[0m : 2.68676
[1mStep[0m  [100/106], [94mLoss[0m : 2.67803

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.485, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80791
[1mStep[0m  [10/106], [94mLoss[0m : 2.12521
[1mStep[0m  [20/106], [94mLoss[0m : 2.19269
[1mStep[0m  [30/106], [94mLoss[0m : 2.44044
[1mStep[0m  [40/106], [94mLoss[0m : 2.27419
[1mStep[0m  [50/106], [94mLoss[0m : 2.33991
[1mStep[0m  [60/106], [94mLoss[0m : 2.62060
[1mStep[0m  [70/106], [94mLoss[0m : 2.52862
[1mStep[0m  [80/106], [94mLoss[0m : 2.56379
[1mStep[0m  [90/106], [94mLoss[0m : 2.29996
[1mStep[0m  [100/106], [94mLoss[0m : 2.47082

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.483, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51191
[1mStep[0m  [10/106], [94mLoss[0m : 2.46418
[1mStep[0m  [20/106], [94mLoss[0m : 2.38950
[1mStep[0m  [30/106], [94mLoss[0m : 2.50509
[1mStep[0m  [40/106], [94mLoss[0m : 2.64936
[1mStep[0m  [50/106], [94mLoss[0m : 2.59707
[1mStep[0m  [60/106], [94mLoss[0m : 2.34109
[1mStep[0m  [70/106], [94mLoss[0m : 2.44617
[1mStep[0m  [80/106], [94mLoss[0m : 2.51122
[1mStep[0m  [90/106], [94mLoss[0m : 2.26767
[1mStep[0m  [100/106], [94mLoss[0m : 2.50058

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60770
[1mStep[0m  [10/106], [94mLoss[0m : 2.72667
[1mStep[0m  [20/106], [94mLoss[0m : 2.49208
[1mStep[0m  [30/106], [94mLoss[0m : 2.44159
[1mStep[0m  [40/106], [94mLoss[0m : 2.29783
[1mStep[0m  [50/106], [94mLoss[0m : 2.46533
[1mStep[0m  [60/106], [94mLoss[0m : 2.67674
[1mStep[0m  [70/106], [94mLoss[0m : 2.61191
[1mStep[0m  [80/106], [94mLoss[0m : 2.62201
[1mStep[0m  [90/106], [94mLoss[0m : 2.33893
[1mStep[0m  [100/106], [94mLoss[0m : 2.61294

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.475, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69277
[1mStep[0m  [10/106], [94mLoss[0m : 2.50373
[1mStep[0m  [20/106], [94mLoss[0m : 2.48960
[1mStep[0m  [30/106], [94mLoss[0m : 2.82561
[1mStep[0m  [40/106], [94mLoss[0m : 2.41428
[1mStep[0m  [50/106], [94mLoss[0m : 2.48325
[1mStep[0m  [60/106], [94mLoss[0m : 2.65619
[1mStep[0m  [70/106], [94mLoss[0m : 2.68137
[1mStep[0m  [80/106], [94mLoss[0m : 2.72323
[1mStep[0m  [90/106], [94mLoss[0m : 2.91017
[1mStep[0m  [100/106], [94mLoss[0m : 2.73909

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46200
[1mStep[0m  [10/106], [94mLoss[0m : 2.27439
[1mStep[0m  [20/106], [94mLoss[0m : 2.19562
[1mStep[0m  [30/106], [94mLoss[0m : 2.12751
[1mStep[0m  [40/106], [94mLoss[0m : 2.39890
[1mStep[0m  [50/106], [94mLoss[0m : 2.46893
[1mStep[0m  [60/106], [94mLoss[0m : 2.67229
[1mStep[0m  [70/106], [94mLoss[0m : 2.56489
[1mStep[0m  [80/106], [94mLoss[0m : 2.54673
[1mStep[0m  [90/106], [94mLoss[0m : 2.69182
[1mStep[0m  [100/106], [94mLoss[0m : 2.55750

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40158
[1mStep[0m  [10/106], [94mLoss[0m : 2.73142
[1mStep[0m  [20/106], [94mLoss[0m : 2.55652
[1mStep[0m  [30/106], [94mLoss[0m : 2.71511
[1mStep[0m  [40/106], [94mLoss[0m : 2.54008
[1mStep[0m  [50/106], [94mLoss[0m : 2.33695
[1mStep[0m  [60/106], [94mLoss[0m : 2.22107
[1mStep[0m  [70/106], [94mLoss[0m : 2.56019
[1mStep[0m  [80/106], [94mLoss[0m : 2.29812
[1mStep[0m  [90/106], [94mLoss[0m : 2.63682
[1mStep[0m  [100/106], [94mLoss[0m : 2.43824

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.467, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55299
[1mStep[0m  [10/106], [94mLoss[0m : 2.57580
[1mStep[0m  [20/106], [94mLoss[0m : 2.63986
[1mStep[0m  [30/106], [94mLoss[0m : 2.58202
[1mStep[0m  [40/106], [94mLoss[0m : 2.48041
[1mStep[0m  [50/106], [94mLoss[0m : 2.85502
[1mStep[0m  [60/106], [94mLoss[0m : 2.67514
[1mStep[0m  [70/106], [94mLoss[0m : 2.60791
[1mStep[0m  [80/106], [94mLoss[0m : 2.42198
[1mStep[0m  [90/106], [94mLoss[0m : 2.25121
[1mStep[0m  [100/106], [94mLoss[0m : 2.33993

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56979
[1mStep[0m  [10/106], [94mLoss[0m : 2.63217
[1mStep[0m  [20/106], [94mLoss[0m : 2.48007
[1mStep[0m  [30/106], [94mLoss[0m : 2.50446
[1mStep[0m  [40/106], [94mLoss[0m : 2.27412
[1mStep[0m  [50/106], [94mLoss[0m : 2.42013
[1mStep[0m  [60/106], [94mLoss[0m : 2.46152
[1mStep[0m  [70/106], [94mLoss[0m : 2.55314
[1mStep[0m  [80/106], [94mLoss[0m : 2.51673
[1mStep[0m  [90/106], [94mLoss[0m : 2.56686
[1mStep[0m  [100/106], [94mLoss[0m : 2.63823

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22126
[1mStep[0m  [10/106], [94mLoss[0m : 2.36496
[1mStep[0m  [20/106], [94mLoss[0m : 2.31008
[1mStep[0m  [30/106], [94mLoss[0m : 2.30202
[1mStep[0m  [40/106], [94mLoss[0m : 2.36776
[1mStep[0m  [50/106], [94mLoss[0m : 2.66109
[1mStep[0m  [60/106], [94mLoss[0m : 2.44675
[1mStep[0m  [70/106], [94mLoss[0m : 2.51995
[1mStep[0m  [80/106], [94mLoss[0m : 2.40679
[1mStep[0m  [90/106], [94mLoss[0m : 2.62884
[1mStep[0m  [100/106], [94mLoss[0m : 2.43747

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29950
[1mStep[0m  [10/106], [94mLoss[0m : 2.38451
[1mStep[0m  [20/106], [94mLoss[0m : 2.29727
[1mStep[0m  [30/106], [94mLoss[0m : 2.69703
[1mStep[0m  [40/106], [94mLoss[0m : 2.53621
[1mStep[0m  [50/106], [94mLoss[0m : 2.62470
[1mStep[0m  [60/106], [94mLoss[0m : 2.49274
[1mStep[0m  [70/106], [94mLoss[0m : 2.74154
[1mStep[0m  [80/106], [94mLoss[0m : 2.55357
[1mStep[0m  [90/106], [94mLoss[0m : 2.30865
[1mStep[0m  [100/106], [94mLoss[0m : 2.53596

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.462, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17414
[1mStep[0m  [10/106], [94mLoss[0m : 2.33753
[1mStep[0m  [20/106], [94mLoss[0m : 2.28220
[1mStep[0m  [30/106], [94mLoss[0m : 2.45352
[1mStep[0m  [40/106], [94mLoss[0m : 2.59440
[1mStep[0m  [50/106], [94mLoss[0m : 2.67390
[1mStep[0m  [60/106], [94mLoss[0m : 2.50514
[1mStep[0m  [70/106], [94mLoss[0m : 2.51051
[1mStep[0m  [80/106], [94mLoss[0m : 2.69757
[1mStep[0m  [90/106], [94mLoss[0m : 2.60371
[1mStep[0m  [100/106], [94mLoss[0m : 2.38493

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33949
[1mStep[0m  [10/106], [94mLoss[0m : 2.35858
[1mStep[0m  [20/106], [94mLoss[0m : 2.82089
[1mStep[0m  [30/106], [94mLoss[0m : 2.78304
[1mStep[0m  [40/106], [94mLoss[0m : 2.47061
[1mStep[0m  [50/106], [94mLoss[0m : 2.27201
[1mStep[0m  [60/106], [94mLoss[0m : 2.54492
[1mStep[0m  [70/106], [94mLoss[0m : 2.70267
[1mStep[0m  [80/106], [94mLoss[0m : 2.63290
[1mStep[0m  [90/106], [94mLoss[0m : 2.66652
[1mStep[0m  [100/106], [94mLoss[0m : 2.29240

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37888
[1mStep[0m  [10/106], [94mLoss[0m : 2.62253
[1mStep[0m  [20/106], [94mLoss[0m : 2.42206
[1mStep[0m  [30/106], [94mLoss[0m : 2.30670
[1mStep[0m  [40/106], [94mLoss[0m : 2.44936
[1mStep[0m  [50/106], [94mLoss[0m : 2.44711
[1mStep[0m  [60/106], [94mLoss[0m : 2.44229
[1mStep[0m  [70/106], [94mLoss[0m : 2.48299
[1mStep[0m  [80/106], [94mLoss[0m : 2.39474
[1mStep[0m  [90/106], [94mLoss[0m : 2.71411
[1mStep[0m  [100/106], [94mLoss[0m : 2.37444

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.466, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52666
[1mStep[0m  [10/106], [94mLoss[0m : 2.58692
[1mStep[0m  [20/106], [94mLoss[0m : 2.17731
[1mStep[0m  [30/106], [94mLoss[0m : 2.65243
[1mStep[0m  [40/106], [94mLoss[0m : 2.39564
[1mStep[0m  [50/106], [94mLoss[0m : 2.38126
[1mStep[0m  [60/106], [94mLoss[0m : 2.38848
[1mStep[0m  [70/106], [94mLoss[0m : 2.73100
[1mStep[0m  [80/106], [94mLoss[0m : 2.24099
[1mStep[0m  [90/106], [94mLoss[0m : 2.58086
[1mStep[0m  [100/106], [94mLoss[0m : 2.27351

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.461, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22188
[1mStep[0m  [10/106], [94mLoss[0m : 2.70991
[1mStep[0m  [20/106], [94mLoss[0m : 2.22541
[1mStep[0m  [30/106], [94mLoss[0m : 2.55738
[1mStep[0m  [40/106], [94mLoss[0m : 2.15446
[1mStep[0m  [50/106], [94mLoss[0m : 2.28092
[1mStep[0m  [60/106], [94mLoss[0m : 2.57014
[1mStep[0m  [70/106], [94mLoss[0m : 2.31996
[1mStep[0m  [80/106], [94mLoss[0m : 2.86171
[1mStep[0m  [90/106], [94mLoss[0m : 2.35670
[1mStep[0m  [100/106], [94mLoss[0m : 2.26012

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28754
[1mStep[0m  [10/106], [94mLoss[0m : 2.72465
[1mStep[0m  [20/106], [94mLoss[0m : 2.68412
[1mStep[0m  [30/106], [94mLoss[0m : 2.41150
[1mStep[0m  [40/106], [94mLoss[0m : 2.42550
[1mStep[0m  [50/106], [94mLoss[0m : 2.43337
[1mStep[0m  [60/106], [94mLoss[0m : 2.36181
[1mStep[0m  [70/106], [94mLoss[0m : 2.67785
[1mStep[0m  [80/106], [94mLoss[0m : 2.26631
[1mStep[0m  [90/106], [94mLoss[0m : 2.57972
[1mStep[0m  [100/106], [94mLoss[0m : 2.34212

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64515
[1mStep[0m  [10/106], [94mLoss[0m : 2.38840
[1mStep[0m  [20/106], [94mLoss[0m : 2.49592
[1mStep[0m  [30/106], [94mLoss[0m : 2.21698
[1mStep[0m  [40/106], [94mLoss[0m : 2.66769
[1mStep[0m  [50/106], [94mLoss[0m : 2.28683
[1mStep[0m  [60/106], [94mLoss[0m : 2.46099
[1mStep[0m  [70/106], [94mLoss[0m : 2.38866
[1mStep[0m  [80/106], [94mLoss[0m : 2.43535
[1mStep[0m  [90/106], [94mLoss[0m : 2.15962
[1mStep[0m  [100/106], [94mLoss[0m : 2.38358

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68968
[1mStep[0m  [10/106], [94mLoss[0m : 2.53293
[1mStep[0m  [20/106], [94mLoss[0m : 2.18440
[1mStep[0m  [30/106], [94mLoss[0m : 2.71379
[1mStep[0m  [40/106], [94mLoss[0m : 2.53018
[1mStep[0m  [50/106], [94mLoss[0m : 2.64015
[1mStep[0m  [60/106], [94mLoss[0m : 2.65534
[1mStep[0m  [70/106], [94mLoss[0m : 2.37605
[1mStep[0m  [80/106], [94mLoss[0m : 2.47593
[1mStep[0m  [90/106], [94mLoss[0m : 2.38246
[1mStep[0m  [100/106], [94mLoss[0m : 2.40820

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63705
[1mStep[0m  [10/106], [94mLoss[0m : 2.62545
[1mStep[0m  [20/106], [94mLoss[0m : 2.13455
[1mStep[0m  [30/106], [94mLoss[0m : 2.39573
[1mStep[0m  [40/106], [94mLoss[0m : 2.46494
[1mStep[0m  [50/106], [94mLoss[0m : 2.63407
[1mStep[0m  [60/106], [94mLoss[0m : 2.28699
[1mStep[0m  [70/106], [94mLoss[0m : 2.29928
[1mStep[0m  [80/106], [94mLoss[0m : 2.52266
[1mStep[0m  [90/106], [94mLoss[0m : 2.58328
[1mStep[0m  [100/106], [94mLoss[0m : 2.43548

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.465, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48741
[1mStep[0m  [10/106], [94mLoss[0m : 2.69574
[1mStep[0m  [20/106], [94mLoss[0m : 2.31401
[1mStep[0m  [30/106], [94mLoss[0m : 2.90467
[1mStep[0m  [40/106], [94mLoss[0m : 2.40326
[1mStep[0m  [50/106], [94mLoss[0m : 2.29303
[1mStep[0m  [60/106], [94mLoss[0m : 2.36790
[1mStep[0m  [70/106], [94mLoss[0m : 2.39690
[1mStep[0m  [80/106], [94mLoss[0m : 2.49146
[1mStep[0m  [90/106], [94mLoss[0m : 2.47770
[1mStep[0m  [100/106], [94mLoss[0m : 2.18291

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31542
[1mStep[0m  [10/106], [94mLoss[0m : 2.32208
[1mStep[0m  [20/106], [94mLoss[0m : 2.51820
[1mStep[0m  [30/106], [94mLoss[0m : 2.44634
[1mStep[0m  [40/106], [94mLoss[0m : 2.62280
[1mStep[0m  [50/106], [94mLoss[0m : 2.28997
[1mStep[0m  [60/106], [94mLoss[0m : 2.09781
[1mStep[0m  [70/106], [94mLoss[0m : 2.43569
[1mStep[0m  [80/106], [94mLoss[0m : 2.33044
[1mStep[0m  [90/106], [94mLoss[0m : 2.44528
[1mStep[0m  [100/106], [94mLoss[0m : 2.60446

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.460, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24462
[1mStep[0m  [10/106], [94mLoss[0m : 2.43185
[1mStep[0m  [20/106], [94mLoss[0m : 2.52596
[1mStep[0m  [30/106], [94mLoss[0m : 2.60329
[1mStep[0m  [40/106], [94mLoss[0m : 2.49134
[1mStep[0m  [50/106], [94mLoss[0m : 2.30070
[1mStep[0m  [60/106], [94mLoss[0m : 2.63785
[1mStep[0m  [70/106], [94mLoss[0m : 2.36553
[1mStep[0m  [80/106], [94mLoss[0m : 2.48275
[1mStep[0m  [90/106], [94mLoss[0m : 2.40478
[1mStep[0m  [100/106], [94mLoss[0m : 2.52109

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.457, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.450
====================================

Phase 2 - Evaluation MAE:  2.449782947324357
MAE score P1      2.528341
MAE score P2      2.449783
loss              2.441915
learning_rate       0.0001
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.96439
[1mStep[0m  [10/106], [94mLoss[0m : 11.11491
[1mStep[0m  [20/106], [94mLoss[0m : 11.59152
[1mStep[0m  [30/106], [94mLoss[0m : 10.65511
[1mStep[0m  [40/106], [94mLoss[0m : 11.24696
[1mStep[0m  [50/106], [94mLoss[0m : 10.50300
[1mStep[0m  [60/106], [94mLoss[0m : 10.72880
[1mStep[0m  [70/106], [94mLoss[0m : 11.14317
[1mStep[0m  [80/106], [94mLoss[0m : 11.11566
[1mStep[0m  [90/106], [94mLoss[0m : 11.33978
[1mStep[0m  [100/106], [94mLoss[0m : 10.63520

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.897, [92mTest[0m: 10.827, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.03429
[1mStep[0m  [10/106], [94mLoss[0m : 10.56277
[1mStep[0m  [20/106], [94mLoss[0m : 10.50416
[1mStep[0m  [30/106], [94mLoss[0m : 10.53076
[1mStep[0m  [40/106], [94mLoss[0m : 10.59492
[1mStep[0m  [50/106], [94mLoss[0m : 11.21847
[1mStep[0m  [60/106], [94mLoss[0m : 11.02715
[1mStep[0m  [70/106], [94mLoss[0m : 11.09029
[1mStep[0m  [80/106], [94mLoss[0m : 10.76130
[1mStep[0m  [90/106], [94mLoss[0m : 11.24250
[1mStep[0m  [100/106], [94mLoss[0m : 10.92736

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.881, [92mTest[0m: 10.862, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.75562
[1mStep[0m  [10/106], [94mLoss[0m : 10.69346
[1mStep[0m  [20/106], [94mLoss[0m : 11.04483
[1mStep[0m  [30/106], [94mLoss[0m : 9.92327
[1mStep[0m  [40/106], [94mLoss[0m : 10.69251
[1mStep[0m  [50/106], [94mLoss[0m : 10.56565
[1mStep[0m  [60/106], [94mLoss[0m : 10.77069
[1mStep[0m  [70/106], [94mLoss[0m : 10.83652
[1mStep[0m  [80/106], [94mLoss[0m : 11.06535
[1mStep[0m  [90/106], [94mLoss[0m : 10.90455
[1mStep[0m  [100/106], [94mLoss[0m : 11.24168

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.851, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.38236
[1mStep[0m  [10/106], [94mLoss[0m : 11.37256
[1mStep[0m  [20/106], [94mLoss[0m : 11.27514
[1mStep[0m  [30/106], [94mLoss[0m : 11.22912
[1mStep[0m  [40/106], [94mLoss[0m : 11.21123
[1mStep[0m  [50/106], [94mLoss[0m : 10.39359
[1mStep[0m  [60/106], [94mLoss[0m : 10.61780
[1mStep[0m  [70/106], [94mLoss[0m : 10.79908
[1mStep[0m  [80/106], [94mLoss[0m : 10.93286
[1mStep[0m  [90/106], [94mLoss[0m : 11.28705
[1mStep[0m  [100/106], [94mLoss[0m : 10.49892

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.832, [92mTest[0m: 10.808, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.45075
[1mStep[0m  [10/106], [94mLoss[0m : 11.34895
[1mStep[0m  [20/106], [94mLoss[0m : 10.63103
[1mStep[0m  [30/106], [94mLoss[0m : 10.99994
[1mStep[0m  [40/106], [94mLoss[0m : 10.73988
[1mStep[0m  [50/106], [94mLoss[0m : 11.10286
[1mStep[0m  [60/106], [94mLoss[0m : 10.74688
[1mStep[0m  [70/106], [94mLoss[0m : 10.75870
[1mStep[0m  [80/106], [94mLoss[0m : 10.49841
[1mStep[0m  [90/106], [94mLoss[0m : 11.50875
[1mStep[0m  [100/106], [94mLoss[0m : 10.40824

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.816, [92mTest[0m: 10.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.74461
[1mStep[0m  [10/106], [94mLoss[0m : 10.29037
[1mStep[0m  [20/106], [94mLoss[0m : 10.24887
[1mStep[0m  [30/106], [94mLoss[0m : 10.74495
[1mStep[0m  [40/106], [94mLoss[0m : 10.45353
[1mStep[0m  [50/106], [94mLoss[0m : 10.98460
[1mStep[0m  [60/106], [94mLoss[0m : 10.36486
[1mStep[0m  [70/106], [94mLoss[0m : 11.10684
[1mStep[0m  [80/106], [94mLoss[0m : 11.12149
[1mStep[0m  [90/106], [94mLoss[0m : 10.41950
[1mStep[0m  [100/106], [94mLoss[0m : 10.58119

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.799, [92mTest[0m: 10.745, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.93034
[1mStep[0m  [10/106], [94mLoss[0m : 11.12209
[1mStep[0m  [20/106], [94mLoss[0m : 10.83208
[1mStep[0m  [30/106], [94mLoss[0m : 10.35776
[1mStep[0m  [40/106], [94mLoss[0m : 10.89963
[1mStep[0m  [50/106], [94mLoss[0m : 10.93586
[1mStep[0m  [60/106], [94mLoss[0m : 10.95405
[1mStep[0m  [70/106], [94mLoss[0m : 11.27213
[1mStep[0m  [80/106], [94mLoss[0m : 11.32114
[1mStep[0m  [90/106], [94mLoss[0m : 11.01181
[1mStep[0m  [100/106], [94mLoss[0m : 10.74909

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.786, [92mTest[0m: 10.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.79875
[1mStep[0m  [10/106], [94mLoss[0m : 10.88225
[1mStep[0m  [20/106], [94mLoss[0m : 10.77102
[1mStep[0m  [30/106], [94mLoss[0m : 10.89612
[1mStep[0m  [40/106], [94mLoss[0m : 11.17541
[1mStep[0m  [50/106], [94mLoss[0m : 10.72479
[1mStep[0m  [60/106], [94mLoss[0m : 10.56150
[1mStep[0m  [70/106], [94mLoss[0m : 11.00676
[1mStep[0m  [80/106], [94mLoss[0m : 10.48113
[1mStep[0m  [90/106], [94mLoss[0m : 10.45135
[1mStep[0m  [100/106], [94mLoss[0m : 10.75512

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.757, [92mTest[0m: 10.702, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.67063
[1mStep[0m  [10/106], [94mLoss[0m : 10.55775
[1mStep[0m  [20/106], [94mLoss[0m : 10.48420
[1mStep[0m  [30/106], [94mLoss[0m : 10.94541
[1mStep[0m  [40/106], [94mLoss[0m : 10.79344
[1mStep[0m  [50/106], [94mLoss[0m : 10.50026
[1mStep[0m  [60/106], [94mLoss[0m : 11.15376
[1mStep[0m  [70/106], [94mLoss[0m : 10.96316
[1mStep[0m  [80/106], [94mLoss[0m : 11.03988
[1mStep[0m  [90/106], [94mLoss[0m : 10.64866
[1mStep[0m  [100/106], [94mLoss[0m : 10.69318

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.740, [92mTest[0m: 10.656, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.74312
[1mStep[0m  [10/106], [94mLoss[0m : 11.29923
[1mStep[0m  [20/106], [94mLoss[0m : 10.71222
[1mStep[0m  [30/106], [94mLoss[0m : 10.61353
[1mStep[0m  [40/106], [94mLoss[0m : 10.64348
[1mStep[0m  [50/106], [94mLoss[0m : 10.36650
[1mStep[0m  [60/106], [94mLoss[0m : 10.97069
[1mStep[0m  [70/106], [94mLoss[0m : 10.47062
[1mStep[0m  [80/106], [94mLoss[0m : 10.52164
[1mStep[0m  [90/106], [94mLoss[0m : 10.53043
[1mStep[0m  [100/106], [94mLoss[0m : 10.63705

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.729, [92mTest[0m: 10.634, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.79958
[1mStep[0m  [10/106], [94mLoss[0m : 10.81218
[1mStep[0m  [20/106], [94mLoss[0m : 10.21550
[1mStep[0m  [30/106], [94mLoss[0m : 11.13773
[1mStep[0m  [40/106], [94mLoss[0m : 10.59357
[1mStep[0m  [50/106], [94mLoss[0m : 10.74844
[1mStep[0m  [60/106], [94mLoss[0m : 10.33559
[1mStep[0m  [70/106], [94mLoss[0m : 10.89379
[1mStep[0m  [80/106], [94mLoss[0m : 10.12045
[1mStep[0m  [90/106], [94mLoss[0m : 9.98220
[1mStep[0m  [100/106], [94mLoss[0m : 10.39980

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.705, [92mTest[0m: 10.627, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.83788
[1mStep[0m  [10/106], [94mLoss[0m : 10.66334
[1mStep[0m  [20/106], [94mLoss[0m : 10.42997
[1mStep[0m  [30/106], [94mLoss[0m : 10.87064
[1mStep[0m  [40/106], [94mLoss[0m : 10.96645
[1mStep[0m  [50/106], [94mLoss[0m : 11.01085
[1mStep[0m  [60/106], [94mLoss[0m : 10.51067
[1mStep[0m  [70/106], [94mLoss[0m : 10.89665
[1mStep[0m  [80/106], [94mLoss[0m : 10.66263
[1mStep[0m  [90/106], [94mLoss[0m : 10.91941
[1mStep[0m  [100/106], [94mLoss[0m : 10.88167

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.682, [92mTest[0m: 10.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.70695
[1mStep[0m  [10/106], [94mLoss[0m : 10.67645
[1mStep[0m  [20/106], [94mLoss[0m : 10.83288
[1mStep[0m  [30/106], [94mLoss[0m : 10.55433
[1mStep[0m  [40/106], [94mLoss[0m : 10.84225
[1mStep[0m  [50/106], [94mLoss[0m : 10.77637
[1mStep[0m  [60/106], [94mLoss[0m : 10.87057
[1mStep[0m  [70/106], [94mLoss[0m : 10.87333
[1mStep[0m  [80/106], [94mLoss[0m : 11.01544
[1mStep[0m  [90/106], [94mLoss[0m : 10.66860
[1mStep[0m  [100/106], [94mLoss[0m : 10.09551

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.673, [92mTest[0m: 10.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.60268
[1mStep[0m  [10/106], [94mLoss[0m : 10.81133
[1mStep[0m  [20/106], [94mLoss[0m : 10.65393
[1mStep[0m  [30/106], [94mLoss[0m : 11.02398
[1mStep[0m  [40/106], [94mLoss[0m : 11.03655
[1mStep[0m  [50/106], [94mLoss[0m : 10.78544
[1mStep[0m  [60/106], [94mLoss[0m : 10.77495
[1mStep[0m  [70/106], [94mLoss[0m : 10.87891
[1mStep[0m  [80/106], [94mLoss[0m : 10.07523
[1mStep[0m  [90/106], [94mLoss[0m : 10.86461
[1mStep[0m  [100/106], [94mLoss[0m : 10.92341

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.652, [92mTest[0m: 10.546, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.56813
[1mStep[0m  [10/106], [94mLoss[0m : 10.50281
[1mStep[0m  [20/106], [94mLoss[0m : 10.07127
[1mStep[0m  [30/106], [94mLoss[0m : 11.41662
[1mStep[0m  [40/106], [94mLoss[0m : 10.51932
[1mStep[0m  [50/106], [94mLoss[0m : 10.53407
[1mStep[0m  [60/106], [94mLoss[0m : 10.60731
[1mStep[0m  [70/106], [94mLoss[0m : 10.73909
[1mStep[0m  [80/106], [94mLoss[0m : 10.74928
[1mStep[0m  [90/106], [94mLoss[0m : 10.55998
[1mStep[0m  [100/106], [94mLoss[0m : 10.56852

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.631, [92mTest[0m: 10.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.34651
[1mStep[0m  [10/106], [94mLoss[0m : 10.50935
[1mStep[0m  [20/106], [94mLoss[0m : 10.26423
[1mStep[0m  [30/106], [94mLoss[0m : 10.26857
[1mStep[0m  [40/106], [94mLoss[0m : 10.52343
[1mStep[0m  [50/106], [94mLoss[0m : 10.17554
[1mStep[0m  [60/106], [94mLoss[0m : 10.22127
[1mStep[0m  [70/106], [94mLoss[0m : 10.49617
[1mStep[0m  [80/106], [94mLoss[0m : 10.74158
[1mStep[0m  [90/106], [94mLoss[0m : 10.97306
[1mStep[0m  [100/106], [94mLoss[0m : 10.82573

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.603, [92mTest[0m: 10.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.91113
[1mStep[0m  [10/106], [94mLoss[0m : 11.02114
[1mStep[0m  [20/106], [94mLoss[0m : 10.79327
[1mStep[0m  [30/106], [94mLoss[0m : 10.69262
[1mStep[0m  [40/106], [94mLoss[0m : 10.19631
[1mStep[0m  [50/106], [94mLoss[0m : 11.00321
[1mStep[0m  [60/106], [94mLoss[0m : 10.42814
[1mStep[0m  [70/106], [94mLoss[0m : 10.75927
[1mStep[0m  [80/106], [94mLoss[0m : 10.39846
[1mStep[0m  [90/106], [94mLoss[0m : 10.72811
[1mStep[0m  [100/106], [94mLoss[0m : 10.47847

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.596, [92mTest[0m: 10.457, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.98405
[1mStep[0m  [10/106], [94mLoss[0m : 10.03314
[1mStep[0m  [20/106], [94mLoss[0m : 10.59661
[1mStep[0m  [30/106], [94mLoss[0m : 10.37088
[1mStep[0m  [40/106], [94mLoss[0m : 10.28670
[1mStep[0m  [50/106], [94mLoss[0m : 10.44586
[1mStep[0m  [60/106], [94mLoss[0m : 10.39614
[1mStep[0m  [70/106], [94mLoss[0m : 10.51910
[1mStep[0m  [80/106], [94mLoss[0m : 10.79062
[1mStep[0m  [90/106], [94mLoss[0m : 10.31131
[1mStep[0m  [100/106], [94mLoss[0m : 10.07343

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.568, [92mTest[0m: 10.434, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.87535
[1mStep[0m  [10/106], [94mLoss[0m : 10.28344
[1mStep[0m  [20/106], [94mLoss[0m : 10.40583
[1mStep[0m  [30/106], [94mLoss[0m : 10.63640
[1mStep[0m  [40/106], [94mLoss[0m : 11.05097
[1mStep[0m  [50/106], [94mLoss[0m : 10.42699
[1mStep[0m  [60/106], [94mLoss[0m : 10.23094
[1mStep[0m  [70/106], [94mLoss[0m : 10.48140
[1mStep[0m  [80/106], [94mLoss[0m : 10.68013
[1mStep[0m  [90/106], [94mLoss[0m : 10.43268
[1mStep[0m  [100/106], [94mLoss[0m : 10.52306

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.546, [92mTest[0m: 10.404, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.30738
[1mStep[0m  [10/106], [94mLoss[0m : 10.03800
[1mStep[0m  [20/106], [94mLoss[0m : 10.89801
[1mStep[0m  [30/106], [94mLoss[0m : 10.11141
[1mStep[0m  [40/106], [94mLoss[0m : 10.32569
[1mStep[0m  [50/106], [94mLoss[0m : 10.57177
[1mStep[0m  [60/106], [94mLoss[0m : 10.52151
[1mStep[0m  [70/106], [94mLoss[0m : 10.44354
[1mStep[0m  [80/106], [94mLoss[0m : 10.54373
[1mStep[0m  [90/106], [94mLoss[0m : 10.26799
[1mStep[0m  [100/106], [94mLoss[0m : 10.56034

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.535, [92mTest[0m: 10.364, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.38322
[1mStep[0m  [10/106], [94mLoss[0m : 10.63875
[1mStep[0m  [20/106], [94mLoss[0m : 10.51160
[1mStep[0m  [30/106], [94mLoss[0m : 10.45761
[1mStep[0m  [40/106], [94mLoss[0m : 10.22255
[1mStep[0m  [50/106], [94mLoss[0m : 10.68934
[1mStep[0m  [60/106], [94mLoss[0m : 9.97692
[1mStep[0m  [70/106], [94mLoss[0m : 10.74509
[1mStep[0m  [80/106], [94mLoss[0m : 11.00502
[1mStep[0m  [90/106], [94mLoss[0m : 10.50494
[1mStep[0m  [100/106], [94mLoss[0m : 10.36381

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.526, [92mTest[0m: 10.366, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.01403
[1mStep[0m  [10/106], [94mLoss[0m : 11.02160
[1mStep[0m  [20/106], [94mLoss[0m : 10.66082
[1mStep[0m  [30/106], [94mLoss[0m : 10.26521
[1mStep[0m  [40/106], [94mLoss[0m : 10.43637
[1mStep[0m  [50/106], [94mLoss[0m : 10.73386
[1mStep[0m  [60/106], [94mLoss[0m : 10.31499
[1mStep[0m  [70/106], [94mLoss[0m : 10.47111
[1mStep[0m  [80/106], [94mLoss[0m : 10.59051
[1mStep[0m  [90/106], [94mLoss[0m : 10.75597
[1mStep[0m  [100/106], [94mLoss[0m : 10.41989

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.491, [92mTest[0m: 10.321, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.04985
[1mStep[0m  [10/106], [94mLoss[0m : 10.23861
[1mStep[0m  [20/106], [94mLoss[0m : 10.54428
[1mStep[0m  [30/106], [94mLoss[0m : 10.08558
[1mStep[0m  [40/106], [94mLoss[0m : 10.22831
[1mStep[0m  [50/106], [94mLoss[0m : 10.51009
[1mStep[0m  [60/106], [94mLoss[0m : 9.96768
[1mStep[0m  [70/106], [94mLoss[0m : 10.49772
[1mStep[0m  [80/106], [94mLoss[0m : 10.43680
[1mStep[0m  [90/106], [94mLoss[0m : 10.13455
[1mStep[0m  [100/106], [94mLoss[0m : 10.62269

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.472, [92mTest[0m: 10.312, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.52606
[1mStep[0m  [10/106], [94mLoss[0m : 10.81939
[1mStep[0m  [20/106], [94mLoss[0m : 10.06274
[1mStep[0m  [30/106], [94mLoss[0m : 9.78262
[1mStep[0m  [40/106], [94mLoss[0m : 10.31939
[1mStep[0m  [50/106], [94mLoss[0m : 10.71660
[1mStep[0m  [60/106], [94mLoss[0m : 10.41351
[1mStep[0m  [70/106], [94mLoss[0m : 10.59234
[1mStep[0m  [80/106], [94mLoss[0m : 10.12528
[1mStep[0m  [90/106], [94mLoss[0m : 10.09646
[1mStep[0m  [100/106], [94mLoss[0m : 10.10978

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.466, [92mTest[0m: 10.256, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.50533
[1mStep[0m  [10/106], [94mLoss[0m : 10.42299
[1mStep[0m  [20/106], [94mLoss[0m : 10.69154
[1mStep[0m  [30/106], [94mLoss[0m : 10.30426
[1mStep[0m  [40/106], [94mLoss[0m : 10.72913
[1mStep[0m  [50/106], [94mLoss[0m : 10.26598
[1mStep[0m  [60/106], [94mLoss[0m : 10.41612
[1mStep[0m  [70/106], [94mLoss[0m : 10.02572
[1mStep[0m  [80/106], [94mLoss[0m : 11.02863
[1mStep[0m  [90/106], [94mLoss[0m : 10.32898
[1mStep[0m  [100/106], [94mLoss[0m : 9.92011

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.444, [92mTest[0m: 10.252, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.16248
[1mStep[0m  [10/106], [94mLoss[0m : 10.71996
[1mStep[0m  [20/106], [94mLoss[0m : 9.98418
[1mStep[0m  [30/106], [94mLoss[0m : 10.92513
[1mStep[0m  [40/106], [94mLoss[0m : 10.44172
[1mStep[0m  [50/106], [94mLoss[0m : 9.91314
[1mStep[0m  [60/106], [94mLoss[0m : 10.98708
[1mStep[0m  [70/106], [94mLoss[0m : 10.13199
[1mStep[0m  [80/106], [94mLoss[0m : 10.32075
[1mStep[0m  [90/106], [94mLoss[0m : 10.87590
[1mStep[0m  [100/106], [94mLoss[0m : 10.68742

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.426, [92mTest[0m: 10.239, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.09386
[1mStep[0m  [10/106], [94mLoss[0m : 9.79033
[1mStep[0m  [20/106], [94mLoss[0m : 10.34313
[1mStep[0m  [30/106], [94mLoss[0m : 10.43366
[1mStep[0m  [40/106], [94mLoss[0m : 10.04949
[1mStep[0m  [50/106], [94mLoss[0m : 10.29469
[1mStep[0m  [60/106], [94mLoss[0m : 10.40120
[1mStep[0m  [70/106], [94mLoss[0m : 10.06380
[1mStep[0m  [80/106], [94mLoss[0m : 10.48087
[1mStep[0m  [90/106], [94mLoss[0m : 10.54383
[1mStep[0m  [100/106], [94mLoss[0m : 10.41381

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.412, [92mTest[0m: 10.233, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.15381
[1mStep[0m  [10/106], [94mLoss[0m : 10.33111
[1mStep[0m  [20/106], [94mLoss[0m : 10.39645
[1mStep[0m  [30/106], [94mLoss[0m : 9.95139
[1mStep[0m  [40/106], [94mLoss[0m : 10.18770
[1mStep[0m  [50/106], [94mLoss[0m : 10.36433
[1mStep[0m  [60/106], [94mLoss[0m : 10.42859
[1mStep[0m  [70/106], [94mLoss[0m : 10.28311
[1mStep[0m  [80/106], [94mLoss[0m : 10.03441
[1mStep[0m  [90/106], [94mLoss[0m : 10.64688
[1mStep[0m  [100/106], [94mLoss[0m : 10.32463

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.395, [92mTest[0m: 10.233, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.64650
[1mStep[0m  [10/106], [94mLoss[0m : 10.58722
[1mStep[0m  [20/106], [94mLoss[0m : 10.42392
[1mStep[0m  [30/106], [94mLoss[0m : 10.59352
[1mStep[0m  [40/106], [94mLoss[0m : 10.04641
[1mStep[0m  [50/106], [94mLoss[0m : 10.66920
[1mStep[0m  [60/106], [94mLoss[0m : 10.54505
[1mStep[0m  [70/106], [94mLoss[0m : 9.71731
[1mStep[0m  [80/106], [94mLoss[0m : 10.69857
[1mStep[0m  [90/106], [94mLoss[0m : 10.43014
[1mStep[0m  [100/106], [94mLoss[0m : 10.71248

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.380, [92mTest[0m: 10.139, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.22162
[1mStep[0m  [10/106], [94mLoss[0m : 10.19087
[1mStep[0m  [20/106], [94mLoss[0m : 10.50071
[1mStep[0m  [30/106], [94mLoss[0m : 10.48832
[1mStep[0m  [40/106], [94mLoss[0m : 9.82178
[1mStep[0m  [50/106], [94mLoss[0m : 10.42208
[1mStep[0m  [60/106], [94mLoss[0m : 10.63256
[1mStep[0m  [70/106], [94mLoss[0m : 9.94352
[1mStep[0m  [80/106], [94mLoss[0m : 10.89158
[1mStep[0m  [90/106], [94mLoss[0m : 10.92744
[1mStep[0m  [100/106], [94mLoss[0m : 10.33894

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.359, [92mTest[0m: 10.177, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.107
====================================

Phase 1 - Evaluation MAE:  10.106885244261544
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.12895
[1mStep[0m  [10/106], [94mLoss[0m : 10.50638
[1mStep[0m  [20/106], [94mLoss[0m : 9.87693
[1mStep[0m  [30/106], [94mLoss[0m : 10.45126
[1mStep[0m  [40/106], [94mLoss[0m : 10.18192
[1mStep[0m  [50/106], [94mLoss[0m : 10.43823
[1mStep[0m  [60/106], [94mLoss[0m : 10.18489
[1mStep[0m  [70/106], [94mLoss[0m : 10.18808
[1mStep[0m  [80/106], [94mLoss[0m : 10.27743
[1mStep[0m  [90/106], [94mLoss[0m : 10.24027
[1mStep[0m  [100/106], [94mLoss[0m : 10.20936

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.339, [92mTest[0m: 10.110, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.10688
[1mStep[0m  [10/106], [94mLoss[0m : 10.03340
[1mStep[0m  [20/106], [94mLoss[0m : 10.18658
[1mStep[0m  [30/106], [94mLoss[0m : 10.46509
[1mStep[0m  [40/106], [94mLoss[0m : 9.94082
[1mStep[0m  [50/106], [94mLoss[0m : 10.22186
[1mStep[0m  [60/106], [94mLoss[0m : 10.86783
[1mStep[0m  [70/106], [94mLoss[0m : 10.29809
[1mStep[0m  [80/106], [94mLoss[0m : 10.77674
[1mStep[0m  [90/106], [94mLoss[0m : 10.04077
[1mStep[0m  [100/106], [94mLoss[0m : 10.51416

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.315, [92mTest[0m: 10.065, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.10298
[1mStep[0m  [10/106], [94mLoss[0m : 10.26752
[1mStep[0m  [20/106], [94mLoss[0m : 10.22969
[1mStep[0m  [30/106], [94mLoss[0m : 10.72592
[1mStep[0m  [40/106], [94mLoss[0m : 9.88018
[1mStep[0m  [50/106], [94mLoss[0m : 9.83489
[1mStep[0m  [60/106], [94mLoss[0m : 10.28228
[1mStep[0m  [70/106], [94mLoss[0m : 10.44749
[1mStep[0m  [80/106], [94mLoss[0m : 10.37263
[1mStep[0m  [90/106], [94mLoss[0m : 9.97958
[1mStep[0m  [100/106], [94mLoss[0m : 10.39196

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.271, [92mTest[0m: 10.053, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.78658
[1mStep[0m  [10/106], [94mLoss[0m : 10.53297
[1mStep[0m  [20/106], [94mLoss[0m : 9.67836
[1mStep[0m  [30/106], [94mLoss[0m : 9.96157
[1mStep[0m  [40/106], [94mLoss[0m : 10.63901
[1mStep[0m  [50/106], [94mLoss[0m : 9.93892
[1mStep[0m  [60/106], [94mLoss[0m : 9.85192
[1mStep[0m  [70/106], [94mLoss[0m : 9.80282
[1mStep[0m  [80/106], [94mLoss[0m : 10.90683
[1mStep[0m  [90/106], [94mLoss[0m : 10.47865
[1mStep[0m  [100/106], [94mLoss[0m : 9.92794

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.233, [92mTest[0m: 10.036, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.20866
[1mStep[0m  [10/106], [94mLoss[0m : 11.03027
[1mStep[0m  [20/106], [94mLoss[0m : 10.58670
[1mStep[0m  [30/106], [94mLoss[0m : 10.39731
[1mStep[0m  [40/106], [94mLoss[0m : 10.32143
[1mStep[0m  [50/106], [94mLoss[0m : 10.30616
[1mStep[0m  [60/106], [94mLoss[0m : 10.65165
[1mStep[0m  [70/106], [94mLoss[0m : 10.25795
[1mStep[0m  [80/106], [94mLoss[0m : 10.71015
[1mStep[0m  [90/106], [94mLoss[0m : 10.27903
[1mStep[0m  [100/106], [94mLoss[0m : 9.92051

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.195, [92mTest[0m: 9.982, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.95673
[1mStep[0m  [10/106], [94mLoss[0m : 9.98721
[1mStep[0m  [20/106], [94mLoss[0m : 10.26694
[1mStep[0m  [30/106], [94mLoss[0m : 10.52640
[1mStep[0m  [40/106], [94mLoss[0m : 10.04427
[1mStep[0m  [50/106], [94mLoss[0m : 10.15052
[1mStep[0m  [60/106], [94mLoss[0m : 10.34869
[1mStep[0m  [70/106], [94mLoss[0m : 10.19927
[1mStep[0m  [80/106], [94mLoss[0m : 10.36928
[1mStep[0m  [90/106], [94mLoss[0m : 10.57945
[1mStep[0m  [100/106], [94mLoss[0m : 10.27733

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.149, [92mTest[0m: 10.019, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.13879
[1mStep[0m  [10/106], [94mLoss[0m : 10.14713
[1mStep[0m  [20/106], [94mLoss[0m : 10.13462
[1mStep[0m  [30/106], [94mLoss[0m : 10.33571
[1mStep[0m  [40/106], [94mLoss[0m : 10.02712
[1mStep[0m  [50/106], [94mLoss[0m : 10.51873
[1mStep[0m  [60/106], [94mLoss[0m : 10.30411
[1mStep[0m  [70/106], [94mLoss[0m : 9.73680
[1mStep[0m  [80/106], [94mLoss[0m : 9.61894
[1mStep[0m  [90/106], [94mLoss[0m : 10.47092
[1mStep[0m  [100/106], [94mLoss[0m : 10.39030

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.111, [92mTest[0m: 9.927, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.16481
[1mStep[0m  [10/106], [94mLoss[0m : 10.69482
[1mStep[0m  [20/106], [94mLoss[0m : 9.89823
[1mStep[0m  [30/106], [94mLoss[0m : 10.50064
[1mStep[0m  [40/106], [94mLoss[0m : 9.65368
[1mStep[0m  [50/106], [94mLoss[0m : 10.94123
[1mStep[0m  [60/106], [94mLoss[0m : 9.79664
[1mStep[0m  [70/106], [94mLoss[0m : 10.44395
[1mStep[0m  [80/106], [94mLoss[0m : 9.94881
[1mStep[0m  [90/106], [94mLoss[0m : 10.40829
[1mStep[0m  [100/106], [94mLoss[0m : 9.92137

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.073, [92mTest[0m: 9.938, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.74180
[1mStep[0m  [10/106], [94mLoss[0m : 10.53122
[1mStep[0m  [20/106], [94mLoss[0m : 9.47246
[1mStep[0m  [30/106], [94mLoss[0m : 10.10990
[1mStep[0m  [40/106], [94mLoss[0m : 10.31951
[1mStep[0m  [50/106], [94mLoss[0m : 9.89519
[1mStep[0m  [60/106], [94mLoss[0m : 10.05276
[1mStep[0m  [70/106], [94mLoss[0m : 10.11410
[1mStep[0m  [80/106], [94mLoss[0m : 10.64181
[1mStep[0m  [90/106], [94mLoss[0m : 10.41567
[1mStep[0m  [100/106], [94mLoss[0m : 9.52625

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.044, [92mTest[0m: 9.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.81776
[1mStep[0m  [10/106], [94mLoss[0m : 9.99751
[1mStep[0m  [20/106], [94mLoss[0m : 9.74387
[1mStep[0m  [30/106], [94mLoss[0m : 10.23265
[1mStep[0m  [40/106], [94mLoss[0m : 9.82431
[1mStep[0m  [50/106], [94mLoss[0m : 9.47605
[1mStep[0m  [60/106], [94mLoss[0m : 10.30074
[1mStep[0m  [70/106], [94mLoss[0m : 9.82209
[1mStep[0m  [80/106], [94mLoss[0m : 10.32431
[1mStep[0m  [90/106], [94mLoss[0m : 10.11868
[1mStep[0m  [100/106], [94mLoss[0m : 9.75508

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.011, [92mTest[0m: 9.893, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.91860
[1mStep[0m  [10/106], [94mLoss[0m : 10.16322
[1mStep[0m  [20/106], [94mLoss[0m : 10.31844
[1mStep[0m  [30/106], [94mLoss[0m : 10.05038
[1mStep[0m  [40/106], [94mLoss[0m : 9.79368
[1mStep[0m  [50/106], [94mLoss[0m : 9.69074
[1mStep[0m  [60/106], [94mLoss[0m : 10.06723
[1mStep[0m  [70/106], [94mLoss[0m : 9.90428
[1mStep[0m  [80/106], [94mLoss[0m : 10.03410
[1mStep[0m  [90/106], [94mLoss[0m : 10.06115
[1mStep[0m  [100/106], [94mLoss[0m : 10.08688

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.973, [92mTest[0m: 9.760, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.71980
[1mStep[0m  [10/106], [94mLoss[0m : 9.86147
[1mStep[0m  [20/106], [94mLoss[0m : 9.79141
[1mStep[0m  [30/106], [94mLoss[0m : 9.64791
[1mStep[0m  [40/106], [94mLoss[0m : 9.60526
[1mStep[0m  [50/106], [94mLoss[0m : 9.52618
[1mStep[0m  [60/106], [94mLoss[0m : 10.37868
[1mStep[0m  [70/106], [94mLoss[0m : 10.28694
[1mStep[0m  [80/106], [94mLoss[0m : 10.25210
[1mStep[0m  [90/106], [94mLoss[0m : 9.59102
[1mStep[0m  [100/106], [94mLoss[0m : 9.84707

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.934, [92mTest[0m: 9.701, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.41889
[1mStep[0m  [10/106], [94mLoss[0m : 9.54440
[1mStep[0m  [20/106], [94mLoss[0m : 9.91309
[1mStep[0m  [30/106], [94mLoss[0m : 9.62734
[1mStep[0m  [40/106], [94mLoss[0m : 10.60449
[1mStep[0m  [50/106], [94mLoss[0m : 9.98122
[1mStep[0m  [60/106], [94mLoss[0m : 9.95512
[1mStep[0m  [70/106], [94mLoss[0m : 9.77551
[1mStep[0m  [80/106], [94mLoss[0m : 10.02453
[1mStep[0m  [90/106], [94mLoss[0m : 10.19843
[1mStep[0m  [100/106], [94mLoss[0m : 9.70205

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.897, [92mTest[0m: 9.641, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.13194
[1mStep[0m  [10/106], [94mLoss[0m : 9.90509
[1mStep[0m  [20/106], [94mLoss[0m : 9.82370
[1mStep[0m  [30/106], [94mLoss[0m : 10.10507
[1mStep[0m  [40/106], [94mLoss[0m : 10.23293
[1mStep[0m  [50/106], [94mLoss[0m : 9.31638
[1mStep[0m  [60/106], [94mLoss[0m : 9.61532
[1mStep[0m  [70/106], [94mLoss[0m : 9.99480
[1mStep[0m  [80/106], [94mLoss[0m : 9.51967
[1mStep[0m  [90/106], [94mLoss[0m : 9.58452
[1mStep[0m  [100/106], [94mLoss[0m : 9.50187

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.861, [92mTest[0m: 9.683, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.98626
[1mStep[0m  [10/106], [94mLoss[0m : 9.75408
[1mStep[0m  [20/106], [94mLoss[0m : 10.03860
[1mStep[0m  [30/106], [94mLoss[0m : 9.67920
[1mStep[0m  [40/106], [94mLoss[0m : 10.50502
[1mStep[0m  [50/106], [94mLoss[0m : 10.00898
[1mStep[0m  [60/106], [94mLoss[0m : 9.61402
[1mStep[0m  [70/106], [94mLoss[0m : 9.96545
[1mStep[0m  [80/106], [94mLoss[0m : 9.77784
[1mStep[0m  [90/106], [94mLoss[0m : 10.09470
[1mStep[0m  [100/106], [94mLoss[0m : 9.79794

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.830, [92mTest[0m: 9.618, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.12454
[1mStep[0m  [10/106], [94mLoss[0m : 9.67424
[1mStep[0m  [20/106], [94mLoss[0m : 10.01794
[1mStep[0m  [30/106], [94mLoss[0m : 9.19376
[1mStep[0m  [40/106], [94mLoss[0m : 9.90229
[1mStep[0m  [50/106], [94mLoss[0m : 10.17983
[1mStep[0m  [60/106], [94mLoss[0m : 9.36764
[1mStep[0m  [70/106], [94mLoss[0m : 9.75718
[1mStep[0m  [80/106], [94mLoss[0m : 9.70608
[1mStep[0m  [90/106], [94mLoss[0m : 10.21873
[1mStep[0m  [100/106], [94mLoss[0m : 9.57259

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.808, [92mTest[0m: 9.576, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.80923
[1mStep[0m  [10/106], [94mLoss[0m : 10.04150
[1mStep[0m  [20/106], [94mLoss[0m : 10.28695
[1mStep[0m  [30/106], [94mLoss[0m : 9.73851
[1mStep[0m  [40/106], [94mLoss[0m : 9.71331
[1mStep[0m  [50/106], [94mLoss[0m : 9.03779
[1mStep[0m  [60/106], [94mLoss[0m : 9.56445
[1mStep[0m  [70/106], [94mLoss[0m : 9.95175
[1mStep[0m  [80/106], [94mLoss[0m : 9.94969
[1mStep[0m  [90/106], [94mLoss[0m : 9.30390
[1mStep[0m  [100/106], [94mLoss[0m : 10.20232

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.758, [92mTest[0m: 9.587, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.20122
[1mStep[0m  [10/106], [94mLoss[0m : 9.90097
[1mStep[0m  [20/106], [94mLoss[0m : 9.74007
[1mStep[0m  [30/106], [94mLoss[0m : 10.12366
[1mStep[0m  [40/106], [94mLoss[0m : 10.37020
[1mStep[0m  [50/106], [94mLoss[0m : 9.83590
[1mStep[0m  [60/106], [94mLoss[0m : 9.94244
[1mStep[0m  [70/106], [94mLoss[0m : 9.66161
[1mStep[0m  [80/106], [94mLoss[0m : 9.55507
[1mStep[0m  [90/106], [94mLoss[0m : 10.14215
[1mStep[0m  [100/106], [94mLoss[0m : 9.76995

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.724, [92mTest[0m: 9.511, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.70689
[1mStep[0m  [10/106], [94mLoss[0m : 9.47532
[1mStep[0m  [20/106], [94mLoss[0m : 9.67484
[1mStep[0m  [30/106], [94mLoss[0m : 9.92383
[1mStep[0m  [40/106], [94mLoss[0m : 9.82276
[1mStep[0m  [50/106], [94mLoss[0m : 9.40099
[1mStep[0m  [60/106], [94mLoss[0m : 9.33177
[1mStep[0m  [70/106], [94mLoss[0m : 10.02157
[1mStep[0m  [80/106], [94mLoss[0m : 9.31941
[1mStep[0m  [90/106], [94mLoss[0m : 9.86811
[1mStep[0m  [100/106], [94mLoss[0m : 9.65032

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.682, [92mTest[0m: 9.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.64264
[1mStep[0m  [10/106], [94mLoss[0m : 9.31505
[1mStep[0m  [20/106], [94mLoss[0m : 9.55608
[1mStep[0m  [30/106], [94mLoss[0m : 9.96589
[1mStep[0m  [40/106], [94mLoss[0m : 10.19203
[1mStep[0m  [50/106], [94mLoss[0m : 9.41699
[1mStep[0m  [60/106], [94mLoss[0m : 9.83339
[1mStep[0m  [70/106], [94mLoss[0m : 9.63567
[1mStep[0m  [80/106], [94mLoss[0m : 9.09628
[1mStep[0m  [90/106], [94mLoss[0m : 9.82326
[1mStep[0m  [100/106], [94mLoss[0m : 9.57635

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.648, [92mTest[0m: 9.542, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.33098
[1mStep[0m  [10/106], [94mLoss[0m : 9.36608
[1mStep[0m  [20/106], [94mLoss[0m : 9.47631
[1mStep[0m  [30/106], [94mLoss[0m : 9.50032
[1mStep[0m  [40/106], [94mLoss[0m : 9.87164
[1mStep[0m  [50/106], [94mLoss[0m : 9.38485
[1mStep[0m  [60/106], [94mLoss[0m : 9.74428
[1mStep[0m  [70/106], [94mLoss[0m : 9.37629
[1mStep[0m  [80/106], [94mLoss[0m : 9.84020
[1mStep[0m  [90/106], [94mLoss[0m : 9.54920
[1mStep[0m  [100/106], [94mLoss[0m : 9.08136

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.622, [92mTest[0m: 9.375, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.91680
[1mStep[0m  [10/106], [94mLoss[0m : 9.76207
[1mStep[0m  [20/106], [94mLoss[0m : 9.63766
[1mStep[0m  [30/106], [94mLoss[0m : 9.54593
[1mStep[0m  [40/106], [94mLoss[0m : 9.43114
[1mStep[0m  [50/106], [94mLoss[0m : 8.82519
[1mStep[0m  [60/106], [94mLoss[0m : 9.41941
[1mStep[0m  [70/106], [94mLoss[0m : 10.16874
[1mStep[0m  [80/106], [94mLoss[0m : 9.31339
[1mStep[0m  [90/106], [94mLoss[0m : 9.69958
[1mStep[0m  [100/106], [94mLoss[0m : 9.92638

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.587, [92mTest[0m: 9.344, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.55543
[1mStep[0m  [10/106], [94mLoss[0m : 9.51786
[1mStep[0m  [20/106], [94mLoss[0m : 9.64516
[1mStep[0m  [30/106], [94mLoss[0m : 9.88510
[1mStep[0m  [40/106], [94mLoss[0m : 9.45308
[1mStep[0m  [50/106], [94mLoss[0m : 8.92792
[1mStep[0m  [60/106], [94mLoss[0m : 9.30870
[1mStep[0m  [70/106], [94mLoss[0m : 9.82956
[1mStep[0m  [80/106], [94mLoss[0m : 9.44507
[1mStep[0m  [90/106], [94mLoss[0m : 9.79899
[1mStep[0m  [100/106], [94mLoss[0m : 9.68795

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.546, [92mTest[0m: 9.426, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.52341
[1mStep[0m  [10/106], [94mLoss[0m : 9.52995
[1mStep[0m  [20/106], [94mLoss[0m : 10.02127
[1mStep[0m  [30/106], [94mLoss[0m : 9.34346
[1mStep[0m  [40/106], [94mLoss[0m : 9.66362
[1mStep[0m  [50/106], [94mLoss[0m : 10.03818
[1mStep[0m  [60/106], [94mLoss[0m : 9.63774
[1mStep[0m  [70/106], [94mLoss[0m : 9.28631
[1mStep[0m  [80/106], [94mLoss[0m : 9.49273
[1mStep[0m  [90/106], [94mLoss[0m : 9.84394
[1mStep[0m  [100/106], [94mLoss[0m : 9.59273

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.515, [92mTest[0m: 9.301, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.51315
[1mStep[0m  [10/106], [94mLoss[0m : 9.74697
[1mStep[0m  [20/106], [94mLoss[0m : 9.92434
[1mStep[0m  [30/106], [94mLoss[0m : 9.33160
[1mStep[0m  [40/106], [94mLoss[0m : 9.82403
[1mStep[0m  [50/106], [94mLoss[0m : 9.72955
[1mStep[0m  [60/106], [94mLoss[0m : 9.89427
[1mStep[0m  [70/106], [94mLoss[0m : 9.45077
[1mStep[0m  [80/106], [94mLoss[0m : 8.70660
[1mStep[0m  [90/106], [94mLoss[0m : 9.17633
[1mStep[0m  [100/106], [94mLoss[0m : 9.67101

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.487, [92mTest[0m: 9.198, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.77525
[1mStep[0m  [10/106], [94mLoss[0m : 9.19893
[1mStep[0m  [20/106], [94mLoss[0m : 8.76719
[1mStep[0m  [30/106], [94mLoss[0m : 8.92001
[1mStep[0m  [40/106], [94mLoss[0m : 9.40822
[1mStep[0m  [50/106], [94mLoss[0m : 9.41225
[1mStep[0m  [60/106], [94mLoss[0m : 9.41615
[1mStep[0m  [70/106], [94mLoss[0m : 9.85466
[1mStep[0m  [80/106], [94mLoss[0m : 10.00034
[1mStep[0m  [90/106], [94mLoss[0m : 9.82917
[1mStep[0m  [100/106], [94mLoss[0m : 9.33740

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.455, [92mTest[0m: 9.259, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.29085
[1mStep[0m  [10/106], [94mLoss[0m : 9.23325
[1mStep[0m  [20/106], [94mLoss[0m : 10.21755
[1mStep[0m  [30/106], [94mLoss[0m : 9.06049
[1mStep[0m  [40/106], [94mLoss[0m : 9.73164
[1mStep[0m  [50/106], [94mLoss[0m : 9.76469
[1mStep[0m  [60/106], [94mLoss[0m : 9.39086
[1mStep[0m  [70/106], [94mLoss[0m : 9.98471
[1mStep[0m  [80/106], [94mLoss[0m : 9.67687
[1mStep[0m  [90/106], [94mLoss[0m : 9.28992
[1mStep[0m  [100/106], [94mLoss[0m : 10.08384

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.414, [92mTest[0m: 9.209, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.01911
[1mStep[0m  [10/106], [94mLoss[0m : 9.08781
[1mStep[0m  [20/106], [94mLoss[0m : 8.73825
[1mStep[0m  [30/106], [94mLoss[0m : 9.20000
[1mStep[0m  [40/106], [94mLoss[0m : 9.28412
[1mStep[0m  [50/106], [94mLoss[0m : 9.25846
[1mStep[0m  [60/106], [94mLoss[0m : 9.58750
[1mStep[0m  [70/106], [94mLoss[0m : 9.91789
[1mStep[0m  [80/106], [94mLoss[0m : 9.45337
[1mStep[0m  [90/106], [94mLoss[0m : 9.61373
[1mStep[0m  [100/106], [94mLoss[0m : 9.33482

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.383, [92mTest[0m: 9.092, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.42630
[1mStep[0m  [10/106], [94mLoss[0m : 9.47600
[1mStep[0m  [20/106], [94mLoss[0m : 9.44958
[1mStep[0m  [30/106], [94mLoss[0m : 9.42017
[1mStep[0m  [40/106], [94mLoss[0m : 8.87345
[1mStep[0m  [50/106], [94mLoss[0m : 8.81535
[1mStep[0m  [60/106], [94mLoss[0m : 9.28173
[1mStep[0m  [70/106], [94mLoss[0m : 9.84033
[1mStep[0m  [80/106], [94mLoss[0m : 9.21714
[1mStep[0m  [90/106], [94mLoss[0m : 9.61400
[1mStep[0m  [100/106], [94mLoss[0m : 9.22798

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.352, [92mTest[0m: 9.027, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.19762
[1mStep[0m  [10/106], [94mLoss[0m : 8.88802
[1mStep[0m  [20/106], [94mLoss[0m : 9.66030
[1mStep[0m  [30/106], [94mLoss[0m : 8.86513
[1mStep[0m  [40/106], [94mLoss[0m : 9.09076
[1mStep[0m  [50/106], [94mLoss[0m : 9.05433
[1mStep[0m  [60/106], [94mLoss[0m : 9.54261
[1mStep[0m  [70/106], [94mLoss[0m : 9.54475
[1mStep[0m  [80/106], [94mLoss[0m : 8.77080
[1mStep[0m  [90/106], [94mLoss[0m : 9.44978
[1mStep[0m  [100/106], [94mLoss[0m : 8.96802

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.323, [92mTest[0m: 8.966, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.054
====================================

Phase 2 - Evaluation MAE:  9.054440768259877
MAE score P1      10.106885
MAE score P2       9.054441
loss               9.323245
learning_rate        0.0001
batch_size              128
hidden_sizes          [100]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay         0.0001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 11.09300
[1mStep[0m  [5/53], [94mLoss[0m : 10.66483
[1mStep[0m  [10/53], [94mLoss[0m : 10.88029
[1mStep[0m  [15/53], [94mLoss[0m : 11.08353
[1mStep[0m  [20/53], [94mLoss[0m : 10.75647
[1mStep[0m  [25/53], [94mLoss[0m : 10.57696
[1mStep[0m  [30/53], [94mLoss[0m : 10.58667
[1mStep[0m  [35/53], [94mLoss[0m : 11.22889
[1mStep[0m  [40/53], [94mLoss[0m : 11.11192
[1mStep[0m  [45/53], [94mLoss[0m : 10.67374
[1mStep[0m  [50/53], [94mLoss[0m : 10.43983

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.852, [92mTest[0m: 10.840, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.84576
[1mStep[0m  [5/53], [94mLoss[0m : 10.73848
[1mStep[0m  [10/53], [94mLoss[0m : 11.06482
[1mStep[0m  [15/53], [94mLoss[0m : 10.88866
[1mStep[0m  [20/53], [94mLoss[0m : 11.23463
[1mStep[0m  [25/53], [94mLoss[0m : 10.90534
[1mStep[0m  [30/53], [94mLoss[0m : 10.89282
[1mStep[0m  [35/53], [94mLoss[0m : 10.88465
[1mStep[0m  [40/53], [94mLoss[0m : 10.56166
[1mStep[0m  [45/53], [94mLoss[0m : 10.78446
[1mStep[0m  [50/53], [94mLoss[0m : 10.66925

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.836, [92mTest[0m: 10.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.04872
[1mStep[0m  [5/53], [94mLoss[0m : 10.66167
[1mStep[0m  [10/53], [94mLoss[0m : 10.66800
[1mStep[0m  [15/53], [94mLoss[0m : 10.70177
[1mStep[0m  [20/53], [94mLoss[0m : 10.97813
[1mStep[0m  [25/53], [94mLoss[0m : 10.97038
[1mStep[0m  [30/53], [94mLoss[0m : 10.82396
[1mStep[0m  [35/53], [94mLoss[0m : 10.88428
[1mStep[0m  [40/53], [94mLoss[0m : 10.81333
[1mStep[0m  [45/53], [94mLoss[0m : 10.63589
[1mStep[0m  [50/53], [94mLoss[0m : 11.16064

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.827, [92mTest[0m: 10.777, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.87164
[1mStep[0m  [5/53], [94mLoss[0m : 10.19706
[1mStep[0m  [10/53], [94mLoss[0m : 11.06334
[1mStep[0m  [15/53], [94mLoss[0m : 10.99098
[1mStep[0m  [20/53], [94mLoss[0m : 10.79107
[1mStep[0m  [25/53], [94mLoss[0m : 11.03847
[1mStep[0m  [30/53], [94mLoss[0m : 11.33813
[1mStep[0m  [35/53], [94mLoss[0m : 10.70918
[1mStep[0m  [40/53], [94mLoss[0m : 11.03380
[1mStep[0m  [45/53], [94mLoss[0m : 11.10799
[1mStep[0m  [50/53], [94mLoss[0m : 10.64309

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.820, [92mTest[0m: 10.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.70074
[1mStep[0m  [5/53], [94mLoss[0m : 10.88003
[1mStep[0m  [10/53], [94mLoss[0m : 10.88944
[1mStep[0m  [15/53], [94mLoss[0m : 11.05381
[1mStep[0m  [20/53], [94mLoss[0m : 10.51064
[1mStep[0m  [25/53], [94mLoss[0m : 10.83839
[1mStep[0m  [30/53], [94mLoss[0m : 11.00484
[1mStep[0m  [35/53], [94mLoss[0m : 10.58760
[1mStep[0m  [40/53], [94mLoss[0m : 10.77888
[1mStep[0m  [45/53], [94mLoss[0m : 10.98876
[1mStep[0m  [50/53], [94mLoss[0m : 10.64199

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.804, [92mTest[0m: 10.726, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.68063
[1mStep[0m  [5/53], [94mLoss[0m : 10.85866
[1mStep[0m  [10/53], [94mLoss[0m : 10.96562
[1mStep[0m  [15/53], [94mLoss[0m : 10.64446
[1mStep[0m  [20/53], [94mLoss[0m : 10.63799
[1mStep[0m  [25/53], [94mLoss[0m : 10.77789
[1mStep[0m  [30/53], [94mLoss[0m : 10.95578
[1mStep[0m  [35/53], [94mLoss[0m : 10.82794
[1mStep[0m  [40/53], [94mLoss[0m : 10.75176
[1mStep[0m  [45/53], [94mLoss[0m : 10.49628
[1mStep[0m  [50/53], [94mLoss[0m : 10.92850

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.798, [92mTest[0m: 10.728, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.03985
[1mStep[0m  [5/53], [94mLoss[0m : 10.64987
[1mStep[0m  [10/53], [94mLoss[0m : 10.72659
[1mStep[0m  [15/53], [94mLoss[0m : 11.13702
[1mStep[0m  [20/53], [94mLoss[0m : 10.75615
[1mStep[0m  [25/53], [94mLoss[0m : 10.81756
[1mStep[0m  [30/53], [94mLoss[0m : 10.95015
[1mStep[0m  [35/53], [94mLoss[0m : 10.54179
[1mStep[0m  [40/53], [94mLoss[0m : 11.13046
[1mStep[0m  [45/53], [94mLoss[0m : 10.78867
[1mStep[0m  [50/53], [94mLoss[0m : 10.64362

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.793, [92mTest[0m: 10.707, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.60866
[1mStep[0m  [5/53], [94mLoss[0m : 11.06349
[1mStep[0m  [10/53], [94mLoss[0m : 10.71096
[1mStep[0m  [15/53], [94mLoss[0m : 10.59695
[1mStep[0m  [20/53], [94mLoss[0m : 10.70449
[1mStep[0m  [25/53], [94mLoss[0m : 10.36109
[1mStep[0m  [30/53], [94mLoss[0m : 10.91344
[1mStep[0m  [35/53], [94mLoss[0m : 10.63172
[1mStep[0m  [40/53], [94mLoss[0m : 10.83097
[1mStep[0m  [45/53], [94mLoss[0m : 11.02831
[1mStep[0m  [50/53], [94mLoss[0m : 10.98843

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.773, [92mTest[0m: 10.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.83012
[1mStep[0m  [5/53], [94mLoss[0m : 10.66071
[1mStep[0m  [10/53], [94mLoss[0m : 10.94075
[1mStep[0m  [15/53], [94mLoss[0m : 10.82916
[1mStep[0m  [20/53], [94mLoss[0m : 10.80688
[1mStep[0m  [25/53], [94mLoss[0m : 10.85490
[1mStep[0m  [30/53], [94mLoss[0m : 10.62868
[1mStep[0m  [35/53], [94mLoss[0m : 10.98816
[1mStep[0m  [40/53], [94mLoss[0m : 10.31462
[1mStep[0m  [45/53], [94mLoss[0m : 10.53698
[1mStep[0m  [50/53], [94mLoss[0m : 10.72398

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.766, [92mTest[0m: 10.683, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.43970
[1mStep[0m  [5/53], [94mLoss[0m : 10.89274
[1mStep[0m  [10/53], [94mLoss[0m : 10.62884
[1mStep[0m  [15/53], [94mLoss[0m : 10.59003
[1mStep[0m  [20/53], [94mLoss[0m : 10.66076
[1mStep[0m  [25/53], [94mLoss[0m : 10.44446
[1mStep[0m  [30/53], [94mLoss[0m : 10.56671
[1mStep[0m  [35/53], [94mLoss[0m : 10.80217
[1mStep[0m  [40/53], [94mLoss[0m : 11.00860
[1mStep[0m  [45/53], [94mLoss[0m : 10.48571
[1mStep[0m  [50/53], [94mLoss[0m : 10.37700

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.758, [92mTest[0m: 10.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.16145
[1mStep[0m  [5/53], [94mLoss[0m : 10.58125
[1mStep[0m  [10/53], [94mLoss[0m : 10.59837
[1mStep[0m  [15/53], [94mLoss[0m : 10.62232
[1mStep[0m  [20/53], [94mLoss[0m : 10.85530
[1mStep[0m  [25/53], [94mLoss[0m : 10.58441
[1mStep[0m  [30/53], [94mLoss[0m : 10.55064
[1mStep[0m  [35/53], [94mLoss[0m : 10.81680
[1mStep[0m  [40/53], [94mLoss[0m : 10.96970
[1mStep[0m  [45/53], [94mLoss[0m : 11.10207
[1mStep[0m  [50/53], [94mLoss[0m : 10.58187

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.748, [92mTest[0m: 10.666, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.46045
[1mStep[0m  [5/53], [94mLoss[0m : 10.91701
[1mStep[0m  [10/53], [94mLoss[0m : 10.90660
[1mStep[0m  [15/53], [94mLoss[0m : 10.37424
[1mStep[0m  [20/53], [94mLoss[0m : 11.13866
[1mStep[0m  [25/53], [94mLoss[0m : 10.88469
[1mStep[0m  [30/53], [94mLoss[0m : 10.97229
[1mStep[0m  [35/53], [94mLoss[0m : 10.56248
[1mStep[0m  [40/53], [94mLoss[0m : 10.52705
[1mStep[0m  [45/53], [94mLoss[0m : 10.31794
[1mStep[0m  [50/53], [94mLoss[0m : 10.56327

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.738, [92mTest[0m: 10.622, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.74237
[1mStep[0m  [5/53], [94mLoss[0m : 10.66799
[1mStep[0m  [10/53], [94mLoss[0m : 10.79251
[1mStep[0m  [15/53], [94mLoss[0m : 10.47045
[1mStep[0m  [20/53], [94mLoss[0m : 10.77617
[1mStep[0m  [25/53], [94mLoss[0m : 10.42123
[1mStep[0m  [30/53], [94mLoss[0m : 10.98310
[1mStep[0m  [35/53], [94mLoss[0m : 10.27670
[1mStep[0m  [40/53], [94mLoss[0m : 10.89363
[1mStep[0m  [45/53], [94mLoss[0m : 10.74628
[1mStep[0m  [50/53], [94mLoss[0m : 10.46319

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.724, [92mTest[0m: 10.623, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.77564
[1mStep[0m  [5/53], [94mLoss[0m : 10.65650
[1mStep[0m  [10/53], [94mLoss[0m : 11.41528
[1mStep[0m  [15/53], [94mLoss[0m : 10.65928
[1mStep[0m  [20/53], [94mLoss[0m : 10.94183
[1mStep[0m  [25/53], [94mLoss[0m : 10.70189
[1mStep[0m  [30/53], [94mLoss[0m : 10.97452
[1mStep[0m  [35/53], [94mLoss[0m : 10.46461
[1mStep[0m  [40/53], [94mLoss[0m : 10.84714
[1mStep[0m  [45/53], [94mLoss[0m : 10.89869
[1mStep[0m  [50/53], [94mLoss[0m : 10.56467

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.726, [92mTest[0m: 10.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.53113
[1mStep[0m  [5/53], [94mLoss[0m : 10.73060
[1mStep[0m  [10/53], [94mLoss[0m : 10.97436
[1mStep[0m  [15/53], [94mLoss[0m : 10.75996
[1mStep[0m  [20/53], [94mLoss[0m : 11.04364
[1mStep[0m  [25/53], [94mLoss[0m : 10.49336
[1mStep[0m  [30/53], [94mLoss[0m : 11.13772
[1mStep[0m  [35/53], [94mLoss[0m : 10.58042
[1mStep[0m  [40/53], [94mLoss[0m : 11.05537
[1mStep[0m  [45/53], [94mLoss[0m : 10.80246
[1mStep[0m  [50/53], [94mLoss[0m : 10.52804

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.717, [92mTest[0m: 10.603, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.41252
[1mStep[0m  [5/53], [94mLoss[0m : 10.99269
[1mStep[0m  [10/53], [94mLoss[0m : 10.91038
[1mStep[0m  [15/53], [94mLoss[0m : 10.20935
[1mStep[0m  [20/53], [94mLoss[0m : 10.43369
[1mStep[0m  [25/53], [94mLoss[0m : 10.24228
[1mStep[0m  [30/53], [94mLoss[0m : 10.37784
[1mStep[0m  [35/53], [94mLoss[0m : 10.58235
[1mStep[0m  [40/53], [94mLoss[0m : 10.76087
[1mStep[0m  [45/53], [94mLoss[0m : 10.48664
[1mStep[0m  [50/53], [94mLoss[0m : 10.50174

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.699, [92mTest[0m: 10.581, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.05314
[1mStep[0m  [5/53], [94mLoss[0m : 10.76278
[1mStep[0m  [10/53], [94mLoss[0m : 10.72546
[1mStep[0m  [15/53], [94mLoss[0m : 10.43043
[1mStep[0m  [20/53], [94mLoss[0m : 10.80671
[1mStep[0m  [25/53], [94mLoss[0m : 10.71810
[1mStep[0m  [30/53], [94mLoss[0m : 10.97402
[1mStep[0m  [35/53], [94mLoss[0m : 10.87470
[1mStep[0m  [40/53], [94mLoss[0m : 10.31697
[1mStep[0m  [45/53], [94mLoss[0m : 10.59039
[1mStep[0m  [50/53], [94mLoss[0m : 10.80072

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.694, [92mTest[0m: 10.581, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.50790
[1mStep[0m  [5/53], [94mLoss[0m : 10.64250
[1mStep[0m  [10/53], [94mLoss[0m : 10.63514
[1mStep[0m  [15/53], [94mLoss[0m : 10.34493
[1mStep[0m  [20/53], [94mLoss[0m : 10.82840
[1mStep[0m  [25/53], [94mLoss[0m : 11.01928
[1mStep[0m  [30/53], [94mLoss[0m : 10.86260
[1mStep[0m  [35/53], [94mLoss[0m : 10.63772
[1mStep[0m  [40/53], [94mLoss[0m : 10.54849
[1mStep[0m  [45/53], [94mLoss[0m : 10.25623
[1mStep[0m  [50/53], [94mLoss[0m : 10.69426

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.680, [92mTest[0m: 10.562, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.41928
[1mStep[0m  [5/53], [94mLoss[0m : 10.86686
[1mStep[0m  [10/53], [94mLoss[0m : 10.88798
[1mStep[0m  [15/53], [94mLoss[0m : 10.97245
[1mStep[0m  [20/53], [94mLoss[0m : 10.56804
[1mStep[0m  [25/53], [94mLoss[0m : 10.85584
[1mStep[0m  [30/53], [94mLoss[0m : 10.34332
[1mStep[0m  [35/53], [94mLoss[0m : 10.93395
[1mStep[0m  [40/53], [94mLoss[0m : 10.48966
[1mStep[0m  [45/53], [94mLoss[0m : 10.60099
[1mStep[0m  [50/53], [94mLoss[0m : 10.71486

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.671, [92mTest[0m: 10.545, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.73853
[1mStep[0m  [5/53], [94mLoss[0m : 11.01302
[1mStep[0m  [10/53], [94mLoss[0m : 10.53868
[1mStep[0m  [15/53], [94mLoss[0m : 10.81505
[1mStep[0m  [20/53], [94mLoss[0m : 10.24924
[1mStep[0m  [25/53], [94mLoss[0m : 10.89156
[1mStep[0m  [30/53], [94mLoss[0m : 10.71584
[1mStep[0m  [35/53], [94mLoss[0m : 10.49278
[1mStep[0m  [40/53], [94mLoss[0m : 10.67682
[1mStep[0m  [45/53], [94mLoss[0m : 10.73520
[1mStep[0m  [50/53], [94mLoss[0m : 10.49776

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.666, [92mTest[0m: 10.526, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.70347
[1mStep[0m  [5/53], [94mLoss[0m : 10.80890
[1mStep[0m  [10/53], [94mLoss[0m : 10.07897
[1mStep[0m  [15/53], [94mLoss[0m : 10.40973
[1mStep[0m  [20/53], [94mLoss[0m : 10.82914
[1mStep[0m  [25/53], [94mLoss[0m : 10.42730
[1mStep[0m  [30/53], [94mLoss[0m : 10.19341
[1mStep[0m  [35/53], [94mLoss[0m : 10.51935
[1mStep[0m  [40/53], [94mLoss[0m : 10.84187
[1mStep[0m  [45/53], [94mLoss[0m : 10.74506
[1mStep[0m  [50/53], [94mLoss[0m : 10.93830

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.658, [92mTest[0m: 10.510, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.03165
[1mStep[0m  [5/53], [94mLoss[0m : 10.64081
[1mStep[0m  [10/53], [94mLoss[0m : 10.23283
[1mStep[0m  [15/53], [94mLoss[0m : 10.50896
[1mStep[0m  [20/53], [94mLoss[0m : 10.98364
[1mStep[0m  [25/53], [94mLoss[0m : 10.21527
[1mStep[0m  [30/53], [94mLoss[0m : 10.54460
[1mStep[0m  [35/53], [94mLoss[0m : 10.50940
[1mStep[0m  [40/53], [94mLoss[0m : 10.90934
[1mStep[0m  [45/53], [94mLoss[0m : 11.17277
[1mStep[0m  [50/53], [94mLoss[0m : 10.54445

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.640, [92mTest[0m: 10.520, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.97722
[1mStep[0m  [5/53], [94mLoss[0m : 10.54726
[1mStep[0m  [10/53], [94mLoss[0m : 10.98025
[1mStep[0m  [15/53], [94mLoss[0m : 10.51323
[1mStep[0m  [20/53], [94mLoss[0m : 10.72350
[1mStep[0m  [25/53], [94mLoss[0m : 10.51964
[1mStep[0m  [30/53], [94mLoss[0m : 10.70173
[1mStep[0m  [35/53], [94mLoss[0m : 10.52221
[1mStep[0m  [40/53], [94mLoss[0m : 10.36483
[1mStep[0m  [45/53], [94mLoss[0m : 10.68944
[1mStep[0m  [50/53], [94mLoss[0m : 10.50202

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.635, [92mTest[0m: 10.489, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.91210
[1mStep[0m  [5/53], [94mLoss[0m : 10.70981
[1mStep[0m  [10/53], [94mLoss[0m : 10.49258
[1mStep[0m  [15/53], [94mLoss[0m : 10.61972
[1mStep[0m  [20/53], [94mLoss[0m : 10.86060
[1mStep[0m  [25/53], [94mLoss[0m : 11.04181
[1mStep[0m  [30/53], [94mLoss[0m : 10.22944
[1mStep[0m  [35/53], [94mLoss[0m : 10.53314
[1mStep[0m  [40/53], [94mLoss[0m : 10.70385
[1mStep[0m  [45/53], [94mLoss[0m : 10.84405
[1mStep[0m  [50/53], [94mLoss[0m : 10.32900

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.630, [92mTest[0m: 10.482, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.57545
[1mStep[0m  [5/53], [94mLoss[0m : 10.80626
[1mStep[0m  [10/53], [94mLoss[0m : 10.42644
[1mStep[0m  [15/53], [94mLoss[0m : 10.79546
[1mStep[0m  [20/53], [94mLoss[0m : 10.34762
[1mStep[0m  [25/53], [94mLoss[0m : 10.73750
[1mStep[0m  [30/53], [94mLoss[0m : 10.34615
[1mStep[0m  [35/53], [94mLoss[0m : 10.94336
[1mStep[0m  [40/53], [94mLoss[0m : 10.92243
[1mStep[0m  [45/53], [94mLoss[0m : 10.41044
[1mStep[0m  [50/53], [94mLoss[0m : 10.71365

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.614, [92mTest[0m: 10.471, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.45374
[1mStep[0m  [5/53], [94mLoss[0m : 10.55497
[1mStep[0m  [10/53], [94mLoss[0m : 11.14103
[1mStep[0m  [15/53], [94mLoss[0m : 10.87628
[1mStep[0m  [20/53], [94mLoss[0m : 10.74474
[1mStep[0m  [25/53], [94mLoss[0m : 10.63227
[1mStep[0m  [30/53], [94mLoss[0m : 10.59590
[1mStep[0m  [35/53], [94mLoss[0m : 10.67005
[1mStep[0m  [40/53], [94mLoss[0m : 10.14955
[1mStep[0m  [45/53], [94mLoss[0m : 10.56532
[1mStep[0m  [50/53], [94mLoss[0m : 10.78676

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.605, [92mTest[0m: 10.459, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.50291
[1mStep[0m  [5/53], [94mLoss[0m : 10.63956
[1mStep[0m  [10/53], [94mLoss[0m : 10.94164
[1mStep[0m  [15/53], [94mLoss[0m : 10.61010
[1mStep[0m  [20/53], [94mLoss[0m : 10.97604
[1mStep[0m  [25/53], [94mLoss[0m : 10.45447
[1mStep[0m  [30/53], [94mLoss[0m : 10.71024
[1mStep[0m  [35/53], [94mLoss[0m : 10.85583
[1mStep[0m  [40/53], [94mLoss[0m : 10.46862
[1mStep[0m  [45/53], [94mLoss[0m : 10.84516
[1mStep[0m  [50/53], [94mLoss[0m : 10.63574

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.596, [92mTest[0m: 10.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.50757
[1mStep[0m  [5/53], [94mLoss[0m : 10.68387
[1mStep[0m  [10/53], [94mLoss[0m : 10.54962
[1mStep[0m  [15/53], [94mLoss[0m : 10.54676
[1mStep[0m  [20/53], [94mLoss[0m : 10.88168
[1mStep[0m  [25/53], [94mLoss[0m : 10.39463
[1mStep[0m  [30/53], [94mLoss[0m : 10.89627
[1mStep[0m  [35/53], [94mLoss[0m : 10.91415
[1mStep[0m  [40/53], [94mLoss[0m : 10.43501
[1mStep[0m  [45/53], [94mLoss[0m : 10.55657
[1mStep[0m  [50/53], [94mLoss[0m : 10.44339

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.595, [92mTest[0m: 10.449, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.68093
[1mStep[0m  [5/53], [94mLoss[0m : 10.36904
[1mStep[0m  [10/53], [94mLoss[0m : 10.62478
[1mStep[0m  [15/53], [94mLoss[0m : 10.35304
[1mStep[0m  [20/53], [94mLoss[0m : 10.40698
[1mStep[0m  [25/53], [94mLoss[0m : 10.73597
[1mStep[0m  [30/53], [94mLoss[0m : 10.58010
[1mStep[0m  [35/53], [94mLoss[0m : 10.74886
[1mStep[0m  [40/53], [94mLoss[0m : 10.28989
[1mStep[0m  [45/53], [94mLoss[0m : 10.64640
[1mStep[0m  [50/53], [94mLoss[0m : 10.55957

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.583, [92mTest[0m: 10.408, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.52016
[1mStep[0m  [5/53], [94mLoss[0m : 10.55419
[1mStep[0m  [10/53], [94mLoss[0m : 10.26573
[1mStep[0m  [15/53], [94mLoss[0m : 10.85995
[1mStep[0m  [20/53], [94mLoss[0m : 10.49931
[1mStep[0m  [25/53], [94mLoss[0m : 10.36671
[1mStep[0m  [30/53], [94mLoss[0m : 10.59820
[1mStep[0m  [35/53], [94mLoss[0m : 10.57730
[1mStep[0m  [40/53], [94mLoss[0m : 10.42554
[1mStep[0m  [45/53], [94mLoss[0m : 10.46506
[1mStep[0m  [50/53], [94mLoss[0m : 10.38645

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.570, [92mTest[0m: 10.414, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.411
====================================

Phase 1 - Evaluation MAE:  10.4113187789917
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 10.57010
[1mStep[0m  [5/53], [94mLoss[0m : 10.48362
[1mStep[0m  [10/53], [94mLoss[0m : 10.55303
[1mStep[0m  [15/53], [94mLoss[0m : 10.21385
[1mStep[0m  [20/53], [94mLoss[0m : 10.62389
[1mStep[0m  [25/53], [94mLoss[0m : 10.58968
[1mStep[0m  [30/53], [94mLoss[0m : 10.33644
[1mStep[0m  [35/53], [94mLoss[0m : 10.59491
[1mStep[0m  [40/53], [94mLoss[0m : 10.48872
[1mStep[0m  [45/53], [94mLoss[0m : 10.24184
[1mStep[0m  [50/53], [94mLoss[0m : 10.55319

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.565, [92mTest[0m: 10.414, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.50484
[1mStep[0m  [5/53], [94mLoss[0m : 10.38973
[1mStep[0m  [10/53], [94mLoss[0m : 10.35351
[1mStep[0m  [15/53], [94mLoss[0m : 10.76078
[1mStep[0m  [20/53], [94mLoss[0m : 10.46500
[1mStep[0m  [25/53], [94mLoss[0m : 10.96442
[1mStep[0m  [30/53], [94mLoss[0m : 10.52967
[1mStep[0m  [35/53], [94mLoss[0m : 10.30144
[1mStep[0m  [40/53], [94mLoss[0m : 10.54643
[1mStep[0m  [45/53], [94mLoss[0m : 10.91107
[1mStep[0m  [50/53], [94mLoss[0m : 10.42495

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.547, [92mTest[0m: 10.377, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.53383
[1mStep[0m  [5/53], [94mLoss[0m : 10.62868
[1mStep[0m  [10/53], [94mLoss[0m : 10.82898
[1mStep[0m  [15/53], [94mLoss[0m : 10.37102
[1mStep[0m  [20/53], [94mLoss[0m : 10.89241
[1mStep[0m  [25/53], [94mLoss[0m : 10.47396
[1mStep[0m  [30/53], [94mLoss[0m : 10.60313
[1mStep[0m  [35/53], [94mLoss[0m : 10.31736
[1mStep[0m  [40/53], [94mLoss[0m : 10.40311
[1mStep[0m  [45/53], [94mLoss[0m : 10.73827
[1mStep[0m  [50/53], [94mLoss[0m : 10.55635

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.532, [92mTest[0m: 10.372, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.92336
[1mStep[0m  [5/53], [94mLoss[0m : 10.76464
[1mStep[0m  [10/53], [94mLoss[0m : 10.38042
[1mStep[0m  [15/53], [94mLoss[0m : 10.54985
[1mStep[0m  [20/53], [94mLoss[0m : 10.27944
[1mStep[0m  [25/53], [94mLoss[0m : 10.38777
[1mStep[0m  [30/53], [94mLoss[0m : 10.27469
[1mStep[0m  [35/53], [94mLoss[0m : 10.59327
[1mStep[0m  [40/53], [94mLoss[0m : 10.60398
[1mStep[0m  [45/53], [94mLoss[0m : 10.51505
[1mStep[0m  [50/53], [94mLoss[0m : 10.42702

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.516, [92mTest[0m: 10.328, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.44764
[1mStep[0m  [5/53], [94mLoss[0m : 9.86518
[1mStep[0m  [10/53], [94mLoss[0m : 10.32804
[1mStep[0m  [15/53], [94mLoss[0m : 10.30777
[1mStep[0m  [20/53], [94mLoss[0m : 10.48581
[1mStep[0m  [25/53], [94mLoss[0m : 10.77777
[1mStep[0m  [30/53], [94mLoss[0m : 10.20130
[1mStep[0m  [35/53], [94mLoss[0m : 10.79154
[1mStep[0m  [40/53], [94mLoss[0m : 10.58352
[1mStep[0m  [45/53], [94mLoss[0m : 10.48592
[1mStep[0m  [50/53], [94mLoss[0m : 10.72701

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.512, [92mTest[0m: 10.332, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.40999
[1mStep[0m  [5/53], [94mLoss[0m : 10.46523
[1mStep[0m  [10/53], [94mLoss[0m : 10.52050
[1mStep[0m  [15/53], [94mLoss[0m : 10.64682
[1mStep[0m  [20/53], [94mLoss[0m : 10.48860
[1mStep[0m  [25/53], [94mLoss[0m : 10.45627
[1mStep[0m  [30/53], [94mLoss[0m : 10.70044
[1mStep[0m  [35/53], [94mLoss[0m : 10.59258
[1mStep[0m  [40/53], [94mLoss[0m : 10.73410
[1mStep[0m  [45/53], [94mLoss[0m : 10.66461
[1mStep[0m  [50/53], [94mLoss[0m : 10.06830

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.482, [92mTest[0m: 10.336, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.73154
[1mStep[0m  [5/53], [94mLoss[0m : 10.66969
[1mStep[0m  [10/53], [94mLoss[0m : 10.55723
[1mStep[0m  [15/53], [94mLoss[0m : 10.48172
[1mStep[0m  [20/53], [94mLoss[0m : 10.39084
[1mStep[0m  [25/53], [94mLoss[0m : 10.83091
[1mStep[0m  [30/53], [94mLoss[0m : 10.45086
[1mStep[0m  [35/53], [94mLoss[0m : 10.27361
[1mStep[0m  [40/53], [94mLoss[0m : 10.29953
[1mStep[0m  [45/53], [94mLoss[0m : 10.31516
[1mStep[0m  [50/53], [94mLoss[0m : 10.45521

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.459, [92mTest[0m: 10.299, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.31218
[1mStep[0m  [5/53], [94mLoss[0m : 10.60588
[1mStep[0m  [10/53], [94mLoss[0m : 10.37073
[1mStep[0m  [15/53], [94mLoss[0m : 10.90761
[1mStep[0m  [20/53], [94mLoss[0m : 10.34708
[1mStep[0m  [25/53], [94mLoss[0m : 10.14454
[1mStep[0m  [30/53], [94mLoss[0m : 10.36039
[1mStep[0m  [35/53], [94mLoss[0m : 10.27113
[1mStep[0m  [40/53], [94mLoss[0m : 10.91931
[1mStep[0m  [45/53], [94mLoss[0m : 10.43738
[1mStep[0m  [50/53], [94mLoss[0m : 10.40713

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.443, [92mTest[0m: 10.317, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.37536
[1mStep[0m  [5/53], [94mLoss[0m : 10.41058
[1mStep[0m  [10/53], [94mLoss[0m : 10.27174
[1mStep[0m  [15/53], [94mLoss[0m : 10.42779
[1mStep[0m  [20/53], [94mLoss[0m : 10.36615
[1mStep[0m  [25/53], [94mLoss[0m : 10.34639
[1mStep[0m  [30/53], [94mLoss[0m : 10.32212
[1mStep[0m  [35/53], [94mLoss[0m : 10.62465
[1mStep[0m  [40/53], [94mLoss[0m : 10.77582
[1mStep[0m  [45/53], [94mLoss[0m : 10.24294
[1mStep[0m  [50/53], [94mLoss[0m : 10.16597

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.424, [92mTest[0m: 10.269, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.30566
[1mStep[0m  [5/53], [94mLoss[0m : 10.49885
[1mStep[0m  [10/53], [94mLoss[0m : 10.62448
[1mStep[0m  [15/53], [94mLoss[0m : 10.72969
[1mStep[0m  [20/53], [94mLoss[0m : 10.46620
[1mStep[0m  [25/53], [94mLoss[0m : 10.49557
[1mStep[0m  [30/53], [94mLoss[0m : 10.30438
[1mStep[0m  [35/53], [94mLoss[0m : 10.09076
[1mStep[0m  [40/53], [94mLoss[0m : 10.75021
[1mStep[0m  [45/53], [94mLoss[0m : 10.53172
[1mStep[0m  [50/53], [94mLoss[0m : 10.49910

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.408, [92mTest[0m: 10.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.26624
[1mStep[0m  [5/53], [94mLoss[0m : 10.31643
[1mStep[0m  [10/53], [94mLoss[0m : 10.63754
[1mStep[0m  [15/53], [94mLoss[0m : 10.37133
[1mStep[0m  [20/53], [94mLoss[0m : 10.47976
[1mStep[0m  [25/53], [94mLoss[0m : 10.41636
[1mStep[0m  [30/53], [94mLoss[0m : 10.24083
[1mStep[0m  [35/53], [94mLoss[0m : 10.33407
[1mStep[0m  [40/53], [94mLoss[0m : 10.29287
[1mStep[0m  [45/53], [94mLoss[0m : 10.25449
[1mStep[0m  [50/53], [94mLoss[0m : 10.54145

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.384, [92mTest[0m: 10.247, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.17977
[1mStep[0m  [5/53], [94mLoss[0m : 10.33388
[1mStep[0m  [10/53], [94mLoss[0m : 10.50144
[1mStep[0m  [15/53], [94mLoss[0m : 10.29867
[1mStep[0m  [20/53], [94mLoss[0m : 10.45846
[1mStep[0m  [25/53], [94mLoss[0m : 10.10458
[1mStep[0m  [30/53], [94mLoss[0m : 10.42677
[1mStep[0m  [35/53], [94mLoss[0m : 10.24560
[1mStep[0m  [40/53], [94mLoss[0m : 10.61005
[1mStep[0m  [45/53], [94mLoss[0m : 10.55042
[1mStep[0m  [50/53], [94mLoss[0m : 9.92976

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.357, [92mTest[0m: 10.241, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.39477
[1mStep[0m  [5/53], [94mLoss[0m : 10.24008
[1mStep[0m  [10/53], [94mLoss[0m : 9.66019
[1mStep[0m  [15/53], [94mLoss[0m : 10.28543
[1mStep[0m  [20/53], [94mLoss[0m : 10.86603
[1mStep[0m  [25/53], [94mLoss[0m : 10.03678
[1mStep[0m  [30/53], [94mLoss[0m : 10.47200
[1mStep[0m  [35/53], [94mLoss[0m : 10.38707
[1mStep[0m  [40/53], [94mLoss[0m : 10.39313
[1mStep[0m  [45/53], [94mLoss[0m : 10.24614
[1mStep[0m  [50/53], [94mLoss[0m : 10.22319

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.343, [92mTest[0m: 10.185, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.22906
[1mStep[0m  [5/53], [94mLoss[0m : 10.40373
[1mStep[0m  [10/53], [94mLoss[0m : 10.37513
[1mStep[0m  [15/53], [94mLoss[0m : 10.74044
[1mStep[0m  [20/53], [94mLoss[0m : 10.49870
[1mStep[0m  [25/53], [94mLoss[0m : 10.22721
[1mStep[0m  [30/53], [94mLoss[0m : 10.21349
[1mStep[0m  [35/53], [94mLoss[0m : 10.27679
[1mStep[0m  [40/53], [94mLoss[0m : 10.84559
[1mStep[0m  [45/53], [94mLoss[0m : 10.28916
[1mStep[0m  [50/53], [94mLoss[0m : 10.13313

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.317, [92mTest[0m: 10.213, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.96614
[1mStep[0m  [5/53], [94mLoss[0m : 10.41613
[1mStep[0m  [10/53], [94mLoss[0m : 10.28639
[1mStep[0m  [15/53], [94mLoss[0m : 10.61059
[1mStep[0m  [20/53], [94mLoss[0m : 10.49902
[1mStep[0m  [25/53], [94mLoss[0m : 9.86908
[1mStep[0m  [30/53], [94mLoss[0m : 9.88737
[1mStep[0m  [35/53], [94mLoss[0m : 10.34396
[1mStep[0m  [40/53], [94mLoss[0m : 10.21088
[1mStep[0m  [45/53], [94mLoss[0m : 10.05453
[1mStep[0m  [50/53], [94mLoss[0m : 9.99595

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.294, [92mTest[0m: 10.167, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.43241
[1mStep[0m  [5/53], [94mLoss[0m : 10.52559
[1mStep[0m  [10/53], [94mLoss[0m : 10.51867
[1mStep[0m  [15/53], [94mLoss[0m : 10.03994
[1mStep[0m  [20/53], [94mLoss[0m : 10.32410
[1mStep[0m  [25/53], [94mLoss[0m : 10.41585
[1mStep[0m  [30/53], [94mLoss[0m : 10.03203
[1mStep[0m  [35/53], [94mLoss[0m : 10.14384
[1mStep[0m  [40/53], [94mLoss[0m : 10.35505
[1mStep[0m  [45/53], [94mLoss[0m : 10.25770
[1mStep[0m  [50/53], [94mLoss[0m : 10.17704

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.271, [92mTest[0m: 10.161, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.49041
[1mStep[0m  [5/53], [94mLoss[0m : 10.30998
[1mStep[0m  [10/53], [94mLoss[0m : 9.97631
[1mStep[0m  [15/53], [94mLoss[0m : 9.94067
[1mStep[0m  [20/53], [94mLoss[0m : 10.15645
[1mStep[0m  [25/53], [94mLoss[0m : 10.22468
[1mStep[0m  [30/53], [94mLoss[0m : 10.63933
[1mStep[0m  [35/53], [94mLoss[0m : 10.43706
[1mStep[0m  [40/53], [94mLoss[0m : 10.09111
[1mStep[0m  [45/53], [94mLoss[0m : 10.72622
[1mStep[0m  [50/53], [94mLoss[0m : 10.51231

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.256, [92mTest[0m: 10.126, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.12454
[1mStep[0m  [5/53], [94mLoss[0m : 10.16685
[1mStep[0m  [10/53], [94mLoss[0m : 10.08875
[1mStep[0m  [15/53], [94mLoss[0m : 10.16289
[1mStep[0m  [20/53], [94mLoss[0m : 10.09791
[1mStep[0m  [25/53], [94mLoss[0m : 10.02767
[1mStep[0m  [30/53], [94mLoss[0m : 10.21110
[1mStep[0m  [35/53], [94mLoss[0m : 10.06415
[1mStep[0m  [40/53], [94mLoss[0m : 10.02061
[1mStep[0m  [45/53], [94mLoss[0m : 10.46807
[1mStep[0m  [50/53], [94mLoss[0m : 9.88742

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.236, [92mTest[0m: 10.120, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.68426
[1mStep[0m  [5/53], [94mLoss[0m : 10.06178
[1mStep[0m  [10/53], [94mLoss[0m : 9.86279
[1mStep[0m  [15/53], [94mLoss[0m : 10.68082
[1mStep[0m  [20/53], [94mLoss[0m : 10.57561
[1mStep[0m  [25/53], [94mLoss[0m : 10.25449
[1mStep[0m  [30/53], [94mLoss[0m : 10.01621
[1mStep[0m  [35/53], [94mLoss[0m : 9.84346
[1mStep[0m  [40/53], [94mLoss[0m : 10.13058
[1mStep[0m  [45/53], [94mLoss[0m : 10.35180
[1mStep[0m  [50/53], [94mLoss[0m : 9.87249

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.222, [92mTest[0m: 10.118, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.19386
[1mStep[0m  [5/53], [94mLoss[0m : 10.31419
[1mStep[0m  [10/53], [94mLoss[0m : 10.47710
[1mStep[0m  [15/53], [94mLoss[0m : 10.25601
[1mStep[0m  [20/53], [94mLoss[0m : 10.27473
[1mStep[0m  [25/53], [94mLoss[0m : 9.77124
[1mStep[0m  [30/53], [94mLoss[0m : 10.34860
[1mStep[0m  [35/53], [94mLoss[0m : 10.26198
[1mStep[0m  [40/53], [94mLoss[0m : 9.96748
[1mStep[0m  [45/53], [94mLoss[0m : 10.02035
[1mStep[0m  [50/53], [94mLoss[0m : 10.35692

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.194, [92mTest[0m: 10.060, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.13279
[1mStep[0m  [5/53], [94mLoss[0m : 9.94632
[1mStep[0m  [10/53], [94mLoss[0m : 10.23322
[1mStep[0m  [15/53], [94mLoss[0m : 10.17895
[1mStep[0m  [20/53], [94mLoss[0m : 10.32456
[1mStep[0m  [25/53], [94mLoss[0m : 10.60059
[1mStep[0m  [30/53], [94mLoss[0m : 10.07958
[1mStep[0m  [35/53], [94mLoss[0m : 10.22417
[1mStep[0m  [40/53], [94mLoss[0m : 10.27209
[1mStep[0m  [45/53], [94mLoss[0m : 10.32994
[1mStep[0m  [50/53], [94mLoss[0m : 10.29716

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.173, [92mTest[0m: 10.043, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.55608
[1mStep[0m  [5/53], [94mLoss[0m : 10.27574
[1mStep[0m  [10/53], [94mLoss[0m : 10.20733
[1mStep[0m  [15/53], [94mLoss[0m : 10.60841
[1mStep[0m  [20/53], [94mLoss[0m : 10.24078
[1mStep[0m  [25/53], [94mLoss[0m : 9.96172
[1mStep[0m  [30/53], [94mLoss[0m : 10.00795
[1mStep[0m  [35/53], [94mLoss[0m : 10.18143
[1mStep[0m  [40/53], [94mLoss[0m : 10.34566
[1mStep[0m  [45/53], [94mLoss[0m : 10.06344
[1mStep[0m  [50/53], [94mLoss[0m : 10.67051

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.164, [92mTest[0m: 10.057, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.49006
[1mStep[0m  [5/53], [94mLoss[0m : 10.06756
[1mStep[0m  [10/53], [94mLoss[0m : 9.81243
[1mStep[0m  [15/53], [94mLoss[0m : 9.91599
[1mStep[0m  [20/53], [94mLoss[0m : 10.29919
[1mStep[0m  [25/53], [94mLoss[0m : 10.12944
[1mStep[0m  [30/53], [94mLoss[0m : 10.22362
[1mStep[0m  [35/53], [94mLoss[0m : 10.09368
[1mStep[0m  [40/53], [94mLoss[0m : 9.97572
[1mStep[0m  [45/53], [94mLoss[0m : 10.19924
[1mStep[0m  [50/53], [94mLoss[0m : 10.21017

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.146, [92mTest[0m: 10.036, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.08504
[1mStep[0m  [5/53], [94mLoss[0m : 10.21603
[1mStep[0m  [10/53], [94mLoss[0m : 10.09839
[1mStep[0m  [15/53], [94mLoss[0m : 9.94162
[1mStep[0m  [20/53], [94mLoss[0m : 10.06966
[1mStep[0m  [25/53], [94mLoss[0m : 10.20508
[1mStep[0m  [30/53], [94mLoss[0m : 9.99778
[1mStep[0m  [35/53], [94mLoss[0m : 10.09462
[1mStep[0m  [40/53], [94mLoss[0m : 9.95695
[1mStep[0m  [45/53], [94mLoss[0m : 10.33238
[1mStep[0m  [50/53], [94mLoss[0m : 10.26403

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.128, [92mTest[0m: 9.974, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.11532
[1mStep[0m  [5/53], [94mLoss[0m : 9.81478
[1mStep[0m  [10/53], [94mLoss[0m : 10.20569
[1mStep[0m  [15/53], [94mLoss[0m : 10.05840
[1mStep[0m  [20/53], [94mLoss[0m : 10.26550
[1mStep[0m  [25/53], [94mLoss[0m : 10.10745
[1mStep[0m  [30/53], [94mLoss[0m : 9.96568
[1mStep[0m  [35/53], [94mLoss[0m : 10.13355
[1mStep[0m  [40/53], [94mLoss[0m : 9.86359
[1mStep[0m  [45/53], [94mLoss[0m : 9.84373
[1mStep[0m  [50/53], [94mLoss[0m : 9.85711

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.110, [92mTest[0m: 9.975, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.05928
[1mStep[0m  [5/53], [94mLoss[0m : 10.04306
[1mStep[0m  [10/53], [94mLoss[0m : 10.32814
[1mStep[0m  [15/53], [94mLoss[0m : 9.69260
[1mStep[0m  [20/53], [94mLoss[0m : 9.84962
[1mStep[0m  [25/53], [94mLoss[0m : 10.04932
[1mStep[0m  [30/53], [94mLoss[0m : 10.03366
[1mStep[0m  [35/53], [94mLoss[0m : 9.91622
[1mStep[0m  [40/53], [94mLoss[0m : 10.19586
[1mStep[0m  [45/53], [94mLoss[0m : 9.21436
[1mStep[0m  [50/53], [94mLoss[0m : 10.26886

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.091, [92mTest[0m: 9.947, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.01525
[1mStep[0m  [5/53], [94mLoss[0m : 9.85459
[1mStep[0m  [10/53], [94mLoss[0m : 9.89865
[1mStep[0m  [15/53], [94mLoss[0m : 9.89528
[1mStep[0m  [20/53], [94mLoss[0m : 10.36476
[1mStep[0m  [25/53], [94mLoss[0m : 10.33837
[1mStep[0m  [30/53], [94mLoss[0m : 10.11192
[1mStep[0m  [35/53], [94mLoss[0m : 10.38259
[1mStep[0m  [40/53], [94mLoss[0m : 9.91887
[1mStep[0m  [45/53], [94mLoss[0m : 9.93927
[1mStep[0m  [50/53], [94mLoss[0m : 9.52908

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.067, [92mTest[0m: 9.972, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.90593
[1mStep[0m  [5/53], [94mLoss[0m : 9.73222
[1mStep[0m  [10/53], [94mLoss[0m : 9.95411
[1mStep[0m  [15/53], [94mLoss[0m : 10.01845
[1mStep[0m  [20/53], [94mLoss[0m : 9.74848
[1mStep[0m  [25/53], [94mLoss[0m : 10.14570
[1mStep[0m  [30/53], [94mLoss[0m : 10.24436
[1mStep[0m  [35/53], [94mLoss[0m : 10.14284
[1mStep[0m  [40/53], [94mLoss[0m : 10.13474
[1mStep[0m  [45/53], [94mLoss[0m : 10.13141
[1mStep[0m  [50/53], [94mLoss[0m : 9.75327

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.058, [92mTest[0m: 9.888, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.93850
[1mStep[0m  [5/53], [94mLoss[0m : 9.35069
[1mStep[0m  [10/53], [94mLoss[0m : 10.10379
[1mStep[0m  [15/53], [94mLoss[0m : 9.82958
[1mStep[0m  [20/53], [94mLoss[0m : 10.17761
[1mStep[0m  [25/53], [94mLoss[0m : 10.24049
[1mStep[0m  [30/53], [94mLoss[0m : 10.05128
[1mStep[0m  [35/53], [94mLoss[0m : 10.00384
[1mStep[0m  [40/53], [94mLoss[0m : 10.37518
[1mStep[0m  [45/53], [94mLoss[0m : 9.89070
[1mStep[0m  [50/53], [94mLoss[0m : 9.61301

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.036, [92mTest[0m: 9.943, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.27259
[1mStep[0m  [5/53], [94mLoss[0m : 10.11548
[1mStep[0m  [10/53], [94mLoss[0m : 10.16192
[1mStep[0m  [15/53], [94mLoss[0m : 9.99542
[1mStep[0m  [20/53], [94mLoss[0m : 10.04368
[1mStep[0m  [25/53], [94mLoss[0m : 10.07788
[1mStep[0m  [30/53], [94mLoss[0m : 10.35499
[1mStep[0m  [35/53], [94mLoss[0m : 9.87528
[1mStep[0m  [40/53], [94mLoss[0m : 10.37222
[1mStep[0m  [45/53], [94mLoss[0m : 9.81096
[1mStep[0m  [50/53], [94mLoss[0m : 9.90106

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.010, [92mTest[0m: 9.808, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.801
====================================

Phase 2 - Evaluation MAE:  9.80052977341872
MAE score P1      10.411319
MAE score P2        9.80053
loss              10.010368
learning_rate        0.0001
batch_size              256
hidden_sizes          [300]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 13, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.37183
[1mStep[0m  [10/106], [94mLoss[0m : 10.72887
[1mStep[0m  [20/106], [94mLoss[0m : 11.00431
[1mStep[0m  [30/106], [94mLoss[0m : 11.04163
[1mStep[0m  [40/106], [94mLoss[0m : 11.07664
[1mStep[0m  [50/106], [94mLoss[0m : 11.37064
[1mStep[0m  [60/106], [94mLoss[0m : 10.49279
[1mStep[0m  [70/106], [94mLoss[0m : 10.80712
[1mStep[0m  [80/106], [94mLoss[0m : 11.08320
[1mStep[0m  [90/106], [94mLoss[0m : 10.65179
[1mStep[0m  [100/106], [94mLoss[0m : 11.39720

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.030, [92mTest[0m: 11.020, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.87610
[1mStep[0m  [10/106], [94mLoss[0m : 10.79852
[1mStep[0m  [20/106], [94mLoss[0m : 11.29398
[1mStep[0m  [30/106], [94mLoss[0m : 10.81622
[1mStep[0m  [40/106], [94mLoss[0m : 10.87983
[1mStep[0m  [50/106], [94mLoss[0m : 10.84067
[1mStep[0m  [60/106], [94mLoss[0m : 10.92443
[1mStep[0m  [70/106], [94mLoss[0m : 10.60996
[1mStep[0m  [80/106], [94mLoss[0m : 10.84691
[1mStep[0m  [90/106], [94mLoss[0m : 10.39224
[1mStep[0m  [100/106], [94mLoss[0m : 11.14432

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 11.011, [92mTest[0m: 11.026, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.98847
[1mStep[0m  [10/106], [94mLoss[0m : 10.90987
[1mStep[0m  [20/106], [94mLoss[0m : 10.80136
[1mStep[0m  [30/106], [94mLoss[0m : 11.57564
[1mStep[0m  [40/106], [94mLoss[0m : 11.05455
[1mStep[0m  [50/106], [94mLoss[0m : 11.10608
[1mStep[0m  [60/106], [94mLoss[0m : 11.60994
[1mStep[0m  [70/106], [94mLoss[0m : 10.94623
[1mStep[0m  [80/106], [94mLoss[0m : 11.59646
[1mStep[0m  [90/106], [94mLoss[0m : 11.09528
[1mStep[0m  [100/106], [94mLoss[0m : 11.07101

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.970, [92mTest[0m: 10.996, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.74701
[1mStep[0m  [10/106], [94mLoss[0m : 10.37303
[1mStep[0m  [20/106], [94mLoss[0m : 11.20299
[1mStep[0m  [30/106], [94mLoss[0m : 10.94069
[1mStep[0m  [40/106], [94mLoss[0m : 10.96099
[1mStep[0m  [50/106], [94mLoss[0m : 10.59914
[1mStep[0m  [60/106], [94mLoss[0m : 10.92665
[1mStep[0m  [70/106], [94mLoss[0m : 11.46137
[1mStep[0m  [80/106], [94mLoss[0m : 10.25161
[1mStep[0m  [90/106], [94mLoss[0m : 11.16732
[1mStep[0m  [100/106], [94mLoss[0m : 10.77555

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.954, [92mTest[0m: 10.974, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.02103
[1mStep[0m  [10/106], [94mLoss[0m : 10.87330
[1mStep[0m  [20/106], [94mLoss[0m : 11.18460
[1mStep[0m  [30/106], [94mLoss[0m : 10.40610
[1mStep[0m  [40/106], [94mLoss[0m : 10.70118
[1mStep[0m  [50/106], [94mLoss[0m : 10.64837
[1mStep[0m  [60/106], [94mLoss[0m : 11.01572
[1mStep[0m  [70/106], [94mLoss[0m : 10.21219
[1mStep[0m  [80/106], [94mLoss[0m : 11.24703
[1mStep[0m  [90/106], [94mLoss[0m : 11.16052
[1mStep[0m  [100/106], [94mLoss[0m : 10.93516

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.923, [92mTest[0m: 10.951, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.09363
[1mStep[0m  [10/106], [94mLoss[0m : 10.94124
[1mStep[0m  [20/106], [94mLoss[0m : 10.65774
[1mStep[0m  [30/106], [94mLoss[0m : 10.54242
[1mStep[0m  [40/106], [94mLoss[0m : 10.82343
[1mStep[0m  [50/106], [94mLoss[0m : 10.70169
[1mStep[0m  [60/106], [94mLoss[0m : 10.61787
[1mStep[0m  [70/106], [94mLoss[0m : 10.86817
[1mStep[0m  [80/106], [94mLoss[0m : 11.13533
[1mStep[0m  [90/106], [94mLoss[0m : 11.33564
[1mStep[0m  [100/106], [94mLoss[0m : 11.05945

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.902, [92mTest[0m: 10.933, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.78555
[1mStep[0m  [10/106], [94mLoss[0m : 10.56515
[1mStep[0m  [20/106], [94mLoss[0m : 11.07540
[1mStep[0m  [30/106], [94mLoss[0m : 11.12749
[1mStep[0m  [40/106], [94mLoss[0m : 10.73590
[1mStep[0m  [50/106], [94mLoss[0m : 10.49245
[1mStep[0m  [60/106], [94mLoss[0m : 10.89367
[1mStep[0m  [70/106], [94mLoss[0m : 10.87481
[1mStep[0m  [80/106], [94mLoss[0m : 11.59483
[1mStep[0m  [90/106], [94mLoss[0m : 11.23690
[1mStep[0m  [100/106], [94mLoss[0m : 10.65192

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.875, [92mTest[0m: 10.890, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.65709
[1mStep[0m  [10/106], [94mLoss[0m : 10.70591
[1mStep[0m  [20/106], [94mLoss[0m : 10.59568
[1mStep[0m  [30/106], [94mLoss[0m : 10.60165
[1mStep[0m  [40/106], [94mLoss[0m : 11.39716
[1mStep[0m  [50/106], [94mLoss[0m : 10.99914
[1mStep[0m  [60/106], [94mLoss[0m : 10.75456
[1mStep[0m  [70/106], [94mLoss[0m : 10.55221
[1mStep[0m  [80/106], [94mLoss[0m : 11.36535
[1mStep[0m  [90/106], [94mLoss[0m : 10.86111
[1mStep[0m  [100/106], [94mLoss[0m : 10.67141

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.855, [92mTest[0m: 10.872, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.90682
[1mStep[0m  [10/106], [94mLoss[0m : 10.86566
[1mStep[0m  [20/106], [94mLoss[0m : 10.72879
[1mStep[0m  [30/106], [94mLoss[0m : 10.77881
[1mStep[0m  [40/106], [94mLoss[0m : 11.11150
[1mStep[0m  [50/106], [94mLoss[0m : 10.63242
[1mStep[0m  [60/106], [94mLoss[0m : 10.59893
[1mStep[0m  [70/106], [94mLoss[0m : 10.62274
[1mStep[0m  [80/106], [94mLoss[0m : 10.77444
[1mStep[0m  [90/106], [94mLoss[0m : 11.52854
[1mStep[0m  [100/106], [94mLoss[0m : 10.64677

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.830, [92mTest[0m: 10.839, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.79139
[1mStep[0m  [10/106], [94mLoss[0m : 10.50986
[1mStep[0m  [20/106], [94mLoss[0m : 11.51617
[1mStep[0m  [30/106], [94mLoss[0m : 10.89715
[1mStep[0m  [40/106], [94mLoss[0m : 11.24238
[1mStep[0m  [50/106], [94mLoss[0m : 10.68798
[1mStep[0m  [60/106], [94mLoss[0m : 10.38014
[1mStep[0m  [70/106], [94mLoss[0m : 10.83473
[1mStep[0m  [80/106], [94mLoss[0m : 10.84201
[1mStep[0m  [90/106], [94mLoss[0m : 11.15109
[1mStep[0m  [100/106], [94mLoss[0m : 11.31137

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.801, [92mTest[0m: 10.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.01387
[1mStep[0m  [10/106], [94mLoss[0m : 10.53412
[1mStep[0m  [20/106], [94mLoss[0m : 10.82642
[1mStep[0m  [30/106], [94mLoss[0m : 11.12474
[1mStep[0m  [40/106], [94mLoss[0m : 10.57508
[1mStep[0m  [50/106], [94mLoss[0m : 10.89643
[1mStep[0m  [60/106], [94mLoss[0m : 10.89600
[1mStep[0m  [70/106], [94mLoss[0m : 10.95425
[1mStep[0m  [80/106], [94mLoss[0m : 10.26074
[1mStep[0m  [90/106], [94mLoss[0m : 10.64249
[1mStep[0m  [100/106], [94mLoss[0m : 10.74368

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.779, [92mTest[0m: 10.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.72386
[1mStep[0m  [10/106], [94mLoss[0m : 10.75263
[1mStep[0m  [20/106], [94mLoss[0m : 11.18663
[1mStep[0m  [30/106], [94mLoss[0m : 10.00832
[1mStep[0m  [40/106], [94mLoss[0m : 10.27018
[1mStep[0m  [50/106], [94mLoss[0m : 10.50786
[1mStep[0m  [60/106], [94mLoss[0m : 10.69783
[1mStep[0m  [70/106], [94mLoss[0m : 10.67196
[1mStep[0m  [80/106], [94mLoss[0m : 10.53259
[1mStep[0m  [90/106], [94mLoss[0m : 10.07821
[1mStep[0m  [100/106], [94mLoss[0m : 10.15581

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.762, [92mTest[0m: 10.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.00729
[1mStep[0m  [10/106], [94mLoss[0m : 10.89250
[1mStep[0m  [20/106], [94mLoss[0m : 10.96111
[1mStep[0m  [30/106], [94mLoss[0m : 11.40950
[1mStep[0m  [40/106], [94mLoss[0m : 11.16262
[1mStep[0m  [50/106], [94mLoss[0m : 10.66936
[1mStep[0m  [60/106], [94mLoss[0m : 10.66911
[1mStep[0m  [70/106], [94mLoss[0m : 11.03795
[1mStep[0m  [80/106], [94mLoss[0m : 10.46058
[1mStep[0m  [90/106], [94mLoss[0m : 10.47755
[1mStep[0m  [100/106], [94mLoss[0m : 10.48160

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.731, [92mTest[0m: 10.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 11.53034
[1mStep[0m  [10/106], [94mLoss[0m : 10.99519
[1mStep[0m  [20/106], [94mLoss[0m : 10.66176
[1mStep[0m  [30/106], [94mLoss[0m : 10.77268
[1mStep[0m  [40/106], [94mLoss[0m : 10.93323
[1mStep[0m  [50/106], [94mLoss[0m : 10.63265
[1mStep[0m  [60/106], [94mLoss[0m : 11.03024
[1mStep[0m  [70/106], [94mLoss[0m : 11.24142
[1mStep[0m  [80/106], [94mLoss[0m : 10.53940
[1mStep[0m  [90/106], [94mLoss[0m : 10.25413
[1mStep[0m  [100/106], [94mLoss[0m : 10.63031

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.715, [92mTest[0m: 10.689, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.61125
[1mStep[0m  [10/106], [94mLoss[0m : 10.74479
[1mStep[0m  [20/106], [94mLoss[0m : 11.08288
[1mStep[0m  [30/106], [94mLoss[0m : 11.33030
[1mStep[0m  [40/106], [94mLoss[0m : 10.18991
[1mStep[0m  [50/106], [94mLoss[0m : 10.74018
[1mStep[0m  [60/106], [94mLoss[0m : 10.25092
[1mStep[0m  [70/106], [94mLoss[0m : 9.92511
[1mStep[0m  [80/106], [94mLoss[0m : 10.16689
[1mStep[0m  [90/106], [94mLoss[0m : 10.66903
[1mStep[0m  [100/106], [94mLoss[0m : 10.51268

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.684, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.81599
[1mStep[0m  [10/106], [94mLoss[0m : 10.37561
[1mStep[0m  [20/106], [94mLoss[0m : 11.09542
[1mStep[0m  [30/106], [94mLoss[0m : 10.56643
[1mStep[0m  [40/106], [94mLoss[0m : 10.66024
[1mStep[0m  [50/106], [94mLoss[0m : 10.50366
[1mStep[0m  [60/106], [94mLoss[0m : 10.92084
[1mStep[0m  [70/106], [94mLoss[0m : 10.75176
[1mStep[0m  [80/106], [94mLoss[0m : 10.86709
[1mStep[0m  [90/106], [94mLoss[0m : 10.44153
[1mStep[0m  [100/106], [94mLoss[0m : 10.39932

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.656, [92mTest[0m: 10.642, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.80002
[1mStep[0m  [10/106], [94mLoss[0m : 10.77389
[1mStep[0m  [20/106], [94mLoss[0m : 10.52163
[1mStep[0m  [30/106], [94mLoss[0m : 10.31346
[1mStep[0m  [40/106], [94mLoss[0m : 10.39465
[1mStep[0m  [50/106], [94mLoss[0m : 10.81736
[1mStep[0m  [60/106], [94mLoss[0m : 10.72338
[1mStep[0m  [70/106], [94mLoss[0m : 10.40925
[1mStep[0m  [80/106], [94mLoss[0m : 10.55283
[1mStep[0m  [90/106], [94mLoss[0m : 10.91184
[1mStep[0m  [100/106], [94mLoss[0m : 11.26350

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.635, [92mTest[0m: 10.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.34176
[1mStep[0m  [10/106], [94mLoss[0m : 10.82738
[1mStep[0m  [20/106], [94mLoss[0m : 10.20636
[1mStep[0m  [30/106], [94mLoss[0m : 10.55654
[1mStep[0m  [40/106], [94mLoss[0m : 10.82188
[1mStep[0m  [50/106], [94mLoss[0m : 10.14536
[1mStep[0m  [60/106], [94mLoss[0m : 10.51164
[1mStep[0m  [70/106], [94mLoss[0m : 10.80640
[1mStep[0m  [80/106], [94mLoss[0m : 10.41994
[1mStep[0m  [90/106], [94mLoss[0m : 10.50934
[1mStep[0m  [100/106], [94mLoss[0m : 11.00132

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.615, [92mTest[0m: 10.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.71659
[1mStep[0m  [10/106], [94mLoss[0m : 10.53338
[1mStep[0m  [20/106], [94mLoss[0m : 10.31486
[1mStep[0m  [30/106], [94mLoss[0m : 10.09765
[1mStep[0m  [40/106], [94mLoss[0m : 11.43744
[1mStep[0m  [50/106], [94mLoss[0m : 10.18559
[1mStep[0m  [60/106], [94mLoss[0m : 11.09588
[1mStep[0m  [70/106], [94mLoss[0m : 11.26998
[1mStep[0m  [80/106], [94mLoss[0m : 10.54000
[1mStep[0m  [90/106], [94mLoss[0m : 11.01230
[1mStep[0m  [100/106], [94mLoss[0m : 10.94704

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.589, [92mTest[0m: 10.554, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.80895
[1mStep[0m  [10/106], [94mLoss[0m : 10.72010
[1mStep[0m  [20/106], [94mLoss[0m : 10.58407
[1mStep[0m  [30/106], [94mLoss[0m : 9.92046
[1mStep[0m  [40/106], [94mLoss[0m : 10.73964
[1mStep[0m  [50/106], [94mLoss[0m : 10.63183
[1mStep[0m  [60/106], [94mLoss[0m : 10.17317
[1mStep[0m  [70/106], [94mLoss[0m : 10.04397
[1mStep[0m  [80/106], [94mLoss[0m : 10.73477
[1mStep[0m  [90/106], [94mLoss[0m : 9.99061
[1mStep[0m  [100/106], [94mLoss[0m : 10.69158

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.556, [92mTest[0m: 10.504, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.86697
[1mStep[0m  [10/106], [94mLoss[0m : 10.67766
[1mStep[0m  [20/106], [94mLoss[0m : 10.69368
[1mStep[0m  [30/106], [94mLoss[0m : 10.36709
[1mStep[0m  [40/106], [94mLoss[0m : 10.67528
[1mStep[0m  [50/106], [94mLoss[0m : 10.64609
[1mStep[0m  [60/106], [94mLoss[0m : 10.25753
[1mStep[0m  [70/106], [94mLoss[0m : 10.16612
[1mStep[0m  [80/106], [94mLoss[0m : 10.73042
[1mStep[0m  [90/106], [94mLoss[0m : 10.50223
[1mStep[0m  [100/106], [94mLoss[0m : 10.10601

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.533, [92mTest[0m: 10.500, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.95146
[1mStep[0m  [10/106], [94mLoss[0m : 10.03083
[1mStep[0m  [20/106], [94mLoss[0m : 10.17358
[1mStep[0m  [30/106], [94mLoss[0m : 10.49471
[1mStep[0m  [40/106], [94mLoss[0m : 10.74141
[1mStep[0m  [50/106], [94mLoss[0m : 10.67849
[1mStep[0m  [60/106], [94mLoss[0m : 10.16648
[1mStep[0m  [70/106], [94mLoss[0m : 11.04467
[1mStep[0m  [80/106], [94mLoss[0m : 10.90313
[1mStep[0m  [90/106], [94mLoss[0m : 10.60818
[1mStep[0m  [100/106], [94mLoss[0m : 10.60642

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.516, [92mTest[0m: 10.458, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.72793
[1mStep[0m  [10/106], [94mLoss[0m : 10.47814
[1mStep[0m  [20/106], [94mLoss[0m : 10.78397
[1mStep[0m  [30/106], [94mLoss[0m : 10.10349
[1mStep[0m  [40/106], [94mLoss[0m : 10.76780
[1mStep[0m  [50/106], [94mLoss[0m : 10.58254
[1mStep[0m  [60/106], [94mLoss[0m : 9.95356
[1mStep[0m  [70/106], [94mLoss[0m : 10.60096
[1mStep[0m  [80/106], [94mLoss[0m : 10.79819
[1mStep[0m  [90/106], [94mLoss[0m : 10.17466
[1mStep[0m  [100/106], [94mLoss[0m : 9.88256

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.495, [92mTest[0m: 10.435, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.67806
[1mStep[0m  [10/106], [94mLoss[0m : 10.30300
[1mStep[0m  [20/106], [94mLoss[0m : 10.22047
[1mStep[0m  [30/106], [94mLoss[0m : 10.37335
[1mStep[0m  [40/106], [94mLoss[0m : 10.20992
[1mStep[0m  [50/106], [94mLoss[0m : 10.24774
[1mStep[0m  [60/106], [94mLoss[0m : 10.54354
[1mStep[0m  [70/106], [94mLoss[0m : 10.88421
[1mStep[0m  [80/106], [94mLoss[0m : 10.68666
[1mStep[0m  [90/106], [94mLoss[0m : 10.54455
[1mStep[0m  [100/106], [94mLoss[0m : 10.44042

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.468, [92mTest[0m: 10.388, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.47973
[1mStep[0m  [10/106], [94mLoss[0m : 10.27960
[1mStep[0m  [20/106], [94mLoss[0m : 10.85702
[1mStep[0m  [30/106], [94mLoss[0m : 10.75421
[1mStep[0m  [40/106], [94mLoss[0m : 10.27232
[1mStep[0m  [50/106], [94mLoss[0m : 10.79255
[1mStep[0m  [60/106], [94mLoss[0m : 10.40686
[1mStep[0m  [70/106], [94mLoss[0m : 10.28826
[1mStep[0m  [80/106], [94mLoss[0m : 10.47349
[1mStep[0m  [90/106], [94mLoss[0m : 10.37565
[1mStep[0m  [100/106], [94mLoss[0m : 10.80693

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.442, [92mTest[0m: 10.402, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.47834
[1mStep[0m  [10/106], [94mLoss[0m : 10.21459
[1mStep[0m  [20/106], [94mLoss[0m : 10.31363
[1mStep[0m  [30/106], [94mLoss[0m : 10.09946
[1mStep[0m  [40/106], [94mLoss[0m : 10.57471
[1mStep[0m  [50/106], [94mLoss[0m : 10.09097
[1mStep[0m  [60/106], [94mLoss[0m : 11.06986
[1mStep[0m  [70/106], [94mLoss[0m : 10.69157
[1mStep[0m  [80/106], [94mLoss[0m : 10.58849
[1mStep[0m  [90/106], [94mLoss[0m : 10.78736
[1mStep[0m  [100/106], [94mLoss[0m : 11.02727

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.430, [92mTest[0m: 10.339, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.80662
[1mStep[0m  [10/106], [94mLoss[0m : 10.27893
[1mStep[0m  [20/106], [94mLoss[0m : 10.85512
[1mStep[0m  [30/106], [94mLoss[0m : 10.49895
[1mStep[0m  [40/106], [94mLoss[0m : 10.70074
[1mStep[0m  [50/106], [94mLoss[0m : 10.47400
[1mStep[0m  [60/106], [94mLoss[0m : 11.33031
[1mStep[0m  [70/106], [94mLoss[0m : 10.10110
[1mStep[0m  [80/106], [94mLoss[0m : 10.17711
[1mStep[0m  [90/106], [94mLoss[0m : 10.75433
[1mStep[0m  [100/106], [94mLoss[0m : 10.46020

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.400, [92mTest[0m: 10.338, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.11039
[1mStep[0m  [10/106], [94mLoss[0m : 10.82427
[1mStep[0m  [20/106], [94mLoss[0m : 10.31070
[1mStep[0m  [30/106], [94mLoss[0m : 10.45409
[1mStep[0m  [40/106], [94mLoss[0m : 9.88501
[1mStep[0m  [50/106], [94mLoss[0m : 10.77694
[1mStep[0m  [60/106], [94mLoss[0m : 10.52502
[1mStep[0m  [70/106], [94mLoss[0m : 10.20240
[1mStep[0m  [80/106], [94mLoss[0m : 10.15085
[1mStep[0m  [90/106], [94mLoss[0m : 9.77177
[1mStep[0m  [100/106], [94mLoss[0m : 10.07948

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.380, [92mTest[0m: 10.334, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.85501
[1mStep[0m  [10/106], [94mLoss[0m : 10.43746
[1mStep[0m  [20/106], [94mLoss[0m : 10.35600
[1mStep[0m  [30/106], [94mLoss[0m : 10.08198
[1mStep[0m  [40/106], [94mLoss[0m : 10.55480
[1mStep[0m  [50/106], [94mLoss[0m : 10.73020
[1mStep[0m  [60/106], [94mLoss[0m : 10.49395
[1mStep[0m  [70/106], [94mLoss[0m : 11.02152
[1mStep[0m  [80/106], [94mLoss[0m : 10.82034
[1mStep[0m  [90/106], [94mLoss[0m : 9.87028
[1mStep[0m  [100/106], [94mLoss[0m : 10.45100

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.353, [92mTest[0m: 10.258, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.41359
[1mStep[0m  [10/106], [94mLoss[0m : 10.49312
[1mStep[0m  [20/106], [94mLoss[0m : 10.39002
[1mStep[0m  [30/106], [94mLoss[0m : 10.09916
[1mStep[0m  [40/106], [94mLoss[0m : 10.13143
[1mStep[0m  [50/106], [94mLoss[0m : 10.80366
[1mStep[0m  [60/106], [94mLoss[0m : 9.94362
[1mStep[0m  [70/106], [94mLoss[0m : 10.25076
[1mStep[0m  [80/106], [94mLoss[0m : 11.10076
[1mStep[0m  [90/106], [94mLoss[0m : 10.69874
[1mStep[0m  [100/106], [94mLoss[0m : 10.44354

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.340, [92mTest[0m: 10.262, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.228
====================================

Phase 1 - Evaluation MAE:  10.2275519461002
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 9.99511
[1mStep[0m  [10/106], [94mLoss[0m : 10.54998
[1mStep[0m  [20/106], [94mLoss[0m : 11.03876
[1mStep[0m  [30/106], [94mLoss[0m : 10.16230
[1mStep[0m  [40/106], [94mLoss[0m : 9.96388
[1mStep[0m  [50/106], [94mLoss[0m : 9.92060
[1mStep[0m  [60/106], [94mLoss[0m : 10.45447
[1mStep[0m  [70/106], [94mLoss[0m : 10.09696
[1mStep[0m  [80/106], [94mLoss[0m : 10.13354
[1mStep[0m  [90/106], [94mLoss[0m : 10.30503
[1mStep[0m  [100/106], [94mLoss[0m : 10.33314

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.311, [92mTest[0m: 10.219, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.92741
[1mStep[0m  [10/106], [94mLoss[0m : 9.84689
[1mStep[0m  [20/106], [94mLoss[0m : 10.87600
[1mStep[0m  [30/106], [94mLoss[0m : 10.58139
[1mStep[0m  [40/106], [94mLoss[0m : 10.16392
[1mStep[0m  [50/106], [94mLoss[0m : 10.21924
[1mStep[0m  [60/106], [94mLoss[0m : 10.12532
[1mStep[0m  [70/106], [94mLoss[0m : 10.60807
[1mStep[0m  [80/106], [94mLoss[0m : 9.68968
[1mStep[0m  [90/106], [94mLoss[0m : 9.89600
[1mStep[0m  [100/106], [94mLoss[0m : 10.37193

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.286, [92mTest[0m: 10.188, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.28560
[1mStep[0m  [10/106], [94mLoss[0m : 10.01680
[1mStep[0m  [20/106], [94mLoss[0m : 10.27655
[1mStep[0m  [30/106], [94mLoss[0m : 10.23636
[1mStep[0m  [40/106], [94mLoss[0m : 9.98248
[1mStep[0m  [50/106], [94mLoss[0m : 10.75961
[1mStep[0m  [60/106], [94mLoss[0m : 10.20483
[1mStep[0m  [70/106], [94mLoss[0m : 10.31176
[1mStep[0m  [80/106], [94mLoss[0m : 10.11903
[1mStep[0m  [90/106], [94mLoss[0m : 10.85690
[1mStep[0m  [100/106], [94mLoss[0m : 9.92701

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.257, [92mTest[0m: 10.163, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.79513
[1mStep[0m  [10/106], [94mLoss[0m : 10.14598
[1mStep[0m  [20/106], [94mLoss[0m : 10.55877
[1mStep[0m  [30/106], [94mLoss[0m : 10.67987
[1mStep[0m  [40/106], [94mLoss[0m : 10.52054
[1mStep[0m  [50/106], [94mLoss[0m : 10.48837
[1mStep[0m  [60/106], [94mLoss[0m : 10.31277
[1mStep[0m  [70/106], [94mLoss[0m : 9.88985
[1mStep[0m  [80/106], [94mLoss[0m : 10.59862
[1mStep[0m  [90/106], [94mLoss[0m : 10.21974
[1mStep[0m  [100/106], [94mLoss[0m : 10.42780

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.228, [92mTest[0m: 10.093, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.12143
[1mStep[0m  [10/106], [94mLoss[0m : 10.16227
[1mStep[0m  [20/106], [94mLoss[0m : 10.50185
[1mStep[0m  [30/106], [94mLoss[0m : 10.12327
[1mStep[0m  [40/106], [94mLoss[0m : 10.02454
[1mStep[0m  [50/106], [94mLoss[0m : 9.85126
[1mStep[0m  [60/106], [94mLoss[0m : 10.05592
[1mStep[0m  [70/106], [94mLoss[0m : 10.34435
[1mStep[0m  [80/106], [94mLoss[0m : 10.04964
[1mStep[0m  [90/106], [94mLoss[0m : 9.43913
[1mStep[0m  [100/106], [94mLoss[0m : 10.08976

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.206, [92mTest[0m: 10.095, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.60276
[1mStep[0m  [10/106], [94mLoss[0m : 10.57925
[1mStep[0m  [20/106], [94mLoss[0m : 10.12860
[1mStep[0m  [30/106], [94mLoss[0m : 10.34223
[1mStep[0m  [40/106], [94mLoss[0m : 10.09801
[1mStep[0m  [50/106], [94mLoss[0m : 10.52394
[1mStep[0m  [60/106], [94mLoss[0m : 10.03032
[1mStep[0m  [70/106], [94mLoss[0m : 9.86956
[1mStep[0m  [80/106], [94mLoss[0m : 10.17746
[1mStep[0m  [90/106], [94mLoss[0m : 10.12254
[1mStep[0m  [100/106], [94mLoss[0m : 9.99538

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.180, [92mTest[0m: 10.075, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.22909
[1mStep[0m  [10/106], [94mLoss[0m : 10.37652
[1mStep[0m  [20/106], [94mLoss[0m : 10.16918
[1mStep[0m  [30/106], [94mLoss[0m : 10.77142
[1mStep[0m  [40/106], [94mLoss[0m : 9.77724
[1mStep[0m  [50/106], [94mLoss[0m : 10.10877
[1mStep[0m  [60/106], [94mLoss[0m : 9.77214
[1mStep[0m  [70/106], [94mLoss[0m : 10.20556
[1mStep[0m  [80/106], [94mLoss[0m : 9.81226
[1mStep[0m  [90/106], [94mLoss[0m : 10.93744
[1mStep[0m  [100/106], [94mLoss[0m : 10.56305

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.146, [92mTest[0m: 10.033, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.71447
[1mStep[0m  [10/106], [94mLoss[0m : 9.81756
[1mStep[0m  [20/106], [94mLoss[0m : 10.51276
[1mStep[0m  [30/106], [94mLoss[0m : 9.94637
[1mStep[0m  [40/106], [94mLoss[0m : 10.58355
[1mStep[0m  [50/106], [94mLoss[0m : 10.35158
[1mStep[0m  [60/106], [94mLoss[0m : 9.91468
[1mStep[0m  [70/106], [94mLoss[0m : 10.27409
[1mStep[0m  [80/106], [94mLoss[0m : 10.19792
[1mStep[0m  [90/106], [94mLoss[0m : 10.28513
[1mStep[0m  [100/106], [94mLoss[0m : 9.90710

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.128, [92mTest[0m: 9.982, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.19407
[1mStep[0m  [10/106], [94mLoss[0m : 9.34721
[1mStep[0m  [20/106], [94mLoss[0m : 10.17830
[1mStep[0m  [30/106], [94mLoss[0m : 10.43848
[1mStep[0m  [40/106], [94mLoss[0m : 9.80827
[1mStep[0m  [50/106], [94mLoss[0m : 10.03944
[1mStep[0m  [60/106], [94mLoss[0m : 10.13599
[1mStep[0m  [70/106], [94mLoss[0m : 10.15027
[1mStep[0m  [80/106], [94mLoss[0m : 10.32738
[1mStep[0m  [90/106], [94mLoss[0m : 9.46211
[1mStep[0m  [100/106], [94mLoss[0m : 9.89398

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.095, [92mTest[0m: 9.940, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.32101
[1mStep[0m  [10/106], [94mLoss[0m : 10.26324
[1mStep[0m  [20/106], [94mLoss[0m : 10.03543
[1mStep[0m  [30/106], [94mLoss[0m : 9.93322
[1mStep[0m  [40/106], [94mLoss[0m : 10.41741
[1mStep[0m  [50/106], [94mLoss[0m : 9.81327
[1mStep[0m  [60/106], [94mLoss[0m : 10.06038
[1mStep[0m  [70/106], [94mLoss[0m : 10.17836
[1mStep[0m  [80/106], [94mLoss[0m : 9.14880
[1mStep[0m  [90/106], [94mLoss[0m : 9.87320
[1mStep[0m  [100/106], [94mLoss[0m : 9.96308

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.059, [92mTest[0m: 9.919, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.47016
[1mStep[0m  [10/106], [94mLoss[0m : 10.04243
[1mStep[0m  [20/106], [94mLoss[0m : 10.20753
[1mStep[0m  [30/106], [94mLoss[0m : 9.88299
[1mStep[0m  [40/106], [94mLoss[0m : 10.50388
[1mStep[0m  [50/106], [94mLoss[0m : 10.27454
[1mStep[0m  [60/106], [94mLoss[0m : 10.48158
[1mStep[0m  [70/106], [94mLoss[0m : 9.35829
[1mStep[0m  [80/106], [94mLoss[0m : 9.77656
[1mStep[0m  [90/106], [94mLoss[0m : 10.12819
[1mStep[0m  [100/106], [94mLoss[0m : 9.76984

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.031, [92mTest[0m: 9.906, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.57634
[1mStep[0m  [10/106], [94mLoss[0m : 10.16720
[1mStep[0m  [20/106], [94mLoss[0m : 9.94019
[1mStep[0m  [30/106], [94mLoss[0m : 9.95949
[1mStep[0m  [40/106], [94mLoss[0m : 9.62506
[1mStep[0m  [50/106], [94mLoss[0m : 9.33294
[1mStep[0m  [60/106], [94mLoss[0m : 9.90034
[1mStep[0m  [70/106], [94mLoss[0m : 10.16738
[1mStep[0m  [80/106], [94mLoss[0m : 9.77758
[1mStep[0m  [90/106], [94mLoss[0m : 10.14108
[1mStep[0m  [100/106], [94mLoss[0m : 9.38968

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.003, [92mTest[0m: 9.879, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.76492
[1mStep[0m  [10/106], [94mLoss[0m : 10.30605
[1mStep[0m  [20/106], [94mLoss[0m : 10.40163
[1mStep[0m  [30/106], [94mLoss[0m : 10.24167
[1mStep[0m  [40/106], [94mLoss[0m : 10.56430
[1mStep[0m  [50/106], [94mLoss[0m : 10.06919
[1mStep[0m  [60/106], [94mLoss[0m : 9.87986
[1mStep[0m  [70/106], [94mLoss[0m : 9.97482
[1mStep[0m  [80/106], [94mLoss[0m : 10.07745
[1mStep[0m  [90/106], [94mLoss[0m : 9.70423
[1mStep[0m  [100/106], [94mLoss[0m : 10.15317

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.971, [92mTest[0m: 9.812, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.24909
[1mStep[0m  [10/106], [94mLoss[0m : 9.72886
[1mStep[0m  [20/106], [94mLoss[0m : 9.84735
[1mStep[0m  [30/106], [94mLoss[0m : 9.94438
[1mStep[0m  [40/106], [94mLoss[0m : 9.33785
[1mStep[0m  [50/106], [94mLoss[0m : 10.05931
[1mStep[0m  [60/106], [94mLoss[0m : 9.32846
[1mStep[0m  [70/106], [94mLoss[0m : 9.48281
[1mStep[0m  [80/106], [94mLoss[0m : 10.50198
[1mStep[0m  [90/106], [94mLoss[0m : 10.41639
[1mStep[0m  [100/106], [94mLoss[0m : 9.97300

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.937, [92mTest[0m: 9.833, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.95957
[1mStep[0m  [10/106], [94mLoss[0m : 9.87728
[1mStep[0m  [20/106], [94mLoss[0m : 10.28463
[1mStep[0m  [30/106], [94mLoss[0m : 9.94825
[1mStep[0m  [40/106], [94mLoss[0m : 9.73301
[1mStep[0m  [50/106], [94mLoss[0m : 9.82275
[1mStep[0m  [60/106], [94mLoss[0m : 9.14927
[1mStep[0m  [70/106], [94mLoss[0m : 9.59754
[1mStep[0m  [80/106], [94mLoss[0m : 9.41068
[1mStep[0m  [90/106], [94mLoss[0m : 9.74074
[1mStep[0m  [100/106], [94mLoss[0m : 9.69543

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.915, [92mTest[0m: 9.767, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.25134
[1mStep[0m  [10/106], [94mLoss[0m : 10.00188
[1mStep[0m  [20/106], [94mLoss[0m : 10.05178
[1mStep[0m  [30/106], [94mLoss[0m : 9.64468
[1mStep[0m  [40/106], [94mLoss[0m : 9.51289
[1mStep[0m  [50/106], [94mLoss[0m : 10.06660
[1mStep[0m  [60/106], [94mLoss[0m : 9.77965
[1mStep[0m  [70/106], [94mLoss[0m : 9.85444
[1mStep[0m  [80/106], [94mLoss[0m : 9.54532
[1mStep[0m  [90/106], [94mLoss[0m : 9.97297
[1mStep[0m  [100/106], [94mLoss[0m : 9.97718

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.890, [92mTest[0m: 9.759, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.79744
[1mStep[0m  [10/106], [94mLoss[0m : 10.32466
[1mStep[0m  [20/106], [94mLoss[0m : 9.35398
[1mStep[0m  [30/106], [94mLoss[0m : 10.07337
[1mStep[0m  [40/106], [94mLoss[0m : 9.81140
[1mStep[0m  [50/106], [94mLoss[0m : 10.32201
[1mStep[0m  [60/106], [94mLoss[0m : 10.14196
[1mStep[0m  [70/106], [94mLoss[0m : 10.00234
[1mStep[0m  [80/106], [94mLoss[0m : 10.22682
[1mStep[0m  [90/106], [94mLoss[0m : 9.32054
[1mStep[0m  [100/106], [94mLoss[0m : 9.97073

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.846, [92mTest[0m: 9.694, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.06401
[1mStep[0m  [10/106], [94mLoss[0m : 9.60746
[1mStep[0m  [20/106], [94mLoss[0m : 9.92723
[1mStep[0m  [30/106], [94mLoss[0m : 9.68197
[1mStep[0m  [40/106], [94mLoss[0m : 9.80925
[1mStep[0m  [50/106], [94mLoss[0m : 9.83529
[1mStep[0m  [60/106], [94mLoss[0m : 9.62817
[1mStep[0m  [70/106], [94mLoss[0m : 9.49338
[1mStep[0m  [80/106], [94mLoss[0m : 9.55077
[1mStep[0m  [90/106], [94mLoss[0m : 9.59450
[1mStep[0m  [100/106], [94mLoss[0m : 10.33347

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.817, [92mTest[0m: 9.690, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.73927
[1mStep[0m  [10/106], [94mLoss[0m : 9.81898
[1mStep[0m  [20/106], [94mLoss[0m : 10.03734
[1mStep[0m  [30/106], [94mLoss[0m : 10.11356
[1mStep[0m  [40/106], [94mLoss[0m : 10.04390
[1mStep[0m  [50/106], [94mLoss[0m : 9.33093
[1mStep[0m  [60/106], [94mLoss[0m : 10.27050
[1mStep[0m  [70/106], [94mLoss[0m : 9.46203
[1mStep[0m  [80/106], [94mLoss[0m : 9.97196
[1mStep[0m  [90/106], [94mLoss[0m : 9.51548
[1mStep[0m  [100/106], [94mLoss[0m : 10.38030

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.786, [92mTest[0m: 9.634, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.57138
[1mStep[0m  [10/106], [94mLoss[0m : 9.71521
[1mStep[0m  [20/106], [94mLoss[0m : 9.85622
[1mStep[0m  [30/106], [94mLoss[0m : 9.72968
[1mStep[0m  [40/106], [94mLoss[0m : 9.55116
[1mStep[0m  [50/106], [94mLoss[0m : 10.15280
[1mStep[0m  [60/106], [94mLoss[0m : 9.25745
[1mStep[0m  [70/106], [94mLoss[0m : 9.44989
[1mStep[0m  [80/106], [94mLoss[0m : 9.55896
[1mStep[0m  [90/106], [94mLoss[0m : 9.16553
[1mStep[0m  [100/106], [94mLoss[0m : 10.04723

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.751, [92mTest[0m: 9.621, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.90508
[1mStep[0m  [10/106], [94mLoss[0m : 9.93264
[1mStep[0m  [20/106], [94mLoss[0m : 10.29534
[1mStep[0m  [30/106], [94mLoss[0m : 9.95864
[1mStep[0m  [40/106], [94mLoss[0m : 9.48056
[1mStep[0m  [50/106], [94mLoss[0m : 10.24265
[1mStep[0m  [60/106], [94mLoss[0m : 9.37007
[1mStep[0m  [70/106], [94mLoss[0m : 9.42290
[1mStep[0m  [80/106], [94mLoss[0m : 9.84141
[1mStep[0m  [90/106], [94mLoss[0m : 9.67722
[1mStep[0m  [100/106], [94mLoss[0m : 9.90003

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.719, [92mTest[0m: 9.583, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.58372
[1mStep[0m  [10/106], [94mLoss[0m : 9.21381
[1mStep[0m  [20/106], [94mLoss[0m : 10.02765
[1mStep[0m  [30/106], [94mLoss[0m : 10.19772
[1mStep[0m  [40/106], [94mLoss[0m : 9.25163
[1mStep[0m  [50/106], [94mLoss[0m : 9.36532
[1mStep[0m  [60/106], [94mLoss[0m : 9.95868
[1mStep[0m  [70/106], [94mLoss[0m : 10.67664
[1mStep[0m  [80/106], [94mLoss[0m : 9.85443
[1mStep[0m  [90/106], [94mLoss[0m : 9.89674
[1mStep[0m  [100/106], [94mLoss[0m : 10.40405

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.697, [92mTest[0m: 9.521, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.79101
[1mStep[0m  [10/106], [94mLoss[0m : 9.30148
[1mStep[0m  [20/106], [94mLoss[0m : 9.57692
[1mStep[0m  [30/106], [94mLoss[0m : 9.66715
[1mStep[0m  [40/106], [94mLoss[0m : 9.63576
[1mStep[0m  [50/106], [94mLoss[0m : 9.43960
[1mStep[0m  [60/106], [94mLoss[0m : 9.54770
[1mStep[0m  [70/106], [94mLoss[0m : 9.69065
[1mStep[0m  [80/106], [94mLoss[0m : 9.51197
[1mStep[0m  [90/106], [94mLoss[0m : 9.71177
[1mStep[0m  [100/106], [94mLoss[0m : 9.82012

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.664, [92mTest[0m: 9.521, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.50907
[1mStep[0m  [10/106], [94mLoss[0m : 9.71191
[1mStep[0m  [20/106], [94mLoss[0m : 9.50867
[1mStep[0m  [30/106], [94mLoss[0m : 9.93984
[1mStep[0m  [40/106], [94mLoss[0m : 9.64484
[1mStep[0m  [50/106], [94mLoss[0m : 9.88191
[1mStep[0m  [60/106], [94mLoss[0m : 9.15777
[1mStep[0m  [70/106], [94mLoss[0m : 9.62547
[1mStep[0m  [80/106], [94mLoss[0m : 9.26098
[1mStep[0m  [90/106], [94mLoss[0m : 10.41245
[1mStep[0m  [100/106], [94mLoss[0m : 9.95093

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.633, [92mTest[0m: 9.413, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.95865
[1mStep[0m  [10/106], [94mLoss[0m : 9.38498
[1mStep[0m  [20/106], [94mLoss[0m : 9.44283
[1mStep[0m  [30/106], [94mLoss[0m : 9.46422
[1mStep[0m  [40/106], [94mLoss[0m : 9.54029
[1mStep[0m  [50/106], [94mLoss[0m : 9.46652
[1mStep[0m  [60/106], [94mLoss[0m : 9.44724
[1mStep[0m  [70/106], [94mLoss[0m : 9.14872
[1mStep[0m  [80/106], [94mLoss[0m : 9.83000
[1mStep[0m  [90/106], [94mLoss[0m : 9.43872
[1mStep[0m  [100/106], [94mLoss[0m : 9.41955

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.603, [92mTest[0m: 9.423, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.43286
[1mStep[0m  [10/106], [94mLoss[0m : 9.38779
[1mStep[0m  [20/106], [94mLoss[0m : 10.05006
[1mStep[0m  [30/106], [94mLoss[0m : 9.27123
[1mStep[0m  [40/106], [94mLoss[0m : 9.56699
[1mStep[0m  [50/106], [94mLoss[0m : 9.83257
[1mStep[0m  [60/106], [94mLoss[0m : 9.31434
[1mStep[0m  [70/106], [94mLoss[0m : 9.55342
[1mStep[0m  [80/106], [94mLoss[0m : 9.47387
[1mStep[0m  [90/106], [94mLoss[0m : 9.58383
[1mStep[0m  [100/106], [94mLoss[0m : 9.29076

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.569, [92mTest[0m: 9.420, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.67950
[1mStep[0m  [10/106], [94mLoss[0m : 9.01750
[1mStep[0m  [20/106], [94mLoss[0m : 9.65817
[1mStep[0m  [30/106], [94mLoss[0m : 9.41881
[1mStep[0m  [40/106], [94mLoss[0m : 9.60196
[1mStep[0m  [50/106], [94mLoss[0m : 9.59184
[1mStep[0m  [60/106], [94mLoss[0m : 9.29726
[1mStep[0m  [70/106], [94mLoss[0m : 9.61717
[1mStep[0m  [80/106], [94mLoss[0m : 9.99225
[1mStep[0m  [90/106], [94mLoss[0m : 9.70106
[1mStep[0m  [100/106], [94mLoss[0m : 10.03305

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.546, [92mTest[0m: 9.337, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.29784
[1mStep[0m  [10/106], [94mLoss[0m : 9.75806
[1mStep[0m  [20/106], [94mLoss[0m : 9.35948
[1mStep[0m  [30/106], [94mLoss[0m : 9.70120
[1mStep[0m  [40/106], [94mLoss[0m : 9.71987
[1mStep[0m  [50/106], [94mLoss[0m : 9.67322
[1mStep[0m  [60/106], [94mLoss[0m : 9.07111
[1mStep[0m  [70/106], [94mLoss[0m : 9.95769
[1mStep[0m  [80/106], [94mLoss[0m : 9.06445
[1mStep[0m  [90/106], [94mLoss[0m : 9.37710
[1mStep[0m  [100/106], [94mLoss[0m : 9.46741

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.516, [92mTest[0m: 9.319, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.26457
[1mStep[0m  [10/106], [94mLoss[0m : 9.65256
[1mStep[0m  [20/106], [94mLoss[0m : 9.82462
[1mStep[0m  [30/106], [94mLoss[0m : 9.57534
[1mStep[0m  [40/106], [94mLoss[0m : 9.79034
[1mStep[0m  [50/106], [94mLoss[0m : 9.55104
[1mStep[0m  [60/106], [94mLoss[0m : 9.30489
[1mStep[0m  [70/106], [94mLoss[0m : 9.35592
[1mStep[0m  [80/106], [94mLoss[0m : 9.49656
[1mStep[0m  [90/106], [94mLoss[0m : 9.71416
[1mStep[0m  [100/106], [94mLoss[0m : 9.13737

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.482, [92mTest[0m: 9.263, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.41655
[1mStep[0m  [10/106], [94mLoss[0m : 9.33358
[1mStep[0m  [20/106], [94mLoss[0m : 9.35354
[1mStep[0m  [30/106], [94mLoss[0m : 9.46713
[1mStep[0m  [40/106], [94mLoss[0m : 9.82094
[1mStep[0m  [50/106], [94mLoss[0m : 9.67655
[1mStep[0m  [60/106], [94mLoss[0m : 9.32424
[1mStep[0m  [70/106], [94mLoss[0m : 9.27960
[1mStep[0m  [80/106], [94mLoss[0m : 9.61302
[1mStep[0m  [90/106], [94mLoss[0m : 9.44122
[1mStep[0m  [100/106], [94mLoss[0m : 9.26703

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.446, [92mTest[0m: 9.217, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.223
====================================

Phase 2 - Evaluation MAE:  9.223094256419056
MAE score P1      10.227552
MAE score P2       9.223094
loss               9.446029
learning_rate        0.0001
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay          0.001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 10.82937
[1mStep[0m  [10/106], [94mLoss[0m : 10.38811
[1mStep[0m  [20/106], [94mLoss[0m : 11.05899
[1mStep[0m  [30/106], [94mLoss[0m : 10.60212
[1mStep[0m  [40/106], [94mLoss[0m : 9.97268
[1mStep[0m  [50/106], [94mLoss[0m : 10.30681
[1mStep[0m  [60/106], [94mLoss[0m : 10.18205
[1mStep[0m  [70/106], [94mLoss[0m : 10.59390
[1mStep[0m  [80/106], [94mLoss[0m : 10.40764
[1mStep[0m  [90/106], [94mLoss[0m : 10.26893
[1mStep[0m  [100/106], [94mLoss[0m : 10.20878

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.362, [92mTest[0m: 10.729, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.83426
[1mStep[0m  [10/106], [94mLoss[0m : 10.41090
[1mStep[0m  [20/106], [94mLoss[0m : 10.31196
[1mStep[0m  [30/106], [94mLoss[0m : 10.66298
[1mStep[0m  [40/106], [94mLoss[0m : 9.64359
[1mStep[0m  [50/106], [94mLoss[0m : 9.51275
[1mStep[0m  [60/106], [94mLoss[0m : 9.67845
[1mStep[0m  [70/106], [94mLoss[0m : 10.20338
[1mStep[0m  [80/106], [94mLoss[0m : 9.39650
[1mStep[0m  [90/106], [94mLoss[0m : 9.52213
[1mStep[0m  [100/106], [94mLoss[0m : 9.37023

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.813, [92mTest[0m: 10.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.42887
[1mStep[0m  [10/106], [94mLoss[0m : 8.85810
[1mStep[0m  [20/106], [94mLoss[0m : 9.05346
[1mStep[0m  [30/106], [94mLoss[0m : 9.03434
[1mStep[0m  [40/106], [94mLoss[0m : 9.50838
[1mStep[0m  [50/106], [94mLoss[0m : 9.50381
[1mStep[0m  [60/106], [94mLoss[0m : 9.20992
[1mStep[0m  [70/106], [94mLoss[0m : 9.17382
[1mStep[0m  [80/106], [94mLoss[0m : 8.66221
[1mStep[0m  [90/106], [94mLoss[0m : 9.26820
[1mStep[0m  [100/106], [94mLoss[0m : 8.87867

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.271, [92mTest[0m: 9.792, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.07839
[1mStep[0m  [10/106], [94mLoss[0m : 9.15614
[1mStep[0m  [20/106], [94mLoss[0m : 8.64223
[1mStep[0m  [30/106], [94mLoss[0m : 9.35931
[1mStep[0m  [40/106], [94mLoss[0m : 8.69845
[1mStep[0m  [50/106], [94mLoss[0m : 9.12034
[1mStep[0m  [60/106], [94mLoss[0m : 9.04041
[1mStep[0m  [70/106], [94mLoss[0m : 8.70507
[1mStep[0m  [80/106], [94mLoss[0m : 8.43195
[1mStep[0m  [90/106], [94mLoss[0m : 8.36707
[1mStep[0m  [100/106], [94mLoss[0m : 8.41117

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.733, [92mTest[0m: 9.337, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.70185
[1mStep[0m  [10/106], [94mLoss[0m : 8.10452
[1mStep[0m  [20/106], [94mLoss[0m : 7.91674
[1mStep[0m  [30/106], [94mLoss[0m : 8.22682
[1mStep[0m  [40/106], [94mLoss[0m : 8.67143
[1mStep[0m  [50/106], [94mLoss[0m : 7.88121
[1mStep[0m  [60/106], [94mLoss[0m : 8.39128
[1mStep[0m  [70/106], [94mLoss[0m : 7.62721
[1mStep[0m  [80/106], [94mLoss[0m : 7.58797
[1mStep[0m  [90/106], [94mLoss[0m : 8.07353
[1mStep[0m  [100/106], [94mLoss[0m : 8.00926

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.192, [92mTest[0m: 8.897, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.79441
[1mStep[0m  [10/106], [94mLoss[0m : 7.56993
[1mStep[0m  [20/106], [94mLoss[0m : 8.03631
[1mStep[0m  [30/106], [94mLoss[0m : 7.71311
[1mStep[0m  [40/106], [94mLoss[0m : 7.57061
[1mStep[0m  [50/106], [94mLoss[0m : 7.43562
[1mStep[0m  [60/106], [94mLoss[0m : 7.34537
[1mStep[0m  [70/106], [94mLoss[0m : 8.58134
[1mStep[0m  [80/106], [94mLoss[0m : 7.79839
[1mStep[0m  [90/106], [94mLoss[0m : 7.72019
[1mStep[0m  [100/106], [94mLoss[0m : 7.48150

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.683, [92mTest[0m: 8.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.28120
[1mStep[0m  [10/106], [94mLoss[0m : 6.98191
[1mStep[0m  [20/106], [94mLoss[0m : 7.37005
[1mStep[0m  [30/106], [94mLoss[0m : 7.19944
[1mStep[0m  [40/106], [94mLoss[0m : 7.32810
[1mStep[0m  [50/106], [94mLoss[0m : 7.12871
[1mStep[0m  [60/106], [94mLoss[0m : 6.75432
[1mStep[0m  [70/106], [94mLoss[0m : 7.39604
[1mStep[0m  [80/106], [94mLoss[0m : 7.32398
[1mStep[0m  [90/106], [94mLoss[0m : 7.21529
[1mStep[0m  [100/106], [94mLoss[0m : 7.03233

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.196, [92mTest[0m: 8.054, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.94381
[1mStep[0m  [10/106], [94mLoss[0m : 7.11734
[1mStep[0m  [20/106], [94mLoss[0m : 6.30590
[1mStep[0m  [30/106], [94mLoss[0m : 6.50462
[1mStep[0m  [40/106], [94mLoss[0m : 6.93551
[1mStep[0m  [50/106], [94mLoss[0m : 7.02787
[1mStep[0m  [60/106], [94mLoss[0m : 6.57317
[1mStep[0m  [70/106], [94mLoss[0m : 6.84578
[1mStep[0m  [80/106], [94mLoss[0m : 6.21484
[1mStep[0m  [90/106], [94mLoss[0m : 6.34688
[1mStep[0m  [100/106], [94mLoss[0m : 7.10813

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.738, [92mTest[0m: 7.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.23669
[1mStep[0m  [10/106], [94mLoss[0m : 6.33412
[1mStep[0m  [20/106], [94mLoss[0m : 6.59139
[1mStep[0m  [30/106], [94mLoss[0m : 6.06741
[1mStep[0m  [40/106], [94mLoss[0m : 6.42446
[1mStep[0m  [50/106], [94mLoss[0m : 6.45867
[1mStep[0m  [60/106], [94mLoss[0m : 6.42236
[1mStep[0m  [70/106], [94mLoss[0m : 5.71231
[1mStep[0m  [80/106], [94mLoss[0m : 6.70017
[1mStep[0m  [90/106], [94mLoss[0m : 6.32266
[1mStep[0m  [100/106], [94mLoss[0m : 6.23823

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.303, [92mTest[0m: 7.233, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.66205
[1mStep[0m  [10/106], [94mLoss[0m : 6.58887
[1mStep[0m  [20/106], [94mLoss[0m : 6.22537
[1mStep[0m  [30/106], [94mLoss[0m : 5.93297
[1mStep[0m  [40/106], [94mLoss[0m : 5.85080
[1mStep[0m  [50/106], [94mLoss[0m : 5.48189
[1mStep[0m  [60/106], [94mLoss[0m : 5.97167
[1mStep[0m  [70/106], [94mLoss[0m : 5.59608
[1mStep[0m  [80/106], [94mLoss[0m : 6.55758
[1mStep[0m  [90/106], [94mLoss[0m : 5.38378
[1mStep[0m  [100/106], [94mLoss[0m : 5.84350

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.928, [92mTest[0m: 6.862, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.87333
[1mStep[0m  [10/106], [94mLoss[0m : 5.71206
[1mStep[0m  [20/106], [94mLoss[0m : 6.36100
[1mStep[0m  [30/106], [94mLoss[0m : 5.73522
[1mStep[0m  [40/106], [94mLoss[0m : 5.71832
[1mStep[0m  [50/106], [94mLoss[0m : 5.77819
[1mStep[0m  [60/106], [94mLoss[0m : 5.64055
[1mStep[0m  [70/106], [94mLoss[0m : 5.49321
[1mStep[0m  [80/106], [94mLoss[0m : 5.40721
[1mStep[0m  [90/106], [94mLoss[0m : 5.39762
[1mStep[0m  [100/106], [94mLoss[0m : 5.80179

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.581, [92mTest[0m: 6.542, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.52622
[1mStep[0m  [10/106], [94mLoss[0m : 5.32470
[1mStep[0m  [20/106], [94mLoss[0m : 5.25176
[1mStep[0m  [30/106], [94mLoss[0m : 5.27902
[1mStep[0m  [40/106], [94mLoss[0m : 5.27372
[1mStep[0m  [50/106], [94mLoss[0m : 5.14349
[1mStep[0m  [60/106], [94mLoss[0m : 5.24520
[1mStep[0m  [70/106], [94mLoss[0m : 5.62468
[1mStep[0m  [80/106], [94mLoss[0m : 5.02475
[1mStep[0m  [90/106], [94mLoss[0m : 5.07342
[1mStep[0m  [100/106], [94mLoss[0m : 5.72997

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.235, [92mTest[0m: 6.139, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.38119
[1mStep[0m  [10/106], [94mLoss[0m : 4.63078
[1mStep[0m  [20/106], [94mLoss[0m : 5.47833
[1mStep[0m  [30/106], [94mLoss[0m : 5.59325
[1mStep[0m  [40/106], [94mLoss[0m : 5.17522
[1mStep[0m  [50/106], [94mLoss[0m : 5.38384
[1mStep[0m  [60/106], [94mLoss[0m : 4.91171
[1mStep[0m  [70/106], [94mLoss[0m : 4.93178
[1mStep[0m  [80/106], [94mLoss[0m : 4.46561
[1mStep[0m  [90/106], [94mLoss[0m : 5.09379
[1mStep[0m  [100/106], [94mLoss[0m : 4.40515

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.934, [92mTest[0m: 5.859, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.68937
[1mStep[0m  [10/106], [94mLoss[0m : 4.93290
[1mStep[0m  [20/106], [94mLoss[0m : 4.69012
[1mStep[0m  [30/106], [94mLoss[0m : 4.34434
[1mStep[0m  [40/106], [94mLoss[0m : 4.29822
[1mStep[0m  [50/106], [94mLoss[0m : 5.15320
[1mStep[0m  [60/106], [94mLoss[0m : 4.43664
[1mStep[0m  [70/106], [94mLoss[0m : 4.59056
[1mStep[0m  [80/106], [94mLoss[0m : 4.76921
[1mStep[0m  [90/106], [94mLoss[0m : 4.58787
[1mStep[0m  [100/106], [94mLoss[0m : 4.56153

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.650, [92mTest[0m: 5.518, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.93335
[1mStep[0m  [10/106], [94mLoss[0m : 4.36866
[1mStep[0m  [20/106], [94mLoss[0m : 4.57931
[1mStep[0m  [30/106], [94mLoss[0m : 4.35581
[1mStep[0m  [40/106], [94mLoss[0m : 4.06110
[1mStep[0m  [50/106], [94mLoss[0m : 4.02272
[1mStep[0m  [60/106], [94mLoss[0m : 4.35993
[1mStep[0m  [70/106], [94mLoss[0m : 4.97618
[1mStep[0m  [80/106], [94mLoss[0m : 4.18359
[1mStep[0m  [90/106], [94mLoss[0m : 4.18960
[1mStep[0m  [100/106], [94mLoss[0m : 4.07042

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.370, [92mTest[0m: 5.233, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.41822
[1mStep[0m  [10/106], [94mLoss[0m : 4.07301
[1mStep[0m  [20/106], [94mLoss[0m : 3.49505
[1mStep[0m  [30/106], [94mLoss[0m : 4.14001
[1mStep[0m  [40/106], [94mLoss[0m : 4.35678
[1mStep[0m  [50/106], [94mLoss[0m : 3.67396
[1mStep[0m  [60/106], [94mLoss[0m : 4.50471
[1mStep[0m  [70/106], [94mLoss[0m : 4.24425
[1mStep[0m  [80/106], [94mLoss[0m : 4.46143
[1mStep[0m  [90/106], [94mLoss[0m : 4.22528
[1mStep[0m  [100/106], [94mLoss[0m : 4.15444

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.135, [92mTest[0m: 4.909, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.28522
[1mStep[0m  [10/106], [94mLoss[0m : 3.97687
[1mStep[0m  [20/106], [94mLoss[0m : 3.93007
[1mStep[0m  [30/106], [94mLoss[0m : 3.87470
[1mStep[0m  [40/106], [94mLoss[0m : 3.38919
[1mStep[0m  [50/106], [94mLoss[0m : 4.03292
[1mStep[0m  [60/106], [94mLoss[0m : 4.21356
[1mStep[0m  [70/106], [94mLoss[0m : 3.72388
[1mStep[0m  [80/106], [94mLoss[0m : 3.65143
[1mStep[0m  [90/106], [94mLoss[0m : 3.70704
[1mStep[0m  [100/106], [94mLoss[0m : 3.76240

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.908, [92mTest[0m: 4.680, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.00389
[1mStep[0m  [10/106], [94mLoss[0m : 3.40336
[1mStep[0m  [20/106], [94mLoss[0m : 3.68206
[1mStep[0m  [30/106], [94mLoss[0m : 3.31031
[1mStep[0m  [40/106], [94mLoss[0m : 3.66044
[1mStep[0m  [50/106], [94mLoss[0m : 3.68571
[1mStep[0m  [60/106], [94mLoss[0m : 3.63744
[1mStep[0m  [70/106], [94mLoss[0m : 3.85428
[1mStep[0m  [80/106], [94mLoss[0m : 3.64461
[1mStep[0m  [90/106], [94mLoss[0m : 3.50586
[1mStep[0m  [100/106], [94mLoss[0m : 3.40157

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.696, [92mTest[0m: 4.353, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.44328
[1mStep[0m  [10/106], [94mLoss[0m : 3.34187
[1mStep[0m  [20/106], [94mLoss[0m : 3.13805
[1mStep[0m  [30/106], [94mLoss[0m : 4.02769
[1mStep[0m  [40/106], [94mLoss[0m : 3.46261
[1mStep[0m  [50/106], [94mLoss[0m : 3.47638
[1mStep[0m  [60/106], [94mLoss[0m : 3.42591
[1mStep[0m  [70/106], [94mLoss[0m : 3.21348
[1mStep[0m  [80/106], [94mLoss[0m : 3.30307
[1mStep[0m  [90/106], [94mLoss[0m : 3.09880
[1mStep[0m  [100/106], [94mLoss[0m : 3.32641

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.518, [92mTest[0m: 4.205, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.38845
[1mStep[0m  [10/106], [94mLoss[0m : 3.94482
[1mStep[0m  [20/106], [94mLoss[0m : 3.80900
[1mStep[0m  [30/106], [94mLoss[0m : 3.03905
[1mStep[0m  [40/106], [94mLoss[0m : 3.34644
[1mStep[0m  [50/106], [94mLoss[0m : 3.23255
[1mStep[0m  [60/106], [94mLoss[0m : 3.29355
[1mStep[0m  [70/106], [94mLoss[0m : 3.45485
[1mStep[0m  [80/106], [94mLoss[0m : 3.23692
[1mStep[0m  [90/106], [94mLoss[0m : 3.26581
[1mStep[0m  [100/106], [94mLoss[0m : 3.30601

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.353, [92mTest[0m: 3.997, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.33415
[1mStep[0m  [10/106], [94mLoss[0m : 2.89661
[1mStep[0m  [20/106], [94mLoss[0m : 2.98244
[1mStep[0m  [30/106], [94mLoss[0m : 3.48015
[1mStep[0m  [40/106], [94mLoss[0m : 3.55601
[1mStep[0m  [50/106], [94mLoss[0m : 2.63823
[1mStep[0m  [60/106], [94mLoss[0m : 3.28164
[1mStep[0m  [70/106], [94mLoss[0m : 3.47496
[1mStep[0m  [80/106], [94mLoss[0m : 3.39900
[1mStep[0m  [90/106], [94mLoss[0m : 3.40385
[1mStep[0m  [100/106], [94mLoss[0m : 3.04218

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.220, [92mTest[0m: 3.730, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.19503
[1mStep[0m  [10/106], [94mLoss[0m : 2.78207
[1mStep[0m  [20/106], [94mLoss[0m : 3.22859
[1mStep[0m  [30/106], [94mLoss[0m : 3.25333
[1mStep[0m  [40/106], [94mLoss[0m : 3.07584
[1mStep[0m  [50/106], [94mLoss[0m : 3.14122
[1mStep[0m  [60/106], [94mLoss[0m : 3.11900
[1mStep[0m  [70/106], [94mLoss[0m : 3.04347
[1mStep[0m  [80/106], [94mLoss[0m : 3.18613
[1mStep[0m  [90/106], [94mLoss[0m : 3.04165
[1mStep[0m  [100/106], [94mLoss[0m : 2.97675

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.119, [92mTest[0m: 3.602, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.92960
[1mStep[0m  [10/106], [94mLoss[0m : 3.06160
[1mStep[0m  [20/106], [94mLoss[0m : 2.57016
[1mStep[0m  [30/106], [94mLoss[0m : 3.03174
[1mStep[0m  [40/106], [94mLoss[0m : 2.87490
[1mStep[0m  [50/106], [94mLoss[0m : 2.82412
[1mStep[0m  [60/106], [94mLoss[0m : 2.37422
[1mStep[0m  [70/106], [94mLoss[0m : 2.63969
[1mStep[0m  [80/106], [94mLoss[0m : 3.03359
[1mStep[0m  [90/106], [94mLoss[0m : 3.26013
[1mStep[0m  [100/106], [94mLoss[0m : 3.57162

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.033, [92mTest[0m: 3.483, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.53154
[1mStep[0m  [10/106], [94mLoss[0m : 3.13777
[1mStep[0m  [20/106], [94mLoss[0m : 2.60752
[1mStep[0m  [30/106], [94mLoss[0m : 2.73861
[1mStep[0m  [40/106], [94mLoss[0m : 3.12408
[1mStep[0m  [50/106], [94mLoss[0m : 2.67857
[1mStep[0m  [60/106], [94mLoss[0m : 2.61505
[1mStep[0m  [70/106], [94mLoss[0m : 2.87427
[1mStep[0m  [80/106], [94mLoss[0m : 2.73037
[1mStep[0m  [90/106], [94mLoss[0m : 2.81738
[1mStep[0m  [100/106], [94mLoss[0m : 3.05101

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.963, [92mTest[0m: 3.391, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.98813
[1mStep[0m  [10/106], [94mLoss[0m : 3.08966
[1mStep[0m  [20/106], [94mLoss[0m : 2.49608
[1mStep[0m  [30/106], [94mLoss[0m : 3.36840
[1mStep[0m  [40/106], [94mLoss[0m : 2.91832
[1mStep[0m  [50/106], [94mLoss[0m : 3.03498
[1mStep[0m  [60/106], [94mLoss[0m : 3.25772
[1mStep[0m  [70/106], [94mLoss[0m : 3.03761
[1mStep[0m  [80/106], [94mLoss[0m : 2.40753
[1mStep[0m  [90/106], [94mLoss[0m : 3.09018
[1mStep[0m  [100/106], [94mLoss[0m : 2.60761

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.891, [92mTest[0m: 3.293, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.05425
[1mStep[0m  [10/106], [94mLoss[0m : 2.97864
[1mStep[0m  [20/106], [94mLoss[0m : 2.76746
[1mStep[0m  [30/106], [94mLoss[0m : 3.24596
[1mStep[0m  [40/106], [94mLoss[0m : 2.78380
[1mStep[0m  [50/106], [94mLoss[0m : 3.03208
[1mStep[0m  [60/106], [94mLoss[0m : 2.87543
[1mStep[0m  [70/106], [94mLoss[0m : 2.74587
[1mStep[0m  [80/106], [94mLoss[0m : 2.42086
[1mStep[0m  [90/106], [94mLoss[0m : 2.48339
[1mStep[0m  [100/106], [94mLoss[0m : 2.62369

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.836, [92mTest[0m: 3.217, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.91133
[1mStep[0m  [10/106], [94mLoss[0m : 2.92895
[1mStep[0m  [20/106], [94mLoss[0m : 3.28454
[1mStep[0m  [30/106], [94mLoss[0m : 2.56088
[1mStep[0m  [40/106], [94mLoss[0m : 3.10096
[1mStep[0m  [50/106], [94mLoss[0m : 2.75186
[1mStep[0m  [60/106], [94mLoss[0m : 2.61143
[1mStep[0m  [70/106], [94mLoss[0m : 2.55679
[1mStep[0m  [80/106], [94mLoss[0m : 3.05653
[1mStep[0m  [90/106], [94mLoss[0m : 2.45169
[1mStep[0m  [100/106], [94mLoss[0m : 2.81744

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.789, [92mTest[0m: 3.108, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.02235
[1mStep[0m  [10/106], [94mLoss[0m : 2.99238
[1mStep[0m  [20/106], [94mLoss[0m : 2.43684
[1mStep[0m  [30/106], [94mLoss[0m : 2.79840
[1mStep[0m  [40/106], [94mLoss[0m : 2.90562
[1mStep[0m  [50/106], [94mLoss[0m : 2.28542
[1mStep[0m  [60/106], [94mLoss[0m : 3.33142
[1mStep[0m  [70/106], [94mLoss[0m : 2.56078
[1mStep[0m  [80/106], [94mLoss[0m : 3.03689
[1mStep[0m  [90/106], [94mLoss[0m : 2.79531
[1mStep[0m  [100/106], [94mLoss[0m : 2.73359

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.753, [92mTest[0m: 3.093, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70752
[1mStep[0m  [10/106], [94mLoss[0m : 2.90923
[1mStep[0m  [20/106], [94mLoss[0m : 2.97197
[1mStep[0m  [30/106], [94mLoss[0m : 2.71951
[1mStep[0m  [40/106], [94mLoss[0m : 2.65841
[1mStep[0m  [50/106], [94mLoss[0m : 2.49626
[1mStep[0m  [60/106], [94mLoss[0m : 2.43783
[1mStep[0m  [70/106], [94mLoss[0m : 2.77833
[1mStep[0m  [80/106], [94mLoss[0m : 2.68766
[1mStep[0m  [90/106], [94mLoss[0m : 2.51112
[1mStep[0m  [100/106], [94mLoss[0m : 2.93414

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.732, [92mTest[0m: 3.019, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55369
[1mStep[0m  [10/106], [94mLoss[0m : 2.85320
[1mStep[0m  [20/106], [94mLoss[0m : 2.55504
[1mStep[0m  [30/106], [94mLoss[0m : 2.80197
[1mStep[0m  [40/106], [94mLoss[0m : 2.99511
[1mStep[0m  [50/106], [94mLoss[0m : 2.57926
[1mStep[0m  [60/106], [94mLoss[0m : 2.90122
[1mStep[0m  [70/106], [94mLoss[0m : 2.99340
[1mStep[0m  [80/106], [94mLoss[0m : 2.95331
[1mStep[0m  [90/106], [94mLoss[0m : 2.91047
[1mStep[0m  [100/106], [94mLoss[0m : 2.96682

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.720, [92mTest[0m: 3.025, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.013
====================================

Phase 1 - Evaluation MAE:  3.0126781463623047
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.69557
[1mStep[0m  [10/106], [94mLoss[0m : 2.71040
[1mStep[0m  [20/106], [94mLoss[0m : 2.51631
[1mStep[0m  [30/106], [94mLoss[0m : 2.52125
[1mStep[0m  [40/106], [94mLoss[0m : 2.44202
[1mStep[0m  [50/106], [94mLoss[0m : 2.68053
[1mStep[0m  [60/106], [94mLoss[0m : 2.44698
[1mStep[0m  [70/106], [94mLoss[0m : 2.64872
[1mStep[0m  [80/106], [94mLoss[0m : 2.80353
[1mStep[0m  [90/106], [94mLoss[0m : 2.43544
[1mStep[0m  [100/106], [94mLoss[0m : 2.58173

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.673, [92mTest[0m: 3.004, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.85961
[1mStep[0m  [10/106], [94mLoss[0m : 2.73008
[1mStep[0m  [20/106], [94mLoss[0m : 2.70296
[1mStep[0m  [30/106], [94mLoss[0m : 2.74730
[1mStep[0m  [40/106], [94mLoss[0m : 2.70088
[1mStep[0m  [50/106], [94mLoss[0m : 2.28041
[1mStep[0m  [60/106], [94mLoss[0m : 2.76346
[1mStep[0m  [70/106], [94mLoss[0m : 2.75114
[1mStep[0m  [80/106], [94mLoss[0m : 2.79330
[1mStep[0m  [90/106], [94mLoss[0m : 2.71188
[1mStep[0m  [100/106], [94mLoss[0m : 2.49036

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.899, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80852
[1mStep[0m  [10/106], [94mLoss[0m : 2.66043
[1mStep[0m  [20/106], [94mLoss[0m : 2.13001
[1mStep[0m  [30/106], [94mLoss[0m : 2.43830
[1mStep[0m  [40/106], [94mLoss[0m : 2.43719
[1mStep[0m  [50/106], [94mLoss[0m : 2.92647
[1mStep[0m  [60/106], [94mLoss[0m : 3.18645
[1mStep[0m  [70/106], [94mLoss[0m : 2.59603
[1mStep[0m  [80/106], [94mLoss[0m : 2.63885
[1mStep[0m  [90/106], [94mLoss[0m : 2.44057
[1mStep[0m  [100/106], [94mLoss[0m : 3.09683

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.817, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36843
[1mStep[0m  [10/106], [94mLoss[0m : 2.80994
[1mStep[0m  [20/106], [94mLoss[0m : 2.38663
[1mStep[0m  [30/106], [94mLoss[0m : 2.77276
[1mStep[0m  [40/106], [94mLoss[0m : 2.86665
[1mStep[0m  [50/106], [94mLoss[0m : 2.63339
[1mStep[0m  [60/106], [94mLoss[0m : 2.62189
[1mStep[0m  [70/106], [94mLoss[0m : 2.66876
[1mStep[0m  [80/106], [94mLoss[0m : 2.64107
[1mStep[0m  [90/106], [94mLoss[0m : 2.68168
[1mStep[0m  [100/106], [94mLoss[0m : 2.78625

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.747, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34390
[1mStep[0m  [10/106], [94mLoss[0m : 2.47583
[1mStep[0m  [20/106], [94mLoss[0m : 2.47837
[1mStep[0m  [30/106], [94mLoss[0m : 2.39120
[1mStep[0m  [40/106], [94mLoss[0m : 3.03567
[1mStep[0m  [50/106], [94mLoss[0m : 2.69209
[1mStep[0m  [60/106], [94mLoss[0m : 2.47928
[1mStep[0m  [70/106], [94mLoss[0m : 2.81212
[1mStep[0m  [80/106], [94mLoss[0m : 2.38183
[1mStep[0m  [90/106], [94mLoss[0m : 2.31803
[1mStep[0m  [100/106], [94mLoss[0m : 2.46963

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41704
[1mStep[0m  [10/106], [94mLoss[0m : 2.60270
[1mStep[0m  [20/106], [94mLoss[0m : 2.25969
[1mStep[0m  [30/106], [94mLoss[0m : 2.32018
[1mStep[0m  [40/106], [94mLoss[0m : 2.71735
[1mStep[0m  [50/106], [94mLoss[0m : 2.65014
[1mStep[0m  [60/106], [94mLoss[0m : 2.75137
[1mStep[0m  [70/106], [94mLoss[0m : 2.50021
[1mStep[0m  [80/106], [94mLoss[0m : 2.40414
[1mStep[0m  [90/106], [94mLoss[0m : 2.64202
[1mStep[0m  [100/106], [94mLoss[0m : 2.61888

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52483
[1mStep[0m  [10/106], [94mLoss[0m : 2.60597
[1mStep[0m  [20/106], [94mLoss[0m : 2.49856
[1mStep[0m  [30/106], [94mLoss[0m : 2.73026
[1mStep[0m  [40/106], [94mLoss[0m : 2.31684
[1mStep[0m  [50/106], [94mLoss[0m : 2.54529
[1mStep[0m  [60/106], [94mLoss[0m : 2.54194
[1mStep[0m  [70/106], [94mLoss[0m : 2.49359
[1mStep[0m  [80/106], [94mLoss[0m : 2.43365
[1mStep[0m  [90/106], [94mLoss[0m : 2.33161
[1mStep[0m  [100/106], [94mLoss[0m : 2.71204

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.707, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61128
[1mStep[0m  [10/106], [94mLoss[0m : 2.35059
[1mStep[0m  [20/106], [94mLoss[0m : 2.59897
[1mStep[0m  [30/106], [94mLoss[0m : 2.30101
[1mStep[0m  [40/106], [94mLoss[0m : 2.68591
[1mStep[0m  [50/106], [94mLoss[0m : 2.48210
[1mStep[0m  [60/106], [94mLoss[0m : 2.72024
[1mStep[0m  [70/106], [94mLoss[0m : 2.41755
[1mStep[0m  [80/106], [94mLoss[0m : 2.80682
[1mStep[0m  [90/106], [94mLoss[0m : 2.58716
[1mStep[0m  [100/106], [94mLoss[0m : 2.81382

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.687, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73700
[1mStep[0m  [10/106], [94mLoss[0m : 2.68783
[1mStep[0m  [20/106], [94mLoss[0m : 2.58302
[1mStep[0m  [30/106], [94mLoss[0m : 2.47277
[1mStep[0m  [40/106], [94mLoss[0m : 2.37454
[1mStep[0m  [50/106], [94mLoss[0m : 2.64932
[1mStep[0m  [60/106], [94mLoss[0m : 2.35141
[1mStep[0m  [70/106], [94mLoss[0m : 2.72089
[1mStep[0m  [80/106], [94mLoss[0m : 2.55276
[1mStep[0m  [90/106], [94mLoss[0m : 2.44837
[1mStep[0m  [100/106], [94mLoss[0m : 2.54644

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.673, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73630
[1mStep[0m  [10/106], [94mLoss[0m : 2.96841
[1mStep[0m  [20/106], [94mLoss[0m : 2.41894
[1mStep[0m  [30/106], [94mLoss[0m : 2.34670
[1mStep[0m  [40/106], [94mLoss[0m : 2.55570
[1mStep[0m  [50/106], [94mLoss[0m : 2.45945
[1mStep[0m  [60/106], [94mLoss[0m : 2.72863
[1mStep[0m  [70/106], [94mLoss[0m : 2.59740
[1mStep[0m  [80/106], [94mLoss[0m : 2.38338
[1mStep[0m  [90/106], [94mLoss[0m : 2.95603
[1mStep[0m  [100/106], [94mLoss[0m : 2.53093

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52446
[1mStep[0m  [10/106], [94mLoss[0m : 2.75348
[1mStep[0m  [20/106], [94mLoss[0m : 2.56765
[1mStep[0m  [30/106], [94mLoss[0m : 2.37239
[1mStep[0m  [40/106], [94mLoss[0m : 2.39186
[1mStep[0m  [50/106], [94mLoss[0m : 2.14168
[1mStep[0m  [60/106], [94mLoss[0m : 2.37619
[1mStep[0m  [70/106], [94mLoss[0m : 2.45717
[1mStep[0m  [80/106], [94mLoss[0m : 2.24123
[1mStep[0m  [90/106], [94mLoss[0m : 2.41768
[1mStep[0m  [100/106], [94mLoss[0m : 2.26610

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.685, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44483
[1mStep[0m  [10/106], [94mLoss[0m : 2.73668
[1mStep[0m  [20/106], [94mLoss[0m : 2.64559
[1mStep[0m  [30/106], [94mLoss[0m : 2.24392
[1mStep[0m  [40/106], [94mLoss[0m : 2.52548
[1mStep[0m  [50/106], [94mLoss[0m : 2.26843
[1mStep[0m  [60/106], [94mLoss[0m : 2.41033
[1mStep[0m  [70/106], [94mLoss[0m : 2.30577
[1mStep[0m  [80/106], [94mLoss[0m : 2.81614
[1mStep[0m  [90/106], [94mLoss[0m : 2.63032
[1mStep[0m  [100/106], [94mLoss[0m : 2.68795

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.681, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65853
[1mStep[0m  [10/106], [94mLoss[0m : 2.71046
[1mStep[0m  [20/106], [94mLoss[0m : 2.50233
[1mStep[0m  [30/106], [94mLoss[0m : 2.30817
[1mStep[0m  [40/106], [94mLoss[0m : 2.57732
[1mStep[0m  [50/106], [94mLoss[0m : 2.64031
[1mStep[0m  [60/106], [94mLoss[0m : 2.33269
[1mStep[0m  [70/106], [94mLoss[0m : 2.55329
[1mStep[0m  [80/106], [94mLoss[0m : 2.48647
[1mStep[0m  [90/106], [94mLoss[0m : 2.43149
[1mStep[0m  [100/106], [94mLoss[0m : 2.54313

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.689, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34387
[1mStep[0m  [10/106], [94mLoss[0m : 2.53605
[1mStep[0m  [20/106], [94mLoss[0m : 2.21572
[1mStep[0m  [30/106], [94mLoss[0m : 2.55748
[1mStep[0m  [40/106], [94mLoss[0m : 2.51037
[1mStep[0m  [50/106], [94mLoss[0m : 2.81489
[1mStep[0m  [60/106], [94mLoss[0m : 2.26570
[1mStep[0m  [70/106], [94mLoss[0m : 2.30813
[1mStep[0m  [80/106], [94mLoss[0m : 3.00619
[1mStep[0m  [90/106], [94mLoss[0m : 2.33063
[1mStep[0m  [100/106], [94mLoss[0m : 2.34527

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.662, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90140
[1mStep[0m  [10/106], [94mLoss[0m : 2.26299
[1mStep[0m  [20/106], [94mLoss[0m : 2.40918
[1mStep[0m  [30/106], [94mLoss[0m : 2.70834
[1mStep[0m  [40/106], [94mLoss[0m : 2.53043
[1mStep[0m  [50/106], [94mLoss[0m : 2.45433
[1mStep[0m  [60/106], [94mLoss[0m : 2.43223
[1mStep[0m  [70/106], [94mLoss[0m : 2.42366
[1mStep[0m  [80/106], [94mLoss[0m : 2.79136
[1mStep[0m  [90/106], [94mLoss[0m : 2.49460
[1mStep[0m  [100/106], [94mLoss[0m : 2.34500

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.675, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23123
[1mStep[0m  [10/106], [94mLoss[0m : 2.42711
[1mStep[0m  [20/106], [94mLoss[0m : 2.77670
[1mStep[0m  [30/106], [94mLoss[0m : 2.66710
[1mStep[0m  [40/106], [94mLoss[0m : 2.43500
[1mStep[0m  [50/106], [94mLoss[0m : 2.39410
[1mStep[0m  [60/106], [94mLoss[0m : 2.38276
[1mStep[0m  [70/106], [94mLoss[0m : 2.71886
[1mStep[0m  [80/106], [94mLoss[0m : 2.62013
[1mStep[0m  [90/106], [94mLoss[0m : 2.46937
[1mStep[0m  [100/106], [94mLoss[0m : 2.66007

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.683, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41107
[1mStep[0m  [10/106], [94mLoss[0m : 2.39052
[1mStep[0m  [20/106], [94mLoss[0m : 2.79508
[1mStep[0m  [30/106], [94mLoss[0m : 2.37788
[1mStep[0m  [40/106], [94mLoss[0m : 2.56443
[1mStep[0m  [50/106], [94mLoss[0m : 2.44019
[1mStep[0m  [60/106], [94mLoss[0m : 2.65773
[1mStep[0m  [70/106], [94mLoss[0m : 2.71921
[1mStep[0m  [80/106], [94mLoss[0m : 2.66437
[1mStep[0m  [90/106], [94mLoss[0m : 2.29819
[1mStep[0m  [100/106], [94mLoss[0m : 2.53333

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.682, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.86569
[1mStep[0m  [10/106], [94mLoss[0m : 2.32158
[1mStep[0m  [20/106], [94mLoss[0m : 2.47257
[1mStep[0m  [30/106], [94mLoss[0m : 2.47624
[1mStep[0m  [40/106], [94mLoss[0m : 2.41415
[1mStep[0m  [50/106], [94mLoss[0m : 2.69183
[1mStep[0m  [60/106], [94mLoss[0m : 2.66802
[1mStep[0m  [70/106], [94mLoss[0m : 2.60538
[1mStep[0m  [80/106], [94mLoss[0m : 2.41480
[1mStep[0m  [90/106], [94mLoss[0m : 2.72695
[1mStep[0m  [100/106], [94mLoss[0m : 2.49653

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.679, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56614
[1mStep[0m  [10/106], [94mLoss[0m : 2.34285
[1mStep[0m  [20/106], [94mLoss[0m : 2.53607
[1mStep[0m  [30/106], [94mLoss[0m : 2.47816
[1mStep[0m  [40/106], [94mLoss[0m : 2.41613
[1mStep[0m  [50/106], [94mLoss[0m : 2.57219
[1mStep[0m  [60/106], [94mLoss[0m : 2.62493
[1mStep[0m  [70/106], [94mLoss[0m : 2.60577
[1mStep[0m  [80/106], [94mLoss[0m : 2.92703
[1mStep[0m  [90/106], [94mLoss[0m : 2.14139
[1mStep[0m  [100/106], [94mLoss[0m : 2.21638

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35102
[1mStep[0m  [10/106], [94mLoss[0m : 2.48867
[1mStep[0m  [20/106], [94mLoss[0m : 2.80438
[1mStep[0m  [30/106], [94mLoss[0m : 2.61733
[1mStep[0m  [40/106], [94mLoss[0m : 2.23349
[1mStep[0m  [50/106], [94mLoss[0m : 2.65587
[1mStep[0m  [60/106], [94mLoss[0m : 2.31099
[1mStep[0m  [70/106], [94mLoss[0m : 2.61952
[1mStep[0m  [80/106], [94mLoss[0m : 2.52733
[1mStep[0m  [90/106], [94mLoss[0m : 2.33473
[1mStep[0m  [100/106], [94mLoss[0m : 2.42717

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.671, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48270
[1mStep[0m  [10/106], [94mLoss[0m : 2.80066
[1mStep[0m  [20/106], [94mLoss[0m : 2.68228
[1mStep[0m  [30/106], [94mLoss[0m : 2.46258
[1mStep[0m  [40/106], [94mLoss[0m : 2.32481
[1mStep[0m  [50/106], [94mLoss[0m : 2.78068
[1mStep[0m  [60/106], [94mLoss[0m : 2.51490
[1mStep[0m  [70/106], [94mLoss[0m : 2.59441
[1mStep[0m  [80/106], [94mLoss[0m : 2.45046
[1mStep[0m  [90/106], [94mLoss[0m : 2.67325
[1mStep[0m  [100/106], [94mLoss[0m : 2.45844

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.693, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61878
[1mStep[0m  [10/106], [94mLoss[0m : 2.43873
[1mStep[0m  [20/106], [94mLoss[0m : 2.43439
[1mStep[0m  [30/106], [94mLoss[0m : 2.45489
[1mStep[0m  [40/106], [94mLoss[0m : 2.57930
[1mStep[0m  [50/106], [94mLoss[0m : 2.53486
[1mStep[0m  [60/106], [94mLoss[0m : 2.63623
[1mStep[0m  [70/106], [94mLoss[0m : 2.63263
[1mStep[0m  [80/106], [94mLoss[0m : 2.63642
[1mStep[0m  [90/106], [94mLoss[0m : 2.51597
[1mStep[0m  [100/106], [94mLoss[0m : 2.52647

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.690, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31744
[1mStep[0m  [10/106], [94mLoss[0m : 2.75758
[1mStep[0m  [20/106], [94mLoss[0m : 2.62970
[1mStep[0m  [30/106], [94mLoss[0m : 2.38953
[1mStep[0m  [40/106], [94mLoss[0m : 2.55588
[1mStep[0m  [50/106], [94mLoss[0m : 2.34916
[1mStep[0m  [60/106], [94mLoss[0m : 2.06638
[1mStep[0m  [70/106], [94mLoss[0m : 2.58633
[1mStep[0m  [80/106], [94mLoss[0m : 2.75128
[1mStep[0m  [90/106], [94mLoss[0m : 2.63748
[1mStep[0m  [100/106], [94mLoss[0m : 2.54294

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.657, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57392
[1mStep[0m  [10/106], [94mLoss[0m : 2.49098
[1mStep[0m  [20/106], [94mLoss[0m : 2.34544
[1mStep[0m  [30/106], [94mLoss[0m : 2.42933
[1mStep[0m  [40/106], [94mLoss[0m : 2.54871
[1mStep[0m  [50/106], [94mLoss[0m : 2.34563
[1mStep[0m  [60/106], [94mLoss[0m : 2.34504
[1mStep[0m  [70/106], [94mLoss[0m : 2.26116
[1mStep[0m  [80/106], [94mLoss[0m : 2.46722
[1mStep[0m  [90/106], [94mLoss[0m : 2.33931
[1mStep[0m  [100/106], [94mLoss[0m : 2.10481

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.666, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39507
[1mStep[0m  [10/106], [94mLoss[0m : 2.41297
[1mStep[0m  [20/106], [94mLoss[0m : 2.66593
[1mStep[0m  [30/106], [94mLoss[0m : 2.65815
[1mStep[0m  [40/106], [94mLoss[0m : 2.87206
[1mStep[0m  [50/106], [94mLoss[0m : 2.40440
[1mStep[0m  [60/106], [94mLoss[0m : 2.41870
[1mStep[0m  [70/106], [94mLoss[0m : 2.40776
[1mStep[0m  [80/106], [94mLoss[0m : 2.54378
[1mStep[0m  [90/106], [94mLoss[0m : 2.72107
[1mStep[0m  [100/106], [94mLoss[0m : 2.48838

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.648, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58767
[1mStep[0m  [10/106], [94mLoss[0m : 2.78585
[1mStep[0m  [20/106], [94mLoss[0m : 2.59965
[1mStep[0m  [30/106], [94mLoss[0m : 2.52835
[1mStep[0m  [40/106], [94mLoss[0m : 2.50418
[1mStep[0m  [50/106], [94mLoss[0m : 2.51556
[1mStep[0m  [60/106], [94mLoss[0m : 2.54126
[1mStep[0m  [70/106], [94mLoss[0m : 2.44134
[1mStep[0m  [80/106], [94mLoss[0m : 2.50538
[1mStep[0m  [90/106], [94mLoss[0m : 2.54865
[1mStep[0m  [100/106], [94mLoss[0m : 2.33102

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.633, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78472
[1mStep[0m  [10/106], [94mLoss[0m : 2.61960
[1mStep[0m  [20/106], [94mLoss[0m : 2.58940
[1mStep[0m  [30/106], [94mLoss[0m : 2.61506
[1mStep[0m  [40/106], [94mLoss[0m : 2.49991
[1mStep[0m  [50/106], [94mLoss[0m : 2.59321
[1mStep[0m  [60/106], [94mLoss[0m : 2.48524
[1mStep[0m  [70/106], [94mLoss[0m : 2.41156
[1mStep[0m  [80/106], [94mLoss[0m : 2.32319
[1mStep[0m  [90/106], [94mLoss[0m : 2.58143
[1mStep[0m  [100/106], [94mLoss[0m : 2.58579

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.627, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74726
[1mStep[0m  [10/106], [94mLoss[0m : 2.43516
[1mStep[0m  [20/106], [94mLoss[0m : 2.34331
[1mStep[0m  [30/106], [94mLoss[0m : 2.77014
[1mStep[0m  [40/106], [94mLoss[0m : 2.36270
[1mStep[0m  [50/106], [94mLoss[0m : 2.45043
[1mStep[0m  [60/106], [94mLoss[0m : 2.47563
[1mStep[0m  [70/106], [94mLoss[0m : 2.22676
[1mStep[0m  [80/106], [94mLoss[0m : 2.58260
[1mStep[0m  [90/106], [94mLoss[0m : 2.68774
[1mStep[0m  [100/106], [94mLoss[0m : 2.36708

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.631, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18810
[1mStep[0m  [10/106], [94mLoss[0m : 2.94782
[1mStep[0m  [20/106], [94mLoss[0m : 2.65550
[1mStep[0m  [30/106], [94mLoss[0m : 2.53379
[1mStep[0m  [40/106], [94mLoss[0m : 2.61979
[1mStep[0m  [50/106], [94mLoss[0m : 2.46316
[1mStep[0m  [60/106], [94mLoss[0m : 2.45039
[1mStep[0m  [70/106], [94mLoss[0m : 2.59869
[1mStep[0m  [80/106], [94mLoss[0m : 2.47317
[1mStep[0m  [90/106], [94mLoss[0m : 2.62386
[1mStep[0m  [100/106], [94mLoss[0m : 2.22087

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.623, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58998
[1mStep[0m  [10/106], [94mLoss[0m : 2.47404
[1mStep[0m  [20/106], [94mLoss[0m : 2.34679
[1mStep[0m  [30/106], [94mLoss[0m : 2.55577
[1mStep[0m  [40/106], [94mLoss[0m : 2.62746
[1mStep[0m  [50/106], [94mLoss[0m : 2.54793
[1mStep[0m  [60/106], [94mLoss[0m : 2.53407
[1mStep[0m  [70/106], [94mLoss[0m : 2.48717
[1mStep[0m  [80/106], [94mLoss[0m : 2.59774
[1mStep[0m  [90/106], [94mLoss[0m : 2.31718
[1mStep[0m  [100/106], [94mLoss[0m : 2.77005

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.595, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.614
====================================

Phase 2 - Evaluation MAE:  2.6142696569550714
MAE score P1      3.012678
MAE score P2       2.61427
loss              2.459343
learning_rate       0.0001
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
