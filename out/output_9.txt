no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  9
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.85075
[1mStep[0m  [2/21], [94mLoss[0m : 10.86596
[1mStep[0m  [4/21], [94mLoss[0m : 10.99674
[1mStep[0m  [6/21], [94mLoss[0m : 10.88281
[1mStep[0m  [8/21], [94mLoss[0m : 11.05391
[1mStep[0m  [10/21], [94mLoss[0m : 10.96743
[1mStep[0m  [12/21], [94mLoss[0m : 11.05786
[1mStep[0m  [14/21], [94mLoss[0m : 10.80249
[1mStep[0m  [16/21], [94mLoss[0m : 10.81512
[1mStep[0m  [18/21], [94mLoss[0m : 11.24330
[1mStep[0m  [20/21], [94mLoss[0m : 10.73458

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.907, [92mTest[0m: 10.858, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69529
[1mStep[0m  [2/21], [94mLoss[0m : 10.61472
[1mStep[0m  [4/21], [94mLoss[0m : 10.55339
[1mStep[0m  [6/21], [94mLoss[0m : 10.93881
[1mStep[0m  [8/21], [94mLoss[0m : 10.56119
[1mStep[0m  [10/21], [94mLoss[0m : 10.93666
[1mStep[0m  [12/21], [94mLoss[0m : 10.61796
[1mStep[0m  [14/21], [94mLoss[0m : 10.83562
[1mStep[0m  [16/21], [94mLoss[0m : 10.63753
[1mStep[0m  [18/21], [94mLoss[0m : 10.61101
[1mStep[0m  [20/21], [94mLoss[0m : 10.37268

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.731, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72559
[1mStep[0m  [2/21], [94mLoss[0m : 10.54541
[1mStep[0m  [4/21], [94mLoss[0m : 10.74249
[1mStep[0m  [6/21], [94mLoss[0m : 10.38902
[1mStep[0m  [8/21], [94mLoss[0m : 10.52785
[1mStep[0m  [10/21], [94mLoss[0m : 10.52595
[1mStep[0m  [12/21], [94mLoss[0m : 10.44997
[1mStep[0m  [14/21], [94mLoss[0m : 10.49933
[1mStep[0m  [16/21], [94mLoss[0m : 10.46424
[1mStep[0m  [18/21], [94mLoss[0m : 10.56687
[1mStep[0m  [20/21], [94mLoss[0m : 10.43888

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.539, [92mTest[0m: 10.541, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50743
[1mStep[0m  [2/21], [94mLoss[0m : 10.30589
[1mStep[0m  [4/21], [94mLoss[0m : 10.59729
[1mStep[0m  [6/21], [94mLoss[0m : 10.47728
[1mStep[0m  [8/21], [94mLoss[0m : 10.25662
[1mStep[0m  [10/21], [94mLoss[0m : 10.36658
[1mStep[0m  [12/21], [94mLoss[0m : 10.35879
[1mStep[0m  [14/21], [94mLoss[0m : 10.31882
[1mStep[0m  [16/21], [94mLoss[0m : 10.08346
[1mStep[0m  [18/21], [94mLoss[0m : 10.32545
[1mStep[0m  [20/21], [94mLoss[0m : 10.15010

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.351, [92mTest[0m: 10.302, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.44423
[1mStep[0m  [2/21], [94mLoss[0m : 10.20028
[1mStep[0m  [4/21], [94mLoss[0m : 9.95675
[1mStep[0m  [6/21], [94mLoss[0m : 10.14434
[1mStep[0m  [8/21], [94mLoss[0m : 10.02342
[1mStep[0m  [10/21], [94mLoss[0m : 10.26502
[1mStep[0m  [12/21], [94mLoss[0m : 10.13925
[1mStep[0m  [14/21], [94mLoss[0m : 10.18022
[1mStep[0m  [16/21], [94mLoss[0m : 10.07063
[1mStep[0m  [18/21], [94mLoss[0m : 10.36986
[1mStep[0m  [20/21], [94mLoss[0m : 10.15111

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.158, [92mTest[0m: 10.070, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42608
[1mStep[0m  [2/21], [94mLoss[0m : 10.02474
[1mStep[0m  [4/21], [94mLoss[0m : 10.37665
[1mStep[0m  [6/21], [94mLoss[0m : 9.80804
[1mStep[0m  [8/21], [94mLoss[0m : 9.88073
[1mStep[0m  [10/21], [94mLoss[0m : 9.88242
[1mStep[0m  [12/21], [94mLoss[0m : 9.73694
[1mStep[0m  [14/21], [94mLoss[0m : 9.58365
[1mStep[0m  [16/21], [94mLoss[0m : 9.80590
[1mStep[0m  [18/21], [94mLoss[0m : 9.83698
[1mStep[0m  [20/21], [94mLoss[0m : 9.83616

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.958, [92mTest[0m: 9.839, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.77236
[1mStep[0m  [2/21], [94mLoss[0m : 9.74143
[1mStep[0m  [4/21], [94mLoss[0m : 9.87154
[1mStep[0m  [6/21], [94mLoss[0m : 9.67013
[1mStep[0m  [8/21], [94mLoss[0m : 9.58587
[1mStep[0m  [10/21], [94mLoss[0m : 9.91499
[1mStep[0m  [12/21], [94mLoss[0m : 9.85592
[1mStep[0m  [14/21], [94mLoss[0m : 9.57126
[1mStep[0m  [16/21], [94mLoss[0m : 9.91979
[1mStep[0m  [18/21], [94mLoss[0m : 9.61012
[1mStep[0m  [20/21], [94mLoss[0m : 9.75187

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.734, [92mTest[0m: 9.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.79074
[1mStep[0m  [2/21], [94mLoss[0m : 9.53629
[1mStep[0m  [4/21], [94mLoss[0m : 9.48560
[1mStep[0m  [6/21], [94mLoss[0m : 9.35427
[1mStep[0m  [8/21], [94mLoss[0m : 9.61096
[1mStep[0m  [10/21], [94mLoss[0m : 9.81694
[1mStep[0m  [12/21], [94mLoss[0m : 9.51554
[1mStep[0m  [14/21], [94mLoss[0m : 9.42458
[1mStep[0m  [16/21], [94mLoss[0m : 9.07062
[1mStep[0m  [18/21], [94mLoss[0m : 9.35932
[1mStep[0m  [20/21], [94mLoss[0m : 9.32518

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.494, [92mTest[0m: 9.283, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.56307
[1mStep[0m  [2/21], [94mLoss[0m : 9.18811
[1mStep[0m  [4/21], [94mLoss[0m : 9.34421
[1mStep[0m  [6/21], [94mLoss[0m : 9.09465
[1mStep[0m  [8/21], [94mLoss[0m : 9.38213
[1mStep[0m  [10/21], [94mLoss[0m : 9.39348
[1mStep[0m  [12/21], [94mLoss[0m : 9.35657
[1mStep[0m  [14/21], [94mLoss[0m : 9.15925
[1mStep[0m  [16/21], [94mLoss[0m : 9.34432
[1mStep[0m  [18/21], [94mLoss[0m : 9.06773
[1mStep[0m  [20/21], [94mLoss[0m : 9.21906

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.236, [92mTest[0m: 8.974, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.23084
[1mStep[0m  [2/21], [94mLoss[0m : 9.50046
[1mStep[0m  [4/21], [94mLoss[0m : 9.06040
[1mStep[0m  [6/21], [94mLoss[0m : 9.14698
[1mStep[0m  [8/21], [94mLoss[0m : 8.92599
[1mStep[0m  [10/21], [94mLoss[0m : 9.12424
[1mStep[0m  [12/21], [94mLoss[0m : 8.83744
[1mStep[0m  [14/21], [94mLoss[0m : 8.81584
[1mStep[0m  [16/21], [94mLoss[0m : 8.80666
[1mStep[0m  [18/21], [94mLoss[0m : 8.53735
[1mStep[0m  [20/21], [94mLoss[0m : 8.87796

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.957, [92mTest[0m: 8.664, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.55255
[1mStep[0m  [2/21], [94mLoss[0m : 8.97056
[1mStep[0m  [4/21], [94mLoss[0m : 8.49346
[1mStep[0m  [6/21], [94mLoss[0m : 8.68256
[1mStep[0m  [8/21], [94mLoss[0m : 8.56925
[1mStep[0m  [10/21], [94mLoss[0m : 8.91658
[1mStep[0m  [12/21], [94mLoss[0m : 8.93597
[1mStep[0m  [14/21], [94mLoss[0m : 8.44044
[1mStep[0m  [16/21], [94mLoss[0m : 8.49306
[1mStep[0m  [18/21], [94mLoss[0m : 8.43214
[1mStep[0m  [20/21], [94mLoss[0m : 8.46756

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.649, [92mTest[0m: 8.281, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.38446
[1mStep[0m  [2/21], [94mLoss[0m : 8.33522
[1mStep[0m  [4/21], [94mLoss[0m : 8.45663
[1mStep[0m  [6/21], [94mLoss[0m : 8.48313
[1mStep[0m  [8/21], [94mLoss[0m : 8.17767
[1mStep[0m  [10/21], [94mLoss[0m : 8.09148
[1mStep[0m  [12/21], [94mLoss[0m : 8.10709
[1mStep[0m  [14/21], [94mLoss[0m : 8.34139
[1mStep[0m  [16/21], [94mLoss[0m : 8.42121
[1mStep[0m  [18/21], [94mLoss[0m : 7.94967
[1mStep[0m  [20/21], [94mLoss[0m : 8.36490

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.307, [92mTest[0m: 8.001, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.12322
[1mStep[0m  [2/21], [94mLoss[0m : 7.86762
[1mStep[0m  [4/21], [94mLoss[0m : 8.17580
[1mStep[0m  [6/21], [94mLoss[0m : 8.28765
[1mStep[0m  [8/21], [94mLoss[0m : 7.83673
[1mStep[0m  [10/21], [94mLoss[0m : 8.20411
[1mStep[0m  [12/21], [94mLoss[0m : 7.90085
[1mStep[0m  [14/21], [94mLoss[0m : 7.99972
[1mStep[0m  [16/21], [94mLoss[0m : 7.83387
[1mStep[0m  [18/21], [94mLoss[0m : 7.76862
[1mStep[0m  [20/21], [94mLoss[0m : 7.95498

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.972, [92mTest[0m: 7.605, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.81355
[1mStep[0m  [2/21], [94mLoss[0m : 7.78066
[1mStep[0m  [4/21], [94mLoss[0m : 7.90769
[1mStep[0m  [6/21], [94mLoss[0m : 7.40787
[1mStep[0m  [8/21], [94mLoss[0m : 7.53594
[1mStep[0m  [10/21], [94mLoss[0m : 7.51533
[1mStep[0m  [12/21], [94mLoss[0m : 7.57933
[1mStep[0m  [14/21], [94mLoss[0m : 7.73483
[1mStep[0m  [16/21], [94mLoss[0m : 7.64371
[1mStep[0m  [18/21], [94mLoss[0m : 7.55472
[1mStep[0m  [20/21], [94mLoss[0m : 7.52281

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.641, [92mTest[0m: 7.174, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.72698
[1mStep[0m  [2/21], [94mLoss[0m : 7.36432
[1mStep[0m  [4/21], [94mLoss[0m : 7.23169
[1mStep[0m  [6/21], [94mLoss[0m : 7.58047
[1mStep[0m  [8/21], [94mLoss[0m : 7.28977
[1mStep[0m  [10/21], [94mLoss[0m : 7.38299
[1mStep[0m  [12/21], [94mLoss[0m : 7.38335
[1mStep[0m  [14/21], [94mLoss[0m : 7.29213
[1mStep[0m  [16/21], [94mLoss[0m : 7.06822
[1mStep[0m  [18/21], [94mLoss[0m : 7.14760
[1mStep[0m  [20/21], [94mLoss[0m : 6.97477

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.305, [92mTest[0m: 6.810, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.03771
[1mStep[0m  [2/21], [94mLoss[0m : 7.24080
[1mStep[0m  [4/21], [94mLoss[0m : 7.23281
[1mStep[0m  [6/21], [94mLoss[0m : 7.04797
[1mStep[0m  [8/21], [94mLoss[0m : 7.00793
[1mStep[0m  [10/21], [94mLoss[0m : 7.08534
[1mStep[0m  [12/21], [94mLoss[0m : 7.00240
[1mStep[0m  [14/21], [94mLoss[0m : 7.09549
[1mStep[0m  [16/21], [94mLoss[0m : 7.03985
[1mStep[0m  [18/21], [94mLoss[0m : 7.04645
[1mStep[0m  [20/21], [94mLoss[0m : 6.90757

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.036, [92mTest[0m: 6.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.85206
[1mStep[0m  [2/21], [94mLoss[0m : 6.68751
[1mStep[0m  [4/21], [94mLoss[0m : 7.29976
[1mStep[0m  [6/21], [94mLoss[0m : 6.76006
[1mStep[0m  [8/21], [94mLoss[0m : 6.57422
[1mStep[0m  [10/21], [94mLoss[0m : 6.75099
[1mStep[0m  [12/21], [94mLoss[0m : 6.93065
[1mStep[0m  [14/21], [94mLoss[0m : 6.64304
[1mStep[0m  [16/21], [94mLoss[0m : 6.75358
[1mStep[0m  [18/21], [94mLoss[0m : 6.66147
[1mStep[0m  [20/21], [94mLoss[0m : 6.42026

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.764, [92mTest[0m: 6.170, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.66269
[1mStep[0m  [2/21], [94mLoss[0m : 6.63873
[1mStep[0m  [4/21], [94mLoss[0m : 6.55277
[1mStep[0m  [6/21], [94mLoss[0m : 6.97109
[1mStep[0m  [8/21], [94mLoss[0m : 6.62368
[1mStep[0m  [10/21], [94mLoss[0m : 6.50789
[1mStep[0m  [12/21], [94mLoss[0m : 6.45828
[1mStep[0m  [14/21], [94mLoss[0m : 6.43273
[1mStep[0m  [16/21], [94mLoss[0m : 6.56341
[1mStep[0m  [18/21], [94mLoss[0m : 6.51720
[1mStep[0m  [20/21], [94mLoss[0m : 6.43055

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.547, [92mTest[0m: 5.883, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.33543
[1mStep[0m  [2/21], [94mLoss[0m : 6.28856
[1mStep[0m  [4/21], [94mLoss[0m : 6.36812
[1mStep[0m  [6/21], [94mLoss[0m : 6.45852
[1mStep[0m  [8/21], [94mLoss[0m : 6.44545
[1mStep[0m  [10/21], [94mLoss[0m : 6.34088
[1mStep[0m  [12/21], [94mLoss[0m : 5.99601
[1mStep[0m  [14/21], [94mLoss[0m : 6.41826
[1mStep[0m  [16/21], [94mLoss[0m : 6.19838
[1mStep[0m  [18/21], [94mLoss[0m : 6.27055
[1mStep[0m  [20/21], [94mLoss[0m : 6.27917

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.312, [92mTest[0m: 5.634, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.17899
[1mStep[0m  [2/21], [94mLoss[0m : 6.19264
[1mStep[0m  [4/21], [94mLoss[0m : 6.01752
[1mStep[0m  [6/21], [94mLoss[0m : 6.22617
[1mStep[0m  [8/21], [94mLoss[0m : 6.21727
[1mStep[0m  [10/21], [94mLoss[0m : 5.91637
[1mStep[0m  [12/21], [94mLoss[0m : 6.40274
[1mStep[0m  [14/21], [94mLoss[0m : 6.12060
[1mStep[0m  [16/21], [94mLoss[0m : 6.08635
[1mStep[0m  [18/21], [94mLoss[0m : 5.72889
[1mStep[0m  [20/21], [94mLoss[0m : 6.30612

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.108, [92mTest[0m: 5.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.92223
[1mStep[0m  [2/21], [94mLoss[0m : 5.86398
[1mStep[0m  [4/21], [94mLoss[0m : 6.17518
[1mStep[0m  [6/21], [94mLoss[0m : 6.18040
[1mStep[0m  [8/21], [94mLoss[0m : 5.81620
[1mStep[0m  [10/21], [94mLoss[0m : 6.00881
[1mStep[0m  [12/21], [94mLoss[0m : 5.76434
[1mStep[0m  [14/21], [94mLoss[0m : 5.92980
[1mStep[0m  [16/21], [94mLoss[0m : 5.62225
[1mStep[0m  [18/21], [94mLoss[0m : 6.01479
[1mStep[0m  [20/21], [94mLoss[0m : 5.99341

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.929, [92mTest[0m: 5.268, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.88660
[1mStep[0m  [2/21], [94mLoss[0m : 5.63946
[1mStep[0m  [4/21], [94mLoss[0m : 5.64015
[1mStep[0m  [6/21], [94mLoss[0m : 5.89635
[1mStep[0m  [8/21], [94mLoss[0m : 5.75256
[1mStep[0m  [10/21], [94mLoss[0m : 5.83268
[1mStep[0m  [12/21], [94mLoss[0m : 5.93780
[1mStep[0m  [14/21], [94mLoss[0m : 5.92743
[1mStep[0m  [16/21], [94mLoss[0m : 5.64331
[1mStep[0m  [18/21], [94mLoss[0m : 5.71126
[1mStep[0m  [20/21], [94mLoss[0m : 5.59736

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.716, [92mTest[0m: 5.104, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.61653
[1mStep[0m  [2/21], [94mLoss[0m : 5.48625
[1mStep[0m  [4/21], [94mLoss[0m : 5.68568
[1mStep[0m  [6/21], [94mLoss[0m : 5.65722
[1mStep[0m  [8/21], [94mLoss[0m : 5.49381
[1mStep[0m  [10/21], [94mLoss[0m : 5.29179
[1mStep[0m  [12/21], [94mLoss[0m : 5.56486
[1mStep[0m  [14/21], [94mLoss[0m : 5.67447
[1mStep[0m  [16/21], [94mLoss[0m : 5.50046
[1mStep[0m  [18/21], [94mLoss[0m : 5.40916
[1mStep[0m  [20/21], [94mLoss[0m : 5.46973

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.565, [92mTest[0m: 4.816, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.41288
[1mStep[0m  [2/21], [94mLoss[0m : 4.93460
[1mStep[0m  [4/21], [94mLoss[0m : 5.59356
[1mStep[0m  [6/21], [94mLoss[0m : 5.11048
[1mStep[0m  [8/21], [94mLoss[0m : 5.57719
[1mStep[0m  [10/21], [94mLoss[0m : 5.69796
[1mStep[0m  [12/21], [94mLoss[0m : 5.50640
[1mStep[0m  [14/21], [94mLoss[0m : 5.05041
[1mStep[0m  [16/21], [94mLoss[0m : 5.18625
[1mStep[0m  [18/21], [94mLoss[0m : 5.54305
[1mStep[0m  [20/21], [94mLoss[0m : 5.36948

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.354, [92mTest[0m: 4.624, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.39154
[1mStep[0m  [2/21], [94mLoss[0m : 5.01190
[1mStep[0m  [4/21], [94mLoss[0m : 5.23597
[1mStep[0m  [6/21], [94mLoss[0m : 5.12915
[1mStep[0m  [8/21], [94mLoss[0m : 5.20958
[1mStep[0m  [10/21], [94mLoss[0m : 5.35155
[1mStep[0m  [12/21], [94mLoss[0m : 5.21813
[1mStep[0m  [14/21], [94mLoss[0m : 5.26390
[1mStep[0m  [16/21], [94mLoss[0m : 5.03836
[1mStep[0m  [18/21], [94mLoss[0m : 5.10991
[1mStep[0m  [20/21], [94mLoss[0m : 4.92026

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.202, [92mTest[0m: 4.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.14578
[1mStep[0m  [2/21], [94mLoss[0m : 5.04151
[1mStep[0m  [4/21], [94mLoss[0m : 5.03455
[1mStep[0m  [6/21], [94mLoss[0m : 5.14831
[1mStep[0m  [8/21], [94mLoss[0m : 4.83856
[1mStep[0m  [10/21], [94mLoss[0m : 5.22317
[1mStep[0m  [12/21], [94mLoss[0m : 4.80017
[1mStep[0m  [14/21], [94mLoss[0m : 4.85322
[1mStep[0m  [16/21], [94mLoss[0m : 4.91960
[1mStep[0m  [18/21], [94mLoss[0m : 5.04179
[1mStep[0m  [20/21], [94mLoss[0m : 5.15959

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.001, [92mTest[0m: 4.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.78318
[1mStep[0m  [2/21], [94mLoss[0m : 4.97532
[1mStep[0m  [4/21], [94mLoss[0m : 4.92607
[1mStep[0m  [6/21], [94mLoss[0m : 4.78173
[1mStep[0m  [8/21], [94mLoss[0m : 4.61167
[1mStep[0m  [10/21], [94mLoss[0m : 5.05382
[1mStep[0m  [12/21], [94mLoss[0m : 4.88644
[1mStep[0m  [14/21], [94mLoss[0m : 4.50242
[1mStep[0m  [16/21], [94mLoss[0m : 4.64326
[1mStep[0m  [18/21], [94mLoss[0m : 4.66026
[1mStep[0m  [20/21], [94mLoss[0m : 4.68230

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.775, [92mTest[0m: 4.128, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.54592
[1mStep[0m  [2/21], [94mLoss[0m : 4.67193
[1mStep[0m  [4/21], [94mLoss[0m : 4.63598
[1mStep[0m  [6/21], [94mLoss[0m : 4.80434
[1mStep[0m  [8/21], [94mLoss[0m : 4.64432
[1mStep[0m  [10/21], [94mLoss[0m : 4.47672
[1mStep[0m  [12/21], [94mLoss[0m : 4.63743
[1mStep[0m  [14/21], [94mLoss[0m : 4.37094
[1mStep[0m  [16/21], [94mLoss[0m : 4.29668
[1mStep[0m  [18/21], [94mLoss[0m : 4.83077
[1mStep[0m  [20/21], [94mLoss[0m : 4.10687

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.575, [92mTest[0m: 3.910, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.13713
[1mStep[0m  [2/21], [94mLoss[0m : 4.64376
[1mStep[0m  [4/21], [94mLoss[0m : 4.22698
[1mStep[0m  [6/21], [94mLoss[0m : 4.32168
[1mStep[0m  [8/21], [94mLoss[0m : 4.20328
[1mStep[0m  [10/21], [94mLoss[0m : 4.39715
[1mStep[0m  [12/21], [94mLoss[0m : 4.60941
[1mStep[0m  [14/21], [94mLoss[0m : 4.38934
[1mStep[0m  [16/21], [94mLoss[0m : 4.40642
[1mStep[0m  [18/21], [94mLoss[0m : 4.41612
[1mStep[0m  [20/21], [94mLoss[0m : 4.54393

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.373, [92mTest[0m: 3.745, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.47536
[1mStep[0m  [2/21], [94mLoss[0m : 4.22974
[1mStep[0m  [4/21], [94mLoss[0m : 4.21723
[1mStep[0m  [6/21], [94mLoss[0m : 4.39549
[1mStep[0m  [8/21], [94mLoss[0m : 3.98225
[1mStep[0m  [10/21], [94mLoss[0m : 4.20818
[1mStep[0m  [12/21], [94mLoss[0m : 4.08291
[1mStep[0m  [14/21], [94mLoss[0m : 4.19981
[1mStep[0m  [16/21], [94mLoss[0m : 4.30827
[1mStep[0m  [18/21], [94mLoss[0m : 4.04146
[1mStep[0m  [20/21], [94mLoss[0m : 3.90765

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.188, [92mTest[0m: 3.571, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.391
====================================

Phase 1 - Evaluation MAE:  3.3914407661982944
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 3.99729
[1mStep[0m  [2/21], [94mLoss[0m : 4.03060
[1mStep[0m  [4/21], [94mLoss[0m : 4.10350
[1mStep[0m  [6/21], [94mLoss[0m : 3.96485
[1mStep[0m  [8/21], [94mLoss[0m : 4.13392
[1mStep[0m  [10/21], [94mLoss[0m : 4.03420
[1mStep[0m  [12/21], [94mLoss[0m : 3.91403
[1mStep[0m  [14/21], [94mLoss[0m : 4.05468
[1mStep[0m  [16/21], [94mLoss[0m : 4.06899
[1mStep[0m  [18/21], [94mLoss[0m : 3.66558
[1mStep[0m  [20/21], [94mLoss[0m : 3.94581

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.990, [92mTest[0m: 3.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.88633
[1mStep[0m  [2/21], [94mLoss[0m : 3.62693
[1mStep[0m  [4/21], [94mLoss[0m : 3.79089
[1mStep[0m  [6/21], [94mLoss[0m : 3.61619
[1mStep[0m  [8/21], [94mLoss[0m : 3.82477
[1mStep[0m  [10/21], [94mLoss[0m : 3.35669
[1mStep[0m  [12/21], [94mLoss[0m : 3.65922
[1mStep[0m  [14/21], [94mLoss[0m : 3.70201
[1mStep[0m  [16/21], [94mLoss[0m : 3.78421
[1mStep[0m  [18/21], [94mLoss[0m : 3.67888
[1mStep[0m  [20/21], [94mLoss[0m : 3.46592

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.675, [92mTest[0m: 3.793, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.68374
[1mStep[0m  [2/21], [94mLoss[0m : 3.52859
[1mStep[0m  [4/21], [94mLoss[0m : 3.42054
[1mStep[0m  [6/21], [94mLoss[0m : 3.27278
[1mStep[0m  [8/21], [94mLoss[0m : 3.36288
[1mStep[0m  [10/21], [94mLoss[0m : 3.13579
[1mStep[0m  [12/21], [94mLoss[0m : 3.43327
[1mStep[0m  [14/21], [94mLoss[0m : 3.27002
[1mStep[0m  [16/21], [94mLoss[0m : 3.40697
[1mStep[0m  [18/21], [94mLoss[0m : 3.25125
[1mStep[0m  [20/21], [94mLoss[0m : 3.15000

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.362, [92mTest[0m: 3.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.17576
[1mStep[0m  [2/21], [94mLoss[0m : 3.28507
[1mStep[0m  [4/21], [94mLoss[0m : 3.29409
[1mStep[0m  [6/21], [94mLoss[0m : 3.01514
[1mStep[0m  [8/21], [94mLoss[0m : 3.27030
[1mStep[0m  [10/21], [94mLoss[0m : 3.34258
[1mStep[0m  [12/21], [94mLoss[0m : 3.06591
[1mStep[0m  [14/21], [94mLoss[0m : 3.19345
[1mStep[0m  [16/21], [94mLoss[0m : 3.15002
[1mStep[0m  [18/21], [94mLoss[0m : 3.02894
[1mStep[0m  [20/21], [94mLoss[0m : 3.16956

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.170, [92mTest[0m: 3.042, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.02143
[1mStep[0m  [2/21], [94mLoss[0m : 3.02015
[1mStep[0m  [4/21], [94mLoss[0m : 2.77399
[1mStep[0m  [6/21], [94mLoss[0m : 3.09058
[1mStep[0m  [8/21], [94mLoss[0m : 2.77598
[1mStep[0m  [10/21], [94mLoss[0m : 3.06760
[1mStep[0m  [12/21], [94mLoss[0m : 2.96335
[1mStep[0m  [14/21], [94mLoss[0m : 3.06391
[1mStep[0m  [16/21], [94mLoss[0m : 2.71319
[1mStep[0m  [18/21], [94mLoss[0m : 2.80795
[1mStep[0m  [20/21], [94mLoss[0m : 2.92883

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.990, [92mTest[0m: 2.815, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76867
[1mStep[0m  [2/21], [94mLoss[0m : 2.78563
[1mStep[0m  [4/21], [94mLoss[0m : 2.88895
[1mStep[0m  [6/21], [94mLoss[0m : 2.73340
[1mStep[0m  [8/21], [94mLoss[0m : 2.89251
[1mStep[0m  [10/21], [94mLoss[0m : 2.86392
[1mStep[0m  [12/21], [94mLoss[0m : 2.91650
[1mStep[0m  [14/21], [94mLoss[0m : 2.91073
[1mStep[0m  [16/21], [94mLoss[0m : 2.86914
[1mStep[0m  [18/21], [94mLoss[0m : 2.71633
[1mStep[0m  [20/21], [94mLoss[0m : 2.64143

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.838, [92mTest[0m: 2.631, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.93255
[1mStep[0m  [2/21], [94mLoss[0m : 2.80713
[1mStep[0m  [4/21], [94mLoss[0m : 2.53536
[1mStep[0m  [6/21], [94mLoss[0m : 2.53543
[1mStep[0m  [8/21], [94mLoss[0m : 2.85338
[1mStep[0m  [10/21], [94mLoss[0m : 2.85293
[1mStep[0m  [12/21], [94mLoss[0m : 2.71483
[1mStep[0m  [14/21], [94mLoss[0m : 2.85176
[1mStep[0m  [16/21], [94mLoss[0m : 2.62918
[1mStep[0m  [18/21], [94mLoss[0m : 2.56165
[1mStep[0m  [20/21], [94mLoss[0m : 2.64624

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51858
[1mStep[0m  [2/21], [94mLoss[0m : 2.66250
[1mStep[0m  [4/21], [94mLoss[0m : 2.68616
[1mStep[0m  [6/21], [94mLoss[0m : 2.76919
[1mStep[0m  [8/21], [94mLoss[0m : 2.72195
[1mStep[0m  [10/21], [94mLoss[0m : 2.64311
[1mStep[0m  [12/21], [94mLoss[0m : 2.78362
[1mStep[0m  [14/21], [94mLoss[0m : 2.72079
[1mStep[0m  [16/21], [94mLoss[0m : 2.63850
[1mStep[0m  [18/21], [94mLoss[0m : 2.67767
[1mStep[0m  [20/21], [94mLoss[0m : 2.70124

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50632
[1mStep[0m  [2/21], [94mLoss[0m : 2.54217
[1mStep[0m  [4/21], [94mLoss[0m : 2.54246
[1mStep[0m  [6/21], [94mLoss[0m : 2.70724
[1mStep[0m  [8/21], [94mLoss[0m : 2.67547
[1mStep[0m  [10/21], [94mLoss[0m : 2.54562
[1mStep[0m  [12/21], [94mLoss[0m : 2.70105
[1mStep[0m  [14/21], [94mLoss[0m : 2.74791
[1mStep[0m  [16/21], [94mLoss[0m : 2.54714
[1mStep[0m  [18/21], [94mLoss[0m : 2.61269
[1mStep[0m  [20/21], [94mLoss[0m : 2.61834

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70374
[1mStep[0m  [2/21], [94mLoss[0m : 2.59683
[1mStep[0m  [4/21], [94mLoss[0m : 2.49034
[1mStep[0m  [6/21], [94mLoss[0m : 2.61664
[1mStep[0m  [8/21], [94mLoss[0m : 2.52326
[1mStep[0m  [10/21], [94mLoss[0m : 2.66084
[1mStep[0m  [12/21], [94mLoss[0m : 2.73270
[1mStep[0m  [14/21], [94mLoss[0m : 2.49338
[1mStep[0m  [16/21], [94mLoss[0m : 2.52664
[1mStep[0m  [18/21], [94mLoss[0m : 2.66198
[1mStep[0m  [20/21], [94mLoss[0m : 2.60111

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48671
[1mStep[0m  [2/21], [94mLoss[0m : 2.46647
[1mStep[0m  [4/21], [94mLoss[0m : 2.46051
[1mStep[0m  [6/21], [94mLoss[0m : 2.73321
[1mStep[0m  [8/21], [94mLoss[0m : 2.38829
[1mStep[0m  [10/21], [94mLoss[0m : 2.57914
[1mStep[0m  [12/21], [94mLoss[0m : 2.50312
[1mStep[0m  [14/21], [94mLoss[0m : 2.50949
[1mStep[0m  [16/21], [94mLoss[0m : 2.58247
[1mStep[0m  [18/21], [94mLoss[0m : 2.41242
[1mStep[0m  [20/21], [94mLoss[0m : 2.62031

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43111
[1mStep[0m  [2/21], [94mLoss[0m : 2.34415
[1mStep[0m  [4/21], [94mLoss[0m : 2.56761
[1mStep[0m  [6/21], [94mLoss[0m : 2.54469
[1mStep[0m  [8/21], [94mLoss[0m : 2.57267
[1mStep[0m  [10/21], [94mLoss[0m : 2.47051
[1mStep[0m  [12/21], [94mLoss[0m : 2.54659
[1mStep[0m  [14/21], [94mLoss[0m : 2.46874
[1mStep[0m  [16/21], [94mLoss[0m : 2.55074
[1mStep[0m  [18/21], [94mLoss[0m : 2.68989
[1mStep[0m  [20/21], [94mLoss[0m : 2.45061

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42245
[1mStep[0m  [2/21], [94mLoss[0m : 2.59163
[1mStep[0m  [4/21], [94mLoss[0m : 2.49612
[1mStep[0m  [6/21], [94mLoss[0m : 2.65445
[1mStep[0m  [8/21], [94mLoss[0m : 2.45021
[1mStep[0m  [10/21], [94mLoss[0m : 2.53534
[1mStep[0m  [12/21], [94mLoss[0m : 2.50037
[1mStep[0m  [14/21], [94mLoss[0m : 2.48682
[1mStep[0m  [16/21], [94mLoss[0m : 2.60427
[1mStep[0m  [18/21], [94mLoss[0m : 2.39796
[1mStep[0m  [20/21], [94mLoss[0m : 2.47396

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40615
[1mStep[0m  [2/21], [94mLoss[0m : 2.48495
[1mStep[0m  [4/21], [94mLoss[0m : 2.52561
[1mStep[0m  [6/21], [94mLoss[0m : 2.47701
[1mStep[0m  [8/21], [94mLoss[0m : 2.55418
[1mStep[0m  [10/21], [94mLoss[0m : 2.42318
[1mStep[0m  [12/21], [94mLoss[0m : 2.58578
[1mStep[0m  [14/21], [94mLoss[0m : 2.59455
[1mStep[0m  [16/21], [94mLoss[0m : 2.43631
[1mStep[0m  [18/21], [94mLoss[0m : 2.63271
[1mStep[0m  [20/21], [94mLoss[0m : 2.55913

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52194
[1mStep[0m  [2/21], [94mLoss[0m : 2.54223
[1mStep[0m  [4/21], [94mLoss[0m : 2.40490
[1mStep[0m  [6/21], [94mLoss[0m : 2.53317
[1mStep[0m  [8/21], [94mLoss[0m : 2.35807
[1mStep[0m  [10/21], [94mLoss[0m : 2.40835
[1mStep[0m  [12/21], [94mLoss[0m : 2.49624
[1mStep[0m  [14/21], [94mLoss[0m : 2.51795
[1mStep[0m  [16/21], [94mLoss[0m : 2.45097
[1mStep[0m  [18/21], [94mLoss[0m : 2.50699
[1mStep[0m  [20/21], [94mLoss[0m : 2.31930

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42051
[1mStep[0m  [2/21], [94mLoss[0m : 2.33137
[1mStep[0m  [4/21], [94mLoss[0m : 2.47707
[1mStep[0m  [6/21], [94mLoss[0m : 2.57731
[1mStep[0m  [8/21], [94mLoss[0m : 2.34666
[1mStep[0m  [10/21], [94mLoss[0m : 2.37286
[1mStep[0m  [12/21], [94mLoss[0m : 2.41520
[1mStep[0m  [14/21], [94mLoss[0m : 2.57709
[1mStep[0m  [16/21], [94mLoss[0m : 2.48509
[1mStep[0m  [18/21], [94mLoss[0m : 2.39786
[1mStep[0m  [20/21], [94mLoss[0m : 2.50175

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34186
[1mStep[0m  [2/21], [94mLoss[0m : 2.52197
[1mStep[0m  [4/21], [94mLoss[0m : 2.35779
[1mStep[0m  [6/21], [94mLoss[0m : 2.47944
[1mStep[0m  [8/21], [94mLoss[0m : 2.46812
[1mStep[0m  [10/21], [94mLoss[0m : 2.44698
[1mStep[0m  [12/21], [94mLoss[0m : 2.47953
[1mStep[0m  [14/21], [94mLoss[0m : 2.28705
[1mStep[0m  [16/21], [94mLoss[0m : 2.32892
[1mStep[0m  [18/21], [94mLoss[0m : 2.45727
[1mStep[0m  [20/21], [94mLoss[0m : 2.53036

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40136
[1mStep[0m  [2/21], [94mLoss[0m : 2.26614
[1mStep[0m  [4/21], [94mLoss[0m : 2.57903
[1mStep[0m  [6/21], [94mLoss[0m : 2.51034
[1mStep[0m  [8/21], [94mLoss[0m : 2.39184
[1mStep[0m  [10/21], [94mLoss[0m : 2.51933
[1mStep[0m  [12/21], [94mLoss[0m : 2.36340
[1mStep[0m  [14/21], [94mLoss[0m : 2.29039
[1mStep[0m  [16/21], [94mLoss[0m : 2.45103
[1mStep[0m  [18/21], [94mLoss[0m : 2.43778
[1mStep[0m  [20/21], [94mLoss[0m : 2.16796

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26564
[1mStep[0m  [2/21], [94mLoss[0m : 2.44248
[1mStep[0m  [4/21], [94mLoss[0m : 2.38614
[1mStep[0m  [6/21], [94mLoss[0m : 2.35027
[1mStep[0m  [8/21], [94mLoss[0m : 2.22525
[1mStep[0m  [10/21], [94mLoss[0m : 2.52806
[1mStep[0m  [12/21], [94mLoss[0m : 2.67758
[1mStep[0m  [14/21], [94mLoss[0m : 2.48717
[1mStep[0m  [16/21], [94mLoss[0m : 2.34488
[1mStep[0m  [18/21], [94mLoss[0m : 2.52211
[1mStep[0m  [20/21], [94mLoss[0m : 2.56307

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30325
[1mStep[0m  [2/21], [94mLoss[0m : 2.30120
[1mStep[0m  [4/21], [94mLoss[0m : 2.38195
[1mStep[0m  [6/21], [94mLoss[0m : 2.47289
[1mStep[0m  [8/21], [94mLoss[0m : 2.24627
[1mStep[0m  [10/21], [94mLoss[0m : 2.41796
[1mStep[0m  [12/21], [94mLoss[0m : 2.26113
[1mStep[0m  [14/21], [94mLoss[0m : 2.45504
[1mStep[0m  [16/21], [94mLoss[0m : 2.47622
[1mStep[0m  [18/21], [94mLoss[0m : 2.34788
[1mStep[0m  [20/21], [94mLoss[0m : 2.35042

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35514
[1mStep[0m  [2/21], [94mLoss[0m : 2.23358
[1mStep[0m  [4/21], [94mLoss[0m : 2.31739
[1mStep[0m  [6/21], [94mLoss[0m : 2.53298
[1mStep[0m  [8/21], [94mLoss[0m : 2.35138
[1mStep[0m  [10/21], [94mLoss[0m : 2.28342
[1mStep[0m  [12/21], [94mLoss[0m : 2.19775
[1mStep[0m  [14/21], [94mLoss[0m : 2.19929
[1mStep[0m  [16/21], [94mLoss[0m : 2.39959
[1mStep[0m  [18/21], [94mLoss[0m : 2.34883
[1mStep[0m  [20/21], [94mLoss[0m : 2.27125

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48420
[1mStep[0m  [2/21], [94mLoss[0m : 2.25869
[1mStep[0m  [4/21], [94mLoss[0m : 2.26288
[1mStep[0m  [6/21], [94mLoss[0m : 2.29578
[1mStep[0m  [8/21], [94mLoss[0m : 2.37307
[1mStep[0m  [10/21], [94mLoss[0m : 2.27581
[1mStep[0m  [12/21], [94mLoss[0m : 2.33969
[1mStep[0m  [14/21], [94mLoss[0m : 2.41954
[1mStep[0m  [16/21], [94mLoss[0m : 2.34835
[1mStep[0m  [18/21], [94mLoss[0m : 2.40698
[1mStep[0m  [20/21], [94mLoss[0m : 2.30784

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54713
[1mStep[0m  [2/21], [94mLoss[0m : 2.35769
[1mStep[0m  [4/21], [94mLoss[0m : 2.38729
[1mStep[0m  [6/21], [94mLoss[0m : 2.27553
[1mStep[0m  [8/21], [94mLoss[0m : 2.31498
[1mStep[0m  [10/21], [94mLoss[0m : 2.21138
[1mStep[0m  [12/21], [94mLoss[0m : 2.25738
[1mStep[0m  [14/21], [94mLoss[0m : 2.21989
[1mStep[0m  [16/21], [94mLoss[0m : 2.33281
[1mStep[0m  [18/21], [94mLoss[0m : 2.29145
[1mStep[0m  [20/21], [94mLoss[0m : 2.30698

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30010
[1mStep[0m  [2/21], [94mLoss[0m : 2.49555
[1mStep[0m  [4/21], [94mLoss[0m : 2.41665
[1mStep[0m  [6/21], [94mLoss[0m : 2.23539
[1mStep[0m  [8/21], [94mLoss[0m : 2.54889
[1mStep[0m  [10/21], [94mLoss[0m : 2.25719
[1mStep[0m  [12/21], [94mLoss[0m : 2.17599
[1mStep[0m  [14/21], [94mLoss[0m : 2.34357
[1mStep[0m  [16/21], [94mLoss[0m : 2.25948
[1mStep[0m  [18/21], [94mLoss[0m : 2.18129
[1mStep[0m  [20/21], [94mLoss[0m : 2.24427

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35840
[1mStep[0m  [2/21], [94mLoss[0m : 2.35249
[1mStep[0m  [4/21], [94mLoss[0m : 2.18828
[1mStep[0m  [6/21], [94mLoss[0m : 2.33089
[1mStep[0m  [8/21], [94mLoss[0m : 2.33261
[1mStep[0m  [10/21], [94mLoss[0m : 2.19960
[1mStep[0m  [12/21], [94mLoss[0m : 2.30141
[1mStep[0m  [14/21], [94mLoss[0m : 2.13154
[1mStep[0m  [16/21], [94mLoss[0m : 2.43666
[1mStep[0m  [18/21], [94mLoss[0m : 2.33419
[1mStep[0m  [20/21], [94mLoss[0m : 2.27491

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27934
[1mStep[0m  [2/21], [94mLoss[0m : 2.30820
[1mStep[0m  [4/21], [94mLoss[0m : 2.38935
[1mStep[0m  [6/21], [94mLoss[0m : 2.29420
[1mStep[0m  [8/21], [94mLoss[0m : 2.27398
[1mStep[0m  [10/21], [94mLoss[0m : 2.13024
[1mStep[0m  [12/21], [94mLoss[0m : 2.43039
[1mStep[0m  [14/21], [94mLoss[0m : 2.24335
[1mStep[0m  [16/21], [94mLoss[0m : 2.22538
[1mStep[0m  [18/21], [94mLoss[0m : 2.15720
[1mStep[0m  [20/21], [94mLoss[0m : 2.17026

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11434
[1mStep[0m  [2/21], [94mLoss[0m : 2.24468
[1mStep[0m  [4/21], [94mLoss[0m : 2.29491
[1mStep[0m  [6/21], [94mLoss[0m : 2.26548
[1mStep[0m  [8/21], [94mLoss[0m : 2.43359
[1mStep[0m  [10/21], [94mLoss[0m : 2.17079
[1mStep[0m  [12/21], [94mLoss[0m : 2.25621
[1mStep[0m  [14/21], [94mLoss[0m : 2.29476
[1mStep[0m  [16/21], [94mLoss[0m : 2.28531
[1mStep[0m  [18/21], [94mLoss[0m : 2.28782
[1mStep[0m  [20/21], [94mLoss[0m : 2.16200

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20778
[1mStep[0m  [2/21], [94mLoss[0m : 2.09170
[1mStep[0m  [4/21], [94mLoss[0m : 2.27588
[1mStep[0m  [6/21], [94mLoss[0m : 2.25999
[1mStep[0m  [8/21], [94mLoss[0m : 2.30846
[1mStep[0m  [10/21], [94mLoss[0m : 2.29050
[1mStep[0m  [12/21], [94mLoss[0m : 2.33154
[1mStep[0m  [14/21], [94mLoss[0m : 2.30452
[1mStep[0m  [16/21], [94mLoss[0m : 2.24167
[1mStep[0m  [18/21], [94mLoss[0m : 2.19736
[1mStep[0m  [20/21], [94mLoss[0m : 2.19196

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.397, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11642
[1mStep[0m  [2/21], [94mLoss[0m : 2.20481
[1mStep[0m  [4/21], [94mLoss[0m : 2.08605
[1mStep[0m  [6/21], [94mLoss[0m : 2.09299
[1mStep[0m  [8/21], [94mLoss[0m : 2.34027
[1mStep[0m  [10/21], [94mLoss[0m : 2.25097
[1mStep[0m  [12/21], [94mLoss[0m : 2.21372
[1mStep[0m  [14/21], [94mLoss[0m : 2.09119
[1mStep[0m  [16/21], [94mLoss[0m : 2.36615
[1mStep[0m  [18/21], [94mLoss[0m : 2.24571
[1mStep[0m  [20/21], [94mLoss[0m : 2.28006

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.234, [92mTest[0m: 2.395, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12925
[1mStep[0m  [2/21], [94mLoss[0m : 2.28016
[1mStep[0m  [4/21], [94mLoss[0m : 2.06370
[1mStep[0m  [6/21], [94mLoss[0m : 2.26157
[1mStep[0m  [8/21], [94mLoss[0m : 2.33227
[1mStep[0m  [10/21], [94mLoss[0m : 2.19616
[1mStep[0m  [12/21], [94mLoss[0m : 2.21846
[1mStep[0m  [14/21], [94mLoss[0m : 2.17802
[1mStep[0m  [16/21], [94mLoss[0m : 2.27523
[1mStep[0m  [18/21], [94mLoss[0m : 2.34794
[1mStep[0m  [20/21], [94mLoss[0m : 2.29035

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 2 - Evaluation MAE:  2.3972179208483015
MAE score P1      3.391441
MAE score P2      2.397218
loss              2.231784
learning_rate      0.00505
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.02349
[1mStep[0m  [2/21], [94mLoss[0m : 10.68941
[1mStep[0m  [4/21], [94mLoss[0m : 10.72057
[1mStep[0m  [6/21], [94mLoss[0m : 10.86070
[1mStep[0m  [8/21], [94mLoss[0m : 10.86378
[1mStep[0m  [10/21], [94mLoss[0m : 10.75859
[1mStep[0m  [12/21], [94mLoss[0m : 10.46178
[1mStep[0m  [14/21], [94mLoss[0m : 11.05316
[1mStep[0m  [16/21], [94mLoss[0m : 10.65354
[1mStep[0m  [18/21], [94mLoss[0m : 10.74033
[1mStep[0m  [20/21], [94mLoss[0m : 10.60183

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.738, [92mTest[0m: 10.785, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42601
[1mStep[0m  [2/21], [94mLoss[0m : 10.46151
[1mStep[0m  [4/21], [94mLoss[0m : 10.62534
[1mStep[0m  [6/21], [94mLoss[0m : 10.49563
[1mStep[0m  [8/21], [94mLoss[0m : 10.75884
[1mStep[0m  [10/21], [94mLoss[0m : 10.41442
[1mStep[0m  [12/21], [94mLoss[0m : 10.86634
[1mStep[0m  [14/21], [94mLoss[0m : 10.51756
[1mStep[0m  [16/21], [94mLoss[0m : 10.81918
[1mStep[0m  [18/21], [94mLoss[0m : 10.20427
[1mStep[0m  [20/21], [94mLoss[0m : 9.93264

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.481, [92mTest[0m: 10.546, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.15775
[1mStep[0m  [2/21], [94mLoss[0m : 10.02438
[1mStep[0m  [4/21], [94mLoss[0m : 10.52991
[1mStep[0m  [6/21], [94mLoss[0m : 10.47046
[1mStep[0m  [8/21], [94mLoss[0m : 10.08302
[1mStep[0m  [10/21], [94mLoss[0m : 10.21294
[1mStep[0m  [12/21], [94mLoss[0m : 10.02661
[1mStep[0m  [14/21], [94mLoss[0m : 10.17652
[1mStep[0m  [16/21], [94mLoss[0m : 10.22911
[1mStep[0m  [18/21], [94mLoss[0m : 10.26595
[1mStep[0m  [20/21], [94mLoss[0m : 10.08484

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.216, [92mTest[0m: 10.226, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33206
[1mStep[0m  [2/21], [94mLoss[0m : 10.26593
[1mStep[0m  [4/21], [94mLoss[0m : 10.03730
[1mStep[0m  [6/21], [94mLoss[0m : 10.10615
[1mStep[0m  [8/21], [94mLoss[0m : 9.95931
[1mStep[0m  [10/21], [94mLoss[0m : 10.08204
[1mStep[0m  [12/21], [94mLoss[0m : 9.74168
[1mStep[0m  [14/21], [94mLoss[0m : 9.82798
[1mStep[0m  [16/21], [94mLoss[0m : 9.81138
[1mStep[0m  [18/21], [94mLoss[0m : 9.93832
[1mStep[0m  [20/21], [94mLoss[0m : 9.82183

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.952, [92mTest[0m: 9.930, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.87613
[1mStep[0m  [2/21], [94mLoss[0m : 9.59592
[1mStep[0m  [4/21], [94mLoss[0m : 9.80804
[1mStep[0m  [6/21], [94mLoss[0m : 9.54769
[1mStep[0m  [8/21], [94mLoss[0m : 9.54875
[1mStep[0m  [10/21], [94mLoss[0m : 9.56565
[1mStep[0m  [12/21], [94mLoss[0m : 9.66938
[1mStep[0m  [14/21], [94mLoss[0m : 9.45547
[1mStep[0m  [16/21], [94mLoss[0m : 9.57647
[1mStep[0m  [18/21], [94mLoss[0m : 9.68496
[1mStep[0m  [20/21], [94mLoss[0m : 9.43199

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.652, [92mTest[0m: 9.599, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.45473
[1mStep[0m  [2/21], [94mLoss[0m : 9.52353
[1mStep[0m  [4/21], [94mLoss[0m : 9.51925
[1mStep[0m  [6/21], [94mLoss[0m : 9.55791
[1mStep[0m  [8/21], [94mLoss[0m : 9.06898
[1mStep[0m  [10/21], [94mLoss[0m : 9.27150
[1mStep[0m  [12/21], [94mLoss[0m : 9.49560
[1mStep[0m  [14/21], [94mLoss[0m : 9.51778
[1mStep[0m  [16/21], [94mLoss[0m : 9.54400
[1mStep[0m  [18/21], [94mLoss[0m : 9.28875
[1mStep[0m  [20/21], [94mLoss[0m : 9.04480

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.328, [92mTest[0m: 9.259, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.95215
[1mStep[0m  [2/21], [94mLoss[0m : 9.22416
[1mStep[0m  [4/21], [94mLoss[0m : 9.04828
[1mStep[0m  [6/21], [94mLoss[0m : 9.02199
[1mStep[0m  [8/21], [94mLoss[0m : 9.00397
[1mStep[0m  [10/21], [94mLoss[0m : 8.94299
[1mStep[0m  [12/21], [94mLoss[0m : 8.93470
[1mStep[0m  [14/21], [94mLoss[0m : 9.24385
[1mStep[0m  [16/21], [94mLoss[0m : 9.13245
[1mStep[0m  [18/21], [94mLoss[0m : 8.81860
[1mStep[0m  [20/21], [94mLoss[0m : 8.73546

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.983, [92mTest[0m: 8.890, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.02551
[1mStep[0m  [2/21], [94mLoss[0m : 8.60598
[1mStep[0m  [4/21], [94mLoss[0m : 8.75841
[1mStep[0m  [6/21], [94mLoss[0m : 8.65349
[1mStep[0m  [8/21], [94mLoss[0m : 8.42308
[1mStep[0m  [10/21], [94mLoss[0m : 8.51596
[1mStep[0m  [12/21], [94mLoss[0m : 8.71953
[1mStep[0m  [14/21], [94mLoss[0m : 8.59467
[1mStep[0m  [16/21], [94mLoss[0m : 8.36843
[1mStep[0m  [18/21], [94mLoss[0m : 8.20001
[1mStep[0m  [20/21], [94mLoss[0m : 8.27040

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.582, [92mTest[0m: 8.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.49631
[1mStep[0m  [2/21], [94mLoss[0m : 8.46008
[1mStep[0m  [4/21], [94mLoss[0m : 8.28543
[1mStep[0m  [6/21], [94mLoss[0m : 7.95124
[1mStep[0m  [8/21], [94mLoss[0m : 8.32110
[1mStep[0m  [10/21], [94mLoss[0m : 7.91326
[1mStep[0m  [12/21], [94mLoss[0m : 8.04512
[1mStep[0m  [14/21], [94mLoss[0m : 7.97505
[1mStep[0m  [16/21], [94mLoss[0m : 7.90943
[1mStep[0m  [18/21], [94mLoss[0m : 7.82358
[1mStep[0m  [20/21], [94mLoss[0m : 8.03898

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.128, [92mTest[0m: 7.958, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.68951
[1mStep[0m  [2/21], [94mLoss[0m : 7.85660
[1mStep[0m  [4/21], [94mLoss[0m : 7.66267
[1mStep[0m  [6/21], [94mLoss[0m : 7.64288
[1mStep[0m  [8/21], [94mLoss[0m : 7.62681
[1mStep[0m  [10/21], [94mLoss[0m : 8.02224
[1mStep[0m  [12/21], [94mLoss[0m : 7.80302
[1mStep[0m  [14/21], [94mLoss[0m : 7.68454
[1mStep[0m  [16/21], [94mLoss[0m : 7.38280
[1mStep[0m  [18/21], [94mLoss[0m : 7.53576
[1mStep[0m  [20/21], [94mLoss[0m : 7.36454

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.622, [92mTest[0m: 7.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.04991
[1mStep[0m  [2/21], [94mLoss[0m : 7.33319
[1mStep[0m  [4/21], [94mLoss[0m : 7.39401
[1mStep[0m  [6/21], [94mLoss[0m : 7.17738
[1mStep[0m  [8/21], [94mLoss[0m : 7.03937
[1mStep[0m  [10/21], [94mLoss[0m : 6.91040
[1mStep[0m  [12/21], [94mLoss[0m : 7.16284
[1mStep[0m  [14/21], [94mLoss[0m : 7.03357
[1mStep[0m  [16/21], [94mLoss[0m : 6.93969
[1mStep[0m  [18/21], [94mLoss[0m : 6.53691
[1mStep[0m  [20/21], [94mLoss[0m : 6.91225

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.064, [92mTest[0m: 6.827, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.91597
[1mStep[0m  [2/21], [94mLoss[0m : 6.61028
[1mStep[0m  [4/21], [94mLoss[0m : 6.71416
[1mStep[0m  [6/21], [94mLoss[0m : 6.77662
[1mStep[0m  [8/21], [94mLoss[0m : 6.52830
[1mStep[0m  [10/21], [94mLoss[0m : 6.69934
[1mStep[0m  [12/21], [94mLoss[0m : 6.52237
[1mStep[0m  [14/21], [94mLoss[0m : 6.30502
[1mStep[0m  [16/21], [94mLoss[0m : 6.54024
[1mStep[0m  [18/21], [94mLoss[0m : 6.15584
[1mStep[0m  [20/21], [94mLoss[0m : 6.32839

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.552, [92mTest[0m: 6.219, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.31372
[1mStep[0m  [2/21], [94mLoss[0m : 6.20957
[1mStep[0m  [4/21], [94mLoss[0m : 6.19101
[1mStep[0m  [6/21], [94mLoss[0m : 6.33485
[1mStep[0m  [8/21], [94mLoss[0m : 5.90262
[1mStep[0m  [10/21], [94mLoss[0m : 6.15437
[1mStep[0m  [12/21], [94mLoss[0m : 5.87304
[1mStep[0m  [14/21], [94mLoss[0m : 6.24960
[1mStep[0m  [16/21], [94mLoss[0m : 5.90247
[1mStep[0m  [18/21], [94mLoss[0m : 5.90903
[1mStep[0m  [20/21], [94mLoss[0m : 5.81770

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.068, [92mTest[0m: 5.599, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.99017
[1mStep[0m  [2/21], [94mLoss[0m : 5.74932
[1mStep[0m  [4/21], [94mLoss[0m : 5.52768
[1mStep[0m  [6/21], [94mLoss[0m : 5.68916
[1mStep[0m  [8/21], [94mLoss[0m : 5.68565
[1mStep[0m  [10/21], [94mLoss[0m : 5.68242
[1mStep[0m  [12/21], [94mLoss[0m : 5.65604
[1mStep[0m  [14/21], [94mLoss[0m : 5.54156
[1mStep[0m  [16/21], [94mLoss[0m : 5.28914
[1mStep[0m  [18/21], [94mLoss[0m : 5.52937
[1mStep[0m  [20/21], [94mLoss[0m : 5.31183

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.615, [92mTest[0m: 5.074, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.07293
[1mStep[0m  [2/21], [94mLoss[0m : 5.23568
[1mStep[0m  [4/21], [94mLoss[0m : 5.25411
[1mStep[0m  [6/21], [94mLoss[0m : 5.12230
[1mStep[0m  [8/21], [94mLoss[0m : 5.26361
[1mStep[0m  [10/21], [94mLoss[0m : 4.95055
[1mStep[0m  [12/21], [94mLoss[0m : 5.17652
[1mStep[0m  [14/21], [94mLoss[0m : 4.87089
[1mStep[0m  [16/21], [94mLoss[0m : 4.71876
[1mStep[0m  [18/21], [94mLoss[0m : 5.10892
[1mStep[0m  [20/21], [94mLoss[0m : 4.90810

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.135, [92mTest[0m: 4.523, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.84530
[1mStep[0m  [2/21], [94mLoss[0m : 4.78014
[1mStep[0m  [4/21], [94mLoss[0m : 4.70036
[1mStep[0m  [6/21], [94mLoss[0m : 4.79364
[1mStep[0m  [8/21], [94mLoss[0m : 4.75841
[1mStep[0m  [10/21], [94mLoss[0m : 4.60740
[1mStep[0m  [12/21], [94mLoss[0m : 4.65972
[1mStep[0m  [14/21], [94mLoss[0m : 4.61588
[1mStep[0m  [16/21], [94mLoss[0m : 4.40655
[1mStep[0m  [18/21], [94mLoss[0m : 4.43631
[1mStep[0m  [20/21], [94mLoss[0m : 4.29688

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.604, [92mTest[0m: 4.059, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.60622
[1mStep[0m  [2/21], [94mLoss[0m : 4.24313
[1mStep[0m  [4/21], [94mLoss[0m : 4.23054
[1mStep[0m  [6/21], [94mLoss[0m : 4.01069
[1mStep[0m  [8/21], [94mLoss[0m : 3.99204
[1mStep[0m  [10/21], [94mLoss[0m : 4.07716
[1mStep[0m  [12/21], [94mLoss[0m : 3.98432
[1mStep[0m  [14/21], [94mLoss[0m : 3.89074
[1mStep[0m  [16/21], [94mLoss[0m : 4.00490
[1mStep[0m  [18/21], [94mLoss[0m : 4.08795
[1mStep[0m  [20/21], [94mLoss[0m : 4.21477

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.133, [92mTest[0m: 3.636, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.94488
[1mStep[0m  [2/21], [94mLoss[0m : 3.93828
[1mStep[0m  [4/21], [94mLoss[0m : 3.75144
[1mStep[0m  [6/21], [94mLoss[0m : 3.74133
[1mStep[0m  [8/21], [94mLoss[0m : 3.84127
[1mStep[0m  [10/21], [94mLoss[0m : 3.62958
[1mStep[0m  [12/21], [94mLoss[0m : 3.59779
[1mStep[0m  [14/21], [94mLoss[0m : 3.66670
[1mStep[0m  [16/21], [94mLoss[0m : 3.48557
[1mStep[0m  [18/21], [94mLoss[0m : 3.52396
[1mStep[0m  [20/21], [94mLoss[0m : 3.44334

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.703, [92mTest[0m: 3.175, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.59706
[1mStep[0m  [2/21], [94mLoss[0m : 3.49744
[1mStep[0m  [4/21], [94mLoss[0m : 3.35130
[1mStep[0m  [6/21], [94mLoss[0m : 3.29895
[1mStep[0m  [8/21], [94mLoss[0m : 3.35656
[1mStep[0m  [10/21], [94mLoss[0m : 3.60731
[1mStep[0m  [12/21], [94mLoss[0m : 3.41456
[1mStep[0m  [14/21], [94mLoss[0m : 3.17356
[1mStep[0m  [16/21], [94mLoss[0m : 3.17900
[1mStep[0m  [18/21], [94mLoss[0m : 3.21361
[1mStep[0m  [20/21], [94mLoss[0m : 3.20468

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.319, [92mTest[0m: 2.896, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.34874
[1mStep[0m  [2/21], [94mLoss[0m : 3.26052
[1mStep[0m  [4/21], [94mLoss[0m : 3.33550
[1mStep[0m  [6/21], [94mLoss[0m : 3.06805
[1mStep[0m  [8/21], [94mLoss[0m : 3.09947
[1mStep[0m  [10/21], [94mLoss[0m : 2.98146
[1mStep[0m  [12/21], [94mLoss[0m : 2.96417
[1mStep[0m  [14/21], [94mLoss[0m : 2.98966
[1mStep[0m  [16/21], [94mLoss[0m : 3.06045
[1mStep[0m  [18/21], [94mLoss[0m : 3.04540
[1mStep[0m  [20/21], [94mLoss[0m : 3.10447

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.102, [92mTest[0m: 2.685, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.00550
[1mStep[0m  [2/21], [94mLoss[0m : 3.00741
[1mStep[0m  [4/21], [94mLoss[0m : 2.90128
[1mStep[0m  [6/21], [94mLoss[0m : 3.01307
[1mStep[0m  [8/21], [94mLoss[0m : 2.96409
[1mStep[0m  [10/21], [94mLoss[0m : 2.88046
[1mStep[0m  [12/21], [94mLoss[0m : 3.05944
[1mStep[0m  [14/21], [94mLoss[0m : 2.99575
[1mStep[0m  [16/21], [94mLoss[0m : 3.02995
[1mStep[0m  [18/21], [94mLoss[0m : 2.97755
[1mStep[0m  [20/21], [94mLoss[0m : 2.70683

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.940, [92mTest[0m: 2.551, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.99460
[1mStep[0m  [2/21], [94mLoss[0m : 2.77344
[1mStep[0m  [4/21], [94mLoss[0m : 2.81749
[1mStep[0m  [6/21], [94mLoss[0m : 2.90589
[1mStep[0m  [8/21], [94mLoss[0m : 2.97280
[1mStep[0m  [10/21], [94mLoss[0m : 2.85991
[1mStep[0m  [12/21], [94mLoss[0m : 2.91562
[1mStep[0m  [14/21], [94mLoss[0m : 3.08969
[1mStep[0m  [16/21], [94mLoss[0m : 2.82313
[1mStep[0m  [18/21], [94mLoss[0m : 2.73225
[1mStep[0m  [20/21], [94mLoss[0m : 2.78469

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.821, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77122
[1mStep[0m  [2/21], [94mLoss[0m : 2.83318
[1mStep[0m  [4/21], [94mLoss[0m : 2.77491
[1mStep[0m  [6/21], [94mLoss[0m : 2.72828
[1mStep[0m  [8/21], [94mLoss[0m : 2.63536
[1mStep[0m  [10/21], [94mLoss[0m : 2.91234
[1mStep[0m  [12/21], [94mLoss[0m : 2.65880
[1mStep[0m  [14/21], [94mLoss[0m : 2.83691
[1mStep[0m  [16/21], [94mLoss[0m : 2.86959
[1mStep[0m  [18/21], [94mLoss[0m : 2.75937
[1mStep[0m  [20/21], [94mLoss[0m : 2.72027

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.790, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.90892
[1mStep[0m  [2/21], [94mLoss[0m : 2.90762
[1mStep[0m  [4/21], [94mLoss[0m : 2.90804
[1mStep[0m  [6/21], [94mLoss[0m : 2.75271
[1mStep[0m  [8/21], [94mLoss[0m : 2.58102
[1mStep[0m  [10/21], [94mLoss[0m : 2.70012
[1mStep[0m  [12/21], [94mLoss[0m : 2.88299
[1mStep[0m  [14/21], [94mLoss[0m : 2.77848
[1mStep[0m  [16/21], [94mLoss[0m : 2.62490
[1mStep[0m  [18/21], [94mLoss[0m : 2.69766
[1mStep[0m  [20/21], [94mLoss[0m : 2.75132

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.761, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78436
[1mStep[0m  [2/21], [94mLoss[0m : 2.71006
[1mStep[0m  [4/21], [94mLoss[0m : 2.71298
[1mStep[0m  [6/21], [94mLoss[0m : 2.83349
[1mStep[0m  [8/21], [94mLoss[0m : 2.52988
[1mStep[0m  [10/21], [94mLoss[0m : 2.70303
[1mStep[0m  [12/21], [94mLoss[0m : 2.77429
[1mStep[0m  [14/21], [94mLoss[0m : 2.66483
[1mStep[0m  [16/21], [94mLoss[0m : 2.75195
[1mStep[0m  [18/21], [94mLoss[0m : 2.72907
[1mStep[0m  [20/21], [94mLoss[0m : 2.78994

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78279
[1mStep[0m  [2/21], [94mLoss[0m : 2.84605
[1mStep[0m  [4/21], [94mLoss[0m : 2.83245
[1mStep[0m  [6/21], [94mLoss[0m : 2.77081
[1mStep[0m  [8/21], [94mLoss[0m : 2.57474
[1mStep[0m  [10/21], [94mLoss[0m : 2.65790
[1mStep[0m  [12/21], [94mLoss[0m : 2.64523
[1mStep[0m  [14/21], [94mLoss[0m : 2.87339
[1mStep[0m  [16/21], [94mLoss[0m : 2.59952
[1mStep[0m  [18/21], [94mLoss[0m : 2.73905
[1mStep[0m  [20/21], [94mLoss[0m : 2.78628

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.720, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59204
[1mStep[0m  [2/21], [94mLoss[0m : 2.75023
[1mStep[0m  [4/21], [94mLoss[0m : 2.83021
[1mStep[0m  [6/21], [94mLoss[0m : 2.57185
[1mStep[0m  [8/21], [94mLoss[0m : 2.50388
[1mStep[0m  [10/21], [94mLoss[0m : 2.73907
[1mStep[0m  [12/21], [94mLoss[0m : 2.79350
[1mStep[0m  [14/21], [94mLoss[0m : 2.66710
[1mStep[0m  [16/21], [94mLoss[0m : 2.86155
[1mStep[0m  [18/21], [94mLoss[0m : 2.64921
[1mStep[0m  [20/21], [94mLoss[0m : 2.86024

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65739
[1mStep[0m  [2/21], [94mLoss[0m : 2.63300
[1mStep[0m  [4/21], [94mLoss[0m : 2.67671
[1mStep[0m  [6/21], [94mLoss[0m : 2.88951
[1mStep[0m  [8/21], [94mLoss[0m : 2.52669
[1mStep[0m  [10/21], [94mLoss[0m : 2.73585
[1mStep[0m  [12/21], [94mLoss[0m : 2.77447
[1mStep[0m  [14/21], [94mLoss[0m : 2.64334
[1mStep[0m  [16/21], [94mLoss[0m : 2.54387
[1mStep[0m  [18/21], [94mLoss[0m : 2.79115
[1mStep[0m  [20/21], [94mLoss[0m : 2.85956

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75530
[1mStep[0m  [2/21], [94mLoss[0m : 2.89739
[1mStep[0m  [4/21], [94mLoss[0m : 2.67300
[1mStep[0m  [6/21], [94mLoss[0m : 2.81449
[1mStep[0m  [8/21], [94mLoss[0m : 2.72380
[1mStep[0m  [10/21], [94mLoss[0m : 2.53451
[1mStep[0m  [12/21], [94mLoss[0m : 2.86738
[1mStep[0m  [14/21], [94mLoss[0m : 2.77309
[1mStep[0m  [16/21], [94mLoss[0m : 2.51269
[1mStep[0m  [18/21], [94mLoss[0m : 2.57133
[1mStep[0m  [20/21], [94mLoss[0m : 2.59671

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.382, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73601
[1mStep[0m  [2/21], [94mLoss[0m : 2.79329
[1mStep[0m  [4/21], [94mLoss[0m : 2.61854
[1mStep[0m  [6/21], [94mLoss[0m : 2.57763
[1mStep[0m  [8/21], [94mLoss[0m : 2.94790
[1mStep[0m  [10/21], [94mLoss[0m : 2.64966
[1mStep[0m  [12/21], [94mLoss[0m : 2.62573
[1mStep[0m  [14/21], [94mLoss[0m : 2.67404
[1mStep[0m  [16/21], [94mLoss[0m : 2.58375
[1mStep[0m  [18/21], [94mLoss[0m : 2.57555
[1mStep[0m  [20/21], [94mLoss[0m : 2.59722

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.374, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.370
====================================

Phase 1 - Evaluation MAE:  2.3703955241612027
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.52597
[1mStep[0m  [2/21], [94mLoss[0m : 2.81823
[1mStep[0m  [4/21], [94mLoss[0m : 2.79952
[1mStep[0m  [6/21], [94mLoss[0m : 2.64676
[1mStep[0m  [8/21], [94mLoss[0m : 2.90715
[1mStep[0m  [10/21], [94mLoss[0m : 2.58545
[1mStep[0m  [12/21], [94mLoss[0m : 2.70123
[1mStep[0m  [14/21], [94mLoss[0m : 2.69521
[1mStep[0m  [16/21], [94mLoss[0m : 2.69348
[1mStep[0m  [18/21], [94mLoss[0m : 2.49165
[1mStep[0m  [20/21], [94mLoss[0m : 2.72011

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51618
[1mStep[0m  [2/21], [94mLoss[0m : 2.58382
[1mStep[0m  [4/21], [94mLoss[0m : 2.70342
[1mStep[0m  [6/21], [94mLoss[0m : 2.78979
[1mStep[0m  [8/21], [94mLoss[0m : 2.67203
[1mStep[0m  [10/21], [94mLoss[0m : 2.65505
[1mStep[0m  [12/21], [94mLoss[0m : 2.72237
[1mStep[0m  [14/21], [94mLoss[0m : 2.83973
[1mStep[0m  [16/21], [94mLoss[0m : 2.68256
[1mStep[0m  [18/21], [94mLoss[0m : 2.63671
[1mStep[0m  [20/21], [94mLoss[0m : 2.78297

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64990
[1mStep[0m  [2/21], [94mLoss[0m : 2.70367
[1mStep[0m  [4/21], [94mLoss[0m : 2.62078
[1mStep[0m  [6/21], [94mLoss[0m : 2.65512
[1mStep[0m  [8/21], [94mLoss[0m : 2.70707
[1mStep[0m  [10/21], [94mLoss[0m : 2.55938
[1mStep[0m  [12/21], [94mLoss[0m : 2.74309
[1mStep[0m  [14/21], [94mLoss[0m : 2.69996
[1mStep[0m  [16/21], [94mLoss[0m : 2.59253
[1mStep[0m  [18/21], [94mLoss[0m : 2.74690
[1mStep[0m  [20/21], [94mLoss[0m : 2.67033

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62393
[1mStep[0m  [2/21], [94mLoss[0m : 2.58701
[1mStep[0m  [4/21], [94mLoss[0m : 2.61724
[1mStep[0m  [6/21], [94mLoss[0m : 2.62162
[1mStep[0m  [8/21], [94mLoss[0m : 2.71608
[1mStep[0m  [10/21], [94mLoss[0m : 2.64326
[1mStep[0m  [12/21], [94mLoss[0m : 2.54958
[1mStep[0m  [14/21], [94mLoss[0m : 2.71371
[1mStep[0m  [16/21], [94mLoss[0m : 2.66912
[1mStep[0m  [18/21], [94mLoss[0m : 2.60369
[1mStep[0m  [20/21], [94mLoss[0m : 2.75395

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71900
[1mStep[0m  [2/21], [94mLoss[0m : 2.67717
[1mStep[0m  [4/21], [94mLoss[0m : 2.51482
[1mStep[0m  [6/21], [94mLoss[0m : 2.70032
[1mStep[0m  [8/21], [94mLoss[0m : 2.82640
[1mStep[0m  [10/21], [94mLoss[0m : 2.59900
[1mStep[0m  [12/21], [94mLoss[0m : 2.58180
[1mStep[0m  [14/21], [94mLoss[0m : 2.61458
[1mStep[0m  [16/21], [94mLoss[0m : 2.60014
[1mStep[0m  [18/21], [94mLoss[0m : 2.62310
[1mStep[0m  [20/21], [94mLoss[0m : 2.66259

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56421
[1mStep[0m  [2/21], [94mLoss[0m : 2.73498
[1mStep[0m  [4/21], [94mLoss[0m : 2.66442
[1mStep[0m  [6/21], [94mLoss[0m : 2.63727
[1mStep[0m  [8/21], [94mLoss[0m : 2.57847
[1mStep[0m  [10/21], [94mLoss[0m : 2.43141
[1mStep[0m  [12/21], [94mLoss[0m : 2.61688
[1mStep[0m  [14/21], [94mLoss[0m : 2.41261
[1mStep[0m  [16/21], [94mLoss[0m : 2.64741
[1mStep[0m  [18/21], [94mLoss[0m : 2.65878
[1mStep[0m  [20/21], [94mLoss[0m : 2.72716

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.577, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46479
[1mStep[0m  [2/21], [94mLoss[0m : 2.51359
[1mStep[0m  [4/21], [94mLoss[0m : 2.82156
[1mStep[0m  [6/21], [94mLoss[0m : 2.69216
[1mStep[0m  [8/21], [94mLoss[0m : 2.49220
[1mStep[0m  [10/21], [94mLoss[0m : 2.56267
[1mStep[0m  [12/21], [94mLoss[0m : 2.49431
[1mStep[0m  [14/21], [94mLoss[0m : 2.63307
[1mStep[0m  [16/21], [94mLoss[0m : 2.49024
[1mStep[0m  [18/21], [94mLoss[0m : 2.66203
[1mStep[0m  [20/21], [94mLoss[0m : 2.58599

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58061
[1mStep[0m  [2/21], [94mLoss[0m : 2.49203
[1mStep[0m  [4/21], [94mLoss[0m : 2.64612
[1mStep[0m  [6/21], [94mLoss[0m : 2.61744
[1mStep[0m  [8/21], [94mLoss[0m : 2.46937
[1mStep[0m  [10/21], [94mLoss[0m : 2.73837
[1mStep[0m  [12/21], [94mLoss[0m : 2.52450
[1mStep[0m  [14/21], [94mLoss[0m : 2.54496
[1mStep[0m  [16/21], [94mLoss[0m : 2.67662
[1mStep[0m  [18/21], [94mLoss[0m : 2.56537
[1mStep[0m  [20/21], [94mLoss[0m : 2.60899

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55926
[1mStep[0m  [2/21], [94mLoss[0m : 2.57301
[1mStep[0m  [4/21], [94mLoss[0m : 2.43644
[1mStep[0m  [6/21], [94mLoss[0m : 2.27939
[1mStep[0m  [8/21], [94mLoss[0m : 2.57091
[1mStep[0m  [10/21], [94mLoss[0m : 2.61684
[1mStep[0m  [12/21], [94mLoss[0m : 2.60990
[1mStep[0m  [14/21], [94mLoss[0m : 2.52944
[1mStep[0m  [16/21], [94mLoss[0m : 2.54043
[1mStep[0m  [18/21], [94mLoss[0m : 2.54496
[1mStep[0m  [20/21], [94mLoss[0m : 2.42026

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54152
[1mStep[0m  [2/21], [94mLoss[0m : 2.45826
[1mStep[0m  [4/21], [94mLoss[0m : 2.38043
[1mStep[0m  [6/21], [94mLoss[0m : 2.52349
[1mStep[0m  [8/21], [94mLoss[0m : 2.63756
[1mStep[0m  [10/21], [94mLoss[0m : 2.45220
[1mStep[0m  [12/21], [94mLoss[0m : 2.70565
[1mStep[0m  [14/21], [94mLoss[0m : 2.59931
[1mStep[0m  [16/21], [94mLoss[0m : 2.53648
[1mStep[0m  [18/21], [94mLoss[0m : 2.51331
[1mStep[0m  [20/21], [94mLoss[0m : 2.51394

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34960
[1mStep[0m  [2/21], [94mLoss[0m : 2.56952
[1mStep[0m  [4/21], [94mLoss[0m : 2.59068
[1mStep[0m  [6/21], [94mLoss[0m : 2.47316
[1mStep[0m  [8/21], [94mLoss[0m : 2.53949
[1mStep[0m  [10/21], [94mLoss[0m : 2.62124
[1mStep[0m  [12/21], [94mLoss[0m : 2.50286
[1mStep[0m  [14/21], [94mLoss[0m : 2.65671
[1mStep[0m  [16/21], [94mLoss[0m : 2.61952
[1mStep[0m  [18/21], [94mLoss[0m : 2.49803
[1mStep[0m  [20/21], [94mLoss[0m : 2.52136

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39261
[1mStep[0m  [2/21], [94mLoss[0m : 2.60423
[1mStep[0m  [4/21], [94mLoss[0m : 2.55265
[1mStep[0m  [6/21], [94mLoss[0m : 2.36993
[1mStep[0m  [8/21], [94mLoss[0m : 2.42763
[1mStep[0m  [10/21], [94mLoss[0m : 2.49839
[1mStep[0m  [12/21], [94mLoss[0m : 2.41498
[1mStep[0m  [14/21], [94mLoss[0m : 2.36176
[1mStep[0m  [16/21], [94mLoss[0m : 2.66263
[1mStep[0m  [18/21], [94mLoss[0m : 2.42148
[1mStep[0m  [20/21], [94mLoss[0m : 2.47953

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29465
[1mStep[0m  [2/21], [94mLoss[0m : 2.57830
[1mStep[0m  [4/21], [94mLoss[0m : 2.48607
[1mStep[0m  [6/21], [94mLoss[0m : 2.38592
[1mStep[0m  [8/21], [94mLoss[0m : 2.44737
[1mStep[0m  [10/21], [94mLoss[0m : 2.51107
[1mStep[0m  [12/21], [94mLoss[0m : 2.56337
[1mStep[0m  [14/21], [94mLoss[0m : 2.32234
[1mStep[0m  [16/21], [94mLoss[0m : 2.41664
[1mStep[0m  [18/21], [94mLoss[0m : 2.45113
[1mStep[0m  [20/21], [94mLoss[0m : 2.66070

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39998
[1mStep[0m  [2/21], [94mLoss[0m : 2.65077
[1mStep[0m  [4/21], [94mLoss[0m : 2.37973
[1mStep[0m  [6/21], [94mLoss[0m : 2.42450
[1mStep[0m  [8/21], [94mLoss[0m : 2.38324
[1mStep[0m  [10/21], [94mLoss[0m : 2.49154
[1mStep[0m  [12/21], [94mLoss[0m : 2.56407
[1mStep[0m  [14/21], [94mLoss[0m : 2.56183
[1mStep[0m  [16/21], [94mLoss[0m : 2.42488
[1mStep[0m  [18/21], [94mLoss[0m : 2.50030
[1mStep[0m  [20/21], [94mLoss[0m : 2.56820

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45299
[1mStep[0m  [2/21], [94mLoss[0m : 2.48345
[1mStep[0m  [4/21], [94mLoss[0m : 2.44632
[1mStep[0m  [6/21], [94mLoss[0m : 2.54115
[1mStep[0m  [8/21], [94mLoss[0m : 2.48976
[1mStep[0m  [10/21], [94mLoss[0m : 2.43261
[1mStep[0m  [12/21], [94mLoss[0m : 2.45606
[1mStep[0m  [14/21], [94mLoss[0m : 2.40836
[1mStep[0m  [16/21], [94mLoss[0m : 2.44638
[1mStep[0m  [18/21], [94mLoss[0m : 2.38564
[1mStep[0m  [20/21], [94mLoss[0m : 2.27849

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38710
[1mStep[0m  [2/21], [94mLoss[0m : 2.46409
[1mStep[0m  [4/21], [94mLoss[0m : 2.40102
[1mStep[0m  [6/21], [94mLoss[0m : 2.52943
[1mStep[0m  [8/21], [94mLoss[0m : 2.33273
[1mStep[0m  [10/21], [94mLoss[0m : 2.34066
[1mStep[0m  [12/21], [94mLoss[0m : 2.50306
[1mStep[0m  [14/21], [94mLoss[0m : 2.52639
[1mStep[0m  [16/21], [94mLoss[0m : 2.37866
[1mStep[0m  [18/21], [94mLoss[0m : 2.45275
[1mStep[0m  [20/21], [94mLoss[0m : 2.20214

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16910
[1mStep[0m  [2/21], [94mLoss[0m : 2.50804
[1mStep[0m  [4/21], [94mLoss[0m : 2.27946
[1mStep[0m  [6/21], [94mLoss[0m : 2.31524
[1mStep[0m  [8/21], [94mLoss[0m : 2.24880
[1mStep[0m  [10/21], [94mLoss[0m : 2.26790
[1mStep[0m  [12/21], [94mLoss[0m : 2.28148
[1mStep[0m  [14/21], [94mLoss[0m : 2.43711
[1mStep[0m  [16/21], [94mLoss[0m : 2.30322
[1mStep[0m  [18/21], [94mLoss[0m : 2.43569
[1mStep[0m  [20/21], [94mLoss[0m : 2.33450

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42019
[1mStep[0m  [2/21], [94mLoss[0m : 2.48419
[1mStep[0m  [4/21], [94mLoss[0m : 2.35077
[1mStep[0m  [6/21], [94mLoss[0m : 2.26204
[1mStep[0m  [8/21], [94mLoss[0m : 2.40383
[1mStep[0m  [10/21], [94mLoss[0m : 2.33455
[1mStep[0m  [12/21], [94mLoss[0m : 2.46143
[1mStep[0m  [14/21], [94mLoss[0m : 2.18661
[1mStep[0m  [16/21], [94mLoss[0m : 2.47205
[1mStep[0m  [18/21], [94mLoss[0m : 2.39461
[1mStep[0m  [20/21], [94mLoss[0m : 2.44725

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34655
[1mStep[0m  [2/21], [94mLoss[0m : 2.31707
[1mStep[0m  [4/21], [94mLoss[0m : 2.35336
[1mStep[0m  [6/21], [94mLoss[0m : 2.15947
[1mStep[0m  [8/21], [94mLoss[0m : 2.38192
[1mStep[0m  [10/21], [94mLoss[0m : 2.40290
[1mStep[0m  [12/21], [94mLoss[0m : 2.24460
[1mStep[0m  [14/21], [94mLoss[0m : 2.29512
[1mStep[0m  [16/21], [94mLoss[0m : 2.15105
[1mStep[0m  [18/21], [94mLoss[0m : 2.43342
[1mStep[0m  [20/21], [94mLoss[0m : 2.35217

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27053
[1mStep[0m  [2/21], [94mLoss[0m : 2.17944
[1mStep[0m  [4/21], [94mLoss[0m : 2.30600
[1mStep[0m  [6/21], [94mLoss[0m : 2.33918
[1mStep[0m  [8/21], [94mLoss[0m : 2.37334
[1mStep[0m  [10/21], [94mLoss[0m : 2.37590
[1mStep[0m  [12/21], [94mLoss[0m : 2.28947
[1mStep[0m  [14/21], [94mLoss[0m : 2.30789
[1mStep[0m  [16/21], [94mLoss[0m : 2.43704
[1mStep[0m  [18/21], [94mLoss[0m : 2.37531
[1mStep[0m  [20/21], [94mLoss[0m : 2.20944

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31670
[1mStep[0m  [2/21], [94mLoss[0m : 2.18241
[1mStep[0m  [4/21], [94mLoss[0m : 2.16029
[1mStep[0m  [6/21], [94mLoss[0m : 2.37457
[1mStep[0m  [8/21], [94mLoss[0m : 2.40374
[1mStep[0m  [10/21], [94mLoss[0m : 2.40173
[1mStep[0m  [12/21], [94mLoss[0m : 2.19677
[1mStep[0m  [14/21], [94mLoss[0m : 2.38011
[1mStep[0m  [16/21], [94mLoss[0m : 2.29091
[1mStep[0m  [18/21], [94mLoss[0m : 2.31323
[1mStep[0m  [20/21], [94mLoss[0m : 2.28668

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.511, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33075
[1mStep[0m  [2/21], [94mLoss[0m : 2.34541
[1mStep[0m  [4/21], [94mLoss[0m : 2.32273
[1mStep[0m  [6/21], [94mLoss[0m : 2.46104
[1mStep[0m  [8/21], [94mLoss[0m : 2.22786
[1mStep[0m  [10/21], [94mLoss[0m : 2.23299
[1mStep[0m  [12/21], [94mLoss[0m : 2.32508
[1mStep[0m  [14/21], [94mLoss[0m : 2.16558
[1mStep[0m  [16/21], [94mLoss[0m : 2.37442
[1mStep[0m  [18/21], [94mLoss[0m : 2.42697
[1mStep[0m  [20/21], [94mLoss[0m : 2.23000

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.480, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16402
[1mStep[0m  [2/21], [94mLoss[0m : 2.21325
[1mStep[0m  [4/21], [94mLoss[0m : 2.36635
[1mStep[0m  [6/21], [94mLoss[0m : 2.13584
[1mStep[0m  [8/21], [94mLoss[0m : 2.24903
[1mStep[0m  [10/21], [94mLoss[0m : 2.33375
[1mStep[0m  [12/21], [94mLoss[0m : 2.40840
[1mStep[0m  [14/21], [94mLoss[0m : 2.35538
[1mStep[0m  [16/21], [94mLoss[0m : 2.26253
[1mStep[0m  [18/21], [94mLoss[0m : 2.35820
[1mStep[0m  [20/21], [94mLoss[0m : 2.22306

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.454, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18125
[1mStep[0m  [2/21], [94mLoss[0m : 2.14747
[1mStep[0m  [4/21], [94mLoss[0m : 2.28633
[1mStep[0m  [6/21], [94mLoss[0m : 2.10489
[1mStep[0m  [8/21], [94mLoss[0m : 2.17329
[1mStep[0m  [10/21], [94mLoss[0m : 2.18835
[1mStep[0m  [12/21], [94mLoss[0m : 2.21571
[1mStep[0m  [14/21], [94mLoss[0m : 2.31957
[1mStep[0m  [16/21], [94mLoss[0m : 2.37828
[1mStep[0m  [18/21], [94mLoss[0m : 2.36809
[1mStep[0m  [20/21], [94mLoss[0m : 2.07221

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25676
[1mStep[0m  [2/21], [94mLoss[0m : 2.16015
[1mStep[0m  [4/21], [94mLoss[0m : 2.25354
[1mStep[0m  [6/21], [94mLoss[0m : 2.10058
[1mStep[0m  [8/21], [94mLoss[0m : 2.22703
[1mStep[0m  [10/21], [94mLoss[0m : 2.06901
[1mStep[0m  [12/21], [94mLoss[0m : 2.19328
[1mStep[0m  [14/21], [94mLoss[0m : 2.34126
[1mStep[0m  [16/21], [94mLoss[0m : 2.10680
[1mStep[0m  [18/21], [94mLoss[0m : 2.36597
[1mStep[0m  [20/21], [94mLoss[0m : 2.36396

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13187
[1mStep[0m  [2/21], [94mLoss[0m : 2.03645
[1mStep[0m  [4/21], [94mLoss[0m : 2.22339
[1mStep[0m  [6/21], [94mLoss[0m : 2.17610
[1mStep[0m  [8/21], [94mLoss[0m : 2.16799
[1mStep[0m  [10/21], [94mLoss[0m : 2.01281
[1mStep[0m  [12/21], [94mLoss[0m : 2.19536
[1mStep[0m  [14/21], [94mLoss[0m : 2.15782
[1mStep[0m  [16/21], [94mLoss[0m : 2.09247
[1mStep[0m  [18/21], [94mLoss[0m : 2.13511
[1mStep[0m  [20/21], [94mLoss[0m : 2.16182

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.451, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16669
[1mStep[0m  [2/21], [94mLoss[0m : 2.23773
[1mStep[0m  [4/21], [94mLoss[0m : 2.06230
[1mStep[0m  [6/21], [94mLoss[0m : 2.24762
[1mStep[0m  [8/21], [94mLoss[0m : 2.33828
[1mStep[0m  [10/21], [94mLoss[0m : 2.10806
[1mStep[0m  [12/21], [94mLoss[0m : 2.16508
[1mStep[0m  [14/21], [94mLoss[0m : 2.09134
[1mStep[0m  [16/21], [94mLoss[0m : 2.13967
[1mStep[0m  [18/21], [94mLoss[0m : 2.14574
[1mStep[0m  [20/21], [94mLoss[0m : 2.12248

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10272
[1mStep[0m  [2/21], [94mLoss[0m : 2.27125
[1mStep[0m  [4/21], [94mLoss[0m : 2.07296
[1mStep[0m  [6/21], [94mLoss[0m : 2.23252
[1mStep[0m  [8/21], [94mLoss[0m : 2.09325
[1mStep[0m  [10/21], [94mLoss[0m : 2.05975
[1mStep[0m  [12/21], [94mLoss[0m : 2.03311
[1mStep[0m  [14/21], [94mLoss[0m : 2.12337
[1mStep[0m  [16/21], [94mLoss[0m : 2.08485
[1mStep[0m  [18/21], [94mLoss[0m : 2.07740
[1mStep[0m  [20/21], [94mLoss[0m : 2.23090

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11252
[1mStep[0m  [2/21], [94mLoss[0m : 2.09577
[1mStep[0m  [4/21], [94mLoss[0m : 2.04058
[1mStep[0m  [6/21], [94mLoss[0m : 2.02258
[1mStep[0m  [8/21], [94mLoss[0m : 2.10347
[1mStep[0m  [10/21], [94mLoss[0m : 2.08383
[1mStep[0m  [12/21], [94mLoss[0m : 2.00131
[1mStep[0m  [14/21], [94mLoss[0m : 2.14565
[1mStep[0m  [16/21], [94mLoss[0m : 2.20992
[1mStep[0m  [18/21], [94mLoss[0m : 2.05271
[1mStep[0m  [20/21], [94mLoss[0m : 2.05122

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07208
[1mStep[0m  [2/21], [94mLoss[0m : 2.17213
[1mStep[0m  [4/21], [94mLoss[0m : 1.93481
[1mStep[0m  [6/21], [94mLoss[0m : 1.97166
[1mStep[0m  [8/21], [94mLoss[0m : 2.17121
[1mStep[0m  [10/21], [94mLoss[0m : 2.08994
[1mStep[0m  [12/21], [94mLoss[0m : 2.20132
[1mStep[0m  [14/21], [94mLoss[0m : 2.05730
[1mStep[0m  [16/21], [94mLoss[0m : 2.09286
[1mStep[0m  [18/21], [94mLoss[0m : 2.16075
[1mStep[0m  [20/21], [94mLoss[0m : 2.24080

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.107, [92mTest[0m: 2.472, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.490
====================================

Phase 2 - Evaluation MAE:  2.4895730359213695
MAE score P1       2.370396
MAE score P2       2.489573
loss                2.10673
learning_rate       0.00505
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 12.16649
[1mStep[0m  [33/339], [94mLoss[0m : 3.11836
[1mStep[0m  [66/339], [94mLoss[0m : 2.55890
[1mStep[0m  [99/339], [94mLoss[0m : 2.25667
[1mStep[0m  [132/339], [94mLoss[0m : 2.60180
[1mStep[0m  [165/339], [94mLoss[0m : 2.67934
[1mStep[0m  [198/339], [94mLoss[0m : 2.34022
[1mStep[0m  [231/339], [94mLoss[0m : 2.39927
[1mStep[0m  [264/339], [94mLoss[0m : 2.88878
[1mStep[0m  [297/339], [94mLoss[0m : 2.33700
[1mStep[0m  [330/339], [94mLoss[0m : 2.69956

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.720, [92mTest[0m: 10.744, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38837
[1mStep[0m  [33/339], [94mLoss[0m : 2.11491
[1mStep[0m  [66/339], [94mLoss[0m : 2.32132
[1mStep[0m  [99/339], [94mLoss[0m : 1.95104
[1mStep[0m  [132/339], [94mLoss[0m : 2.92448
[1mStep[0m  [165/339], [94mLoss[0m : 2.50554
[1mStep[0m  [198/339], [94mLoss[0m : 2.19934
[1mStep[0m  [231/339], [94mLoss[0m : 2.35651
[1mStep[0m  [264/339], [94mLoss[0m : 2.24276
[1mStep[0m  [297/339], [94mLoss[0m : 1.89539
[1mStep[0m  [330/339], [94mLoss[0m : 1.74323

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02722
[1mStep[0m  [33/339], [94mLoss[0m : 2.53314
[1mStep[0m  [66/339], [94mLoss[0m : 2.94431
[1mStep[0m  [99/339], [94mLoss[0m : 2.87435
[1mStep[0m  [132/339], [94mLoss[0m : 2.25560
[1mStep[0m  [165/339], [94mLoss[0m : 2.67482
[1mStep[0m  [198/339], [94mLoss[0m : 2.38704
[1mStep[0m  [231/339], [94mLoss[0m : 2.94930
[1mStep[0m  [264/339], [94mLoss[0m : 2.08798
[1mStep[0m  [297/339], [94mLoss[0m : 2.76926
[1mStep[0m  [330/339], [94mLoss[0m : 2.45707

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71793
[1mStep[0m  [33/339], [94mLoss[0m : 2.47239
[1mStep[0m  [66/339], [94mLoss[0m : 2.84097
[1mStep[0m  [99/339], [94mLoss[0m : 2.45225
[1mStep[0m  [132/339], [94mLoss[0m : 2.07260
[1mStep[0m  [165/339], [94mLoss[0m : 2.57336
[1mStep[0m  [198/339], [94mLoss[0m : 2.64146
[1mStep[0m  [231/339], [94mLoss[0m : 2.37782
[1mStep[0m  [264/339], [94mLoss[0m : 2.15236
[1mStep[0m  [297/339], [94mLoss[0m : 2.30249
[1mStep[0m  [330/339], [94mLoss[0m : 2.03220

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56709
[1mStep[0m  [33/339], [94mLoss[0m : 2.03038
[1mStep[0m  [66/339], [94mLoss[0m : 2.09750
[1mStep[0m  [99/339], [94mLoss[0m : 2.56852
[1mStep[0m  [132/339], [94mLoss[0m : 2.78957
[1mStep[0m  [165/339], [94mLoss[0m : 2.39567
[1mStep[0m  [198/339], [94mLoss[0m : 2.33410
[1mStep[0m  [231/339], [94mLoss[0m : 2.09333
[1mStep[0m  [264/339], [94mLoss[0m : 2.92314
[1mStep[0m  [297/339], [94mLoss[0m : 2.64576
[1mStep[0m  [330/339], [94mLoss[0m : 2.80112

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37531
[1mStep[0m  [33/339], [94mLoss[0m : 2.64394
[1mStep[0m  [66/339], [94mLoss[0m : 2.99769
[1mStep[0m  [99/339], [94mLoss[0m : 2.43266
[1mStep[0m  [132/339], [94mLoss[0m : 2.75058
[1mStep[0m  [165/339], [94mLoss[0m : 2.57358
[1mStep[0m  [198/339], [94mLoss[0m : 2.42612
[1mStep[0m  [231/339], [94mLoss[0m : 2.18650
[1mStep[0m  [264/339], [94mLoss[0m : 2.50398
[1mStep[0m  [297/339], [94mLoss[0m : 2.27998
[1mStep[0m  [330/339], [94mLoss[0m : 1.96246

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64426
[1mStep[0m  [33/339], [94mLoss[0m : 2.31961
[1mStep[0m  [66/339], [94mLoss[0m : 2.33360
[1mStep[0m  [99/339], [94mLoss[0m : 2.88924
[1mStep[0m  [132/339], [94mLoss[0m : 2.62883
[1mStep[0m  [165/339], [94mLoss[0m : 2.05172
[1mStep[0m  [198/339], [94mLoss[0m : 2.00241
[1mStep[0m  [231/339], [94mLoss[0m : 2.26112
[1mStep[0m  [264/339], [94mLoss[0m : 2.34439
[1mStep[0m  [297/339], [94mLoss[0m : 2.39996
[1mStep[0m  [330/339], [94mLoss[0m : 2.66075

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69366
[1mStep[0m  [33/339], [94mLoss[0m : 2.45113
[1mStep[0m  [66/339], [94mLoss[0m : 2.62562
[1mStep[0m  [99/339], [94mLoss[0m : 2.04270
[1mStep[0m  [132/339], [94mLoss[0m : 2.76507
[1mStep[0m  [165/339], [94mLoss[0m : 2.77757
[1mStep[0m  [198/339], [94mLoss[0m : 2.42248
[1mStep[0m  [231/339], [94mLoss[0m : 2.17209
[1mStep[0m  [264/339], [94mLoss[0m : 1.90278
[1mStep[0m  [297/339], [94mLoss[0m : 2.56018
[1mStep[0m  [330/339], [94mLoss[0m : 2.21474

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94161
[1mStep[0m  [33/339], [94mLoss[0m : 2.28408
[1mStep[0m  [66/339], [94mLoss[0m : 2.85539
[1mStep[0m  [99/339], [94mLoss[0m : 2.57466
[1mStep[0m  [132/339], [94mLoss[0m : 2.85838
[1mStep[0m  [165/339], [94mLoss[0m : 2.31297
[1mStep[0m  [198/339], [94mLoss[0m : 2.17186
[1mStep[0m  [231/339], [94mLoss[0m : 2.67639
[1mStep[0m  [264/339], [94mLoss[0m : 2.44383
[1mStep[0m  [297/339], [94mLoss[0m : 1.80637
[1mStep[0m  [330/339], [94mLoss[0m : 2.21217

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27063
[1mStep[0m  [33/339], [94mLoss[0m : 1.93389
[1mStep[0m  [66/339], [94mLoss[0m : 2.49434
[1mStep[0m  [99/339], [94mLoss[0m : 2.25111
[1mStep[0m  [132/339], [94mLoss[0m : 2.49148
[1mStep[0m  [165/339], [94mLoss[0m : 1.72210
[1mStep[0m  [198/339], [94mLoss[0m : 2.48971
[1mStep[0m  [231/339], [94mLoss[0m : 1.76057
[1mStep[0m  [264/339], [94mLoss[0m : 2.20822
[1mStep[0m  [297/339], [94mLoss[0m : 3.12821
[1mStep[0m  [330/339], [94mLoss[0m : 2.12464

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73212
[1mStep[0m  [33/339], [94mLoss[0m : 2.41389
[1mStep[0m  [66/339], [94mLoss[0m : 2.32157
[1mStep[0m  [99/339], [94mLoss[0m : 2.18783
[1mStep[0m  [132/339], [94mLoss[0m : 2.09010
[1mStep[0m  [165/339], [94mLoss[0m : 2.67679
[1mStep[0m  [198/339], [94mLoss[0m : 2.32032
[1mStep[0m  [231/339], [94mLoss[0m : 2.42322
[1mStep[0m  [264/339], [94mLoss[0m : 2.20724
[1mStep[0m  [297/339], [94mLoss[0m : 2.50154
[1mStep[0m  [330/339], [94mLoss[0m : 2.31128

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07613
[1mStep[0m  [33/339], [94mLoss[0m : 2.54578
[1mStep[0m  [66/339], [94mLoss[0m : 2.88371
[1mStep[0m  [99/339], [94mLoss[0m : 1.97337
[1mStep[0m  [132/339], [94mLoss[0m : 1.95391
[1mStep[0m  [165/339], [94mLoss[0m : 2.56180
[1mStep[0m  [198/339], [94mLoss[0m : 2.19878
[1mStep[0m  [231/339], [94mLoss[0m : 2.67915
[1mStep[0m  [264/339], [94mLoss[0m : 1.99468
[1mStep[0m  [297/339], [94mLoss[0m : 2.45476
[1mStep[0m  [330/339], [94mLoss[0m : 2.88860

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66758
[1mStep[0m  [33/339], [94mLoss[0m : 2.68642
[1mStep[0m  [66/339], [94mLoss[0m : 2.08150
[1mStep[0m  [99/339], [94mLoss[0m : 1.87035
[1mStep[0m  [132/339], [94mLoss[0m : 2.04106
[1mStep[0m  [165/339], [94mLoss[0m : 2.67408
[1mStep[0m  [198/339], [94mLoss[0m : 2.17035
[1mStep[0m  [231/339], [94mLoss[0m : 2.21701
[1mStep[0m  [264/339], [94mLoss[0m : 1.78285
[1mStep[0m  [297/339], [94mLoss[0m : 2.69874
[1mStep[0m  [330/339], [94mLoss[0m : 2.15498

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15350
[1mStep[0m  [33/339], [94mLoss[0m : 1.84945
[1mStep[0m  [66/339], [94mLoss[0m : 2.85862
[1mStep[0m  [99/339], [94mLoss[0m : 2.64386
[1mStep[0m  [132/339], [94mLoss[0m : 2.30856
[1mStep[0m  [165/339], [94mLoss[0m : 2.24753
[1mStep[0m  [198/339], [94mLoss[0m : 2.62712
[1mStep[0m  [231/339], [94mLoss[0m : 1.81949
[1mStep[0m  [264/339], [94mLoss[0m : 2.71022
[1mStep[0m  [297/339], [94mLoss[0m : 2.28012
[1mStep[0m  [330/339], [94mLoss[0m : 2.39992

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38386
[1mStep[0m  [33/339], [94mLoss[0m : 2.37972
[1mStep[0m  [66/339], [94mLoss[0m : 2.72053
[1mStep[0m  [99/339], [94mLoss[0m : 2.97036
[1mStep[0m  [132/339], [94mLoss[0m : 2.85861
[1mStep[0m  [165/339], [94mLoss[0m : 2.00898
[1mStep[0m  [198/339], [94mLoss[0m : 2.20572
[1mStep[0m  [231/339], [94mLoss[0m : 3.07356
[1mStep[0m  [264/339], [94mLoss[0m : 2.15755
[1mStep[0m  [297/339], [94mLoss[0m : 2.78053
[1mStep[0m  [330/339], [94mLoss[0m : 2.46248

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13847
[1mStep[0m  [33/339], [94mLoss[0m : 2.18183
[1mStep[0m  [66/339], [94mLoss[0m : 2.13872
[1mStep[0m  [99/339], [94mLoss[0m : 2.58590
[1mStep[0m  [132/339], [94mLoss[0m : 2.20655
[1mStep[0m  [165/339], [94mLoss[0m : 1.40014
[1mStep[0m  [198/339], [94mLoss[0m : 2.42725
[1mStep[0m  [231/339], [94mLoss[0m : 2.85048
[1mStep[0m  [264/339], [94mLoss[0m : 2.32182
[1mStep[0m  [297/339], [94mLoss[0m : 3.09758
[1mStep[0m  [330/339], [94mLoss[0m : 2.45040

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78688
[1mStep[0m  [33/339], [94mLoss[0m : 2.94678
[1mStep[0m  [66/339], [94mLoss[0m : 2.69934
[1mStep[0m  [99/339], [94mLoss[0m : 2.35018
[1mStep[0m  [132/339], [94mLoss[0m : 2.37957
[1mStep[0m  [165/339], [94mLoss[0m : 2.14453
[1mStep[0m  [198/339], [94mLoss[0m : 2.84990
[1mStep[0m  [231/339], [94mLoss[0m : 2.15739
[1mStep[0m  [264/339], [94mLoss[0m : 2.06120
[1mStep[0m  [297/339], [94mLoss[0m : 2.83746
[1mStep[0m  [330/339], [94mLoss[0m : 1.77802

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39780
[1mStep[0m  [33/339], [94mLoss[0m : 1.96184
[1mStep[0m  [66/339], [94mLoss[0m : 2.15993
[1mStep[0m  [99/339], [94mLoss[0m : 2.71576
[1mStep[0m  [132/339], [94mLoss[0m : 2.36273
[1mStep[0m  [165/339], [94mLoss[0m : 2.53261
[1mStep[0m  [198/339], [94mLoss[0m : 1.97802
[1mStep[0m  [231/339], [94mLoss[0m : 1.79259
[1mStep[0m  [264/339], [94mLoss[0m : 2.75443
[1mStep[0m  [297/339], [94mLoss[0m : 2.13205
[1mStep[0m  [330/339], [94mLoss[0m : 2.16306

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29400
[1mStep[0m  [33/339], [94mLoss[0m : 3.41402
[1mStep[0m  [66/339], [94mLoss[0m : 1.98966
[1mStep[0m  [99/339], [94mLoss[0m : 2.10557
[1mStep[0m  [132/339], [94mLoss[0m : 2.33178
[1mStep[0m  [165/339], [94mLoss[0m : 2.38112
[1mStep[0m  [198/339], [94mLoss[0m : 2.51206
[1mStep[0m  [231/339], [94mLoss[0m : 2.44383
[1mStep[0m  [264/339], [94mLoss[0m : 1.66246
[1mStep[0m  [297/339], [94mLoss[0m : 2.50571
[1mStep[0m  [330/339], [94mLoss[0m : 2.23687

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07891
[1mStep[0m  [33/339], [94mLoss[0m : 2.22037
[1mStep[0m  [66/339], [94mLoss[0m : 2.48468
[1mStep[0m  [99/339], [94mLoss[0m : 2.07635
[1mStep[0m  [132/339], [94mLoss[0m : 2.41580
[1mStep[0m  [165/339], [94mLoss[0m : 2.20795
[1mStep[0m  [198/339], [94mLoss[0m : 2.12085
[1mStep[0m  [231/339], [94mLoss[0m : 2.55213
[1mStep[0m  [264/339], [94mLoss[0m : 2.54212
[1mStep[0m  [297/339], [94mLoss[0m : 1.94585
[1mStep[0m  [330/339], [94mLoss[0m : 2.87241

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53237
[1mStep[0m  [33/339], [94mLoss[0m : 2.65765
[1mStep[0m  [66/339], [94mLoss[0m : 1.95744
[1mStep[0m  [99/339], [94mLoss[0m : 2.44721
[1mStep[0m  [132/339], [94mLoss[0m : 2.45419
[1mStep[0m  [165/339], [94mLoss[0m : 2.51905
[1mStep[0m  [198/339], [94mLoss[0m : 2.43511
[1mStep[0m  [231/339], [94mLoss[0m : 2.14510
[1mStep[0m  [264/339], [94mLoss[0m : 2.49578
[1mStep[0m  [297/339], [94mLoss[0m : 1.95699
[1mStep[0m  [330/339], [94mLoss[0m : 2.34634

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47028
[1mStep[0m  [33/339], [94mLoss[0m : 2.24154
[1mStep[0m  [66/339], [94mLoss[0m : 2.29641
[1mStep[0m  [99/339], [94mLoss[0m : 1.89112
[1mStep[0m  [132/339], [94mLoss[0m : 2.18790
[1mStep[0m  [165/339], [94mLoss[0m : 2.42662
[1mStep[0m  [198/339], [94mLoss[0m : 2.31209
[1mStep[0m  [231/339], [94mLoss[0m : 2.65614
[1mStep[0m  [264/339], [94mLoss[0m : 2.58356
[1mStep[0m  [297/339], [94mLoss[0m : 3.01043
[1mStep[0m  [330/339], [94mLoss[0m : 1.98839

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40188
[1mStep[0m  [33/339], [94mLoss[0m : 2.88256
[1mStep[0m  [66/339], [94mLoss[0m : 2.43097
[1mStep[0m  [99/339], [94mLoss[0m : 2.44599
[1mStep[0m  [132/339], [94mLoss[0m : 2.22910
[1mStep[0m  [165/339], [94mLoss[0m : 2.40545
[1mStep[0m  [198/339], [94mLoss[0m : 2.80680
[1mStep[0m  [231/339], [94mLoss[0m : 2.56701
[1mStep[0m  [264/339], [94mLoss[0m : 2.30476
[1mStep[0m  [297/339], [94mLoss[0m : 1.73347
[1mStep[0m  [330/339], [94mLoss[0m : 2.01587

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26393
[1mStep[0m  [33/339], [94mLoss[0m : 3.19132
[1mStep[0m  [66/339], [94mLoss[0m : 2.26927
[1mStep[0m  [99/339], [94mLoss[0m : 1.66772
[1mStep[0m  [132/339], [94mLoss[0m : 2.83526
[1mStep[0m  [165/339], [94mLoss[0m : 1.84384
[1mStep[0m  [198/339], [94mLoss[0m : 2.33371
[1mStep[0m  [231/339], [94mLoss[0m : 2.76326
[1mStep[0m  [264/339], [94mLoss[0m : 1.95965
[1mStep[0m  [297/339], [94mLoss[0m : 2.24635
[1mStep[0m  [330/339], [94mLoss[0m : 2.40196

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78957
[1mStep[0m  [33/339], [94mLoss[0m : 2.98871
[1mStep[0m  [66/339], [94mLoss[0m : 2.33662
[1mStep[0m  [99/339], [94mLoss[0m : 2.26510
[1mStep[0m  [132/339], [94mLoss[0m : 2.63778
[1mStep[0m  [165/339], [94mLoss[0m : 3.21070
[1mStep[0m  [198/339], [94mLoss[0m : 2.32675
[1mStep[0m  [231/339], [94mLoss[0m : 2.17981
[1mStep[0m  [264/339], [94mLoss[0m : 2.13053
[1mStep[0m  [297/339], [94mLoss[0m : 2.66322
[1mStep[0m  [330/339], [94mLoss[0m : 2.32098

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78576
[1mStep[0m  [33/339], [94mLoss[0m : 2.58850
[1mStep[0m  [66/339], [94mLoss[0m : 1.94833
[1mStep[0m  [99/339], [94mLoss[0m : 2.25701
[1mStep[0m  [132/339], [94mLoss[0m : 2.47189
[1mStep[0m  [165/339], [94mLoss[0m : 2.26787
[1mStep[0m  [198/339], [94mLoss[0m : 1.78223
[1mStep[0m  [231/339], [94mLoss[0m : 1.92892
[1mStep[0m  [264/339], [94mLoss[0m : 2.31836
[1mStep[0m  [297/339], [94mLoss[0m : 2.23897
[1mStep[0m  [330/339], [94mLoss[0m : 1.88209

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38938
[1mStep[0m  [33/339], [94mLoss[0m : 2.57044
[1mStep[0m  [66/339], [94mLoss[0m : 2.73794
[1mStep[0m  [99/339], [94mLoss[0m : 2.49944
[1mStep[0m  [132/339], [94mLoss[0m : 2.28453
[1mStep[0m  [165/339], [94mLoss[0m : 2.13133
[1mStep[0m  [198/339], [94mLoss[0m : 2.12283
[1mStep[0m  [231/339], [94mLoss[0m : 3.39587
[1mStep[0m  [264/339], [94mLoss[0m : 2.13324
[1mStep[0m  [297/339], [94mLoss[0m : 2.38363
[1mStep[0m  [330/339], [94mLoss[0m : 2.25111

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36031
[1mStep[0m  [33/339], [94mLoss[0m : 2.88800
[1mStep[0m  [66/339], [94mLoss[0m : 2.80636
[1mStep[0m  [99/339], [94mLoss[0m : 2.67279
[1mStep[0m  [132/339], [94mLoss[0m : 2.19099
[1mStep[0m  [165/339], [94mLoss[0m : 2.84892
[1mStep[0m  [198/339], [94mLoss[0m : 2.76750
[1mStep[0m  [231/339], [94mLoss[0m : 2.47605
[1mStep[0m  [264/339], [94mLoss[0m : 2.50570
[1mStep[0m  [297/339], [94mLoss[0m : 2.12543
[1mStep[0m  [330/339], [94mLoss[0m : 2.32118

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85847
[1mStep[0m  [33/339], [94mLoss[0m : 2.92025
[1mStep[0m  [66/339], [94mLoss[0m : 2.55122
[1mStep[0m  [99/339], [94mLoss[0m : 2.37192
[1mStep[0m  [132/339], [94mLoss[0m : 2.96496
[1mStep[0m  [165/339], [94mLoss[0m : 2.47649
[1mStep[0m  [198/339], [94mLoss[0m : 2.10417
[1mStep[0m  [231/339], [94mLoss[0m : 2.80631
[1mStep[0m  [264/339], [94mLoss[0m : 2.01921
[1mStep[0m  [297/339], [94mLoss[0m : 2.56181
[1mStep[0m  [330/339], [94mLoss[0m : 2.50191

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11829
[1mStep[0m  [33/339], [94mLoss[0m : 3.04601
[1mStep[0m  [66/339], [94mLoss[0m : 2.61000
[1mStep[0m  [99/339], [94mLoss[0m : 2.03343
[1mStep[0m  [132/339], [94mLoss[0m : 1.77200
[1mStep[0m  [165/339], [94mLoss[0m : 2.09669
[1mStep[0m  [198/339], [94mLoss[0m : 2.83880
[1mStep[0m  [231/339], [94mLoss[0m : 3.03442
[1mStep[0m  [264/339], [94mLoss[0m : 2.40450
[1mStep[0m  [297/339], [94mLoss[0m : 2.36798
[1mStep[0m  [330/339], [94mLoss[0m : 2.45441

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.355, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.3391778658976596
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.25304
[1mStep[0m  [33/339], [94mLoss[0m : 2.06814
[1mStep[0m  [66/339], [94mLoss[0m : 2.69588
[1mStep[0m  [99/339], [94mLoss[0m : 2.18416
[1mStep[0m  [132/339], [94mLoss[0m : 2.14120
[1mStep[0m  [165/339], [94mLoss[0m : 2.45957
[1mStep[0m  [198/339], [94mLoss[0m : 3.09576
[1mStep[0m  [231/339], [94mLoss[0m : 2.15657
[1mStep[0m  [264/339], [94mLoss[0m : 3.20011
[1mStep[0m  [297/339], [94mLoss[0m : 2.73138
[1mStep[0m  [330/339], [94mLoss[0m : 2.88179

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52043
[1mStep[0m  [33/339], [94mLoss[0m : 1.86617
[1mStep[0m  [66/339], [94mLoss[0m : 3.04095
[1mStep[0m  [99/339], [94mLoss[0m : 2.63918
[1mStep[0m  [132/339], [94mLoss[0m : 2.27631
[1mStep[0m  [165/339], [94mLoss[0m : 2.14049
[1mStep[0m  [198/339], [94mLoss[0m : 3.32513
[1mStep[0m  [231/339], [94mLoss[0m : 1.58721
[1mStep[0m  [264/339], [94mLoss[0m : 2.49268
[1mStep[0m  [297/339], [94mLoss[0m : 1.97323
[1mStep[0m  [330/339], [94mLoss[0m : 1.96502

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37455
[1mStep[0m  [33/339], [94mLoss[0m : 2.47554
[1mStep[0m  [66/339], [94mLoss[0m : 2.57160
[1mStep[0m  [99/339], [94mLoss[0m : 2.03823
[1mStep[0m  [132/339], [94mLoss[0m : 2.26820
[1mStep[0m  [165/339], [94mLoss[0m : 2.17447
[1mStep[0m  [198/339], [94mLoss[0m : 2.92889
[1mStep[0m  [231/339], [94mLoss[0m : 2.18060
[1mStep[0m  [264/339], [94mLoss[0m : 1.80677
[1mStep[0m  [297/339], [94mLoss[0m : 2.45549
[1mStep[0m  [330/339], [94mLoss[0m : 2.31275

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14762
[1mStep[0m  [33/339], [94mLoss[0m : 2.64816
[1mStep[0m  [66/339], [94mLoss[0m : 2.54253
[1mStep[0m  [99/339], [94mLoss[0m : 1.69058
[1mStep[0m  [132/339], [94mLoss[0m : 2.75258
[1mStep[0m  [165/339], [94mLoss[0m : 1.87742
[1mStep[0m  [198/339], [94mLoss[0m : 2.39004
[1mStep[0m  [231/339], [94mLoss[0m : 1.83565
[1mStep[0m  [264/339], [94mLoss[0m : 2.54099
[1mStep[0m  [297/339], [94mLoss[0m : 1.53148
[1mStep[0m  [330/339], [94mLoss[0m : 2.75195

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63987
[1mStep[0m  [33/339], [94mLoss[0m : 2.45404
[1mStep[0m  [66/339], [94mLoss[0m : 2.04777
[1mStep[0m  [99/339], [94mLoss[0m : 2.09108
[1mStep[0m  [132/339], [94mLoss[0m : 2.45636
[1mStep[0m  [165/339], [94mLoss[0m : 1.87154
[1mStep[0m  [198/339], [94mLoss[0m : 2.61722
[1mStep[0m  [231/339], [94mLoss[0m : 2.19145
[1mStep[0m  [264/339], [94mLoss[0m : 1.65615
[1mStep[0m  [297/339], [94mLoss[0m : 1.66208
[1mStep[0m  [330/339], [94mLoss[0m : 2.73993

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85570
[1mStep[0m  [33/339], [94mLoss[0m : 2.54591
[1mStep[0m  [66/339], [94mLoss[0m : 1.37942
[1mStep[0m  [99/339], [94mLoss[0m : 2.17742
[1mStep[0m  [132/339], [94mLoss[0m : 1.80426
[1mStep[0m  [165/339], [94mLoss[0m : 2.29945
[1mStep[0m  [198/339], [94mLoss[0m : 1.45373
[1mStep[0m  [231/339], [94mLoss[0m : 1.99494
[1mStep[0m  [264/339], [94mLoss[0m : 1.86468
[1mStep[0m  [297/339], [94mLoss[0m : 1.98460
[1mStep[0m  [330/339], [94mLoss[0m : 1.99679

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67728
[1mStep[0m  [33/339], [94mLoss[0m : 2.07205
[1mStep[0m  [66/339], [94mLoss[0m : 2.27483
[1mStep[0m  [99/339], [94mLoss[0m : 1.75221
[1mStep[0m  [132/339], [94mLoss[0m : 1.80565
[1mStep[0m  [165/339], [94mLoss[0m : 2.00894
[1mStep[0m  [198/339], [94mLoss[0m : 1.66309
[1mStep[0m  [231/339], [94mLoss[0m : 1.81680
[1mStep[0m  [264/339], [94mLoss[0m : 2.12365
[1mStep[0m  [297/339], [94mLoss[0m : 2.18503
[1mStep[0m  [330/339], [94mLoss[0m : 1.19697

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60779
[1mStep[0m  [33/339], [94mLoss[0m : 1.69386
[1mStep[0m  [66/339], [94mLoss[0m : 2.04267
[1mStep[0m  [99/339], [94mLoss[0m : 1.58617
[1mStep[0m  [132/339], [94mLoss[0m : 2.32135
[1mStep[0m  [165/339], [94mLoss[0m : 1.73250
[1mStep[0m  [198/339], [94mLoss[0m : 1.83762
[1mStep[0m  [231/339], [94mLoss[0m : 2.09909
[1mStep[0m  [264/339], [94mLoss[0m : 2.09190
[1mStep[0m  [297/339], [94mLoss[0m : 2.17611
[1mStep[0m  [330/339], [94mLoss[0m : 2.20955

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12351
[1mStep[0m  [33/339], [94mLoss[0m : 2.06462
[1mStep[0m  [66/339], [94mLoss[0m : 1.73968
[1mStep[0m  [99/339], [94mLoss[0m : 2.23839
[1mStep[0m  [132/339], [94mLoss[0m : 2.17694
[1mStep[0m  [165/339], [94mLoss[0m : 1.64881
[1mStep[0m  [198/339], [94mLoss[0m : 1.84630
[1mStep[0m  [231/339], [94mLoss[0m : 1.44449
[1mStep[0m  [264/339], [94mLoss[0m : 1.74279
[1mStep[0m  [297/339], [94mLoss[0m : 1.54030
[1mStep[0m  [330/339], [94mLoss[0m : 1.67443

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30755
[1mStep[0m  [33/339], [94mLoss[0m : 1.62771
[1mStep[0m  [66/339], [94mLoss[0m : 2.35206
[1mStep[0m  [99/339], [94mLoss[0m : 1.79508
[1mStep[0m  [132/339], [94mLoss[0m : 1.62003
[1mStep[0m  [165/339], [94mLoss[0m : 1.74998
[1mStep[0m  [198/339], [94mLoss[0m : 1.49161
[1mStep[0m  [231/339], [94mLoss[0m : 2.14951
[1mStep[0m  [264/339], [94mLoss[0m : 1.61557
[1mStep[0m  [297/339], [94mLoss[0m : 1.62781
[1mStep[0m  [330/339], [94mLoss[0m : 1.54681

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94083
[1mStep[0m  [33/339], [94mLoss[0m : 1.36510
[1mStep[0m  [66/339], [94mLoss[0m : 1.99531
[1mStep[0m  [99/339], [94mLoss[0m : 1.77689
[1mStep[0m  [132/339], [94mLoss[0m : 2.78049
[1mStep[0m  [165/339], [94mLoss[0m : 1.91939
[1mStep[0m  [198/339], [94mLoss[0m : 2.18011
[1mStep[0m  [231/339], [94mLoss[0m : 2.16780
[1mStep[0m  [264/339], [94mLoss[0m : 1.78502
[1mStep[0m  [297/339], [94mLoss[0m : 1.84100
[1mStep[0m  [330/339], [94mLoss[0m : 1.60375

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.554, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66716
[1mStep[0m  [33/339], [94mLoss[0m : 1.94585
[1mStep[0m  [66/339], [94mLoss[0m : 1.70372
[1mStep[0m  [99/339], [94mLoss[0m : 2.12280
[1mStep[0m  [132/339], [94mLoss[0m : 1.44175
[1mStep[0m  [165/339], [94mLoss[0m : 2.10602
[1mStep[0m  [198/339], [94mLoss[0m : 1.57793
[1mStep[0m  [231/339], [94mLoss[0m : 1.60414
[1mStep[0m  [264/339], [94mLoss[0m : 1.33344
[1mStep[0m  [297/339], [94mLoss[0m : 1.67694
[1mStep[0m  [330/339], [94mLoss[0m : 1.85162

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.581, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59890
[1mStep[0m  [33/339], [94mLoss[0m : 1.52531
[1mStep[0m  [66/339], [94mLoss[0m : 1.62093
[1mStep[0m  [99/339], [94mLoss[0m : 1.76373
[1mStep[0m  [132/339], [94mLoss[0m : 2.19480
[1mStep[0m  [165/339], [94mLoss[0m : 2.11230
[1mStep[0m  [198/339], [94mLoss[0m : 2.13869
[1mStep[0m  [231/339], [94mLoss[0m : 2.06030
[1mStep[0m  [264/339], [94mLoss[0m : 1.73773
[1mStep[0m  [297/339], [94mLoss[0m : 1.44438
[1mStep[0m  [330/339], [94mLoss[0m : 1.66438

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92690
[1mStep[0m  [33/339], [94mLoss[0m : 1.48422
[1mStep[0m  [66/339], [94mLoss[0m : 1.65747
[1mStep[0m  [99/339], [94mLoss[0m : 1.32090
[1mStep[0m  [132/339], [94mLoss[0m : 1.55291
[1mStep[0m  [165/339], [94mLoss[0m : 1.41008
[1mStep[0m  [198/339], [94mLoss[0m : 1.61597
[1mStep[0m  [231/339], [94mLoss[0m : 1.91651
[1mStep[0m  [264/339], [94mLoss[0m : 1.57497
[1mStep[0m  [297/339], [94mLoss[0m : 1.67094
[1mStep[0m  [330/339], [94mLoss[0m : 1.82316

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40745
[1mStep[0m  [33/339], [94mLoss[0m : 1.44725
[1mStep[0m  [66/339], [94mLoss[0m : 1.83319
[1mStep[0m  [99/339], [94mLoss[0m : 1.91534
[1mStep[0m  [132/339], [94mLoss[0m : 1.63911
[1mStep[0m  [165/339], [94mLoss[0m : 1.89664
[1mStep[0m  [198/339], [94mLoss[0m : 1.34349
[1mStep[0m  [231/339], [94mLoss[0m : 1.24101
[1mStep[0m  [264/339], [94mLoss[0m : 1.22640
[1mStep[0m  [297/339], [94mLoss[0m : 1.38957
[1mStep[0m  [330/339], [94mLoss[0m : 2.08330

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.724, [92mTest[0m: 2.544, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38302
[1mStep[0m  [33/339], [94mLoss[0m : 1.57563
[1mStep[0m  [66/339], [94mLoss[0m : 1.39923
[1mStep[0m  [99/339], [94mLoss[0m : 1.25601
[1mStep[0m  [132/339], [94mLoss[0m : 1.65980
[1mStep[0m  [165/339], [94mLoss[0m : 1.59290
[1mStep[0m  [198/339], [94mLoss[0m : 1.87883
[1mStep[0m  [231/339], [94mLoss[0m : 1.72343
[1mStep[0m  [264/339], [94mLoss[0m : 1.49829
[1mStep[0m  [297/339], [94mLoss[0m : 1.98370
[1mStep[0m  [330/339], [94mLoss[0m : 1.54708

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.570, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83968
[1mStep[0m  [33/339], [94mLoss[0m : 2.00764
[1mStep[0m  [66/339], [94mLoss[0m : 1.84769
[1mStep[0m  [99/339], [94mLoss[0m : 1.88094
[1mStep[0m  [132/339], [94mLoss[0m : 1.94278
[1mStep[0m  [165/339], [94mLoss[0m : 1.55274
[1mStep[0m  [198/339], [94mLoss[0m : 1.56239
[1mStep[0m  [231/339], [94mLoss[0m : 1.78523
[1mStep[0m  [264/339], [94mLoss[0m : 1.55394
[1mStep[0m  [297/339], [94mLoss[0m : 2.28276
[1mStep[0m  [330/339], [94mLoss[0m : 1.54725

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.685, [92mTest[0m: 2.480, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43475
[1mStep[0m  [33/339], [94mLoss[0m : 1.12941
[1mStep[0m  [66/339], [94mLoss[0m : 1.25425
[1mStep[0m  [99/339], [94mLoss[0m : 1.79788
[1mStep[0m  [132/339], [94mLoss[0m : 1.71935
[1mStep[0m  [165/339], [94mLoss[0m : 1.59331
[1mStep[0m  [198/339], [94mLoss[0m : 1.88096
[1mStep[0m  [231/339], [94mLoss[0m : 1.65719
[1mStep[0m  [264/339], [94mLoss[0m : 1.88629
[1mStep[0m  [297/339], [94mLoss[0m : 1.64708
[1mStep[0m  [330/339], [94mLoss[0m : 1.61522

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76096
[1mStep[0m  [33/339], [94mLoss[0m : 1.71054
[1mStep[0m  [66/339], [94mLoss[0m : 1.43837
[1mStep[0m  [99/339], [94mLoss[0m : 1.15987
[1mStep[0m  [132/339], [94mLoss[0m : 1.42209
[1mStep[0m  [165/339], [94mLoss[0m : 1.35439
[1mStep[0m  [198/339], [94mLoss[0m : 1.65531
[1mStep[0m  [231/339], [94mLoss[0m : 1.40160
[1mStep[0m  [264/339], [94mLoss[0m : 1.59064
[1mStep[0m  [297/339], [94mLoss[0m : 2.06585
[1mStep[0m  [330/339], [94mLoss[0m : 1.57436

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40730
[1mStep[0m  [33/339], [94mLoss[0m : 1.22371
[1mStep[0m  [66/339], [94mLoss[0m : 1.74878
[1mStep[0m  [99/339], [94mLoss[0m : 1.53245
[1mStep[0m  [132/339], [94mLoss[0m : 1.68948
[1mStep[0m  [165/339], [94mLoss[0m : 1.39101
[1mStep[0m  [198/339], [94mLoss[0m : 1.69441
[1mStep[0m  [231/339], [94mLoss[0m : 1.21035
[1mStep[0m  [264/339], [94mLoss[0m : 2.55684
[1mStep[0m  [297/339], [94mLoss[0m : 1.68474
[1mStep[0m  [330/339], [94mLoss[0m : 1.55324

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.557, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.28918
[1mStep[0m  [33/339], [94mLoss[0m : 1.63268
[1mStep[0m  [66/339], [94mLoss[0m : 1.51040
[1mStep[0m  [99/339], [94mLoss[0m : 1.66624
[1mStep[0m  [132/339], [94mLoss[0m : 1.51671
[1mStep[0m  [165/339], [94mLoss[0m : 2.24476
[1mStep[0m  [198/339], [94mLoss[0m : 1.51320
[1mStep[0m  [231/339], [94mLoss[0m : 1.41026
[1mStep[0m  [264/339], [94mLoss[0m : 1.37689
[1mStep[0m  [297/339], [94mLoss[0m : 1.48374
[1mStep[0m  [330/339], [94mLoss[0m : 1.47908

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.484, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.14209
[1mStep[0m  [33/339], [94mLoss[0m : 1.67896
[1mStep[0m  [66/339], [94mLoss[0m : 1.39601
[1mStep[0m  [99/339], [94mLoss[0m : 2.14558
[1mStep[0m  [132/339], [94mLoss[0m : 1.18386
[1mStep[0m  [165/339], [94mLoss[0m : 1.67021
[1mStep[0m  [198/339], [94mLoss[0m : 1.43516
[1mStep[0m  [231/339], [94mLoss[0m : 1.65712
[1mStep[0m  [264/339], [94mLoss[0m : 1.61434
[1mStep[0m  [297/339], [94mLoss[0m : 1.62804
[1mStep[0m  [330/339], [94mLoss[0m : 1.61007

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57498
[1mStep[0m  [33/339], [94mLoss[0m : 1.37644
[1mStep[0m  [66/339], [94mLoss[0m : 1.65320
[1mStep[0m  [99/339], [94mLoss[0m : 1.28365
[1mStep[0m  [132/339], [94mLoss[0m : 1.61305
[1mStep[0m  [165/339], [94mLoss[0m : 1.55511
[1mStep[0m  [198/339], [94mLoss[0m : 1.15950
[1mStep[0m  [231/339], [94mLoss[0m : 1.51205
[1mStep[0m  [264/339], [94mLoss[0m : 1.77925
[1mStep[0m  [297/339], [94mLoss[0m : 1.34340
[1mStep[0m  [330/339], [94mLoss[0m : 1.64055

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.25014
[1mStep[0m  [33/339], [94mLoss[0m : 1.27801
[1mStep[0m  [66/339], [94mLoss[0m : 1.51827
[1mStep[0m  [99/339], [94mLoss[0m : 1.49894
[1mStep[0m  [132/339], [94mLoss[0m : 1.66827
[1mStep[0m  [165/339], [94mLoss[0m : 1.78411
[1mStep[0m  [198/339], [94mLoss[0m : 1.63731
[1mStep[0m  [231/339], [94mLoss[0m : 1.40728
[1mStep[0m  [264/339], [94mLoss[0m : 1.25230
[1mStep[0m  [297/339], [94mLoss[0m : 1.36908
[1mStep[0m  [330/339], [94mLoss[0m : 1.35180

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.515, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66153
[1mStep[0m  [33/339], [94mLoss[0m : 1.13534
[1mStep[0m  [66/339], [94mLoss[0m : 1.33614
[1mStep[0m  [99/339], [94mLoss[0m : 1.49386
[1mStep[0m  [132/339], [94mLoss[0m : 1.33193
[1mStep[0m  [165/339], [94mLoss[0m : 1.54237
[1mStep[0m  [198/339], [94mLoss[0m : 1.26908
[1mStep[0m  [231/339], [94mLoss[0m : 1.02019
[1mStep[0m  [264/339], [94mLoss[0m : 1.55846
[1mStep[0m  [297/339], [94mLoss[0m : 1.26566
[1mStep[0m  [330/339], [94mLoss[0m : 1.34021

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61344
[1mStep[0m  [33/339], [94mLoss[0m : 1.31209
[1mStep[0m  [66/339], [94mLoss[0m : 1.56407
[1mStep[0m  [99/339], [94mLoss[0m : 1.56231
[1mStep[0m  [132/339], [94mLoss[0m : 1.59979
[1mStep[0m  [165/339], [94mLoss[0m : 1.46037
[1mStep[0m  [198/339], [94mLoss[0m : 1.38841
[1mStep[0m  [231/339], [94mLoss[0m : 1.73306
[1mStep[0m  [264/339], [94mLoss[0m : 1.10414
[1mStep[0m  [297/339], [94mLoss[0m : 0.99834
[1mStep[0m  [330/339], [94mLoss[0m : 1.49240

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35818
[1mStep[0m  [33/339], [94mLoss[0m : 1.38268
[1mStep[0m  [66/339], [94mLoss[0m : 1.68479
[1mStep[0m  [99/339], [94mLoss[0m : 1.56309
[1mStep[0m  [132/339], [94mLoss[0m : 1.26823
[1mStep[0m  [165/339], [94mLoss[0m : 1.67489
[1mStep[0m  [198/339], [94mLoss[0m : 1.38512
[1mStep[0m  [231/339], [94mLoss[0m : 1.08160
[1mStep[0m  [264/339], [94mLoss[0m : 1.39840
[1mStep[0m  [297/339], [94mLoss[0m : 1.84351
[1mStep[0m  [330/339], [94mLoss[0m : 1.78901

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.448, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45699
[1mStep[0m  [33/339], [94mLoss[0m : 1.29652
[1mStep[0m  [66/339], [94mLoss[0m : 1.42696
[1mStep[0m  [99/339], [94mLoss[0m : 0.99969
[1mStep[0m  [132/339], [94mLoss[0m : 1.60747
[1mStep[0m  [165/339], [94mLoss[0m : 1.24928
[1mStep[0m  [198/339], [94mLoss[0m : 1.35964
[1mStep[0m  [231/339], [94mLoss[0m : 1.37783
[1mStep[0m  [264/339], [94mLoss[0m : 1.22496
[1mStep[0m  [297/339], [94mLoss[0m : 1.60119
[1mStep[0m  [330/339], [94mLoss[0m : 1.58085

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.437, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.468
====================================

Phase 2 - Evaluation MAE:  2.468402773933073
MAE score P1      2.339178
MAE score P2      2.468403
loss              1.437137
learning_rate     0.007525
batch_size              32
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 11.23434
[1mStep[0m  [33/339], [94mLoss[0m : 2.19098
[1mStep[0m  [66/339], [94mLoss[0m : 2.69864
[1mStep[0m  [99/339], [94mLoss[0m : 2.24783
[1mStep[0m  [132/339], [94mLoss[0m : 1.91894
[1mStep[0m  [165/339], [94mLoss[0m : 2.95692
[1mStep[0m  [198/339], [94mLoss[0m : 2.61496
[1mStep[0m  [231/339], [94mLoss[0m : 2.27844
[1mStep[0m  [264/339], [94mLoss[0m : 2.54678
[1mStep[0m  [297/339], [94mLoss[0m : 2.55346
[1mStep[0m  [330/339], [94mLoss[0m : 2.57784

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.868, [92mTest[0m: 10.980, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.22741
[1mStep[0m  [33/339], [94mLoss[0m : 1.95763
[1mStep[0m  [66/339], [94mLoss[0m : 3.19787
[1mStep[0m  [99/339], [94mLoss[0m : 2.11166
[1mStep[0m  [132/339], [94mLoss[0m : 2.30168
[1mStep[0m  [165/339], [94mLoss[0m : 1.79588
[1mStep[0m  [198/339], [94mLoss[0m : 2.36192
[1mStep[0m  [231/339], [94mLoss[0m : 2.42930
[1mStep[0m  [264/339], [94mLoss[0m : 2.46539
[1mStep[0m  [297/339], [94mLoss[0m : 2.37887
[1mStep[0m  [330/339], [94mLoss[0m : 2.18550

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24760
[1mStep[0m  [33/339], [94mLoss[0m : 2.45934
[1mStep[0m  [66/339], [94mLoss[0m : 2.63254
[1mStep[0m  [99/339], [94mLoss[0m : 2.01826
[1mStep[0m  [132/339], [94mLoss[0m : 2.90638
[1mStep[0m  [165/339], [94mLoss[0m : 2.30985
[1mStep[0m  [198/339], [94mLoss[0m : 1.70915
[1mStep[0m  [231/339], [94mLoss[0m : 2.35395
[1mStep[0m  [264/339], [94mLoss[0m : 2.35509
[1mStep[0m  [297/339], [94mLoss[0m : 2.63104
[1mStep[0m  [330/339], [94mLoss[0m : 2.33380

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95464
[1mStep[0m  [33/339], [94mLoss[0m : 2.18994
[1mStep[0m  [66/339], [94mLoss[0m : 3.45256
[1mStep[0m  [99/339], [94mLoss[0m : 2.76469
[1mStep[0m  [132/339], [94mLoss[0m : 2.67194
[1mStep[0m  [165/339], [94mLoss[0m : 2.34870
[1mStep[0m  [198/339], [94mLoss[0m : 2.71780
[1mStep[0m  [231/339], [94mLoss[0m : 2.42888
[1mStep[0m  [264/339], [94mLoss[0m : 2.29223
[1mStep[0m  [297/339], [94mLoss[0m : 3.08947
[1mStep[0m  [330/339], [94mLoss[0m : 2.08371

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78874
[1mStep[0m  [33/339], [94mLoss[0m : 2.15863
[1mStep[0m  [66/339], [94mLoss[0m : 2.47844
[1mStep[0m  [99/339], [94mLoss[0m : 2.83297
[1mStep[0m  [132/339], [94mLoss[0m : 2.49006
[1mStep[0m  [165/339], [94mLoss[0m : 2.77944
[1mStep[0m  [198/339], [94mLoss[0m : 1.98008
[1mStep[0m  [231/339], [94mLoss[0m : 3.02800
[1mStep[0m  [264/339], [94mLoss[0m : 1.80057
[1mStep[0m  [297/339], [94mLoss[0m : 2.26121
[1mStep[0m  [330/339], [94mLoss[0m : 2.76011

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.74166
[1mStep[0m  [33/339], [94mLoss[0m : 2.97146
[1mStep[0m  [66/339], [94mLoss[0m : 2.37332
[1mStep[0m  [99/339], [94mLoss[0m : 2.35656
[1mStep[0m  [132/339], [94mLoss[0m : 2.10349
[1mStep[0m  [165/339], [94mLoss[0m : 2.56566
[1mStep[0m  [198/339], [94mLoss[0m : 2.72734
[1mStep[0m  [231/339], [94mLoss[0m : 2.65561
[1mStep[0m  [264/339], [94mLoss[0m : 2.80934
[1mStep[0m  [297/339], [94mLoss[0m : 2.21746
[1mStep[0m  [330/339], [94mLoss[0m : 2.54573

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56377
[1mStep[0m  [33/339], [94mLoss[0m : 2.55993
[1mStep[0m  [66/339], [94mLoss[0m : 2.28048
[1mStep[0m  [99/339], [94mLoss[0m : 2.67145
[1mStep[0m  [132/339], [94mLoss[0m : 2.70363
[1mStep[0m  [165/339], [94mLoss[0m : 2.47009
[1mStep[0m  [198/339], [94mLoss[0m : 2.42241
[1mStep[0m  [231/339], [94mLoss[0m : 2.27339
[1mStep[0m  [264/339], [94mLoss[0m : 2.24462
[1mStep[0m  [297/339], [94mLoss[0m : 2.48089
[1mStep[0m  [330/339], [94mLoss[0m : 2.06644

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48982
[1mStep[0m  [33/339], [94mLoss[0m : 1.85711
[1mStep[0m  [66/339], [94mLoss[0m : 2.96447
[1mStep[0m  [99/339], [94mLoss[0m : 2.40127
[1mStep[0m  [132/339], [94mLoss[0m : 2.49365
[1mStep[0m  [165/339], [94mLoss[0m : 2.95924
[1mStep[0m  [198/339], [94mLoss[0m : 2.19666
[1mStep[0m  [231/339], [94mLoss[0m : 2.51301
[1mStep[0m  [264/339], [94mLoss[0m : 3.19091
[1mStep[0m  [297/339], [94mLoss[0m : 2.55072
[1mStep[0m  [330/339], [94mLoss[0m : 2.51166

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40921
[1mStep[0m  [33/339], [94mLoss[0m : 2.05833
[1mStep[0m  [66/339], [94mLoss[0m : 2.09156
[1mStep[0m  [99/339], [94mLoss[0m : 2.32894
[1mStep[0m  [132/339], [94mLoss[0m : 2.30050
[1mStep[0m  [165/339], [94mLoss[0m : 2.34095
[1mStep[0m  [198/339], [94mLoss[0m : 2.44630
[1mStep[0m  [231/339], [94mLoss[0m : 2.53321
[1mStep[0m  [264/339], [94mLoss[0m : 2.07807
[1mStep[0m  [297/339], [94mLoss[0m : 1.93597
[1mStep[0m  [330/339], [94mLoss[0m : 2.62041

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30914
[1mStep[0m  [33/339], [94mLoss[0m : 2.26546
[1mStep[0m  [66/339], [94mLoss[0m : 2.18118
[1mStep[0m  [99/339], [94mLoss[0m : 2.34557
[1mStep[0m  [132/339], [94mLoss[0m : 2.29778
[1mStep[0m  [165/339], [94mLoss[0m : 2.46749
[1mStep[0m  [198/339], [94mLoss[0m : 2.50803
[1mStep[0m  [231/339], [94mLoss[0m : 2.84852
[1mStep[0m  [264/339], [94mLoss[0m : 2.90672
[1mStep[0m  [297/339], [94mLoss[0m : 2.70363
[1mStep[0m  [330/339], [94mLoss[0m : 2.40574

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17497
[1mStep[0m  [33/339], [94mLoss[0m : 2.29600
[1mStep[0m  [66/339], [94mLoss[0m : 2.07886
[1mStep[0m  [99/339], [94mLoss[0m : 2.26976
[1mStep[0m  [132/339], [94mLoss[0m : 2.70363
[1mStep[0m  [165/339], [94mLoss[0m : 2.40311
[1mStep[0m  [198/339], [94mLoss[0m : 2.38066
[1mStep[0m  [231/339], [94mLoss[0m : 2.15399
[1mStep[0m  [264/339], [94mLoss[0m : 2.73203
[1mStep[0m  [297/339], [94mLoss[0m : 2.53108
[1mStep[0m  [330/339], [94mLoss[0m : 2.99897

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39833
[1mStep[0m  [33/339], [94mLoss[0m : 2.41465
[1mStep[0m  [66/339], [94mLoss[0m : 1.86860
[1mStep[0m  [99/339], [94mLoss[0m : 2.69866
[1mStep[0m  [132/339], [94mLoss[0m : 1.82073
[1mStep[0m  [165/339], [94mLoss[0m : 2.27341
[1mStep[0m  [198/339], [94mLoss[0m : 2.16996
[1mStep[0m  [231/339], [94mLoss[0m : 2.17733
[1mStep[0m  [264/339], [94mLoss[0m : 2.30725
[1mStep[0m  [297/339], [94mLoss[0m : 2.17143
[1mStep[0m  [330/339], [94mLoss[0m : 2.59086

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70578
[1mStep[0m  [33/339], [94mLoss[0m : 2.90111
[1mStep[0m  [66/339], [94mLoss[0m : 2.55987
[1mStep[0m  [99/339], [94mLoss[0m : 2.00614
[1mStep[0m  [132/339], [94mLoss[0m : 2.63528
[1mStep[0m  [165/339], [94mLoss[0m : 2.24935
[1mStep[0m  [198/339], [94mLoss[0m : 2.28522
[1mStep[0m  [231/339], [94mLoss[0m : 2.31163
[1mStep[0m  [264/339], [94mLoss[0m : 2.98203
[1mStep[0m  [297/339], [94mLoss[0m : 2.08428
[1mStep[0m  [330/339], [94mLoss[0m : 2.33209

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00531
[1mStep[0m  [33/339], [94mLoss[0m : 3.11746
[1mStep[0m  [66/339], [94mLoss[0m : 2.13594
[1mStep[0m  [99/339], [94mLoss[0m : 2.76418
[1mStep[0m  [132/339], [94mLoss[0m : 2.49442
[1mStep[0m  [165/339], [94mLoss[0m : 2.58138
[1mStep[0m  [198/339], [94mLoss[0m : 3.39861
[1mStep[0m  [231/339], [94mLoss[0m : 3.13333
[1mStep[0m  [264/339], [94mLoss[0m : 2.95722
[1mStep[0m  [297/339], [94mLoss[0m : 2.26841
[1mStep[0m  [330/339], [94mLoss[0m : 2.62928

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82728
[1mStep[0m  [33/339], [94mLoss[0m : 2.06822
[1mStep[0m  [66/339], [94mLoss[0m : 2.28952
[1mStep[0m  [99/339], [94mLoss[0m : 2.11041
[1mStep[0m  [132/339], [94mLoss[0m : 2.53462
[1mStep[0m  [165/339], [94mLoss[0m : 2.20595
[1mStep[0m  [198/339], [94mLoss[0m : 2.01476
[1mStep[0m  [231/339], [94mLoss[0m : 2.92407
[1mStep[0m  [264/339], [94mLoss[0m : 2.41172
[1mStep[0m  [297/339], [94mLoss[0m : 1.88129
[1mStep[0m  [330/339], [94mLoss[0m : 2.69763

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49440
[1mStep[0m  [33/339], [94mLoss[0m : 2.29255
[1mStep[0m  [66/339], [94mLoss[0m : 2.14777
[1mStep[0m  [99/339], [94mLoss[0m : 2.34229
[1mStep[0m  [132/339], [94mLoss[0m : 2.48417
[1mStep[0m  [165/339], [94mLoss[0m : 2.14858
[1mStep[0m  [198/339], [94mLoss[0m : 2.23018
[1mStep[0m  [231/339], [94mLoss[0m : 2.96073
[1mStep[0m  [264/339], [94mLoss[0m : 2.66121
[1mStep[0m  [297/339], [94mLoss[0m : 2.30372
[1mStep[0m  [330/339], [94mLoss[0m : 2.33191

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16089
[1mStep[0m  [33/339], [94mLoss[0m : 2.60008
[1mStep[0m  [66/339], [94mLoss[0m : 3.25499
[1mStep[0m  [99/339], [94mLoss[0m : 2.99249
[1mStep[0m  [132/339], [94mLoss[0m : 2.45491
[1mStep[0m  [165/339], [94mLoss[0m : 2.84981
[1mStep[0m  [198/339], [94mLoss[0m : 2.95009
[1mStep[0m  [231/339], [94mLoss[0m : 2.96613
[1mStep[0m  [264/339], [94mLoss[0m : 2.90954
[1mStep[0m  [297/339], [94mLoss[0m : 2.81779
[1mStep[0m  [330/339], [94mLoss[0m : 2.80547

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75876
[1mStep[0m  [33/339], [94mLoss[0m : 1.95542
[1mStep[0m  [66/339], [94mLoss[0m : 2.36062
[1mStep[0m  [99/339], [94mLoss[0m : 2.44914
[1mStep[0m  [132/339], [94mLoss[0m : 2.84183
[1mStep[0m  [165/339], [94mLoss[0m : 1.79539
[1mStep[0m  [198/339], [94mLoss[0m : 2.25725
[1mStep[0m  [231/339], [94mLoss[0m : 2.69681
[1mStep[0m  [264/339], [94mLoss[0m : 2.49698
[1mStep[0m  [297/339], [94mLoss[0m : 2.29120
[1mStep[0m  [330/339], [94mLoss[0m : 2.64672

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00526
[1mStep[0m  [33/339], [94mLoss[0m : 2.93004
[1mStep[0m  [66/339], [94mLoss[0m : 2.24048
[1mStep[0m  [99/339], [94mLoss[0m : 2.54512
[1mStep[0m  [132/339], [94mLoss[0m : 2.47980
[1mStep[0m  [165/339], [94mLoss[0m : 2.71142
[1mStep[0m  [198/339], [94mLoss[0m : 3.14958
[1mStep[0m  [231/339], [94mLoss[0m : 2.36076
[1mStep[0m  [264/339], [94mLoss[0m : 2.46638
[1mStep[0m  [297/339], [94mLoss[0m : 2.70665
[1mStep[0m  [330/339], [94mLoss[0m : 2.73893

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51527
[1mStep[0m  [33/339], [94mLoss[0m : 2.34320
[1mStep[0m  [66/339], [94mLoss[0m : 2.33658
[1mStep[0m  [99/339], [94mLoss[0m : 2.10347
[1mStep[0m  [132/339], [94mLoss[0m : 2.66700
[1mStep[0m  [165/339], [94mLoss[0m : 2.29693
[1mStep[0m  [198/339], [94mLoss[0m : 2.34472
[1mStep[0m  [231/339], [94mLoss[0m : 2.52233
[1mStep[0m  [264/339], [94mLoss[0m : 2.43152
[1mStep[0m  [297/339], [94mLoss[0m : 2.35078
[1mStep[0m  [330/339], [94mLoss[0m : 2.20131

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35716
[1mStep[0m  [33/339], [94mLoss[0m : 2.21709
[1mStep[0m  [66/339], [94mLoss[0m : 2.24898
[1mStep[0m  [99/339], [94mLoss[0m : 2.13341
[1mStep[0m  [132/339], [94mLoss[0m : 2.33107
[1mStep[0m  [165/339], [94mLoss[0m : 2.49527
[1mStep[0m  [198/339], [94mLoss[0m : 2.70767
[1mStep[0m  [231/339], [94mLoss[0m : 2.35501
[1mStep[0m  [264/339], [94mLoss[0m : 2.42681
[1mStep[0m  [297/339], [94mLoss[0m : 2.60442
[1mStep[0m  [330/339], [94mLoss[0m : 2.37080

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81700
[1mStep[0m  [33/339], [94mLoss[0m : 2.23934
[1mStep[0m  [66/339], [94mLoss[0m : 2.16487
[1mStep[0m  [99/339], [94mLoss[0m : 2.29923
[1mStep[0m  [132/339], [94mLoss[0m : 3.02284
[1mStep[0m  [165/339], [94mLoss[0m : 2.59198
[1mStep[0m  [198/339], [94mLoss[0m : 1.88450
[1mStep[0m  [231/339], [94mLoss[0m : 2.70665
[1mStep[0m  [264/339], [94mLoss[0m : 2.04025
[1mStep[0m  [297/339], [94mLoss[0m : 2.96534
[1mStep[0m  [330/339], [94mLoss[0m : 2.16419

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.355, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71811
[1mStep[0m  [33/339], [94mLoss[0m : 2.61781
[1mStep[0m  [66/339], [94mLoss[0m : 2.12057
[1mStep[0m  [99/339], [94mLoss[0m : 2.57190
[1mStep[0m  [132/339], [94mLoss[0m : 2.81292
[1mStep[0m  [165/339], [94mLoss[0m : 2.89013
[1mStep[0m  [198/339], [94mLoss[0m : 2.24455
[1mStep[0m  [231/339], [94mLoss[0m : 1.90489
[1mStep[0m  [264/339], [94mLoss[0m : 2.92318
[1mStep[0m  [297/339], [94mLoss[0m : 2.80551
[1mStep[0m  [330/339], [94mLoss[0m : 2.10667

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66002
[1mStep[0m  [33/339], [94mLoss[0m : 2.52684
[1mStep[0m  [66/339], [94mLoss[0m : 2.38370
[1mStep[0m  [99/339], [94mLoss[0m : 2.78950
[1mStep[0m  [132/339], [94mLoss[0m : 2.29334
[1mStep[0m  [165/339], [94mLoss[0m : 2.50061
[1mStep[0m  [198/339], [94mLoss[0m : 2.06123
[1mStep[0m  [231/339], [94mLoss[0m : 2.91406
[1mStep[0m  [264/339], [94mLoss[0m : 2.68492
[1mStep[0m  [297/339], [94mLoss[0m : 3.18844
[1mStep[0m  [330/339], [94mLoss[0m : 2.03909

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75376
[1mStep[0m  [33/339], [94mLoss[0m : 2.91545
[1mStep[0m  [66/339], [94mLoss[0m : 1.75900
[1mStep[0m  [99/339], [94mLoss[0m : 2.28301
[1mStep[0m  [132/339], [94mLoss[0m : 2.29326
[1mStep[0m  [165/339], [94mLoss[0m : 2.26775
[1mStep[0m  [198/339], [94mLoss[0m : 2.29891
[1mStep[0m  [231/339], [94mLoss[0m : 2.42425
[1mStep[0m  [264/339], [94mLoss[0m : 2.33178
[1mStep[0m  [297/339], [94mLoss[0m : 2.68427
[1mStep[0m  [330/339], [94mLoss[0m : 2.09761

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38141
[1mStep[0m  [33/339], [94mLoss[0m : 2.11012
[1mStep[0m  [66/339], [94mLoss[0m : 3.43587
[1mStep[0m  [99/339], [94mLoss[0m : 2.27667
[1mStep[0m  [132/339], [94mLoss[0m : 2.97872
[1mStep[0m  [165/339], [94mLoss[0m : 2.51498
[1mStep[0m  [198/339], [94mLoss[0m : 2.87310
[1mStep[0m  [231/339], [94mLoss[0m : 1.99009
[1mStep[0m  [264/339], [94mLoss[0m : 3.08187
[1mStep[0m  [297/339], [94mLoss[0m : 2.24489
[1mStep[0m  [330/339], [94mLoss[0m : 2.16938

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54811
[1mStep[0m  [33/339], [94mLoss[0m : 2.40650
[1mStep[0m  [66/339], [94mLoss[0m : 2.58059
[1mStep[0m  [99/339], [94mLoss[0m : 2.68582
[1mStep[0m  [132/339], [94mLoss[0m : 2.87390
[1mStep[0m  [165/339], [94mLoss[0m : 2.85597
[1mStep[0m  [198/339], [94mLoss[0m : 1.88362
[1mStep[0m  [231/339], [94mLoss[0m : 2.55032
[1mStep[0m  [264/339], [94mLoss[0m : 2.95484
[1mStep[0m  [297/339], [94mLoss[0m : 2.81653
[1mStep[0m  [330/339], [94mLoss[0m : 2.22227

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16632
[1mStep[0m  [33/339], [94mLoss[0m : 2.98192
[1mStep[0m  [66/339], [94mLoss[0m : 1.88381
[1mStep[0m  [99/339], [94mLoss[0m : 2.37407
[1mStep[0m  [132/339], [94mLoss[0m : 2.19712
[1mStep[0m  [165/339], [94mLoss[0m : 1.86336
[1mStep[0m  [198/339], [94mLoss[0m : 2.03414
[1mStep[0m  [231/339], [94mLoss[0m : 2.36759
[1mStep[0m  [264/339], [94mLoss[0m : 2.35468
[1mStep[0m  [297/339], [94mLoss[0m : 2.05863
[1mStep[0m  [330/339], [94mLoss[0m : 2.56871

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.12123
[1mStep[0m  [33/339], [94mLoss[0m : 2.35348
[1mStep[0m  [66/339], [94mLoss[0m : 2.10029
[1mStep[0m  [99/339], [94mLoss[0m : 2.38320
[1mStep[0m  [132/339], [94mLoss[0m : 1.93637
[1mStep[0m  [165/339], [94mLoss[0m : 2.27145
[1mStep[0m  [198/339], [94mLoss[0m : 2.19539
[1mStep[0m  [231/339], [94mLoss[0m : 2.30709
[1mStep[0m  [264/339], [94mLoss[0m : 2.21570
[1mStep[0m  [297/339], [94mLoss[0m : 2.41870
[1mStep[0m  [330/339], [94mLoss[0m : 3.15064

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34057
[1mStep[0m  [33/339], [94mLoss[0m : 2.57774
[1mStep[0m  [66/339], [94mLoss[0m : 2.59833
[1mStep[0m  [99/339], [94mLoss[0m : 2.42578
[1mStep[0m  [132/339], [94mLoss[0m : 2.42847
[1mStep[0m  [165/339], [94mLoss[0m : 2.82388
[1mStep[0m  [198/339], [94mLoss[0m : 2.13968
[1mStep[0m  [231/339], [94mLoss[0m : 2.31599
[1mStep[0m  [264/339], [94mLoss[0m : 2.30396
[1mStep[0m  [297/339], [94mLoss[0m : 2.31997
[1mStep[0m  [330/339], [94mLoss[0m : 2.57071

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.336
====================================

Phase 1 - Evaluation MAE:  2.3364866602737293
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.47666
[1mStep[0m  [33/339], [94mLoss[0m : 2.52072
[1mStep[0m  [66/339], [94mLoss[0m : 2.45722
[1mStep[0m  [99/339], [94mLoss[0m : 1.74135
[1mStep[0m  [132/339], [94mLoss[0m : 2.70453
[1mStep[0m  [165/339], [94mLoss[0m : 3.00508
[1mStep[0m  [198/339], [94mLoss[0m : 2.49067
[1mStep[0m  [231/339], [94mLoss[0m : 2.09234
[1mStep[0m  [264/339], [94mLoss[0m : 2.22736
[1mStep[0m  [297/339], [94mLoss[0m : 2.70820
[1mStep[0m  [330/339], [94mLoss[0m : 2.81550

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91059
[1mStep[0m  [33/339], [94mLoss[0m : 1.71775
[1mStep[0m  [66/339], [94mLoss[0m : 1.66797
[1mStep[0m  [99/339], [94mLoss[0m : 2.35466
[1mStep[0m  [132/339], [94mLoss[0m : 1.98614
[1mStep[0m  [165/339], [94mLoss[0m : 2.55122
[1mStep[0m  [198/339], [94mLoss[0m : 2.29978
[1mStep[0m  [231/339], [94mLoss[0m : 2.02304
[1mStep[0m  [264/339], [94mLoss[0m : 2.46317
[1mStep[0m  [297/339], [94mLoss[0m : 2.26176
[1mStep[0m  [330/339], [94mLoss[0m : 2.15181

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10523
[1mStep[0m  [33/339], [94mLoss[0m : 2.58150
[1mStep[0m  [66/339], [94mLoss[0m : 3.17233
[1mStep[0m  [99/339], [94mLoss[0m : 2.72320
[1mStep[0m  [132/339], [94mLoss[0m : 1.92772
[1mStep[0m  [165/339], [94mLoss[0m : 2.29182
[1mStep[0m  [198/339], [94mLoss[0m : 2.24994
[1mStep[0m  [231/339], [94mLoss[0m : 1.84836
[1mStep[0m  [264/339], [94mLoss[0m : 2.17004
[1mStep[0m  [297/339], [94mLoss[0m : 2.34513
[1mStep[0m  [330/339], [94mLoss[0m : 2.03642

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28493
[1mStep[0m  [33/339], [94mLoss[0m : 2.54946
[1mStep[0m  [66/339], [94mLoss[0m : 1.82421
[1mStep[0m  [99/339], [94mLoss[0m : 2.05887
[1mStep[0m  [132/339], [94mLoss[0m : 1.92620
[1mStep[0m  [165/339], [94mLoss[0m : 2.32474
[1mStep[0m  [198/339], [94mLoss[0m : 2.42732
[1mStep[0m  [231/339], [94mLoss[0m : 1.48083
[1mStep[0m  [264/339], [94mLoss[0m : 2.56451
[1mStep[0m  [297/339], [94mLoss[0m : 2.53552
[1mStep[0m  [330/339], [94mLoss[0m : 2.63477

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.233, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73552
[1mStep[0m  [33/339], [94mLoss[0m : 1.59421
[1mStep[0m  [66/339], [94mLoss[0m : 1.83330
[1mStep[0m  [99/339], [94mLoss[0m : 1.72516
[1mStep[0m  [132/339], [94mLoss[0m : 2.41505
[1mStep[0m  [165/339], [94mLoss[0m : 2.39562
[1mStep[0m  [198/339], [94mLoss[0m : 1.97196
[1mStep[0m  [231/339], [94mLoss[0m : 1.87063
[1mStep[0m  [264/339], [94mLoss[0m : 1.94717
[1mStep[0m  [297/339], [94mLoss[0m : 2.02337
[1mStep[0m  [330/339], [94mLoss[0m : 2.12240

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93177
[1mStep[0m  [33/339], [94mLoss[0m : 1.91853
[1mStep[0m  [66/339], [94mLoss[0m : 1.92292
[1mStep[0m  [99/339], [94mLoss[0m : 2.01197
[1mStep[0m  [132/339], [94mLoss[0m : 2.28026
[1mStep[0m  [165/339], [94mLoss[0m : 2.64247
[1mStep[0m  [198/339], [94mLoss[0m : 2.18684
[1mStep[0m  [231/339], [94mLoss[0m : 2.51267
[1mStep[0m  [264/339], [94mLoss[0m : 2.29654
[1mStep[0m  [297/339], [94mLoss[0m : 2.48403
[1mStep[0m  [330/339], [94mLoss[0m : 2.09525

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75964
[1mStep[0m  [33/339], [94mLoss[0m : 1.91618
[1mStep[0m  [66/339], [94mLoss[0m : 2.03070
[1mStep[0m  [99/339], [94mLoss[0m : 1.75827
[1mStep[0m  [132/339], [94mLoss[0m : 1.67899
[1mStep[0m  [165/339], [94mLoss[0m : 1.31670
[1mStep[0m  [198/339], [94mLoss[0m : 1.76660
[1mStep[0m  [231/339], [94mLoss[0m : 1.98656
[1mStep[0m  [264/339], [94mLoss[0m : 2.11677
[1mStep[0m  [297/339], [94mLoss[0m : 2.38851
[1mStep[0m  [330/339], [94mLoss[0m : 1.98550

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50676
[1mStep[0m  [33/339], [94mLoss[0m : 2.33821
[1mStep[0m  [66/339], [94mLoss[0m : 2.06430
[1mStep[0m  [99/339], [94mLoss[0m : 2.03531
[1mStep[0m  [132/339], [94mLoss[0m : 2.60042
[1mStep[0m  [165/339], [94mLoss[0m : 2.14136
[1mStep[0m  [198/339], [94mLoss[0m : 2.25509
[1mStep[0m  [231/339], [94mLoss[0m : 2.15471
[1mStep[0m  [264/339], [94mLoss[0m : 2.48952
[1mStep[0m  [297/339], [94mLoss[0m : 2.69718
[1mStep[0m  [330/339], [94mLoss[0m : 1.97098

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60719
[1mStep[0m  [33/339], [94mLoss[0m : 2.00636
[1mStep[0m  [66/339], [94mLoss[0m : 2.50942
[1mStep[0m  [99/339], [94mLoss[0m : 1.90391
[1mStep[0m  [132/339], [94mLoss[0m : 2.50575
[1mStep[0m  [165/339], [94mLoss[0m : 2.41232
[1mStep[0m  [198/339], [94mLoss[0m : 1.66870
[1mStep[0m  [231/339], [94mLoss[0m : 2.01705
[1mStep[0m  [264/339], [94mLoss[0m : 1.97398
[1mStep[0m  [297/339], [94mLoss[0m : 1.96941
[1mStep[0m  [330/339], [94mLoss[0m : 2.05652

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89142
[1mStep[0m  [33/339], [94mLoss[0m : 2.01867
[1mStep[0m  [66/339], [94mLoss[0m : 1.80023
[1mStep[0m  [99/339], [94mLoss[0m : 1.72719
[1mStep[0m  [132/339], [94mLoss[0m : 1.93519
[1mStep[0m  [165/339], [94mLoss[0m : 1.60606
[1mStep[0m  [198/339], [94mLoss[0m : 1.75415
[1mStep[0m  [231/339], [94mLoss[0m : 1.86743
[1mStep[0m  [264/339], [94mLoss[0m : 1.89898
[1mStep[0m  [297/339], [94mLoss[0m : 1.57497
[1mStep[0m  [330/339], [94mLoss[0m : 2.61096

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84626
[1mStep[0m  [33/339], [94mLoss[0m : 2.26350
[1mStep[0m  [66/339], [94mLoss[0m : 1.77461
[1mStep[0m  [99/339], [94mLoss[0m : 1.84018
[1mStep[0m  [132/339], [94mLoss[0m : 1.62909
[1mStep[0m  [165/339], [94mLoss[0m : 1.82059
[1mStep[0m  [198/339], [94mLoss[0m : 1.69491
[1mStep[0m  [231/339], [94mLoss[0m : 1.68895
[1mStep[0m  [264/339], [94mLoss[0m : 1.92285
[1mStep[0m  [297/339], [94mLoss[0m : 2.08663
[1mStep[0m  [330/339], [94mLoss[0m : 1.65659

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95967
[1mStep[0m  [33/339], [94mLoss[0m : 1.73258
[1mStep[0m  [66/339], [94mLoss[0m : 2.05272
[1mStep[0m  [99/339], [94mLoss[0m : 2.30359
[1mStep[0m  [132/339], [94mLoss[0m : 2.13852
[1mStep[0m  [165/339], [94mLoss[0m : 1.72328
[1mStep[0m  [198/339], [94mLoss[0m : 1.60678
[1mStep[0m  [231/339], [94mLoss[0m : 2.34059
[1mStep[0m  [264/339], [94mLoss[0m : 1.65445
[1mStep[0m  [297/339], [94mLoss[0m : 2.31942
[1mStep[0m  [330/339], [94mLoss[0m : 1.86121

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89277
[1mStep[0m  [33/339], [94mLoss[0m : 1.77246
[1mStep[0m  [66/339], [94mLoss[0m : 1.41213
[1mStep[0m  [99/339], [94mLoss[0m : 2.10463
[1mStep[0m  [132/339], [94mLoss[0m : 1.72695
[1mStep[0m  [165/339], [94mLoss[0m : 1.72938
[1mStep[0m  [198/339], [94mLoss[0m : 1.69470
[1mStep[0m  [231/339], [94mLoss[0m : 2.13760
[1mStep[0m  [264/339], [94mLoss[0m : 1.58589
[1mStep[0m  [297/339], [94mLoss[0m : 1.62237
[1mStep[0m  [330/339], [94mLoss[0m : 1.81191

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16060
[1mStep[0m  [33/339], [94mLoss[0m : 1.78241
[1mStep[0m  [66/339], [94mLoss[0m : 1.44749
[1mStep[0m  [99/339], [94mLoss[0m : 1.98731
[1mStep[0m  [132/339], [94mLoss[0m : 1.81735
[1mStep[0m  [165/339], [94mLoss[0m : 1.93442
[1mStep[0m  [198/339], [94mLoss[0m : 2.14938
[1mStep[0m  [231/339], [94mLoss[0m : 1.60113
[1mStep[0m  [264/339], [94mLoss[0m : 1.90694
[1mStep[0m  [297/339], [94mLoss[0m : 1.60243
[1mStep[0m  [330/339], [94mLoss[0m : 2.12631

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.03979
[1mStep[0m  [33/339], [94mLoss[0m : 1.47669
[1mStep[0m  [66/339], [94mLoss[0m : 1.55785
[1mStep[0m  [99/339], [94mLoss[0m : 1.39547
[1mStep[0m  [132/339], [94mLoss[0m : 1.67473
[1mStep[0m  [165/339], [94mLoss[0m : 1.98224
[1mStep[0m  [198/339], [94mLoss[0m : 1.46590
[1mStep[0m  [231/339], [94mLoss[0m : 1.70775
[1mStep[0m  [264/339], [94mLoss[0m : 1.52463
[1mStep[0m  [297/339], [94mLoss[0m : 1.68096
[1mStep[0m  [330/339], [94mLoss[0m : 1.77911

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32435
[1mStep[0m  [33/339], [94mLoss[0m : 1.50097
[1mStep[0m  [66/339], [94mLoss[0m : 1.62625
[1mStep[0m  [99/339], [94mLoss[0m : 1.69358
[1mStep[0m  [132/339], [94mLoss[0m : 1.49946
[1mStep[0m  [165/339], [94mLoss[0m : 1.78053
[1mStep[0m  [198/339], [94mLoss[0m : 1.89957
[1mStep[0m  [231/339], [94mLoss[0m : 1.31092
[1mStep[0m  [264/339], [94mLoss[0m : 1.77094
[1mStep[0m  [297/339], [94mLoss[0m : 1.55698
[1mStep[0m  [330/339], [94mLoss[0m : 1.46376

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.742, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64266
[1mStep[0m  [33/339], [94mLoss[0m : 2.09242
[1mStep[0m  [66/339], [94mLoss[0m : 1.99337
[1mStep[0m  [99/339], [94mLoss[0m : 1.56540
[1mStep[0m  [132/339], [94mLoss[0m : 1.41695
[1mStep[0m  [165/339], [94mLoss[0m : 1.82167
[1mStep[0m  [198/339], [94mLoss[0m : 1.57887
[1mStep[0m  [231/339], [94mLoss[0m : 1.84964
[1mStep[0m  [264/339], [94mLoss[0m : 1.24864
[1mStep[0m  [297/339], [94mLoss[0m : 2.08206
[1mStep[0m  [330/339], [94mLoss[0m : 2.11551

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.04439
[1mStep[0m  [33/339], [94mLoss[0m : 1.49068
[1mStep[0m  [66/339], [94mLoss[0m : 2.31052
[1mStep[0m  [99/339], [94mLoss[0m : 1.56785
[1mStep[0m  [132/339], [94mLoss[0m : 1.66511
[1mStep[0m  [165/339], [94mLoss[0m : 1.24708
[1mStep[0m  [198/339], [94mLoss[0m : 1.54715
[1mStep[0m  [231/339], [94mLoss[0m : 1.73502
[1mStep[0m  [264/339], [94mLoss[0m : 1.44985
[1mStep[0m  [297/339], [94mLoss[0m : 1.33674
[1mStep[0m  [330/339], [94mLoss[0m : 1.71752

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.679, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73545
[1mStep[0m  [33/339], [94mLoss[0m : 1.73542
[1mStep[0m  [66/339], [94mLoss[0m : 1.73461
[1mStep[0m  [99/339], [94mLoss[0m : 1.99948
[1mStep[0m  [132/339], [94mLoss[0m : 2.10860
[1mStep[0m  [165/339], [94mLoss[0m : 1.82297
[1mStep[0m  [198/339], [94mLoss[0m : 1.73221
[1mStep[0m  [231/339], [94mLoss[0m : 2.21115
[1mStep[0m  [264/339], [94mLoss[0m : 1.54279
[1mStep[0m  [297/339], [94mLoss[0m : 1.83423
[1mStep[0m  [330/339], [94mLoss[0m : 2.16144

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.679, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47930
[1mStep[0m  [33/339], [94mLoss[0m : 1.61756
[1mStep[0m  [66/339], [94mLoss[0m : 1.40902
[1mStep[0m  [99/339], [94mLoss[0m : 2.01977
[1mStep[0m  [132/339], [94mLoss[0m : 1.39463
[1mStep[0m  [165/339], [94mLoss[0m : 1.78351
[1mStep[0m  [198/339], [94mLoss[0m : 2.20773
[1mStep[0m  [231/339], [94mLoss[0m : 1.65852
[1mStep[0m  [264/339], [94mLoss[0m : 1.91142
[1mStep[0m  [297/339], [94mLoss[0m : 1.49855
[1mStep[0m  [330/339], [94mLoss[0m : 1.74815

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53483
[1mStep[0m  [33/339], [94mLoss[0m : 1.24090
[1mStep[0m  [66/339], [94mLoss[0m : 1.51271
[1mStep[0m  [99/339], [94mLoss[0m : 1.84775
[1mStep[0m  [132/339], [94mLoss[0m : 1.48239
[1mStep[0m  [165/339], [94mLoss[0m : 1.27903
[1mStep[0m  [198/339], [94mLoss[0m : 1.70795
[1mStep[0m  [231/339], [94mLoss[0m : 1.54966
[1mStep[0m  [264/339], [94mLoss[0m : 1.50613
[1mStep[0m  [297/339], [94mLoss[0m : 1.90054
[1mStep[0m  [330/339], [94mLoss[0m : 1.58307

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.48065
[1mStep[0m  [33/339], [94mLoss[0m : 1.21334
[1mStep[0m  [66/339], [94mLoss[0m : 1.64344
[1mStep[0m  [99/339], [94mLoss[0m : 1.73541
[1mStep[0m  [132/339], [94mLoss[0m : 1.76320
[1mStep[0m  [165/339], [94mLoss[0m : 1.88050
[1mStep[0m  [198/339], [94mLoss[0m : 1.76078
[1mStep[0m  [231/339], [94mLoss[0m : 1.54547
[1mStep[0m  [264/339], [94mLoss[0m : 1.45490
[1mStep[0m  [297/339], [94mLoss[0m : 2.10089
[1mStep[0m  [330/339], [94mLoss[0m : 1.86410

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13851
[1mStep[0m  [33/339], [94mLoss[0m : 1.49751
[1mStep[0m  [66/339], [94mLoss[0m : 1.76096
[1mStep[0m  [99/339], [94mLoss[0m : 1.87876
[1mStep[0m  [132/339], [94mLoss[0m : 2.47310
[1mStep[0m  [165/339], [94mLoss[0m : 1.30823
[1mStep[0m  [198/339], [94mLoss[0m : 1.65542
[1mStep[0m  [231/339], [94mLoss[0m : 1.49588
[1mStep[0m  [264/339], [94mLoss[0m : 1.34055
[1mStep[0m  [297/339], [94mLoss[0m : 2.21833
[1mStep[0m  [330/339], [94mLoss[0m : 1.60463

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.492, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.16434
[1mStep[0m  [33/339], [94mLoss[0m : 1.98264
[1mStep[0m  [66/339], [94mLoss[0m : 1.38298
[1mStep[0m  [99/339], [94mLoss[0m : 1.37157
[1mStep[0m  [132/339], [94mLoss[0m : 1.85406
[1mStep[0m  [165/339], [94mLoss[0m : 1.72212
[1mStep[0m  [198/339], [94mLoss[0m : 2.03529
[1mStep[0m  [231/339], [94mLoss[0m : 1.42913
[1mStep[0m  [264/339], [94mLoss[0m : 1.30136
[1mStep[0m  [297/339], [94mLoss[0m : 1.78331
[1mStep[0m  [330/339], [94mLoss[0m : 1.97259

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24294
[1mStep[0m  [33/339], [94mLoss[0m : 1.65491
[1mStep[0m  [66/339], [94mLoss[0m : 1.75288
[1mStep[0m  [99/339], [94mLoss[0m : 1.07990
[1mStep[0m  [132/339], [94mLoss[0m : 1.27547
[1mStep[0m  [165/339], [94mLoss[0m : 1.79893
[1mStep[0m  [198/339], [94mLoss[0m : 1.35413
[1mStep[0m  [231/339], [94mLoss[0m : 1.19798
[1mStep[0m  [264/339], [94mLoss[0m : 1.66477
[1mStep[0m  [297/339], [94mLoss[0m : 1.90721
[1mStep[0m  [330/339], [94mLoss[0m : 0.96650

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.505, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51064
[1mStep[0m  [33/339], [94mLoss[0m : 2.00889
[1mStep[0m  [66/339], [94mLoss[0m : 1.34185
[1mStep[0m  [99/339], [94mLoss[0m : 1.33683
[1mStep[0m  [132/339], [94mLoss[0m : 1.43490
[1mStep[0m  [165/339], [94mLoss[0m : 1.38952
[1mStep[0m  [198/339], [94mLoss[0m : 1.62069
[1mStep[0m  [231/339], [94mLoss[0m : 1.42443
[1mStep[0m  [264/339], [94mLoss[0m : 1.41371
[1mStep[0m  [297/339], [94mLoss[0m : 1.51819
[1mStep[0m  [330/339], [94mLoss[0m : 1.84747

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.513, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86152
[1mStep[0m  [33/339], [94mLoss[0m : 1.73566
[1mStep[0m  [66/339], [94mLoss[0m : 1.19851
[1mStep[0m  [99/339], [94mLoss[0m : 1.35667
[1mStep[0m  [132/339], [94mLoss[0m : 0.96497
[1mStep[0m  [165/339], [94mLoss[0m : 1.02435
[1mStep[0m  [198/339], [94mLoss[0m : 1.81255
[1mStep[0m  [231/339], [94mLoss[0m : 1.87603
[1mStep[0m  [264/339], [94mLoss[0m : 1.55978
[1mStep[0m  [297/339], [94mLoss[0m : 1.23913
[1mStep[0m  [330/339], [94mLoss[0m : 1.83816

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64869
[1mStep[0m  [33/339], [94mLoss[0m : 1.71953
[1mStep[0m  [66/339], [94mLoss[0m : 1.70585
[1mStep[0m  [99/339], [94mLoss[0m : 1.48034
[1mStep[0m  [132/339], [94mLoss[0m : 1.38012
[1mStep[0m  [165/339], [94mLoss[0m : 1.16835
[1mStep[0m  [198/339], [94mLoss[0m : 1.56457
[1mStep[0m  [231/339], [94mLoss[0m : 1.48791
[1mStep[0m  [264/339], [94mLoss[0m : 1.31368
[1mStep[0m  [297/339], [94mLoss[0m : 0.99742
[1mStep[0m  [330/339], [94mLoss[0m : 1.23497

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.573, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41056
[1mStep[0m  [33/339], [94mLoss[0m : 1.20866
[1mStep[0m  [66/339], [94mLoss[0m : 1.37633
[1mStep[0m  [99/339], [94mLoss[0m : 1.45939
[1mStep[0m  [132/339], [94mLoss[0m : 1.33252
[1mStep[0m  [165/339], [94mLoss[0m : 1.30876
[1mStep[0m  [198/339], [94mLoss[0m : 1.75053
[1mStep[0m  [231/339], [94mLoss[0m : 1.23382
[1mStep[0m  [264/339], [94mLoss[0m : 1.89457
[1mStep[0m  [297/339], [94mLoss[0m : 1.35493
[1mStep[0m  [330/339], [94mLoss[0m : 1.72514

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73104
[1mStep[0m  [33/339], [94mLoss[0m : 1.66154
[1mStep[0m  [66/339], [94mLoss[0m : 1.14030
[1mStep[0m  [99/339], [94mLoss[0m : 1.13784
[1mStep[0m  [132/339], [94mLoss[0m : 2.05166
[1mStep[0m  [165/339], [94mLoss[0m : 1.17869
[1mStep[0m  [198/339], [94mLoss[0m : 1.37721
[1mStep[0m  [231/339], [94mLoss[0m : 1.35268
[1mStep[0m  [264/339], [94mLoss[0m : 1.15691
[1mStep[0m  [297/339], [94mLoss[0m : 1.55682
[1mStep[0m  [330/339], [94mLoss[0m : 1.20150

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.450, [92mTest[0m: 2.531, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.556
====================================

Phase 2 - Evaluation MAE:  2.555844083296514
MAE score P1      2.336487
MAE score P2      2.555844
loss              1.449563
learning_rate     0.007525
batch_size              32
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 9.48969
[1mStep[0m  [33/339], [94mLoss[0m : 2.34585
[1mStep[0m  [66/339], [94mLoss[0m : 3.26233
[1mStep[0m  [99/339], [94mLoss[0m : 2.33711
[1mStep[0m  [132/339], [94mLoss[0m : 3.16265
[1mStep[0m  [165/339], [94mLoss[0m : 3.22276
[1mStep[0m  [198/339], [94mLoss[0m : 2.58460
[1mStep[0m  [231/339], [94mLoss[0m : 2.65519
[1mStep[0m  [264/339], [94mLoss[0m : 2.72734
[1mStep[0m  [297/339], [94mLoss[0m : 2.60212
[1mStep[0m  [330/339], [94mLoss[0m : 2.10723

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.931, [92mTest[0m: 10.789, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60883
[1mStep[0m  [33/339], [94mLoss[0m : 2.50741
[1mStep[0m  [66/339], [94mLoss[0m : 3.23524
[1mStep[0m  [99/339], [94mLoss[0m : 2.81066
[1mStep[0m  [132/339], [94mLoss[0m : 2.60101
[1mStep[0m  [165/339], [94mLoss[0m : 2.41517
[1mStep[0m  [198/339], [94mLoss[0m : 2.02730
[1mStep[0m  [231/339], [94mLoss[0m : 2.50853
[1mStep[0m  [264/339], [94mLoss[0m : 2.01307
[1mStep[0m  [297/339], [94mLoss[0m : 3.35690
[1mStep[0m  [330/339], [94mLoss[0m : 2.75724

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82762
[1mStep[0m  [33/339], [94mLoss[0m : 2.55291
[1mStep[0m  [66/339], [94mLoss[0m : 3.31660
[1mStep[0m  [99/339], [94mLoss[0m : 2.31169
[1mStep[0m  [132/339], [94mLoss[0m : 2.92576
[1mStep[0m  [165/339], [94mLoss[0m : 2.02006
[1mStep[0m  [198/339], [94mLoss[0m : 1.65960
[1mStep[0m  [231/339], [94mLoss[0m : 3.07449
[1mStep[0m  [264/339], [94mLoss[0m : 2.14110
[1mStep[0m  [297/339], [94mLoss[0m : 2.08776
[1mStep[0m  [330/339], [94mLoss[0m : 2.47775

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66427
[1mStep[0m  [33/339], [94mLoss[0m : 2.20075
[1mStep[0m  [66/339], [94mLoss[0m : 2.96433
[1mStep[0m  [99/339], [94mLoss[0m : 2.95972
[1mStep[0m  [132/339], [94mLoss[0m : 2.36684
[1mStep[0m  [165/339], [94mLoss[0m : 3.15174
[1mStep[0m  [198/339], [94mLoss[0m : 2.60007
[1mStep[0m  [231/339], [94mLoss[0m : 2.85049
[1mStep[0m  [264/339], [94mLoss[0m : 2.66193
[1mStep[0m  [297/339], [94mLoss[0m : 2.87106
[1mStep[0m  [330/339], [94mLoss[0m : 3.04988

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39579
[1mStep[0m  [33/339], [94mLoss[0m : 2.59800
[1mStep[0m  [66/339], [94mLoss[0m : 2.45397
[1mStep[0m  [99/339], [94mLoss[0m : 2.33759
[1mStep[0m  [132/339], [94mLoss[0m : 2.40929
[1mStep[0m  [165/339], [94mLoss[0m : 3.80846
[1mStep[0m  [198/339], [94mLoss[0m : 1.65902
[1mStep[0m  [231/339], [94mLoss[0m : 2.73198
[1mStep[0m  [264/339], [94mLoss[0m : 1.88944
[1mStep[0m  [297/339], [94mLoss[0m : 2.34571
[1mStep[0m  [330/339], [94mLoss[0m : 2.65829

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61547
[1mStep[0m  [33/339], [94mLoss[0m : 2.47803
[1mStep[0m  [66/339], [94mLoss[0m : 2.56041
[1mStep[0m  [99/339], [94mLoss[0m : 2.71238
[1mStep[0m  [132/339], [94mLoss[0m : 2.82924
[1mStep[0m  [165/339], [94mLoss[0m : 2.20255
[1mStep[0m  [198/339], [94mLoss[0m : 2.57626
[1mStep[0m  [231/339], [94mLoss[0m : 2.12423
[1mStep[0m  [264/339], [94mLoss[0m : 2.08336
[1mStep[0m  [297/339], [94mLoss[0m : 2.19886
[1mStep[0m  [330/339], [94mLoss[0m : 2.48861

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19868
[1mStep[0m  [33/339], [94mLoss[0m : 1.94206
[1mStep[0m  [66/339], [94mLoss[0m : 2.70952
[1mStep[0m  [99/339], [94mLoss[0m : 2.65429
[1mStep[0m  [132/339], [94mLoss[0m : 2.55268
[1mStep[0m  [165/339], [94mLoss[0m : 2.91588
[1mStep[0m  [198/339], [94mLoss[0m : 2.45165
[1mStep[0m  [231/339], [94mLoss[0m : 2.38157
[1mStep[0m  [264/339], [94mLoss[0m : 2.59144
[1mStep[0m  [297/339], [94mLoss[0m : 2.45034
[1mStep[0m  [330/339], [94mLoss[0m : 2.42623

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67820
[1mStep[0m  [33/339], [94mLoss[0m : 3.18512
[1mStep[0m  [66/339], [94mLoss[0m : 1.90035
[1mStep[0m  [99/339], [94mLoss[0m : 2.96444
[1mStep[0m  [132/339], [94mLoss[0m : 2.36806
[1mStep[0m  [165/339], [94mLoss[0m : 2.36574
[1mStep[0m  [198/339], [94mLoss[0m : 2.55522
[1mStep[0m  [231/339], [94mLoss[0m : 2.61154
[1mStep[0m  [264/339], [94mLoss[0m : 2.36391
[1mStep[0m  [297/339], [94mLoss[0m : 2.16195
[1mStep[0m  [330/339], [94mLoss[0m : 2.07122

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86976
[1mStep[0m  [33/339], [94mLoss[0m : 1.93873
[1mStep[0m  [66/339], [94mLoss[0m : 2.29684
[1mStep[0m  [99/339], [94mLoss[0m : 2.46041
[1mStep[0m  [132/339], [94mLoss[0m : 2.87068
[1mStep[0m  [165/339], [94mLoss[0m : 2.31563
[1mStep[0m  [198/339], [94mLoss[0m : 1.87565
[1mStep[0m  [231/339], [94mLoss[0m : 2.60270
[1mStep[0m  [264/339], [94mLoss[0m : 2.00659
[1mStep[0m  [297/339], [94mLoss[0m : 2.36849
[1mStep[0m  [330/339], [94mLoss[0m : 2.52460

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18780
[1mStep[0m  [33/339], [94mLoss[0m : 2.13048
[1mStep[0m  [66/339], [94mLoss[0m : 1.96331
[1mStep[0m  [99/339], [94mLoss[0m : 2.37629
[1mStep[0m  [132/339], [94mLoss[0m : 2.38468
[1mStep[0m  [165/339], [94mLoss[0m : 2.82943
[1mStep[0m  [198/339], [94mLoss[0m : 2.60047
[1mStep[0m  [231/339], [94mLoss[0m : 2.22030
[1mStep[0m  [264/339], [94mLoss[0m : 2.26021
[1mStep[0m  [297/339], [94mLoss[0m : 1.92126
[1mStep[0m  [330/339], [94mLoss[0m : 1.98434

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50864
[1mStep[0m  [33/339], [94mLoss[0m : 3.06001
[1mStep[0m  [66/339], [94mLoss[0m : 2.83482
[1mStep[0m  [99/339], [94mLoss[0m : 2.64749
[1mStep[0m  [132/339], [94mLoss[0m : 2.30005
[1mStep[0m  [165/339], [94mLoss[0m : 2.40422
[1mStep[0m  [198/339], [94mLoss[0m : 2.22858
[1mStep[0m  [231/339], [94mLoss[0m : 2.33596
[1mStep[0m  [264/339], [94mLoss[0m : 2.86363
[1mStep[0m  [297/339], [94mLoss[0m : 2.53198
[1mStep[0m  [330/339], [94mLoss[0m : 2.27727

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.319, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77603
[1mStep[0m  [33/339], [94mLoss[0m : 2.15539
[1mStep[0m  [66/339], [94mLoss[0m : 2.69064
[1mStep[0m  [99/339], [94mLoss[0m : 2.02331
[1mStep[0m  [132/339], [94mLoss[0m : 2.71153
[1mStep[0m  [165/339], [94mLoss[0m : 2.30208
[1mStep[0m  [198/339], [94mLoss[0m : 2.57621
[1mStep[0m  [231/339], [94mLoss[0m : 2.53961
[1mStep[0m  [264/339], [94mLoss[0m : 2.55920
[1mStep[0m  [297/339], [94mLoss[0m : 2.09805
[1mStep[0m  [330/339], [94mLoss[0m : 2.90784

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54383
[1mStep[0m  [33/339], [94mLoss[0m : 2.65505
[1mStep[0m  [66/339], [94mLoss[0m : 2.64270
[1mStep[0m  [99/339], [94mLoss[0m : 2.47178
[1mStep[0m  [132/339], [94mLoss[0m : 2.97739
[1mStep[0m  [165/339], [94mLoss[0m : 1.88382
[1mStep[0m  [198/339], [94mLoss[0m : 2.30612
[1mStep[0m  [231/339], [94mLoss[0m : 3.21018
[1mStep[0m  [264/339], [94mLoss[0m : 3.02787
[1mStep[0m  [297/339], [94mLoss[0m : 2.69857
[1mStep[0m  [330/339], [94mLoss[0m : 2.07474

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.316, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80659
[1mStep[0m  [33/339], [94mLoss[0m : 2.20079
[1mStep[0m  [66/339], [94mLoss[0m : 2.70520
[1mStep[0m  [99/339], [94mLoss[0m : 2.27344
[1mStep[0m  [132/339], [94mLoss[0m : 2.05715
[1mStep[0m  [165/339], [94mLoss[0m : 2.24725
[1mStep[0m  [198/339], [94mLoss[0m : 2.47983
[1mStep[0m  [231/339], [94mLoss[0m : 2.36967
[1mStep[0m  [264/339], [94mLoss[0m : 2.32523
[1mStep[0m  [297/339], [94mLoss[0m : 1.95111
[1mStep[0m  [330/339], [94mLoss[0m : 2.17421

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.283, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82379
[1mStep[0m  [33/339], [94mLoss[0m : 2.41055
[1mStep[0m  [66/339], [94mLoss[0m : 2.06791
[1mStep[0m  [99/339], [94mLoss[0m : 2.77298
[1mStep[0m  [132/339], [94mLoss[0m : 1.61027
[1mStep[0m  [165/339], [94mLoss[0m : 2.94502
[1mStep[0m  [198/339], [94mLoss[0m : 2.55237
[1mStep[0m  [231/339], [94mLoss[0m : 2.18893
[1mStep[0m  [264/339], [94mLoss[0m : 2.23937
[1mStep[0m  [297/339], [94mLoss[0m : 1.81903
[1mStep[0m  [330/339], [94mLoss[0m : 2.30514

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07090
[1mStep[0m  [33/339], [94mLoss[0m : 2.31488
[1mStep[0m  [66/339], [94mLoss[0m : 2.71762
[1mStep[0m  [99/339], [94mLoss[0m : 2.18063
[1mStep[0m  [132/339], [94mLoss[0m : 2.48420
[1mStep[0m  [165/339], [94mLoss[0m : 1.91565
[1mStep[0m  [198/339], [94mLoss[0m : 2.21929
[1mStep[0m  [231/339], [94mLoss[0m : 1.88848
[1mStep[0m  [264/339], [94mLoss[0m : 1.96743
[1mStep[0m  [297/339], [94mLoss[0m : 2.26863
[1mStep[0m  [330/339], [94mLoss[0m : 2.21399

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.292, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13036
[1mStep[0m  [33/339], [94mLoss[0m : 2.38270
[1mStep[0m  [66/339], [94mLoss[0m : 2.63795
[1mStep[0m  [99/339], [94mLoss[0m : 1.82707
[1mStep[0m  [132/339], [94mLoss[0m : 2.48743
[1mStep[0m  [165/339], [94mLoss[0m : 1.76648
[1mStep[0m  [198/339], [94mLoss[0m : 2.70082
[1mStep[0m  [231/339], [94mLoss[0m : 2.30059
[1mStep[0m  [264/339], [94mLoss[0m : 2.20680
[1mStep[0m  [297/339], [94mLoss[0m : 2.74911
[1mStep[0m  [330/339], [94mLoss[0m : 2.20142

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.298, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82315
[1mStep[0m  [33/339], [94mLoss[0m : 1.90047
[1mStep[0m  [66/339], [94mLoss[0m : 2.16079
[1mStep[0m  [99/339], [94mLoss[0m : 2.98074
[1mStep[0m  [132/339], [94mLoss[0m : 2.56536
[1mStep[0m  [165/339], [94mLoss[0m : 2.03209
[1mStep[0m  [198/339], [94mLoss[0m : 2.00614
[1mStep[0m  [231/339], [94mLoss[0m : 2.13069
[1mStep[0m  [264/339], [94mLoss[0m : 2.57228
[1mStep[0m  [297/339], [94mLoss[0m : 2.47200
[1mStep[0m  [330/339], [94mLoss[0m : 2.76763

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.301, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67069
[1mStep[0m  [33/339], [94mLoss[0m : 2.32493
[1mStep[0m  [66/339], [94mLoss[0m : 2.26692
[1mStep[0m  [99/339], [94mLoss[0m : 2.18530
[1mStep[0m  [132/339], [94mLoss[0m : 1.98745
[1mStep[0m  [165/339], [94mLoss[0m : 2.12677
[1mStep[0m  [198/339], [94mLoss[0m : 2.22791
[1mStep[0m  [231/339], [94mLoss[0m : 2.74916
[1mStep[0m  [264/339], [94mLoss[0m : 2.34161
[1mStep[0m  [297/339], [94mLoss[0m : 2.98801
[1mStep[0m  [330/339], [94mLoss[0m : 1.99115

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.285, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01570
[1mStep[0m  [33/339], [94mLoss[0m : 2.06425
[1mStep[0m  [66/339], [94mLoss[0m : 2.04086
[1mStep[0m  [99/339], [94mLoss[0m : 2.20254
[1mStep[0m  [132/339], [94mLoss[0m : 2.48191
[1mStep[0m  [165/339], [94mLoss[0m : 2.90931
[1mStep[0m  [198/339], [94mLoss[0m : 2.38334
[1mStep[0m  [231/339], [94mLoss[0m : 2.51459
[1mStep[0m  [264/339], [94mLoss[0m : 2.64781
[1mStep[0m  [297/339], [94mLoss[0m : 2.41096
[1mStep[0m  [330/339], [94mLoss[0m : 2.44267

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08920
[1mStep[0m  [33/339], [94mLoss[0m : 2.79847
[1mStep[0m  [66/339], [94mLoss[0m : 2.61713
[1mStep[0m  [99/339], [94mLoss[0m : 2.35587
[1mStep[0m  [132/339], [94mLoss[0m : 1.94083
[1mStep[0m  [165/339], [94mLoss[0m : 2.24562
[1mStep[0m  [198/339], [94mLoss[0m : 2.42955
[1mStep[0m  [231/339], [94mLoss[0m : 2.40890
[1mStep[0m  [264/339], [94mLoss[0m : 2.53813
[1mStep[0m  [297/339], [94mLoss[0m : 1.78952
[1mStep[0m  [330/339], [94mLoss[0m : 2.40716

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71735
[1mStep[0m  [33/339], [94mLoss[0m : 2.21330
[1mStep[0m  [66/339], [94mLoss[0m : 2.64923
[1mStep[0m  [99/339], [94mLoss[0m : 1.76347
[1mStep[0m  [132/339], [94mLoss[0m : 2.22388
[1mStep[0m  [165/339], [94mLoss[0m : 2.32806
[1mStep[0m  [198/339], [94mLoss[0m : 2.55935
[1mStep[0m  [231/339], [94mLoss[0m : 2.23855
[1mStep[0m  [264/339], [94mLoss[0m : 2.03317
[1mStep[0m  [297/339], [94mLoss[0m : 2.02081
[1mStep[0m  [330/339], [94mLoss[0m : 2.67211

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17938
[1mStep[0m  [33/339], [94mLoss[0m : 2.18447
[1mStep[0m  [66/339], [94mLoss[0m : 2.44225
[1mStep[0m  [99/339], [94mLoss[0m : 2.38859
[1mStep[0m  [132/339], [94mLoss[0m : 2.64418
[1mStep[0m  [165/339], [94mLoss[0m : 2.55562
[1mStep[0m  [198/339], [94mLoss[0m : 2.51137
[1mStep[0m  [231/339], [94mLoss[0m : 2.28341
[1mStep[0m  [264/339], [94mLoss[0m : 2.07776
[1mStep[0m  [297/339], [94mLoss[0m : 2.51234
[1mStep[0m  [330/339], [94mLoss[0m : 2.48007

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.314, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01890
[1mStep[0m  [33/339], [94mLoss[0m : 2.24551
[1mStep[0m  [66/339], [94mLoss[0m : 2.00387
[1mStep[0m  [99/339], [94mLoss[0m : 2.26432
[1mStep[0m  [132/339], [94mLoss[0m : 1.97963
[1mStep[0m  [165/339], [94mLoss[0m : 1.74336
[1mStep[0m  [198/339], [94mLoss[0m : 2.65867
[1mStep[0m  [231/339], [94mLoss[0m : 2.20743
[1mStep[0m  [264/339], [94mLoss[0m : 2.05960
[1mStep[0m  [297/339], [94mLoss[0m : 2.39282
[1mStep[0m  [330/339], [94mLoss[0m : 2.05556

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.310, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98277
[1mStep[0m  [33/339], [94mLoss[0m : 2.76390
[1mStep[0m  [66/339], [94mLoss[0m : 2.49778
[1mStep[0m  [99/339], [94mLoss[0m : 1.66330
[1mStep[0m  [132/339], [94mLoss[0m : 2.99626
[1mStep[0m  [165/339], [94mLoss[0m : 2.65448
[1mStep[0m  [198/339], [94mLoss[0m : 2.18752
[1mStep[0m  [231/339], [94mLoss[0m : 2.58143
[1mStep[0m  [264/339], [94mLoss[0m : 2.36735
[1mStep[0m  [297/339], [94mLoss[0m : 2.97860
[1mStep[0m  [330/339], [94mLoss[0m : 2.71387

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38606
[1mStep[0m  [33/339], [94mLoss[0m : 2.31848
[1mStep[0m  [66/339], [94mLoss[0m : 2.77633
[1mStep[0m  [99/339], [94mLoss[0m : 2.18067
[1mStep[0m  [132/339], [94mLoss[0m : 2.41598
[1mStep[0m  [165/339], [94mLoss[0m : 1.82030
[1mStep[0m  [198/339], [94mLoss[0m : 2.68458
[1mStep[0m  [231/339], [94mLoss[0m : 1.82845
[1mStep[0m  [264/339], [94mLoss[0m : 2.37948
[1mStep[0m  [297/339], [94mLoss[0m : 2.25360
[1mStep[0m  [330/339], [94mLoss[0m : 2.58775

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54115
[1mStep[0m  [33/339], [94mLoss[0m : 2.45260
[1mStep[0m  [66/339], [94mLoss[0m : 2.60351
[1mStep[0m  [99/339], [94mLoss[0m : 1.89512
[1mStep[0m  [132/339], [94mLoss[0m : 1.96366
[1mStep[0m  [165/339], [94mLoss[0m : 2.32733
[1mStep[0m  [198/339], [94mLoss[0m : 2.20976
[1mStep[0m  [231/339], [94mLoss[0m : 2.70651
[1mStep[0m  [264/339], [94mLoss[0m : 1.99909
[1mStep[0m  [297/339], [94mLoss[0m : 2.23732
[1mStep[0m  [330/339], [94mLoss[0m : 2.43204

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81564
[1mStep[0m  [33/339], [94mLoss[0m : 2.29229
[1mStep[0m  [66/339], [94mLoss[0m : 1.50139
[1mStep[0m  [99/339], [94mLoss[0m : 1.96082
[1mStep[0m  [132/339], [94mLoss[0m : 2.01645
[1mStep[0m  [165/339], [94mLoss[0m : 2.88206
[1mStep[0m  [198/339], [94mLoss[0m : 2.48200
[1mStep[0m  [231/339], [94mLoss[0m : 2.94822
[1mStep[0m  [264/339], [94mLoss[0m : 2.29616
[1mStep[0m  [297/339], [94mLoss[0m : 2.02182
[1mStep[0m  [330/339], [94mLoss[0m : 2.32900

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.292, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67736
[1mStep[0m  [33/339], [94mLoss[0m : 2.00878
[1mStep[0m  [66/339], [94mLoss[0m : 2.76785
[1mStep[0m  [99/339], [94mLoss[0m : 2.57022
[1mStep[0m  [132/339], [94mLoss[0m : 1.99465
[1mStep[0m  [165/339], [94mLoss[0m : 2.95879
[1mStep[0m  [198/339], [94mLoss[0m : 2.46257
[1mStep[0m  [231/339], [94mLoss[0m : 2.08283
[1mStep[0m  [264/339], [94mLoss[0m : 2.27813
[1mStep[0m  [297/339], [94mLoss[0m : 2.48923
[1mStep[0m  [330/339], [94mLoss[0m : 2.07614

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80471
[1mStep[0m  [33/339], [94mLoss[0m : 2.29007
[1mStep[0m  [66/339], [94mLoss[0m : 2.05407
[1mStep[0m  [99/339], [94mLoss[0m : 1.93048
[1mStep[0m  [132/339], [94mLoss[0m : 2.70268
[1mStep[0m  [165/339], [94mLoss[0m : 2.16205
[1mStep[0m  [198/339], [94mLoss[0m : 2.10366
[1mStep[0m  [231/339], [94mLoss[0m : 2.07271
[1mStep[0m  [264/339], [94mLoss[0m : 2.52252
[1mStep[0m  [297/339], [94mLoss[0m : 2.49340
[1mStep[0m  [330/339], [94mLoss[0m : 2.36077

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.315
====================================

Phase 1 - Evaluation MAE:  2.3148699682370752
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.63614
[1mStep[0m  [33/339], [94mLoss[0m : 2.29727
[1mStep[0m  [66/339], [94mLoss[0m : 2.41152
[1mStep[0m  [99/339], [94mLoss[0m : 2.43866
[1mStep[0m  [132/339], [94mLoss[0m : 2.34285
[1mStep[0m  [165/339], [94mLoss[0m : 2.40232
[1mStep[0m  [198/339], [94mLoss[0m : 3.20687
[1mStep[0m  [231/339], [94mLoss[0m : 2.24215
[1mStep[0m  [264/339], [94mLoss[0m : 3.03571
[1mStep[0m  [297/339], [94mLoss[0m : 2.44046
[1mStep[0m  [330/339], [94mLoss[0m : 1.83886

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.315, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21957
[1mStep[0m  [33/339], [94mLoss[0m : 2.89032
[1mStep[0m  [66/339], [94mLoss[0m : 2.46741
[1mStep[0m  [99/339], [94mLoss[0m : 1.88185
[1mStep[0m  [132/339], [94mLoss[0m : 2.06715
[1mStep[0m  [165/339], [94mLoss[0m : 2.39085
[1mStep[0m  [198/339], [94mLoss[0m : 2.49974
[1mStep[0m  [231/339], [94mLoss[0m : 2.33819
[1mStep[0m  [264/339], [94mLoss[0m : 1.95727
[1mStep[0m  [297/339], [94mLoss[0m : 2.08372
[1mStep[0m  [330/339], [94mLoss[0m : 3.26017

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13956
[1mStep[0m  [33/339], [94mLoss[0m : 2.30841
[1mStep[0m  [66/339], [94mLoss[0m : 2.77548
[1mStep[0m  [99/339], [94mLoss[0m : 2.33744
[1mStep[0m  [132/339], [94mLoss[0m : 1.95309
[1mStep[0m  [165/339], [94mLoss[0m : 2.47131
[1mStep[0m  [198/339], [94mLoss[0m : 1.89956
[1mStep[0m  [231/339], [94mLoss[0m : 2.26412
[1mStep[0m  [264/339], [94mLoss[0m : 3.30278
[1mStep[0m  [297/339], [94mLoss[0m : 2.23196
[1mStep[0m  [330/339], [94mLoss[0m : 2.60392

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35193
[1mStep[0m  [33/339], [94mLoss[0m : 2.20722
[1mStep[0m  [66/339], [94mLoss[0m : 2.27004
[1mStep[0m  [99/339], [94mLoss[0m : 1.73938
[1mStep[0m  [132/339], [94mLoss[0m : 1.72951
[1mStep[0m  [165/339], [94mLoss[0m : 2.61134
[1mStep[0m  [198/339], [94mLoss[0m : 2.99694
[1mStep[0m  [231/339], [94mLoss[0m : 3.21894
[1mStep[0m  [264/339], [94mLoss[0m : 3.11331
[1mStep[0m  [297/339], [94mLoss[0m : 2.26686
[1mStep[0m  [330/339], [94mLoss[0m : 2.60746

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.04608
[1mStep[0m  [33/339], [94mLoss[0m : 2.15217
[1mStep[0m  [66/339], [94mLoss[0m : 1.89301
[1mStep[0m  [99/339], [94mLoss[0m : 2.29507
[1mStep[0m  [132/339], [94mLoss[0m : 1.62245
[1mStep[0m  [165/339], [94mLoss[0m : 1.63197
[1mStep[0m  [198/339], [94mLoss[0m : 1.51062
[1mStep[0m  [231/339], [94mLoss[0m : 2.22562
[1mStep[0m  [264/339], [94mLoss[0m : 2.14501
[1mStep[0m  [297/339], [94mLoss[0m : 1.89347
[1mStep[0m  [330/339], [94mLoss[0m : 2.35256

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90400
[1mStep[0m  [33/339], [94mLoss[0m : 2.07974
[1mStep[0m  [66/339], [94mLoss[0m : 1.83508
[1mStep[0m  [99/339], [94mLoss[0m : 2.33782
[1mStep[0m  [132/339], [94mLoss[0m : 2.57815
[1mStep[0m  [165/339], [94mLoss[0m : 1.77676
[1mStep[0m  [198/339], [94mLoss[0m : 1.95350
[1mStep[0m  [231/339], [94mLoss[0m : 1.71671
[1mStep[0m  [264/339], [94mLoss[0m : 1.82166
[1mStep[0m  [297/339], [94mLoss[0m : 2.17572
[1mStep[0m  [330/339], [94mLoss[0m : 1.83153

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84987
[1mStep[0m  [33/339], [94mLoss[0m : 2.62607
[1mStep[0m  [66/339], [94mLoss[0m : 2.08240
[1mStep[0m  [99/339], [94mLoss[0m : 1.94330
[1mStep[0m  [132/339], [94mLoss[0m : 2.10658
[1mStep[0m  [165/339], [94mLoss[0m : 2.50318
[1mStep[0m  [198/339], [94mLoss[0m : 2.10797
[1mStep[0m  [231/339], [94mLoss[0m : 1.55790
[1mStep[0m  [264/339], [94mLoss[0m : 1.93144
[1mStep[0m  [297/339], [94mLoss[0m : 2.10337
[1mStep[0m  [330/339], [94mLoss[0m : 2.57794

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87219
[1mStep[0m  [33/339], [94mLoss[0m : 2.18594
[1mStep[0m  [66/339], [94mLoss[0m : 2.24766
[1mStep[0m  [99/339], [94mLoss[0m : 1.93478
[1mStep[0m  [132/339], [94mLoss[0m : 2.03142
[1mStep[0m  [165/339], [94mLoss[0m : 1.69639
[1mStep[0m  [198/339], [94mLoss[0m : 1.57472
[1mStep[0m  [231/339], [94mLoss[0m : 2.38813
[1mStep[0m  [264/339], [94mLoss[0m : 1.86306
[1mStep[0m  [297/339], [94mLoss[0m : 1.91345
[1mStep[0m  [330/339], [94mLoss[0m : 2.06188

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88091
[1mStep[0m  [33/339], [94mLoss[0m : 2.16369
[1mStep[0m  [66/339], [94mLoss[0m : 1.40982
[1mStep[0m  [99/339], [94mLoss[0m : 1.57557
[1mStep[0m  [132/339], [94mLoss[0m : 1.99484
[1mStep[0m  [165/339], [94mLoss[0m : 1.85913
[1mStep[0m  [198/339], [94mLoss[0m : 1.90334
[1mStep[0m  [231/339], [94mLoss[0m : 2.40048
[1mStep[0m  [264/339], [94mLoss[0m : 1.70084
[1mStep[0m  [297/339], [94mLoss[0m : 1.98480
[1mStep[0m  [330/339], [94mLoss[0m : 2.06181

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79203
[1mStep[0m  [33/339], [94mLoss[0m : 1.53509
[1mStep[0m  [66/339], [94mLoss[0m : 2.00807
[1mStep[0m  [99/339], [94mLoss[0m : 1.72696
[1mStep[0m  [132/339], [94mLoss[0m : 2.18750
[1mStep[0m  [165/339], [94mLoss[0m : 2.09228
[1mStep[0m  [198/339], [94mLoss[0m : 2.26752
[1mStep[0m  [231/339], [94mLoss[0m : 1.95418
[1mStep[0m  [264/339], [94mLoss[0m : 2.81675
[1mStep[0m  [297/339], [94mLoss[0m : 2.10861
[1mStep[0m  [330/339], [94mLoss[0m : 1.88705

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56543
[1mStep[0m  [33/339], [94mLoss[0m : 1.34047
[1mStep[0m  [66/339], [94mLoss[0m : 1.46334
[1mStep[0m  [99/339], [94mLoss[0m : 1.83168
[1mStep[0m  [132/339], [94mLoss[0m : 2.06996
[1mStep[0m  [165/339], [94mLoss[0m : 2.31613
[1mStep[0m  [198/339], [94mLoss[0m : 2.23073
[1mStep[0m  [231/339], [94mLoss[0m : 1.54642
[1mStep[0m  [264/339], [94mLoss[0m : 2.22704
[1mStep[0m  [297/339], [94mLoss[0m : 2.17562
[1mStep[0m  [330/339], [94mLoss[0m : 2.05120

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02349
[1mStep[0m  [33/339], [94mLoss[0m : 2.13449
[1mStep[0m  [66/339], [94mLoss[0m : 2.47604
[1mStep[0m  [99/339], [94mLoss[0m : 1.86376
[1mStep[0m  [132/339], [94mLoss[0m : 2.07838
[1mStep[0m  [165/339], [94mLoss[0m : 2.52647
[1mStep[0m  [198/339], [94mLoss[0m : 1.68789
[1mStep[0m  [231/339], [94mLoss[0m : 1.45433
[1mStep[0m  [264/339], [94mLoss[0m : 2.12040
[1mStep[0m  [297/339], [94mLoss[0m : 1.98594
[1mStep[0m  [330/339], [94mLoss[0m : 2.05913

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.58817
[1mStep[0m  [33/339], [94mLoss[0m : 1.89723
[1mStep[0m  [66/339], [94mLoss[0m : 1.93916
[1mStep[0m  [99/339], [94mLoss[0m : 1.56150
[1mStep[0m  [132/339], [94mLoss[0m : 1.11231
[1mStep[0m  [165/339], [94mLoss[0m : 1.61153
[1mStep[0m  [198/339], [94mLoss[0m : 1.90576
[1mStep[0m  [231/339], [94mLoss[0m : 2.05350
[1mStep[0m  [264/339], [94mLoss[0m : 1.37509
[1mStep[0m  [297/339], [94mLoss[0m : 2.84880
[1mStep[0m  [330/339], [94mLoss[0m : 1.95564

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68313
[1mStep[0m  [33/339], [94mLoss[0m : 1.30236
[1mStep[0m  [66/339], [94mLoss[0m : 2.38865
[1mStep[0m  [99/339], [94mLoss[0m : 1.83816
[1mStep[0m  [132/339], [94mLoss[0m : 1.95894
[1mStep[0m  [165/339], [94mLoss[0m : 1.70615
[1mStep[0m  [198/339], [94mLoss[0m : 2.11848
[1mStep[0m  [231/339], [94mLoss[0m : 2.20494
[1mStep[0m  [264/339], [94mLoss[0m : 1.85209
[1mStep[0m  [297/339], [94mLoss[0m : 2.16388
[1mStep[0m  [330/339], [94mLoss[0m : 1.70940

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63136
[1mStep[0m  [33/339], [94mLoss[0m : 1.67806
[1mStep[0m  [66/339], [94mLoss[0m : 2.21830
[1mStep[0m  [99/339], [94mLoss[0m : 2.18230
[1mStep[0m  [132/339], [94mLoss[0m : 1.80128
[1mStep[0m  [165/339], [94mLoss[0m : 1.94101
[1mStep[0m  [198/339], [94mLoss[0m : 1.98960
[1mStep[0m  [231/339], [94mLoss[0m : 1.94883
[1mStep[0m  [264/339], [94mLoss[0m : 1.67950
[1mStep[0m  [297/339], [94mLoss[0m : 2.01385
[1mStep[0m  [330/339], [94mLoss[0m : 1.81447

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63514
[1mStep[0m  [33/339], [94mLoss[0m : 1.78552
[1mStep[0m  [66/339], [94mLoss[0m : 1.71635
[1mStep[0m  [99/339], [94mLoss[0m : 1.68957
[1mStep[0m  [132/339], [94mLoss[0m : 1.66332
[1mStep[0m  [165/339], [94mLoss[0m : 1.84913
[1mStep[0m  [198/339], [94mLoss[0m : 1.55617
[1mStep[0m  [231/339], [94mLoss[0m : 2.03092
[1mStep[0m  [264/339], [94mLoss[0m : 1.67580
[1mStep[0m  [297/339], [94mLoss[0m : 1.76250
[1mStep[0m  [330/339], [94mLoss[0m : 1.56726

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52854
[1mStep[0m  [33/339], [94mLoss[0m : 1.83950
[1mStep[0m  [66/339], [94mLoss[0m : 1.95610
[1mStep[0m  [99/339], [94mLoss[0m : 1.64485
[1mStep[0m  [132/339], [94mLoss[0m : 1.73508
[1mStep[0m  [165/339], [94mLoss[0m : 1.67700
[1mStep[0m  [198/339], [94mLoss[0m : 1.83817
[1mStep[0m  [231/339], [94mLoss[0m : 1.45679
[1mStep[0m  [264/339], [94mLoss[0m : 1.26395
[1mStep[0m  [297/339], [94mLoss[0m : 2.28119
[1mStep[0m  [330/339], [94mLoss[0m : 1.75042

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24940
[1mStep[0m  [33/339], [94mLoss[0m : 1.44408
[1mStep[0m  [66/339], [94mLoss[0m : 1.55087
[1mStep[0m  [99/339], [94mLoss[0m : 2.10016
[1mStep[0m  [132/339], [94mLoss[0m : 2.04041
[1mStep[0m  [165/339], [94mLoss[0m : 1.97873
[1mStep[0m  [198/339], [94mLoss[0m : 1.92332
[1mStep[0m  [231/339], [94mLoss[0m : 1.51776
[1mStep[0m  [264/339], [94mLoss[0m : 1.68792
[1mStep[0m  [297/339], [94mLoss[0m : 1.94035
[1mStep[0m  [330/339], [94mLoss[0m : 1.86181

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01246
[1mStep[0m  [33/339], [94mLoss[0m : 1.85224
[1mStep[0m  [66/339], [94mLoss[0m : 1.54961
[1mStep[0m  [99/339], [94mLoss[0m : 2.19371
[1mStep[0m  [132/339], [94mLoss[0m : 1.78486
[1mStep[0m  [165/339], [94mLoss[0m : 1.99280
[1mStep[0m  [198/339], [94mLoss[0m : 1.54912
[1mStep[0m  [231/339], [94mLoss[0m : 1.76864
[1mStep[0m  [264/339], [94mLoss[0m : 2.36988
[1mStep[0m  [297/339], [94mLoss[0m : 2.18552
[1mStep[0m  [330/339], [94mLoss[0m : 1.92384

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81260
[1mStep[0m  [33/339], [94mLoss[0m : 1.67890
[1mStep[0m  [66/339], [94mLoss[0m : 2.14040
[1mStep[0m  [99/339], [94mLoss[0m : 1.62772
[1mStep[0m  [132/339], [94mLoss[0m : 2.52177
[1mStep[0m  [165/339], [94mLoss[0m : 2.08266
[1mStep[0m  [198/339], [94mLoss[0m : 1.87885
[1mStep[0m  [231/339], [94mLoss[0m : 1.66531
[1mStep[0m  [264/339], [94mLoss[0m : 1.96683
[1mStep[0m  [297/339], [94mLoss[0m : 1.77712
[1mStep[0m  [330/339], [94mLoss[0m : 1.82097

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.644, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62948
[1mStep[0m  [33/339], [94mLoss[0m : 1.78770
[1mStep[0m  [66/339], [94mLoss[0m : 1.60273
[1mStep[0m  [99/339], [94mLoss[0m : 1.72063
[1mStep[0m  [132/339], [94mLoss[0m : 1.98884
[1mStep[0m  [165/339], [94mLoss[0m : 1.91759
[1mStep[0m  [198/339], [94mLoss[0m : 2.32869
[1mStep[0m  [231/339], [94mLoss[0m : 1.84705
[1mStep[0m  [264/339], [94mLoss[0m : 1.43299
[1mStep[0m  [297/339], [94mLoss[0m : 1.76418
[1mStep[0m  [330/339], [94mLoss[0m : 1.41868

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.551, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89873
[1mStep[0m  [33/339], [94mLoss[0m : 1.63742
[1mStep[0m  [66/339], [94mLoss[0m : 1.61993
[1mStep[0m  [99/339], [94mLoss[0m : 1.50482
[1mStep[0m  [132/339], [94mLoss[0m : 1.86030
[1mStep[0m  [165/339], [94mLoss[0m : 1.42481
[1mStep[0m  [198/339], [94mLoss[0m : 1.74238
[1mStep[0m  [231/339], [94mLoss[0m : 1.60282
[1mStep[0m  [264/339], [94mLoss[0m : 1.56100
[1mStep[0m  [297/339], [94mLoss[0m : 1.64567
[1mStep[0m  [330/339], [94mLoss[0m : 1.83304

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.589, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32462
[1mStep[0m  [33/339], [94mLoss[0m : 1.25742
[1mStep[0m  [66/339], [94mLoss[0m : 1.79856
[1mStep[0m  [99/339], [94mLoss[0m : 1.82252
[1mStep[0m  [132/339], [94mLoss[0m : 2.25708
[1mStep[0m  [165/339], [94mLoss[0m : 1.79482
[1mStep[0m  [198/339], [94mLoss[0m : 1.43699
[1mStep[0m  [231/339], [94mLoss[0m : 1.41116
[1mStep[0m  [264/339], [94mLoss[0m : 1.44840
[1mStep[0m  [297/339], [94mLoss[0m : 1.20370
[1mStep[0m  [330/339], [94mLoss[0m : 1.29658

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72888
[1mStep[0m  [33/339], [94mLoss[0m : 1.82472
[1mStep[0m  [66/339], [94mLoss[0m : 1.79702
[1mStep[0m  [99/339], [94mLoss[0m : 1.49828
[1mStep[0m  [132/339], [94mLoss[0m : 1.55747
[1mStep[0m  [165/339], [94mLoss[0m : 1.35806
[1mStep[0m  [198/339], [94mLoss[0m : 1.83146
[1mStep[0m  [231/339], [94mLoss[0m : 2.08545
[1mStep[0m  [264/339], [94mLoss[0m : 2.07313
[1mStep[0m  [297/339], [94mLoss[0m : 2.64946
[1mStep[0m  [330/339], [94mLoss[0m : 1.66313

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38758
[1mStep[0m  [33/339], [94mLoss[0m : 1.29562
[1mStep[0m  [66/339], [94mLoss[0m : 1.97650
[1mStep[0m  [99/339], [94mLoss[0m : 1.40491
[1mStep[0m  [132/339], [94mLoss[0m : 1.96240
[1mStep[0m  [165/339], [94mLoss[0m : 1.36615
[1mStep[0m  [198/339], [94mLoss[0m : 1.77824
[1mStep[0m  [231/339], [94mLoss[0m : 1.30493
[1mStep[0m  [264/339], [94mLoss[0m : 1.73209
[1mStep[0m  [297/339], [94mLoss[0m : 1.45511
[1mStep[0m  [330/339], [94mLoss[0m : 1.68392

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.10625
[1mStep[0m  [33/339], [94mLoss[0m : 1.41326
[1mStep[0m  [66/339], [94mLoss[0m : 1.50545
[1mStep[0m  [99/339], [94mLoss[0m : 1.27654
[1mStep[0m  [132/339], [94mLoss[0m : 1.61690
[1mStep[0m  [165/339], [94mLoss[0m : 2.20232
[1mStep[0m  [198/339], [94mLoss[0m : 1.70023
[1mStep[0m  [231/339], [94mLoss[0m : 2.06743
[1mStep[0m  [264/339], [94mLoss[0m : 1.30892
[1mStep[0m  [297/339], [94mLoss[0m : 1.53310
[1mStep[0m  [330/339], [94mLoss[0m : 1.54939

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.634, [92mTest[0m: 2.528, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42314
[1mStep[0m  [33/339], [94mLoss[0m : 1.30306
[1mStep[0m  [66/339], [94mLoss[0m : 1.18208
[1mStep[0m  [99/339], [94mLoss[0m : 1.30170
[1mStep[0m  [132/339], [94mLoss[0m : 1.61340
[1mStep[0m  [165/339], [94mLoss[0m : 1.53396
[1mStep[0m  [198/339], [94mLoss[0m : 1.91251
[1mStep[0m  [231/339], [94mLoss[0m : 1.82539
[1mStep[0m  [264/339], [94mLoss[0m : 1.94668
[1mStep[0m  [297/339], [94mLoss[0m : 1.32835
[1mStep[0m  [330/339], [94mLoss[0m : 1.63491

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.617, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38726
[1mStep[0m  [33/339], [94mLoss[0m : 1.87836
[1mStep[0m  [66/339], [94mLoss[0m : 1.35629
[1mStep[0m  [99/339], [94mLoss[0m : 0.98245
[1mStep[0m  [132/339], [94mLoss[0m : 1.52478
[1mStep[0m  [165/339], [94mLoss[0m : 1.87703
[1mStep[0m  [198/339], [94mLoss[0m : 1.35934
[1mStep[0m  [231/339], [94mLoss[0m : 1.84424
[1mStep[0m  [264/339], [94mLoss[0m : 1.77599
[1mStep[0m  [297/339], [94mLoss[0m : 1.59316
[1mStep[0m  [330/339], [94mLoss[0m : 1.38535

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79760
[1mStep[0m  [33/339], [94mLoss[0m : 1.80239
[1mStep[0m  [66/339], [94mLoss[0m : 1.54111
[1mStep[0m  [99/339], [94mLoss[0m : 1.70390
[1mStep[0m  [132/339], [94mLoss[0m : 1.77203
[1mStep[0m  [165/339], [94mLoss[0m : 1.52647
[1mStep[0m  [198/339], [94mLoss[0m : 1.62860
[1mStep[0m  [231/339], [94mLoss[0m : 1.90202
[1mStep[0m  [264/339], [94mLoss[0m : 1.48025
[1mStep[0m  [297/339], [94mLoss[0m : 1.45734
[1mStep[0m  [330/339], [94mLoss[0m : 1.35820

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.55974
[1mStep[0m  [33/339], [94mLoss[0m : 1.39804
[1mStep[0m  [66/339], [94mLoss[0m : 1.83595
[1mStep[0m  [99/339], [94mLoss[0m : 1.23969
[1mStep[0m  [132/339], [94mLoss[0m : 1.31788
[1mStep[0m  [165/339], [94mLoss[0m : 1.66572
[1mStep[0m  [198/339], [94mLoss[0m : 2.06504
[1mStep[0m  [231/339], [94mLoss[0m : 1.15336
[1mStep[0m  [264/339], [94mLoss[0m : 1.64347
[1mStep[0m  [297/339], [94mLoss[0m : 1.59746
[1mStep[0m  [330/339], [94mLoss[0m : 1.72031

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.534, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.527
====================================

Phase 2 - Evaluation MAE:  2.5267164654436365
MAE score P1       2.31487
MAE score P2      2.526716
loss               1.58775
learning_rate     0.007525
batch_size              32
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 10.57977
[1mStep[0m  [33/339], [94mLoss[0m : 6.11338
[1mStep[0m  [66/339], [94mLoss[0m : 3.54810
[1mStep[0m  [99/339], [94mLoss[0m : 3.55312
[1mStep[0m  [132/339], [94mLoss[0m : 2.88740
[1mStep[0m  [165/339], [94mLoss[0m : 2.15683
[1mStep[0m  [198/339], [94mLoss[0m : 3.15923
[1mStep[0m  [231/339], [94mLoss[0m : 2.90484
[1mStep[0m  [264/339], [94mLoss[0m : 2.93566
[1mStep[0m  [297/339], [94mLoss[0m : 2.28329
[1mStep[0m  [330/339], [94mLoss[0m : 2.65613

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.644, [92mTest[0m: 11.040, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10920
[1mStep[0m  [33/339], [94mLoss[0m : 2.64524
[1mStep[0m  [66/339], [94mLoss[0m : 2.21808
[1mStep[0m  [99/339], [94mLoss[0m : 3.08863
[1mStep[0m  [132/339], [94mLoss[0m : 2.08821
[1mStep[0m  [165/339], [94mLoss[0m : 2.56327
[1mStep[0m  [198/339], [94mLoss[0m : 2.15754
[1mStep[0m  [231/339], [94mLoss[0m : 2.38426
[1mStep[0m  [264/339], [94mLoss[0m : 3.15682
[1mStep[0m  [297/339], [94mLoss[0m : 3.36573
[1mStep[0m  [330/339], [94mLoss[0m : 2.84582

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.750, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83982
[1mStep[0m  [33/339], [94mLoss[0m : 2.62128
[1mStep[0m  [66/339], [94mLoss[0m : 2.89885
[1mStep[0m  [99/339], [94mLoss[0m : 2.24552
[1mStep[0m  [132/339], [94mLoss[0m : 2.03584
[1mStep[0m  [165/339], [94mLoss[0m : 2.84236
[1mStep[0m  [198/339], [94mLoss[0m : 2.73476
[1mStep[0m  [231/339], [94mLoss[0m : 3.01897
[1mStep[0m  [264/339], [94mLoss[0m : 2.23799
[1mStep[0m  [297/339], [94mLoss[0m : 2.78995
[1mStep[0m  [330/339], [94mLoss[0m : 2.26021

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41486
[1mStep[0m  [33/339], [94mLoss[0m : 2.21630
[1mStep[0m  [66/339], [94mLoss[0m : 2.41243
[1mStep[0m  [99/339], [94mLoss[0m : 2.03918
[1mStep[0m  [132/339], [94mLoss[0m : 3.17226
[1mStep[0m  [165/339], [94mLoss[0m : 2.52101
[1mStep[0m  [198/339], [94mLoss[0m : 2.08287
[1mStep[0m  [231/339], [94mLoss[0m : 2.51881
[1mStep[0m  [264/339], [94mLoss[0m : 2.17726
[1mStep[0m  [297/339], [94mLoss[0m : 3.02306
[1mStep[0m  [330/339], [94mLoss[0m : 2.33031

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54276
[1mStep[0m  [33/339], [94mLoss[0m : 2.91373
[1mStep[0m  [66/339], [94mLoss[0m : 3.33163
[1mStep[0m  [99/339], [94mLoss[0m : 2.89292
[1mStep[0m  [132/339], [94mLoss[0m : 2.74918
[1mStep[0m  [165/339], [94mLoss[0m : 2.68831
[1mStep[0m  [198/339], [94mLoss[0m : 3.47757
[1mStep[0m  [231/339], [94mLoss[0m : 2.33533
[1mStep[0m  [264/339], [94mLoss[0m : 2.98999
[1mStep[0m  [297/339], [94mLoss[0m : 2.62050
[1mStep[0m  [330/339], [94mLoss[0m : 3.08368

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.42830
[1mStep[0m  [33/339], [94mLoss[0m : 2.46272
[1mStep[0m  [66/339], [94mLoss[0m : 2.80074
[1mStep[0m  [99/339], [94mLoss[0m : 3.36994
[1mStep[0m  [132/339], [94mLoss[0m : 2.74989
[1mStep[0m  [165/339], [94mLoss[0m : 3.09174
[1mStep[0m  [198/339], [94mLoss[0m : 3.44660
[1mStep[0m  [231/339], [94mLoss[0m : 2.55483
[1mStep[0m  [264/339], [94mLoss[0m : 2.92428
[1mStep[0m  [297/339], [94mLoss[0m : 1.96386
[1mStep[0m  [330/339], [94mLoss[0m : 2.56934

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25053
[1mStep[0m  [33/339], [94mLoss[0m : 3.08164
[1mStep[0m  [66/339], [94mLoss[0m : 3.09771
[1mStep[0m  [99/339], [94mLoss[0m : 2.92335
[1mStep[0m  [132/339], [94mLoss[0m : 2.57613
[1mStep[0m  [165/339], [94mLoss[0m : 2.30270
[1mStep[0m  [198/339], [94mLoss[0m : 2.42643
[1mStep[0m  [231/339], [94mLoss[0m : 2.91177
[1mStep[0m  [264/339], [94mLoss[0m : 3.12771
[1mStep[0m  [297/339], [94mLoss[0m : 2.47772
[1mStep[0m  [330/339], [94mLoss[0m : 3.30088

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.74063
[1mStep[0m  [33/339], [94mLoss[0m : 2.42752
[1mStep[0m  [66/339], [94mLoss[0m : 2.86206
[1mStep[0m  [99/339], [94mLoss[0m : 2.32444
[1mStep[0m  [132/339], [94mLoss[0m : 2.30904
[1mStep[0m  [165/339], [94mLoss[0m : 2.20224
[1mStep[0m  [198/339], [94mLoss[0m : 1.96354
[1mStep[0m  [231/339], [94mLoss[0m : 2.70389
[1mStep[0m  [264/339], [94mLoss[0m : 2.38928
[1mStep[0m  [297/339], [94mLoss[0m : 2.70621
[1mStep[0m  [330/339], [94mLoss[0m : 2.19125

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.317, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83077
[1mStep[0m  [33/339], [94mLoss[0m : 2.44158
[1mStep[0m  [66/339], [94mLoss[0m : 2.84874
[1mStep[0m  [99/339], [94mLoss[0m : 2.13596
[1mStep[0m  [132/339], [94mLoss[0m : 2.32962
[1mStep[0m  [165/339], [94mLoss[0m : 2.48879
[1mStep[0m  [198/339], [94mLoss[0m : 2.77701
[1mStep[0m  [231/339], [94mLoss[0m : 2.21408
[1mStep[0m  [264/339], [94mLoss[0m : 2.39232
[1mStep[0m  [297/339], [94mLoss[0m : 2.94318
[1mStep[0m  [330/339], [94mLoss[0m : 2.47082

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66540
[1mStep[0m  [33/339], [94mLoss[0m : 2.39973
[1mStep[0m  [66/339], [94mLoss[0m : 2.51874
[1mStep[0m  [99/339], [94mLoss[0m : 2.62727
[1mStep[0m  [132/339], [94mLoss[0m : 2.45909
[1mStep[0m  [165/339], [94mLoss[0m : 3.25430
[1mStep[0m  [198/339], [94mLoss[0m : 2.69360
[1mStep[0m  [231/339], [94mLoss[0m : 2.82923
[1mStep[0m  [264/339], [94mLoss[0m : 2.51042
[1mStep[0m  [297/339], [94mLoss[0m : 2.86584
[1mStep[0m  [330/339], [94mLoss[0m : 2.66047

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.314, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21545
[1mStep[0m  [33/339], [94mLoss[0m : 2.56167
[1mStep[0m  [66/339], [94mLoss[0m : 2.15955
[1mStep[0m  [99/339], [94mLoss[0m : 2.68794
[1mStep[0m  [132/339], [94mLoss[0m : 2.24781
[1mStep[0m  [165/339], [94mLoss[0m : 1.99793
[1mStep[0m  [198/339], [94mLoss[0m : 2.82011
[1mStep[0m  [231/339], [94mLoss[0m : 2.49605
[1mStep[0m  [264/339], [94mLoss[0m : 3.19328
[1mStep[0m  [297/339], [94mLoss[0m : 2.19112
[1mStep[0m  [330/339], [94mLoss[0m : 1.72273

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54201
[1mStep[0m  [33/339], [94mLoss[0m : 2.34312
[1mStep[0m  [66/339], [94mLoss[0m : 2.85615
[1mStep[0m  [99/339], [94mLoss[0m : 1.95702
[1mStep[0m  [132/339], [94mLoss[0m : 2.70843
[1mStep[0m  [165/339], [94mLoss[0m : 3.08747
[1mStep[0m  [198/339], [94mLoss[0m : 2.66846
[1mStep[0m  [231/339], [94mLoss[0m : 2.08446
[1mStep[0m  [264/339], [94mLoss[0m : 2.65916
[1mStep[0m  [297/339], [94mLoss[0m : 2.68194
[1mStep[0m  [330/339], [94mLoss[0m : 2.80307

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.305, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18539
[1mStep[0m  [33/339], [94mLoss[0m : 2.67416
[1mStep[0m  [66/339], [94mLoss[0m : 2.82365
[1mStep[0m  [99/339], [94mLoss[0m : 2.46532
[1mStep[0m  [132/339], [94mLoss[0m : 2.31943
[1mStep[0m  [165/339], [94mLoss[0m : 3.07655
[1mStep[0m  [198/339], [94mLoss[0m : 2.47179
[1mStep[0m  [231/339], [94mLoss[0m : 2.52181
[1mStep[0m  [264/339], [94mLoss[0m : 2.70844
[1mStep[0m  [297/339], [94mLoss[0m : 2.14509
[1mStep[0m  [330/339], [94mLoss[0m : 2.39233

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.303, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51665
[1mStep[0m  [33/339], [94mLoss[0m : 1.78828
[1mStep[0m  [66/339], [94mLoss[0m : 2.74375
[1mStep[0m  [99/339], [94mLoss[0m : 2.54615
[1mStep[0m  [132/339], [94mLoss[0m : 1.75610
[1mStep[0m  [165/339], [94mLoss[0m : 1.93639
[1mStep[0m  [198/339], [94mLoss[0m : 2.66126
[1mStep[0m  [231/339], [94mLoss[0m : 2.94831
[1mStep[0m  [264/339], [94mLoss[0m : 2.31708
[1mStep[0m  [297/339], [94mLoss[0m : 2.10895
[1mStep[0m  [330/339], [94mLoss[0m : 2.23315

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.15311
[1mStep[0m  [33/339], [94mLoss[0m : 2.61737
[1mStep[0m  [66/339], [94mLoss[0m : 3.02254
[1mStep[0m  [99/339], [94mLoss[0m : 2.98906
[1mStep[0m  [132/339], [94mLoss[0m : 2.15573
[1mStep[0m  [165/339], [94mLoss[0m : 2.81207
[1mStep[0m  [198/339], [94mLoss[0m : 2.71288
[1mStep[0m  [231/339], [94mLoss[0m : 2.60784
[1mStep[0m  [264/339], [94mLoss[0m : 2.47359
[1mStep[0m  [297/339], [94mLoss[0m : 1.49910
[1mStep[0m  [330/339], [94mLoss[0m : 2.81678

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30165
[1mStep[0m  [33/339], [94mLoss[0m : 2.63175
[1mStep[0m  [66/339], [94mLoss[0m : 2.00358
[1mStep[0m  [99/339], [94mLoss[0m : 2.54367
[1mStep[0m  [132/339], [94mLoss[0m : 2.31168
[1mStep[0m  [165/339], [94mLoss[0m : 1.95696
[1mStep[0m  [198/339], [94mLoss[0m : 2.43789
[1mStep[0m  [231/339], [94mLoss[0m : 2.73081
[1mStep[0m  [264/339], [94mLoss[0m : 2.44497
[1mStep[0m  [297/339], [94mLoss[0m : 2.83629
[1mStep[0m  [330/339], [94mLoss[0m : 2.26938

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.301, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44992
[1mStep[0m  [33/339], [94mLoss[0m : 2.26872
[1mStep[0m  [66/339], [94mLoss[0m : 2.70715
[1mStep[0m  [99/339], [94mLoss[0m : 2.37043
[1mStep[0m  [132/339], [94mLoss[0m : 2.55863
[1mStep[0m  [165/339], [94mLoss[0m : 2.75452
[1mStep[0m  [198/339], [94mLoss[0m : 2.78393
[1mStep[0m  [231/339], [94mLoss[0m : 2.39563
[1mStep[0m  [264/339], [94mLoss[0m : 1.70217
[1mStep[0m  [297/339], [94mLoss[0m : 2.72745
[1mStep[0m  [330/339], [94mLoss[0m : 2.46378

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40122
[1mStep[0m  [33/339], [94mLoss[0m : 2.53243
[1mStep[0m  [66/339], [94mLoss[0m : 1.94635
[1mStep[0m  [99/339], [94mLoss[0m : 2.67446
[1mStep[0m  [132/339], [94mLoss[0m : 2.31193
[1mStep[0m  [165/339], [94mLoss[0m : 2.59477
[1mStep[0m  [198/339], [94mLoss[0m : 2.55151
[1mStep[0m  [231/339], [94mLoss[0m : 2.76166
[1mStep[0m  [264/339], [94mLoss[0m : 2.29299
[1mStep[0m  [297/339], [94mLoss[0m : 2.34438
[1mStep[0m  [330/339], [94mLoss[0m : 1.98775

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.304, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29408
[1mStep[0m  [33/339], [94mLoss[0m : 3.16225
[1mStep[0m  [66/339], [94mLoss[0m : 3.32188
[1mStep[0m  [99/339], [94mLoss[0m : 2.57001
[1mStep[0m  [132/339], [94mLoss[0m : 2.63305
[1mStep[0m  [165/339], [94mLoss[0m : 1.92838
[1mStep[0m  [198/339], [94mLoss[0m : 2.68981
[1mStep[0m  [231/339], [94mLoss[0m : 2.63392
[1mStep[0m  [264/339], [94mLoss[0m : 2.97811
[1mStep[0m  [297/339], [94mLoss[0m : 3.08917
[1mStep[0m  [330/339], [94mLoss[0m : 2.21997

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.292, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43558
[1mStep[0m  [33/339], [94mLoss[0m : 3.82673
[1mStep[0m  [66/339], [94mLoss[0m : 2.33162
[1mStep[0m  [99/339], [94mLoss[0m : 2.68883
[1mStep[0m  [132/339], [94mLoss[0m : 1.76802
[1mStep[0m  [165/339], [94mLoss[0m : 2.59429
[1mStep[0m  [198/339], [94mLoss[0m : 2.55001
[1mStep[0m  [231/339], [94mLoss[0m : 3.36755
[1mStep[0m  [264/339], [94mLoss[0m : 2.44375
[1mStep[0m  [297/339], [94mLoss[0m : 2.49481
[1mStep[0m  [330/339], [94mLoss[0m : 2.40675

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33564
[1mStep[0m  [33/339], [94mLoss[0m : 1.93179
[1mStep[0m  [66/339], [94mLoss[0m : 2.20710
[1mStep[0m  [99/339], [94mLoss[0m : 2.05537
[1mStep[0m  [132/339], [94mLoss[0m : 2.85356
[1mStep[0m  [165/339], [94mLoss[0m : 2.70751
[1mStep[0m  [198/339], [94mLoss[0m : 2.73136
[1mStep[0m  [231/339], [94mLoss[0m : 2.53798
[1mStep[0m  [264/339], [94mLoss[0m : 2.84902
[1mStep[0m  [297/339], [94mLoss[0m : 2.63229
[1mStep[0m  [330/339], [94mLoss[0m : 2.17488

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.299, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63783
[1mStep[0m  [33/339], [94mLoss[0m : 2.72986
[1mStep[0m  [66/339], [94mLoss[0m : 2.99045
[1mStep[0m  [99/339], [94mLoss[0m : 2.14859
[1mStep[0m  [132/339], [94mLoss[0m : 2.40483
[1mStep[0m  [165/339], [94mLoss[0m : 2.55047
[1mStep[0m  [198/339], [94mLoss[0m : 2.63139
[1mStep[0m  [231/339], [94mLoss[0m : 1.97069
[1mStep[0m  [264/339], [94mLoss[0m : 2.74774
[1mStep[0m  [297/339], [94mLoss[0m : 2.80540
[1mStep[0m  [330/339], [94mLoss[0m : 2.49176

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47956
[1mStep[0m  [33/339], [94mLoss[0m : 2.50739
[1mStep[0m  [66/339], [94mLoss[0m : 2.10991
[1mStep[0m  [99/339], [94mLoss[0m : 2.40104
[1mStep[0m  [132/339], [94mLoss[0m : 2.37215
[1mStep[0m  [165/339], [94mLoss[0m : 2.22534
[1mStep[0m  [198/339], [94mLoss[0m : 2.18350
[1mStep[0m  [231/339], [94mLoss[0m : 2.70151
[1mStep[0m  [264/339], [94mLoss[0m : 2.16989
[1mStep[0m  [297/339], [94mLoss[0m : 2.34200
[1mStep[0m  [330/339], [94mLoss[0m : 2.15917

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.309, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12858
[1mStep[0m  [33/339], [94mLoss[0m : 2.65081
[1mStep[0m  [66/339], [94mLoss[0m : 2.73178
[1mStep[0m  [99/339], [94mLoss[0m : 2.72813
[1mStep[0m  [132/339], [94mLoss[0m : 2.78755
[1mStep[0m  [165/339], [94mLoss[0m : 2.33169
[1mStep[0m  [198/339], [94mLoss[0m : 2.53666
[1mStep[0m  [231/339], [94mLoss[0m : 2.83378
[1mStep[0m  [264/339], [94mLoss[0m : 2.64540
[1mStep[0m  [297/339], [94mLoss[0m : 2.16555
[1mStep[0m  [330/339], [94mLoss[0m : 2.72342

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.300, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27990
[1mStep[0m  [33/339], [94mLoss[0m : 2.87175
[1mStep[0m  [66/339], [94mLoss[0m : 2.64263
[1mStep[0m  [99/339], [94mLoss[0m : 2.62015
[1mStep[0m  [132/339], [94mLoss[0m : 2.42831
[1mStep[0m  [165/339], [94mLoss[0m : 2.29974
[1mStep[0m  [198/339], [94mLoss[0m : 2.00147
[1mStep[0m  [231/339], [94mLoss[0m : 2.62029
[1mStep[0m  [264/339], [94mLoss[0m : 2.42499
[1mStep[0m  [297/339], [94mLoss[0m : 2.30068
[1mStep[0m  [330/339], [94mLoss[0m : 2.47288

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.294, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31720
[1mStep[0m  [33/339], [94mLoss[0m : 2.19052
[1mStep[0m  [66/339], [94mLoss[0m : 2.85101
[1mStep[0m  [99/339], [94mLoss[0m : 2.73085
[1mStep[0m  [132/339], [94mLoss[0m : 2.40588
[1mStep[0m  [165/339], [94mLoss[0m : 2.08648
[1mStep[0m  [198/339], [94mLoss[0m : 2.38690
[1mStep[0m  [231/339], [94mLoss[0m : 2.57666
[1mStep[0m  [264/339], [94mLoss[0m : 2.71014
[1mStep[0m  [297/339], [94mLoss[0m : 2.34663
[1mStep[0m  [330/339], [94mLoss[0m : 3.17046

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.294, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95387
[1mStep[0m  [33/339], [94mLoss[0m : 2.38143
[1mStep[0m  [66/339], [94mLoss[0m : 2.25610
[1mStep[0m  [99/339], [94mLoss[0m : 2.33410
[1mStep[0m  [132/339], [94mLoss[0m : 1.81347
[1mStep[0m  [165/339], [94mLoss[0m : 3.00243
[1mStep[0m  [198/339], [94mLoss[0m : 2.53851
[1mStep[0m  [231/339], [94mLoss[0m : 2.43817
[1mStep[0m  [264/339], [94mLoss[0m : 2.21071
[1mStep[0m  [297/339], [94mLoss[0m : 2.64864
[1mStep[0m  [330/339], [94mLoss[0m : 2.21369

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.301, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.74169
[1mStep[0m  [33/339], [94mLoss[0m : 2.84207
[1mStep[0m  [66/339], [94mLoss[0m : 2.50059
[1mStep[0m  [99/339], [94mLoss[0m : 2.59498
[1mStep[0m  [132/339], [94mLoss[0m : 2.62997
[1mStep[0m  [165/339], [94mLoss[0m : 2.84673
[1mStep[0m  [198/339], [94mLoss[0m : 2.81456
[1mStep[0m  [231/339], [94mLoss[0m : 2.33235
[1mStep[0m  [264/339], [94mLoss[0m : 2.00025
[1mStep[0m  [297/339], [94mLoss[0m : 2.72220
[1mStep[0m  [330/339], [94mLoss[0m : 2.65250

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67578
[1mStep[0m  [33/339], [94mLoss[0m : 2.83040
[1mStep[0m  [66/339], [94mLoss[0m : 3.02222
[1mStep[0m  [99/339], [94mLoss[0m : 2.10285
[1mStep[0m  [132/339], [94mLoss[0m : 1.81633
[1mStep[0m  [165/339], [94mLoss[0m : 2.14134
[1mStep[0m  [198/339], [94mLoss[0m : 2.50803
[1mStep[0m  [231/339], [94mLoss[0m : 2.50590
[1mStep[0m  [264/339], [94mLoss[0m : 2.90413
[1mStep[0m  [297/339], [94mLoss[0m : 1.98397
[1mStep[0m  [330/339], [94mLoss[0m : 1.96649

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15641
[1mStep[0m  [33/339], [94mLoss[0m : 2.55037
[1mStep[0m  [66/339], [94mLoss[0m : 2.47216
[1mStep[0m  [99/339], [94mLoss[0m : 2.60663
[1mStep[0m  [132/339], [94mLoss[0m : 2.22281
[1mStep[0m  [165/339], [94mLoss[0m : 2.17188
[1mStep[0m  [198/339], [94mLoss[0m : 2.53205
[1mStep[0m  [231/339], [94mLoss[0m : 2.25317
[1mStep[0m  [264/339], [94mLoss[0m : 2.77648
[1mStep[0m  [297/339], [94mLoss[0m : 1.94629
[1mStep[0m  [330/339], [94mLoss[0m : 3.28691

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.300, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.288
====================================

Phase 1 - Evaluation MAE:  2.288166322539338
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.70051
[1mStep[0m  [33/339], [94mLoss[0m : 2.69862
[1mStep[0m  [66/339], [94mLoss[0m : 2.31171
[1mStep[0m  [99/339], [94mLoss[0m : 2.60658
[1mStep[0m  [132/339], [94mLoss[0m : 2.37483
[1mStep[0m  [165/339], [94mLoss[0m : 2.33454
[1mStep[0m  [198/339], [94mLoss[0m : 1.54716
[1mStep[0m  [231/339], [94mLoss[0m : 2.19551
[1mStep[0m  [264/339], [94mLoss[0m : 3.41725
[1mStep[0m  [297/339], [94mLoss[0m : 2.51195
[1mStep[0m  [330/339], [94mLoss[0m : 2.51707

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.289, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84577
[1mStep[0m  [33/339], [94mLoss[0m : 2.23688
[1mStep[0m  [66/339], [94mLoss[0m : 2.97052
[1mStep[0m  [99/339], [94mLoss[0m : 2.21850
[1mStep[0m  [132/339], [94mLoss[0m : 2.32068
[1mStep[0m  [165/339], [94mLoss[0m : 2.40994
[1mStep[0m  [198/339], [94mLoss[0m : 2.58366
[1mStep[0m  [231/339], [94mLoss[0m : 2.40217
[1mStep[0m  [264/339], [94mLoss[0m : 1.64064
[1mStep[0m  [297/339], [94mLoss[0m : 2.64013
[1mStep[0m  [330/339], [94mLoss[0m : 2.37185

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70768
[1mStep[0m  [33/339], [94mLoss[0m : 1.98204
[1mStep[0m  [66/339], [94mLoss[0m : 2.44821
[1mStep[0m  [99/339], [94mLoss[0m : 2.16094
[1mStep[0m  [132/339], [94mLoss[0m : 2.56048
[1mStep[0m  [165/339], [94mLoss[0m : 2.97162
[1mStep[0m  [198/339], [94mLoss[0m : 2.87311
[1mStep[0m  [231/339], [94mLoss[0m : 2.75436
[1mStep[0m  [264/339], [94mLoss[0m : 2.88010
[1mStep[0m  [297/339], [94mLoss[0m : 2.20033
[1mStep[0m  [330/339], [94mLoss[0m : 2.57867

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58294
[1mStep[0m  [33/339], [94mLoss[0m : 2.39660
[1mStep[0m  [66/339], [94mLoss[0m : 2.14148
[1mStep[0m  [99/339], [94mLoss[0m : 2.43455
[1mStep[0m  [132/339], [94mLoss[0m : 2.00630
[1mStep[0m  [165/339], [94mLoss[0m : 2.31419
[1mStep[0m  [198/339], [94mLoss[0m : 2.32212
[1mStep[0m  [231/339], [94mLoss[0m : 2.11636
[1mStep[0m  [264/339], [94mLoss[0m : 2.00765
[1mStep[0m  [297/339], [94mLoss[0m : 3.08537
[1mStep[0m  [330/339], [94mLoss[0m : 2.03237

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.15856
[1mStep[0m  [33/339], [94mLoss[0m : 2.43110
[1mStep[0m  [66/339], [94mLoss[0m : 2.05348
[1mStep[0m  [99/339], [94mLoss[0m : 2.28558
[1mStep[0m  [132/339], [94mLoss[0m : 1.71291
[1mStep[0m  [165/339], [94mLoss[0m : 2.18587
[1mStep[0m  [198/339], [94mLoss[0m : 2.53230
[1mStep[0m  [231/339], [94mLoss[0m : 2.17153
[1mStep[0m  [264/339], [94mLoss[0m : 2.30451
[1mStep[0m  [297/339], [94mLoss[0m : 2.33700
[1mStep[0m  [330/339], [94mLoss[0m : 1.66692

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10345
[1mStep[0m  [33/339], [94mLoss[0m : 2.48571
[1mStep[0m  [66/339], [94mLoss[0m : 1.74168
[1mStep[0m  [99/339], [94mLoss[0m : 2.46582
[1mStep[0m  [132/339], [94mLoss[0m : 2.62112
[1mStep[0m  [165/339], [94mLoss[0m : 1.34863
[1mStep[0m  [198/339], [94mLoss[0m : 1.76705
[1mStep[0m  [231/339], [94mLoss[0m : 1.97149
[1mStep[0m  [264/339], [94mLoss[0m : 2.24882
[1mStep[0m  [297/339], [94mLoss[0m : 2.07794
[1mStep[0m  [330/339], [94mLoss[0m : 1.85734

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66305
[1mStep[0m  [33/339], [94mLoss[0m : 2.48560
[1mStep[0m  [66/339], [94mLoss[0m : 2.31314
[1mStep[0m  [99/339], [94mLoss[0m : 2.12795
[1mStep[0m  [132/339], [94mLoss[0m : 1.95223
[1mStep[0m  [165/339], [94mLoss[0m : 1.85350
[1mStep[0m  [198/339], [94mLoss[0m : 2.00750
[1mStep[0m  [231/339], [94mLoss[0m : 2.57055
[1mStep[0m  [264/339], [94mLoss[0m : 2.35878
[1mStep[0m  [297/339], [94mLoss[0m : 2.85312
[1mStep[0m  [330/339], [94mLoss[0m : 2.24222

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64356
[1mStep[0m  [33/339], [94mLoss[0m : 2.12917
[1mStep[0m  [66/339], [94mLoss[0m : 1.82028
[1mStep[0m  [99/339], [94mLoss[0m : 2.03424
[1mStep[0m  [132/339], [94mLoss[0m : 2.62636
[1mStep[0m  [165/339], [94mLoss[0m : 2.01513
[1mStep[0m  [198/339], [94mLoss[0m : 1.61767
[1mStep[0m  [231/339], [94mLoss[0m : 1.98039
[1mStep[0m  [264/339], [94mLoss[0m : 2.27618
[1mStep[0m  [297/339], [94mLoss[0m : 1.75469
[1mStep[0m  [330/339], [94mLoss[0m : 1.89908

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57041
[1mStep[0m  [33/339], [94mLoss[0m : 2.06854
[1mStep[0m  [66/339], [94mLoss[0m : 1.75557
[1mStep[0m  [99/339], [94mLoss[0m : 2.26796
[1mStep[0m  [132/339], [94mLoss[0m : 1.65846
[1mStep[0m  [165/339], [94mLoss[0m : 1.82814
[1mStep[0m  [198/339], [94mLoss[0m : 2.37294
[1mStep[0m  [231/339], [94mLoss[0m : 2.42809
[1mStep[0m  [264/339], [94mLoss[0m : 2.76882
[1mStep[0m  [297/339], [94mLoss[0m : 1.99515
[1mStep[0m  [330/339], [94mLoss[0m : 2.52554

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.31715
[1mStep[0m  [33/339], [94mLoss[0m : 2.18536
[1mStep[0m  [66/339], [94mLoss[0m : 1.81497
[1mStep[0m  [99/339], [94mLoss[0m : 2.20840
[1mStep[0m  [132/339], [94mLoss[0m : 1.42626
[1mStep[0m  [165/339], [94mLoss[0m : 1.58033
[1mStep[0m  [198/339], [94mLoss[0m : 2.60298
[1mStep[0m  [231/339], [94mLoss[0m : 1.63632
[1mStep[0m  [264/339], [94mLoss[0m : 1.71601
[1mStep[0m  [297/339], [94mLoss[0m : 2.08922
[1mStep[0m  [330/339], [94mLoss[0m : 2.16310

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69069
[1mStep[0m  [33/339], [94mLoss[0m : 1.90129
[1mStep[0m  [66/339], [94mLoss[0m : 1.91246
[1mStep[0m  [99/339], [94mLoss[0m : 1.84806
[1mStep[0m  [132/339], [94mLoss[0m : 2.18065
[1mStep[0m  [165/339], [94mLoss[0m : 1.71208
[1mStep[0m  [198/339], [94mLoss[0m : 2.26860
[1mStep[0m  [231/339], [94mLoss[0m : 2.15539
[1mStep[0m  [264/339], [94mLoss[0m : 1.86748
[1mStep[0m  [297/339], [94mLoss[0m : 1.71076
[1mStep[0m  [330/339], [94mLoss[0m : 1.95191

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17519
[1mStep[0m  [33/339], [94mLoss[0m : 1.98362
[1mStep[0m  [66/339], [94mLoss[0m : 2.15245
[1mStep[0m  [99/339], [94mLoss[0m : 2.11685
[1mStep[0m  [132/339], [94mLoss[0m : 1.93117
[1mStep[0m  [165/339], [94mLoss[0m : 2.13900
[1mStep[0m  [198/339], [94mLoss[0m : 2.84214
[1mStep[0m  [231/339], [94mLoss[0m : 2.00546
[1mStep[0m  [264/339], [94mLoss[0m : 2.14364
[1mStep[0m  [297/339], [94mLoss[0m : 1.47188
[1mStep[0m  [330/339], [94mLoss[0m : 2.11776

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45679
[1mStep[0m  [33/339], [94mLoss[0m : 1.59720
[1mStep[0m  [66/339], [94mLoss[0m : 2.31098
[1mStep[0m  [99/339], [94mLoss[0m : 2.20891
[1mStep[0m  [132/339], [94mLoss[0m : 2.08993
[1mStep[0m  [165/339], [94mLoss[0m : 2.02402
[1mStep[0m  [198/339], [94mLoss[0m : 1.48652
[1mStep[0m  [231/339], [94mLoss[0m : 2.02027
[1mStep[0m  [264/339], [94mLoss[0m : 2.07630
[1mStep[0m  [297/339], [94mLoss[0m : 2.28077
[1mStep[0m  [330/339], [94mLoss[0m : 2.22626

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79383
[1mStep[0m  [33/339], [94mLoss[0m : 2.07634
[1mStep[0m  [66/339], [94mLoss[0m : 2.15540
[1mStep[0m  [99/339], [94mLoss[0m : 1.44551
[1mStep[0m  [132/339], [94mLoss[0m : 1.39382
[1mStep[0m  [165/339], [94mLoss[0m : 1.72634
[1mStep[0m  [198/339], [94mLoss[0m : 2.11912
[1mStep[0m  [231/339], [94mLoss[0m : 1.76987
[1mStep[0m  [264/339], [94mLoss[0m : 2.27196
[1mStep[0m  [297/339], [94mLoss[0m : 2.20319
[1mStep[0m  [330/339], [94mLoss[0m : 2.10625

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52981
[1mStep[0m  [33/339], [94mLoss[0m : 1.78474
[1mStep[0m  [66/339], [94mLoss[0m : 1.43565
[1mStep[0m  [99/339], [94mLoss[0m : 1.95605
[1mStep[0m  [132/339], [94mLoss[0m : 1.69442
[1mStep[0m  [165/339], [94mLoss[0m : 1.77257
[1mStep[0m  [198/339], [94mLoss[0m : 1.95162
[1mStep[0m  [231/339], [94mLoss[0m : 2.36129
[1mStep[0m  [264/339], [94mLoss[0m : 1.94996
[1mStep[0m  [297/339], [94mLoss[0m : 1.78519
[1mStep[0m  [330/339], [94mLoss[0m : 1.79933

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78843
[1mStep[0m  [33/339], [94mLoss[0m : 1.72568
[1mStep[0m  [66/339], [94mLoss[0m : 1.67735
[1mStep[0m  [99/339], [94mLoss[0m : 1.84841
[1mStep[0m  [132/339], [94mLoss[0m : 2.30538
[1mStep[0m  [165/339], [94mLoss[0m : 1.88388
[1mStep[0m  [198/339], [94mLoss[0m : 1.47869
[1mStep[0m  [231/339], [94mLoss[0m : 1.73675
[1mStep[0m  [264/339], [94mLoss[0m : 2.33412
[1mStep[0m  [297/339], [94mLoss[0m : 2.14078
[1mStep[0m  [330/339], [94mLoss[0m : 1.80017

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95551
[1mStep[0m  [33/339], [94mLoss[0m : 2.01757
[1mStep[0m  [66/339], [94mLoss[0m : 1.85225
[1mStep[0m  [99/339], [94mLoss[0m : 1.68064
[1mStep[0m  [132/339], [94mLoss[0m : 1.34920
[1mStep[0m  [165/339], [94mLoss[0m : 2.22599
[1mStep[0m  [198/339], [94mLoss[0m : 2.04711
[1mStep[0m  [231/339], [94mLoss[0m : 1.63337
[1mStep[0m  [264/339], [94mLoss[0m : 1.49947
[1mStep[0m  [297/339], [94mLoss[0m : 2.30376
[1mStep[0m  [330/339], [94mLoss[0m : 2.55507

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78234
[1mStep[0m  [33/339], [94mLoss[0m : 2.59502
[1mStep[0m  [66/339], [94mLoss[0m : 2.02423
[1mStep[0m  [99/339], [94mLoss[0m : 2.31166
[1mStep[0m  [132/339], [94mLoss[0m : 1.60062
[1mStep[0m  [165/339], [94mLoss[0m : 1.60482
[1mStep[0m  [198/339], [94mLoss[0m : 2.04353
[1mStep[0m  [231/339], [94mLoss[0m : 1.73951
[1mStep[0m  [264/339], [94mLoss[0m : 1.44828
[1mStep[0m  [297/339], [94mLoss[0m : 1.87523
[1mStep[0m  [330/339], [94mLoss[0m : 1.81296

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.50662
[1mStep[0m  [33/339], [94mLoss[0m : 1.51534
[1mStep[0m  [66/339], [94mLoss[0m : 1.94057
[1mStep[0m  [99/339], [94mLoss[0m : 1.69909
[1mStep[0m  [132/339], [94mLoss[0m : 1.78191
[1mStep[0m  [165/339], [94mLoss[0m : 1.75845
[1mStep[0m  [198/339], [94mLoss[0m : 1.65937
[1mStep[0m  [231/339], [94mLoss[0m : 2.11610
[1mStep[0m  [264/339], [94mLoss[0m : 1.54447
[1mStep[0m  [297/339], [94mLoss[0m : 2.11975
[1mStep[0m  [330/339], [94mLoss[0m : 1.90664

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74775
[1mStep[0m  [33/339], [94mLoss[0m : 1.76169
[1mStep[0m  [66/339], [94mLoss[0m : 1.70436
[1mStep[0m  [99/339], [94mLoss[0m : 2.15261
[1mStep[0m  [132/339], [94mLoss[0m : 1.98174
[1mStep[0m  [165/339], [94mLoss[0m : 2.03772
[1mStep[0m  [198/339], [94mLoss[0m : 1.81523
[1mStep[0m  [231/339], [94mLoss[0m : 2.54883
[1mStep[0m  [264/339], [94mLoss[0m : 2.19170
[1mStep[0m  [297/339], [94mLoss[0m : 1.89585
[1mStep[0m  [330/339], [94mLoss[0m : 1.97310

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60376
[1mStep[0m  [33/339], [94mLoss[0m : 2.37397
[1mStep[0m  [66/339], [94mLoss[0m : 1.75436
[1mStep[0m  [99/339], [94mLoss[0m : 1.91066
[1mStep[0m  [132/339], [94mLoss[0m : 2.58045
[1mStep[0m  [165/339], [94mLoss[0m : 1.84009
[1mStep[0m  [198/339], [94mLoss[0m : 1.73354
[1mStep[0m  [231/339], [94mLoss[0m : 1.85656
[1mStep[0m  [264/339], [94mLoss[0m : 1.66875
[1mStep[0m  [297/339], [94mLoss[0m : 1.87162
[1mStep[0m  [330/339], [94mLoss[0m : 2.01846

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.544, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76992
[1mStep[0m  [33/339], [94mLoss[0m : 1.62477
[1mStep[0m  [66/339], [94mLoss[0m : 1.69520
[1mStep[0m  [99/339], [94mLoss[0m : 2.19137
[1mStep[0m  [132/339], [94mLoss[0m : 1.39387
[1mStep[0m  [165/339], [94mLoss[0m : 2.61995
[1mStep[0m  [198/339], [94mLoss[0m : 1.32428
[1mStep[0m  [231/339], [94mLoss[0m : 2.26341
[1mStep[0m  [264/339], [94mLoss[0m : 2.17445
[1mStep[0m  [297/339], [94mLoss[0m : 1.94989
[1mStep[0m  [330/339], [94mLoss[0m : 1.92153

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.452, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68182
[1mStep[0m  [33/339], [94mLoss[0m : 1.39439
[1mStep[0m  [66/339], [94mLoss[0m : 1.92707
[1mStep[0m  [99/339], [94mLoss[0m : 1.86893
[1mStep[0m  [132/339], [94mLoss[0m : 1.67823
[1mStep[0m  [165/339], [94mLoss[0m : 1.85125
[1mStep[0m  [198/339], [94mLoss[0m : 2.02483
[1mStep[0m  [231/339], [94mLoss[0m : 1.44104
[1mStep[0m  [264/339], [94mLoss[0m : 1.89396
[1mStep[0m  [297/339], [94mLoss[0m : 1.92588
[1mStep[0m  [330/339], [94mLoss[0m : 1.85814

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.55507
[1mStep[0m  [33/339], [94mLoss[0m : 1.94529
[1mStep[0m  [66/339], [94mLoss[0m : 1.64019
[1mStep[0m  [99/339], [94mLoss[0m : 2.16798
[1mStep[0m  [132/339], [94mLoss[0m : 1.74891
[1mStep[0m  [165/339], [94mLoss[0m : 1.59595
[1mStep[0m  [198/339], [94mLoss[0m : 1.64035
[1mStep[0m  [231/339], [94mLoss[0m : 1.08455
[1mStep[0m  [264/339], [94mLoss[0m : 1.36532
[1mStep[0m  [297/339], [94mLoss[0m : 1.69908
[1mStep[0m  [330/339], [94mLoss[0m : 1.75052

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78646
[1mStep[0m  [33/339], [94mLoss[0m : 1.30580
[1mStep[0m  [66/339], [94mLoss[0m : 1.80272
[1mStep[0m  [99/339], [94mLoss[0m : 1.99921
[1mStep[0m  [132/339], [94mLoss[0m : 1.96801
[1mStep[0m  [165/339], [94mLoss[0m : 1.79375
[1mStep[0m  [198/339], [94mLoss[0m : 2.26836
[1mStep[0m  [231/339], [94mLoss[0m : 1.59523
[1mStep[0m  [264/339], [94mLoss[0m : 1.54929
[1mStep[0m  [297/339], [94mLoss[0m : 1.91115
[1mStep[0m  [330/339], [94mLoss[0m : 1.79582

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.540, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61643
[1mStep[0m  [33/339], [94mLoss[0m : 1.81719
[1mStep[0m  [66/339], [94mLoss[0m : 1.79530
[1mStep[0m  [99/339], [94mLoss[0m : 1.19488
[1mStep[0m  [132/339], [94mLoss[0m : 1.80870
[1mStep[0m  [165/339], [94mLoss[0m : 1.68180
[1mStep[0m  [198/339], [94mLoss[0m : 1.28488
[1mStep[0m  [231/339], [94mLoss[0m : 2.05499
[1mStep[0m  [264/339], [94mLoss[0m : 1.84169
[1mStep[0m  [297/339], [94mLoss[0m : 1.63183
[1mStep[0m  [330/339], [94mLoss[0m : 1.57099

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06797
[1mStep[0m  [33/339], [94mLoss[0m : 2.01290
[1mStep[0m  [66/339], [94mLoss[0m : 1.77212
[1mStep[0m  [99/339], [94mLoss[0m : 1.90887
[1mStep[0m  [132/339], [94mLoss[0m : 1.72010
[1mStep[0m  [165/339], [94mLoss[0m : 1.92776
[1mStep[0m  [198/339], [94mLoss[0m : 1.97034
[1mStep[0m  [231/339], [94mLoss[0m : 1.42391
[1mStep[0m  [264/339], [94mLoss[0m : 2.15820
[1mStep[0m  [297/339], [94mLoss[0m : 1.69857
[1mStep[0m  [330/339], [94mLoss[0m : 1.68583

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.20680
[1mStep[0m  [33/339], [94mLoss[0m : 2.22972
[1mStep[0m  [66/339], [94mLoss[0m : 1.51376
[1mStep[0m  [99/339], [94mLoss[0m : 1.84119
[1mStep[0m  [132/339], [94mLoss[0m : 2.06525
[1mStep[0m  [165/339], [94mLoss[0m : 1.25960
[1mStep[0m  [198/339], [94mLoss[0m : 2.60895
[1mStep[0m  [231/339], [94mLoss[0m : 2.14734
[1mStep[0m  [264/339], [94mLoss[0m : 1.34260
[1mStep[0m  [297/339], [94mLoss[0m : 1.29029
[1mStep[0m  [330/339], [94mLoss[0m : 1.39441

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.512, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.31794
[1mStep[0m  [33/339], [94mLoss[0m : 1.62714
[1mStep[0m  [66/339], [94mLoss[0m : 1.68216
[1mStep[0m  [99/339], [94mLoss[0m : 1.32427
[1mStep[0m  [132/339], [94mLoss[0m : 1.77508
[1mStep[0m  [165/339], [94mLoss[0m : 1.89520
[1mStep[0m  [198/339], [94mLoss[0m : 1.59783
[1mStep[0m  [231/339], [94mLoss[0m : 1.92446
[1mStep[0m  [264/339], [94mLoss[0m : 1.51446
[1mStep[0m  [297/339], [94mLoss[0m : 1.79386
[1mStep[0m  [330/339], [94mLoss[0m : 1.88059

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60407
[1mStep[0m  [33/339], [94mLoss[0m : 1.68535
[1mStep[0m  [66/339], [94mLoss[0m : 1.73743
[1mStep[0m  [99/339], [94mLoss[0m : 1.75593
[1mStep[0m  [132/339], [94mLoss[0m : 1.66662
[1mStep[0m  [165/339], [94mLoss[0m : 2.04274
[1mStep[0m  [198/339], [94mLoss[0m : 1.72081
[1mStep[0m  [231/339], [94mLoss[0m : 1.47060
[1mStep[0m  [264/339], [94mLoss[0m : 2.05206
[1mStep[0m  [297/339], [94mLoss[0m : 1.65362
[1mStep[0m  [330/339], [94mLoss[0m : 1.86923

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.487
====================================

Phase 2 - Evaluation MAE:  2.4872899751747606
MAE score P1      2.288166
MAE score P2       2.48729
loss              1.715895
learning_rate     0.007525
batch_size              32
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.14687
[1mStep[0m  [2/21], [94mLoss[0m : 10.98992
[1mStep[0m  [4/21], [94mLoss[0m : 10.77103
[1mStep[0m  [6/21], [94mLoss[0m : 10.08632
[1mStep[0m  [8/21], [94mLoss[0m : 9.83386
[1mStep[0m  [10/21], [94mLoss[0m : 9.91999
[1mStep[0m  [12/21], [94mLoss[0m : 9.37275
[1mStep[0m  [14/21], [94mLoss[0m : 8.82927
[1mStep[0m  [16/21], [94mLoss[0m : 8.63804
[1mStep[0m  [18/21], [94mLoss[0m : 8.56012
[1mStep[0m  [20/21], [94mLoss[0m : 8.25538

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.725, [92mTest[0m: 11.064, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.82808
[1mStep[0m  [2/21], [94mLoss[0m : 7.52408
[1mStep[0m  [4/21], [94mLoss[0m : 7.34907
[1mStep[0m  [6/21], [94mLoss[0m : 6.94834
[1mStep[0m  [8/21], [94mLoss[0m : 6.70101
[1mStep[0m  [10/21], [94mLoss[0m : 6.58151
[1mStep[0m  [12/21], [94mLoss[0m : 6.23951
[1mStep[0m  [14/21], [94mLoss[0m : 5.82209
[1mStep[0m  [16/21], [94mLoss[0m : 5.43516
[1mStep[0m  [18/21], [94mLoss[0m : 5.22212
[1mStep[0m  [20/21], [94mLoss[0m : 4.93590

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.415, [92mTest[0m: 9.173, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.01470
[1mStep[0m  [2/21], [94mLoss[0m : 4.51127
[1mStep[0m  [4/21], [94mLoss[0m : 4.36229
[1mStep[0m  [6/21], [94mLoss[0m : 4.26462
[1mStep[0m  [8/21], [94mLoss[0m : 4.15677
[1mStep[0m  [10/21], [94mLoss[0m : 3.80201
[1mStep[0m  [12/21], [94mLoss[0m : 3.77992
[1mStep[0m  [14/21], [94mLoss[0m : 3.52823
[1mStep[0m  [16/21], [94mLoss[0m : 3.46744
[1mStep[0m  [18/21], [94mLoss[0m : 3.32700
[1mStep[0m  [20/21], [94mLoss[0m : 2.98336

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.912, [92mTest[0m: 6.023, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.29983
[1mStep[0m  [2/21], [94mLoss[0m : 3.04302
[1mStep[0m  [4/21], [94mLoss[0m : 3.11874
[1mStep[0m  [6/21], [94mLoss[0m : 3.09220
[1mStep[0m  [8/21], [94mLoss[0m : 3.03797
[1mStep[0m  [10/21], [94mLoss[0m : 2.97647
[1mStep[0m  [12/21], [94mLoss[0m : 2.97334
[1mStep[0m  [14/21], [94mLoss[0m : 3.09603
[1mStep[0m  [16/21], [94mLoss[0m : 2.86818
[1mStep[0m  [18/21], [94mLoss[0m : 2.84658
[1mStep[0m  [20/21], [94mLoss[0m : 2.89110

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.008, [92mTest[0m: 3.703, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75730
[1mStep[0m  [2/21], [94mLoss[0m : 2.81231
[1mStep[0m  [4/21], [94mLoss[0m : 2.89914
[1mStep[0m  [6/21], [94mLoss[0m : 2.90266
[1mStep[0m  [8/21], [94mLoss[0m : 2.80309
[1mStep[0m  [10/21], [94mLoss[0m : 2.89506
[1mStep[0m  [12/21], [94mLoss[0m : 2.84082
[1mStep[0m  [14/21], [94mLoss[0m : 2.80960
[1mStep[0m  [16/21], [94mLoss[0m : 2.79893
[1mStep[0m  [18/21], [94mLoss[0m : 2.86755
[1mStep[0m  [20/21], [94mLoss[0m : 2.85671

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.819, [92mTest[0m: 3.029, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.89069
[1mStep[0m  [2/21], [94mLoss[0m : 2.75509
[1mStep[0m  [4/21], [94mLoss[0m : 2.81126
[1mStep[0m  [6/21], [94mLoss[0m : 2.78463
[1mStep[0m  [8/21], [94mLoss[0m : 2.67095
[1mStep[0m  [10/21], [94mLoss[0m : 2.62413
[1mStep[0m  [12/21], [94mLoss[0m : 2.82479
[1mStep[0m  [14/21], [94mLoss[0m : 2.67421
[1mStep[0m  [16/21], [94mLoss[0m : 2.87399
[1mStep[0m  [18/21], [94mLoss[0m : 2.55430
[1mStep[0m  [20/21], [94mLoss[0m : 2.72078

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.755, [92mTest[0m: 2.767, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.89486
[1mStep[0m  [2/21], [94mLoss[0m : 2.64498
[1mStep[0m  [4/21], [94mLoss[0m : 2.82458
[1mStep[0m  [6/21], [94mLoss[0m : 2.55432
[1mStep[0m  [8/21], [94mLoss[0m : 2.65840
[1mStep[0m  [10/21], [94mLoss[0m : 2.81025
[1mStep[0m  [12/21], [94mLoss[0m : 2.73787
[1mStep[0m  [14/21], [94mLoss[0m : 2.77279
[1mStep[0m  [16/21], [94mLoss[0m : 2.65028
[1mStep[0m  [18/21], [94mLoss[0m : 2.85446
[1mStep[0m  [20/21], [94mLoss[0m : 2.82949

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.636, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68494
[1mStep[0m  [2/21], [94mLoss[0m : 2.70097
[1mStep[0m  [4/21], [94mLoss[0m : 2.77304
[1mStep[0m  [6/21], [94mLoss[0m : 2.69440
[1mStep[0m  [8/21], [94mLoss[0m : 2.61963
[1mStep[0m  [10/21], [94mLoss[0m : 2.72366
[1mStep[0m  [12/21], [94mLoss[0m : 2.83405
[1mStep[0m  [14/21], [94mLoss[0m : 2.61022
[1mStep[0m  [16/21], [94mLoss[0m : 2.87138
[1mStep[0m  [18/21], [94mLoss[0m : 2.79433
[1mStep[0m  [20/21], [94mLoss[0m : 2.58336

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.595, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59167
[1mStep[0m  [2/21], [94mLoss[0m : 2.68324
[1mStep[0m  [4/21], [94mLoss[0m : 2.60331
[1mStep[0m  [6/21], [94mLoss[0m : 2.56595
[1mStep[0m  [8/21], [94mLoss[0m : 2.63472
[1mStep[0m  [10/21], [94mLoss[0m : 2.56553
[1mStep[0m  [12/21], [94mLoss[0m : 2.74750
[1mStep[0m  [14/21], [94mLoss[0m : 2.73320
[1mStep[0m  [16/21], [94mLoss[0m : 2.62485
[1mStep[0m  [18/21], [94mLoss[0m : 2.69247
[1mStep[0m  [20/21], [94mLoss[0m : 2.57862

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.589, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76002
[1mStep[0m  [2/21], [94mLoss[0m : 2.51464
[1mStep[0m  [4/21], [94mLoss[0m : 2.82954
[1mStep[0m  [6/21], [94mLoss[0m : 2.70578
[1mStep[0m  [8/21], [94mLoss[0m : 2.63840
[1mStep[0m  [10/21], [94mLoss[0m : 2.70574
[1mStep[0m  [12/21], [94mLoss[0m : 2.76326
[1mStep[0m  [14/21], [94mLoss[0m : 2.73140
[1mStep[0m  [16/21], [94mLoss[0m : 2.58653
[1mStep[0m  [18/21], [94mLoss[0m : 2.78305
[1mStep[0m  [20/21], [94mLoss[0m : 2.82834

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.518, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68398
[1mStep[0m  [2/21], [94mLoss[0m : 2.73786
[1mStep[0m  [4/21], [94mLoss[0m : 2.57045
[1mStep[0m  [6/21], [94mLoss[0m : 2.75823
[1mStep[0m  [8/21], [94mLoss[0m : 2.70615
[1mStep[0m  [10/21], [94mLoss[0m : 2.71388
[1mStep[0m  [12/21], [94mLoss[0m : 2.71517
[1mStep[0m  [14/21], [94mLoss[0m : 2.71328
[1mStep[0m  [16/21], [94mLoss[0m : 2.67323
[1mStep[0m  [18/21], [94mLoss[0m : 2.75524
[1mStep[0m  [20/21], [94mLoss[0m : 2.75429

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48596
[1mStep[0m  [2/21], [94mLoss[0m : 2.62361
[1mStep[0m  [4/21], [94mLoss[0m : 2.70338
[1mStep[0m  [6/21], [94mLoss[0m : 2.63102
[1mStep[0m  [8/21], [94mLoss[0m : 2.64925
[1mStep[0m  [10/21], [94mLoss[0m : 2.80817
[1mStep[0m  [12/21], [94mLoss[0m : 2.68745
[1mStep[0m  [14/21], [94mLoss[0m : 2.67586
[1mStep[0m  [16/21], [94mLoss[0m : 2.57263
[1mStep[0m  [18/21], [94mLoss[0m : 2.59916
[1mStep[0m  [20/21], [94mLoss[0m : 2.44478

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.494, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82700
[1mStep[0m  [2/21], [94mLoss[0m : 2.73751
[1mStep[0m  [4/21], [94mLoss[0m : 2.67529
[1mStep[0m  [6/21], [94mLoss[0m : 2.79148
[1mStep[0m  [8/21], [94mLoss[0m : 2.66035
[1mStep[0m  [10/21], [94mLoss[0m : 2.42909
[1mStep[0m  [12/21], [94mLoss[0m : 2.70585
[1mStep[0m  [14/21], [94mLoss[0m : 2.67906
[1mStep[0m  [16/21], [94mLoss[0m : 2.60084
[1mStep[0m  [18/21], [94mLoss[0m : 2.76219
[1mStep[0m  [20/21], [94mLoss[0m : 2.81472

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53062
[1mStep[0m  [2/21], [94mLoss[0m : 2.64291
[1mStep[0m  [4/21], [94mLoss[0m : 2.84668
[1mStep[0m  [6/21], [94mLoss[0m : 2.42148
[1mStep[0m  [8/21], [94mLoss[0m : 2.57768
[1mStep[0m  [10/21], [94mLoss[0m : 2.56588
[1mStep[0m  [12/21], [94mLoss[0m : 2.59672
[1mStep[0m  [14/21], [94mLoss[0m : 2.83585
[1mStep[0m  [16/21], [94mLoss[0m : 2.59478
[1mStep[0m  [18/21], [94mLoss[0m : 2.63241
[1mStep[0m  [20/21], [94mLoss[0m : 2.63372

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71481
[1mStep[0m  [2/21], [94mLoss[0m : 2.47045
[1mStep[0m  [4/21], [94mLoss[0m : 2.56871
[1mStep[0m  [6/21], [94mLoss[0m : 2.83174
[1mStep[0m  [8/21], [94mLoss[0m : 2.63209
[1mStep[0m  [10/21], [94mLoss[0m : 2.74647
[1mStep[0m  [12/21], [94mLoss[0m : 2.54278
[1mStep[0m  [14/21], [94mLoss[0m : 2.53091
[1mStep[0m  [16/21], [94mLoss[0m : 2.61690
[1mStep[0m  [18/21], [94mLoss[0m : 2.52544
[1mStep[0m  [20/21], [94mLoss[0m : 2.71323

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54247
[1mStep[0m  [2/21], [94mLoss[0m : 2.62869
[1mStep[0m  [4/21], [94mLoss[0m : 2.77271
[1mStep[0m  [6/21], [94mLoss[0m : 2.62630
[1mStep[0m  [8/21], [94mLoss[0m : 2.51946
[1mStep[0m  [10/21], [94mLoss[0m : 2.52716
[1mStep[0m  [12/21], [94mLoss[0m : 2.69233
[1mStep[0m  [14/21], [94mLoss[0m : 2.72682
[1mStep[0m  [16/21], [94mLoss[0m : 2.59680
[1mStep[0m  [18/21], [94mLoss[0m : 2.37467
[1mStep[0m  [20/21], [94mLoss[0m : 2.55581

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59507
[1mStep[0m  [2/21], [94mLoss[0m : 2.63278
[1mStep[0m  [4/21], [94mLoss[0m : 2.41323
[1mStep[0m  [6/21], [94mLoss[0m : 2.54930
[1mStep[0m  [8/21], [94mLoss[0m : 2.56477
[1mStep[0m  [10/21], [94mLoss[0m : 2.58438
[1mStep[0m  [12/21], [94mLoss[0m : 2.60474
[1mStep[0m  [14/21], [94mLoss[0m : 2.58593
[1mStep[0m  [16/21], [94mLoss[0m : 2.61255
[1mStep[0m  [18/21], [94mLoss[0m : 2.49181
[1mStep[0m  [20/21], [94mLoss[0m : 2.58445

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52660
[1mStep[0m  [2/21], [94mLoss[0m : 2.53289
[1mStep[0m  [4/21], [94mLoss[0m : 2.59764
[1mStep[0m  [6/21], [94mLoss[0m : 2.65055
[1mStep[0m  [8/21], [94mLoss[0m : 2.62687
[1mStep[0m  [10/21], [94mLoss[0m : 2.60279
[1mStep[0m  [12/21], [94mLoss[0m : 2.52982
[1mStep[0m  [14/21], [94mLoss[0m : 2.49725
[1mStep[0m  [16/21], [94mLoss[0m : 2.71684
[1mStep[0m  [18/21], [94mLoss[0m : 2.50245
[1mStep[0m  [20/21], [94mLoss[0m : 2.63841

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59800
[1mStep[0m  [2/21], [94mLoss[0m : 2.59021
[1mStep[0m  [4/21], [94mLoss[0m : 2.46997
[1mStep[0m  [6/21], [94mLoss[0m : 2.65714
[1mStep[0m  [8/21], [94mLoss[0m : 2.65086
[1mStep[0m  [10/21], [94mLoss[0m : 2.61301
[1mStep[0m  [12/21], [94mLoss[0m : 2.54706
[1mStep[0m  [14/21], [94mLoss[0m : 2.56824
[1mStep[0m  [16/21], [94mLoss[0m : 2.70954
[1mStep[0m  [18/21], [94mLoss[0m : 2.55116
[1mStep[0m  [20/21], [94mLoss[0m : 2.78633

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59584
[1mStep[0m  [2/21], [94mLoss[0m : 2.64900
[1mStep[0m  [4/21], [94mLoss[0m : 2.78298
[1mStep[0m  [6/21], [94mLoss[0m : 2.65848
[1mStep[0m  [8/21], [94mLoss[0m : 2.58316
[1mStep[0m  [10/21], [94mLoss[0m : 2.63460
[1mStep[0m  [12/21], [94mLoss[0m : 2.54993
[1mStep[0m  [14/21], [94mLoss[0m : 2.59061
[1mStep[0m  [16/21], [94mLoss[0m : 2.67428
[1mStep[0m  [18/21], [94mLoss[0m : 2.54253
[1mStep[0m  [20/21], [94mLoss[0m : 2.56286

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64643
[1mStep[0m  [2/21], [94mLoss[0m : 2.56798
[1mStep[0m  [4/21], [94mLoss[0m : 2.69710
[1mStep[0m  [6/21], [94mLoss[0m : 2.50887
[1mStep[0m  [8/21], [94mLoss[0m : 2.58694
[1mStep[0m  [10/21], [94mLoss[0m : 2.68205
[1mStep[0m  [12/21], [94mLoss[0m : 2.53656
[1mStep[0m  [14/21], [94mLoss[0m : 2.77307
[1mStep[0m  [16/21], [94mLoss[0m : 2.44302
[1mStep[0m  [18/21], [94mLoss[0m : 2.56668
[1mStep[0m  [20/21], [94mLoss[0m : 2.63376

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55071
[1mStep[0m  [2/21], [94mLoss[0m : 2.53050
[1mStep[0m  [4/21], [94mLoss[0m : 2.71650
[1mStep[0m  [6/21], [94mLoss[0m : 2.68942
[1mStep[0m  [8/21], [94mLoss[0m : 2.79793
[1mStep[0m  [10/21], [94mLoss[0m : 2.49277
[1mStep[0m  [12/21], [94mLoss[0m : 2.46632
[1mStep[0m  [14/21], [94mLoss[0m : 2.76354
[1mStep[0m  [16/21], [94mLoss[0m : 2.70145
[1mStep[0m  [18/21], [94mLoss[0m : 2.61694
[1mStep[0m  [20/21], [94mLoss[0m : 2.73734

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.418, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66067
[1mStep[0m  [2/21], [94mLoss[0m : 2.62012
[1mStep[0m  [4/21], [94mLoss[0m : 2.68400
[1mStep[0m  [6/21], [94mLoss[0m : 2.56959
[1mStep[0m  [8/21], [94mLoss[0m : 2.53047
[1mStep[0m  [10/21], [94mLoss[0m : 2.60208
[1mStep[0m  [12/21], [94mLoss[0m : 2.78007
[1mStep[0m  [14/21], [94mLoss[0m : 2.42082
[1mStep[0m  [16/21], [94mLoss[0m : 2.68296
[1mStep[0m  [18/21], [94mLoss[0m : 2.43879
[1mStep[0m  [20/21], [94mLoss[0m : 2.82165

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70403
[1mStep[0m  [2/21], [94mLoss[0m : 2.72359
[1mStep[0m  [4/21], [94mLoss[0m : 2.50693
[1mStep[0m  [6/21], [94mLoss[0m : 2.46901
[1mStep[0m  [8/21], [94mLoss[0m : 2.47795
[1mStep[0m  [10/21], [94mLoss[0m : 2.59836
[1mStep[0m  [12/21], [94mLoss[0m : 2.52427
[1mStep[0m  [14/21], [94mLoss[0m : 2.62951
[1mStep[0m  [16/21], [94mLoss[0m : 2.69620
[1mStep[0m  [18/21], [94mLoss[0m : 2.64533
[1mStep[0m  [20/21], [94mLoss[0m : 2.54418

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67162
[1mStep[0m  [2/21], [94mLoss[0m : 2.51173
[1mStep[0m  [4/21], [94mLoss[0m : 2.51751
[1mStep[0m  [6/21], [94mLoss[0m : 2.82008
[1mStep[0m  [8/21], [94mLoss[0m : 2.57946
[1mStep[0m  [10/21], [94mLoss[0m : 2.34457
[1mStep[0m  [12/21], [94mLoss[0m : 2.62163
[1mStep[0m  [14/21], [94mLoss[0m : 2.61947
[1mStep[0m  [16/21], [94mLoss[0m : 2.61731
[1mStep[0m  [18/21], [94mLoss[0m : 2.65415
[1mStep[0m  [20/21], [94mLoss[0m : 2.43422

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68176
[1mStep[0m  [2/21], [94mLoss[0m : 2.61957
[1mStep[0m  [4/21], [94mLoss[0m : 2.55013
[1mStep[0m  [6/21], [94mLoss[0m : 2.49020
[1mStep[0m  [8/21], [94mLoss[0m : 2.53930
[1mStep[0m  [10/21], [94mLoss[0m : 2.72048
[1mStep[0m  [12/21], [94mLoss[0m : 2.65729
[1mStep[0m  [14/21], [94mLoss[0m : 2.54356
[1mStep[0m  [16/21], [94mLoss[0m : 2.43650
[1mStep[0m  [18/21], [94mLoss[0m : 2.52071
[1mStep[0m  [20/21], [94mLoss[0m : 2.74158

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42367
[1mStep[0m  [2/21], [94mLoss[0m : 2.37620
[1mStep[0m  [4/21], [94mLoss[0m : 2.67358
[1mStep[0m  [6/21], [94mLoss[0m : 2.42067
[1mStep[0m  [8/21], [94mLoss[0m : 2.42600
[1mStep[0m  [10/21], [94mLoss[0m : 2.70534
[1mStep[0m  [12/21], [94mLoss[0m : 2.65351
[1mStep[0m  [14/21], [94mLoss[0m : 2.47801
[1mStep[0m  [16/21], [94mLoss[0m : 2.48043
[1mStep[0m  [18/21], [94mLoss[0m : 2.58866
[1mStep[0m  [20/21], [94mLoss[0m : 2.60461

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63732
[1mStep[0m  [2/21], [94mLoss[0m : 2.54688
[1mStep[0m  [4/21], [94mLoss[0m : 2.63670
[1mStep[0m  [6/21], [94mLoss[0m : 2.59111
[1mStep[0m  [8/21], [94mLoss[0m : 2.68250
[1mStep[0m  [10/21], [94mLoss[0m : 2.55480
[1mStep[0m  [12/21], [94mLoss[0m : 2.46530
[1mStep[0m  [14/21], [94mLoss[0m : 2.58461
[1mStep[0m  [16/21], [94mLoss[0m : 2.67244
[1mStep[0m  [18/21], [94mLoss[0m : 2.45284
[1mStep[0m  [20/21], [94mLoss[0m : 2.71116

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67853
[1mStep[0m  [2/21], [94mLoss[0m : 2.61466
[1mStep[0m  [4/21], [94mLoss[0m : 2.74453
[1mStep[0m  [6/21], [94mLoss[0m : 2.46034
[1mStep[0m  [8/21], [94mLoss[0m : 2.52421
[1mStep[0m  [10/21], [94mLoss[0m : 2.69528
[1mStep[0m  [12/21], [94mLoss[0m : 2.61564
[1mStep[0m  [14/21], [94mLoss[0m : 2.50684
[1mStep[0m  [16/21], [94mLoss[0m : 2.52110
[1mStep[0m  [18/21], [94mLoss[0m : 2.60004
[1mStep[0m  [20/21], [94mLoss[0m : 2.72355

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.429, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56801
[1mStep[0m  [2/21], [94mLoss[0m : 2.60231
[1mStep[0m  [4/21], [94mLoss[0m : 2.58768
[1mStep[0m  [6/21], [94mLoss[0m : 2.52362
[1mStep[0m  [8/21], [94mLoss[0m : 2.41760
[1mStep[0m  [10/21], [94mLoss[0m : 2.49381
[1mStep[0m  [12/21], [94mLoss[0m : 2.55010
[1mStep[0m  [14/21], [94mLoss[0m : 2.48755
[1mStep[0m  [16/21], [94mLoss[0m : 2.51283
[1mStep[0m  [18/21], [94mLoss[0m : 2.57066
[1mStep[0m  [20/21], [94mLoss[0m : 2.50431

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.393, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.374
====================================

Phase 1 - Evaluation MAE:  2.373833724430629
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.55445
[1mStep[0m  [2/21], [94mLoss[0m : 2.50212
[1mStep[0m  [4/21], [94mLoss[0m : 2.59418
[1mStep[0m  [6/21], [94mLoss[0m : 2.63245
[1mStep[0m  [8/21], [94mLoss[0m : 2.57431
[1mStep[0m  [10/21], [94mLoss[0m : 2.65815
[1mStep[0m  [12/21], [94mLoss[0m : 2.60586
[1mStep[0m  [14/21], [94mLoss[0m : 2.58494
[1mStep[0m  [16/21], [94mLoss[0m : 2.49704
[1mStep[0m  [18/21], [94mLoss[0m : 2.50614
[1mStep[0m  [20/21], [94mLoss[0m : 2.67686

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83918
[1mStep[0m  [2/21], [94mLoss[0m : 2.48479
[1mStep[0m  [4/21], [94mLoss[0m : 2.53983
[1mStep[0m  [6/21], [94mLoss[0m : 2.53343
[1mStep[0m  [8/21], [94mLoss[0m : 2.61237
[1mStep[0m  [10/21], [94mLoss[0m : 2.58265
[1mStep[0m  [12/21], [94mLoss[0m : 2.63574
[1mStep[0m  [14/21], [94mLoss[0m : 2.54992
[1mStep[0m  [16/21], [94mLoss[0m : 2.57602
[1mStep[0m  [18/21], [94mLoss[0m : 2.63010
[1mStep[0m  [20/21], [94mLoss[0m : 2.57685

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.789, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46724
[1mStep[0m  [2/21], [94mLoss[0m : 2.54349
[1mStep[0m  [4/21], [94mLoss[0m : 2.51718
[1mStep[0m  [6/21], [94mLoss[0m : 2.62920
[1mStep[0m  [8/21], [94mLoss[0m : 2.43357
[1mStep[0m  [10/21], [94mLoss[0m : 2.68840
[1mStep[0m  [12/21], [94mLoss[0m : 2.57695
[1mStep[0m  [14/21], [94mLoss[0m : 2.71416
[1mStep[0m  [16/21], [94mLoss[0m : 2.70751
[1mStep[0m  [18/21], [94mLoss[0m : 2.71369
[1mStep[0m  [20/21], [94mLoss[0m : 2.49782

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43809
[1mStep[0m  [2/21], [94mLoss[0m : 2.56404
[1mStep[0m  [4/21], [94mLoss[0m : 2.56000
[1mStep[0m  [6/21], [94mLoss[0m : 2.54141
[1mStep[0m  [8/21], [94mLoss[0m : 2.57862
[1mStep[0m  [10/21], [94mLoss[0m : 2.61609
[1mStep[0m  [12/21], [94mLoss[0m : 2.57347
[1mStep[0m  [14/21], [94mLoss[0m : 2.56106
[1mStep[0m  [16/21], [94mLoss[0m : 2.52991
[1mStep[0m  [18/21], [94mLoss[0m : 2.59989
[1mStep[0m  [20/21], [94mLoss[0m : 2.56920

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48645
[1mStep[0m  [2/21], [94mLoss[0m : 2.56623
[1mStep[0m  [4/21], [94mLoss[0m : 2.47946
[1mStep[0m  [6/21], [94mLoss[0m : 2.55394
[1mStep[0m  [8/21], [94mLoss[0m : 2.64553
[1mStep[0m  [10/21], [94mLoss[0m : 2.59314
[1mStep[0m  [12/21], [94mLoss[0m : 2.44737
[1mStep[0m  [14/21], [94mLoss[0m : 2.58259
[1mStep[0m  [16/21], [94mLoss[0m : 2.54147
[1mStep[0m  [18/21], [94mLoss[0m : 2.46679
[1mStep[0m  [20/21], [94mLoss[0m : 2.57579

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47222
[1mStep[0m  [2/21], [94mLoss[0m : 2.70920
[1mStep[0m  [4/21], [94mLoss[0m : 2.46424
[1mStep[0m  [6/21], [94mLoss[0m : 2.58970
[1mStep[0m  [8/21], [94mLoss[0m : 2.51093
[1mStep[0m  [10/21], [94mLoss[0m : 2.42830
[1mStep[0m  [12/21], [94mLoss[0m : 2.54576
[1mStep[0m  [14/21], [94mLoss[0m : 2.48315
[1mStep[0m  [16/21], [94mLoss[0m : 2.52987
[1mStep[0m  [18/21], [94mLoss[0m : 2.50632
[1mStep[0m  [20/21], [94mLoss[0m : 2.47044

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47637
[1mStep[0m  [2/21], [94mLoss[0m : 2.46757
[1mStep[0m  [4/21], [94mLoss[0m : 2.46857
[1mStep[0m  [6/21], [94mLoss[0m : 2.60636
[1mStep[0m  [8/21], [94mLoss[0m : 2.61830
[1mStep[0m  [10/21], [94mLoss[0m : 2.66145
[1mStep[0m  [12/21], [94mLoss[0m : 2.55077
[1mStep[0m  [14/21], [94mLoss[0m : 2.41483
[1mStep[0m  [16/21], [94mLoss[0m : 2.66316
[1mStep[0m  [18/21], [94mLoss[0m : 2.49716
[1mStep[0m  [20/21], [94mLoss[0m : 2.50486

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32450
[1mStep[0m  [2/21], [94mLoss[0m : 2.33529
[1mStep[0m  [4/21], [94mLoss[0m : 2.39453
[1mStep[0m  [6/21], [94mLoss[0m : 2.36927
[1mStep[0m  [8/21], [94mLoss[0m : 2.38703
[1mStep[0m  [10/21], [94mLoss[0m : 2.54778
[1mStep[0m  [12/21], [94mLoss[0m : 2.36655
[1mStep[0m  [14/21], [94mLoss[0m : 2.50254
[1mStep[0m  [16/21], [94mLoss[0m : 2.46201
[1mStep[0m  [18/21], [94mLoss[0m : 2.58386
[1mStep[0m  [20/21], [94mLoss[0m : 2.28479

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40177
[1mStep[0m  [2/21], [94mLoss[0m : 2.39284
[1mStep[0m  [4/21], [94mLoss[0m : 2.40634
[1mStep[0m  [6/21], [94mLoss[0m : 2.35100
[1mStep[0m  [8/21], [94mLoss[0m : 2.44161
[1mStep[0m  [10/21], [94mLoss[0m : 2.39967
[1mStep[0m  [12/21], [94mLoss[0m : 2.40925
[1mStep[0m  [14/21], [94mLoss[0m : 2.36286
[1mStep[0m  [16/21], [94mLoss[0m : 2.44260
[1mStep[0m  [18/21], [94mLoss[0m : 2.53796
[1mStep[0m  [20/21], [94mLoss[0m : 2.50262

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43602
[1mStep[0m  [2/21], [94mLoss[0m : 2.48384
[1mStep[0m  [4/21], [94mLoss[0m : 2.59390
[1mStep[0m  [6/21], [94mLoss[0m : 2.45645
[1mStep[0m  [8/21], [94mLoss[0m : 2.48513
[1mStep[0m  [10/21], [94mLoss[0m : 2.54181
[1mStep[0m  [12/21], [94mLoss[0m : 2.25352
[1mStep[0m  [14/21], [94mLoss[0m : 2.42204
[1mStep[0m  [16/21], [94mLoss[0m : 2.22257
[1mStep[0m  [18/21], [94mLoss[0m : 2.39461
[1mStep[0m  [20/21], [94mLoss[0m : 2.45340

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35113
[1mStep[0m  [2/21], [94mLoss[0m : 2.36206
[1mStep[0m  [4/21], [94mLoss[0m : 2.47523
[1mStep[0m  [6/21], [94mLoss[0m : 2.58633
[1mStep[0m  [8/21], [94mLoss[0m : 2.35737
[1mStep[0m  [10/21], [94mLoss[0m : 2.32504
[1mStep[0m  [12/21], [94mLoss[0m : 2.44519
[1mStep[0m  [14/21], [94mLoss[0m : 2.38743
[1mStep[0m  [16/21], [94mLoss[0m : 2.52648
[1mStep[0m  [18/21], [94mLoss[0m : 2.36325
[1mStep[0m  [20/21], [94mLoss[0m : 2.27579

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33153
[1mStep[0m  [2/21], [94mLoss[0m : 2.18385
[1mStep[0m  [4/21], [94mLoss[0m : 2.46829
[1mStep[0m  [6/21], [94mLoss[0m : 2.46651
[1mStep[0m  [8/21], [94mLoss[0m : 2.42949
[1mStep[0m  [10/21], [94mLoss[0m : 2.60255
[1mStep[0m  [12/21], [94mLoss[0m : 2.41013
[1mStep[0m  [14/21], [94mLoss[0m : 2.39621
[1mStep[0m  [16/21], [94mLoss[0m : 2.41896
[1mStep[0m  [18/21], [94mLoss[0m : 2.39837
[1mStep[0m  [20/21], [94mLoss[0m : 2.33930

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30613
[1mStep[0m  [2/21], [94mLoss[0m : 2.42678
[1mStep[0m  [4/21], [94mLoss[0m : 2.23461
[1mStep[0m  [6/21], [94mLoss[0m : 2.45536
[1mStep[0m  [8/21], [94mLoss[0m : 2.32335
[1mStep[0m  [10/21], [94mLoss[0m : 2.42088
[1mStep[0m  [12/21], [94mLoss[0m : 2.37960
[1mStep[0m  [14/21], [94mLoss[0m : 2.40096
[1mStep[0m  [16/21], [94mLoss[0m : 2.44797
[1mStep[0m  [18/21], [94mLoss[0m : 2.32881
[1mStep[0m  [20/21], [94mLoss[0m : 2.59335

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36748
[1mStep[0m  [2/21], [94mLoss[0m : 2.37748
[1mStep[0m  [4/21], [94mLoss[0m : 2.36632
[1mStep[0m  [6/21], [94mLoss[0m : 2.37451
[1mStep[0m  [8/21], [94mLoss[0m : 2.26961
[1mStep[0m  [10/21], [94mLoss[0m : 2.10201
[1mStep[0m  [12/21], [94mLoss[0m : 2.40830
[1mStep[0m  [14/21], [94mLoss[0m : 2.28430
[1mStep[0m  [16/21], [94mLoss[0m : 2.30559
[1mStep[0m  [18/21], [94mLoss[0m : 2.26374
[1mStep[0m  [20/21], [94mLoss[0m : 2.29208

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32928
[1mStep[0m  [2/21], [94mLoss[0m : 2.31607
[1mStep[0m  [4/21], [94mLoss[0m : 2.34426
[1mStep[0m  [6/21], [94mLoss[0m : 2.32953
[1mStep[0m  [8/21], [94mLoss[0m : 2.34850
[1mStep[0m  [10/21], [94mLoss[0m : 2.32964
[1mStep[0m  [12/21], [94mLoss[0m : 2.34929
[1mStep[0m  [14/21], [94mLoss[0m : 2.15154
[1mStep[0m  [16/21], [94mLoss[0m : 2.25622
[1mStep[0m  [18/21], [94mLoss[0m : 2.35368
[1mStep[0m  [20/21], [94mLoss[0m : 2.19940

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36219
[1mStep[0m  [2/21], [94mLoss[0m : 2.39222
[1mStep[0m  [4/21], [94mLoss[0m : 2.30183
[1mStep[0m  [6/21], [94mLoss[0m : 2.28320
[1mStep[0m  [8/21], [94mLoss[0m : 2.29671
[1mStep[0m  [10/21], [94mLoss[0m : 2.16439
[1mStep[0m  [12/21], [94mLoss[0m : 2.32218
[1mStep[0m  [14/21], [94mLoss[0m : 2.15292
[1mStep[0m  [16/21], [94mLoss[0m : 2.30003
[1mStep[0m  [18/21], [94mLoss[0m : 2.32850
[1mStep[0m  [20/21], [94mLoss[0m : 2.17489

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26967
[1mStep[0m  [2/21], [94mLoss[0m : 2.24045
[1mStep[0m  [4/21], [94mLoss[0m : 2.22463
[1mStep[0m  [6/21], [94mLoss[0m : 2.29616
[1mStep[0m  [8/21], [94mLoss[0m : 2.15793
[1mStep[0m  [10/21], [94mLoss[0m : 2.25962
[1mStep[0m  [12/21], [94mLoss[0m : 2.23564
[1mStep[0m  [14/21], [94mLoss[0m : 2.21057
[1mStep[0m  [16/21], [94mLoss[0m : 2.23587
[1mStep[0m  [18/21], [94mLoss[0m : 2.35362
[1mStep[0m  [20/21], [94mLoss[0m : 2.14963

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.237, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20708
[1mStep[0m  [2/21], [94mLoss[0m : 2.14466
[1mStep[0m  [4/21], [94mLoss[0m : 2.24459
[1mStep[0m  [6/21], [94mLoss[0m : 2.16226
[1mStep[0m  [8/21], [94mLoss[0m : 2.12943
[1mStep[0m  [10/21], [94mLoss[0m : 2.19326
[1mStep[0m  [12/21], [94mLoss[0m : 2.08162
[1mStep[0m  [14/21], [94mLoss[0m : 2.30847
[1mStep[0m  [16/21], [94mLoss[0m : 2.24631
[1mStep[0m  [18/21], [94mLoss[0m : 2.19279
[1mStep[0m  [20/21], [94mLoss[0m : 2.28065

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15559
[1mStep[0m  [2/21], [94mLoss[0m : 2.18156
[1mStep[0m  [4/21], [94mLoss[0m : 2.16712
[1mStep[0m  [6/21], [94mLoss[0m : 2.20346
[1mStep[0m  [8/21], [94mLoss[0m : 2.17791
[1mStep[0m  [10/21], [94mLoss[0m : 2.32734
[1mStep[0m  [12/21], [94mLoss[0m : 2.21636
[1mStep[0m  [14/21], [94mLoss[0m : 2.24545
[1mStep[0m  [16/21], [94mLoss[0m : 2.09753
[1mStep[0m  [18/21], [94mLoss[0m : 2.27261
[1mStep[0m  [20/21], [94mLoss[0m : 2.24126

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11659
[1mStep[0m  [2/21], [94mLoss[0m : 2.19212
[1mStep[0m  [4/21], [94mLoss[0m : 2.12918
[1mStep[0m  [6/21], [94mLoss[0m : 2.26105
[1mStep[0m  [8/21], [94mLoss[0m : 2.09306
[1mStep[0m  [10/21], [94mLoss[0m : 2.31267
[1mStep[0m  [12/21], [94mLoss[0m : 2.25464
[1mStep[0m  [14/21], [94mLoss[0m : 2.19419
[1mStep[0m  [16/21], [94mLoss[0m : 2.14554
[1mStep[0m  [18/21], [94mLoss[0m : 2.03732
[1mStep[0m  [20/21], [94mLoss[0m : 2.26867

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23791
[1mStep[0m  [2/21], [94mLoss[0m : 2.16616
[1mStep[0m  [4/21], [94mLoss[0m : 2.05483
[1mStep[0m  [6/21], [94mLoss[0m : 2.19646
[1mStep[0m  [8/21], [94mLoss[0m : 2.19814
[1mStep[0m  [10/21], [94mLoss[0m : 2.32377
[1mStep[0m  [12/21], [94mLoss[0m : 2.21284
[1mStep[0m  [14/21], [94mLoss[0m : 2.11002
[1mStep[0m  [16/21], [94mLoss[0m : 1.98504
[1mStep[0m  [18/21], [94mLoss[0m : 2.17332
[1mStep[0m  [20/21], [94mLoss[0m : 2.30327

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15454
[1mStep[0m  [2/21], [94mLoss[0m : 2.07783
[1mStep[0m  [4/21], [94mLoss[0m : 2.17517
[1mStep[0m  [6/21], [94mLoss[0m : 2.10029
[1mStep[0m  [8/21], [94mLoss[0m : 2.26183
[1mStep[0m  [10/21], [94mLoss[0m : 2.27878
[1mStep[0m  [12/21], [94mLoss[0m : 2.05486
[1mStep[0m  [14/21], [94mLoss[0m : 2.10236
[1mStep[0m  [16/21], [94mLoss[0m : 2.23225
[1mStep[0m  [18/21], [94mLoss[0m : 2.15495
[1mStep[0m  [20/21], [94mLoss[0m : 2.12227

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18781
[1mStep[0m  [2/21], [94mLoss[0m : 2.03837
[1mStep[0m  [4/21], [94mLoss[0m : 2.16539
[1mStep[0m  [6/21], [94mLoss[0m : 2.02790
[1mStep[0m  [8/21], [94mLoss[0m : 2.07627
[1mStep[0m  [10/21], [94mLoss[0m : 2.19996
[1mStep[0m  [12/21], [94mLoss[0m : 1.99019
[1mStep[0m  [14/21], [94mLoss[0m : 2.01981
[1mStep[0m  [16/21], [94mLoss[0m : 2.17180
[1mStep[0m  [18/21], [94mLoss[0m : 2.11213
[1mStep[0m  [20/21], [94mLoss[0m : 2.11212

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96548
[1mStep[0m  [2/21], [94mLoss[0m : 2.10780
[1mStep[0m  [4/21], [94mLoss[0m : 2.04737
[1mStep[0m  [6/21], [94mLoss[0m : 2.14614
[1mStep[0m  [8/21], [94mLoss[0m : 2.11807
[1mStep[0m  [10/21], [94mLoss[0m : 2.01177
[1mStep[0m  [12/21], [94mLoss[0m : 2.05773
[1mStep[0m  [14/21], [94mLoss[0m : 2.10065
[1mStep[0m  [16/21], [94mLoss[0m : 2.06747
[1mStep[0m  [18/21], [94mLoss[0m : 2.12443
[1mStep[0m  [20/21], [94mLoss[0m : 2.07117

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95372
[1mStep[0m  [2/21], [94mLoss[0m : 2.15724
[1mStep[0m  [4/21], [94mLoss[0m : 2.07196
[1mStep[0m  [6/21], [94mLoss[0m : 1.99664
[1mStep[0m  [8/21], [94mLoss[0m : 2.03826
[1mStep[0m  [10/21], [94mLoss[0m : 2.12928
[1mStep[0m  [12/21], [94mLoss[0m : 2.02220
[1mStep[0m  [14/21], [94mLoss[0m : 2.06627
[1mStep[0m  [16/21], [94mLoss[0m : 2.00461
[1mStep[0m  [18/21], [94mLoss[0m : 2.01950
[1mStep[0m  [20/21], [94mLoss[0m : 2.05841

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.445, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93776
[1mStep[0m  [2/21], [94mLoss[0m : 2.04481
[1mStep[0m  [4/21], [94mLoss[0m : 2.10044
[1mStep[0m  [6/21], [94mLoss[0m : 2.00675
[1mStep[0m  [8/21], [94mLoss[0m : 2.01798
[1mStep[0m  [10/21], [94mLoss[0m : 2.00994
[1mStep[0m  [12/21], [94mLoss[0m : 2.13212
[1mStep[0m  [14/21], [94mLoss[0m : 1.96856
[1mStep[0m  [16/21], [94mLoss[0m : 2.07713
[1mStep[0m  [18/21], [94mLoss[0m : 2.09163
[1mStep[0m  [20/21], [94mLoss[0m : 2.10366

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01576
[1mStep[0m  [2/21], [94mLoss[0m : 2.04516
[1mStep[0m  [4/21], [94mLoss[0m : 2.01689
[1mStep[0m  [6/21], [94mLoss[0m : 2.07007
[1mStep[0m  [8/21], [94mLoss[0m : 1.99826
[1mStep[0m  [10/21], [94mLoss[0m : 2.04837
[1mStep[0m  [12/21], [94mLoss[0m : 2.05783
[1mStep[0m  [14/21], [94mLoss[0m : 2.04471
[1mStep[0m  [16/21], [94mLoss[0m : 2.04618
[1mStep[0m  [18/21], [94mLoss[0m : 2.04050
[1mStep[0m  [20/21], [94mLoss[0m : 2.05711

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90065
[1mStep[0m  [2/21], [94mLoss[0m : 1.83419
[1mStep[0m  [4/21], [94mLoss[0m : 2.01702
[1mStep[0m  [6/21], [94mLoss[0m : 1.91367
[1mStep[0m  [8/21], [94mLoss[0m : 2.04027
[1mStep[0m  [10/21], [94mLoss[0m : 2.15874
[1mStep[0m  [12/21], [94mLoss[0m : 2.05395
[1mStep[0m  [14/21], [94mLoss[0m : 1.98210
[1mStep[0m  [16/21], [94mLoss[0m : 2.10471
[1mStep[0m  [18/21], [94mLoss[0m : 1.98209
[1mStep[0m  [20/21], [94mLoss[0m : 2.03938

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90719
[1mStep[0m  [2/21], [94mLoss[0m : 1.88823
[1mStep[0m  [4/21], [94mLoss[0m : 1.98449
[1mStep[0m  [6/21], [94mLoss[0m : 1.86660
[1mStep[0m  [8/21], [94mLoss[0m : 2.01537
[1mStep[0m  [10/21], [94mLoss[0m : 2.15078
[1mStep[0m  [12/21], [94mLoss[0m : 1.94256
[1mStep[0m  [14/21], [94mLoss[0m : 2.05556
[1mStep[0m  [16/21], [94mLoss[0m : 1.89331
[1mStep[0m  [18/21], [94mLoss[0m : 1.83619
[1mStep[0m  [20/21], [94mLoss[0m : 2.02835

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.473, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96927
[1mStep[0m  [2/21], [94mLoss[0m : 1.92097
[1mStep[0m  [4/21], [94mLoss[0m : 2.03220
[1mStep[0m  [6/21], [94mLoss[0m : 2.06470
[1mStep[0m  [8/21], [94mLoss[0m : 1.99419
[1mStep[0m  [10/21], [94mLoss[0m : 1.74034
[1mStep[0m  [12/21], [94mLoss[0m : 2.07880
[1mStep[0m  [14/21], [94mLoss[0m : 2.00897
[1mStep[0m  [16/21], [94mLoss[0m : 2.05714
[1mStep[0m  [18/21], [94mLoss[0m : 1.96593
[1mStep[0m  [20/21], [94mLoss[0m : 1.90231

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.464, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.457
====================================

Phase 2 - Evaluation MAE:  2.4567363602774486
MAE score P1      2.373834
MAE score P2      2.456736
loss              1.950855
learning_rate      0.00505
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.57355
[1mStep[0m  [33/339], [94mLoss[0m : 2.77105
[1mStep[0m  [66/339], [94mLoss[0m : 2.99724
[1mStep[0m  [99/339], [94mLoss[0m : 2.84578
[1mStep[0m  [132/339], [94mLoss[0m : 2.68268
[1mStep[0m  [165/339], [94mLoss[0m : 2.35925
[1mStep[0m  [198/339], [94mLoss[0m : 2.63804
[1mStep[0m  [231/339], [94mLoss[0m : 2.26582
[1mStep[0m  [264/339], [94mLoss[0m : 3.23478
[1mStep[0m  [297/339], [94mLoss[0m : 2.89621
[1mStep[0m  [330/339], [94mLoss[0m : 2.67654

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.871, [92mTest[0m: 10.942, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32366
[1mStep[0m  [33/339], [94mLoss[0m : 2.08496
[1mStep[0m  [66/339], [94mLoss[0m : 1.86732
[1mStep[0m  [99/339], [94mLoss[0m : 2.83227
[1mStep[0m  [132/339], [94mLoss[0m : 2.96905
[1mStep[0m  [165/339], [94mLoss[0m : 2.57595
[1mStep[0m  [198/339], [94mLoss[0m : 2.77992
[1mStep[0m  [231/339], [94mLoss[0m : 2.40582
[1mStep[0m  [264/339], [94mLoss[0m : 2.25914
[1mStep[0m  [297/339], [94mLoss[0m : 2.72534
[1mStep[0m  [330/339], [94mLoss[0m : 2.71306

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.94346
[1mStep[0m  [33/339], [94mLoss[0m : 2.78564
[1mStep[0m  [66/339], [94mLoss[0m : 2.93735
[1mStep[0m  [99/339], [94mLoss[0m : 2.27707
[1mStep[0m  [132/339], [94mLoss[0m : 2.27953
[1mStep[0m  [165/339], [94mLoss[0m : 2.63950
[1mStep[0m  [198/339], [94mLoss[0m : 1.75904
[1mStep[0m  [231/339], [94mLoss[0m : 2.00007
[1mStep[0m  [264/339], [94mLoss[0m : 2.47102
[1mStep[0m  [297/339], [94mLoss[0m : 2.84849
[1mStep[0m  [330/339], [94mLoss[0m : 2.50367

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17854
[1mStep[0m  [33/339], [94mLoss[0m : 2.27341
[1mStep[0m  [66/339], [94mLoss[0m : 2.34396
[1mStep[0m  [99/339], [94mLoss[0m : 2.07776
[1mStep[0m  [132/339], [94mLoss[0m : 2.79897
[1mStep[0m  [165/339], [94mLoss[0m : 2.00955
[1mStep[0m  [198/339], [94mLoss[0m : 2.92815
[1mStep[0m  [231/339], [94mLoss[0m : 2.65923
[1mStep[0m  [264/339], [94mLoss[0m : 2.58032
[1mStep[0m  [297/339], [94mLoss[0m : 2.36117
[1mStep[0m  [330/339], [94mLoss[0m : 1.84677

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89480
[1mStep[0m  [33/339], [94mLoss[0m : 2.60227
[1mStep[0m  [66/339], [94mLoss[0m : 2.64988
[1mStep[0m  [99/339], [94mLoss[0m : 2.13893
[1mStep[0m  [132/339], [94mLoss[0m : 2.07724
[1mStep[0m  [165/339], [94mLoss[0m : 1.98960
[1mStep[0m  [198/339], [94mLoss[0m : 1.79069
[1mStep[0m  [231/339], [94mLoss[0m : 2.60579
[1mStep[0m  [264/339], [94mLoss[0m : 3.24101
[1mStep[0m  [297/339], [94mLoss[0m : 2.58092
[1mStep[0m  [330/339], [94mLoss[0m : 2.23943

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63081
[1mStep[0m  [33/339], [94mLoss[0m : 2.64083
[1mStep[0m  [66/339], [94mLoss[0m : 2.29899
[1mStep[0m  [99/339], [94mLoss[0m : 2.46746
[1mStep[0m  [132/339], [94mLoss[0m : 2.23271
[1mStep[0m  [165/339], [94mLoss[0m : 2.50075
[1mStep[0m  [198/339], [94mLoss[0m : 2.24230
[1mStep[0m  [231/339], [94mLoss[0m : 1.92467
[1mStep[0m  [264/339], [94mLoss[0m : 2.12021
[1mStep[0m  [297/339], [94mLoss[0m : 2.58418
[1mStep[0m  [330/339], [94mLoss[0m : 2.68784

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27674
[1mStep[0m  [33/339], [94mLoss[0m : 2.90039
[1mStep[0m  [66/339], [94mLoss[0m : 1.99766
[1mStep[0m  [99/339], [94mLoss[0m : 2.28251
[1mStep[0m  [132/339], [94mLoss[0m : 2.83310
[1mStep[0m  [165/339], [94mLoss[0m : 2.16345
[1mStep[0m  [198/339], [94mLoss[0m : 2.06273
[1mStep[0m  [231/339], [94mLoss[0m : 2.90307
[1mStep[0m  [264/339], [94mLoss[0m : 2.43101
[1mStep[0m  [297/339], [94mLoss[0m : 2.75940
[1mStep[0m  [330/339], [94mLoss[0m : 2.78055

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41065
[1mStep[0m  [33/339], [94mLoss[0m : 2.73558
[1mStep[0m  [66/339], [94mLoss[0m : 2.54590
[1mStep[0m  [99/339], [94mLoss[0m : 2.68593
[1mStep[0m  [132/339], [94mLoss[0m : 2.76787
[1mStep[0m  [165/339], [94mLoss[0m : 2.49880
[1mStep[0m  [198/339], [94mLoss[0m : 2.72451
[1mStep[0m  [231/339], [94mLoss[0m : 2.58848
[1mStep[0m  [264/339], [94mLoss[0m : 2.36836
[1mStep[0m  [297/339], [94mLoss[0m : 1.74608
[1mStep[0m  [330/339], [94mLoss[0m : 2.94356

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22369
[1mStep[0m  [33/339], [94mLoss[0m : 2.41001
[1mStep[0m  [66/339], [94mLoss[0m : 1.91674
[1mStep[0m  [99/339], [94mLoss[0m : 2.80533
[1mStep[0m  [132/339], [94mLoss[0m : 2.34859
[1mStep[0m  [165/339], [94mLoss[0m : 2.30091
[1mStep[0m  [198/339], [94mLoss[0m : 2.95180
[1mStep[0m  [231/339], [94mLoss[0m : 2.56280
[1mStep[0m  [264/339], [94mLoss[0m : 2.09828
[1mStep[0m  [297/339], [94mLoss[0m : 3.35437
[1mStep[0m  [330/339], [94mLoss[0m : 2.31280

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.311, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24610
[1mStep[0m  [33/339], [94mLoss[0m : 2.15194
[1mStep[0m  [66/339], [94mLoss[0m : 1.98332
[1mStep[0m  [99/339], [94mLoss[0m : 2.02559
[1mStep[0m  [132/339], [94mLoss[0m : 2.47758
[1mStep[0m  [165/339], [94mLoss[0m : 2.72682
[1mStep[0m  [198/339], [94mLoss[0m : 2.30307
[1mStep[0m  [231/339], [94mLoss[0m : 2.25210
[1mStep[0m  [264/339], [94mLoss[0m : 2.66606
[1mStep[0m  [297/339], [94mLoss[0m : 2.32934
[1mStep[0m  [330/339], [94mLoss[0m : 2.07088

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.316, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.02765
[1mStep[0m  [33/339], [94mLoss[0m : 2.15690
[1mStep[0m  [66/339], [94mLoss[0m : 1.93958
[1mStep[0m  [99/339], [94mLoss[0m : 3.08013
[1mStep[0m  [132/339], [94mLoss[0m : 2.12505
[1mStep[0m  [165/339], [94mLoss[0m : 2.71877
[1mStep[0m  [198/339], [94mLoss[0m : 2.43150
[1mStep[0m  [231/339], [94mLoss[0m : 1.92250
[1mStep[0m  [264/339], [94mLoss[0m : 2.48743
[1mStep[0m  [297/339], [94mLoss[0m : 2.75283
[1mStep[0m  [330/339], [94mLoss[0m : 2.36843

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58617
[1mStep[0m  [33/339], [94mLoss[0m : 1.51922
[1mStep[0m  [66/339], [94mLoss[0m : 2.95921
[1mStep[0m  [99/339], [94mLoss[0m : 2.19308
[1mStep[0m  [132/339], [94mLoss[0m : 2.71934
[1mStep[0m  [165/339], [94mLoss[0m : 2.55291
[1mStep[0m  [198/339], [94mLoss[0m : 1.69659
[1mStep[0m  [231/339], [94mLoss[0m : 1.90472
[1mStep[0m  [264/339], [94mLoss[0m : 2.55349
[1mStep[0m  [297/339], [94mLoss[0m : 2.42894
[1mStep[0m  [330/339], [94mLoss[0m : 2.42697

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92624
[1mStep[0m  [33/339], [94mLoss[0m : 2.62101
[1mStep[0m  [66/339], [94mLoss[0m : 2.02553
[1mStep[0m  [99/339], [94mLoss[0m : 2.51127
[1mStep[0m  [132/339], [94mLoss[0m : 2.11845
[1mStep[0m  [165/339], [94mLoss[0m : 1.97555
[1mStep[0m  [198/339], [94mLoss[0m : 2.45480
[1mStep[0m  [231/339], [94mLoss[0m : 2.37030
[1mStep[0m  [264/339], [94mLoss[0m : 2.60614
[1mStep[0m  [297/339], [94mLoss[0m : 2.35928
[1mStep[0m  [330/339], [94mLoss[0m : 2.15201

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01454
[1mStep[0m  [33/339], [94mLoss[0m : 2.43753
[1mStep[0m  [66/339], [94mLoss[0m : 2.61584
[1mStep[0m  [99/339], [94mLoss[0m : 2.43863
[1mStep[0m  [132/339], [94mLoss[0m : 2.27915
[1mStep[0m  [165/339], [94mLoss[0m : 2.14274
[1mStep[0m  [198/339], [94mLoss[0m : 2.84907
[1mStep[0m  [231/339], [94mLoss[0m : 2.23670
[1mStep[0m  [264/339], [94mLoss[0m : 1.58863
[1mStep[0m  [297/339], [94mLoss[0m : 1.97527
[1mStep[0m  [330/339], [94mLoss[0m : 2.35822

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29096
[1mStep[0m  [33/339], [94mLoss[0m : 2.06324
[1mStep[0m  [66/339], [94mLoss[0m : 1.94751
[1mStep[0m  [99/339], [94mLoss[0m : 1.94545
[1mStep[0m  [132/339], [94mLoss[0m : 2.61342
[1mStep[0m  [165/339], [94mLoss[0m : 2.39226
[1mStep[0m  [198/339], [94mLoss[0m : 2.53951
[1mStep[0m  [231/339], [94mLoss[0m : 2.22193
[1mStep[0m  [264/339], [94mLoss[0m : 2.40359
[1mStep[0m  [297/339], [94mLoss[0m : 2.34853
[1mStep[0m  [330/339], [94mLoss[0m : 2.03930

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68577
[1mStep[0m  [33/339], [94mLoss[0m : 2.31608
[1mStep[0m  [66/339], [94mLoss[0m : 2.47547
[1mStep[0m  [99/339], [94mLoss[0m : 2.13222
[1mStep[0m  [132/339], [94mLoss[0m : 2.02583
[1mStep[0m  [165/339], [94mLoss[0m : 2.70957
[1mStep[0m  [198/339], [94mLoss[0m : 2.85029
[1mStep[0m  [231/339], [94mLoss[0m : 2.31084
[1mStep[0m  [264/339], [94mLoss[0m : 2.74814
[1mStep[0m  [297/339], [94mLoss[0m : 2.33931
[1mStep[0m  [330/339], [94mLoss[0m : 2.25715

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.302, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08895
[1mStep[0m  [33/339], [94mLoss[0m : 2.12256
[1mStep[0m  [66/339], [94mLoss[0m : 2.41109
[1mStep[0m  [99/339], [94mLoss[0m : 2.41911
[1mStep[0m  [132/339], [94mLoss[0m : 2.27270
[1mStep[0m  [165/339], [94mLoss[0m : 2.56452
[1mStep[0m  [198/339], [94mLoss[0m : 2.27785
[1mStep[0m  [231/339], [94mLoss[0m : 2.34515
[1mStep[0m  [264/339], [94mLoss[0m : 2.88573
[1mStep[0m  [297/339], [94mLoss[0m : 2.02183
[1mStep[0m  [330/339], [94mLoss[0m : 2.52825

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10815
[1mStep[0m  [33/339], [94mLoss[0m : 2.06563
[1mStep[0m  [66/339], [94mLoss[0m : 2.43275
[1mStep[0m  [99/339], [94mLoss[0m : 1.64792
[1mStep[0m  [132/339], [94mLoss[0m : 3.02067
[1mStep[0m  [165/339], [94mLoss[0m : 2.43466
[1mStep[0m  [198/339], [94mLoss[0m : 2.12916
[1mStep[0m  [231/339], [94mLoss[0m : 2.22879
[1mStep[0m  [264/339], [94mLoss[0m : 2.01019
[1mStep[0m  [297/339], [94mLoss[0m : 2.69278
[1mStep[0m  [330/339], [94mLoss[0m : 2.26421

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.92308
[1mStep[0m  [33/339], [94mLoss[0m : 2.44480
[1mStep[0m  [66/339], [94mLoss[0m : 2.31197
[1mStep[0m  [99/339], [94mLoss[0m : 2.67011
[1mStep[0m  [132/339], [94mLoss[0m : 2.24782
[1mStep[0m  [165/339], [94mLoss[0m : 1.92775
[1mStep[0m  [198/339], [94mLoss[0m : 2.99662
[1mStep[0m  [231/339], [94mLoss[0m : 2.05962
[1mStep[0m  [264/339], [94mLoss[0m : 2.33790
[1mStep[0m  [297/339], [94mLoss[0m : 2.14971
[1mStep[0m  [330/339], [94mLoss[0m : 2.14893

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73399
[1mStep[0m  [33/339], [94mLoss[0m : 1.88532
[1mStep[0m  [66/339], [94mLoss[0m : 1.67161
[1mStep[0m  [99/339], [94mLoss[0m : 1.97666
[1mStep[0m  [132/339], [94mLoss[0m : 1.75595
[1mStep[0m  [165/339], [94mLoss[0m : 2.13349
[1mStep[0m  [198/339], [94mLoss[0m : 2.69157
[1mStep[0m  [231/339], [94mLoss[0m : 2.03012
[1mStep[0m  [264/339], [94mLoss[0m : 2.14344
[1mStep[0m  [297/339], [94mLoss[0m : 2.40827
[1mStep[0m  [330/339], [94mLoss[0m : 2.01764

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.309, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71190
[1mStep[0m  [33/339], [94mLoss[0m : 2.38749
[1mStep[0m  [66/339], [94mLoss[0m : 2.13077
[1mStep[0m  [99/339], [94mLoss[0m : 2.12007
[1mStep[0m  [132/339], [94mLoss[0m : 2.46950
[1mStep[0m  [165/339], [94mLoss[0m : 2.72230
[1mStep[0m  [198/339], [94mLoss[0m : 2.47911
[1mStep[0m  [231/339], [94mLoss[0m : 2.62283
[1mStep[0m  [264/339], [94mLoss[0m : 1.93336
[1mStep[0m  [297/339], [94mLoss[0m : 2.99538
[1mStep[0m  [330/339], [94mLoss[0m : 2.53459

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06364
[1mStep[0m  [33/339], [94mLoss[0m : 2.59507
[1mStep[0m  [66/339], [94mLoss[0m : 2.10300
[1mStep[0m  [99/339], [94mLoss[0m : 2.68710
[1mStep[0m  [132/339], [94mLoss[0m : 2.43260
[1mStep[0m  [165/339], [94mLoss[0m : 2.26545
[1mStep[0m  [198/339], [94mLoss[0m : 2.13919
[1mStep[0m  [231/339], [94mLoss[0m : 1.37490
[1mStep[0m  [264/339], [94mLoss[0m : 2.23085
[1mStep[0m  [297/339], [94mLoss[0m : 2.43754
[1mStep[0m  [330/339], [94mLoss[0m : 2.22829

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.318, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87729
[1mStep[0m  [33/339], [94mLoss[0m : 2.56833
[1mStep[0m  [66/339], [94mLoss[0m : 2.65803
[1mStep[0m  [99/339], [94mLoss[0m : 2.57795
[1mStep[0m  [132/339], [94mLoss[0m : 2.78767
[1mStep[0m  [165/339], [94mLoss[0m : 2.36920
[1mStep[0m  [198/339], [94mLoss[0m : 1.92657
[1mStep[0m  [231/339], [94mLoss[0m : 2.26218
[1mStep[0m  [264/339], [94mLoss[0m : 2.28683
[1mStep[0m  [297/339], [94mLoss[0m : 2.45290
[1mStep[0m  [330/339], [94mLoss[0m : 1.92045

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65003
[1mStep[0m  [33/339], [94mLoss[0m : 2.23201
[1mStep[0m  [66/339], [94mLoss[0m : 2.80786
[1mStep[0m  [99/339], [94mLoss[0m : 2.17690
[1mStep[0m  [132/339], [94mLoss[0m : 2.67263
[1mStep[0m  [165/339], [94mLoss[0m : 2.38204
[1mStep[0m  [198/339], [94mLoss[0m : 1.91566
[1mStep[0m  [231/339], [94mLoss[0m : 2.37656
[1mStep[0m  [264/339], [94mLoss[0m : 1.81615
[1mStep[0m  [297/339], [94mLoss[0m : 2.18598
[1mStep[0m  [330/339], [94mLoss[0m : 2.44305

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.302, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31616
[1mStep[0m  [33/339], [94mLoss[0m : 1.65667
[1mStep[0m  [66/339], [94mLoss[0m : 2.39032
[1mStep[0m  [99/339], [94mLoss[0m : 1.91928
[1mStep[0m  [132/339], [94mLoss[0m : 2.37729
[1mStep[0m  [165/339], [94mLoss[0m : 2.90602
[1mStep[0m  [198/339], [94mLoss[0m : 2.38051
[1mStep[0m  [231/339], [94mLoss[0m : 2.14899
[1mStep[0m  [264/339], [94mLoss[0m : 2.45547
[1mStep[0m  [297/339], [94mLoss[0m : 2.28249
[1mStep[0m  [330/339], [94mLoss[0m : 2.53397

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.311, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23435
[1mStep[0m  [33/339], [94mLoss[0m : 2.19015
[1mStep[0m  [66/339], [94mLoss[0m : 1.92591
[1mStep[0m  [99/339], [94mLoss[0m : 2.75078
[1mStep[0m  [132/339], [94mLoss[0m : 2.58180
[1mStep[0m  [165/339], [94mLoss[0m : 2.26704
[1mStep[0m  [198/339], [94mLoss[0m : 2.71245
[1mStep[0m  [231/339], [94mLoss[0m : 1.85384
[1mStep[0m  [264/339], [94mLoss[0m : 1.88133
[1mStep[0m  [297/339], [94mLoss[0m : 2.80706
[1mStep[0m  [330/339], [94mLoss[0m : 2.31598

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59955
[1mStep[0m  [33/339], [94mLoss[0m : 2.62767
[1mStep[0m  [66/339], [94mLoss[0m : 2.75693
[1mStep[0m  [99/339], [94mLoss[0m : 2.53948
[1mStep[0m  [132/339], [94mLoss[0m : 2.09134
[1mStep[0m  [165/339], [94mLoss[0m : 2.33346
[1mStep[0m  [198/339], [94mLoss[0m : 2.77835
[1mStep[0m  [231/339], [94mLoss[0m : 1.97128
[1mStep[0m  [264/339], [94mLoss[0m : 2.67260
[1mStep[0m  [297/339], [94mLoss[0m : 2.44636
[1mStep[0m  [330/339], [94mLoss[0m : 2.66816

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.305, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72599
[1mStep[0m  [33/339], [94mLoss[0m : 2.18809
[1mStep[0m  [66/339], [94mLoss[0m : 2.18296
[1mStep[0m  [99/339], [94mLoss[0m : 2.25459
[1mStep[0m  [132/339], [94mLoss[0m : 3.03246
[1mStep[0m  [165/339], [94mLoss[0m : 1.97974
[1mStep[0m  [198/339], [94mLoss[0m : 2.82096
[1mStep[0m  [231/339], [94mLoss[0m : 2.56283
[1mStep[0m  [264/339], [94mLoss[0m : 2.86297
[1mStep[0m  [297/339], [94mLoss[0m : 2.62709
[1mStep[0m  [330/339], [94mLoss[0m : 2.81304

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19507
[1mStep[0m  [33/339], [94mLoss[0m : 2.21092
[1mStep[0m  [66/339], [94mLoss[0m : 2.81501
[1mStep[0m  [99/339], [94mLoss[0m : 2.37172
[1mStep[0m  [132/339], [94mLoss[0m : 2.47246
[1mStep[0m  [165/339], [94mLoss[0m : 2.16119
[1mStep[0m  [198/339], [94mLoss[0m : 2.61919
[1mStep[0m  [231/339], [94mLoss[0m : 2.44027
[1mStep[0m  [264/339], [94mLoss[0m : 2.22288
[1mStep[0m  [297/339], [94mLoss[0m : 2.22094
[1mStep[0m  [330/339], [94mLoss[0m : 2.37095

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57671
[1mStep[0m  [33/339], [94mLoss[0m : 2.91995
[1mStep[0m  [66/339], [94mLoss[0m : 1.62497
[1mStep[0m  [99/339], [94mLoss[0m : 2.68391
[1mStep[0m  [132/339], [94mLoss[0m : 2.19857
[1mStep[0m  [165/339], [94mLoss[0m : 2.42698
[1mStep[0m  [198/339], [94mLoss[0m : 2.78267
[1mStep[0m  [231/339], [94mLoss[0m : 2.26334
[1mStep[0m  [264/339], [94mLoss[0m : 2.32462
[1mStep[0m  [297/339], [94mLoss[0m : 2.48735
[1mStep[0m  [330/339], [94mLoss[0m : 2.67450

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.305, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.286
====================================

Phase 1 - Evaluation MAE:  2.286415376494416
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.27415
[1mStep[0m  [33/339], [94mLoss[0m : 2.48254
[1mStep[0m  [66/339], [94mLoss[0m : 2.41136
[1mStep[0m  [99/339], [94mLoss[0m : 2.35730
[1mStep[0m  [132/339], [94mLoss[0m : 2.57460
[1mStep[0m  [165/339], [94mLoss[0m : 2.15680
[1mStep[0m  [198/339], [94mLoss[0m : 3.23752
[1mStep[0m  [231/339], [94mLoss[0m : 2.74063
[1mStep[0m  [264/339], [94mLoss[0m : 2.42379
[1mStep[0m  [297/339], [94mLoss[0m : 2.37284
[1mStep[0m  [330/339], [94mLoss[0m : 2.25778

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.286, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31316
[1mStep[0m  [33/339], [94mLoss[0m : 2.12052
[1mStep[0m  [66/339], [94mLoss[0m : 2.12123
[1mStep[0m  [99/339], [94mLoss[0m : 2.87677
[1mStep[0m  [132/339], [94mLoss[0m : 2.14406
[1mStep[0m  [165/339], [94mLoss[0m : 1.90048
[1mStep[0m  [198/339], [94mLoss[0m : 2.13401
[1mStep[0m  [231/339], [94mLoss[0m : 2.33029
[1mStep[0m  [264/339], [94mLoss[0m : 2.35859
[1mStep[0m  [297/339], [94mLoss[0m : 3.19429
[1mStep[0m  [330/339], [94mLoss[0m : 2.54783

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78488
[1mStep[0m  [33/339], [94mLoss[0m : 2.24394
[1mStep[0m  [66/339], [94mLoss[0m : 2.51433
[1mStep[0m  [99/339], [94mLoss[0m : 2.24510
[1mStep[0m  [132/339], [94mLoss[0m : 2.12950
[1mStep[0m  [165/339], [94mLoss[0m : 2.70376
[1mStep[0m  [198/339], [94mLoss[0m : 2.09935
[1mStep[0m  [231/339], [94mLoss[0m : 2.95587
[1mStep[0m  [264/339], [94mLoss[0m : 1.99931
[1mStep[0m  [297/339], [94mLoss[0m : 2.29322
[1mStep[0m  [330/339], [94mLoss[0m : 2.38593

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92994
[1mStep[0m  [33/339], [94mLoss[0m : 2.40993
[1mStep[0m  [66/339], [94mLoss[0m : 2.56443
[1mStep[0m  [99/339], [94mLoss[0m : 1.79334
[1mStep[0m  [132/339], [94mLoss[0m : 2.16467
[1mStep[0m  [165/339], [94mLoss[0m : 2.43224
[1mStep[0m  [198/339], [94mLoss[0m : 2.90141
[1mStep[0m  [231/339], [94mLoss[0m : 2.06977
[1mStep[0m  [264/339], [94mLoss[0m : 2.39113
[1mStep[0m  [297/339], [94mLoss[0m : 2.17896
[1mStep[0m  [330/339], [94mLoss[0m : 2.50236

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86354
[1mStep[0m  [33/339], [94mLoss[0m : 2.32689
[1mStep[0m  [66/339], [94mLoss[0m : 2.27422
[1mStep[0m  [99/339], [94mLoss[0m : 2.02126
[1mStep[0m  [132/339], [94mLoss[0m : 2.72876
[1mStep[0m  [165/339], [94mLoss[0m : 2.26824
[1mStep[0m  [198/339], [94mLoss[0m : 1.45468
[1mStep[0m  [231/339], [94mLoss[0m : 1.87752
[1mStep[0m  [264/339], [94mLoss[0m : 2.36445
[1mStep[0m  [297/339], [94mLoss[0m : 2.08631
[1mStep[0m  [330/339], [94mLoss[0m : 2.42782

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88639
[1mStep[0m  [33/339], [94mLoss[0m : 2.55798
[1mStep[0m  [66/339], [94mLoss[0m : 2.05846
[1mStep[0m  [99/339], [94mLoss[0m : 2.37287
[1mStep[0m  [132/339], [94mLoss[0m : 1.43105
[1mStep[0m  [165/339], [94mLoss[0m : 1.95763
[1mStep[0m  [198/339], [94mLoss[0m : 2.63530
[1mStep[0m  [231/339], [94mLoss[0m : 2.33260
[1mStep[0m  [264/339], [94mLoss[0m : 2.40500
[1mStep[0m  [297/339], [94mLoss[0m : 2.97714
[1mStep[0m  [330/339], [94mLoss[0m : 2.32799

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19029
[1mStep[0m  [33/339], [94mLoss[0m : 2.46149
[1mStep[0m  [66/339], [94mLoss[0m : 2.51736
[1mStep[0m  [99/339], [94mLoss[0m : 1.86646
[1mStep[0m  [132/339], [94mLoss[0m : 2.31214
[1mStep[0m  [165/339], [94mLoss[0m : 2.27458
[1mStep[0m  [198/339], [94mLoss[0m : 1.84099
[1mStep[0m  [231/339], [94mLoss[0m : 2.00460
[1mStep[0m  [264/339], [94mLoss[0m : 2.49465
[1mStep[0m  [297/339], [94mLoss[0m : 2.00734
[1mStep[0m  [330/339], [94mLoss[0m : 1.76046

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31377
[1mStep[0m  [33/339], [94mLoss[0m : 2.72933
[1mStep[0m  [66/339], [94mLoss[0m : 2.33782
[1mStep[0m  [99/339], [94mLoss[0m : 2.34295
[1mStep[0m  [132/339], [94mLoss[0m : 2.42336
[1mStep[0m  [165/339], [94mLoss[0m : 1.56713
[1mStep[0m  [198/339], [94mLoss[0m : 2.20644
[1mStep[0m  [231/339], [94mLoss[0m : 2.26711
[1mStep[0m  [264/339], [94mLoss[0m : 1.52218
[1mStep[0m  [297/339], [94mLoss[0m : 1.97307
[1mStep[0m  [330/339], [94mLoss[0m : 2.45329

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74087
[1mStep[0m  [33/339], [94mLoss[0m : 2.17623
[1mStep[0m  [66/339], [94mLoss[0m : 1.74369
[1mStep[0m  [99/339], [94mLoss[0m : 2.01499
[1mStep[0m  [132/339], [94mLoss[0m : 2.03375
[1mStep[0m  [165/339], [94mLoss[0m : 2.60255
[1mStep[0m  [198/339], [94mLoss[0m : 2.02043
[1mStep[0m  [231/339], [94mLoss[0m : 2.05395
[1mStep[0m  [264/339], [94mLoss[0m : 1.62809
[1mStep[0m  [297/339], [94mLoss[0m : 2.35312
[1mStep[0m  [330/339], [94mLoss[0m : 1.90227

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91927
[1mStep[0m  [33/339], [94mLoss[0m : 1.87239
[1mStep[0m  [66/339], [94mLoss[0m : 1.81515
[1mStep[0m  [99/339], [94mLoss[0m : 1.58098
[1mStep[0m  [132/339], [94mLoss[0m : 2.31145
[1mStep[0m  [165/339], [94mLoss[0m : 1.69979
[1mStep[0m  [198/339], [94mLoss[0m : 1.85664
[1mStep[0m  [231/339], [94mLoss[0m : 2.08042
[1mStep[0m  [264/339], [94mLoss[0m : 2.08333
[1mStep[0m  [297/339], [94mLoss[0m : 1.81425
[1mStep[0m  [330/339], [94mLoss[0m : 1.47514

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53820
[1mStep[0m  [33/339], [94mLoss[0m : 2.15836
[1mStep[0m  [66/339], [94mLoss[0m : 1.85858
[1mStep[0m  [99/339], [94mLoss[0m : 1.51371
[1mStep[0m  [132/339], [94mLoss[0m : 1.80822
[1mStep[0m  [165/339], [94mLoss[0m : 2.06879
[1mStep[0m  [198/339], [94mLoss[0m : 2.12462
[1mStep[0m  [231/339], [94mLoss[0m : 1.88290
[1mStep[0m  [264/339], [94mLoss[0m : 1.61522
[1mStep[0m  [297/339], [94mLoss[0m : 1.69724
[1mStep[0m  [330/339], [94mLoss[0m : 2.24260

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52743
[1mStep[0m  [33/339], [94mLoss[0m : 1.66126
[1mStep[0m  [66/339], [94mLoss[0m : 1.77007
[1mStep[0m  [99/339], [94mLoss[0m : 1.70399
[1mStep[0m  [132/339], [94mLoss[0m : 1.92160
[1mStep[0m  [165/339], [94mLoss[0m : 2.12616
[1mStep[0m  [198/339], [94mLoss[0m : 1.71764
[1mStep[0m  [231/339], [94mLoss[0m : 1.32160
[1mStep[0m  [264/339], [94mLoss[0m : 1.44046
[1mStep[0m  [297/339], [94mLoss[0m : 1.80505
[1mStep[0m  [330/339], [94mLoss[0m : 2.43543

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00926
[1mStep[0m  [33/339], [94mLoss[0m : 2.20284
[1mStep[0m  [66/339], [94mLoss[0m : 1.53683
[1mStep[0m  [99/339], [94mLoss[0m : 1.52499
[1mStep[0m  [132/339], [94mLoss[0m : 2.08752
[1mStep[0m  [165/339], [94mLoss[0m : 1.53145
[1mStep[0m  [198/339], [94mLoss[0m : 1.46230
[1mStep[0m  [231/339], [94mLoss[0m : 2.00346
[1mStep[0m  [264/339], [94mLoss[0m : 1.57613
[1mStep[0m  [297/339], [94mLoss[0m : 1.79973
[1mStep[0m  [330/339], [94mLoss[0m : 1.80926

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70071
[1mStep[0m  [33/339], [94mLoss[0m : 1.49299
[1mStep[0m  [66/339], [94mLoss[0m : 1.90410
[1mStep[0m  [99/339], [94mLoss[0m : 1.98122
[1mStep[0m  [132/339], [94mLoss[0m : 2.32509
[1mStep[0m  [165/339], [94mLoss[0m : 1.45420
[1mStep[0m  [198/339], [94mLoss[0m : 2.12830
[1mStep[0m  [231/339], [94mLoss[0m : 2.05555
[1mStep[0m  [264/339], [94mLoss[0m : 2.02261
[1mStep[0m  [297/339], [94mLoss[0m : 1.98980
[1mStep[0m  [330/339], [94mLoss[0m : 1.75598

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25731
[1mStep[0m  [33/339], [94mLoss[0m : 1.62700
[1mStep[0m  [66/339], [94mLoss[0m : 2.19131
[1mStep[0m  [99/339], [94mLoss[0m : 1.75226
[1mStep[0m  [132/339], [94mLoss[0m : 1.50921
[1mStep[0m  [165/339], [94mLoss[0m : 1.63515
[1mStep[0m  [198/339], [94mLoss[0m : 1.50469
[1mStep[0m  [231/339], [94mLoss[0m : 1.85685
[1mStep[0m  [264/339], [94mLoss[0m : 1.28736
[1mStep[0m  [297/339], [94mLoss[0m : 1.49138
[1mStep[0m  [330/339], [94mLoss[0m : 1.68087

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02993
[1mStep[0m  [33/339], [94mLoss[0m : 2.01767
[1mStep[0m  [66/339], [94mLoss[0m : 1.55987
[1mStep[0m  [99/339], [94mLoss[0m : 1.90244
[1mStep[0m  [132/339], [94mLoss[0m : 1.87297
[1mStep[0m  [165/339], [94mLoss[0m : 1.97461
[1mStep[0m  [198/339], [94mLoss[0m : 1.42707
[1mStep[0m  [231/339], [94mLoss[0m : 2.25461
[1mStep[0m  [264/339], [94mLoss[0m : 1.94844
[1mStep[0m  [297/339], [94mLoss[0m : 1.88602
[1mStep[0m  [330/339], [94mLoss[0m : 2.05120

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95261
[1mStep[0m  [33/339], [94mLoss[0m : 1.53094
[1mStep[0m  [66/339], [94mLoss[0m : 1.45047
[1mStep[0m  [99/339], [94mLoss[0m : 2.04288
[1mStep[0m  [132/339], [94mLoss[0m : 1.75637
[1mStep[0m  [165/339], [94mLoss[0m : 1.84307
[1mStep[0m  [198/339], [94mLoss[0m : 1.47696
[1mStep[0m  [231/339], [94mLoss[0m : 1.83565
[1mStep[0m  [264/339], [94mLoss[0m : 1.61729
[1mStep[0m  [297/339], [94mLoss[0m : 1.76439
[1mStep[0m  [330/339], [94mLoss[0m : 1.39787

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46153
[1mStep[0m  [33/339], [94mLoss[0m : 1.49785
[1mStep[0m  [66/339], [94mLoss[0m : 1.54670
[1mStep[0m  [99/339], [94mLoss[0m : 1.67163
[1mStep[0m  [132/339], [94mLoss[0m : 2.09898
[1mStep[0m  [165/339], [94mLoss[0m : 2.24861
[1mStep[0m  [198/339], [94mLoss[0m : 1.61918
[1mStep[0m  [231/339], [94mLoss[0m : 1.76938
[1mStep[0m  [264/339], [94mLoss[0m : 2.04637
[1mStep[0m  [297/339], [94mLoss[0m : 2.09167
[1mStep[0m  [330/339], [94mLoss[0m : 1.63530

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47958
[1mStep[0m  [33/339], [94mLoss[0m : 1.76028
[1mStep[0m  [66/339], [94mLoss[0m : 1.60128
[1mStep[0m  [99/339], [94mLoss[0m : 1.57162
[1mStep[0m  [132/339], [94mLoss[0m : 2.04126
[1mStep[0m  [165/339], [94mLoss[0m : 2.00940
[1mStep[0m  [198/339], [94mLoss[0m : 2.04538
[1mStep[0m  [231/339], [94mLoss[0m : 1.94097
[1mStep[0m  [264/339], [94mLoss[0m : 1.94051
[1mStep[0m  [297/339], [94mLoss[0m : 1.90916
[1mStep[0m  [330/339], [94mLoss[0m : 1.33030

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.658, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64419
[1mStep[0m  [33/339], [94mLoss[0m : 1.27678
[1mStep[0m  [66/339], [94mLoss[0m : 1.44561
[1mStep[0m  [99/339], [94mLoss[0m : 2.01347
[1mStep[0m  [132/339], [94mLoss[0m : 1.81199
[1mStep[0m  [165/339], [94mLoss[0m : 1.31322
[1mStep[0m  [198/339], [94mLoss[0m : 1.43604
[1mStep[0m  [231/339], [94mLoss[0m : 2.05025
[1mStep[0m  [264/339], [94mLoss[0m : 1.65724
[1mStep[0m  [297/339], [94mLoss[0m : 1.51051
[1mStep[0m  [330/339], [94mLoss[0m : 1.54582

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.540, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63828
[1mStep[0m  [33/339], [94mLoss[0m : 1.84389
[1mStep[0m  [66/339], [94mLoss[0m : 1.76106
[1mStep[0m  [99/339], [94mLoss[0m : 1.97389
[1mStep[0m  [132/339], [94mLoss[0m : 1.71807
[1mStep[0m  [165/339], [94mLoss[0m : 1.57524
[1mStep[0m  [198/339], [94mLoss[0m : 2.03943
[1mStep[0m  [231/339], [94mLoss[0m : 1.96696
[1mStep[0m  [264/339], [94mLoss[0m : 1.19878
[1mStep[0m  [297/339], [94mLoss[0m : 1.73929
[1mStep[0m  [330/339], [94mLoss[0m : 1.18863

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59698
[1mStep[0m  [33/339], [94mLoss[0m : 1.42658
[1mStep[0m  [66/339], [94mLoss[0m : 1.40357
[1mStep[0m  [99/339], [94mLoss[0m : 2.15916
[1mStep[0m  [132/339], [94mLoss[0m : 1.32546
[1mStep[0m  [165/339], [94mLoss[0m : 1.70702
[1mStep[0m  [198/339], [94mLoss[0m : 1.64150
[1mStep[0m  [231/339], [94mLoss[0m : 1.49931
[1mStep[0m  [264/339], [94mLoss[0m : 1.64534
[1mStep[0m  [297/339], [94mLoss[0m : 1.82210
[1mStep[0m  [330/339], [94mLoss[0m : 1.19160

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.486, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84776
[1mStep[0m  [33/339], [94mLoss[0m : 2.12571
[1mStep[0m  [66/339], [94mLoss[0m : 1.41349
[1mStep[0m  [99/339], [94mLoss[0m : 1.61807
[1mStep[0m  [132/339], [94mLoss[0m : 1.21114
[1mStep[0m  [165/339], [94mLoss[0m : 1.65469
[1mStep[0m  [198/339], [94mLoss[0m : 1.45641
[1mStep[0m  [231/339], [94mLoss[0m : 1.78140
[1mStep[0m  [264/339], [94mLoss[0m : 2.00159
[1mStep[0m  [297/339], [94mLoss[0m : 1.39677
[1mStep[0m  [330/339], [94mLoss[0m : 1.73484

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.36704
[1mStep[0m  [33/339], [94mLoss[0m : 1.77924
[1mStep[0m  [66/339], [94mLoss[0m : 1.98850
[1mStep[0m  [99/339], [94mLoss[0m : 1.53598
[1mStep[0m  [132/339], [94mLoss[0m : 1.47688
[1mStep[0m  [165/339], [94mLoss[0m : 1.23512
[1mStep[0m  [198/339], [94mLoss[0m : 1.66638
[1mStep[0m  [231/339], [94mLoss[0m : 1.71007
[1mStep[0m  [264/339], [94mLoss[0m : 2.03706
[1mStep[0m  [297/339], [94mLoss[0m : 1.30761
[1mStep[0m  [330/339], [94mLoss[0m : 1.53148

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59654
[1mStep[0m  [33/339], [94mLoss[0m : 1.70680
[1mStep[0m  [66/339], [94mLoss[0m : 1.69652
[1mStep[0m  [99/339], [94mLoss[0m : 1.52992
[1mStep[0m  [132/339], [94mLoss[0m : 1.45879
[1mStep[0m  [165/339], [94mLoss[0m : 1.46286
[1mStep[0m  [198/339], [94mLoss[0m : 1.41161
[1mStep[0m  [231/339], [94mLoss[0m : 1.86965
[1mStep[0m  [264/339], [94mLoss[0m : 1.67259
[1mStep[0m  [297/339], [94mLoss[0m : 1.65077
[1mStep[0m  [330/339], [94mLoss[0m : 1.57263

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14374
[1mStep[0m  [33/339], [94mLoss[0m : 1.51477
[1mStep[0m  [66/339], [94mLoss[0m : 1.61871
[1mStep[0m  [99/339], [94mLoss[0m : 1.30133
[1mStep[0m  [132/339], [94mLoss[0m : 1.91323
[1mStep[0m  [165/339], [94mLoss[0m : 1.48493
[1mStep[0m  [198/339], [94mLoss[0m : 1.70326
[1mStep[0m  [231/339], [94mLoss[0m : 1.88932
[1mStep[0m  [264/339], [94mLoss[0m : 1.78621
[1mStep[0m  [297/339], [94mLoss[0m : 1.35393
[1mStep[0m  [330/339], [94mLoss[0m : 1.58620

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.494, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.50527
[1mStep[0m  [33/339], [94mLoss[0m : 1.69278
[1mStep[0m  [66/339], [94mLoss[0m : 1.43627
[1mStep[0m  [99/339], [94mLoss[0m : 1.21314
[1mStep[0m  [132/339], [94mLoss[0m : 1.42715
[1mStep[0m  [165/339], [94mLoss[0m : 1.56745
[1mStep[0m  [198/339], [94mLoss[0m : 1.58428
[1mStep[0m  [231/339], [94mLoss[0m : 2.06139
[1mStep[0m  [264/339], [94mLoss[0m : 1.51329
[1mStep[0m  [297/339], [94mLoss[0m : 1.32355
[1mStep[0m  [330/339], [94mLoss[0m : 1.32247

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.497, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79269
[1mStep[0m  [33/339], [94mLoss[0m : 1.73950
[1mStep[0m  [66/339], [94mLoss[0m : 0.97351
[1mStep[0m  [99/339], [94mLoss[0m : 1.12653
[1mStep[0m  [132/339], [94mLoss[0m : 1.88071
[1mStep[0m  [165/339], [94mLoss[0m : 1.70303
[1mStep[0m  [198/339], [94mLoss[0m : 1.61335
[1mStep[0m  [231/339], [94mLoss[0m : 1.64829
[1mStep[0m  [264/339], [94mLoss[0m : 1.26812
[1mStep[0m  [297/339], [94mLoss[0m : 1.50716
[1mStep[0m  [330/339], [94mLoss[0m : 1.95350

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77064
[1mStep[0m  [33/339], [94mLoss[0m : 1.29799
[1mStep[0m  [66/339], [94mLoss[0m : 1.62644
[1mStep[0m  [99/339], [94mLoss[0m : 1.76911
[1mStep[0m  [132/339], [94mLoss[0m : 1.57650
[1mStep[0m  [165/339], [94mLoss[0m : 1.57520
[1mStep[0m  [198/339], [94mLoss[0m : 1.20707
[1mStep[0m  [231/339], [94mLoss[0m : 1.40220
[1mStep[0m  [264/339], [94mLoss[0m : 1.95186
[1mStep[0m  [297/339], [94mLoss[0m : 1.60069
[1mStep[0m  [330/339], [94mLoss[0m : 1.80390

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.480, [92mTest[0m: 2.515, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34666
[1mStep[0m  [33/339], [94mLoss[0m : 1.86634
[1mStep[0m  [66/339], [94mLoss[0m : 1.51701
[1mStep[0m  [99/339], [94mLoss[0m : 1.59984
[1mStep[0m  [132/339], [94mLoss[0m : 1.06025
[1mStep[0m  [165/339], [94mLoss[0m : 1.22514
[1mStep[0m  [198/339], [94mLoss[0m : 2.24252
[1mStep[0m  [231/339], [94mLoss[0m : 1.21695
[1mStep[0m  [264/339], [94mLoss[0m : 1.61710
[1mStep[0m  [297/339], [94mLoss[0m : 1.36703
[1mStep[0m  [330/339], [94mLoss[0m : 1.28307

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.456, [92mTest[0m: 2.524, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.474
====================================

Phase 2 - Evaluation MAE:  2.473874042519426
MAE score P1      2.286415
MAE score P2      2.473874
loss              1.456329
learning_rate     0.007525
batch_size              32
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.06834
[1mStep[0m  [2/21], [94mLoss[0m : 10.90839
[1mStep[0m  [4/21], [94mLoss[0m : 10.49048
[1mStep[0m  [6/21], [94mLoss[0m : 10.49170
[1mStep[0m  [8/21], [94mLoss[0m : 10.38851
[1mStep[0m  [10/21], [94mLoss[0m : 9.55178
[1mStep[0m  [12/21], [94mLoss[0m : 9.57430
[1mStep[0m  [14/21], [94mLoss[0m : 9.09104
[1mStep[0m  [16/21], [94mLoss[0m : 8.94450
[1mStep[0m  [18/21], [94mLoss[0m : 8.36242
[1mStep[0m  [20/21], [94mLoss[0m : 8.00571

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.707, [92mTest[0m: 10.872, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.82054
[1mStep[0m  [2/21], [94mLoss[0m : 7.64201
[1mStep[0m  [4/21], [94mLoss[0m : 7.58792
[1mStep[0m  [6/21], [94mLoss[0m : 7.31024
[1mStep[0m  [8/21], [94mLoss[0m : 6.49583
[1mStep[0m  [10/21], [94mLoss[0m : 6.28758
[1mStep[0m  [12/21], [94mLoss[0m : 6.04959
[1mStep[0m  [14/21], [94mLoss[0m : 5.82266
[1mStep[0m  [16/21], [94mLoss[0m : 5.49853
[1mStep[0m  [18/21], [94mLoss[0m : 5.51696
[1mStep[0m  [20/21], [94mLoss[0m : 5.33251

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.478, [92mTest[0m: 9.711, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.82027
[1mStep[0m  [2/21], [94mLoss[0m : 4.63124
[1mStep[0m  [4/21], [94mLoss[0m : 4.58501
[1mStep[0m  [6/21], [94mLoss[0m : 4.27292
[1mStep[0m  [8/21], [94mLoss[0m : 3.90209
[1mStep[0m  [10/21], [94mLoss[0m : 3.58398
[1mStep[0m  [12/21], [94mLoss[0m : 3.62942
[1mStep[0m  [14/21], [94mLoss[0m : 3.65926
[1mStep[0m  [16/21], [94mLoss[0m : 3.54258
[1mStep[0m  [18/21], [94mLoss[0m : 3.29836
[1mStep[0m  [20/21], [94mLoss[0m : 3.26372

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.959, [92mTest[0m: 7.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.28722
[1mStep[0m  [2/21], [94mLoss[0m : 3.32845
[1mStep[0m  [4/21], [94mLoss[0m : 3.08837
[1mStep[0m  [6/21], [94mLoss[0m : 3.04166
[1mStep[0m  [8/21], [94mLoss[0m : 3.09510
[1mStep[0m  [10/21], [94mLoss[0m : 2.89898
[1mStep[0m  [12/21], [94mLoss[0m : 2.85698
[1mStep[0m  [14/21], [94mLoss[0m : 2.84496
[1mStep[0m  [16/21], [94mLoss[0m : 3.06085
[1mStep[0m  [18/21], [94mLoss[0m : 2.96490
[1mStep[0m  [20/21], [94mLoss[0m : 2.92207

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.031, [92mTest[0m: 5.237, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84209
[1mStep[0m  [2/21], [94mLoss[0m : 2.94249
[1mStep[0m  [4/21], [94mLoss[0m : 3.01607
[1mStep[0m  [6/21], [94mLoss[0m : 2.81519
[1mStep[0m  [8/21], [94mLoss[0m : 2.85926
[1mStep[0m  [10/21], [94mLoss[0m : 2.72149
[1mStep[0m  [12/21], [94mLoss[0m : 2.85742
[1mStep[0m  [14/21], [94mLoss[0m : 2.92281
[1mStep[0m  [16/21], [94mLoss[0m : 2.76038
[1mStep[0m  [18/21], [94mLoss[0m : 3.05049
[1mStep[0m  [20/21], [94mLoss[0m : 2.90968

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.860, [92mTest[0m: 4.260, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83210
[1mStep[0m  [2/21], [94mLoss[0m : 2.68481
[1mStep[0m  [4/21], [94mLoss[0m : 2.74534
[1mStep[0m  [6/21], [94mLoss[0m : 2.82439
[1mStep[0m  [8/21], [94mLoss[0m : 2.87945
[1mStep[0m  [10/21], [94mLoss[0m : 2.87770
[1mStep[0m  [12/21], [94mLoss[0m : 2.77734
[1mStep[0m  [14/21], [94mLoss[0m : 2.67167
[1mStep[0m  [16/21], [94mLoss[0m : 2.71706
[1mStep[0m  [18/21], [94mLoss[0m : 2.71222
[1mStep[0m  [20/21], [94mLoss[0m : 2.84334

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.819, [92mTest[0m: 3.776, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66110
[1mStep[0m  [2/21], [94mLoss[0m : 2.94021
[1mStep[0m  [4/21], [94mLoss[0m : 2.69485
[1mStep[0m  [6/21], [94mLoss[0m : 2.80380
[1mStep[0m  [8/21], [94mLoss[0m : 2.77852
[1mStep[0m  [10/21], [94mLoss[0m : 2.76032
[1mStep[0m  [12/21], [94mLoss[0m : 2.86615
[1mStep[0m  [14/21], [94mLoss[0m : 2.62442
[1mStep[0m  [16/21], [94mLoss[0m : 2.70289
[1mStep[0m  [18/21], [94mLoss[0m : 2.60271
[1mStep[0m  [20/21], [94mLoss[0m : 2.88675

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.772, [92mTest[0m: 3.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75633
[1mStep[0m  [2/21], [94mLoss[0m : 2.70354
[1mStep[0m  [4/21], [94mLoss[0m : 2.88046
[1mStep[0m  [6/21], [94mLoss[0m : 2.59364
[1mStep[0m  [8/21], [94mLoss[0m : 2.84572
[1mStep[0m  [10/21], [94mLoss[0m : 2.65906
[1mStep[0m  [12/21], [94mLoss[0m : 2.66311
[1mStep[0m  [14/21], [94mLoss[0m : 2.83249
[1mStep[0m  [16/21], [94mLoss[0m : 2.91547
[1mStep[0m  [18/21], [94mLoss[0m : 2.86873
[1mStep[0m  [20/21], [94mLoss[0m : 2.75783

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.787, [92mTest[0m: 3.280, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70796
[1mStep[0m  [2/21], [94mLoss[0m : 2.84994
[1mStep[0m  [4/21], [94mLoss[0m : 2.69918
[1mStep[0m  [6/21], [94mLoss[0m : 2.72938
[1mStep[0m  [8/21], [94mLoss[0m : 2.71758
[1mStep[0m  [10/21], [94mLoss[0m : 2.76886
[1mStep[0m  [12/21], [94mLoss[0m : 2.81644
[1mStep[0m  [14/21], [94mLoss[0m : 2.80881
[1mStep[0m  [16/21], [94mLoss[0m : 2.70915
[1mStep[0m  [18/21], [94mLoss[0m : 2.74692
[1mStep[0m  [20/21], [94mLoss[0m : 2.73001

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.739, [92mTest[0m: 3.175, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80575
[1mStep[0m  [2/21], [94mLoss[0m : 2.62950
[1mStep[0m  [4/21], [94mLoss[0m : 2.68192
[1mStep[0m  [6/21], [94mLoss[0m : 2.68828
[1mStep[0m  [8/21], [94mLoss[0m : 2.72984
[1mStep[0m  [10/21], [94mLoss[0m : 2.61472
[1mStep[0m  [12/21], [94mLoss[0m : 2.70452
[1mStep[0m  [14/21], [94mLoss[0m : 2.68722
[1mStep[0m  [16/21], [94mLoss[0m : 2.67142
[1mStep[0m  [18/21], [94mLoss[0m : 2.60221
[1mStep[0m  [20/21], [94mLoss[0m : 2.79645

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.720, [92mTest[0m: 3.142, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.88466
[1mStep[0m  [2/21], [94mLoss[0m : 2.82330
[1mStep[0m  [4/21], [94mLoss[0m : 2.71556
[1mStep[0m  [6/21], [94mLoss[0m : 2.59045
[1mStep[0m  [8/21], [94mLoss[0m : 2.60453
[1mStep[0m  [10/21], [94mLoss[0m : 2.57834
[1mStep[0m  [12/21], [94mLoss[0m : 2.66994
[1mStep[0m  [14/21], [94mLoss[0m : 2.59740
[1mStep[0m  [16/21], [94mLoss[0m : 2.81790
[1mStep[0m  [18/21], [94mLoss[0m : 2.78228
[1mStep[0m  [20/21], [94mLoss[0m : 2.69321

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.723, [92mTest[0m: 3.014, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76896
[1mStep[0m  [2/21], [94mLoss[0m : 2.61400
[1mStep[0m  [4/21], [94mLoss[0m : 2.73174
[1mStep[0m  [6/21], [94mLoss[0m : 2.81655
[1mStep[0m  [8/21], [94mLoss[0m : 2.70329
[1mStep[0m  [10/21], [94mLoss[0m : 2.66740
[1mStep[0m  [12/21], [94mLoss[0m : 2.60754
[1mStep[0m  [14/21], [94mLoss[0m : 2.61577
[1mStep[0m  [16/21], [94mLoss[0m : 2.65157
[1mStep[0m  [18/21], [94mLoss[0m : 2.67914
[1mStep[0m  [20/21], [94mLoss[0m : 2.72983

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.708, [92mTest[0m: 3.049, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68864
[1mStep[0m  [2/21], [94mLoss[0m : 2.72564
[1mStep[0m  [4/21], [94mLoss[0m : 2.61058
[1mStep[0m  [6/21], [94mLoss[0m : 2.67256
[1mStep[0m  [8/21], [94mLoss[0m : 2.80745
[1mStep[0m  [10/21], [94mLoss[0m : 2.51532
[1mStep[0m  [12/21], [94mLoss[0m : 2.57585
[1mStep[0m  [14/21], [94mLoss[0m : 2.90759
[1mStep[0m  [16/21], [94mLoss[0m : 2.84642
[1mStep[0m  [18/21], [94mLoss[0m : 2.85084
[1mStep[0m  [20/21], [94mLoss[0m : 2.59154

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.699, [92mTest[0m: 3.006, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57489
[1mStep[0m  [2/21], [94mLoss[0m : 2.88769
[1mStep[0m  [4/21], [94mLoss[0m : 2.66187
[1mStep[0m  [6/21], [94mLoss[0m : 2.75196
[1mStep[0m  [8/21], [94mLoss[0m : 2.70404
[1mStep[0m  [10/21], [94mLoss[0m : 2.59917
[1mStep[0m  [12/21], [94mLoss[0m : 2.68058
[1mStep[0m  [14/21], [94mLoss[0m : 2.60996
[1mStep[0m  [16/21], [94mLoss[0m : 2.64814
[1mStep[0m  [18/21], [94mLoss[0m : 2.79039
[1mStep[0m  [20/21], [94mLoss[0m : 2.85837

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.715, [92mTest[0m: 2.944, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52329
[1mStep[0m  [2/21], [94mLoss[0m : 2.62906
[1mStep[0m  [4/21], [94mLoss[0m : 2.58976
[1mStep[0m  [6/21], [94mLoss[0m : 2.70644
[1mStep[0m  [8/21], [94mLoss[0m : 2.75827
[1mStep[0m  [10/21], [94mLoss[0m : 2.59524
[1mStep[0m  [12/21], [94mLoss[0m : 2.69479
[1mStep[0m  [14/21], [94mLoss[0m : 3.04294
[1mStep[0m  [16/21], [94mLoss[0m : 2.52646
[1mStep[0m  [18/21], [94mLoss[0m : 2.63934
[1mStep[0m  [20/21], [94mLoss[0m : 2.63490

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.688, [92mTest[0m: 2.909, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79861
[1mStep[0m  [2/21], [94mLoss[0m : 2.65590
[1mStep[0m  [4/21], [94mLoss[0m : 2.63621
[1mStep[0m  [6/21], [94mLoss[0m : 2.68558
[1mStep[0m  [8/21], [94mLoss[0m : 2.73924
[1mStep[0m  [10/21], [94mLoss[0m : 2.79717
[1mStep[0m  [12/21], [94mLoss[0m : 2.76218
[1mStep[0m  [14/21], [94mLoss[0m : 2.63204
[1mStep[0m  [16/21], [94mLoss[0m : 2.77319
[1mStep[0m  [18/21], [94mLoss[0m : 2.53441
[1mStep[0m  [20/21], [94mLoss[0m : 2.70952

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.896, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64449
[1mStep[0m  [2/21], [94mLoss[0m : 2.75100
[1mStep[0m  [4/21], [94mLoss[0m : 2.87383
[1mStep[0m  [6/21], [94mLoss[0m : 2.72892
[1mStep[0m  [8/21], [94mLoss[0m : 2.50544
[1mStep[0m  [10/21], [94mLoss[0m : 2.56411
[1mStep[0m  [12/21], [94mLoss[0m : 2.56083
[1mStep[0m  [14/21], [94mLoss[0m : 2.91332
[1mStep[0m  [16/21], [94mLoss[0m : 2.67804
[1mStep[0m  [18/21], [94mLoss[0m : 2.57332
[1mStep[0m  [20/21], [94mLoss[0m : 2.77466

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.915, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70594
[1mStep[0m  [2/21], [94mLoss[0m : 2.73839
[1mStep[0m  [4/21], [94mLoss[0m : 2.78170
[1mStep[0m  [6/21], [94mLoss[0m : 2.46059
[1mStep[0m  [8/21], [94mLoss[0m : 2.85335
[1mStep[0m  [10/21], [94mLoss[0m : 2.68286
[1mStep[0m  [12/21], [94mLoss[0m : 2.62321
[1mStep[0m  [14/21], [94mLoss[0m : 2.71806
[1mStep[0m  [16/21], [94mLoss[0m : 2.58242
[1mStep[0m  [18/21], [94mLoss[0m : 2.70138
[1mStep[0m  [20/21], [94mLoss[0m : 2.76463

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.850, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79254
[1mStep[0m  [2/21], [94mLoss[0m : 2.72050
[1mStep[0m  [4/21], [94mLoss[0m : 2.60827
[1mStep[0m  [6/21], [94mLoss[0m : 2.62218
[1mStep[0m  [8/21], [94mLoss[0m : 2.53873
[1mStep[0m  [10/21], [94mLoss[0m : 2.51856
[1mStep[0m  [12/21], [94mLoss[0m : 2.66103
[1mStep[0m  [14/21], [94mLoss[0m : 2.75210
[1mStep[0m  [16/21], [94mLoss[0m : 2.62533
[1mStep[0m  [18/21], [94mLoss[0m : 2.66282
[1mStep[0m  [20/21], [94mLoss[0m : 2.61696

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.876, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56237
[1mStep[0m  [2/21], [94mLoss[0m : 2.61605
[1mStep[0m  [4/21], [94mLoss[0m : 2.53747
[1mStep[0m  [6/21], [94mLoss[0m : 2.49936
[1mStep[0m  [8/21], [94mLoss[0m : 2.49423
[1mStep[0m  [10/21], [94mLoss[0m : 2.60453
[1mStep[0m  [12/21], [94mLoss[0m : 2.60781
[1mStep[0m  [14/21], [94mLoss[0m : 2.66974
[1mStep[0m  [16/21], [94mLoss[0m : 2.50318
[1mStep[0m  [18/21], [94mLoss[0m : 2.71013
[1mStep[0m  [20/21], [94mLoss[0m : 2.61768

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.805, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58176
[1mStep[0m  [2/21], [94mLoss[0m : 2.80827
[1mStep[0m  [4/21], [94mLoss[0m : 2.52375
[1mStep[0m  [6/21], [94mLoss[0m : 2.71481
[1mStep[0m  [8/21], [94mLoss[0m : 2.65598
[1mStep[0m  [10/21], [94mLoss[0m : 2.57264
[1mStep[0m  [12/21], [94mLoss[0m : 2.59984
[1mStep[0m  [14/21], [94mLoss[0m : 2.68879
[1mStep[0m  [16/21], [94mLoss[0m : 2.83244
[1mStep[0m  [18/21], [94mLoss[0m : 2.75191
[1mStep[0m  [20/21], [94mLoss[0m : 2.60173

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.821, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81782
[1mStep[0m  [2/21], [94mLoss[0m : 2.61416
[1mStep[0m  [4/21], [94mLoss[0m : 2.63820
[1mStep[0m  [6/21], [94mLoss[0m : 2.66822
[1mStep[0m  [8/21], [94mLoss[0m : 2.64053
[1mStep[0m  [10/21], [94mLoss[0m : 2.66153
[1mStep[0m  [12/21], [94mLoss[0m : 2.78033
[1mStep[0m  [14/21], [94mLoss[0m : 2.61525
[1mStep[0m  [16/21], [94mLoss[0m : 2.56698
[1mStep[0m  [18/21], [94mLoss[0m : 2.54959
[1mStep[0m  [20/21], [94mLoss[0m : 2.35536

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.780, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64366
[1mStep[0m  [2/21], [94mLoss[0m : 2.62854
[1mStep[0m  [4/21], [94mLoss[0m : 2.59786
[1mStep[0m  [6/21], [94mLoss[0m : 2.58390
[1mStep[0m  [8/21], [94mLoss[0m : 2.47705
[1mStep[0m  [10/21], [94mLoss[0m : 2.66530
[1mStep[0m  [12/21], [94mLoss[0m : 2.57403
[1mStep[0m  [14/21], [94mLoss[0m : 2.55046
[1mStep[0m  [16/21], [94mLoss[0m : 2.66589
[1mStep[0m  [18/21], [94mLoss[0m : 2.62797
[1mStep[0m  [20/21], [94mLoss[0m : 2.82617

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.784, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60897
[1mStep[0m  [2/21], [94mLoss[0m : 2.63334
[1mStep[0m  [4/21], [94mLoss[0m : 2.85065
[1mStep[0m  [6/21], [94mLoss[0m : 2.49803
[1mStep[0m  [8/21], [94mLoss[0m : 2.63739
[1mStep[0m  [10/21], [94mLoss[0m : 2.49900
[1mStep[0m  [12/21], [94mLoss[0m : 2.66975
[1mStep[0m  [14/21], [94mLoss[0m : 2.68581
[1mStep[0m  [16/21], [94mLoss[0m : 2.74307
[1mStep[0m  [18/21], [94mLoss[0m : 2.60112
[1mStep[0m  [20/21], [94mLoss[0m : 2.55079

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.719, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58739
[1mStep[0m  [2/21], [94mLoss[0m : 2.66065
[1mStep[0m  [4/21], [94mLoss[0m : 2.58788
[1mStep[0m  [6/21], [94mLoss[0m : 2.66979
[1mStep[0m  [8/21], [94mLoss[0m : 2.49982
[1mStep[0m  [10/21], [94mLoss[0m : 2.68794
[1mStep[0m  [12/21], [94mLoss[0m : 2.68360
[1mStep[0m  [14/21], [94mLoss[0m : 2.58137
[1mStep[0m  [16/21], [94mLoss[0m : 2.63270
[1mStep[0m  [18/21], [94mLoss[0m : 2.75362
[1mStep[0m  [20/21], [94mLoss[0m : 2.68703

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.760, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61090
[1mStep[0m  [2/21], [94mLoss[0m : 2.70076
[1mStep[0m  [4/21], [94mLoss[0m : 2.65029
[1mStep[0m  [6/21], [94mLoss[0m : 2.55304
[1mStep[0m  [8/21], [94mLoss[0m : 2.82433
[1mStep[0m  [10/21], [94mLoss[0m : 2.66231
[1mStep[0m  [12/21], [94mLoss[0m : 2.57725
[1mStep[0m  [14/21], [94mLoss[0m : 2.81373
[1mStep[0m  [16/21], [94mLoss[0m : 2.64310
[1mStep[0m  [18/21], [94mLoss[0m : 2.71505
[1mStep[0m  [20/21], [94mLoss[0m : 2.53120

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.748, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61619
[1mStep[0m  [2/21], [94mLoss[0m : 2.52927
[1mStep[0m  [4/21], [94mLoss[0m : 2.65016
[1mStep[0m  [6/21], [94mLoss[0m : 2.59181
[1mStep[0m  [8/21], [94mLoss[0m : 2.76529
[1mStep[0m  [10/21], [94mLoss[0m : 2.51894
[1mStep[0m  [12/21], [94mLoss[0m : 2.41669
[1mStep[0m  [14/21], [94mLoss[0m : 2.72280
[1mStep[0m  [16/21], [94mLoss[0m : 2.70377
[1mStep[0m  [18/21], [94mLoss[0m : 2.53000
[1mStep[0m  [20/21], [94mLoss[0m : 2.79125

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.720, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54420
[1mStep[0m  [2/21], [94mLoss[0m : 2.59153
[1mStep[0m  [4/21], [94mLoss[0m : 2.66817
[1mStep[0m  [6/21], [94mLoss[0m : 2.57529
[1mStep[0m  [8/21], [94mLoss[0m : 2.67322
[1mStep[0m  [10/21], [94mLoss[0m : 2.58448
[1mStep[0m  [12/21], [94mLoss[0m : 2.55168
[1mStep[0m  [14/21], [94mLoss[0m : 2.67881
[1mStep[0m  [16/21], [94mLoss[0m : 2.64385
[1mStep[0m  [18/21], [94mLoss[0m : 2.69898
[1mStep[0m  [20/21], [94mLoss[0m : 2.70568

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.687, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75242
[1mStep[0m  [2/21], [94mLoss[0m : 2.68288
[1mStep[0m  [4/21], [94mLoss[0m : 2.49120
[1mStep[0m  [6/21], [94mLoss[0m : 2.66104
[1mStep[0m  [8/21], [94mLoss[0m : 2.62557
[1mStep[0m  [10/21], [94mLoss[0m : 2.77275
[1mStep[0m  [12/21], [94mLoss[0m : 2.44356
[1mStep[0m  [14/21], [94mLoss[0m : 2.71644
[1mStep[0m  [16/21], [94mLoss[0m : 2.59832
[1mStep[0m  [18/21], [94mLoss[0m : 2.61617
[1mStep[0m  [20/21], [94mLoss[0m : 2.63462

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.685, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69784
[1mStep[0m  [2/21], [94mLoss[0m : 2.59495
[1mStep[0m  [4/21], [94mLoss[0m : 2.79186
[1mStep[0m  [6/21], [94mLoss[0m : 2.48646
[1mStep[0m  [8/21], [94mLoss[0m : 2.61338
[1mStep[0m  [10/21], [94mLoss[0m : 2.59879
[1mStep[0m  [12/21], [94mLoss[0m : 2.62901
[1mStep[0m  [14/21], [94mLoss[0m : 2.54051
[1mStep[0m  [16/21], [94mLoss[0m : 2.69059
[1mStep[0m  [18/21], [94mLoss[0m : 2.54647
[1mStep[0m  [20/21], [94mLoss[0m : 2.70593

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.654, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.661
====================================

Phase 1 - Evaluation MAE:  2.6610473224094937
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.56832
[1mStep[0m  [2/21], [94mLoss[0m : 2.75553
[1mStep[0m  [4/21], [94mLoss[0m : 2.74520
[1mStep[0m  [6/21], [94mLoss[0m : 2.63059
[1mStep[0m  [8/21], [94mLoss[0m : 2.57496
[1mStep[0m  [10/21], [94mLoss[0m : 2.70599
[1mStep[0m  [12/21], [94mLoss[0m : 2.59675
[1mStep[0m  [14/21], [94mLoss[0m : 2.70560
[1mStep[0m  [16/21], [94mLoss[0m : 2.78672
[1mStep[0m  [18/21], [94mLoss[0m : 2.68709
[1mStep[0m  [20/21], [94mLoss[0m : 2.82369

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.658, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71381
[1mStep[0m  [2/21], [94mLoss[0m : 2.65347
[1mStep[0m  [4/21], [94mLoss[0m : 2.77297
[1mStep[0m  [6/21], [94mLoss[0m : 2.62657
[1mStep[0m  [8/21], [94mLoss[0m : 2.63567
[1mStep[0m  [10/21], [94mLoss[0m : 2.74089
[1mStep[0m  [12/21], [94mLoss[0m : 2.66263
[1mStep[0m  [14/21], [94mLoss[0m : 2.51881
[1mStep[0m  [16/21], [94mLoss[0m : 2.76491
[1mStep[0m  [18/21], [94mLoss[0m : 2.77820
[1mStep[0m  [20/21], [94mLoss[0m : 2.69409

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.868, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60321
[1mStep[0m  [2/21], [94mLoss[0m : 2.56977
[1mStep[0m  [4/21], [94mLoss[0m : 2.55760
[1mStep[0m  [6/21], [94mLoss[0m : 2.59201
[1mStep[0m  [8/21], [94mLoss[0m : 2.48938
[1mStep[0m  [10/21], [94mLoss[0m : 2.61433
[1mStep[0m  [12/21], [94mLoss[0m : 2.67660
[1mStep[0m  [14/21], [94mLoss[0m : 2.64112
[1mStep[0m  [16/21], [94mLoss[0m : 2.65378
[1mStep[0m  [18/21], [94mLoss[0m : 2.79881
[1mStep[0m  [20/21], [94mLoss[0m : 2.72972

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.683, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77085
[1mStep[0m  [2/21], [94mLoss[0m : 2.68349
[1mStep[0m  [4/21], [94mLoss[0m : 2.62408
[1mStep[0m  [6/21], [94mLoss[0m : 2.66449
[1mStep[0m  [8/21], [94mLoss[0m : 2.61268
[1mStep[0m  [10/21], [94mLoss[0m : 2.69910
[1mStep[0m  [12/21], [94mLoss[0m : 2.62128
[1mStep[0m  [14/21], [94mLoss[0m : 2.73215
[1mStep[0m  [16/21], [94mLoss[0m : 2.63798
[1mStep[0m  [18/21], [94mLoss[0m : 2.39857
[1mStep[0m  [20/21], [94mLoss[0m : 2.58581

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.626, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56591
[1mStep[0m  [2/21], [94mLoss[0m : 2.70118
[1mStep[0m  [4/21], [94mLoss[0m : 2.52724
[1mStep[0m  [6/21], [94mLoss[0m : 2.68749
[1mStep[0m  [8/21], [94mLoss[0m : 2.57130
[1mStep[0m  [10/21], [94mLoss[0m : 2.72971
[1mStep[0m  [12/21], [94mLoss[0m : 2.69338
[1mStep[0m  [14/21], [94mLoss[0m : 2.71949
[1mStep[0m  [16/21], [94mLoss[0m : 2.73142
[1mStep[0m  [18/21], [94mLoss[0m : 2.58958
[1mStep[0m  [20/21], [94mLoss[0m : 2.50203

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.622, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65575
[1mStep[0m  [2/21], [94mLoss[0m : 2.62175
[1mStep[0m  [4/21], [94mLoss[0m : 2.47000
[1mStep[0m  [6/21], [94mLoss[0m : 2.67515
[1mStep[0m  [8/21], [94mLoss[0m : 2.62122
[1mStep[0m  [10/21], [94mLoss[0m : 2.48961
[1mStep[0m  [12/21], [94mLoss[0m : 2.64678
[1mStep[0m  [14/21], [94mLoss[0m : 2.61304
[1mStep[0m  [16/21], [94mLoss[0m : 2.60537
[1mStep[0m  [18/21], [94mLoss[0m : 2.59509
[1mStep[0m  [20/21], [94mLoss[0m : 2.65829

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.627, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58259
[1mStep[0m  [2/21], [94mLoss[0m : 2.45027
[1mStep[0m  [4/21], [94mLoss[0m : 2.54278
[1mStep[0m  [6/21], [94mLoss[0m : 2.81763
[1mStep[0m  [8/21], [94mLoss[0m : 2.63448
[1mStep[0m  [10/21], [94mLoss[0m : 2.59212
[1mStep[0m  [12/21], [94mLoss[0m : 2.67894
[1mStep[0m  [14/21], [94mLoss[0m : 2.61309
[1mStep[0m  [16/21], [94mLoss[0m : 2.51965
[1mStep[0m  [18/21], [94mLoss[0m : 2.61723
[1mStep[0m  [20/21], [94mLoss[0m : 2.54997

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.599, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46603
[1mStep[0m  [2/21], [94mLoss[0m : 2.61079
[1mStep[0m  [4/21], [94mLoss[0m : 2.57021
[1mStep[0m  [6/21], [94mLoss[0m : 2.43044
[1mStep[0m  [8/21], [94mLoss[0m : 2.58792
[1mStep[0m  [10/21], [94mLoss[0m : 2.38640
[1mStep[0m  [12/21], [94mLoss[0m : 2.54371
[1mStep[0m  [14/21], [94mLoss[0m : 2.54633
[1mStep[0m  [16/21], [94mLoss[0m : 2.49625
[1mStep[0m  [18/21], [94mLoss[0m : 2.71412
[1mStep[0m  [20/21], [94mLoss[0m : 2.47737

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.560, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57067
[1mStep[0m  [2/21], [94mLoss[0m : 2.48872
[1mStep[0m  [4/21], [94mLoss[0m : 2.47822
[1mStep[0m  [6/21], [94mLoss[0m : 2.50311
[1mStep[0m  [8/21], [94mLoss[0m : 2.50407
[1mStep[0m  [10/21], [94mLoss[0m : 2.67889
[1mStep[0m  [12/21], [94mLoss[0m : 2.58292
[1mStep[0m  [14/21], [94mLoss[0m : 2.66506
[1mStep[0m  [16/21], [94mLoss[0m : 2.48740
[1mStep[0m  [18/21], [94mLoss[0m : 2.76662
[1mStep[0m  [20/21], [94mLoss[0m : 2.54279

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.606, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58449
[1mStep[0m  [2/21], [94mLoss[0m : 2.59958
[1mStep[0m  [4/21], [94mLoss[0m : 2.53047
[1mStep[0m  [6/21], [94mLoss[0m : 2.55812
[1mStep[0m  [8/21], [94mLoss[0m : 2.56723
[1mStep[0m  [10/21], [94mLoss[0m : 2.51203
[1mStep[0m  [12/21], [94mLoss[0m : 2.42465
[1mStep[0m  [14/21], [94mLoss[0m : 2.51207
[1mStep[0m  [16/21], [94mLoss[0m : 2.62320
[1mStep[0m  [18/21], [94mLoss[0m : 2.50740
[1mStep[0m  [20/21], [94mLoss[0m : 2.55167

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.603, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56300
[1mStep[0m  [2/21], [94mLoss[0m : 2.62515
[1mStep[0m  [4/21], [94mLoss[0m : 2.54605
[1mStep[0m  [6/21], [94mLoss[0m : 2.54925
[1mStep[0m  [8/21], [94mLoss[0m : 2.56473
[1mStep[0m  [10/21], [94mLoss[0m : 2.48785
[1mStep[0m  [12/21], [94mLoss[0m : 2.45359
[1mStep[0m  [14/21], [94mLoss[0m : 2.48233
[1mStep[0m  [16/21], [94mLoss[0m : 2.57182
[1mStep[0m  [18/21], [94mLoss[0m : 2.35825
[1mStep[0m  [20/21], [94mLoss[0m : 2.59330

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.613, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46957
[1mStep[0m  [2/21], [94mLoss[0m : 2.54266
[1mStep[0m  [4/21], [94mLoss[0m : 2.64910
[1mStep[0m  [6/21], [94mLoss[0m : 2.45471
[1mStep[0m  [8/21], [94mLoss[0m : 2.41255
[1mStep[0m  [10/21], [94mLoss[0m : 2.54472
[1mStep[0m  [12/21], [94mLoss[0m : 2.54119
[1mStep[0m  [14/21], [94mLoss[0m : 2.54505
[1mStep[0m  [16/21], [94mLoss[0m : 2.59169
[1mStep[0m  [18/21], [94mLoss[0m : 2.42307
[1mStep[0m  [20/21], [94mLoss[0m : 2.57240

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.575, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63991
[1mStep[0m  [2/21], [94mLoss[0m : 2.61606
[1mStep[0m  [4/21], [94mLoss[0m : 2.36217
[1mStep[0m  [6/21], [94mLoss[0m : 2.51788
[1mStep[0m  [8/21], [94mLoss[0m : 2.40376
[1mStep[0m  [10/21], [94mLoss[0m : 2.50959
[1mStep[0m  [12/21], [94mLoss[0m : 2.45947
[1mStep[0m  [14/21], [94mLoss[0m : 2.38012
[1mStep[0m  [16/21], [94mLoss[0m : 2.45981
[1mStep[0m  [18/21], [94mLoss[0m : 2.47999
[1mStep[0m  [20/21], [94mLoss[0m : 2.44349

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.665, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55852
[1mStep[0m  [2/21], [94mLoss[0m : 2.59110
[1mStep[0m  [4/21], [94mLoss[0m : 2.53883
[1mStep[0m  [6/21], [94mLoss[0m : 2.64877
[1mStep[0m  [8/21], [94mLoss[0m : 2.40990
[1mStep[0m  [10/21], [94mLoss[0m : 2.41356
[1mStep[0m  [12/21], [94mLoss[0m : 2.34080
[1mStep[0m  [14/21], [94mLoss[0m : 2.55239
[1mStep[0m  [16/21], [94mLoss[0m : 2.45879
[1mStep[0m  [18/21], [94mLoss[0m : 2.51617
[1mStep[0m  [20/21], [94mLoss[0m : 2.39757

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.633, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43068
[1mStep[0m  [2/21], [94mLoss[0m : 2.52758
[1mStep[0m  [4/21], [94mLoss[0m : 2.39995
[1mStep[0m  [6/21], [94mLoss[0m : 2.54193
[1mStep[0m  [8/21], [94mLoss[0m : 2.54046
[1mStep[0m  [10/21], [94mLoss[0m : 2.42077
[1mStep[0m  [12/21], [94mLoss[0m : 2.37643
[1mStep[0m  [14/21], [94mLoss[0m : 2.46754
[1mStep[0m  [16/21], [94mLoss[0m : 2.45076
[1mStep[0m  [18/21], [94mLoss[0m : 2.56670
[1mStep[0m  [20/21], [94mLoss[0m : 2.49837

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.666, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55763
[1mStep[0m  [2/21], [94mLoss[0m : 2.30661
[1mStep[0m  [4/21], [94mLoss[0m : 2.49412
[1mStep[0m  [6/21], [94mLoss[0m : 2.34548
[1mStep[0m  [8/21], [94mLoss[0m : 2.61774
[1mStep[0m  [10/21], [94mLoss[0m : 2.52694
[1mStep[0m  [12/21], [94mLoss[0m : 2.49868
[1mStep[0m  [14/21], [94mLoss[0m : 2.37992
[1mStep[0m  [16/21], [94mLoss[0m : 2.44409
[1mStep[0m  [18/21], [94mLoss[0m : 2.43792
[1mStep[0m  [20/21], [94mLoss[0m : 2.52307

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.652, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43780
[1mStep[0m  [2/21], [94mLoss[0m : 2.48422
[1mStep[0m  [4/21], [94mLoss[0m : 2.32457
[1mStep[0m  [6/21], [94mLoss[0m : 2.52343
[1mStep[0m  [8/21], [94mLoss[0m : 2.49075
[1mStep[0m  [10/21], [94mLoss[0m : 2.31498
[1mStep[0m  [12/21], [94mLoss[0m : 2.34073
[1mStep[0m  [14/21], [94mLoss[0m : 2.60908
[1mStep[0m  [16/21], [94mLoss[0m : 2.37905
[1mStep[0m  [18/21], [94mLoss[0m : 2.42481
[1mStep[0m  [20/21], [94mLoss[0m : 2.39004

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.685, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39402
[1mStep[0m  [2/21], [94mLoss[0m : 2.35271
[1mStep[0m  [4/21], [94mLoss[0m : 2.32711
[1mStep[0m  [6/21], [94mLoss[0m : 2.57191
[1mStep[0m  [8/21], [94mLoss[0m : 2.54887
[1mStep[0m  [10/21], [94mLoss[0m : 2.38684
[1mStep[0m  [12/21], [94mLoss[0m : 2.24051
[1mStep[0m  [14/21], [94mLoss[0m : 2.37946
[1mStep[0m  [16/21], [94mLoss[0m : 2.37439
[1mStep[0m  [18/21], [94mLoss[0m : 2.63403
[1mStep[0m  [20/21], [94mLoss[0m : 2.53774

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.687, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26186
[1mStep[0m  [2/21], [94mLoss[0m : 2.49518
[1mStep[0m  [4/21], [94mLoss[0m : 2.39458
[1mStep[0m  [6/21], [94mLoss[0m : 2.50378
[1mStep[0m  [8/21], [94mLoss[0m : 2.24984
[1mStep[0m  [10/21], [94mLoss[0m : 2.44804
[1mStep[0m  [12/21], [94mLoss[0m : 2.54094
[1mStep[0m  [14/21], [94mLoss[0m : 2.43513
[1mStep[0m  [16/21], [94mLoss[0m : 2.46381
[1mStep[0m  [18/21], [94mLoss[0m : 2.27189
[1mStep[0m  [20/21], [94mLoss[0m : 2.37187

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.654, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42648
[1mStep[0m  [2/21], [94mLoss[0m : 2.49906
[1mStep[0m  [4/21], [94mLoss[0m : 2.27305
[1mStep[0m  [6/21], [94mLoss[0m : 2.46575
[1mStep[0m  [8/21], [94mLoss[0m : 2.39064
[1mStep[0m  [10/21], [94mLoss[0m : 2.42354
[1mStep[0m  [12/21], [94mLoss[0m : 2.23079
[1mStep[0m  [14/21], [94mLoss[0m : 2.41146
[1mStep[0m  [16/21], [94mLoss[0m : 2.40248
[1mStep[0m  [18/21], [94mLoss[0m : 2.47909
[1mStep[0m  [20/21], [94mLoss[0m : 2.37719

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.665, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35771
[1mStep[0m  [2/21], [94mLoss[0m : 2.27475
[1mStep[0m  [4/21], [94mLoss[0m : 2.40851
[1mStep[0m  [6/21], [94mLoss[0m : 2.45813
[1mStep[0m  [8/21], [94mLoss[0m : 2.31041
[1mStep[0m  [10/21], [94mLoss[0m : 2.45654
[1mStep[0m  [12/21], [94mLoss[0m : 2.49762
[1mStep[0m  [14/21], [94mLoss[0m : 2.39548
[1mStep[0m  [16/21], [94mLoss[0m : 2.43431
[1mStep[0m  [18/21], [94mLoss[0m : 2.30032
[1mStep[0m  [20/21], [94mLoss[0m : 2.35295

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.690, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54963
[1mStep[0m  [2/21], [94mLoss[0m : 2.33911
[1mStep[0m  [4/21], [94mLoss[0m : 2.45504
[1mStep[0m  [6/21], [94mLoss[0m : 2.30240
[1mStep[0m  [8/21], [94mLoss[0m : 2.30681
[1mStep[0m  [10/21], [94mLoss[0m : 2.33836
[1mStep[0m  [12/21], [94mLoss[0m : 2.45822
[1mStep[0m  [14/21], [94mLoss[0m : 2.46079
[1mStep[0m  [16/21], [94mLoss[0m : 2.42148
[1mStep[0m  [18/21], [94mLoss[0m : 2.40650
[1mStep[0m  [20/21], [94mLoss[0m : 2.37075

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.658, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39403
[1mStep[0m  [2/21], [94mLoss[0m : 2.23941
[1mStep[0m  [4/21], [94mLoss[0m : 2.43097
[1mStep[0m  [6/21], [94mLoss[0m : 2.35959
[1mStep[0m  [8/21], [94mLoss[0m : 2.12278
[1mStep[0m  [10/21], [94mLoss[0m : 2.19300
[1mStep[0m  [12/21], [94mLoss[0m : 2.38235
[1mStep[0m  [14/21], [94mLoss[0m : 2.17310
[1mStep[0m  [16/21], [94mLoss[0m : 2.47344
[1mStep[0m  [18/21], [94mLoss[0m : 2.33000
[1mStep[0m  [20/21], [94mLoss[0m : 2.48104

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.673, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34621
[1mStep[0m  [2/21], [94mLoss[0m : 2.35052
[1mStep[0m  [4/21], [94mLoss[0m : 2.36162
[1mStep[0m  [6/21], [94mLoss[0m : 2.23988
[1mStep[0m  [8/21], [94mLoss[0m : 2.41805
[1mStep[0m  [10/21], [94mLoss[0m : 2.27133
[1mStep[0m  [12/21], [94mLoss[0m : 2.25986
[1mStep[0m  [14/21], [94mLoss[0m : 2.43449
[1mStep[0m  [16/21], [94mLoss[0m : 2.38000
[1mStep[0m  [18/21], [94mLoss[0m : 2.29911
[1mStep[0m  [20/21], [94mLoss[0m : 2.38227

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.733, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27318
[1mStep[0m  [2/21], [94mLoss[0m : 2.26404
[1mStep[0m  [4/21], [94mLoss[0m : 2.25083
[1mStep[0m  [6/21], [94mLoss[0m : 2.37476
[1mStep[0m  [8/21], [94mLoss[0m : 2.27203
[1mStep[0m  [10/21], [94mLoss[0m : 2.40622
[1mStep[0m  [12/21], [94mLoss[0m : 2.36737
[1mStep[0m  [14/21], [94mLoss[0m : 2.16538
[1mStep[0m  [16/21], [94mLoss[0m : 2.22150
[1mStep[0m  [18/21], [94mLoss[0m : 2.25004
[1mStep[0m  [20/21], [94mLoss[0m : 2.28047

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.746, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24125
[1mStep[0m  [2/21], [94mLoss[0m : 2.29011
[1mStep[0m  [4/21], [94mLoss[0m : 2.31994
[1mStep[0m  [6/21], [94mLoss[0m : 2.22637
[1mStep[0m  [8/21], [94mLoss[0m : 2.26878
[1mStep[0m  [10/21], [94mLoss[0m : 2.31483
[1mStep[0m  [12/21], [94mLoss[0m : 2.17294
[1mStep[0m  [14/21], [94mLoss[0m : 2.49404
[1mStep[0m  [16/21], [94mLoss[0m : 2.41338
[1mStep[0m  [18/21], [94mLoss[0m : 2.13762
[1mStep[0m  [20/21], [94mLoss[0m : 2.19687

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.791, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.21030
[1mStep[0m  [2/21], [94mLoss[0m : 2.45113
[1mStep[0m  [4/21], [94mLoss[0m : 2.48678
[1mStep[0m  [6/21], [94mLoss[0m : 2.09810
[1mStep[0m  [8/21], [94mLoss[0m : 2.21761
[1mStep[0m  [10/21], [94mLoss[0m : 2.18215
[1mStep[0m  [12/21], [94mLoss[0m : 2.36848
[1mStep[0m  [14/21], [94mLoss[0m : 2.28855
[1mStep[0m  [16/21], [94mLoss[0m : 2.24879
[1mStep[0m  [18/21], [94mLoss[0m : 2.34846
[1mStep[0m  [20/21], [94mLoss[0m : 2.30443

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.739, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28324
[1mStep[0m  [2/21], [94mLoss[0m : 2.23317
[1mStep[0m  [4/21], [94mLoss[0m : 2.20797
[1mStep[0m  [6/21], [94mLoss[0m : 2.35078
[1mStep[0m  [8/21], [94mLoss[0m : 2.32504
[1mStep[0m  [10/21], [94mLoss[0m : 2.28202
[1mStep[0m  [12/21], [94mLoss[0m : 2.21613
[1mStep[0m  [14/21], [94mLoss[0m : 2.21854
[1mStep[0m  [16/21], [94mLoss[0m : 2.23121
[1mStep[0m  [18/21], [94mLoss[0m : 2.17026
[1mStep[0m  [20/21], [94mLoss[0m : 2.13949

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.765, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26564
[1mStep[0m  [2/21], [94mLoss[0m : 2.10853
[1mStep[0m  [4/21], [94mLoss[0m : 2.21071
[1mStep[0m  [6/21], [94mLoss[0m : 2.16892
[1mStep[0m  [8/21], [94mLoss[0m : 2.21227
[1mStep[0m  [10/21], [94mLoss[0m : 2.26091
[1mStep[0m  [12/21], [94mLoss[0m : 2.22264
[1mStep[0m  [14/21], [94mLoss[0m : 2.20605
[1mStep[0m  [16/21], [94mLoss[0m : 2.46313
[1mStep[0m  [18/21], [94mLoss[0m : 2.33862
[1mStep[0m  [20/21], [94mLoss[0m : 2.14332

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.769, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16462
[1mStep[0m  [2/21], [94mLoss[0m : 2.32524
[1mStep[0m  [4/21], [94mLoss[0m : 2.12260
[1mStep[0m  [6/21], [94mLoss[0m : 2.19359
[1mStep[0m  [8/21], [94mLoss[0m : 2.19081
[1mStep[0m  [10/21], [94mLoss[0m : 2.14475
[1mStep[0m  [12/21], [94mLoss[0m : 2.22401
[1mStep[0m  [14/21], [94mLoss[0m : 2.19893
[1mStep[0m  [16/21], [94mLoss[0m : 2.26916
[1mStep[0m  [18/21], [94mLoss[0m : 2.10596
[1mStep[0m  [20/21], [94mLoss[0m : 2.25510

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.768, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.763
====================================

Phase 2 - Evaluation MAE:  2.7627673149108887
MAE score P1        2.661047
MAE score P2        2.762767
loss                2.226547
learning_rate        0.00505
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.5
weight_decay          0.0001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 11.93645
[1mStep[0m  [33/339], [94mLoss[0m : 8.32129
[1mStep[0m  [66/339], [94mLoss[0m : 3.48147
[1mStep[0m  [99/339], [94mLoss[0m : 2.61171
[1mStep[0m  [132/339], [94mLoss[0m : 2.62594
[1mStep[0m  [165/339], [94mLoss[0m : 2.25432
[1mStep[0m  [198/339], [94mLoss[0m : 2.47751
[1mStep[0m  [231/339], [94mLoss[0m : 3.03320
[1mStep[0m  [264/339], [94mLoss[0m : 2.16498
[1mStep[0m  [297/339], [94mLoss[0m : 2.31045
[1mStep[0m  [330/339], [94mLoss[0m : 2.06761

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.747, [92mTest[0m: 10.960, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49557
[1mStep[0m  [33/339], [94mLoss[0m : 2.26926
[1mStep[0m  [66/339], [94mLoss[0m : 2.68748
[1mStep[0m  [99/339], [94mLoss[0m : 2.27501
[1mStep[0m  [132/339], [94mLoss[0m : 2.94758
[1mStep[0m  [165/339], [94mLoss[0m : 3.00025
[1mStep[0m  [198/339], [94mLoss[0m : 2.83398
[1mStep[0m  [231/339], [94mLoss[0m : 2.03809
[1mStep[0m  [264/339], [94mLoss[0m : 2.79098
[1mStep[0m  [297/339], [94mLoss[0m : 2.74993
[1mStep[0m  [330/339], [94mLoss[0m : 1.74682

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21901
[1mStep[0m  [33/339], [94mLoss[0m : 2.68372
[1mStep[0m  [66/339], [94mLoss[0m : 2.42720
[1mStep[0m  [99/339], [94mLoss[0m : 1.76188
[1mStep[0m  [132/339], [94mLoss[0m : 2.15358
[1mStep[0m  [165/339], [94mLoss[0m : 2.84185
[1mStep[0m  [198/339], [94mLoss[0m : 2.23453
[1mStep[0m  [231/339], [94mLoss[0m : 2.84803
[1mStep[0m  [264/339], [94mLoss[0m : 2.87684
[1mStep[0m  [297/339], [94mLoss[0m : 2.31861
[1mStep[0m  [330/339], [94mLoss[0m : 1.79215

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77669
[1mStep[0m  [33/339], [94mLoss[0m : 3.06881
[1mStep[0m  [66/339], [94mLoss[0m : 1.91675
[1mStep[0m  [99/339], [94mLoss[0m : 2.73931
[1mStep[0m  [132/339], [94mLoss[0m : 2.36650
[1mStep[0m  [165/339], [94mLoss[0m : 2.39281
[1mStep[0m  [198/339], [94mLoss[0m : 2.22009
[1mStep[0m  [231/339], [94mLoss[0m : 2.45549
[1mStep[0m  [264/339], [94mLoss[0m : 1.94679
[1mStep[0m  [297/339], [94mLoss[0m : 3.09491
[1mStep[0m  [330/339], [94mLoss[0m : 2.70531

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13178
[1mStep[0m  [33/339], [94mLoss[0m : 2.59945
[1mStep[0m  [66/339], [94mLoss[0m : 2.08176
[1mStep[0m  [99/339], [94mLoss[0m : 2.95271
[1mStep[0m  [132/339], [94mLoss[0m : 2.34450
[1mStep[0m  [165/339], [94mLoss[0m : 2.24565
[1mStep[0m  [198/339], [94mLoss[0m : 2.34706
[1mStep[0m  [231/339], [94mLoss[0m : 2.53626
[1mStep[0m  [264/339], [94mLoss[0m : 2.17419
[1mStep[0m  [297/339], [94mLoss[0m : 2.68072
[1mStep[0m  [330/339], [94mLoss[0m : 2.30987

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50414
[1mStep[0m  [33/339], [94mLoss[0m : 1.81995
[1mStep[0m  [66/339], [94mLoss[0m : 3.23932
[1mStep[0m  [99/339], [94mLoss[0m : 2.81410
[1mStep[0m  [132/339], [94mLoss[0m : 2.58038
[1mStep[0m  [165/339], [94mLoss[0m : 2.71218
[1mStep[0m  [198/339], [94mLoss[0m : 2.81324
[1mStep[0m  [231/339], [94mLoss[0m : 2.53001
[1mStep[0m  [264/339], [94mLoss[0m : 2.58771
[1mStep[0m  [297/339], [94mLoss[0m : 1.97238
[1mStep[0m  [330/339], [94mLoss[0m : 2.68947

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44972
[1mStep[0m  [33/339], [94mLoss[0m : 2.55858
[1mStep[0m  [66/339], [94mLoss[0m : 2.49371
[1mStep[0m  [99/339], [94mLoss[0m : 2.33436
[1mStep[0m  [132/339], [94mLoss[0m : 2.77120
[1mStep[0m  [165/339], [94mLoss[0m : 2.89262
[1mStep[0m  [198/339], [94mLoss[0m : 2.36441
[1mStep[0m  [231/339], [94mLoss[0m : 2.74411
[1mStep[0m  [264/339], [94mLoss[0m : 2.38580
[1mStep[0m  [297/339], [94mLoss[0m : 2.02713
[1mStep[0m  [330/339], [94mLoss[0m : 2.64484

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75089
[1mStep[0m  [33/339], [94mLoss[0m : 2.16581
[1mStep[0m  [66/339], [94mLoss[0m : 2.43244
[1mStep[0m  [99/339], [94mLoss[0m : 2.31137
[1mStep[0m  [132/339], [94mLoss[0m : 2.31086
[1mStep[0m  [165/339], [94mLoss[0m : 2.01686
[1mStep[0m  [198/339], [94mLoss[0m : 2.32027
[1mStep[0m  [231/339], [94mLoss[0m : 2.22198
[1mStep[0m  [264/339], [94mLoss[0m : 1.99537
[1mStep[0m  [297/339], [94mLoss[0m : 1.71314
[1mStep[0m  [330/339], [94mLoss[0m : 2.09427

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39481
[1mStep[0m  [33/339], [94mLoss[0m : 2.23331
[1mStep[0m  [66/339], [94mLoss[0m : 2.43545
[1mStep[0m  [99/339], [94mLoss[0m : 2.27140
[1mStep[0m  [132/339], [94mLoss[0m : 2.71437
[1mStep[0m  [165/339], [94mLoss[0m : 2.59600
[1mStep[0m  [198/339], [94mLoss[0m : 2.89062
[1mStep[0m  [231/339], [94mLoss[0m : 1.86370
[1mStep[0m  [264/339], [94mLoss[0m : 1.75514
[1mStep[0m  [297/339], [94mLoss[0m : 2.52747
[1mStep[0m  [330/339], [94mLoss[0m : 2.77567

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.06622
[1mStep[0m  [33/339], [94mLoss[0m : 3.03242
[1mStep[0m  [66/339], [94mLoss[0m : 1.87550
[1mStep[0m  [99/339], [94mLoss[0m : 2.41681
[1mStep[0m  [132/339], [94mLoss[0m : 2.83429
[1mStep[0m  [165/339], [94mLoss[0m : 2.33776
[1mStep[0m  [198/339], [94mLoss[0m : 2.40610
[1mStep[0m  [231/339], [94mLoss[0m : 2.59968
[1mStep[0m  [264/339], [94mLoss[0m : 2.97308
[1mStep[0m  [297/339], [94mLoss[0m : 1.91244
[1mStep[0m  [330/339], [94mLoss[0m : 2.52605

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31252
[1mStep[0m  [33/339], [94mLoss[0m : 2.41999
[1mStep[0m  [66/339], [94mLoss[0m : 2.37739
[1mStep[0m  [99/339], [94mLoss[0m : 2.00480
[1mStep[0m  [132/339], [94mLoss[0m : 2.58048
[1mStep[0m  [165/339], [94mLoss[0m : 2.66222
[1mStep[0m  [198/339], [94mLoss[0m : 2.45148
[1mStep[0m  [231/339], [94mLoss[0m : 3.05269
[1mStep[0m  [264/339], [94mLoss[0m : 2.43036
[1mStep[0m  [297/339], [94mLoss[0m : 3.11051
[1mStep[0m  [330/339], [94mLoss[0m : 2.34589

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30347
[1mStep[0m  [33/339], [94mLoss[0m : 2.65647
[1mStep[0m  [66/339], [94mLoss[0m : 2.13045
[1mStep[0m  [99/339], [94mLoss[0m : 2.79777
[1mStep[0m  [132/339], [94mLoss[0m : 2.24753
[1mStep[0m  [165/339], [94mLoss[0m : 2.23500
[1mStep[0m  [198/339], [94mLoss[0m : 2.68591
[1mStep[0m  [231/339], [94mLoss[0m : 2.61035
[1mStep[0m  [264/339], [94mLoss[0m : 2.54454
[1mStep[0m  [297/339], [94mLoss[0m : 2.54078
[1mStep[0m  [330/339], [94mLoss[0m : 2.16306

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29450
[1mStep[0m  [33/339], [94mLoss[0m : 2.22984
[1mStep[0m  [66/339], [94mLoss[0m : 2.96523
[1mStep[0m  [99/339], [94mLoss[0m : 2.90353
[1mStep[0m  [132/339], [94mLoss[0m : 2.56279
[1mStep[0m  [165/339], [94mLoss[0m : 2.92099
[1mStep[0m  [198/339], [94mLoss[0m : 2.45603
[1mStep[0m  [231/339], [94mLoss[0m : 2.21473
[1mStep[0m  [264/339], [94mLoss[0m : 1.85640
[1mStep[0m  [297/339], [94mLoss[0m : 2.50995
[1mStep[0m  [330/339], [94mLoss[0m : 2.05988

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.29002
[1mStep[0m  [33/339], [94mLoss[0m : 2.24398
[1mStep[0m  [66/339], [94mLoss[0m : 2.17835
[1mStep[0m  [99/339], [94mLoss[0m : 2.22103
[1mStep[0m  [132/339], [94mLoss[0m : 2.40959
[1mStep[0m  [165/339], [94mLoss[0m : 2.43935
[1mStep[0m  [198/339], [94mLoss[0m : 3.45018
[1mStep[0m  [231/339], [94mLoss[0m : 2.58243
[1mStep[0m  [264/339], [94mLoss[0m : 2.55495
[1mStep[0m  [297/339], [94mLoss[0m : 2.56328
[1mStep[0m  [330/339], [94mLoss[0m : 3.09727

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16631
[1mStep[0m  [33/339], [94mLoss[0m : 1.73788
[1mStep[0m  [66/339], [94mLoss[0m : 2.30776
[1mStep[0m  [99/339], [94mLoss[0m : 1.96629
[1mStep[0m  [132/339], [94mLoss[0m : 2.22385
[1mStep[0m  [165/339], [94mLoss[0m : 1.78369
[1mStep[0m  [198/339], [94mLoss[0m : 2.52404
[1mStep[0m  [231/339], [94mLoss[0m : 2.31597
[1mStep[0m  [264/339], [94mLoss[0m : 2.34572
[1mStep[0m  [297/339], [94mLoss[0m : 2.57082
[1mStep[0m  [330/339], [94mLoss[0m : 2.40406

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04218
[1mStep[0m  [33/339], [94mLoss[0m : 2.50945
[1mStep[0m  [66/339], [94mLoss[0m : 2.90423
[1mStep[0m  [99/339], [94mLoss[0m : 2.72624
[1mStep[0m  [132/339], [94mLoss[0m : 2.49020
[1mStep[0m  [165/339], [94mLoss[0m : 2.01025
[1mStep[0m  [198/339], [94mLoss[0m : 2.47027
[1mStep[0m  [231/339], [94mLoss[0m : 2.30200
[1mStep[0m  [264/339], [94mLoss[0m : 2.43251
[1mStep[0m  [297/339], [94mLoss[0m : 1.93511
[1mStep[0m  [330/339], [94mLoss[0m : 2.39733

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15876
[1mStep[0m  [33/339], [94mLoss[0m : 2.42378
[1mStep[0m  [66/339], [94mLoss[0m : 2.01493
[1mStep[0m  [99/339], [94mLoss[0m : 2.48814
[1mStep[0m  [132/339], [94mLoss[0m : 2.00090
[1mStep[0m  [165/339], [94mLoss[0m : 2.31545
[1mStep[0m  [198/339], [94mLoss[0m : 2.58966
[1mStep[0m  [231/339], [94mLoss[0m : 2.41392
[1mStep[0m  [264/339], [94mLoss[0m : 2.14092
[1mStep[0m  [297/339], [94mLoss[0m : 2.07989
[1mStep[0m  [330/339], [94mLoss[0m : 2.53675

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21230
[1mStep[0m  [33/339], [94mLoss[0m : 2.18124
[1mStep[0m  [66/339], [94mLoss[0m : 2.73233
[1mStep[0m  [99/339], [94mLoss[0m : 2.30820
[1mStep[0m  [132/339], [94mLoss[0m : 2.62224
[1mStep[0m  [165/339], [94mLoss[0m : 2.02422
[1mStep[0m  [198/339], [94mLoss[0m : 2.28526
[1mStep[0m  [231/339], [94mLoss[0m : 2.62848
[1mStep[0m  [264/339], [94mLoss[0m : 2.23552
[1mStep[0m  [297/339], [94mLoss[0m : 2.04624
[1mStep[0m  [330/339], [94mLoss[0m : 2.58288

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75882
[1mStep[0m  [33/339], [94mLoss[0m : 2.39887
[1mStep[0m  [66/339], [94mLoss[0m : 2.39264
[1mStep[0m  [99/339], [94mLoss[0m : 2.17489
[1mStep[0m  [132/339], [94mLoss[0m : 2.11522
[1mStep[0m  [165/339], [94mLoss[0m : 2.01459
[1mStep[0m  [198/339], [94mLoss[0m : 1.88249
[1mStep[0m  [231/339], [94mLoss[0m : 2.24814
[1mStep[0m  [264/339], [94mLoss[0m : 2.42952
[1mStep[0m  [297/339], [94mLoss[0m : 2.70625
[1mStep[0m  [330/339], [94mLoss[0m : 2.16430

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.308, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40213
[1mStep[0m  [33/339], [94mLoss[0m : 2.64467
[1mStep[0m  [66/339], [94mLoss[0m : 2.68107
[1mStep[0m  [99/339], [94mLoss[0m : 2.17624
[1mStep[0m  [132/339], [94mLoss[0m : 2.02553
[1mStep[0m  [165/339], [94mLoss[0m : 1.93430
[1mStep[0m  [198/339], [94mLoss[0m : 2.09827
[1mStep[0m  [231/339], [94mLoss[0m : 2.73904
[1mStep[0m  [264/339], [94mLoss[0m : 2.83975
[1mStep[0m  [297/339], [94mLoss[0m : 2.85044
[1mStep[0m  [330/339], [94mLoss[0m : 1.92119

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.382, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24018
[1mStep[0m  [33/339], [94mLoss[0m : 2.73770
[1mStep[0m  [66/339], [94mLoss[0m : 2.29570
[1mStep[0m  [99/339], [94mLoss[0m : 2.15735
[1mStep[0m  [132/339], [94mLoss[0m : 1.69156
[1mStep[0m  [165/339], [94mLoss[0m : 1.90732
[1mStep[0m  [198/339], [94mLoss[0m : 2.77212
[1mStep[0m  [231/339], [94mLoss[0m : 2.34743
[1mStep[0m  [264/339], [94mLoss[0m : 2.18986
[1mStep[0m  [297/339], [94mLoss[0m : 2.09719
[1mStep[0m  [330/339], [94mLoss[0m : 2.05449

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11552
[1mStep[0m  [33/339], [94mLoss[0m : 2.44419
[1mStep[0m  [66/339], [94mLoss[0m : 2.05818
[1mStep[0m  [99/339], [94mLoss[0m : 3.06003
[1mStep[0m  [132/339], [94mLoss[0m : 2.58096
[1mStep[0m  [165/339], [94mLoss[0m : 2.12333
[1mStep[0m  [198/339], [94mLoss[0m : 1.79802
[1mStep[0m  [231/339], [94mLoss[0m : 2.32637
[1mStep[0m  [264/339], [94mLoss[0m : 2.42324
[1mStep[0m  [297/339], [94mLoss[0m : 2.44956
[1mStep[0m  [330/339], [94mLoss[0m : 2.13880

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08754
[1mStep[0m  [33/339], [94mLoss[0m : 2.18957
[1mStep[0m  [66/339], [94mLoss[0m : 2.06933
[1mStep[0m  [99/339], [94mLoss[0m : 2.49211
[1mStep[0m  [132/339], [94mLoss[0m : 2.36344
[1mStep[0m  [165/339], [94mLoss[0m : 2.74164
[1mStep[0m  [198/339], [94mLoss[0m : 1.91199
[1mStep[0m  [231/339], [94mLoss[0m : 2.40878
[1mStep[0m  [264/339], [94mLoss[0m : 2.90063
[1mStep[0m  [297/339], [94mLoss[0m : 2.72007
[1mStep[0m  [330/339], [94mLoss[0m : 2.28200

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.400, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43337
[1mStep[0m  [33/339], [94mLoss[0m : 2.34148
[1mStep[0m  [66/339], [94mLoss[0m : 1.88811
[1mStep[0m  [99/339], [94mLoss[0m : 2.46572
[1mStep[0m  [132/339], [94mLoss[0m : 1.89806
[1mStep[0m  [165/339], [94mLoss[0m : 2.16971
[1mStep[0m  [198/339], [94mLoss[0m : 2.37143
[1mStep[0m  [231/339], [94mLoss[0m : 2.53826
[1mStep[0m  [264/339], [94mLoss[0m : 2.40379
[1mStep[0m  [297/339], [94mLoss[0m : 2.65679
[1mStep[0m  [330/339], [94mLoss[0m : 2.13869

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.359, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35476
[1mStep[0m  [33/339], [94mLoss[0m : 2.47407
[1mStep[0m  [66/339], [94mLoss[0m : 2.78081
[1mStep[0m  [99/339], [94mLoss[0m : 2.73771
[1mStep[0m  [132/339], [94mLoss[0m : 2.40800
[1mStep[0m  [165/339], [94mLoss[0m : 3.26148
[1mStep[0m  [198/339], [94mLoss[0m : 3.07786
[1mStep[0m  [231/339], [94mLoss[0m : 2.74536
[1mStep[0m  [264/339], [94mLoss[0m : 2.10330
[1mStep[0m  [297/339], [94mLoss[0m : 2.75423
[1mStep[0m  [330/339], [94mLoss[0m : 2.03171

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.310, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56955
[1mStep[0m  [33/339], [94mLoss[0m : 2.24649
[1mStep[0m  [66/339], [94mLoss[0m : 2.44376
[1mStep[0m  [99/339], [94mLoss[0m : 2.58414
[1mStep[0m  [132/339], [94mLoss[0m : 2.34850
[1mStep[0m  [165/339], [94mLoss[0m : 2.15409
[1mStep[0m  [198/339], [94mLoss[0m : 2.29382
[1mStep[0m  [231/339], [94mLoss[0m : 1.94991
[1mStep[0m  [264/339], [94mLoss[0m : 2.00190
[1mStep[0m  [297/339], [94mLoss[0m : 2.43730
[1mStep[0m  [330/339], [94mLoss[0m : 2.32988

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.370, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31772
[1mStep[0m  [33/339], [94mLoss[0m : 2.03908
[1mStep[0m  [66/339], [94mLoss[0m : 2.05789
[1mStep[0m  [99/339], [94mLoss[0m : 2.55259
[1mStep[0m  [132/339], [94mLoss[0m : 1.94836
[1mStep[0m  [165/339], [94mLoss[0m : 1.92974
[1mStep[0m  [198/339], [94mLoss[0m : 2.23904
[1mStep[0m  [231/339], [94mLoss[0m : 2.75450
[1mStep[0m  [264/339], [94mLoss[0m : 2.06133
[1mStep[0m  [297/339], [94mLoss[0m : 2.23780
[1mStep[0m  [330/339], [94mLoss[0m : 2.09909

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40195
[1mStep[0m  [33/339], [94mLoss[0m : 2.32399
[1mStep[0m  [66/339], [94mLoss[0m : 1.97896
[1mStep[0m  [99/339], [94mLoss[0m : 2.20794
[1mStep[0m  [132/339], [94mLoss[0m : 1.49099
[1mStep[0m  [165/339], [94mLoss[0m : 2.18586
[1mStep[0m  [198/339], [94mLoss[0m : 2.45661
[1mStep[0m  [231/339], [94mLoss[0m : 2.65735
[1mStep[0m  [264/339], [94mLoss[0m : 1.92110
[1mStep[0m  [297/339], [94mLoss[0m : 2.93907
[1mStep[0m  [330/339], [94mLoss[0m : 2.28809

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.378, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76880
[1mStep[0m  [33/339], [94mLoss[0m : 2.21275
[1mStep[0m  [66/339], [94mLoss[0m : 2.22899
[1mStep[0m  [99/339], [94mLoss[0m : 2.36204
[1mStep[0m  [132/339], [94mLoss[0m : 2.26597
[1mStep[0m  [165/339], [94mLoss[0m : 2.69413
[1mStep[0m  [198/339], [94mLoss[0m : 2.53180
[1mStep[0m  [231/339], [94mLoss[0m : 2.25184
[1mStep[0m  [264/339], [94mLoss[0m : 2.49078
[1mStep[0m  [297/339], [94mLoss[0m : 1.83202
[1mStep[0m  [330/339], [94mLoss[0m : 2.44215

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89946
[1mStep[0m  [33/339], [94mLoss[0m : 2.50525
[1mStep[0m  [66/339], [94mLoss[0m : 2.10099
[1mStep[0m  [99/339], [94mLoss[0m : 2.47360
[1mStep[0m  [132/339], [94mLoss[0m : 2.86522
[1mStep[0m  [165/339], [94mLoss[0m : 1.98520
[1mStep[0m  [198/339], [94mLoss[0m : 2.40985
[1mStep[0m  [231/339], [94mLoss[0m : 3.18003
[1mStep[0m  [264/339], [94mLoss[0m : 2.16838
[1mStep[0m  [297/339], [94mLoss[0m : 2.39799
[1mStep[0m  [330/339], [94mLoss[0m : 2.33714

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.379, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.341
====================================

Phase 1 - Evaluation MAE:  2.3407751374540076
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.20348
[1mStep[0m  [33/339], [94mLoss[0m : 2.79980
[1mStep[0m  [66/339], [94mLoss[0m : 2.37821
[1mStep[0m  [99/339], [94mLoss[0m : 2.36058
[1mStep[0m  [132/339], [94mLoss[0m : 2.66495
[1mStep[0m  [165/339], [94mLoss[0m : 2.45094
[1mStep[0m  [198/339], [94mLoss[0m : 3.49916
[1mStep[0m  [231/339], [94mLoss[0m : 2.40180
[1mStep[0m  [264/339], [94mLoss[0m : 2.30062
[1mStep[0m  [297/339], [94mLoss[0m : 2.63442
[1mStep[0m  [330/339], [94mLoss[0m : 2.30965

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02335
[1mStep[0m  [33/339], [94mLoss[0m : 2.20227
[1mStep[0m  [66/339], [94mLoss[0m : 2.43037
[1mStep[0m  [99/339], [94mLoss[0m : 1.93765
[1mStep[0m  [132/339], [94mLoss[0m : 2.90353
[1mStep[0m  [165/339], [94mLoss[0m : 2.75703
[1mStep[0m  [198/339], [94mLoss[0m : 2.38559
[1mStep[0m  [231/339], [94mLoss[0m : 2.34881
[1mStep[0m  [264/339], [94mLoss[0m : 2.25603
[1mStep[0m  [297/339], [94mLoss[0m : 2.36692
[1mStep[0m  [330/339], [94mLoss[0m : 3.05034

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30444
[1mStep[0m  [33/339], [94mLoss[0m : 1.96625
[1mStep[0m  [66/339], [94mLoss[0m : 2.63463
[1mStep[0m  [99/339], [94mLoss[0m : 2.15096
[1mStep[0m  [132/339], [94mLoss[0m : 1.95444
[1mStep[0m  [165/339], [94mLoss[0m : 2.61654
[1mStep[0m  [198/339], [94mLoss[0m : 2.66483
[1mStep[0m  [231/339], [94mLoss[0m : 1.62009
[1mStep[0m  [264/339], [94mLoss[0m : 2.52242
[1mStep[0m  [297/339], [94mLoss[0m : 2.00338
[1mStep[0m  [330/339], [94mLoss[0m : 2.14700

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95920
[1mStep[0m  [33/339], [94mLoss[0m : 2.12512
[1mStep[0m  [66/339], [94mLoss[0m : 1.80483
[1mStep[0m  [99/339], [94mLoss[0m : 2.44535
[1mStep[0m  [132/339], [94mLoss[0m : 2.16203
[1mStep[0m  [165/339], [94mLoss[0m : 1.66302
[1mStep[0m  [198/339], [94mLoss[0m : 2.12086
[1mStep[0m  [231/339], [94mLoss[0m : 2.27679
[1mStep[0m  [264/339], [94mLoss[0m : 2.65644
[1mStep[0m  [297/339], [94mLoss[0m : 2.50853
[1mStep[0m  [330/339], [94mLoss[0m : 3.07501

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00873
[1mStep[0m  [33/339], [94mLoss[0m : 2.02223
[1mStep[0m  [66/339], [94mLoss[0m : 2.35120
[1mStep[0m  [99/339], [94mLoss[0m : 2.51421
[1mStep[0m  [132/339], [94mLoss[0m : 2.44396
[1mStep[0m  [165/339], [94mLoss[0m : 2.72511
[1mStep[0m  [198/339], [94mLoss[0m : 1.98712
[1mStep[0m  [231/339], [94mLoss[0m : 2.42848
[1mStep[0m  [264/339], [94mLoss[0m : 2.37259
[1mStep[0m  [297/339], [94mLoss[0m : 2.63997
[1mStep[0m  [330/339], [94mLoss[0m : 2.06512

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.230, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.58616
[1mStep[0m  [33/339], [94mLoss[0m : 2.10336
[1mStep[0m  [66/339], [94mLoss[0m : 2.34795
[1mStep[0m  [99/339], [94mLoss[0m : 1.88738
[1mStep[0m  [132/339], [94mLoss[0m : 3.18252
[1mStep[0m  [165/339], [94mLoss[0m : 2.74237
[1mStep[0m  [198/339], [94mLoss[0m : 1.50378
[1mStep[0m  [231/339], [94mLoss[0m : 1.63215
[1mStep[0m  [264/339], [94mLoss[0m : 2.31206
[1mStep[0m  [297/339], [94mLoss[0m : 2.01508
[1mStep[0m  [330/339], [94mLoss[0m : 2.83978

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09329
[1mStep[0m  [33/339], [94mLoss[0m : 2.03511
[1mStep[0m  [66/339], [94mLoss[0m : 1.85439
[1mStep[0m  [99/339], [94mLoss[0m : 2.17237
[1mStep[0m  [132/339], [94mLoss[0m : 1.89177
[1mStep[0m  [165/339], [94mLoss[0m : 1.49772
[1mStep[0m  [198/339], [94mLoss[0m : 2.80909
[1mStep[0m  [231/339], [94mLoss[0m : 2.33885
[1mStep[0m  [264/339], [94mLoss[0m : 2.60828
[1mStep[0m  [297/339], [94mLoss[0m : 2.52631
[1mStep[0m  [330/339], [94mLoss[0m : 2.87015

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27002
[1mStep[0m  [33/339], [94mLoss[0m : 1.94821
[1mStep[0m  [66/339], [94mLoss[0m : 2.00602
[1mStep[0m  [99/339], [94mLoss[0m : 2.07395
[1mStep[0m  [132/339], [94mLoss[0m : 2.18016
[1mStep[0m  [165/339], [94mLoss[0m : 1.83325
[1mStep[0m  [198/339], [94mLoss[0m : 2.36289
[1mStep[0m  [231/339], [94mLoss[0m : 2.13443
[1mStep[0m  [264/339], [94mLoss[0m : 2.20127
[1mStep[0m  [297/339], [94mLoss[0m : 1.77294
[1mStep[0m  [330/339], [94mLoss[0m : 2.49588

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87741
[1mStep[0m  [33/339], [94mLoss[0m : 2.35773
[1mStep[0m  [66/339], [94mLoss[0m : 1.60601
[1mStep[0m  [99/339], [94mLoss[0m : 2.83533
[1mStep[0m  [132/339], [94mLoss[0m : 2.33856
[1mStep[0m  [165/339], [94mLoss[0m : 1.91262
[1mStep[0m  [198/339], [94mLoss[0m : 1.74556
[1mStep[0m  [231/339], [94mLoss[0m : 2.64592
[1mStep[0m  [264/339], [94mLoss[0m : 2.29677
[1mStep[0m  [297/339], [94mLoss[0m : 2.47716
[1mStep[0m  [330/339], [94mLoss[0m : 1.71335

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.073, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46518
[1mStep[0m  [33/339], [94mLoss[0m : 1.94537
[1mStep[0m  [66/339], [94mLoss[0m : 1.82166
[1mStep[0m  [99/339], [94mLoss[0m : 1.71926
[1mStep[0m  [132/339], [94mLoss[0m : 1.69488
[1mStep[0m  [165/339], [94mLoss[0m : 2.19782
[1mStep[0m  [198/339], [94mLoss[0m : 2.22594
[1mStep[0m  [231/339], [94mLoss[0m : 2.00273
[1mStep[0m  [264/339], [94mLoss[0m : 1.58089
[1mStep[0m  [297/339], [94mLoss[0m : 1.93592
[1mStep[0m  [330/339], [94mLoss[0m : 2.26796

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83716
[1mStep[0m  [33/339], [94mLoss[0m : 1.78511
[1mStep[0m  [66/339], [94mLoss[0m : 1.87521
[1mStep[0m  [99/339], [94mLoss[0m : 2.09203
[1mStep[0m  [132/339], [94mLoss[0m : 1.59485
[1mStep[0m  [165/339], [94mLoss[0m : 1.76813
[1mStep[0m  [198/339], [94mLoss[0m : 1.63680
[1mStep[0m  [231/339], [94mLoss[0m : 2.33010
[1mStep[0m  [264/339], [94mLoss[0m : 1.72705
[1mStep[0m  [297/339], [94mLoss[0m : 2.47509
[1mStep[0m  [330/339], [94mLoss[0m : 2.50354

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42868
[1mStep[0m  [33/339], [94mLoss[0m : 2.04743
[1mStep[0m  [66/339], [94mLoss[0m : 1.80004
[1mStep[0m  [99/339], [94mLoss[0m : 2.07829
[1mStep[0m  [132/339], [94mLoss[0m : 1.68108
[1mStep[0m  [165/339], [94mLoss[0m : 1.56416
[1mStep[0m  [198/339], [94mLoss[0m : 2.54339
[1mStep[0m  [231/339], [94mLoss[0m : 1.60549
[1mStep[0m  [264/339], [94mLoss[0m : 2.16509
[1mStep[0m  [297/339], [94mLoss[0m : 1.90868
[1mStep[0m  [330/339], [94mLoss[0m : 1.38495

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91350
[1mStep[0m  [33/339], [94mLoss[0m : 2.48315
[1mStep[0m  [66/339], [94mLoss[0m : 1.82757
[1mStep[0m  [99/339], [94mLoss[0m : 1.34799
[1mStep[0m  [132/339], [94mLoss[0m : 2.20318
[1mStep[0m  [165/339], [94mLoss[0m : 2.10447
[1mStep[0m  [198/339], [94mLoss[0m : 2.63718
[1mStep[0m  [231/339], [94mLoss[0m : 2.25903
[1mStep[0m  [264/339], [94mLoss[0m : 2.02852
[1mStep[0m  [297/339], [94mLoss[0m : 2.48539
[1mStep[0m  [330/339], [94mLoss[0m : 1.90152

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06301
[1mStep[0m  [33/339], [94mLoss[0m : 2.06198
[1mStep[0m  [66/339], [94mLoss[0m : 1.58036
[1mStep[0m  [99/339], [94mLoss[0m : 2.39153
[1mStep[0m  [132/339], [94mLoss[0m : 1.57802
[1mStep[0m  [165/339], [94mLoss[0m : 2.28890
[1mStep[0m  [198/339], [94mLoss[0m : 2.47037
[1mStep[0m  [231/339], [94mLoss[0m : 1.35531
[1mStep[0m  [264/339], [94mLoss[0m : 1.72571
[1mStep[0m  [297/339], [94mLoss[0m : 1.80701
[1mStep[0m  [330/339], [94mLoss[0m : 2.24290

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48300
[1mStep[0m  [33/339], [94mLoss[0m : 1.77038
[1mStep[0m  [66/339], [94mLoss[0m : 1.75793
[1mStep[0m  [99/339], [94mLoss[0m : 2.23562
[1mStep[0m  [132/339], [94mLoss[0m : 2.30999
[1mStep[0m  [165/339], [94mLoss[0m : 2.10922
[1mStep[0m  [198/339], [94mLoss[0m : 1.62749
[1mStep[0m  [231/339], [94mLoss[0m : 1.18821
[1mStep[0m  [264/339], [94mLoss[0m : 1.56783
[1mStep[0m  [297/339], [94mLoss[0m : 1.78807
[1mStep[0m  [330/339], [94mLoss[0m : 1.52312

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25189
[1mStep[0m  [33/339], [94mLoss[0m : 1.83349
[1mStep[0m  [66/339], [94mLoss[0m : 1.57935
[1mStep[0m  [99/339], [94mLoss[0m : 1.83981
[1mStep[0m  [132/339], [94mLoss[0m : 1.49687
[1mStep[0m  [165/339], [94mLoss[0m : 1.53794
[1mStep[0m  [198/339], [94mLoss[0m : 1.56326
[1mStep[0m  [231/339], [94mLoss[0m : 1.78467
[1mStep[0m  [264/339], [94mLoss[0m : 1.49321
[1mStep[0m  [297/339], [94mLoss[0m : 1.58281
[1mStep[0m  [330/339], [94mLoss[0m : 1.77727

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96125
[1mStep[0m  [33/339], [94mLoss[0m : 1.64801
[1mStep[0m  [66/339], [94mLoss[0m : 2.16129
[1mStep[0m  [99/339], [94mLoss[0m : 2.51116
[1mStep[0m  [132/339], [94mLoss[0m : 1.63343
[1mStep[0m  [165/339], [94mLoss[0m : 1.70447
[1mStep[0m  [198/339], [94mLoss[0m : 1.68085
[1mStep[0m  [231/339], [94mLoss[0m : 1.52734
[1mStep[0m  [264/339], [94mLoss[0m : 1.91424
[1mStep[0m  [297/339], [94mLoss[0m : 1.69658
[1mStep[0m  [330/339], [94mLoss[0m : 1.55918

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78438
[1mStep[0m  [33/339], [94mLoss[0m : 1.75403
[1mStep[0m  [66/339], [94mLoss[0m : 2.39949
[1mStep[0m  [99/339], [94mLoss[0m : 2.35626
[1mStep[0m  [132/339], [94mLoss[0m : 1.99281
[1mStep[0m  [165/339], [94mLoss[0m : 1.93944
[1mStep[0m  [198/339], [94mLoss[0m : 1.96440
[1mStep[0m  [231/339], [94mLoss[0m : 1.53375
[1mStep[0m  [264/339], [94mLoss[0m : 1.96087
[1mStep[0m  [297/339], [94mLoss[0m : 1.67486
[1mStep[0m  [330/339], [94mLoss[0m : 1.43748

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.575, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81299
[1mStep[0m  [33/339], [94mLoss[0m : 1.50987
[1mStep[0m  [66/339], [94mLoss[0m : 1.40283
[1mStep[0m  [99/339], [94mLoss[0m : 2.33952
[1mStep[0m  [132/339], [94mLoss[0m : 2.04751
[1mStep[0m  [165/339], [94mLoss[0m : 1.51518
[1mStep[0m  [198/339], [94mLoss[0m : 1.60229
[1mStep[0m  [231/339], [94mLoss[0m : 1.87477
[1mStep[0m  [264/339], [94mLoss[0m : 1.95004
[1mStep[0m  [297/339], [94mLoss[0m : 1.75574
[1mStep[0m  [330/339], [94mLoss[0m : 1.20349

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.742, [92mTest[0m: 2.558, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99701
[1mStep[0m  [33/339], [94mLoss[0m : 1.46480
[1mStep[0m  [66/339], [94mLoss[0m : 1.60937
[1mStep[0m  [99/339], [94mLoss[0m : 1.80419
[1mStep[0m  [132/339], [94mLoss[0m : 2.09679
[1mStep[0m  [165/339], [94mLoss[0m : 1.51158
[1mStep[0m  [198/339], [94mLoss[0m : 1.38260
[1mStep[0m  [231/339], [94mLoss[0m : 2.21936
[1mStep[0m  [264/339], [94mLoss[0m : 1.21532
[1mStep[0m  [297/339], [94mLoss[0m : 2.10935
[1mStep[0m  [330/339], [94mLoss[0m : 2.20342

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.541, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99142
[1mStep[0m  [33/339], [94mLoss[0m : 1.36106
[1mStep[0m  [66/339], [94mLoss[0m : 1.43403
[1mStep[0m  [99/339], [94mLoss[0m : 1.41959
[1mStep[0m  [132/339], [94mLoss[0m : 2.33320
[1mStep[0m  [165/339], [94mLoss[0m : 1.81971
[1mStep[0m  [198/339], [94mLoss[0m : 1.52535
[1mStep[0m  [231/339], [94mLoss[0m : 1.39378
[1mStep[0m  [264/339], [94mLoss[0m : 2.08521
[1mStep[0m  [297/339], [94mLoss[0m : 1.57033
[1mStep[0m  [330/339], [94mLoss[0m : 1.52329

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24275
[1mStep[0m  [33/339], [94mLoss[0m : 1.79519
[1mStep[0m  [66/339], [94mLoss[0m : 1.76316
[1mStep[0m  [99/339], [94mLoss[0m : 1.96668
[1mStep[0m  [132/339], [94mLoss[0m : 1.05658
[1mStep[0m  [165/339], [94mLoss[0m : 2.01641
[1mStep[0m  [198/339], [94mLoss[0m : 2.16501
[1mStep[0m  [231/339], [94mLoss[0m : 1.57973
[1mStep[0m  [264/339], [94mLoss[0m : 1.76611
[1mStep[0m  [297/339], [94mLoss[0m : 1.74986
[1mStep[0m  [330/339], [94mLoss[0m : 1.67898

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.491, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.44255
[1mStep[0m  [33/339], [94mLoss[0m : 1.32220
[1mStep[0m  [66/339], [94mLoss[0m : 1.46559
[1mStep[0m  [99/339], [94mLoss[0m : 1.56113
[1mStep[0m  [132/339], [94mLoss[0m : 1.40598
[1mStep[0m  [165/339], [94mLoss[0m : 1.52209
[1mStep[0m  [198/339], [94mLoss[0m : 1.79476
[1mStep[0m  [231/339], [94mLoss[0m : 1.66800
[1mStep[0m  [264/339], [94mLoss[0m : 1.54176
[1mStep[0m  [297/339], [94mLoss[0m : 1.51527
[1mStep[0m  [330/339], [94mLoss[0m : 1.30235

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.542, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47643
[1mStep[0m  [33/339], [94mLoss[0m : 1.34319
[1mStep[0m  [66/339], [94mLoss[0m : 1.07277
[1mStep[0m  [99/339], [94mLoss[0m : 1.29140
[1mStep[0m  [132/339], [94mLoss[0m : 1.10674
[1mStep[0m  [165/339], [94mLoss[0m : 1.59564
[1mStep[0m  [198/339], [94mLoss[0m : 1.42550
[1mStep[0m  [231/339], [94mLoss[0m : 1.65391
[1mStep[0m  [264/339], [94mLoss[0m : 1.72830
[1mStep[0m  [297/339], [94mLoss[0m : 1.39764
[1mStep[0m  [330/339], [94mLoss[0m : 1.70579

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.32026
[1mStep[0m  [33/339], [94mLoss[0m : 2.40616
[1mStep[0m  [66/339], [94mLoss[0m : 1.18352
[1mStep[0m  [99/339], [94mLoss[0m : 1.30656
[1mStep[0m  [132/339], [94mLoss[0m : 1.24602
[1mStep[0m  [165/339], [94mLoss[0m : 1.69482
[1mStep[0m  [198/339], [94mLoss[0m : 1.37927
[1mStep[0m  [231/339], [94mLoss[0m : 1.76249
[1mStep[0m  [264/339], [94mLoss[0m : 1.80988
[1mStep[0m  [297/339], [94mLoss[0m : 1.57826
[1mStep[0m  [330/339], [94mLoss[0m : 1.47748

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.736, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82108
[1mStep[0m  [33/339], [94mLoss[0m : 1.88787
[1mStep[0m  [66/339], [94mLoss[0m : 1.35582
[1mStep[0m  [99/339], [94mLoss[0m : 2.11125
[1mStep[0m  [132/339], [94mLoss[0m : 1.64866
[1mStep[0m  [165/339], [94mLoss[0m : 1.44976
[1mStep[0m  [198/339], [94mLoss[0m : 1.81088
[1mStep[0m  [231/339], [94mLoss[0m : 1.32629
[1mStep[0m  [264/339], [94mLoss[0m : 1.79587
[1mStep[0m  [297/339], [94mLoss[0m : 1.56802
[1mStep[0m  [330/339], [94mLoss[0m : 1.89391

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.546, [92mTest[0m: 2.510, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.58584
[1mStep[0m  [33/339], [94mLoss[0m : 1.24429
[1mStep[0m  [66/339], [94mLoss[0m : 1.34909
[1mStep[0m  [99/339], [94mLoss[0m : 1.71685
[1mStep[0m  [132/339], [94mLoss[0m : 2.10950
[1mStep[0m  [165/339], [94mLoss[0m : 1.50570
[1mStep[0m  [198/339], [94mLoss[0m : 1.39104
[1mStep[0m  [231/339], [94mLoss[0m : 1.80060
[1mStep[0m  [264/339], [94mLoss[0m : 1.38551
[1mStep[0m  [297/339], [94mLoss[0m : 1.64318
[1mStep[0m  [330/339], [94mLoss[0m : 1.54662

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.544, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43376
[1mStep[0m  [33/339], [94mLoss[0m : 1.54051
[1mStep[0m  [66/339], [94mLoss[0m : 1.58958
[1mStep[0m  [99/339], [94mLoss[0m : 1.79601
[1mStep[0m  [132/339], [94mLoss[0m : 2.00339
[1mStep[0m  [165/339], [94mLoss[0m : 1.77751
[1mStep[0m  [198/339], [94mLoss[0m : 1.24938
[1mStep[0m  [231/339], [94mLoss[0m : 1.67741
[1mStep[0m  [264/339], [94mLoss[0m : 1.42102
[1mStep[0m  [297/339], [94mLoss[0m : 2.12312
[1mStep[0m  [330/339], [94mLoss[0m : 1.31770

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.503, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46360
[1mStep[0m  [33/339], [94mLoss[0m : 1.31847
[1mStep[0m  [66/339], [94mLoss[0m : 1.91033
[1mStep[0m  [99/339], [94mLoss[0m : 1.05033
[1mStep[0m  [132/339], [94mLoss[0m : 1.96770
[1mStep[0m  [165/339], [94mLoss[0m : 1.43777
[1mStep[0m  [198/339], [94mLoss[0m : 1.31664
[1mStep[0m  [231/339], [94mLoss[0m : 1.48452
[1mStep[0m  [264/339], [94mLoss[0m : 1.72968
[1mStep[0m  [297/339], [94mLoss[0m : 1.23888
[1mStep[0m  [330/339], [94mLoss[0m : 1.27459

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.604, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.08025
[1mStep[0m  [33/339], [94mLoss[0m : 1.39629
[1mStep[0m  [66/339], [94mLoss[0m : 1.09302
[1mStep[0m  [99/339], [94mLoss[0m : 1.86149
[1mStep[0m  [132/339], [94mLoss[0m : 1.03438
[1mStep[0m  [165/339], [94mLoss[0m : 1.56982
[1mStep[0m  [198/339], [94mLoss[0m : 1.62929
[1mStep[0m  [231/339], [94mLoss[0m : 1.45985
[1mStep[0m  [264/339], [94mLoss[0m : 1.98642
[1mStep[0m  [297/339], [94mLoss[0m : 1.24942
[1mStep[0m  [330/339], [94mLoss[0m : 1.42360

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.580, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.542
====================================

Phase 2 - Evaluation MAE:  2.542081780138269
MAE score P1        2.340775
MAE score P2        2.542082
loss                1.471355
learning_rate       0.007525
batch_size                32
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.9
weight_decay          0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.10067
[1mStep[0m  [2/21], [94mLoss[0m : 10.88885
[1mStep[0m  [4/21], [94mLoss[0m : 10.55601
[1mStep[0m  [6/21], [94mLoss[0m : 10.22051
[1mStep[0m  [8/21], [94mLoss[0m : 9.99724
[1mStep[0m  [10/21], [94mLoss[0m : 9.58414
[1mStep[0m  [12/21], [94mLoss[0m : 9.29723
[1mStep[0m  [14/21], [94mLoss[0m : 9.04994
[1mStep[0m  [16/21], [94mLoss[0m : 8.51715
[1mStep[0m  [18/21], [94mLoss[0m : 8.55204
[1mStep[0m  [20/21], [94mLoss[0m : 7.79038

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.630, [92mTest[0m: 11.135, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.15158
[1mStep[0m  [2/21], [94mLoss[0m : 7.87919
[1mStep[0m  [4/21], [94mLoss[0m : 7.56908
[1mStep[0m  [6/21], [94mLoss[0m : 6.99548
[1mStep[0m  [8/21], [94mLoss[0m : 6.91709
[1mStep[0m  [10/21], [94mLoss[0m : 6.41239
[1mStep[0m  [12/21], [94mLoss[0m : 6.23633
[1mStep[0m  [14/21], [94mLoss[0m : 6.18963
[1mStep[0m  [16/21], [94mLoss[0m : 5.71331
[1mStep[0m  [18/21], [94mLoss[0m : 5.29765
[1mStep[0m  [20/21], [94mLoss[0m : 5.14172

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.579, [92mTest[0m: 8.067, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.04659
[1mStep[0m  [2/21], [94mLoss[0m : 4.53041
[1mStep[0m  [4/21], [94mLoss[0m : 4.72810
[1mStep[0m  [6/21], [94mLoss[0m : 4.26078
[1mStep[0m  [8/21], [94mLoss[0m : 4.27807
[1mStep[0m  [10/21], [94mLoss[0m : 4.13887
[1mStep[0m  [12/21], [94mLoss[0m : 4.03150
[1mStep[0m  [14/21], [94mLoss[0m : 3.60908
[1mStep[0m  [16/21], [94mLoss[0m : 3.85228
[1mStep[0m  [18/21], [94mLoss[0m : 3.76617
[1mStep[0m  [20/21], [94mLoss[0m : 3.46260

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.153, [92mTest[0m: 5.078, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.51456
[1mStep[0m  [2/21], [94mLoss[0m : 3.34668
[1mStep[0m  [4/21], [94mLoss[0m : 3.52167
[1mStep[0m  [6/21], [94mLoss[0m : 3.15374
[1mStep[0m  [8/21], [94mLoss[0m : 2.84237
[1mStep[0m  [10/21], [94mLoss[0m : 3.15473
[1mStep[0m  [12/21], [94mLoss[0m : 3.08628
[1mStep[0m  [14/21], [94mLoss[0m : 3.28690
[1mStep[0m  [16/21], [94mLoss[0m : 3.15811
[1mStep[0m  [18/21], [94mLoss[0m : 3.06421
[1mStep[0m  [20/21], [94mLoss[0m : 2.87350

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.210, [92mTest[0m: 3.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.07354
[1mStep[0m  [2/21], [94mLoss[0m : 3.12301
[1mStep[0m  [4/21], [94mLoss[0m : 2.88878
[1mStep[0m  [6/21], [94mLoss[0m : 2.88235
[1mStep[0m  [8/21], [94mLoss[0m : 2.86184
[1mStep[0m  [10/21], [94mLoss[0m : 2.80262
[1mStep[0m  [12/21], [94mLoss[0m : 2.93876
[1mStep[0m  [14/21], [94mLoss[0m : 2.74235
[1mStep[0m  [16/21], [94mLoss[0m : 2.83642
[1mStep[0m  [18/21], [94mLoss[0m : 2.70970
[1mStep[0m  [20/21], [94mLoss[0m : 2.85527

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.879, [92mTest[0m: 2.899, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72368
[1mStep[0m  [2/21], [94mLoss[0m : 2.69048
[1mStep[0m  [4/21], [94mLoss[0m : 2.82905
[1mStep[0m  [6/21], [94mLoss[0m : 2.61902
[1mStep[0m  [8/21], [94mLoss[0m : 2.82819
[1mStep[0m  [10/21], [94mLoss[0m : 2.83161
[1mStep[0m  [12/21], [94mLoss[0m : 2.62548
[1mStep[0m  [14/21], [94mLoss[0m : 2.72967
[1mStep[0m  [16/21], [94mLoss[0m : 2.71491
[1mStep[0m  [18/21], [94mLoss[0m : 2.70779
[1mStep[0m  [20/21], [94mLoss[0m : 2.60633

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.738, [92mTest[0m: 2.649, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66316
[1mStep[0m  [2/21], [94mLoss[0m : 2.67025
[1mStep[0m  [4/21], [94mLoss[0m : 2.67567
[1mStep[0m  [6/21], [94mLoss[0m : 2.71690
[1mStep[0m  [8/21], [94mLoss[0m : 2.86641
[1mStep[0m  [10/21], [94mLoss[0m : 2.56012
[1mStep[0m  [12/21], [94mLoss[0m : 2.56161
[1mStep[0m  [14/21], [94mLoss[0m : 2.68143
[1mStep[0m  [16/21], [94mLoss[0m : 2.62448
[1mStep[0m  [18/21], [94mLoss[0m : 2.52344
[1mStep[0m  [20/21], [94mLoss[0m : 2.74389

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63669
[1mStep[0m  [2/21], [94mLoss[0m : 2.59300
[1mStep[0m  [4/21], [94mLoss[0m : 2.65163
[1mStep[0m  [6/21], [94mLoss[0m : 2.56566
[1mStep[0m  [8/21], [94mLoss[0m : 2.58121
[1mStep[0m  [10/21], [94mLoss[0m : 2.58333
[1mStep[0m  [12/21], [94mLoss[0m : 2.62887
[1mStep[0m  [14/21], [94mLoss[0m : 2.61210
[1mStep[0m  [16/21], [94mLoss[0m : 2.67038
[1mStep[0m  [18/21], [94mLoss[0m : 2.41593
[1mStep[0m  [20/21], [94mLoss[0m : 2.70759

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64072
[1mStep[0m  [2/21], [94mLoss[0m : 2.61892
[1mStep[0m  [4/21], [94mLoss[0m : 2.67319
[1mStep[0m  [6/21], [94mLoss[0m : 2.55656
[1mStep[0m  [8/21], [94mLoss[0m : 2.53347
[1mStep[0m  [10/21], [94mLoss[0m : 2.67184
[1mStep[0m  [12/21], [94mLoss[0m : 2.55754
[1mStep[0m  [14/21], [94mLoss[0m : 2.49935
[1mStep[0m  [16/21], [94mLoss[0m : 2.60520
[1mStep[0m  [18/21], [94mLoss[0m : 2.54748
[1mStep[0m  [20/21], [94mLoss[0m : 2.54921

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73513
[1mStep[0m  [2/21], [94mLoss[0m : 2.61942
[1mStep[0m  [4/21], [94mLoss[0m : 2.51467
[1mStep[0m  [6/21], [94mLoss[0m : 2.46881
[1mStep[0m  [8/21], [94mLoss[0m : 2.69462
[1mStep[0m  [10/21], [94mLoss[0m : 2.52553
[1mStep[0m  [12/21], [94mLoss[0m : 2.52971
[1mStep[0m  [14/21], [94mLoss[0m : 2.52469
[1mStep[0m  [16/21], [94mLoss[0m : 2.45924
[1mStep[0m  [18/21], [94mLoss[0m : 2.64225
[1mStep[0m  [20/21], [94mLoss[0m : 2.40567

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71252
[1mStep[0m  [2/21], [94mLoss[0m : 2.57035
[1mStep[0m  [4/21], [94mLoss[0m : 2.46450
[1mStep[0m  [6/21], [94mLoss[0m : 2.46482
[1mStep[0m  [8/21], [94mLoss[0m : 2.64937
[1mStep[0m  [10/21], [94mLoss[0m : 2.42494
[1mStep[0m  [12/21], [94mLoss[0m : 2.69103
[1mStep[0m  [14/21], [94mLoss[0m : 2.63826
[1mStep[0m  [16/21], [94mLoss[0m : 2.44945
[1mStep[0m  [18/21], [94mLoss[0m : 2.63680
[1mStep[0m  [20/21], [94mLoss[0m : 2.55674

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62964
[1mStep[0m  [2/21], [94mLoss[0m : 2.57047
[1mStep[0m  [4/21], [94mLoss[0m : 2.48659
[1mStep[0m  [6/21], [94mLoss[0m : 2.45289
[1mStep[0m  [8/21], [94mLoss[0m : 2.47648
[1mStep[0m  [10/21], [94mLoss[0m : 2.53835
[1mStep[0m  [12/21], [94mLoss[0m : 2.55957
[1mStep[0m  [14/21], [94mLoss[0m : 2.62668
[1mStep[0m  [16/21], [94mLoss[0m : 2.57495
[1mStep[0m  [18/21], [94mLoss[0m : 2.51175
[1mStep[0m  [20/21], [94mLoss[0m : 2.69253

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52013
[1mStep[0m  [2/21], [94mLoss[0m : 2.46696
[1mStep[0m  [4/21], [94mLoss[0m : 2.62457
[1mStep[0m  [6/21], [94mLoss[0m : 2.54211
[1mStep[0m  [8/21], [94mLoss[0m : 2.69570
[1mStep[0m  [10/21], [94mLoss[0m : 2.49505
[1mStep[0m  [12/21], [94mLoss[0m : 2.50414
[1mStep[0m  [14/21], [94mLoss[0m : 2.69982
[1mStep[0m  [16/21], [94mLoss[0m : 2.51304
[1mStep[0m  [18/21], [94mLoss[0m : 2.54378
[1mStep[0m  [20/21], [94mLoss[0m : 2.47226

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41513
[1mStep[0m  [2/21], [94mLoss[0m : 2.58666
[1mStep[0m  [4/21], [94mLoss[0m : 2.63364
[1mStep[0m  [6/21], [94mLoss[0m : 2.43509
[1mStep[0m  [8/21], [94mLoss[0m : 2.43956
[1mStep[0m  [10/21], [94mLoss[0m : 2.49683
[1mStep[0m  [12/21], [94mLoss[0m : 2.62335
[1mStep[0m  [14/21], [94mLoss[0m : 2.52697
[1mStep[0m  [16/21], [94mLoss[0m : 2.44671
[1mStep[0m  [18/21], [94mLoss[0m : 2.62775
[1mStep[0m  [20/21], [94mLoss[0m : 2.66153

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43333
[1mStep[0m  [2/21], [94mLoss[0m : 2.47070
[1mStep[0m  [4/21], [94mLoss[0m : 2.52480
[1mStep[0m  [6/21], [94mLoss[0m : 2.61261
[1mStep[0m  [8/21], [94mLoss[0m : 2.51365
[1mStep[0m  [10/21], [94mLoss[0m : 2.57294
[1mStep[0m  [12/21], [94mLoss[0m : 2.50193
[1mStep[0m  [14/21], [94mLoss[0m : 2.47728
[1mStep[0m  [16/21], [94mLoss[0m : 2.47544
[1mStep[0m  [18/21], [94mLoss[0m : 2.45323
[1mStep[0m  [20/21], [94mLoss[0m : 2.49319

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53552
[1mStep[0m  [2/21], [94mLoss[0m : 2.55354
[1mStep[0m  [4/21], [94mLoss[0m : 2.55396
[1mStep[0m  [6/21], [94mLoss[0m : 2.54952
[1mStep[0m  [8/21], [94mLoss[0m : 2.61349
[1mStep[0m  [10/21], [94mLoss[0m : 2.66876
[1mStep[0m  [12/21], [94mLoss[0m : 2.51967
[1mStep[0m  [14/21], [94mLoss[0m : 2.43397
[1mStep[0m  [16/21], [94mLoss[0m : 2.59941
[1mStep[0m  [18/21], [94mLoss[0m : 2.49850
[1mStep[0m  [20/21], [94mLoss[0m : 2.58846

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48889
[1mStep[0m  [2/21], [94mLoss[0m : 2.50268
[1mStep[0m  [4/21], [94mLoss[0m : 2.53902
[1mStep[0m  [6/21], [94mLoss[0m : 2.45469
[1mStep[0m  [8/21], [94mLoss[0m : 2.38397
[1mStep[0m  [10/21], [94mLoss[0m : 2.56465
[1mStep[0m  [12/21], [94mLoss[0m : 2.40868
[1mStep[0m  [14/21], [94mLoss[0m : 2.59170
[1mStep[0m  [16/21], [94mLoss[0m : 2.63182
[1mStep[0m  [18/21], [94mLoss[0m : 2.74891
[1mStep[0m  [20/21], [94mLoss[0m : 2.59739

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55122
[1mStep[0m  [2/21], [94mLoss[0m : 2.70714
[1mStep[0m  [4/21], [94mLoss[0m : 2.52307
[1mStep[0m  [6/21], [94mLoss[0m : 2.44806
[1mStep[0m  [8/21], [94mLoss[0m : 2.49900
[1mStep[0m  [10/21], [94mLoss[0m : 2.41439
[1mStep[0m  [12/21], [94mLoss[0m : 2.48247
[1mStep[0m  [14/21], [94mLoss[0m : 2.52810
[1mStep[0m  [16/21], [94mLoss[0m : 2.46229
[1mStep[0m  [18/21], [94mLoss[0m : 2.66899
[1mStep[0m  [20/21], [94mLoss[0m : 2.55411

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58063
[1mStep[0m  [2/21], [94mLoss[0m : 2.39175
[1mStep[0m  [4/21], [94mLoss[0m : 2.52884
[1mStep[0m  [6/21], [94mLoss[0m : 2.65462
[1mStep[0m  [8/21], [94mLoss[0m : 2.52854
[1mStep[0m  [10/21], [94mLoss[0m : 2.51587
[1mStep[0m  [12/21], [94mLoss[0m : 2.66529
[1mStep[0m  [14/21], [94mLoss[0m : 2.41968
[1mStep[0m  [16/21], [94mLoss[0m : 2.40174
[1mStep[0m  [18/21], [94mLoss[0m : 2.39880
[1mStep[0m  [20/21], [94mLoss[0m : 2.49953

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47825
[1mStep[0m  [2/21], [94mLoss[0m : 2.56810
[1mStep[0m  [4/21], [94mLoss[0m : 2.64684
[1mStep[0m  [6/21], [94mLoss[0m : 2.43805
[1mStep[0m  [8/21], [94mLoss[0m : 2.44620
[1mStep[0m  [10/21], [94mLoss[0m : 2.44463
[1mStep[0m  [12/21], [94mLoss[0m : 2.58274
[1mStep[0m  [14/21], [94mLoss[0m : 2.59917
[1mStep[0m  [16/21], [94mLoss[0m : 2.56795
[1mStep[0m  [18/21], [94mLoss[0m : 2.66020
[1mStep[0m  [20/21], [94mLoss[0m : 2.54721

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46374
[1mStep[0m  [2/21], [94mLoss[0m : 2.55950
[1mStep[0m  [4/21], [94mLoss[0m : 2.65644
[1mStep[0m  [6/21], [94mLoss[0m : 2.60026
[1mStep[0m  [8/21], [94mLoss[0m : 2.48499
[1mStep[0m  [10/21], [94mLoss[0m : 2.60265
[1mStep[0m  [12/21], [94mLoss[0m : 2.40692
[1mStep[0m  [14/21], [94mLoss[0m : 2.59882
[1mStep[0m  [16/21], [94mLoss[0m : 2.47843
[1mStep[0m  [18/21], [94mLoss[0m : 2.56804
[1mStep[0m  [20/21], [94mLoss[0m : 2.66877

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.391, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42530
[1mStep[0m  [2/21], [94mLoss[0m : 2.49173
[1mStep[0m  [4/21], [94mLoss[0m : 2.71139
[1mStep[0m  [6/21], [94mLoss[0m : 2.38135
[1mStep[0m  [8/21], [94mLoss[0m : 2.40907
[1mStep[0m  [10/21], [94mLoss[0m : 2.59674
[1mStep[0m  [12/21], [94mLoss[0m : 2.44791
[1mStep[0m  [14/21], [94mLoss[0m : 2.55706
[1mStep[0m  [16/21], [94mLoss[0m : 2.54505
[1mStep[0m  [18/21], [94mLoss[0m : 2.64462
[1mStep[0m  [20/21], [94mLoss[0m : 2.48438

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.388, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54169
[1mStep[0m  [2/21], [94mLoss[0m : 2.51190
[1mStep[0m  [4/21], [94mLoss[0m : 2.73534
[1mStep[0m  [6/21], [94mLoss[0m : 2.61910
[1mStep[0m  [8/21], [94mLoss[0m : 2.52716
[1mStep[0m  [10/21], [94mLoss[0m : 2.51780
[1mStep[0m  [12/21], [94mLoss[0m : 2.49800
[1mStep[0m  [14/21], [94mLoss[0m : 2.53857
[1mStep[0m  [16/21], [94mLoss[0m : 2.52953
[1mStep[0m  [18/21], [94mLoss[0m : 2.47051
[1mStep[0m  [20/21], [94mLoss[0m : 2.47513

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51110
[1mStep[0m  [2/21], [94mLoss[0m : 2.53457
[1mStep[0m  [4/21], [94mLoss[0m : 2.59869
[1mStep[0m  [6/21], [94mLoss[0m : 2.62814
[1mStep[0m  [8/21], [94mLoss[0m : 2.55859
[1mStep[0m  [10/21], [94mLoss[0m : 2.32016
[1mStep[0m  [12/21], [94mLoss[0m : 2.51722
[1mStep[0m  [14/21], [94mLoss[0m : 2.45903
[1mStep[0m  [16/21], [94mLoss[0m : 2.67718
[1mStep[0m  [18/21], [94mLoss[0m : 2.58370
[1mStep[0m  [20/21], [94mLoss[0m : 2.54130

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.380, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54481
[1mStep[0m  [2/21], [94mLoss[0m : 2.50502
[1mStep[0m  [4/21], [94mLoss[0m : 2.57898
[1mStep[0m  [6/21], [94mLoss[0m : 2.53683
[1mStep[0m  [8/21], [94mLoss[0m : 2.43024
[1mStep[0m  [10/21], [94mLoss[0m : 2.50555
[1mStep[0m  [12/21], [94mLoss[0m : 2.54562
[1mStep[0m  [14/21], [94mLoss[0m : 2.50378
[1mStep[0m  [16/21], [94mLoss[0m : 2.47680
[1mStep[0m  [18/21], [94mLoss[0m : 2.49921
[1mStep[0m  [20/21], [94mLoss[0m : 2.58747

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48132
[1mStep[0m  [2/21], [94mLoss[0m : 2.55295
[1mStep[0m  [4/21], [94mLoss[0m : 2.54330
[1mStep[0m  [6/21], [94mLoss[0m : 2.44468
[1mStep[0m  [8/21], [94mLoss[0m : 2.58580
[1mStep[0m  [10/21], [94mLoss[0m : 2.32300
[1mStep[0m  [12/21], [94mLoss[0m : 2.50643
[1mStep[0m  [14/21], [94mLoss[0m : 2.52901
[1mStep[0m  [16/21], [94mLoss[0m : 2.74345
[1mStep[0m  [18/21], [94mLoss[0m : 2.54064
[1mStep[0m  [20/21], [94mLoss[0m : 2.56358

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39420
[1mStep[0m  [2/21], [94mLoss[0m : 2.49846
[1mStep[0m  [4/21], [94mLoss[0m : 2.63076
[1mStep[0m  [6/21], [94mLoss[0m : 2.57701
[1mStep[0m  [8/21], [94mLoss[0m : 2.52832
[1mStep[0m  [10/21], [94mLoss[0m : 2.61359
[1mStep[0m  [12/21], [94mLoss[0m : 2.48293
[1mStep[0m  [14/21], [94mLoss[0m : 2.56471
[1mStep[0m  [16/21], [94mLoss[0m : 2.59742
[1mStep[0m  [18/21], [94mLoss[0m : 2.55070
[1mStep[0m  [20/21], [94mLoss[0m : 2.55126

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.378, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58002
[1mStep[0m  [2/21], [94mLoss[0m : 2.73024
[1mStep[0m  [4/21], [94mLoss[0m : 2.46896
[1mStep[0m  [6/21], [94mLoss[0m : 2.58422
[1mStep[0m  [8/21], [94mLoss[0m : 2.43025
[1mStep[0m  [10/21], [94mLoss[0m : 2.47570
[1mStep[0m  [12/21], [94mLoss[0m : 2.57371
[1mStep[0m  [14/21], [94mLoss[0m : 2.52881
[1mStep[0m  [16/21], [94mLoss[0m : 2.52088
[1mStep[0m  [18/21], [94mLoss[0m : 2.50053
[1mStep[0m  [20/21], [94mLoss[0m : 2.62249

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46680
[1mStep[0m  [2/21], [94mLoss[0m : 2.60580
[1mStep[0m  [4/21], [94mLoss[0m : 2.58828
[1mStep[0m  [6/21], [94mLoss[0m : 2.47735
[1mStep[0m  [8/21], [94mLoss[0m : 2.47559
[1mStep[0m  [10/21], [94mLoss[0m : 2.46205
[1mStep[0m  [12/21], [94mLoss[0m : 2.56628
[1mStep[0m  [14/21], [94mLoss[0m : 2.52686
[1mStep[0m  [16/21], [94mLoss[0m : 2.54273
[1mStep[0m  [18/21], [94mLoss[0m : 2.70053
[1mStep[0m  [20/21], [94mLoss[0m : 2.50561

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54892
[1mStep[0m  [2/21], [94mLoss[0m : 2.66452
[1mStep[0m  [4/21], [94mLoss[0m : 2.64644
[1mStep[0m  [6/21], [94mLoss[0m : 2.47294
[1mStep[0m  [8/21], [94mLoss[0m : 2.44811
[1mStep[0m  [10/21], [94mLoss[0m : 2.49340
[1mStep[0m  [12/21], [94mLoss[0m : 2.46918
[1mStep[0m  [14/21], [94mLoss[0m : 2.52087
[1mStep[0m  [16/21], [94mLoss[0m : 2.46909
[1mStep[0m  [18/21], [94mLoss[0m : 2.53795
[1mStep[0m  [20/21], [94mLoss[0m : 2.69607

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.364
====================================

Phase 1 - Evaluation MAE:  2.364147220339094
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.59161
[1mStep[0m  [2/21], [94mLoss[0m : 2.44637
[1mStep[0m  [4/21], [94mLoss[0m : 2.54937
[1mStep[0m  [6/21], [94mLoss[0m : 2.55468
[1mStep[0m  [8/21], [94mLoss[0m : 2.47969
[1mStep[0m  [10/21], [94mLoss[0m : 2.52657
[1mStep[0m  [12/21], [94mLoss[0m : 2.39023
[1mStep[0m  [14/21], [94mLoss[0m : 2.60096
[1mStep[0m  [16/21], [94mLoss[0m : 2.40689
[1mStep[0m  [18/21], [94mLoss[0m : 2.47447
[1mStep[0m  [20/21], [94mLoss[0m : 2.52208

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51877
[1mStep[0m  [2/21], [94mLoss[0m : 2.50645
[1mStep[0m  [4/21], [94mLoss[0m : 2.56672
[1mStep[0m  [6/21], [94mLoss[0m : 2.52640
[1mStep[0m  [8/21], [94mLoss[0m : 2.61084
[1mStep[0m  [10/21], [94mLoss[0m : 2.47743
[1mStep[0m  [12/21], [94mLoss[0m : 2.42219
[1mStep[0m  [14/21], [94mLoss[0m : 2.48168
[1mStep[0m  [16/21], [94mLoss[0m : 2.43461
[1mStep[0m  [18/21], [94mLoss[0m : 2.47127
[1mStep[0m  [20/21], [94mLoss[0m : 2.59548

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67180
[1mStep[0m  [2/21], [94mLoss[0m : 2.60112
[1mStep[0m  [4/21], [94mLoss[0m : 2.38528
[1mStep[0m  [6/21], [94mLoss[0m : 2.43216
[1mStep[0m  [8/21], [94mLoss[0m : 2.45461
[1mStep[0m  [10/21], [94mLoss[0m : 2.48147
[1mStep[0m  [12/21], [94mLoss[0m : 2.51096
[1mStep[0m  [14/21], [94mLoss[0m : 2.48642
[1mStep[0m  [16/21], [94mLoss[0m : 2.49400
[1mStep[0m  [18/21], [94mLoss[0m : 2.46657
[1mStep[0m  [20/21], [94mLoss[0m : 2.48291

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63889
[1mStep[0m  [2/21], [94mLoss[0m : 2.48405
[1mStep[0m  [4/21], [94mLoss[0m : 2.46511
[1mStep[0m  [6/21], [94mLoss[0m : 2.53641
[1mStep[0m  [8/21], [94mLoss[0m : 2.56097
[1mStep[0m  [10/21], [94mLoss[0m : 2.48043
[1mStep[0m  [12/21], [94mLoss[0m : 2.31574
[1mStep[0m  [14/21], [94mLoss[0m : 2.34540
[1mStep[0m  [16/21], [94mLoss[0m : 2.42706
[1mStep[0m  [18/21], [94mLoss[0m : 2.58347
[1mStep[0m  [20/21], [94mLoss[0m : 2.52201

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49672
[1mStep[0m  [2/21], [94mLoss[0m : 2.37478
[1mStep[0m  [4/21], [94mLoss[0m : 2.49186
[1mStep[0m  [6/21], [94mLoss[0m : 2.39234
[1mStep[0m  [8/21], [94mLoss[0m : 2.49324
[1mStep[0m  [10/21], [94mLoss[0m : 2.61284
[1mStep[0m  [12/21], [94mLoss[0m : 2.54435
[1mStep[0m  [14/21], [94mLoss[0m : 2.48041
[1mStep[0m  [16/21], [94mLoss[0m : 2.62959
[1mStep[0m  [18/21], [94mLoss[0m : 2.42411
[1mStep[0m  [20/21], [94mLoss[0m : 2.54607

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58965
[1mStep[0m  [2/21], [94mLoss[0m : 2.44615
[1mStep[0m  [4/21], [94mLoss[0m : 2.39170
[1mStep[0m  [6/21], [94mLoss[0m : 2.51578
[1mStep[0m  [8/21], [94mLoss[0m : 2.41392
[1mStep[0m  [10/21], [94mLoss[0m : 2.38657
[1mStep[0m  [12/21], [94mLoss[0m : 2.37983
[1mStep[0m  [14/21], [94mLoss[0m : 2.31573
[1mStep[0m  [16/21], [94mLoss[0m : 2.46238
[1mStep[0m  [18/21], [94mLoss[0m : 2.40627
[1mStep[0m  [20/21], [94mLoss[0m : 2.62449

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34516
[1mStep[0m  [2/21], [94mLoss[0m : 2.44998
[1mStep[0m  [4/21], [94mLoss[0m : 2.60835
[1mStep[0m  [6/21], [94mLoss[0m : 2.48120
[1mStep[0m  [8/21], [94mLoss[0m : 2.43956
[1mStep[0m  [10/21], [94mLoss[0m : 2.48981
[1mStep[0m  [12/21], [94mLoss[0m : 2.35160
[1mStep[0m  [14/21], [94mLoss[0m : 2.39815
[1mStep[0m  [16/21], [94mLoss[0m : 2.58112
[1mStep[0m  [18/21], [94mLoss[0m : 2.33988
[1mStep[0m  [20/21], [94mLoss[0m : 2.39688

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50749
[1mStep[0m  [2/21], [94mLoss[0m : 2.39199
[1mStep[0m  [4/21], [94mLoss[0m : 2.53016
[1mStep[0m  [6/21], [94mLoss[0m : 2.35669
[1mStep[0m  [8/21], [94mLoss[0m : 2.35034
[1mStep[0m  [10/21], [94mLoss[0m : 2.51700
[1mStep[0m  [12/21], [94mLoss[0m : 2.40455
[1mStep[0m  [14/21], [94mLoss[0m : 2.48451
[1mStep[0m  [16/21], [94mLoss[0m : 2.31008
[1mStep[0m  [18/21], [94mLoss[0m : 2.56962
[1mStep[0m  [20/21], [94mLoss[0m : 2.48947

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56728
[1mStep[0m  [2/21], [94mLoss[0m : 2.42147
[1mStep[0m  [4/21], [94mLoss[0m : 2.44673
[1mStep[0m  [6/21], [94mLoss[0m : 2.45774
[1mStep[0m  [8/21], [94mLoss[0m : 2.64579
[1mStep[0m  [10/21], [94mLoss[0m : 2.38328
[1mStep[0m  [12/21], [94mLoss[0m : 2.32747
[1mStep[0m  [14/21], [94mLoss[0m : 2.41747
[1mStep[0m  [16/21], [94mLoss[0m : 2.37883
[1mStep[0m  [18/21], [94mLoss[0m : 2.57064
[1mStep[0m  [20/21], [94mLoss[0m : 2.51581

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51772
[1mStep[0m  [2/21], [94mLoss[0m : 2.60203
[1mStep[0m  [4/21], [94mLoss[0m : 2.33675
[1mStep[0m  [6/21], [94mLoss[0m : 2.61243
[1mStep[0m  [8/21], [94mLoss[0m : 2.36206
[1mStep[0m  [10/21], [94mLoss[0m : 2.54428
[1mStep[0m  [12/21], [94mLoss[0m : 2.48784
[1mStep[0m  [14/21], [94mLoss[0m : 2.49091
[1mStep[0m  [16/21], [94mLoss[0m : 2.39703
[1mStep[0m  [18/21], [94mLoss[0m : 2.42508
[1mStep[0m  [20/21], [94mLoss[0m : 2.39292

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54349
[1mStep[0m  [2/21], [94mLoss[0m : 2.28345
[1mStep[0m  [4/21], [94mLoss[0m : 2.52738
[1mStep[0m  [6/21], [94mLoss[0m : 2.52167
[1mStep[0m  [8/21], [94mLoss[0m : 2.40247
[1mStep[0m  [10/21], [94mLoss[0m : 2.68916
[1mStep[0m  [12/21], [94mLoss[0m : 2.37440
[1mStep[0m  [14/21], [94mLoss[0m : 2.29641
[1mStep[0m  [16/21], [94mLoss[0m : 2.38161
[1mStep[0m  [18/21], [94mLoss[0m : 2.41281
[1mStep[0m  [20/21], [94mLoss[0m : 2.46544

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41848
[1mStep[0m  [2/21], [94mLoss[0m : 2.37827
[1mStep[0m  [4/21], [94mLoss[0m : 2.47848
[1mStep[0m  [6/21], [94mLoss[0m : 2.47792
[1mStep[0m  [8/21], [94mLoss[0m : 2.56342
[1mStep[0m  [10/21], [94mLoss[0m : 2.45774
[1mStep[0m  [12/21], [94mLoss[0m : 2.42267
[1mStep[0m  [14/21], [94mLoss[0m : 2.47055
[1mStep[0m  [16/21], [94mLoss[0m : 2.53581
[1mStep[0m  [18/21], [94mLoss[0m : 2.36128
[1mStep[0m  [20/21], [94mLoss[0m : 2.45813

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38167
[1mStep[0m  [2/21], [94mLoss[0m : 2.47081
[1mStep[0m  [4/21], [94mLoss[0m : 2.54837
[1mStep[0m  [6/21], [94mLoss[0m : 2.44451
[1mStep[0m  [8/21], [94mLoss[0m : 2.36590
[1mStep[0m  [10/21], [94mLoss[0m : 2.62132
[1mStep[0m  [12/21], [94mLoss[0m : 2.46444
[1mStep[0m  [14/21], [94mLoss[0m : 2.49679
[1mStep[0m  [16/21], [94mLoss[0m : 2.37187
[1mStep[0m  [18/21], [94mLoss[0m : 2.37625
[1mStep[0m  [20/21], [94mLoss[0m : 2.51529

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32505
[1mStep[0m  [2/21], [94mLoss[0m : 2.49853
[1mStep[0m  [4/21], [94mLoss[0m : 2.42978
[1mStep[0m  [6/21], [94mLoss[0m : 2.47273
[1mStep[0m  [8/21], [94mLoss[0m : 2.55005
[1mStep[0m  [10/21], [94mLoss[0m : 2.44875
[1mStep[0m  [12/21], [94mLoss[0m : 2.41988
[1mStep[0m  [14/21], [94mLoss[0m : 2.23397
[1mStep[0m  [16/21], [94mLoss[0m : 2.63833
[1mStep[0m  [18/21], [94mLoss[0m : 2.48408
[1mStep[0m  [20/21], [94mLoss[0m : 2.43377

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51813
[1mStep[0m  [2/21], [94mLoss[0m : 2.52419
[1mStep[0m  [4/21], [94mLoss[0m : 2.45621
[1mStep[0m  [6/21], [94mLoss[0m : 2.25727
[1mStep[0m  [8/21], [94mLoss[0m : 2.43645
[1mStep[0m  [10/21], [94mLoss[0m : 2.39727
[1mStep[0m  [12/21], [94mLoss[0m : 2.51097
[1mStep[0m  [14/21], [94mLoss[0m : 2.53606
[1mStep[0m  [16/21], [94mLoss[0m : 2.47712
[1mStep[0m  [18/21], [94mLoss[0m : 2.40521
[1mStep[0m  [20/21], [94mLoss[0m : 2.32745

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39374
[1mStep[0m  [2/21], [94mLoss[0m : 2.50925
[1mStep[0m  [4/21], [94mLoss[0m : 2.39021
[1mStep[0m  [6/21], [94mLoss[0m : 2.36861
[1mStep[0m  [8/21], [94mLoss[0m : 2.46807
[1mStep[0m  [10/21], [94mLoss[0m : 2.41098
[1mStep[0m  [12/21], [94mLoss[0m : 2.33352
[1mStep[0m  [14/21], [94mLoss[0m : 2.47389
[1mStep[0m  [16/21], [94mLoss[0m : 2.44957
[1mStep[0m  [18/21], [94mLoss[0m : 2.53493
[1mStep[0m  [20/21], [94mLoss[0m : 2.50371

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29167
[1mStep[0m  [2/21], [94mLoss[0m : 2.46131
[1mStep[0m  [4/21], [94mLoss[0m : 2.32398
[1mStep[0m  [6/21], [94mLoss[0m : 2.49652
[1mStep[0m  [8/21], [94mLoss[0m : 2.49503
[1mStep[0m  [10/21], [94mLoss[0m : 2.33968
[1mStep[0m  [12/21], [94mLoss[0m : 2.46760
[1mStep[0m  [14/21], [94mLoss[0m : 2.47490
[1mStep[0m  [16/21], [94mLoss[0m : 2.40955
[1mStep[0m  [18/21], [94mLoss[0m : 2.36080
[1mStep[0m  [20/21], [94mLoss[0m : 2.46975

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30121
[1mStep[0m  [2/21], [94mLoss[0m : 2.36493
[1mStep[0m  [4/21], [94mLoss[0m : 2.42333
[1mStep[0m  [6/21], [94mLoss[0m : 2.49538
[1mStep[0m  [8/21], [94mLoss[0m : 2.44249
[1mStep[0m  [10/21], [94mLoss[0m : 2.30886
[1mStep[0m  [12/21], [94mLoss[0m : 2.46570
[1mStep[0m  [14/21], [94mLoss[0m : 2.40611
[1mStep[0m  [16/21], [94mLoss[0m : 2.37227
[1mStep[0m  [18/21], [94mLoss[0m : 2.44379
[1mStep[0m  [20/21], [94mLoss[0m : 2.47091

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43072
[1mStep[0m  [2/21], [94mLoss[0m : 2.67508
[1mStep[0m  [4/21], [94mLoss[0m : 2.37391
[1mStep[0m  [6/21], [94mLoss[0m : 2.57250
[1mStep[0m  [8/21], [94mLoss[0m : 2.52037
[1mStep[0m  [10/21], [94mLoss[0m : 2.55859
[1mStep[0m  [12/21], [94mLoss[0m : 2.28540
[1mStep[0m  [14/21], [94mLoss[0m : 2.40852
[1mStep[0m  [16/21], [94mLoss[0m : 2.45456
[1mStep[0m  [18/21], [94mLoss[0m : 2.37059
[1mStep[0m  [20/21], [94mLoss[0m : 2.43280

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45163
[1mStep[0m  [2/21], [94mLoss[0m : 2.42730
[1mStep[0m  [4/21], [94mLoss[0m : 2.41680
[1mStep[0m  [6/21], [94mLoss[0m : 2.37578
[1mStep[0m  [8/21], [94mLoss[0m : 2.55552
[1mStep[0m  [10/21], [94mLoss[0m : 2.25396
[1mStep[0m  [12/21], [94mLoss[0m : 2.43274
[1mStep[0m  [14/21], [94mLoss[0m : 2.30881
[1mStep[0m  [16/21], [94mLoss[0m : 2.43289
[1mStep[0m  [18/21], [94mLoss[0m : 2.41371
[1mStep[0m  [20/21], [94mLoss[0m : 2.36814

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44973
[1mStep[0m  [2/21], [94mLoss[0m : 2.43417
[1mStep[0m  [4/21], [94mLoss[0m : 2.30175
[1mStep[0m  [6/21], [94mLoss[0m : 2.35286
[1mStep[0m  [8/21], [94mLoss[0m : 2.51906
[1mStep[0m  [10/21], [94mLoss[0m : 2.38677
[1mStep[0m  [12/21], [94mLoss[0m : 2.52980
[1mStep[0m  [14/21], [94mLoss[0m : 2.43478
[1mStep[0m  [16/21], [94mLoss[0m : 2.40990
[1mStep[0m  [18/21], [94mLoss[0m : 2.35751
[1mStep[0m  [20/21], [94mLoss[0m : 2.30929

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36252
[1mStep[0m  [2/21], [94mLoss[0m : 2.32191
[1mStep[0m  [4/21], [94mLoss[0m : 2.24627
[1mStep[0m  [6/21], [94mLoss[0m : 2.29478
[1mStep[0m  [8/21], [94mLoss[0m : 2.42586
[1mStep[0m  [10/21], [94mLoss[0m : 2.33603
[1mStep[0m  [12/21], [94mLoss[0m : 2.38304
[1mStep[0m  [14/21], [94mLoss[0m : 2.21717
[1mStep[0m  [16/21], [94mLoss[0m : 2.55760
[1mStep[0m  [18/21], [94mLoss[0m : 2.38576
[1mStep[0m  [20/21], [94mLoss[0m : 2.52188

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.388, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51714
[1mStep[0m  [2/21], [94mLoss[0m : 2.26052
[1mStep[0m  [4/21], [94mLoss[0m : 2.57165
[1mStep[0m  [6/21], [94mLoss[0m : 2.36068
[1mStep[0m  [8/21], [94mLoss[0m : 2.33588
[1mStep[0m  [10/21], [94mLoss[0m : 2.29753
[1mStep[0m  [12/21], [94mLoss[0m : 2.31137
[1mStep[0m  [14/21], [94mLoss[0m : 2.41680
[1mStep[0m  [16/21], [94mLoss[0m : 2.40664
[1mStep[0m  [18/21], [94mLoss[0m : 2.56849
[1mStep[0m  [20/21], [94mLoss[0m : 2.41290

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37026
[1mStep[0m  [2/21], [94mLoss[0m : 2.31358
[1mStep[0m  [4/21], [94mLoss[0m : 2.41891
[1mStep[0m  [6/21], [94mLoss[0m : 2.39420
[1mStep[0m  [8/21], [94mLoss[0m : 2.24379
[1mStep[0m  [10/21], [94mLoss[0m : 2.48954
[1mStep[0m  [12/21], [94mLoss[0m : 2.28099
[1mStep[0m  [14/21], [94mLoss[0m : 2.48543
[1mStep[0m  [16/21], [94mLoss[0m : 2.27499
[1mStep[0m  [18/21], [94mLoss[0m : 2.29471
[1mStep[0m  [20/21], [94mLoss[0m : 2.46411

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33556
[1mStep[0m  [2/21], [94mLoss[0m : 2.41979
[1mStep[0m  [4/21], [94mLoss[0m : 2.33854
[1mStep[0m  [6/21], [94mLoss[0m : 2.33610
[1mStep[0m  [8/21], [94mLoss[0m : 2.46130
[1mStep[0m  [10/21], [94mLoss[0m : 2.37368
[1mStep[0m  [12/21], [94mLoss[0m : 2.20593
[1mStep[0m  [14/21], [94mLoss[0m : 2.42520
[1mStep[0m  [16/21], [94mLoss[0m : 2.45214
[1mStep[0m  [18/21], [94mLoss[0m : 2.34610
[1mStep[0m  [20/21], [94mLoss[0m : 2.32639

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43452
[1mStep[0m  [2/21], [94mLoss[0m : 2.38841
[1mStep[0m  [4/21], [94mLoss[0m : 2.47781
[1mStep[0m  [6/21], [94mLoss[0m : 2.37515
[1mStep[0m  [8/21], [94mLoss[0m : 2.43709
[1mStep[0m  [10/21], [94mLoss[0m : 2.17060
[1mStep[0m  [12/21], [94mLoss[0m : 2.38914
[1mStep[0m  [14/21], [94mLoss[0m : 2.47940
[1mStep[0m  [16/21], [94mLoss[0m : 2.50832
[1mStep[0m  [18/21], [94mLoss[0m : 2.45360
[1mStep[0m  [20/21], [94mLoss[0m : 2.35177

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37991
[1mStep[0m  [2/21], [94mLoss[0m : 2.52492
[1mStep[0m  [4/21], [94mLoss[0m : 2.30875
[1mStep[0m  [6/21], [94mLoss[0m : 2.36818
[1mStep[0m  [8/21], [94mLoss[0m : 2.27425
[1mStep[0m  [10/21], [94mLoss[0m : 2.34400
[1mStep[0m  [12/21], [94mLoss[0m : 2.45761
[1mStep[0m  [14/21], [94mLoss[0m : 2.42014
[1mStep[0m  [16/21], [94mLoss[0m : 2.28376
[1mStep[0m  [18/21], [94mLoss[0m : 2.26402
[1mStep[0m  [20/21], [94mLoss[0m : 2.38011

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36172
[1mStep[0m  [2/21], [94mLoss[0m : 2.23686
[1mStep[0m  [4/21], [94mLoss[0m : 2.35627
[1mStep[0m  [6/21], [94mLoss[0m : 2.45179
[1mStep[0m  [8/21], [94mLoss[0m : 2.33158
[1mStep[0m  [10/21], [94mLoss[0m : 2.40478
[1mStep[0m  [12/21], [94mLoss[0m : 2.23529
[1mStep[0m  [14/21], [94mLoss[0m : 2.33483
[1mStep[0m  [16/21], [94mLoss[0m : 2.27923
[1mStep[0m  [18/21], [94mLoss[0m : 2.29450
[1mStep[0m  [20/21], [94mLoss[0m : 2.48642

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44224
[1mStep[0m  [2/21], [94mLoss[0m : 2.48881
[1mStep[0m  [4/21], [94mLoss[0m : 2.35230
[1mStep[0m  [6/21], [94mLoss[0m : 2.28454
[1mStep[0m  [8/21], [94mLoss[0m : 2.46301
[1mStep[0m  [10/21], [94mLoss[0m : 2.22784
[1mStep[0m  [12/21], [94mLoss[0m : 2.34261
[1mStep[0m  [14/21], [94mLoss[0m : 2.26073
[1mStep[0m  [16/21], [94mLoss[0m : 2.40806
[1mStep[0m  [18/21], [94mLoss[0m : 2.31833
[1mStep[0m  [20/21], [94mLoss[0m : 2.28603

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29633
[1mStep[0m  [2/21], [94mLoss[0m : 2.35604
[1mStep[0m  [4/21], [94mLoss[0m : 2.28726
[1mStep[0m  [6/21], [94mLoss[0m : 2.39773
[1mStep[0m  [8/21], [94mLoss[0m : 2.45544
[1mStep[0m  [10/21], [94mLoss[0m : 2.18027
[1mStep[0m  [12/21], [94mLoss[0m : 2.16983
[1mStep[0m  [14/21], [94mLoss[0m : 2.42544
[1mStep[0m  [16/21], [94mLoss[0m : 2.46436
[1mStep[0m  [18/21], [94mLoss[0m : 2.30340
[1mStep[0m  [20/21], [94mLoss[0m : 2.30251

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.439
====================================

Phase 2 - Evaluation MAE:  2.4387807846069336
MAE score P1        2.364147
MAE score P2        2.438781
loss                2.331525
learning_rate        0.00505
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay          0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.02814
[1mStep[0m  [2/21], [94mLoss[0m : 11.06484
[1mStep[0m  [4/21], [94mLoss[0m : 11.02049
[1mStep[0m  [6/21], [94mLoss[0m : 10.84690
[1mStep[0m  [8/21], [94mLoss[0m : 10.98073
[1mStep[0m  [10/21], [94mLoss[0m : 11.13615
[1mStep[0m  [12/21], [94mLoss[0m : 10.72270
[1mStep[0m  [14/21], [94mLoss[0m : 10.97407
[1mStep[0m  [16/21], [94mLoss[0m : 11.02212
[1mStep[0m  [18/21], [94mLoss[0m : 10.99026
[1mStep[0m  [20/21], [94mLoss[0m : 10.96047

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.991, [92mTest[0m: 10.966, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.06480
[1mStep[0m  [2/21], [94mLoss[0m : 11.07411
[1mStep[0m  [4/21], [94mLoss[0m : 10.47868
[1mStep[0m  [6/21], [94mLoss[0m : 10.89789
[1mStep[0m  [8/21], [94mLoss[0m : 10.97290
[1mStep[0m  [10/21], [94mLoss[0m : 10.67602
[1mStep[0m  [12/21], [94mLoss[0m : 10.95651
[1mStep[0m  [14/21], [94mLoss[0m : 10.57457
[1mStep[0m  [16/21], [94mLoss[0m : 11.01546
[1mStep[0m  [18/21], [94mLoss[0m : 10.75968
[1mStep[0m  [20/21], [94mLoss[0m : 10.49933

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.842, [92mTest[0m: 10.794, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.05271
[1mStep[0m  [2/21], [94mLoss[0m : 10.62669
[1mStep[0m  [4/21], [94mLoss[0m : 10.46508
[1mStep[0m  [6/21], [94mLoss[0m : 10.68450
[1mStep[0m  [8/21], [94mLoss[0m : 10.41178
[1mStep[0m  [10/21], [94mLoss[0m : 10.71570
[1mStep[0m  [12/21], [94mLoss[0m : 10.86272
[1mStep[0m  [14/21], [94mLoss[0m : 10.55308
[1mStep[0m  [16/21], [94mLoss[0m : 10.68737
[1mStep[0m  [18/21], [94mLoss[0m : 10.82250
[1mStep[0m  [20/21], [94mLoss[0m : 10.51070

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.691, [92mTest[0m: 10.637, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.46324
[1mStep[0m  [2/21], [94mLoss[0m : 10.46486
[1mStep[0m  [4/21], [94mLoss[0m : 10.57974
[1mStep[0m  [6/21], [94mLoss[0m : 10.58497
[1mStep[0m  [8/21], [94mLoss[0m : 10.61804
[1mStep[0m  [10/21], [94mLoss[0m : 10.85068
[1mStep[0m  [12/21], [94mLoss[0m : 10.62529
[1mStep[0m  [14/21], [94mLoss[0m : 10.58400
[1mStep[0m  [16/21], [94mLoss[0m : 10.32058
[1mStep[0m  [18/21], [94mLoss[0m : 10.46922
[1mStep[0m  [20/21], [94mLoss[0m : 10.60207

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.542, [92mTest[0m: 10.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.64096
[1mStep[0m  [2/21], [94mLoss[0m : 10.22464
[1mStep[0m  [4/21], [94mLoss[0m : 10.27412
[1mStep[0m  [6/21], [94mLoss[0m : 10.45917
[1mStep[0m  [8/21], [94mLoss[0m : 10.26089
[1mStep[0m  [10/21], [94mLoss[0m : 10.16205
[1mStep[0m  [12/21], [94mLoss[0m : 10.44983
[1mStep[0m  [14/21], [94mLoss[0m : 10.47097
[1mStep[0m  [16/21], [94mLoss[0m : 10.45580
[1mStep[0m  [18/21], [94mLoss[0m : 10.37020
[1mStep[0m  [20/21], [94mLoss[0m : 10.27468

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.380, [92mTest[0m: 10.268, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.36817
[1mStep[0m  [2/21], [94mLoss[0m : 10.16044
[1mStep[0m  [4/21], [94mLoss[0m : 10.43312
[1mStep[0m  [6/21], [94mLoss[0m : 10.19006
[1mStep[0m  [8/21], [94mLoss[0m : 10.39846
[1mStep[0m  [10/21], [94mLoss[0m : 10.40842
[1mStep[0m  [12/21], [94mLoss[0m : 10.47301
[1mStep[0m  [14/21], [94mLoss[0m : 10.12125
[1mStep[0m  [16/21], [94mLoss[0m : 9.88905
[1mStep[0m  [18/21], [94mLoss[0m : 10.02583
[1mStep[0m  [20/21], [94mLoss[0m : 10.35854

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.229, [92mTest[0m: 10.053, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.21067
[1mStep[0m  [2/21], [94mLoss[0m : 9.99632
[1mStep[0m  [4/21], [94mLoss[0m : 10.03471
[1mStep[0m  [6/21], [94mLoss[0m : 10.09789
[1mStep[0m  [8/21], [94mLoss[0m : 10.02457
[1mStep[0m  [10/21], [94mLoss[0m : 10.16853
[1mStep[0m  [12/21], [94mLoss[0m : 10.04000
[1mStep[0m  [14/21], [94mLoss[0m : 10.07192
[1mStep[0m  [16/21], [94mLoss[0m : 10.04590
[1mStep[0m  [18/21], [94mLoss[0m : 9.91584
[1mStep[0m  [20/21], [94mLoss[0m : 9.93384

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.057, [92mTest[0m: 9.866, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.93191
[1mStep[0m  [2/21], [94mLoss[0m : 10.02367
[1mStep[0m  [4/21], [94mLoss[0m : 9.84732
[1mStep[0m  [6/21], [94mLoss[0m : 9.98349
[1mStep[0m  [8/21], [94mLoss[0m : 9.90718
[1mStep[0m  [10/21], [94mLoss[0m : 10.20011
[1mStep[0m  [12/21], [94mLoss[0m : 9.73699
[1mStep[0m  [14/21], [94mLoss[0m : 9.80608
[1mStep[0m  [16/21], [94mLoss[0m : 9.85709
[1mStep[0m  [18/21], [94mLoss[0m : 9.75404
[1mStep[0m  [20/21], [94mLoss[0m : 9.63220

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.874, [92mTest[0m: 9.647, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.00303
[1mStep[0m  [2/21], [94mLoss[0m : 9.34095
[1mStep[0m  [4/21], [94mLoss[0m : 9.91102
[1mStep[0m  [6/21], [94mLoss[0m : 9.89026
[1mStep[0m  [8/21], [94mLoss[0m : 9.58870
[1mStep[0m  [10/21], [94mLoss[0m : 9.54210
[1mStep[0m  [12/21], [94mLoss[0m : 9.89137
[1mStep[0m  [14/21], [94mLoss[0m : 9.89951
[1mStep[0m  [16/21], [94mLoss[0m : 9.52541
[1mStep[0m  [18/21], [94mLoss[0m : 9.46115
[1mStep[0m  [20/21], [94mLoss[0m : 9.96374

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.702, [92mTest[0m: 9.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.52181
[1mStep[0m  [2/21], [94mLoss[0m : 9.43884
[1mStep[0m  [4/21], [94mLoss[0m : 9.64044
[1mStep[0m  [6/21], [94mLoss[0m : 9.50144
[1mStep[0m  [8/21], [94mLoss[0m : 9.29163
[1mStep[0m  [10/21], [94mLoss[0m : 9.63626
[1mStep[0m  [12/21], [94mLoss[0m : 9.58069
[1mStep[0m  [14/21], [94mLoss[0m : 9.35388
[1mStep[0m  [16/21], [94mLoss[0m : 9.45088
[1mStep[0m  [18/21], [94mLoss[0m : 9.56239
[1mStep[0m  [20/21], [94mLoss[0m : 9.45929

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.514, [92mTest[0m: 9.261, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.50953
[1mStep[0m  [2/21], [94mLoss[0m : 9.15766
[1mStep[0m  [4/21], [94mLoss[0m : 9.08396
[1mStep[0m  [6/21], [94mLoss[0m : 9.26613
[1mStep[0m  [8/21], [94mLoss[0m : 9.40543
[1mStep[0m  [10/21], [94mLoss[0m : 9.24218
[1mStep[0m  [12/21], [94mLoss[0m : 9.44961
[1mStep[0m  [14/21], [94mLoss[0m : 9.47652
[1mStep[0m  [16/21], [94mLoss[0m : 9.41951
[1mStep[0m  [18/21], [94mLoss[0m : 9.32378
[1mStep[0m  [20/21], [94mLoss[0m : 9.13631

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.310, [92mTest[0m: 9.003, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.03017
[1mStep[0m  [2/21], [94mLoss[0m : 9.30321
[1mStep[0m  [4/21], [94mLoss[0m : 9.23689
[1mStep[0m  [6/21], [94mLoss[0m : 9.30443
[1mStep[0m  [8/21], [94mLoss[0m : 8.96802
[1mStep[0m  [10/21], [94mLoss[0m : 9.15619
[1mStep[0m  [12/21], [94mLoss[0m : 8.96938
[1mStep[0m  [14/21], [94mLoss[0m : 9.14919
[1mStep[0m  [16/21], [94mLoss[0m : 8.99605
[1mStep[0m  [18/21], [94mLoss[0m : 8.88841
[1mStep[0m  [20/21], [94mLoss[0m : 8.91487

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.090, [92mTest[0m: 8.753, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.85867
[1mStep[0m  [2/21], [94mLoss[0m : 8.94304
[1mStep[0m  [4/21], [94mLoss[0m : 8.95337
[1mStep[0m  [6/21], [94mLoss[0m : 8.83803
[1mStep[0m  [8/21], [94mLoss[0m : 8.93107
[1mStep[0m  [10/21], [94mLoss[0m : 8.96268
[1mStep[0m  [12/21], [94mLoss[0m : 8.80176
[1mStep[0m  [14/21], [94mLoss[0m : 8.71768
[1mStep[0m  [16/21], [94mLoss[0m : 8.72971
[1mStep[0m  [18/21], [94mLoss[0m : 8.89797
[1mStep[0m  [20/21], [94mLoss[0m : 8.75536

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.851, [92mTest[0m: 8.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.86887
[1mStep[0m  [2/21], [94mLoss[0m : 8.69771
[1mStep[0m  [4/21], [94mLoss[0m : 8.60165
[1mStep[0m  [6/21], [94mLoss[0m : 8.86218
[1mStep[0m  [8/21], [94mLoss[0m : 8.31473
[1mStep[0m  [10/21], [94mLoss[0m : 8.63332
[1mStep[0m  [12/21], [94mLoss[0m : 8.75105
[1mStep[0m  [14/21], [94mLoss[0m : 8.34377
[1mStep[0m  [16/21], [94mLoss[0m : 8.42882
[1mStep[0m  [18/21], [94mLoss[0m : 8.43116
[1mStep[0m  [20/21], [94mLoss[0m : 8.40001

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.579, [92mTest[0m: 8.185, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.40436
[1mStep[0m  [2/21], [94mLoss[0m : 8.59151
[1mStep[0m  [4/21], [94mLoss[0m : 8.26234
[1mStep[0m  [6/21], [94mLoss[0m : 8.40435
[1mStep[0m  [8/21], [94mLoss[0m : 8.28624
[1mStep[0m  [10/21], [94mLoss[0m : 8.61960
[1mStep[0m  [12/21], [94mLoss[0m : 7.90741
[1mStep[0m  [14/21], [94mLoss[0m : 8.17351
[1mStep[0m  [16/21], [94mLoss[0m : 8.33048
[1mStep[0m  [18/21], [94mLoss[0m : 8.11000
[1mStep[0m  [20/21], [94mLoss[0m : 8.22386

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.300, [92mTest[0m: 7.855, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.15208
[1mStep[0m  [2/21], [94mLoss[0m : 8.13136
[1mStep[0m  [4/21], [94mLoss[0m : 7.87232
[1mStep[0m  [6/21], [94mLoss[0m : 8.08484
[1mStep[0m  [8/21], [94mLoss[0m : 7.82035
[1mStep[0m  [10/21], [94mLoss[0m : 7.91327
[1mStep[0m  [12/21], [94mLoss[0m : 8.12187
[1mStep[0m  [14/21], [94mLoss[0m : 7.74882
[1mStep[0m  [16/21], [94mLoss[0m : 7.78063
[1mStep[0m  [18/21], [94mLoss[0m : 8.16411
[1mStep[0m  [20/21], [94mLoss[0m : 7.64810

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.977, [92mTest[0m: 7.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.80011
[1mStep[0m  [2/21], [94mLoss[0m : 7.82189
[1mStep[0m  [4/21], [94mLoss[0m : 7.90893
[1mStep[0m  [6/21], [94mLoss[0m : 7.76848
[1mStep[0m  [8/21], [94mLoss[0m : 7.72209
[1mStep[0m  [10/21], [94mLoss[0m : 7.70208
[1mStep[0m  [12/21], [94mLoss[0m : 7.48107
[1mStep[0m  [14/21], [94mLoss[0m : 7.85354
[1mStep[0m  [16/21], [94mLoss[0m : 7.40874
[1mStep[0m  [18/21], [94mLoss[0m : 7.29404
[1mStep[0m  [20/21], [94mLoss[0m : 7.34578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.680, [92mTest[0m: 7.234, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.72024
[1mStep[0m  [2/21], [94mLoss[0m : 7.29214
[1mStep[0m  [4/21], [94mLoss[0m : 7.24038
[1mStep[0m  [6/21], [94mLoss[0m : 7.38529
[1mStep[0m  [8/21], [94mLoss[0m : 7.29949
[1mStep[0m  [10/21], [94mLoss[0m : 7.24022
[1mStep[0m  [12/21], [94mLoss[0m : 7.29314
[1mStep[0m  [14/21], [94mLoss[0m : 7.57958
[1mStep[0m  [16/21], [94mLoss[0m : 7.25519
[1mStep[0m  [18/21], [94mLoss[0m : 7.31550
[1mStep[0m  [20/21], [94mLoss[0m : 7.15183

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.361, [92mTest[0m: 6.794, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.89159
[1mStep[0m  [2/21], [94mLoss[0m : 7.15370
[1mStep[0m  [4/21], [94mLoss[0m : 7.31708
[1mStep[0m  [6/21], [94mLoss[0m : 7.13546
[1mStep[0m  [8/21], [94mLoss[0m : 6.96772
[1mStep[0m  [10/21], [94mLoss[0m : 6.95500
[1mStep[0m  [12/21], [94mLoss[0m : 7.28502
[1mStep[0m  [14/21], [94mLoss[0m : 6.99561
[1mStep[0m  [16/21], [94mLoss[0m : 6.96743
[1mStep[0m  [18/21], [94mLoss[0m : 6.84922
[1mStep[0m  [20/21], [94mLoss[0m : 7.06337

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.050, [92mTest[0m: 6.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.11979
[1mStep[0m  [2/21], [94mLoss[0m : 6.81571
[1mStep[0m  [4/21], [94mLoss[0m : 6.80747
[1mStep[0m  [6/21], [94mLoss[0m : 6.74427
[1mStep[0m  [8/21], [94mLoss[0m : 6.76178
[1mStep[0m  [10/21], [94mLoss[0m : 6.63911
[1mStep[0m  [12/21], [94mLoss[0m : 6.43319
[1mStep[0m  [14/21], [94mLoss[0m : 6.55598
[1mStep[0m  [16/21], [94mLoss[0m : 6.57768
[1mStep[0m  [18/21], [94mLoss[0m : 6.60296
[1mStep[0m  [20/21], [94mLoss[0m : 6.53137

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.722, [92mTest[0m: 6.146, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.26078
[1mStep[0m  [2/21], [94mLoss[0m : 6.76517
[1mStep[0m  [4/21], [94mLoss[0m : 6.56700
[1mStep[0m  [6/21], [94mLoss[0m : 6.38030
[1mStep[0m  [8/21], [94mLoss[0m : 6.52122
[1mStep[0m  [10/21], [94mLoss[0m : 6.45052
[1mStep[0m  [12/21], [94mLoss[0m : 6.28548
[1mStep[0m  [14/21], [94mLoss[0m : 6.46637
[1mStep[0m  [16/21], [94mLoss[0m : 6.43906
[1mStep[0m  [18/21], [94mLoss[0m : 6.45484
[1mStep[0m  [20/21], [94mLoss[0m : 6.06993

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.447, [92mTest[0m: 5.800, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.59630
[1mStep[0m  [2/21], [94mLoss[0m : 6.15595
[1mStep[0m  [4/21], [94mLoss[0m : 6.28313
[1mStep[0m  [6/21], [94mLoss[0m : 6.46706
[1mStep[0m  [8/21], [94mLoss[0m : 6.18486
[1mStep[0m  [10/21], [94mLoss[0m : 6.31115
[1mStep[0m  [12/21], [94mLoss[0m : 6.09372
[1mStep[0m  [14/21], [94mLoss[0m : 6.22461
[1mStep[0m  [16/21], [94mLoss[0m : 5.86923
[1mStep[0m  [18/21], [94mLoss[0m : 6.12346
[1mStep[0m  [20/21], [94mLoss[0m : 5.92246

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.207, [92mTest[0m: 5.480, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.08124
[1mStep[0m  [2/21], [94mLoss[0m : 5.56427
[1mStep[0m  [4/21], [94mLoss[0m : 6.11772
[1mStep[0m  [6/21], [94mLoss[0m : 6.12593
[1mStep[0m  [8/21], [94mLoss[0m : 5.73053
[1mStep[0m  [10/21], [94mLoss[0m : 5.94665
[1mStep[0m  [12/21], [94mLoss[0m : 5.89347
[1mStep[0m  [14/21], [94mLoss[0m : 5.97456
[1mStep[0m  [16/21], [94mLoss[0m : 5.94997
[1mStep[0m  [18/21], [94mLoss[0m : 5.95083
[1mStep[0m  [20/21], [94mLoss[0m : 5.70372

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.948, [92mTest[0m: 5.288, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.78447
[1mStep[0m  [2/21], [94mLoss[0m : 5.87153
[1mStep[0m  [4/21], [94mLoss[0m : 5.74843
[1mStep[0m  [6/21], [94mLoss[0m : 5.78543
[1mStep[0m  [8/21], [94mLoss[0m : 5.66312
[1mStep[0m  [10/21], [94mLoss[0m : 5.69991
[1mStep[0m  [12/21], [94mLoss[0m : 5.46181
[1mStep[0m  [14/21], [94mLoss[0m : 5.68122
[1mStep[0m  [16/21], [94mLoss[0m : 5.72396
[1mStep[0m  [18/21], [94mLoss[0m : 5.74330
[1mStep[0m  [20/21], [94mLoss[0m : 5.33072

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.689, [92mTest[0m: 5.033, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.76482
[1mStep[0m  [2/21], [94mLoss[0m : 5.36889
[1mStep[0m  [4/21], [94mLoss[0m : 5.56516
[1mStep[0m  [6/21], [94mLoss[0m : 5.43530
[1mStep[0m  [8/21], [94mLoss[0m : 5.50486
[1mStep[0m  [10/21], [94mLoss[0m : 5.36463
[1mStep[0m  [12/21], [94mLoss[0m : 5.63728
[1mStep[0m  [14/21], [94mLoss[0m : 5.20533
[1mStep[0m  [16/21], [94mLoss[0m : 5.39093
[1mStep[0m  [18/21], [94mLoss[0m : 5.31038
[1mStep[0m  [20/21], [94mLoss[0m : 5.58641

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.457, [92mTest[0m: 4.766, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.36657
[1mStep[0m  [2/21], [94mLoss[0m : 5.29380
[1mStep[0m  [4/21], [94mLoss[0m : 4.91360
[1mStep[0m  [6/21], [94mLoss[0m : 5.38475
[1mStep[0m  [8/21], [94mLoss[0m : 5.34850
[1mStep[0m  [10/21], [94mLoss[0m : 5.33955
[1mStep[0m  [12/21], [94mLoss[0m : 5.47072
[1mStep[0m  [14/21], [94mLoss[0m : 5.12619
[1mStep[0m  [16/21], [94mLoss[0m : 5.00722
[1mStep[0m  [18/21], [94mLoss[0m : 5.06709
[1mStep[0m  [20/21], [94mLoss[0m : 5.16656

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.196, [92mTest[0m: 4.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.04543
[1mStep[0m  [2/21], [94mLoss[0m : 5.18733
[1mStep[0m  [4/21], [94mLoss[0m : 4.91022
[1mStep[0m  [6/21], [94mLoss[0m : 4.88441
[1mStep[0m  [8/21], [94mLoss[0m : 5.30226
[1mStep[0m  [10/21], [94mLoss[0m : 5.01538
[1mStep[0m  [12/21], [94mLoss[0m : 5.12002
[1mStep[0m  [14/21], [94mLoss[0m : 4.77117
[1mStep[0m  [16/21], [94mLoss[0m : 4.80427
[1mStep[0m  [18/21], [94mLoss[0m : 4.80854
[1mStep[0m  [20/21], [94mLoss[0m : 5.02606

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.962, [92mTest[0m: 4.312, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.86635
[1mStep[0m  [2/21], [94mLoss[0m : 4.76485
[1mStep[0m  [4/21], [94mLoss[0m : 4.85721
[1mStep[0m  [6/21], [94mLoss[0m : 4.61397
[1mStep[0m  [8/21], [94mLoss[0m : 4.49694
[1mStep[0m  [10/21], [94mLoss[0m : 4.75551
[1mStep[0m  [12/21], [94mLoss[0m : 4.89627
[1mStep[0m  [14/21], [94mLoss[0m : 4.63150
[1mStep[0m  [16/21], [94mLoss[0m : 4.48702
[1mStep[0m  [18/21], [94mLoss[0m : 4.75877
[1mStep[0m  [20/21], [94mLoss[0m : 4.44454

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.711, [92mTest[0m: 4.034, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.45191
[1mStep[0m  [2/21], [94mLoss[0m : 4.57085
[1mStep[0m  [4/21], [94mLoss[0m : 4.47457
[1mStep[0m  [6/21], [94mLoss[0m : 4.39261
[1mStep[0m  [8/21], [94mLoss[0m : 4.49961
[1mStep[0m  [10/21], [94mLoss[0m : 4.58511
[1mStep[0m  [12/21], [94mLoss[0m : 4.50784
[1mStep[0m  [14/21], [94mLoss[0m : 4.14794
[1mStep[0m  [16/21], [94mLoss[0m : 4.39445
[1mStep[0m  [18/21], [94mLoss[0m : 4.34349
[1mStep[0m  [20/21], [94mLoss[0m : 4.22811

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.461, [92mTest[0m: 3.850, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.29973
[1mStep[0m  [2/21], [94mLoss[0m : 4.46737
[1mStep[0m  [4/21], [94mLoss[0m : 4.19675
[1mStep[0m  [6/21], [94mLoss[0m : 4.33922
[1mStep[0m  [8/21], [94mLoss[0m : 4.47421
[1mStep[0m  [10/21], [94mLoss[0m : 4.18065
[1mStep[0m  [12/21], [94mLoss[0m : 4.08633
[1mStep[0m  [14/21], [94mLoss[0m : 4.16389
[1mStep[0m  [16/21], [94mLoss[0m : 4.24763
[1mStep[0m  [18/21], [94mLoss[0m : 3.99187
[1mStep[0m  [20/21], [94mLoss[0m : 4.19324

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.221, [92mTest[0m: 3.664, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.476
====================================

Phase 1 - Evaluation MAE:  3.4757051127297536
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 4.18186
[1mStep[0m  [2/21], [94mLoss[0m : 4.01944
[1mStep[0m  [4/21], [94mLoss[0m : 4.21618
[1mStep[0m  [6/21], [94mLoss[0m : 4.13566
[1mStep[0m  [8/21], [94mLoss[0m : 4.13736
[1mStep[0m  [10/21], [94mLoss[0m : 3.69201
[1mStep[0m  [12/21], [94mLoss[0m : 4.10815
[1mStep[0m  [14/21], [94mLoss[0m : 4.09526
[1mStep[0m  [16/21], [94mLoss[0m : 3.68880
[1mStep[0m  [18/21], [94mLoss[0m : 3.98113
[1mStep[0m  [20/21], [94mLoss[0m : 4.15059

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.033, [92mTest[0m: 3.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.85578
[1mStep[0m  [2/21], [94mLoss[0m : 3.83837
[1mStep[0m  [4/21], [94mLoss[0m : 3.86303
[1mStep[0m  [6/21], [94mLoss[0m : 3.76111
[1mStep[0m  [8/21], [94mLoss[0m : 3.79304
[1mStep[0m  [10/21], [94mLoss[0m : 3.98058
[1mStep[0m  [12/21], [94mLoss[0m : 3.76916
[1mStep[0m  [14/21], [94mLoss[0m : 3.79208
[1mStep[0m  [16/21], [94mLoss[0m : 3.48379
[1mStep[0m  [18/21], [94mLoss[0m : 3.64149
[1mStep[0m  [20/21], [94mLoss[0m : 3.44358

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.721, [92mTest[0m: 3.718, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.62663
[1mStep[0m  [2/21], [94mLoss[0m : 3.32936
[1mStep[0m  [4/21], [94mLoss[0m : 3.59904
[1mStep[0m  [6/21], [94mLoss[0m : 3.46265
[1mStep[0m  [8/21], [94mLoss[0m : 3.45682
[1mStep[0m  [10/21], [94mLoss[0m : 3.40924
[1mStep[0m  [12/21], [94mLoss[0m : 3.52770
[1mStep[0m  [14/21], [94mLoss[0m : 3.54054
[1mStep[0m  [16/21], [94mLoss[0m : 3.52601
[1mStep[0m  [18/21], [94mLoss[0m : 3.30249
[1mStep[0m  [20/21], [94mLoss[0m : 3.40434

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.480, [92mTest[0m: 3.136, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.55831
[1mStep[0m  [2/21], [94mLoss[0m : 3.26055
[1mStep[0m  [4/21], [94mLoss[0m : 3.11327
[1mStep[0m  [6/21], [94mLoss[0m : 3.44017
[1mStep[0m  [8/21], [94mLoss[0m : 3.40247
[1mStep[0m  [10/21], [94mLoss[0m : 3.43787
[1mStep[0m  [12/21], [94mLoss[0m : 3.43899
[1mStep[0m  [14/21], [94mLoss[0m : 3.19047
[1mStep[0m  [16/21], [94mLoss[0m : 3.26348
[1mStep[0m  [18/21], [94mLoss[0m : 3.19766
[1mStep[0m  [20/21], [94mLoss[0m : 3.05779

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.262, [92mTest[0m: 2.976, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.28306
[1mStep[0m  [2/21], [94mLoss[0m : 3.18878
[1mStep[0m  [4/21], [94mLoss[0m : 3.07099
[1mStep[0m  [6/21], [94mLoss[0m : 3.07788
[1mStep[0m  [8/21], [94mLoss[0m : 3.09057
[1mStep[0m  [10/21], [94mLoss[0m : 3.20396
[1mStep[0m  [12/21], [94mLoss[0m : 3.17278
[1mStep[0m  [14/21], [94mLoss[0m : 2.85291
[1mStep[0m  [16/21], [94mLoss[0m : 2.99080
[1mStep[0m  [18/21], [94mLoss[0m : 3.10618
[1mStep[0m  [20/21], [94mLoss[0m : 2.83765

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.093, [92mTest[0m: 2.717, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.99137
[1mStep[0m  [2/21], [94mLoss[0m : 3.07552
[1mStep[0m  [4/21], [94mLoss[0m : 3.01217
[1mStep[0m  [6/21], [94mLoss[0m : 2.80478
[1mStep[0m  [8/21], [94mLoss[0m : 3.10673
[1mStep[0m  [10/21], [94mLoss[0m : 2.92286
[1mStep[0m  [12/21], [94mLoss[0m : 2.94460
[1mStep[0m  [14/21], [94mLoss[0m : 2.82655
[1mStep[0m  [16/21], [94mLoss[0m : 2.91473
[1mStep[0m  [18/21], [94mLoss[0m : 2.88582
[1mStep[0m  [20/21], [94mLoss[0m : 2.90206

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.934, [92mTest[0m: 2.668, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79295
[1mStep[0m  [2/21], [94mLoss[0m : 2.95379
[1mStep[0m  [4/21], [94mLoss[0m : 2.97896
[1mStep[0m  [6/21], [94mLoss[0m : 2.68009
[1mStep[0m  [8/21], [94mLoss[0m : 2.81626
[1mStep[0m  [10/21], [94mLoss[0m : 2.86684
[1mStep[0m  [12/21], [94mLoss[0m : 2.98663
[1mStep[0m  [14/21], [94mLoss[0m : 2.86735
[1mStep[0m  [16/21], [94mLoss[0m : 2.75075
[1mStep[0m  [18/21], [94mLoss[0m : 2.66455
[1mStep[0m  [20/21], [94mLoss[0m : 2.83834

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.819, [92mTest[0m: 2.598, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77781
[1mStep[0m  [2/21], [94mLoss[0m : 2.65814
[1mStep[0m  [4/21], [94mLoss[0m : 2.53973
[1mStep[0m  [6/21], [94mLoss[0m : 2.65794
[1mStep[0m  [8/21], [94mLoss[0m : 2.78827
[1mStep[0m  [10/21], [94mLoss[0m : 2.72028
[1mStep[0m  [12/21], [94mLoss[0m : 2.82725
[1mStep[0m  [14/21], [94mLoss[0m : 2.72616
[1mStep[0m  [16/21], [94mLoss[0m : 2.78316
[1mStep[0m  [18/21], [94mLoss[0m : 2.67780
[1mStep[0m  [20/21], [94mLoss[0m : 2.76140

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.534, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81044
[1mStep[0m  [2/21], [94mLoss[0m : 2.68651
[1mStep[0m  [4/21], [94mLoss[0m : 2.68473
[1mStep[0m  [6/21], [94mLoss[0m : 2.87238
[1mStep[0m  [8/21], [94mLoss[0m : 2.56826
[1mStep[0m  [10/21], [94mLoss[0m : 2.68350
[1mStep[0m  [12/21], [94mLoss[0m : 2.56725
[1mStep[0m  [14/21], [94mLoss[0m : 2.60392
[1mStep[0m  [16/21], [94mLoss[0m : 2.78725
[1mStep[0m  [18/21], [94mLoss[0m : 2.62798
[1mStep[0m  [20/21], [94mLoss[0m : 2.59145

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70287
[1mStep[0m  [2/21], [94mLoss[0m : 2.70950
[1mStep[0m  [4/21], [94mLoss[0m : 2.72641
[1mStep[0m  [6/21], [94mLoss[0m : 2.50666
[1mStep[0m  [8/21], [94mLoss[0m : 2.56906
[1mStep[0m  [10/21], [94mLoss[0m : 2.61221
[1mStep[0m  [12/21], [94mLoss[0m : 2.64130
[1mStep[0m  [14/21], [94mLoss[0m : 2.48875
[1mStep[0m  [16/21], [94mLoss[0m : 2.54206
[1mStep[0m  [18/21], [94mLoss[0m : 2.63556
[1mStep[0m  [20/21], [94mLoss[0m : 2.67433

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48542
[1mStep[0m  [2/21], [94mLoss[0m : 2.53165
[1mStep[0m  [4/21], [94mLoss[0m : 2.60135
[1mStep[0m  [6/21], [94mLoss[0m : 2.61370
[1mStep[0m  [8/21], [94mLoss[0m : 2.61465
[1mStep[0m  [10/21], [94mLoss[0m : 2.62095
[1mStep[0m  [12/21], [94mLoss[0m : 2.65232
[1mStep[0m  [14/21], [94mLoss[0m : 2.77181
[1mStep[0m  [16/21], [94mLoss[0m : 2.54719
[1mStep[0m  [18/21], [94mLoss[0m : 2.68805
[1mStep[0m  [20/21], [94mLoss[0m : 2.57128

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52700
[1mStep[0m  [2/21], [94mLoss[0m : 2.44805
[1mStep[0m  [4/21], [94mLoss[0m : 2.55326
[1mStep[0m  [6/21], [94mLoss[0m : 2.61341
[1mStep[0m  [8/21], [94mLoss[0m : 2.43246
[1mStep[0m  [10/21], [94mLoss[0m : 2.48650
[1mStep[0m  [12/21], [94mLoss[0m : 2.54946
[1mStep[0m  [14/21], [94mLoss[0m : 2.59146
[1mStep[0m  [16/21], [94mLoss[0m : 2.63667
[1mStep[0m  [18/21], [94mLoss[0m : 2.52898
[1mStep[0m  [20/21], [94mLoss[0m : 2.70917

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55815
[1mStep[0m  [2/21], [94mLoss[0m : 2.56802
[1mStep[0m  [4/21], [94mLoss[0m : 2.67992
[1mStep[0m  [6/21], [94mLoss[0m : 2.46443
[1mStep[0m  [8/21], [94mLoss[0m : 2.53679
[1mStep[0m  [10/21], [94mLoss[0m : 2.43756
[1mStep[0m  [12/21], [94mLoss[0m : 2.54373
[1mStep[0m  [14/21], [94mLoss[0m : 2.56646
[1mStep[0m  [16/21], [94mLoss[0m : 2.75012
[1mStep[0m  [18/21], [94mLoss[0m : 2.66213
[1mStep[0m  [20/21], [94mLoss[0m : 2.72323

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59992
[1mStep[0m  [2/21], [94mLoss[0m : 2.62999
[1mStep[0m  [4/21], [94mLoss[0m : 2.53790
[1mStep[0m  [6/21], [94mLoss[0m : 2.49406
[1mStep[0m  [8/21], [94mLoss[0m : 2.52994
[1mStep[0m  [10/21], [94mLoss[0m : 2.38283
[1mStep[0m  [12/21], [94mLoss[0m : 2.48426
[1mStep[0m  [14/21], [94mLoss[0m : 2.54429
[1mStep[0m  [16/21], [94mLoss[0m : 2.57586
[1mStep[0m  [18/21], [94mLoss[0m : 2.52200
[1mStep[0m  [20/21], [94mLoss[0m : 2.54983

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38316
[1mStep[0m  [2/21], [94mLoss[0m : 2.42678
[1mStep[0m  [4/21], [94mLoss[0m : 2.40992
[1mStep[0m  [6/21], [94mLoss[0m : 2.45230
[1mStep[0m  [8/21], [94mLoss[0m : 2.58635
[1mStep[0m  [10/21], [94mLoss[0m : 2.46969
[1mStep[0m  [12/21], [94mLoss[0m : 2.46206
[1mStep[0m  [14/21], [94mLoss[0m : 2.55948
[1mStep[0m  [16/21], [94mLoss[0m : 2.41747
[1mStep[0m  [18/21], [94mLoss[0m : 2.45040
[1mStep[0m  [20/21], [94mLoss[0m : 2.49843

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.504, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53183
[1mStep[0m  [2/21], [94mLoss[0m : 2.53459
[1mStep[0m  [4/21], [94mLoss[0m : 2.62225
[1mStep[0m  [6/21], [94mLoss[0m : 2.46585
[1mStep[0m  [8/21], [94mLoss[0m : 2.52017
[1mStep[0m  [10/21], [94mLoss[0m : 2.49745
[1mStep[0m  [12/21], [94mLoss[0m : 2.58555
[1mStep[0m  [14/21], [94mLoss[0m : 2.44288
[1mStep[0m  [16/21], [94mLoss[0m : 2.51842
[1mStep[0m  [18/21], [94mLoss[0m : 2.54715
[1mStep[0m  [20/21], [94mLoss[0m : 2.62009

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51037
[1mStep[0m  [2/21], [94mLoss[0m : 2.76540
[1mStep[0m  [4/21], [94mLoss[0m : 2.37616
[1mStep[0m  [6/21], [94mLoss[0m : 2.39796
[1mStep[0m  [8/21], [94mLoss[0m : 2.38653
[1mStep[0m  [10/21], [94mLoss[0m : 2.39125
[1mStep[0m  [12/21], [94mLoss[0m : 2.42606
[1mStep[0m  [14/21], [94mLoss[0m : 2.61948
[1mStep[0m  [16/21], [94mLoss[0m : 2.43316
[1mStep[0m  [18/21], [94mLoss[0m : 2.36762
[1mStep[0m  [20/21], [94mLoss[0m : 2.50045

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36775
[1mStep[0m  [2/21], [94mLoss[0m : 2.37386
[1mStep[0m  [4/21], [94mLoss[0m : 2.38806
[1mStep[0m  [6/21], [94mLoss[0m : 2.32800
[1mStep[0m  [8/21], [94mLoss[0m : 2.44157
[1mStep[0m  [10/21], [94mLoss[0m : 2.50114
[1mStep[0m  [12/21], [94mLoss[0m : 2.44963
[1mStep[0m  [14/21], [94mLoss[0m : 2.47933
[1mStep[0m  [16/21], [94mLoss[0m : 2.48322
[1mStep[0m  [18/21], [94mLoss[0m : 2.36600
[1mStep[0m  [20/21], [94mLoss[0m : 2.43224

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.566, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36623
[1mStep[0m  [2/21], [94mLoss[0m : 2.61314
[1mStep[0m  [4/21], [94mLoss[0m : 2.38686
[1mStep[0m  [6/21], [94mLoss[0m : 2.42057
[1mStep[0m  [8/21], [94mLoss[0m : 2.54679
[1mStep[0m  [10/21], [94mLoss[0m : 2.51211
[1mStep[0m  [12/21], [94mLoss[0m : 2.32065
[1mStep[0m  [14/21], [94mLoss[0m : 2.57065
[1mStep[0m  [16/21], [94mLoss[0m : 2.48078
[1mStep[0m  [18/21], [94mLoss[0m : 2.46205
[1mStep[0m  [20/21], [94mLoss[0m : 2.51613

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.540, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47243
[1mStep[0m  [2/21], [94mLoss[0m : 2.57511
[1mStep[0m  [4/21], [94mLoss[0m : 2.38341
[1mStep[0m  [6/21], [94mLoss[0m : 2.40811
[1mStep[0m  [8/21], [94mLoss[0m : 2.37352
[1mStep[0m  [10/21], [94mLoss[0m : 2.30232
[1mStep[0m  [12/21], [94mLoss[0m : 2.45797
[1mStep[0m  [14/21], [94mLoss[0m : 2.45637
[1mStep[0m  [16/21], [94mLoss[0m : 2.55154
[1mStep[0m  [18/21], [94mLoss[0m : 2.35160
[1mStep[0m  [20/21], [94mLoss[0m : 2.40914

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37309
[1mStep[0m  [2/21], [94mLoss[0m : 2.39726
[1mStep[0m  [4/21], [94mLoss[0m : 2.44797
[1mStep[0m  [6/21], [94mLoss[0m : 2.52203
[1mStep[0m  [8/21], [94mLoss[0m : 2.37137
[1mStep[0m  [10/21], [94mLoss[0m : 2.38332
[1mStep[0m  [12/21], [94mLoss[0m : 2.40537
[1mStep[0m  [14/21], [94mLoss[0m : 2.41044
[1mStep[0m  [16/21], [94mLoss[0m : 2.26371
[1mStep[0m  [18/21], [94mLoss[0m : 2.60848
[1mStep[0m  [20/21], [94mLoss[0m : 2.37225

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41451
[1mStep[0m  [2/21], [94mLoss[0m : 2.35702
[1mStep[0m  [4/21], [94mLoss[0m : 2.44749
[1mStep[0m  [6/21], [94mLoss[0m : 2.50133
[1mStep[0m  [8/21], [94mLoss[0m : 2.39728
[1mStep[0m  [10/21], [94mLoss[0m : 2.43189
[1mStep[0m  [12/21], [94mLoss[0m : 2.42241
[1mStep[0m  [14/21], [94mLoss[0m : 2.16050
[1mStep[0m  [16/21], [94mLoss[0m : 2.35678
[1mStep[0m  [18/21], [94mLoss[0m : 2.42953
[1mStep[0m  [20/21], [94mLoss[0m : 2.51576

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.587, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36385
[1mStep[0m  [2/21], [94mLoss[0m : 2.41734
[1mStep[0m  [4/21], [94mLoss[0m : 2.33376
[1mStep[0m  [6/21], [94mLoss[0m : 2.41283
[1mStep[0m  [8/21], [94mLoss[0m : 2.32709
[1mStep[0m  [10/21], [94mLoss[0m : 2.48668
[1mStep[0m  [12/21], [94mLoss[0m : 2.37397
[1mStep[0m  [14/21], [94mLoss[0m : 2.33705
[1mStep[0m  [16/21], [94mLoss[0m : 2.48334
[1mStep[0m  [18/21], [94mLoss[0m : 2.47306
[1mStep[0m  [20/21], [94mLoss[0m : 2.28242

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.554, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48645
[1mStep[0m  [2/21], [94mLoss[0m : 2.37456
[1mStep[0m  [4/21], [94mLoss[0m : 2.43480
[1mStep[0m  [6/21], [94mLoss[0m : 2.35559
[1mStep[0m  [8/21], [94mLoss[0m : 2.31182
[1mStep[0m  [10/21], [94mLoss[0m : 2.31848
[1mStep[0m  [12/21], [94mLoss[0m : 2.42372
[1mStep[0m  [14/21], [94mLoss[0m : 2.32022
[1mStep[0m  [16/21], [94mLoss[0m : 2.32154
[1mStep[0m  [18/21], [94mLoss[0m : 2.29205
[1mStep[0m  [20/21], [94mLoss[0m : 2.19179

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.568, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34264
[1mStep[0m  [2/21], [94mLoss[0m : 2.28401
[1mStep[0m  [4/21], [94mLoss[0m : 2.33665
[1mStep[0m  [6/21], [94mLoss[0m : 2.49245
[1mStep[0m  [8/21], [94mLoss[0m : 2.31027
[1mStep[0m  [10/21], [94mLoss[0m : 2.44739
[1mStep[0m  [12/21], [94mLoss[0m : 2.42818
[1mStep[0m  [14/21], [94mLoss[0m : 2.32755
[1mStep[0m  [16/21], [94mLoss[0m : 2.40272
[1mStep[0m  [18/21], [94mLoss[0m : 2.56590
[1mStep[0m  [20/21], [94mLoss[0m : 2.41011

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.567, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24552
[1mStep[0m  [2/21], [94mLoss[0m : 2.18270
[1mStep[0m  [4/21], [94mLoss[0m : 2.37622
[1mStep[0m  [6/21], [94mLoss[0m : 2.22222
[1mStep[0m  [8/21], [94mLoss[0m : 2.29983
[1mStep[0m  [10/21], [94mLoss[0m : 2.37969
[1mStep[0m  [12/21], [94mLoss[0m : 2.27876
[1mStep[0m  [14/21], [94mLoss[0m : 2.43574
[1mStep[0m  [16/21], [94mLoss[0m : 2.22144
[1mStep[0m  [18/21], [94mLoss[0m : 2.26011
[1mStep[0m  [20/21], [94mLoss[0m : 2.38645

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.521, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33143
[1mStep[0m  [2/21], [94mLoss[0m : 2.27777
[1mStep[0m  [4/21], [94mLoss[0m : 2.24598
[1mStep[0m  [6/21], [94mLoss[0m : 2.15701
[1mStep[0m  [8/21], [94mLoss[0m : 2.49267
[1mStep[0m  [10/21], [94mLoss[0m : 2.41365
[1mStep[0m  [12/21], [94mLoss[0m : 2.42034
[1mStep[0m  [14/21], [94mLoss[0m : 2.28881
[1mStep[0m  [16/21], [94mLoss[0m : 2.25758
[1mStep[0m  [18/21], [94mLoss[0m : 2.24262
[1mStep[0m  [20/21], [94mLoss[0m : 2.41809

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.557, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17244
[1mStep[0m  [2/21], [94mLoss[0m : 2.35788
[1mStep[0m  [4/21], [94mLoss[0m : 2.25789
[1mStep[0m  [6/21], [94mLoss[0m : 2.24982
[1mStep[0m  [8/21], [94mLoss[0m : 2.28503
[1mStep[0m  [10/21], [94mLoss[0m : 2.27148
[1mStep[0m  [12/21], [94mLoss[0m : 2.27927
[1mStep[0m  [14/21], [94mLoss[0m : 2.29492
[1mStep[0m  [16/21], [94mLoss[0m : 2.43385
[1mStep[0m  [18/21], [94mLoss[0m : 2.34766
[1mStep[0m  [20/21], [94mLoss[0m : 2.37616

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.549, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29551
[1mStep[0m  [2/21], [94mLoss[0m : 2.46898
[1mStep[0m  [4/21], [94mLoss[0m : 2.32389
[1mStep[0m  [6/21], [94mLoss[0m : 2.36341
[1mStep[0m  [8/21], [94mLoss[0m : 2.16459
[1mStep[0m  [10/21], [94mLoss[0m : 2.29328
[1mStep[0m  [12/21], [94mLoss[0m : 2.31343
[1mStep[0m  [14/21], [94mLoss[0m : 2.35930
[1mStep[0m  [16/21], [94mLoss[0m : 2.12853
[1mStep[0m  [18/21], [94mLoss[0m : 2.26577
[1mStep[0m  [20/21], [94mLoss[0m : 2.23758

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.532, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36950
[1mStep[0m  [2/21], [94mLoss[0m : 2.28659
[1mStep[0m  [4/21], [94mLoss[0m : 2.46210
[1mStep[0m  [6/21], [94mLoss[0m : 2.30396
[1mStep[0m  [8/21], [94mLoss[0m : 2.45435
[1mStep[0m  [10/21], [94mLoss[0m : 2.18826
[1mStep[0m  [12/21], [94mLoss[0m : 2.31802
[1mStep[0m  [14/21], [94mLoss[0m : 2.30414
[1mStep[0m  [16/21], [94mLoss[0m : 2.28927
[1mStep[0m  [18/21], [94mLoss[0m : 2.29691
[1mStep[0m  [20/21], [94mLoss[0m : 2.28265

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.521
====================================

Phase 2 - Evaluation MAE:  2.5208512374332974
MAE score P1       3.475705
MAE score P2       2.520851
loss               2.284999
learning_rate       0.00505
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 10.70091
[1mStep[0m  [33/339], [94mLoss[0m : 10.16513
[1mStep[0m  [66/339], [94mLoss[0m : 8.97784
[1mStep[0m  [99/339], [94mLoss[0m : 8.32389
[1mStep[0m  [132/339], [94mLoss[0m : 8.30729
[1mStep[0m  [165/339], [94mLoss[0m : 7.20227
[1mStep[0m  [198/339], [94mLoss[0m : 4.82056
[1mStep[0m  [231/339], [94mLoss[0m : 4.10730
[1mStep[0m  [264/339], [94mLoss[0m : 2.05160
[1mStep[0m  [297/339], [94mLoss[0m : 3.36848
[1mStep[0m  [330/339], [94mLoss[0m : 3.45545

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.269, [92mTest[0m: 10.876, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80569
[1mStep[0m  [33/339], [94mLoss[0m : 2.39199
[1mStep[0m  [66/339], [94mLoss[0m : 2.32263
[1mStep[0m  [99/339], [94mLoss[0m : 3.03610
[1mStep[0m  [132/339], [94mLoss[0m : 2.32781
[1mStep[0m  [165/339], [94mLoss[0m : 2.97240
[1mStep[0m  [198/339], [94mLoss[0m : 3.14492
[1mStep[0m  [231/339], [94mLoss[0m : 2.04448
[1mStep[0m  [264/339], [94mLoss[0m : 3.17390
[1mStep[0m  [297/339], [94mLoss[0m : 2.22929
[1mStep[0m  [330/339], [94mLoss[0m : 2.56456

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.860, [92mTest[0m: 2.628, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95243
[1mStep[0m  [33/339], [94mLoss[0m : 3.29928
[1mStep[0m  [66/339], [94mLoss[0m : 2.74899
[1mStep[0m  [99/339], [94mLoss[0m : 2.72249
[1mStep[0m  [132/339], [94mLoss[0m : 2.13841
[1mStep[0m  [165/339], [94mLoss[0m : 2.85072
[1mStep[0m  [198/339], [94mLoss[0m : 3.02756
[1mStep[0m  [231/339], [94mLoss[0m : 3.04993
[1mStep[0m  [264/339], [94mLoss[0m : 2.21430
[1mStep[0m  [297/339], [94mLoss[0m : 1.82267
[1mStep[0m  [330/339], [94mLoss[0m : 2.63929

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80158
[1mStep[0m  [33/339], [94mLoss[0m : 3.22486
[1mStep[0m  [66/339], [94mLoss[0m : 2.98039
[1mStep[0m  [99/339], [94mLoss[0m : 2.56439
[1mStep[0m  [132/339], [94mLoss[0m : 2.45149
[1mStep[0m  [165/339], [94mLoss[0m : 2.71247
[1mStep[0m  [198/339], [94mLoss[0m : 2.95027
[1mStep[0m  [231/339], [94mLoss[0m : 2.64832
[1mStep[0m  [264/339], [94mLoss[0m : 2.77684
[1mStep[0m  [297/339], [94mLoss[0m : 1.72056
[1mStep[0m  [330/339], [94mLoss[0m : 2.76271

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02817
[1mStep[0m  [33/339], [94mLoss[0m : 2.95094
[1mStep[0m  [66/339], [94mLoss[0m : 2.91452
[1mStep[0m  [99/339], [94mLoss[0m : 2.35082
[1mStep[0m  [132/339], [94mLoss[0m : 2.80052
[1mStep[0m  [165/339], [94mLoss[0m : 3.01241
[1mStep[0m  [198/339], [94mLoss[0m : 2.32299
[1mStep[0m  [231/339], [94mLoss[0m : 2.09913
[1mStep[0m  [264/339], [94mLoss[0m : 2.41935
[1mStep[0m  [297/339], [94mLoss[0m : 2.16296
[1mStep[0m  [330/339], [94mLoss[0m : 2.57468

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16779
[1mStep[0m  [33/339], [94mLoss[0m : 2.39355
[1mStep[0m  [66/339], [94mLoss[0m : 2.11104
[1mStep[0m  [99/339], [94mLoss[0m : 3.01799
[1mStep[0m  [132/339], [94mLoss[0m : 2.69142
[1mStep[0m  [165/339], [94mLoss[0m : 2.21096
[1mStep[0m  [198/339], [94mLoss[0m : 2.64462
[1mStep[0m  [231/339], [94mLoss[0m : 2.73278
[1mStep[0m  [264/339], [94mLoss[0m : 2.24293
[1mStep[0m  [297/339], [94mLoss[0m : 2.38665
[1mStep[0m  [330/339], [94mLoss[0m : 2.71611

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77749
[1mStep[0m  [33/339], [94mLoss[0m : 1.86801
[1mStep[0m  [66/339], [94mLoss[0m : 2.65382
[1mStep[0m  [99/339], [94mLoss[0m : 2.83206
[1mStep[0m  [132/339], [94mLoss[0m : 2.54155
[1mStep[0m  [165/339], [94mLoss[0m : 2.71710
[1mStep[0m  [198/339], [94mLoss[0m : 2.32351
[1mStep[0m  [231/339], [94mLoss[0m : 2.42184
[1mStep[0m  [264/339], [94mLoss[0m : 2.56052
[1mStep[0m  [297/339], [94mLoss[0m : 2.38876
[1mStep[0m  [330/339], [94mLoss[0m : 2.54867

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30649
[1mStep[0m  [33/339], [94mLoss[0m : 2.08508
[1mStep[0m  [66/339], [94mLoss[0m : 2.51065
[1mStep[0m  [99/339], [94mLoss[0m : 2.57996
[1mStep[0m  [132/339], [94mLoss[0m : 2.95096
[1mStep[0m  [165/339], [94mLoss[0m : 3.37786
[1mStep[0m  [198/339], [94mLoss[0m : 2.12318
[1mStep[0m  [231/339], [94mLoss[0m : 2.65601
[1mStep[0m  [264/339], [94mLoss[0m : 1.64398
[1mStep[0m  [297/339], [94mLoss[0m : 2.10917
[1mStep[0m  [330/339], [94mLoss[0m : 2.88637

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73666
[1mStep[0m  [33/339], [94mLoss[0m : 2.88087
[1mStep[0m  [66/339], [94mLoss[0m : 2.70968
[1mStep[0m  [99/339], [94mLoss[0m : 2.38376
[1mStep[0m  [132/339], [94mLoss[0m : 2.45177
[1mStep[0m  [165/339], [94mLoss[0m : 2.50845
[1mStep[0m  [198/339], [94mLoss[0m : 2.13150
[1mStep[0m  [231/339], [94mLoss[0m : 2.95415
[1mStep[0m  [264/339], [94mLoss[0m : 2.68465
[1mStep[0m  [297/339], [94mLoss[0m : 2.51753
[1mStep[0m  [330/339], [94mLoss[0m : 2.92724

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50575
[1mStep[0m  [33/339], [94mLoss[0m : 2.25068
[1mStep[0m  [66/339], [94mLoss[0m : 2.70129
[1mStep[0m  [99/339], [94mLoss[0m : 2.18321
[1mStep[0m  [132/339], [94mLoss[0m : 2.80017
[1mStep[0m  [165/339], [94mLoss[0m : 2.70514
[1mStep[0m  [198/339], [94mLoss[0m : 2.52737
[1mStep[0m  [231/339], [94mLoss[0m : 2.43336
[1mStep[0m  [264/339], [94mLoss[0m : 2.08773
[1mStep[0m  [297/339], [94mLoss[0m : 1.76490
[1mStep[0m  [330/339], [94mLoss[0m : 2.24811

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40523
[1mStep[0m  [33/339], [94mLoss[0m : 1.94521
[1mStep[0m  [66/339], [94mLoss[0m : 2.69531
[1mStep[0m  [99/339], [94mLoss[0m : 2.54864
[1mStep[0m  [132/339], [94mLoss[0m : 2.29804
[1mStep[0m  [165/339], [94mLoss[0m : 2.39318
[1mStep[0m  [198/339], [94mLoss[0m : 2.46958
[1mStep[0m  [231/339], [94mLoss[0m : 2.08980
[1mStep[0m  [264/339], [94mLoss[0m : 2.27959
[1mStep[0m  [297/339], [94mLoss[0m : 2.91047
[1mStep[0m  [330/339], [94mLoss[0m : 2.61015

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87599
[1mStep[0m  [33/339], [94mLoss[0m : 1.83571
[1mStep[0m  [66/339], [94mLoss[0m : 2.16586
[1mStep[0m  [99/339], [94mLoss[0m : 2.64417
[1mStep[0m  [132/339], [94mLoss[0m : 2.42965
[1mStep[0m  [165/339], [94mLoss[0m : 2.67845
[1mStep[0m  [198/339], [94mLoss[0m : 2.10855
[1mStep[0m  [231/339], [94mLoss[0m : 2.50531
[1mStep[0m  [264/339], [94mLoss[0m : 2.99358
[1mStep[0m  [297/339], [94mLoss[0m : 2.92051
[1mStep[0m  [330/339], [94mLoss[0m : 2.33763

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66033
[1mStep[0m  [33/339], [94mLoss[0m : 2.69116
[1mStep[0m  [66/339], [94mLoss[0m : 2.62954
[1mStep[0m  [99/339], [94mLoss[0m : 2.46554
[1mStep[0m  [132/339], [94mLoss[0m : 2.57416
[1mStep[0m  [165/339], [94mLoss[0m : 2.15895
[1mStep[0m  [198/339], [94mLoss[0m : 2.93555
[1mStep[0m  [231/339], [94mLoss[0m : 2.57090
[1mStep[0m  [264/339], [94mLoss[0m : 2.73361
[1mStep[0m  [297/339], [94mLoss[0m : 2.36543
[1mStep[0m  [330/339], [94mLoss[0m : 2.81980

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56973
[1mStep[0m  [33/339], [94mLoss[0m : 1.68391
[1mStep[0m  [66/339], [94mLoss[0m : 1.91373
[1mStep[0m  [99/339], [94mLoss[0m : 2.34319
[1mStep[0m  [132/339], [94mLoss[0m : 2.74770
[1mStep[0m  [165/339], [94mLoss[0m : 2.61725
[1mStep[0m  [198/339], [94mLoss[0m : 2.40781
[1mStep[0m  [231/339], [94mLoss[0m : 2.55626
[1mStep[0m  [264/339], [94mLoss[0m : 2.29701
[1mStep[0m  [297/339], [94mLoss[0m : 2.37351
[1mStep[0m  [330/339], [94mLoss[0m : 2.40890

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44597
[1mStep[0m  [33/339], [94mLoss[0m : 2.22225
[1mStep[0m  [66/339], [94mLoss[0m : 2.48675
[1mStep[0m  [99/339], [94mLoss[0m : 2.36249
[1mStep[0m  [132/339], [94mLoss[0m : 2.54048
[1mStep[0m  [165/339], [94mLoss[0m : 1.88201
[1mStep[0m  [198/339], [94mLoss[0m : 3.26194
[1mStep[0m  [231/339], [94mLoss[0m : 2.70780
[1mStep[0m  [264/339], [94mLoss[0m : 2.74332
[1mStep[0m  [297/339], [94mLoss[0m : 1.94356
[1mStep[0m  [330/339], [94mLoss[0m : 2.57211

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85953
[1mStep[0m  [33/339], [94mLoss[0m : 2.27887
[1mStep[0m  [66/339], [94mLoss[0m : 1.86596
[1mStep[0m  [99/339], [94mLoss[0m : 2.17314
[1mStep[0m  [132/339], [94mLoss[0m : 2.47738
[1mStep[0m  [165/339], [94mLoss[0m : 2.54801
[1mStep[0m  [198/339], [94mLoss[0m : 1.87114
[1mStep[0m  [231/339], [94mLoss[0m : 1.67292
[1mStep[0m  [264/339], [94mLoss[0m : 2.18348
[1mStep[0m  [297/339], [94mLoss[0m : 2.66608
[1mStep[0m  [330/339], [94mLoss[0m : 2.67877

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24391
[1mStep[0m  [33/339], [94mLoss[0m : 2.52180
[1mStep[0m  [66/339], [94mLoss[0m : 1.86098
[1mStep[0m  [99/339], [94mLoss[0m : 3.10044
[1mStep[0m  [132/339], [94mLoss[0m : 2.03243
[1mStep[0m  [165/339], [94mLoss[0m : 2.23343
[1mStep[0m  [198/339], [94mLoss[0m : 2.38809
[1mStep[0m  [231/339], [94mLoss[0m : 2.18317
[1mStep[0m  [264/339], [94mLoss[0m : 2.30862
[1mStep[0m  [297/339], [94mLoss[0m : 2.54394
[1mStep[0m  [330/339], [94mLoss[0m : 2.70736

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79514
[1mStep[0m  [33/339], [94mLoss[0m : 2.04799
[1mStep[0m  [66/339], [94mLoss[0m : 2.77011
[1mStep[0m  [99/339], [94mLoss[0m : 2.33103
[1mStep[0m  [132/339], [94mLoss[0m : 2.82511
[1mStep[0m  [165/339], [94mLoss[0m : 3.06576
[1mStep[0m  [198/339], [94mLoss[0m : 2.12538
[1mStep[0m  [231/339], [94mLoss[0m : 2.73513
[1mStep[0m  [264/339], [94mLoss[0m : 2.37549
[1mStep[0m  [297/339], [94mLoss[0m : 2.63358
[1mStep[0m  [330/339], [94mLoss[0m : 2.24002

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56506
[1mStep[0m  [33/339], [94mLoss[0m : 2.23472
[1mStep[0m  [66/339], [94mLoss[0m : 2.45795
[1mStep[0m  [99/339], [94mLoss[0m : 2.22046
[1mStep[0m  [132/339], [94mLoss[0m : 2.51234
[1mStep[0m  [165/339], [94mLoss[0m : 2.57028
[1mStep[0m  [198/339], [94mLoss[0m : 2.16144
[1mStep[0m  [231/339], [94mLoss[0m : 2.01142
[1mStep[0m  [264/339], [94mLoss[0m : 2.80211
[1mStep[0m  [297/339], [94mLoss[0m : 2.20970
[1mStep[0m  [330/339], [94mLoss[0m : 2.42694

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02680
[1mStep[0m  [33/339], [94mLoss[0m : 2.09181
[1mStep[0m  [66/339], [94mLoss[0m : 2.44503
[1mStep[0m  [99/339], [94mLoss[0m : 2.48445
[1mStep[0m  [132/339], [94mLoss[0m : 1.98344
[1mStep[0m  [165/339], [94mLoss[0m : 2.79458
[1mStep[0m  [198/339], [94mLoss[0m : 3.09157
[1mStep[0m  [231/339], [94mLoss[0m : 2.59141
[1mStep[0m  [264/339], [94mLoss[0m : 2.34439
[1mStep[0m  [297/339], [94mLoss[0m : 2.38649
[1mStep[0m  [330/339], [94mLoss[0m : 2.61887

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75850
[1mStep[0m  [33/339], [94mLoss[0m : 2.42692
[1mStep[0m  [66/339], [94mLoss[0m : 2.24848
[1mStep[0m  [99/339], [94mLoss[0m : 2.14895
[1mStep[0m  [132/339], [94mLoss[0m : 1.84106
[1mStep[0m  [165/339], [94mLoss[0m : 2.51831
[1mStep[0m  [198/339], [94mLoss[0m : 2.42099
[1mStep[0m  [231/339], [94mLoss[0m : 2.73888
[1mStep[0m  [264/339], [94mLoss[0m : 2.46180
[1mStep[0m  [297/339], [94mLoss[0m : 2.53812
[1mStep[0m  [330/339], [94mLoss[0m : 2.24701

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22239
[1mStep[0m  [33/339], [94mLoss[0m : 2.46940
[1mStep[0m  [66/339], [94mLoss[0m : 2.37726
[1mStep[0m  [99/339], [94mLoss[0m : 2.41482
[1mStep[0m  [132/339], [94mLoss[0m : 2.79571
[1mStep[0m  [165/339], [94mLoss[0m : 2.04002
[1mStep[0m  [198/339], [94mLoss[0m : 2.06632
[1mStep[0m  [231/339], [94mLoss[0m : 2.70910
[1mStep[0m  [264/339], [94mLoss[0m : 2.62391
[1mStep[0m  [297/339], [94mLoss[0m : 2.69126
[1mStep[0m  [330/339], [94mLoss[0m : 2.26989

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.86931
[1mStep[0m  [33/339], [94mLoss[0m : 2.13678
[1mStep[0m  [66/339], [94mLoss[0m : 2.00024
[1mStep[0m  [99/339], [94mLoss[0m : 2.67998
[1mStep[0m  [132/339], [94mLoss[0m : 2.67593
[1mStep[0m  [165/339], [94mLoss[0m : 2.53633
[1mStep[0m  [198/339], [94mLoss[0m : 2.49734
[1mStep[0m  [231/339], [94mLoss[0m : 2.95739
[1mStep[0m  [264/339], [94mLoss[0m : 1.99128
[1mStep[0m  [297/339], [94mLoss[0m : 2.04187
[1mStep[0m  [330/339], [94mLoss[0m : 2.19897

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06811
[1mStep[0m  [33/339], [94mLoss[0m : 2.57476
[1mStep[0m  [66/339], [94mLoss[0m : 2.77283
[1mStep[0m  [99/339], [94mLoss[0m : 2.53617
[1mStep[0m  [132/339], [94mLoss[0m : 2.24525
[1mStep[0m  [165/339], [94mLoss[0m : 2.24845
[1mStep[0m  [198/339], [94mLoss[0m : 2.08870
[1mStep[0m  [231/339], [94mLoss[0m : 2.39007
[1mStep[0m  [264/339], [94mLoss[0m : 2.24459
[1mStep[0m  [297/339], [94mLoss[0m : 2.18658
[1mStep[0m  [330/339], [94mLoss[0m : 2.41007

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91729
[1mStep[0m  [33/339], [94mLoss[0m : 2.04541
[1mStep[0m  [66/339], [94mLoss[0m : 1.92776
[1mStep[0m  [99/339], [94mLoss[0m : 2.39425
[1mStep[0m  [132/339], [94mLoss[0m : 2.85955
[1mStep[0m  [165/339], [94mLoss[0m : 2.00467
[1mStep[0m  [198/339], [94mLoss[0m : 1.75861
[1mStep[0m  [231/339], [94mLoss[0m : 1.94332
[1mStep[0m  [264/339], [94mLoss[0m : 2.52655
[1mStep[0m  [297/339], [94mLoss[0m : 2.20167
[1mStep[0m  [330/339], [94mLoss[0m : 2.38872

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53923
[1mStep[0m  [33/339], [94mLoss[0m : 2.78625
[1mStep[0m  [66/339], [94mLoss[0m : 2.44360
[1mStep[0m  [99/339], [94mLoss[0m : 3.02842
[1mStep[0m  [132/339], [94mLoss[0m : 2.44637
[1mStep[0m  [165/339], [94mLoss[0m : 1.95089
[1mStep[0m  [198/339], [94mLoss[0m : 2.75208
[1mStep[0m  [231/339], [94mLoss[0m : 2.12187
[1mStep[0m  [264/339], [94mLoss[0m : 2.35368
[1mStep[0m  [297/339], [94mLoss[0m : 2.91401
[1mStep[0m  [330/339], [94mLoss[0m : 2.19547

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57070
[1mStep[0m  [33/339], [94mLoss[0m : 2.67171
[1mStep[0m  [66/339], [94mLoss[0m : 2.22462
[1mStep[0m  [99/339], [94mLoss[0m : 2.39112
[1mStep[0m  [132/339], [94mLoss[0m : 1.99342
[1mStep[0m  [165/339], [94mLoss[0m : 2.63487
[1mStep[0m  [198/339], [94mLoss[0m : 2.08201
[1mStep[0m  [231/339], [94mLoss[0m : 2.22002
[1mStep[0m  [264/339], [94mLoss[0m : 2.95117
[1mStep[0m  [297/339], [94mLoss[0m : 1.49611
[1mStep[0m  [330/339], [94mLoss[0m : 2.43016

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91724
[1mStep[0m  [33/339], [94mLoss[0m : 2.28083
[1mStep[0m  [66/339], [94mLoss[0m : 2.08776
[1mStep[0m  [99/339], [94mLoss[0m : 2.21003
[1mStep[0m  [132/339], [94mLoss[0m : 2.21659
[1mStep[0m  [165/339], [94mLoss[0m : 1.79656
[1mStep[0m  [198/339], [94mLoss[0m : 2.78006
[1mStep[0m  [231/339], [94mLoss[0m : 2.29882
[1mStep[0m  [264/339], [94mLoss[0m : 2.19614
[1mStep[0m  [297/339], [94mLoss[0m : 2.37595
[1mStep[0m  [330/339], [94mLoss[0m : 2.13675

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37149
[1mStep[0m  [33/339], [94mLoss[0m : 2.24429
[1mStep[0m  [66/339], [94mLoss[0m : 2.12144
[1mStep[0m  [99/339], [94mLoss[0m : 1.98579
[1mStep[0m  [132/339], [94mLoss[0m : 2.31978
[1mStep[0m  [165/339], [94mLoss[0m : 1.82221
[1mStep[0m  [198/339], [94mLoss[0m : 2.05927
[1mStep[0m  [231/339], [94mLoss[0m : 2.14001
[1mStep[0m  [264/339], [94mLoss[0m : 2.50425
[1mStep[0m  [297/339], [94mLoss[0m : 2.28065
[1mStep[0m  [330/339], [94mLoss[0m : 2.20394

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14901
[1mStep[0m  [33/339], [94mLoss[0m : 2.12040
[1mStep[0m  [66/339], [94mLoss[0m : 2.53759
[1mStep[0m  [99/339], [94mLoss[0m : 2.50120
[1mStep[0m  [132/339], [94mLoss[0m : 2.03426
[1mStep[0m  [165/339], [94mLoss[0m : 2.83466
[1mStep[0m  [198/339], [94mLoss[0m : 2.45003
[1mStep[0m  [231/339], [94mLoss[0m : 2.30437
[1mStep[0m  [264/339], [94mLoss[0m : 2.36494
[1mStep[0m  [297/339], [94mLoss[0m : 2.28701
[1mStep[0m  [330/339], [94mLoss[0m : 2.49734

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.365
====================================

Phase 1 - Evaluation MAE:  2.364701935675292
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.56275
[1mStep[0m  [33/339], [94mLoss[0m : 3.02567
[1mStep[0m  [66/339], [94mLoss[0m : 2.64870
[1mStep[0m  [99/339], [94mLoss[0m : 2.37682
[1mStep[0m  [132/339], [94mLoss[0m : 2.51102
[1mStep[0m  [165/339], [94mLoss[0m : 2.18910
[1mStep[0m  [198/339], [94mLoss[0m : 2.27425
[1mStep[0m  [231/339], [94mLoss[0m : 2.31532
[1mStep[0m  [264/339], [94mLoss[0m : 1.97576
[1mStep[0m  [297/339], [94mLoss[0m : 2.69171
[1mStep[0m  [330/339], [94mLoss[0m : 2.38625

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.79671
[1mStep[0m  [33/339], [94mLoss[0m : 2.90022
[1mStep[0m  [66/339], [94mLoss[0m : 2.38989
[1mStep[0m  [99/339], [94mLoss[0m : 2.65935
[1mStep[0m  [132/339], [94mLoss[0m : 2.47847
[1mStep[0m  [165/339], [94mLoss[0m : 2.89933
[1mStep[0m  [198/339], [94mLoss[0m : 2.62794
[1mStep[0m  [231/339], [94mLoss[0m : 2.60653
[1mStep[0m  [264/339], [94mLoss[0m : 3.19646
[1mStep[0m  [297/339], [94mLoss[0m : 2.24784
[1mStep[0m  [330/339], [94mLoss[0m : 2.27576

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37290
[1mStep[0m  [33/339], [94mLoss[0m : 2.36786
[1mStep[0m  [66/339], [94mLoss[0m : 2.12220
[1mStep[0m  [99/339], [94mLoss[0m : 2.51801
[1mStep[0m  [132/339], [94mLoss[0m : 2.33523
[1mStep[0m  [165/339], [94mLoss[0m : 2.20253
[1mStep[0m  [198/339], [94mLoss[0m : 2.41461
[1mStep[0m  [231/339], [94mLoss[0m : 2.10662
[1mStep[0m  [264/339], [94mLoss[0m : 2.06640
[1mStep[0m  [297/339], [94mLoss[0m : 2.37530
[1mStep[0m  [330/339], [94mLoss[0m : 2.80159

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30319
[1mStep[0m  [33/339], [94mLoss[0m : 2.27548
[1mStep[0m  [66/339], [94mLoss[0m : 2.26534
[1mStep[0m  [99/339], [94mLoss[0m : 1.99613
[1mStep[0m  [132/339], [94mLoss[0m : 2.53768
[1mStep[0m  [165/339], [94mLoss[0m : 2.08270
[1mStep[0m  [198/339], [94mLoss[0m : 2.42023
[1mStep[0m  [231/339], [94mLoss[0m : 2.58793
[1mStep[0m  [264/339], [94mLoss[0m : 2.33061
[1mStep[0m  [297/339], [94mLoss[0m : 2.46155
[1mStep[0m  [330/339], [94mLoss[0m : 2.29779

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59145
[1mStep[0m  [33/339], [94mLoss[0m : 2.12372
[1mStep[0m  [66/339], [94mLoss[0m : 2.87816
[1mStep[0m  [99/339], [94mLoss[0m : 1.99851
[1mStep[0m  [132/339], [94mLoss[0m : 2.59516
[1mStep[0m  [165/339], [94mLoss[0m : 2.43494
[1mStep[0m  [198/339], [94mLoss[0m : 2.50108
[1mStep[0m  [231/339], [94mLoss[0m : 1.84140
[1mStep[0m  [264/339], [94mLoss[0m : 2.34834
[1mStep[0m  [297/339], [94mLoss[0m : 2.15111
[1mStep[0m  [330/339], [94mLoss[0m : 2.28245

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69236
[1mStep[0m  [33/339], [94mLoss[0m : 2.47191
[1mStep[0m  [66/339], [94mLoss[0m : 1.40940
[1mStep[0m  [99/339], [94mLoss[0m : 1.93389
[1mStep[0m  [132/339], [94mLoss[0m : 1.54723
[1mStep[0m  [165/339], [94mLoss[0m : 2.04530
[1mStep[0m  [198/339], [94mLoss[0m : 3.61445
[1mStep[0m  [231/339], [94mLoss[0m : 2.08089
[1mStep[0m  [264/339], [94mLoss[0m : 2.15939
[1mStep[0m  [297/339], [94mLoss[0m : 2.76234
[1mStep[0m  [330/339], [94mLoss[0m : 1.89199

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28427
[1mStep[0m  [33/339], [94mLoss[0m : 2.30372
[1mStep[0m  [66/339], [94mLoss[0m : 2.31542
[1mStep[0m  [99/339], [94mLoss[0m : 1.84164
[1mStep[0m  [132/339], [94mLoss[0m : 2.47815
[1mStep[0m  [165/339], [94mLoss[0m : 1.96624
[1mStep[0m  [198/339], [94mLoss[0m : 2.06436
[1mStep[0m  [231/339], [94mLoss[0m : 2.29845
[1mStep[0m  [264/339], [94mLoss[0m : 2.10689
[1mStep[0m  [297/339], [94mLoss[0m : 2.21478
[1mStep[0m  [330/339], [94mLoss[0m : 2.43678

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.50882
[1mStep[0m  [33/339], [94mLoss[0m : 2.42152
[1mStep[0m  [66/339], [94mLoss[0m : 1.68989
[1mStep[0m  [99/339], [94mLoss[0m : 1.90323
[1mStep[0m  [132/339], [94mLoss[0m : 2.67603
[1mStep[0m  [165/339], [94mLoss[0m : 1.99758
[1mStep[0m  [198/339], [94mLoss[0m : 1.77329
[1mStep[0m  [231/339], [94mLoss[0m : 2.16752
[1mStep[0m  [264/339], [94mLoss[0m : 2.24741
[1mStep[0m  [297/339], [94mLoss[0m : 2.25404
[1mStep[0m  [330/339], [94mLoss[0m : 2.06134

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73150
[1mStep[0m  [33/339], [94mLoss[0m : 1.97569
[1mStep[0m  [66/339], [94mLoss[0m : 1.84337
[1mStep[0m  [99/339], [94mLoss[0m : 1.97705
[1mStep[0m  [132/339], [94mLoss[0m : 1.71522
[1mStep[0m  [165/339], [94mLoss[0m : 2.32327
[1mStep[0m  [198/339], [94mLoss[0m : 1.69798
[1mStep[0m  [231/339], [94mLoss[0m : 1.95462
[1mStep[0m  [264/339], [94mLoss[0m : 2.09313
[1mStep[0m  [297/339], [94mLoss[0m : 2.34890
[1mStep[0m  [330/339], [94mLoss[0m : 2.50116

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.490, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63872
[1mStep[0m  [33/339], [94mLoss[0m : 1.90347
[1mStep[0m  [66/339], [94mLoss[0m : 1.67492
[1mStep[0m  [99/339], [94mLoss[0m : 1.46383
[1mStep[0m  [132/339], [94mLoss[0m : 1.41802
[1mStep[0m  [165/339], [94mLoss[0m : 1.99004
[1mStep[0m  [198/339], [94mLoss[0m : 2.42337
[1mStep[0m  [231/339], [94mLoss[0m : 2.47172
[1mStep[0m  [264/339], [94mLoss[0m : 1.67853
[1mStep[0m  [297/339], [94mLoss[0m : 2.17692
[1mStep[0m  [330/339], [94mLoss[0m : 1.83494

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.955, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15857
[1mStep[0m  [33/339], [94mLoss[0m : 1.49247
[1mStep[0m  [66/339], [94mLoss[0m : 1.90673
[1mStep[0m  [99/339], [94mLoss[0m : 1.48915
[1mStep[0m  [132/339], [94mLoss[0m : 2.06233
[1mStep[0m  [165/339], [94mLoss[0m : 2.14188
[1mStep[0m  [198/339], [94mLoss[0m : 2.11331
[1mStep[0m  [231/339], [94mLoss[0m : 1.58705
[1mStep[0m  [264/339], [94mLoss[0m : 2.04615
[1mStep[0m  [297/339], [94mLoss[0m : 2.03977
[1mStep[0m  [330/339], [94mLoss[0m : 1.96849

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84675
[1mStep[0m  [33/339], [94mLoss[0m : 1.75398
[1mStep[0m  [66/339], [94mLoss[0m : 1.63192
[1mStep[0m  [99/339], [94mLoss[0m : 1.85460
[1mStep[0m  [132/339], [94mLoss[0m : 1.92646
[1mStep[0m  [165/339], [94mLoss[0m : 1.57108
[1mStep[0m  [198/339], [94mLoss[0m : 1.89866
[1mStep[0m  [231/339], [94mLoss[0m : 2.03136
[1mStep[0m  [264/339], [94mLoss[0m : 1.82217
[1mStep[0m  [297/339], [94mLoss[0m : 2.14521
[1mStep[0m  [330/339], [94mLoss[0m : 2.07773

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.49966
[1mStep[0m  [33/339], [94mLoss[0m : 1.50236
[1mStep[0m  [66/339], [94mLoss[0m : 1.36113
[1mStep[0m  [99/339], [94mLoss[0m : 1.97650
[1mStep[0m  [132/339], [94mLoss[0m : 2.10794
[1mStep[0m  [165/339], [94mLoss[0m : 1.73739
[1mStep[0m  [198/339], [94mLoss[0m : 2.28982
[1mStep[0m  [231/339], [94mLoss[0m : 2.16166
[1mStep[0m  [264/339], [94mLoss[0m : 1.71498
[1mStep[0m  [297/339], [94mLoss[0m : 2.13224
[1mStep[0m  [330/339], [94mLoss[0m : 3.32928

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64249
[1mStep[0m  [33/339], [94mLoss[0m : 1.63178
[1mStep[0m  [66/339], [94mLoss[0m : 1.15823
[1mStep[0m  [99/339], [94mLoss[0m : 1.84166
[1mStep[0m  [132/339], [94mLoss[0m : 1.55857
[1mStep[0m  [165/339], [94mLoss[0m : 2.14042
[1mStep[0m  [198/339], [94mLoss[0m : 1.70669
[1mStep[0m  [231/339], [94mLoss[0m : 1.69253
[1mStep[0m  [264/339], [94mLoss[0m : 2.03469
[1mStep[0m  [297/339], [94mLoss[0m : 1.97187
[1mStep[0m  [330/339], [94mLoss[0m : 1.62944

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85352
[1mStep[0m  [33/339], [94mLoss[0m : 1.51133
[1mStep[0m  [66/339], [94mLoss[0m : 2.02440
[1mStep[0m  [99/339], [94mLoss[0m : 1.79351
[1mStep[0m  [132/339], [94mLoss[0m : 2.04299
[1mStep[0m  [165/339], [94mLoss[0m : 2.03807
[1mStep[0m  [198/339], [94mLoss[0m : 2.20423
[1mStep[0m  [231/339], [94mLoss[0m : 1.56764
[1mStep[0m  [264/339], [94mLoss[0m : 2.40834
[1mStep[0m  [297/339], [94mLoss[0m : 1.65833
[1mStep[0m  [330/339], [94mLoss[0m : 1.61264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.777, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38172
[1mStep[0m  [33/339], [94mLoss[0m : 2.05735
[1mStep[0m  [66/339], [94mLoss[0m : 1.47667
[1mStep[0m  [99/339], [94mLoss[0m : 1.79567
[1mStep[0m  [132/339], [94mLoss[0m : 1.81980
[1mStep[0m  [165/339], [94mLoss[0m : 2.14552
[1mStep[0m  [198/339], [94mLoss[0m : 1.43205
[1mStep[0m  [231/339], [94mLoss[0m : 1.85442
[1mStep[0m  [264/339], [94mLoss[0m : 1.42806
[1mStep[0m  [297/339], [94mLoss[0m : 1.98941
[1mStep[0m  [330/339], [94mLoss[0m : 1.85949

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.766, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52692
[1mStep[0m  [33/339], [94mLoss[0m : 1.78874
[1mStep[0m  [66/339], [94mLoss[0m : 2.01422
[1mStep[0m  [99/339], [94mLoss[0m : 1.66820
[1mStep[0m  [132/339], [94mLoss[0m : 1.36322
[1mStep[0m  [165/339], [94mLoss[0m : 1.69156
[1mStep[0m  [198/339], [94mLoss[0m : 1.49799
[1mStep[0m  [231/339], [94mLoss[0m : 1.66202
[1mStep[0m  [264/339], [94mLoss[0m : 1.92732
[1mStep[0m  [297/339], [94mLoss[0m : 1.56196
[1mStep[0m  [330/339], [94mLoss[0m : 1.49254

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.706, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51638
[1mStep[0m  [33/339], [94mLoss[0m : 1.79661
[1mStep[0m  [66/339], [94mLoss[0m : 1.94009
[1mStep[0m  [99/339], [94mLoss[0m : 1.24921
[1mStep[0m  [132/339], [94mLoss[0m : 1.65124
[1mStep[0m  [165/339], [94mLoss[0m : 1.44563
[1mStep[0m  [198/339], [94mLoss[0m : 2.16454
[1mStep[0m  [231/339], [94mLoss[0m : 2.02257
[1mStep[0m  [264/339], [94mLoss[0m : 1.56904
[1mStep[0m  [297/339], [94mLoss[0m : 1.67026
[1mStep[0m  [330/339], [94mLoss[0m : 2.10047

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.21987
[1mStep[0m  [33/339], [94mLoss[0m : 1.32064
[1mStep[0m  [66/339], [94mLoss[0m : 1.75120
[1mStep[0m  [99/339], [94mLoss[0m : 1.69240
[1mStep[0m  [132/339], [94mLoss[0m : 1.48974
[1mStep[0m  [165/339], [94mLoss[0m : 1.17386
[1mStep[0m  [198/339], [94mLoss[0m : 1.67967
[1mStep[0m  [231/339], [94mLoss[0m : 1.42991
[1mStep[0m  [264/339], [94mLoss[0m : 1.25964
[1mStep[0m  [297/339], [94mLoss[0m : 2.09888
[1mStep[0m  [330/339], [94mLoss[0m : 1.71006

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68891
[1mStep[0m  [33/339], [94mLoss[0m : 1.63259
[1mStep[0m  [66/339], [94mLoss[0m : 1.24181
[1mStep[0m  [99/339], [94mLoss[0m : 1.24570
[1mStep[0m  [132/339], [94mLoss[0m : 1.40670
[1mStep[0m  [165/339], [94mLoss[0m : 2.04923
[1mStep[0m  [198/339], [94mLoss[0m : 1.36401
[1mStep[0m  [231/339], [94mLoss[0m : 1.81594
[1mStep[0m  [264/339], [94mLoss[0m : 1.38405
[1mStep[0m  [297/339], [94mLoss[0m : 1.52229
[1mStep[0m  [330/339], [94mLoss[0m : 1.74939

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72954
[1mStep[0m  [33/339], [94mLoss[0m : 1.64811
[1mStep[0m  [66/339], [94mLoss[0m : 1.23114
[1mStep[0m  [99/339], [94mLoss[0m : 1.41911
[1mStep[0m  [132/339], [94mLoss[0m : 1.46455
[1mStep[0m  [165/339], [94mLoss[0m : 2.08866
[1mStep[0m  [198/339], [94mLoss[0m : 1.68647
[1mStep[0m  [231/339], [94mLoss[0m : 1.49568
[1mStep[0m  [264/339], [94mLoss[0m : 1.79378
[1mStep[0m  [297/339], [94mLoss[0m : 1.70503
[1mStep[0m  [330/339], [94mLoss[0m : 1.54289

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.426, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77141
[1mStep[0m  [33/339], [94mLoss[0m : 1.22099
[1mStep[0m  [66/339], [94mLoss[0m : 1.39529
[1mStep[0m  [99/339], [94mLoss[0m : 1.34686
[1mStep[0m  [132/339], [94mLoss[0m : 1.27359
[1mStep[0m  [165/339], [94mLoss[0m : 1.23161
[1mStep[0m  [198/339], [94mLoss[0m : 1.57871
[1mStep[0m  [231/339], [94mLoss[0m : 1.72201
[1mStep[0m  [264/339], [94mLoss[0m : 1.36388
[1mStep[0m  [297/339], [94mLoss[0m : 1.63146
[1mStep[0m  [330/339], [94mLoss[0m : 1.40154

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.569, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47897
[1mStep[0m  [33/339], [94mLoss[0m : 1.49951
[1mStep[0m  [66/339], [94mLoss[0m : 0.93065
[1mStep[0m  [99/339], [94mLoss[0m : 1.41723
[1mStep[0m  [132/339], [94mLoss[0m : 1.38716
[1mStep[0m  [165/339], [94mLoss[0m : 1.77284
[1mStep[0m  [198/339], [94mLoss[0m : 1.21525
[1mStep[0m  [231/339], [94mLoss[0m : 1.43380
[1mStep[0m  [264/339], [94mLoss[0m : 1.60696
[1mStep[0m  [297/339], [94mLoss[0m : 1.44916
[1mStep[0m  [330/339], [94mLoss[0m : 1.32012

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.34833
[1mStep[0m  [33/339], [94mLoss[0m : 1.85779
[1mStep[0m  [66/339], [94mLoss[0m : 1.65815
[1mStep[0m  [99/339], [94mLoss[0m : 1.35826
[1mStep[0m  [132/339], [94mLoss[0m : 1.75258
[1mStep[0m  [165/339], [94mLoss[0m : 1.22815
[1mStep[0m  [198/339], [94mLoss[0m : 2.29177
[1mStep[0m  [231/339], [94mLoss[0m : 1.09282
[1mStep[0m  [264/339], [94mLoss[0m : 1.30158
[1mStep[0m  [297/339], [94mLoss[0m : 1.37757
[1mStep[0m  [330/339], [94mLoss[0m : 1.65008

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.505, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86214
[1mStep[0m  [33/339], [94mLoss[0m : 1.68235
[1mStep[0m  [66/339], [94mLoss[0m : 1.79805
[1mStep[0m  [99/339], [94mLoss[0m : 1.43175
[1mStep[0m  [132/339], [94mLoss[0m : 1.28317
[1mStep[0m  [165/339], [94mLoss[0m : 1.41098
[1mStep[0m  [198/339], [94mLoss[0m : 2.28579
[1mStep[0m  [231/339], [94mLoss[0m : 1.38516
[1mStep[0m  [264/339], [94mLoss[0m : 1.68960
[1mStep[0m  [297/339], [94mLoss[0m : 2.04118
[1mStep[0m  [330/339], [94mLoss[0m : 2.07874

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47246
[1mStep[0m  [33/339], [94mLoss[0m : 2.11943
[1mStep[0m  [66/339], [94mLoss[0m : 1.62114
[1mStep[0m  [99/339], [94mLoss[0m : 1.83014
[1mStep[0m  [132/339], [94mLoss[0m : 1.46294
[1mStep[0m  [165/339], [94mLoss[0m : 1.87438
[1mStep[0m  [198/339], [94mLoss[0m : 1.27127
[1mStep[0m  [231/339], [94mLoss[0m : 1.37767
[1mStep[0m  [264/339], [94mLoss[0m : 1.36300
[1mStep[0m  [297/339], [94mLoss[0m : 1.58424
[1mStep[0m  [330/339], [94mLoss[0m : 1.22418

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.567, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.31974
[1mStep[0m  [33/339], [94mLoss[0m : 1.47664
[1mStep[0m  [66/339], [94mLoss[0m : 1.65889
[1mStep[0m  [99/339], [94mLoss[0m : 1.24869
[1mStep[0m  [132/339], [94mLoss[0m : 1.09393
[1mStep[0m  [165/339], [94mLoss[0m : 1.30919
[1mStep[0m  [198/339], [94mLoss[0m : 1.34307
[1mStep[0m  [231/339], [94mLoss[0m : 1.35512
[1mStep[0m  [264/339], [94mLoss[0m : 1.60920
[1mStep[0m  [297/339], [94mLoss[0m : 1.42310
[1mStep[0m  [330/339], [94mLoss[0m : 1.28155

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.48598
[1mStep[0m  [33/339], [94mLoss[0m : 1.39957
[1mStep[0m  [66/339], [94mLoss[0m : 1.26722
[1mStep[0m  [99/339], [94mLoss[0m : 1.46571
[1mStep[0m  [132/339], [94mLoss[0m : 1.58348
[1mStep[0m  [165/339], [94mLoss[0m : 1.25516
[1mStep[0m  [198/339], [94mLoss[0m : 1.31326
[1mStep[0m  [231/339], [94mLoss[0m : 1.44643
[1mStep[0m  [264/339], [94mLoss[0m : 1.35911
[1mStep[0m  [297/339], [94mLoss[0m : 1.49681
[1mStep[0m  [330/339], [94mLoss[0m : 1.12010

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.454, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40775
[1mStep[0m  [33/339], [94mLoss[0m : 1.45008
[1mStep[0m  [66/339], [94mLoss[0m : 1.05019
[1mStep[0m  [99/339], [94mLoss[0m : 1.52001
[1mStep[0m  [132/339], [94mLoss[0m : 1.27128
[1mStep[0m  [165/339], [94mLoss[0m : 1.05651
[1mStep[0m  [198/339], [94mLoss[0m : 1.21972
[1mStep[0m  [231/339], [94mLoss[0m : 1.57447
[1mStep[0m  [264/339], [94mLoss[0m : 1.51528
[1mStep[0m  [297/339], [94mLoss[0m : 1.22850
[1mStep[0m  [330/339], [94mLoss[0m : 1.38846

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.535, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57292
[1mStep[0m  [33/339], [94mLoss[0m : 1.14072
[1mStep[0m  [66/339], [94mLoss[0m : 1.37294
[1mStep[0m  [99/339], [94mLoss[0m : 1.35783
[1mStep[0m  [132/339], [94mLoss[0m : 1.26263
[1mStep[0m  [165/339], [94mLoss[0m : 1.39340
[1mStep[0m  [198/339], [94mLoss[0m : 1.21596
[1mStep[0m  [231/339], [94mLoss[0m : 1.52419
[1mStep[0m  [264/339], [94mLoss[0m : 0.99616
[1mStep[0m  [297/339], [94mLoss[0m : 1.67999
[1mStep[0m  [330/339], [94mLoss[0m : 1.58582

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.537
====================================

Phase 2 - Evaluation MAE:  2.5366534764787794
MAE score P1      2.364702
MAE score P2      2.536653
loss              1.427422
learning_rate     0.007525
batch_size              32
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.89715
[1mStep[0m  [2/21], [94mLoss[0m : 10.80469
[1mStep[0m  [4/21], [94mLoss[0m : 10.92558
[1mStep[0m  [6/21], [94mLoss[0m : 10.67608
[1mStep[0m  [8/21], [94mLoss[0m : 10.53832
[1mStep[0m  [10/21], [94mLoss[0m : 10.68518
[1mStep[0m  [12/21], [94mLoss[0m : 10.44326
[1mStep[0m  [14/21], [94mLoss[0m : 10.38904
[1mStep[0m  [16/21], [94mLoss[0m : 10.21060
[1mStep[0m  [18/21], [94mLoss[0m : 10.23558
[1mStep[0m  [20/21], [94mLoss[0m : 10.26604

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.544, [92mTest[0m: 10.871, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.25136
[1mStep[0m  [2/21], [94mLoss[0m : 9.85497
[1mStep[0m  [4/21], [94mLoss[0m : 9.76220
[1mStep[0m  [6/21], [94mLoss[0m : 9.68561
[1mStep[0m  [8/21], [94mLoss[0m : 9.34167
[1mStep[0m  [10/21], [94mLoss[0m : 9.24883
[1mStep[0m  [12/21], [94mLoss[0m : 9.26812
[1mStep[0m  [14/21], [94mLoss[0m : 9.32420
[1mStep[0m  [16/21], [94mLoss[0m : 9.02936
[1mStep[0m  [18/21], [94mLoss[0m : 8.95981
[1mStep[0m  [20/21], [94mLoss[0m : 8.46731

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.423, [92mTest[0m: 9.996, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.91953
[1mStep[0m  [2/21], [94mLoss[0m : 8.42749
[1mStep[0m  [4/21], [94mLoss[0m : 8.21997
[1mStep[0m  [6/21], [94mLoss[0m : 8.32770
[1mStep[0m  [8/21], [94mLoss[0m : 7.93909
[1mStep[0m  [10/21], [94mLoss[0m : 7.86391
[1mStep[0m  [12/21], [94mLoss[0m : 7.49538
[1mStep[0m  [14/21], [94mLoss[0m : 7.10214
[1mStep[0m  [16/21], [94mLoss[0m : 7.38183
[1mStep[0m  [18/21], [94mLoss[0m : 6.90265
[1mStep[0m  [20/21], [94mLoss[0m : 6.75541

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.735, [92mTest[0m: 8.661, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.41331
[1mStep[0m  [2/21], [94mLoss[0m : 6.56391
[1mStep[0m  [4/21], [94mLoss[0m : 6.21606
[1mStep[0m  [6/21], [94mLoss[0m : 5.94499
[1mStep[0m  [8/21], [94mLoss[0m : 5.84812
[1mStep[0m  [10/21], [94mLoss[0m : 5.84632
[1mStep[0m  [12/21], [94mLoss[0m : 5.53475
[1mStep[0m  [14/21], [94mLoss[0m : 5.32082
[1mStep[0m  [16/21], [94mLoss[0m : 5.26150
[1mStep[0m  [18/21], [94mLoss[0m : 4.78830
[1mStep[0m  [20/21], [94mLoss[0m : 4.68933

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.647, [92mTest[0m: 6.729, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.58466
[1mStep[0m  [2/21], [94mLoss[0m : 4.30927
[1mStep[0m  [4/21], [94mLoss[0m : 4.08111
[1mStep[0m  [6/21], [94mLoss[0m : 4.23947
[1mStep[0m  [8/21], [94mLoss[0m : 3.81960
[1mStep[0m  [10/21], [94mLoss[0m : 3.86327
[1mStep[0m  [12/21], [94mLoss[0m : 3.28190
[1mStep[0m  [14/21], [94mLoss[0m : 3.44435
[1mStep[0m  [16/21], [94mLoss[0m : 3.10900
[1mStep[0m  [18/21], [94mLoss[0m : 2.84179
[1mStep[0m  [20/21], [94mLoss[0m : 2.83459

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.659, [92mTest[0m: 3.846, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.93760
[1mStep[0m  [2/21], [94mLoss[0m : 2.86441
[1mStep[0m  [4/21], [94mLoss[0m : 2.73194
[1mStep[0m  [6/21], [94mLoss[0m : 2.55897
[1mStep[0m  [8/21], [94mLoss[0m : 2.65226
[1mStep[0m  [10/21], [94mLoss[0m : 2.70591
[1mStep[0m  [12/21], [94mLoss[0m : 2.65048
[1mStep[0m  [14/21], [94mLoss[0m : 2.61842
[1mStep[0m  [16/21], [94mLoss[0m : 2.44540
[1mStep[0m  [18/21], [94mLoss[0m : 2.65256
[1mStep[0m  [20/21], [94mLoss[0m : 2.52006

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53459
[1mStep[0m  [2/21], [94mLoss[0m : 2.59865
[1mStep[0m  [4/21], [94mLoss[0m : 2.47900
[1mStep[0m  [6/21], [94mLoss[0m : 2.51788
[1mStep[0m  [8/21], [94mLoss[0m : 2.53647
[1mStep[0m  [10/21], [94mLoss[0m : 2.55411
[1mStep[0m  [12/21], [94mLoss[0m : 2.48432
[1mStep[0m  [14/21], [94mLoss[0m : 2.65096
[1mStep[0m  [16/21], [94mLoss[0m : 2.46397
[1mStep[0m  [18/21], [94mLoss[0m : 2.62916
[1mStep[0m  [20/21], [94mLoss[0m : 2.63093

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43324
[1mStep[0m  [2/21], [94mLoss[0m : 2.58790
[1mStep[0m  [4/21], [94mLoss[0m : 2.58248
[1mStep[0m  [6/21], [94mLoss[0m : 2.42974
[1mStep[0m  [8/21], [94mLoss[0m : 2.44832
[1mStep[0m  [10/21], [94mLoss[0m : 2.52058
[1mStep[0m  [12/21], [94mLoss[0m : 2.63517
[1mStep[0m  [14/21], [94mLoss[0m : 2.56452
[1mStep[0m  [16/21], [94mLoss[0m : 2.38425
[1mStep[0m  [18/21], [94mLoss[0m : 2.57515
[1mStep[0m  [20/21], [94mLoss[0m : 2.51071

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44361
[1mStep[0m  [2/21], [94mLoss[0m : 2.33411
[1mStep[0m  [4/21], [94mLoss[0m : 2.51330
[1mStep[0m  [6/21], [94mLoss[0m : 2.44682
[1mStep[0m  [8/21], [94mLoss[0m : 2.43758
[1mStep[0m  [10/21], [94mLoss[0m : 2.53184
[1mStep[0m  [12/21], [94mLoss[0m : 2.59867
[1mStep[0m  [14/21], [94mLoss[0m : 2.52554
[1mStep[0m  [16/21], [94mLoss[0m : 2.62686
[1mStep[0m  [18/21], [94mLoss[0m : 2.37221
[1mStep[0m  [20/21], [94mLoss[0m : 2.53292

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53372
[1mStep[0m  [2/21], [94mLoss[0m : 2.50917
[1mStep[0m  [4/21], [94mLoss[0m : 2.35419
[1mStep[0m  [6/21], [94mLoss[0m : 2.56928
[1mStep[0m  [8/21], [94mLoss[0m : 2.52302
[1mStep[0m  [10/21], [94mLoss[0m : 2.56669
[1mStep[0m  [12/21], [94mLoss[0m : 2.51872
[1mStep[0m  [14/21], [94mLoss[0m : 2.43694
[1mStep[0m  [16/21], [94mLoss[0m : 2.56162
[1mStep[0m  [18/21], [94mLoss[0m : 2.36945
[1mStep[0m  [20/21], [94mLoss[0m : 2.66196

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51184
[1mStep[0m  [2/21], [94mLoss[0m : 2.55264
[1mStep[0m  [4/21], [94mLoss[0m : 2.50719
[1mStep[0m  [6/21], [94mLoss[0m : 2.57640
[1mStep[0m  [8/21], [94mLoss[0m : 2.54763
[1mStep[0m  [10/21], [94mLoss[0m : 2.26946
[1mStep[0m  [12/21], [94mLoss[0m : 2.59856
[1mStep[0m  [14/21], [94mLoss[0m : 2.55238
[1mStep[0m  [16/21], [94mLoss[0m : 2.35213
[1mStep[0m  [18/21], [94mLoss[0m : 2.61029
[1mStep[0m  [20/21], [94mLoss[0m : 2.28846

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51220
[1mStep[0m  [2/21], [94mLoss[0m : 2.44019
[1mStep[0m  [4/21], [94mLoss[0m : 2.38269
[1mStep[0m  [6/21], [94mLoss[0m : 2.43624
[1mStep[0m  [8/21], [94mLoss[0m : 2.40656
[1mStep[0m  [10/21], [94mLoss[0m : 2.34023
[1mStep[0m  [12/21], [94mLoss[0m : 2.38392
[1mStep[0m  [14/21], [94mLoss[0m : 2.59335
[1mStep[0m  [16/21], [94mLoss[0m : 2.54720
[1mStep[0m  [18/21], [94mLoss[0m : 2.44041
[1mStep[0m  [20/21], [94mLoss[0m : 2.55690

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37985
[1mStep[0m  [2/21], [94mLoss[0m : 2.61335
[1mStep[0m  [4/21], [94mLoss[0m : 2.47497
[1mStep[0m  [6/21], [94mLoss[0m : 2.35107
[1mStep[0m  [8/21], [94mLoss[0m : 2.42406
[1mStep[0m  [10/21], [94mLoss[0m : 2.51832
[1mStep[0m  [12/21], [94mLoss[0m : 2.45659
[1mStep[0m  [14/21], [94mLoss[0m : 2.56813
[1mStep[0m  [16/21], [94mLoss[0m : 2.52496
[1mStep[0m  [18/21], [94mLoss[0m : 2.43513
[1mStep[0m  [20/21], [94mLoss[0m : 2.49461

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53069
[1mStep[0m  [2/21], [94mLoss[0m : 2.52868
[1mStep[0m  [4/21], [94mLoss[0m : 2.38026
[1mStep[0m  [6/21], [94mLoss[0m : 2.36193
[1mStep[0m  [8/21], [94mLoss[0m : 2.51728
[1mStep[0m  [10/21], [94mLoss[0m : 2.56220
[1mStep[0m  [12/21], [94mLoss[0m : 2.56372
[1mStep[0m  [14/21], [94mLoss[0m : 2.56664
[1mStep[0m  [16/21], [94mLoss[0m : 2.29523
[1mStep[0m  [18/21], [94mLoss[0m : 2.42352
[1mStep[0m  [20/21], [94mLoss[0m : 2.47429

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44994
[1mStep[0m  [2/21], [94mLoss[0m : 2.26492
[1mStep[0m  [4/21], [94mLoss[0m : 2.38115
[1mStep[0m  [6/21], [94mLoss[0m : 2.44785
[1mStep[0m  [8/21], [94mLoss[0m : 2.50948
[1mStep[0m  [10/21], [94mLoss[0m : 2.37021
[1mStep[0m  [12/21], [94mLoss[0m : 2.39155
[1mStep[0m  [14/21], [94mLoss[0m : 2.60995
[1mStep[0m  [16/21], [94mLoss[0m : 2.65499
[1mStep[0m  [18/21], [94mLoss[0m : 2.43025
[1mStep[0m  [20/21], [94mLoss[0m : 2.49950

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.317, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52666
[1mStep[0m  [2/21], [94mLoss[0m : 2.38419
[1mStep[0m  [4/21], [94mLoss[0m : 2.48210
[1mStep[0m  [6/21], [94mLoss[0m : 2.52339
[1mStep[0m  [8/21], [94mLoss[0m : 2.52170
[1mStep[0m  [10/21], [94mLoss[0m : 2.36469
[1mStep[0m  [12/21], [94mLoss[0m : 2.43612
[1mStep[0m  [14/21], [94mLoss[0m : 2.46423
[1mStep[0m  [16/21], [94mLoss[0m : 2.57558
[1mStep[0m  [18/21], [94mLoss[0m : 2.44840
[1mStep[0m  [20/21], [94mLoss[0m : 2.41675

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42161
[1mStep[0m  [2/21], [94mLoss[0m : 2.42236
[1mStep[0m  [4/21], [94mLoss[0m : 2.39712
[1mStep[0m  [6/21], [94mLoss[0m : 2.56053
[1mStep[0m  [8/21], [94mLoss[0m : 2.55986
[1mStep[0m  [10/21], [94mLoss[0m : 2.57342
[1mStep[0m  [12/21], [94mLoss[0m : 2.52926
[1mStep[0m  [14/21], [94mLoss[0m : 2.54596
[1mStep[0m  [16/21], [94mLoss[0m : 2.42010
[1mStep[0m  [18/21], [94mLoss[0m : 2.25680
[1mStep[0m  [20/21], [94mLoss[0m : 2.46197

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35165
[1mStep[0m  [2/21], [94mLoss[0m : 2.47414
[1mStep[0m  [4/21], [94mLoss[0m : 2.39253
[1mStep[0m  [6/21], [94mLoss[0m : 2.51077
[1mStep[0m  [8/21], [94mLoss[0m : 2.59290
[1mStep[0m  [10/21], [94mLoss[0m : 2.47918
[1mStep[0m  [12/21], [94mLoss[0m : 2.51318
[1mStep[0m  [14/21], [94mLoss[0m : 2.45658
[1mStep[0m  [16/21], [94mLoss[0m : 2.30928
[1mStep[0m  [18/21], [94mLoss[0m : 2.38856
[1mStep[0m  [20/21], [94mLoss[0m : 2.58807

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45222
[1mStep[0m  [2/21], [94mLoss[0m : 2.62547
[1mStep[0m  [4/21], [94mLoss[0m : 2.42785
[1mStep[0m  [6/21], [94mLoss[0m : 2.30355
[1mStep[0m  [8/21], [94mLoss[0m : 2.50092
[1mStep[0m  [10/21], [94mLoss[0m : 2.31266
[1mStep[0m  [12/21], [94mLoss[0m : 2.35209
[1mStep[0m  [14/21], [94mLoss[0m : 2.49355
[1mStep[0m  [16/21], [94mLoss[0m : 2.46654
[1mStep[0m  [18/21], [94mLoss[0m : 2.40508
[1mStep[0m  [20/21], [94mLoss[0m : 2.29313

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49896
[1mStep[0m  [2/21], [94mLoss[0m : 2.45868
[1mStep[0m  [4/21], [94mLoss[0m : 2.37238
[1mStep[0m  [6/21], [94mLoss[0m : 2.44060
[1mStep[0m  [8/21], [94mLoss[0m : 2.48798
[1mStep[0m  [10/21], [94mLoss[0m : 2.45522
[1mStep[0m  [12/21], [94mLoss[0m : 2.44651
[1mStep[0m  [14/21], [94mLoss[0m : 2.27170
[1mStep[0m  [16/21], [94mLoss[0m : 2.29098
[1mStep[0m  [18/21], [94mLoss[0m : 2.39966
[1mStep[0m  [20/21], [94mLoss[0m : 2.37985

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40252
[1mStep[0m  [2/21], [94mLoss[0m : 2.29643
[1mStep[0m  [4/21], [94mLoss[0m : 2.39003
[1mStep[0m  [6/21], [94mLoss[0m : 2.35203
[1mStep[0m  [8/21], [94mLoss[0m : 2.51806
[1mStep[0m  [10/21], [94mLoss[0m : 2.58437
[1mStep[0m  [12/21], [94mLoss[0m : 2.37854
[1mStep[0m  [14/21], [94mLoss[0m : 2.54663
[1mStep[0m  [16/21], [94mLoss[0m : 2.39102
[1mStep[0m  [18/21], [94mLoss[0m : 2.47533
[1mStep[0m  [20/21], [94mLoss[0m : 2.60959

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49132
[1mStep[0m  [2/21], [94mLoss[0m : 2.48351
[1mStep[0m  [4/21], [94mLoss[0m : 2.24260
[1mStep[0m  [6/21], [94mLoss[0m : 2.49073
[1mStep[0m  [8/21], [94mLoss[0m : 2.29779
[1mStep[0m  [10/21], [94mLoss[0m : 2.41584
[1mStep[0m  [12/21], [94mLoss[0m : 2.44535
[1mStep[0m  [14/21], [94mLoss[0m : 2.55773
[1mStep[0m  [16/21], [94mLoss[0m : 2.42308
[1mStep[0m  [18/21], [94mLoss[0m : 2.37970
[1mStep[0m  [20/21], [94mLoss[0m : 2.36425

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39445
[1mStep[0m  [2/21], [94mLoss[0m : 2.38606
[1mStep[0m  [4/21], [94mLoss[0m : 2.45271
[1mStep[0m  [6/21], [94mLoss[0m : 2.30103
[1mStep[0m  [8/21], [94mLoss[0m : 2.58099
[1mStep[0m  [10/21], [94mLoss[0m : 2.45365
[1mStep[0m  [12/21], [94mLoss[0m : 2.34564
[1mStep[0m  [14/21], [94mLoss[0m : 2.35089
[1mStep[0m  [16/21], [94mLoss[0m : 2.56888
[1mStep[0m  [18/21], [94mLoss[0m : 2.43684
[1mStep[0m  [20/21], [94mLoss[0m : 2.50613

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22455
[1mStep[0m  [2/21], [94mLoss[0m : 2.24951
[1mStep[0m  [4/21], [94mLoss[0m : 2.51045
[1mStep[0m  [6/21], [94mLoss[0m : 2.50773
[1mStep[0m  [8/21], [94mLoss[0m : 2.43625
[1mStep[0m  [10/21], [94mLoss[0m : 2.38801
[1mStep[0m  [12/21], [94mLoss[0m : 2.57511
[1mStep[0m  [14/21], [94mLoss[0m : 2.47533
[1mStep[0m  [16/21], [94mLoss[0m : 2.43764
[1mStep[0m  [18/21], [94mLoss[0m : 2.51222
[1mStep[0m  [20/21], [94mLoss[0m : 2.51654

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31546
[1mStep[0m  [2/21], [94mLoss[0m : 2.32220
[1mStep[0m  [4/21], [94mLoss[0m : 2.54595
[1mStep[0m  [6/21], [94mLoss[0m : 2.40441
[1mStep[0m  [8/21], [94mLoss[0m : 2.44561
[1mStep[0m  [10/21], [94mLoss[0m : 2.46777
[1mStep[0m  [12/21], [94mLoss[0m : 2.42292
[1mStep[0m  [14/21], [94mLoss[0m : 2.32139
[1mStep[0m  [16/21], [94mLoss[0m : 2.57179
[1mStep[0m  [18/21], [94mLoss[0m : 2.36983
[1mStep[0m  [20/21], [94mLoss[0m : 2.40090

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44024
[1mStep[0m  [2/21], [94mLoss[0m : 2.51943
[1mStep[0m  [4/21], [94mLoss[0m : 2.36341
[1mStep[0m  [6/21], [94mLoss[0m : 2.43361
[1mStep[0m  [8/21], [94mLoss[0m : 2.33552
[1mStep[0m  [10/21], [94mLoss[0m : 2.45180
[1mStep[0m  [12/21], [94mLoss[0m : 2.39277
[1mStep[0m  [14/21], [94mLoss[0m : 2.42650
[1mStep[0m  [16/21], [94mLoss[0m : 2.35068
[1mStep[0m  [18/21], [94mLoss[0m : 2.54806
[1mStep[0m  [20/21], [94mLoss[0m : 2.31796

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48613
[1mStep[0m  [2/21], [94mLoss[0m : 2.43338
[1mStep[0m  [4/21], [94mLoss[0m : 2.37684
[1mStep[0m  [6/21], [94mLoss[0m : 2.47835
[1mStep[0m  [8/21], [94mLoss[0m : 2.37941
[1mStep[0m  [10/21], [94mLoss[0m : 2.48123
[1mStep[0m  [12/21], [94mLoss[0m : 2.35348
[1mStep[0m  [14/21], [94mLoss[0m : 2.39683
[1mStep[0m  [16/21], [94mLoss[0m : 2.33859
[1mStep[0m  [18/21], [94mLoss[0m : 2.39796
[1mStep[0m  [20/21], [94mLoss[0m : 2.47678

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48113
[1mStep[0m  [2/21], [94mLoss[0m : 2.33856
[1mStep[0m  [4/21], [94mLoss[0m : 2.47677
[1mStep[0m  [6/21], [94mLoss[0m : 2.33352
[1mStep[0m  [8/21], [94mLoss[0m : 2.22320
[1mStep[0m  [10/21], [94mLoss[0m : 2.40780
[1mStep[0m  [12/21], [94mLoss[0m : 2.23348
[1mStep[0m  [14/21], [94mLoss[0m : 2.44255
[1mStep[0m  [16/21], [94mLoss[0m : 2.37172
[1mStep[0m  [18/21], [94mLoss[0m : 2.36350
[1mStep[0m  [20/21], [94mLoss[0m : 2.44304

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.306, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53840
[1mStep[0m  [2/21], [94mLoss[0m : 2.41992
[1mStep[0m  [4/21], [94mLoss[0m : 2.37797
[1mStep[0m  [6/21], [94mLoss[0m : 2.37650
[1mStep[0m  [8/21], [94mLoss[0m : 2.33290
[1mStep[0m  [10/21], [94mLoss[0m : 2.27059
[1mStep[0m  [12/21], [94mLoss[0m : 2.47673
[1mStep[0m  [14/21], [94mLoss[0m : 2.44152
[1mStep[0m  [16/21], [94mLoss[0m : 2.39946
[1mStep[0m  [18/21], [94mLoss[0m : 2.48453
[1mStep[0m  [20/21], [94mLoss[0m : 2.49574

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40162
[1mStep[0m  [2/21], [94mLoss[0m : 2.28078
[1mStep[0m  [4/21], [94mLoss[0m : 2.56539
[1mStep[0m  [6/21], [94mLoss[0m : 2.36398
[1mStep[0m  [8/21], [94mLoss[0m : 2.38022
[1mStep[0m  [10/21], [94mLoss[0m : 2.34015
[1mStep[0m  [12/21], [94mLoss[0m : 2.35145
[1mStep[0m  [14/21], [94mLoss[0m : 2.53661
[1mStep[0m  [16/21], [94mLoss[0m : 2.46665
[1mStep[0m  [18/21], [94mLoss[0m : 2.37263
[1mStep[0m  [20/21], [94mLoss[0m : 2.40587

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.312
====================================

Phase 1 - Evaluation MAE:  2.312080042702811
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.30075
[1mStep[0m  [2/21], [94mLoss[0m : 2.37325
[1mStep[0m  [4/21], [94mLoss[0m : 2.45437
[1mStep[0m  [6/21], [94mLoss[0m : 2.48463
[1mStep[0m  [8/21], [94mLoss[0m : 2.33263
[1mStep[0m  [10/21], [94mLoss[0m : 2.50117
[1mStep[0m  [12/21], [94mLoss[0m : 2.67640
[1mStep[0m  [14/21], [94mLoss[0m : 2.60572
[1mStep[0m  [16/21], [94mLoss[0m : 2.46527
[1mStep[0m  [18/21], [94mLoss[0m : 2.41244
[1mStep[0m  [20/21], [94mLoss[0m : 2.37949

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.316, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49103
[1mStep[0m  [2/21], [94mLoss[0m : 2.38765
[1mStep[0m  [4/21], [94mLoss[0m : 2.61628
[1mStep[0m  [6/21], [94mLoss[0m : 2.44286
[1mStep[0m  [8/21], [94mLoss[0m : 2.34375
[1mStep[0m  [10/21], [94mLoss[0m : 2.29449
[1mStep[0m  [12/21], [94mLoss[0m : 2.33232
[1mStep[0m  [14/21], [94mLoss[0m : 2.46509
[1mStep[0m  [16/21], [94mLoss[0m : 2.27874
[1mStep[0m  [18/21], [94mLoss[0m : 2.32573
[1mStep[0m  [20/21], [94mLoss[0m : 2.32404

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.921, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42425
[1mStep[0m  [2/21], [94mLoss[0m : 2.39186
[1mStep[0m  [4/21], [94mLoss[0m : 2.26533
[1mStep[0m  [6/21], [94mLoss[0m : 2.27712
[1mStep[0m  [8/21], [94mLoss[0m : 2.29007
[1mStep[0m  [10/21], [94mLoss[0m : 2.42349
[1mStep[0m  [12/21], [94mLoss[0m : 2.31435
[1mStep[0m  [14/21], [94mLoss[0m : 2.24133
[1mStep[0m  [16/21], [94mLoss[0m : 2.30774
[1mStep[0m  [18/21], [94mLoss[0m : 2.36665
[1mStep[0m  [20/21], [94mLoss[0m : 2.32942

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.810, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23723
[1mStep[0m  [2/21], [94mLoss[0m : 2.24843
[1mStep[0m  [4/21], [94mLoss[0m : 2.27501
[1mStep[0m  [6/21], [94mLoss[0m : 2.30344
[1mStep[0m  [8/21], [94mLoss[0m : 2.16526
[1mStep[0m  [10/21], [94mLoss[0m : 2.27254
[1mStep[0m  [12/21], [94mLoss[0m : 2.22954
[1mStep[0m  [14/21], [94mLoss[0m : 2.24740
[1mStep[0m  [16/21], [94mLoss[0m : 2.22997
[1mStep[0m  [18/21], [94mLoss[0m : 2.25849
[1mStep[0m  [20/21], [94mLoss[0m : 2.32307

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19507
[1mStep[0m  [2/21], [94mLoss[0m : 2.26705
[1mStep[0m  [4/21], [94mLoss[0m : 2.11613
[1mStep[0m  [6/21], [94mLoss[0m : 2.22474
[1mStep[0m  [8/21], [94mLoss[0m : 2.21707
[1mStep[0m  [10/21], [94mLoss[0m : 2.20272
[1mStep[0m  [12/21], [94mLoss[0m : 2.24367
[1mStep[0m  [14/21], [94mLoss[0m : 2.23862
[1mStep[0m  [16/21], [94mLoss[0m : 2.22481
[1mStep[0m  [18/21], [94mLoss[0m : 2.23847
[1mStep[0m  [20/21], [94mLoss[0m : 2.15742

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34434
[1mStep[0m  [2/21], [94mLoss[0m : 2.07506
[1mStep[0m  [4/21], [94mLoss[0m : 2.11043
[1mStep[0m  [6/21], [94mLoss[0m : 2.02577
[1mStep[0m  [8/21], [94mLoss[0m : 2.07108
[1mStep[0m  [10/21], [94mLoss[0m : 2.08607
[1mStep[0m  [12/21], [94mLoss[0m : 2.04153
[1mStep[0m  [14/21], [94mLoss[0m : 2.15487
[1mStep[0m  [16/21], [94mLoss[0m : 2.09281
[1mStep[0m  [18/21], [94mLoss[0m : 2.22559
[1mStep[0m  [20/21], [94mLoss[0m : 2.15664

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.567, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97059
[1mStep[0m  [2/21], [94mLoss[0m : 1.94116
[1mStep[0m  [4/21], [94mLoss[0m : 1.92845
[1mStep[0m  [6/21], [94mLoss[0m : 2.00058
[1mStep[0m  [8/21], [94mLoss[0m : 2.26381
[1mStep[0m  [10/21], [94mLoss[0m : 1.94957
[1mStep[0m  [12/21], [94mLoss[0m : 1.98783
[1mStep[0m  [14/21], [94mLoss[0m : 2.00947
[1mStep[0m  [16/21], [94mLoss[0m : 2.22322
[1mStep[0m  [18/21], [94mLoss[0m : 2.04495
[1mStep[0m  [20/21], [94mLoss[0m : 2.13296

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.04342
[1mStep[0m  [2/21], [94mLoss[0m : 2.06582
[1mStep[0m  [4/21], [94mLoss[0m : 2.14014
[1mStep[0m  [6/21], [94mLoss[0m : 1.95949
[1mStep[0m  [8/21], [94mLoss[0m : 1.99267
[1mStep[0m  [10/21], [94mLoss[0m : 1.87284
[1mStep[0m  [12/21], [94mLoss[0m : 1.99516
[1mStep[0m  [14/21], [94mLoss[0m : 1.96225
[1mStep[0m  [16/21], [94mLoss[0m : 2.01517
[1mStep[0m  [18/21], [94mLoss[0m : 2.01435
[1mStep[0m  [20/21], [94mLoss[0m : 1.88672

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.610, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.03467
[1mStep[0m  [2/21], [94mLoss[0m : 1.94642
[1mStep[0m  [4/21], [94mLoss[0m : 1.90975
[1mStep[0m  [6/21], [94mLoss[0m : 1.85296
[1mStep[0m  [8/21], [94mLoss[0m : 1.97522
[1mStep[0m  [10/21], [94mLoss[0m : 1.88397
[1mStep[0m  [12/21], [94mLoss[0m : 1.95073
[1mStep[0m  [14/21], [94mLoss[0m : 2.13610
[1mStep[0m  [16/21], [94mLoss[0m : 1.87596
[1mStep[0m  [18/21], [94mLoss[0m : 2.07526
[1mStep[0m  [20/21], [94mLoss[0m : 1.97483

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91983
[1mStep[0m  [2/21], [94mLoss[0m : 1.80370
[1mStep[0m  [4/21], [94mLoss[0m : 1.93719
[1mStep[0m  [6/21], [94mLoss[0m : 1.79208
[1mStep[0m  [8/21], [94mLoss[0m : 1.79180
[1mStep[0m  [10/21], [94mLoss[0m : 1.92088
[1mStep[0m  [12/21], [94mLoss[0m : 1.93170
[1mStep[0m  [14/21], [94mLoss[0m : 1.91363
[1mStep[0m  [16/21], [94mLoss[0m : 1.83962
[1mStep[0m  [18/21], [94mLoss[0m : 1.91554
[1mStep[0m  [20/21], [94mLoss[0m : 1.90576

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.877, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.83085
[1mStep[0m  [2/21], [94mLoss[0m : 1.68432
[1mStep[0m  [4/21], [94mLoss[0m : 1.80106
[1mStep[0m  [6/21], [94mLoss[0m : 1.75551
[1mStep[0m  [8/21], [94mLoss[0m : 1.75946
[1mStep[0m  [10/21], [94mLoss[0m : 1.92333
[1mStep[0m  [12/21], [94mLoss[0m : 1.84624
[1mStep[0m  [14/21], [94mLoss[0m : 1.75623
[1mStep[0m  [16/21], [94mLoss[0m : 1.80560
[1mStep[0m  [18/21], [94mLoss[0m : 1.74123
[1mStep[0m  [20/21], [94mLoss[0m : 1.99700

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.69718
[1mStep[0m  [2/21], [94mLoss[0m : 1.80083
[1mStep[0m  [4/21], [94mLoss[0m : 1.73324
[1mStep[0m  [6/21], [94mLoss[0m : 1.80397
[1mStep[0m  [8/21], [94mLoss[0m : 1.68662
[1mStep[0m  [10/21], [94mLoss[0m : 1.85955
[1mStep[0m  [12/21], [94mLoss[0m : 1.89808
[1mStep[0m  [14/21], [94mLoss[0m : 1.81162
[1mStep[0m  [16/21], [94mLoss[0m : 1.85243
[1mStep[0m  [18/21], [94mLoss[0m : 1.85487
[1mStep[0m  [20/21], [94mLoss[0m : 1.84694

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93305
[1mStep[0m  [2/21], [94mLoss[0m : 1.63881
[1mStep[0m  [4/21], [94mLoss[0m : 1.75701
[1mStep[0m  [6/21], [94mLoss[0m : 1.75884
[1mStep[0m  [8/21], [94mLoss[0m : 1.74200
[1mStep[0m  [10/21], [94mLoss[0m : 1.66454
[1mStep[0m  [12/21], [94mLoss[0m : 1.72302
[1mStep[0m  [14/21], [94mLoss[0m : 1.77680
[1mStep[0m  [16/21], [94mLoss[0m : 1.73659
[1mStep[0m  [18/21], [94mLoss[0m : 1.74956
[1mStep[0m  [20/21], [94mLoss[0m : 1.74234

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.75470
[1mStep[0m  [2/21], [94mLoss[0m : 1.65125
[1mStep[0m  [4/21], [94mLoss[0m : 1.77015
[1mStep[0m  [6/21], [94mLoss[0m : 1.73157
[1mStep[0m  [8/21], [94mLoss[0m : 1.72127
[1mStep[0m  [10/21], [94mLoss[0m : 1.67451
[1mStep[0m  [12/21], [94mLoss[0m : 1.64306
[1mStep[0m  [14/21], [94mLoss[0m : 1.69880
[1mStep[0m  [16/21], [94mLoss[0m : 1.66786
[1mStep[0m  [18/21], [94mLoss[0m : 1.64847
[1mStep[0m  [20/21], [94mLoss[0m : 1.72201

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.690, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.60348
[1mStep[0m  [2/21], [94mLoss[0m : 1.54322
[1mStep[0m  [4/21], [94mLoss[0m : 1.58115
[1mStep[0m  [6/21], [94mLoss[0m : 1.70434
[1mStep[0m  [8/21], [94mLoss[0m : 1.53514
[1mStep[0m  [10/21], [94mLoss[0m : 1.56500
[1mStep[0m  [12/21], [94mLoss[0m : 1.70650
[1mStep[0m  [14/21], [94mLoss[0m : 1.76804
[1mStep[0m  [16/21], [94mLoss[0m : 1.73187
[1mStep[0m  [18/21], [94mLoss[0m : 1.72306
[1mStep[0m  [20/21], [94mLoss[0m : 1.73983

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.67138
[1mStep[0m  [2/21], [94mLoss[0m : 1.69940
[1mStep[0m  [4/21], [94mLoss[0m : 1.64045
[1mStep[0m  [6/21], [94mLoss[0m : 1.56553
[1mStep[0m  [8/21], [94mLoss[0m : 1.64126
[1mStep[0m  [10/21], [94mLoss[0m : 1.69840
[1mStep[0m  [12/21], [94mLoss[0m : 1.47393
[1mStep[0m  [14/21], [94mLoss[0m : 1.57554
[1mStep[0m  [16/21], [94mLoss[0m : 1.64478
[1mStep[0m  [18/21], [94mLoss[0m : 1.75061
[1mStep[0m  [20/21], [94mLoss[0m : 1.63536

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61701
[1mStep[0m  [2/21], [94mLoss[0m : 1.60205
[1mStep[0m  [4/21], [94mLoss[0m : 1.48868
[1mStep[0m  [6/21], [94mLoss[0m : 1.56008
[1mStep[0m  [8/21], [94mLoss[0m : 1.68387
[1mStep[0m  [10/21], [94mLoss[0m : 1.59983
[1mStep[0m  [12/21], [94mLoss[0m : 1.66630
[1mStep[0m  [14/21], [94mLoss[0m : 1.59587
[1mStep[0m  [16/21], [94mLoss[0m : 1.57814
[1mStep[0m  [18/21], [94mLoss[0m : 1.66959
[1mStep[0m  [20/21], [94mLoss[0m : 1.77550

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51860
[1mStep[0m  [2/21], [94mLoss[0m : 1.50032
[1mStep[0m  [4/21], [94mLoss[0m : 1.56775
[1mStep[0m  [6/21], [94mLoss[0m : 1.55508
[1mStep[0m  [8/21], [94mLoss[0m : 1.55236
[1mStep[0m  [10/21], [94mLoss[0m : 1.52840
[1mStep[0m  [12/21], [94mLoss[0m : 1.53010
[1mStep[0m  [14/21], [94mLoss[0m : 1.52171
[1mStep[0m  [16/21], [94mLoss[0m : 1.59514
[1mStep[0m  [18/21], [94mLoss[0m : 1.50272
[1mStep[0m  [20/21], [94mLoss[0m : 1.72000

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.56121
[1mStep[0m  [2/21], [94mLoss[0m : 1.62026
[1mStep[0m  [4/21], [94mLoss[0m : 1.52540
[1mStep[0m  [6/21], [94mLoss[0m : 1.60814
[1mStep[0m  [8/21], [94mLoss[0m : 1.47944
[1mStep[0m  [10/21], [94mLoss[0m : 1.54858
[1mStep[0m  [12/21], [94mLoss[0m : 1.63786
[1mStep[0m  [14/21], [94mLoss[0m : 1.58822
[1mStep[0m  [16/21], [94mLoss[0m : 1.53992
[1mStep[0m  [18/21], [94mLoss[0m : 1.55327
[1mStep[0m  [20/21], [94mLoss[0m : 1.72010

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.50485
[1mStep[0m  [2/21], [94mLoss[0m : 1.49019
[1mStep[0m  [4/21], [94mLoss[0m : 1.47480
[1mStep[0m  [6/21], [94mLoss[0m : 1.45130
[1mStep[0m  [8/21], [94mLoss[0m : 1.55435
[1mStep[0m  [10/21], [94mLoss[0m : 1.60200
[1mStep[0m  [12/21], [94mLoss[0m : 1.56403
[1mStep[0m  [14/21], [94mLoss[0m : 1.46893
[1mStep[0m  [16/21], [94mLoss[0m : 1.55510
[1mStep[0m  [18/21], [94mLoss[0m : 1.60506
[1mStep[0m  [20/21], [94mLoss[0m : 1.52546

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.601, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.38989
[1mStep[0m  [2/21], [94mLoss[0m : 1.36976
[1mStep[0m  [4/21], [94mLoss[0m : 1.36456
[1mStep[0m  [6/21], [94mLoss[0m : 1.45257
[1mStep[0m  [8/21], [94mLoss[0m : 1.53288
[1mStep[0m  [10/21], [94mLoss[0m : 1.57891
[1mStep[0m  [12/21], [94mLoss[0m : 1.54593
[1mStep[0m  [14/21], [94mLoss[0m : 1.54529
[1mStep[0m  [16/21], [94mLoss[0m : 1.51000
[1mStep[0m  [18/21], [94mLoss[0m : 1.41596
[1mStep[0m  [20/21], [94mLoss[0m : 1.55887

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46061
[1mStep[0m  [2/21], [94mLoss[0m : 1.32983
[1mStep[0m  [4/21], [94mLoss[0m : 1.41873
[1mStep[0m  [6/21], [94mLoss[0m : 1.38685
[1mStep[0m  [8/21], [94mLoss[0m : 1.43225
[1mStep[0m  [10/21], [94mLoss[0m : 1.44252
[1mStep[0m  [12/21], [94mLoss[0m : 1.44987
[1mStep[0m  [14/21], [94mLoss[0m : 1.45815
[1mStep[0m  [16/21], [94mLoss[0m : 1.45339
[1mStep[0m  [18/21], [94mLoss[0m : 1.48257
[1mStep[0m  [20/21], [94mLoss[0m : 1.39805

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.419, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.34733
[1mStep[0m  [2/21], [94mLoss[0m : 1.41098
[1mStep[0m  [4/21], [94mLoss[0m : 1.39879
[1mStep[0m  [6/21], [94mLoss[0m : 1.42986
[1mStep[0m  [8/21], [94mLoss[0m : 1.49267
[1mStep[0m  [10/21], [94mLoss[0m : 1.46138
[1mStep[0m  [12/21], [94mLoss[0m : 1.45740
[1mStep[0m  [14/21], [94mLoss[0m : 1.42097
[1mStep[0m  [16/21], [94mLoss[0m : 1.40752
[1mStep[0m  [18/21], [94mLoss[0m : 1.45135
[1mStep[0m  [20/21], [94mLoss[0m : 1.45348

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.472, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.31537
[1mStep[0m  [2/21], [94mLoss[0m : 1.34689
[1mStep[0m  [4/21], [94mLoss[0m : 1.33494
[1mStep[0m  [6/21], [94mLoss[0m : 1.35188
[1mStep[0m  [8/21], [94mLoss[0m : 1.33218
[1mStep[0m  [10/21], [94mLoss[0m : 1.39484
[1mStep[0m  [12/21], [94mLoss[0m : 1.38573
[1mStep[0m  [14/21], [94mLoss[0m : 1.38098
[1mStep[0m  [16/21], [94mLoss[0m : 1.49364
[1mStep[0m  [18/21], [94mLoss[0m : 1.35980
[1mStep[0m  [20/21], [94mLoss[0m : 1.46921

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.391, [92mTest[0m: 2.538, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.484
====================================

Phase 2 - Evaluation MAE:  2.483989953994751
MAE score P1        2.31208
MAE score P2        2.48399
loss               1.391161
learning_rate       0.00505
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.97314
[1mStep[0m  [2/21], [94mLoss[0m : 10.57519
[1mStep[0m  [4/21], [94mLoss[0m : 10.44965
[1mStep[0m  [6/21], [94mLoss[0m : 10.21157
[1mStep[0m  [8/21], [94mLoss[0m : 9.70679
[1mStep[0m  [10/21], [94mLoss[0m : 9.37542
[1mStep[0m  [12/21], [94mLoss[0m : 9.12121
[1mStep[0m  [14/21], [94mLoss[0m : 8.97171
[1mStep[0m  [16/21], [94mLoss[0m : 8.89982
[1mStep[0m  [18/21], [94mLoss[0m : 8.50988
[1mStep[0m  [20/21], [94mLoss[0m : 8.34692

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.566, [92mTest[0m: 10.775, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.50219
[1mStep[0m  [2/21], [94mLoss[0m : 7.88169
[1mStep[0m  [4/21], [94mLoss[0m : 7.49150
[1mStep[0m  [6/21], [94mLoss[0m : 7.05111
[1mStep[0m  [8/21], [94mLoss[0m : 6.92806
[1mStep[0m  [10/21], [94mLoss[0m : 7.03643
[1mStep[0m  [12/21], [94mLoss[0m : 6.10055
[1mStep[0m  [14/21], [94mLoss[0m : 6.03331
[1mStep[0m  [16/21], [94mLoss[0m : 5.85162
[1mStep[0m  [18/21], [94mLoss[0m : 5.82437
[1mStep[0m  [20/21], [94mLoss[0m : 5.39026

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.710, [92mTest[0m: 8.076, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.20072
[1mStep[0m  [2/21], [94mLoss[0m : 4.89447
[1mStep[0m  [4/21], [94mLoss[0m : 4.71459
[1mStep[0m  [6/21], [94mLoss[0m : 4.64334
[1mStep[0m  [8/21], [94mLoss[0m : 4.32569
[1mStep[0m  [10/21], [94mLoss[0m : 4.15301
[1mStep[0m  [12/21], [94mLoss[0m : 4.23764
[1mStep[0m  [14/21], [94mLoss[0m : 3.88647
[1mStep[0m  [16/21], [94mLoss[0m : 3.80908
[1mStep[0m  [18/21], [94mLoss[0m : 3.72182
[1mStep[0m  [20/21], [94mLoss[0m : 4.08353

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.347, [92mTest[0m: 5.272, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.70284
[1mStep[0m  [2/21], [94mLoss[0m : 3.42384
[1mStep[0m  [4/21], [94mLoss[0m : 3.43079
[1mStep[0m  [6/21], [94mLoss[0m : 3.59659
[1mStep[0m  [8/21], [94mLoss[0m : 3.50351
[1mStep[0m  [10/21], [94mLoss[0m : 3.42677
[1mStep[0m  [12/21], [94mLoss[0m : 2.91941
[1mStep[0m  [14/21], [94mLoss[0m : 3.10553
[1mStep[0m  [16/21], [94mLoss[0m : 3.22263
[1mStep[0m  [18/21], [94mLoss[0m : 3.10406
[1mStep[0m  [20/21], [94mLoss[0m : 3.10498

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.294, [92mTest[0m: 3.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.99678
[1mStep[0m  [2/21], [94mLoss[0m : 2.78248
[1mStep[0m  [4/21], [94mLoss[0m : 3.10624
[1mStep[0m  [6/21], [94mLoss[0m : 3.01843
[1mStep[0m  [8/21], [94mLoss[0m : 2.97802
[1mStep[0m  [10/21], [94mLoss[0m : 3.13365
[1mStep[0m  [12/21], [94mLoss[0m : 2.78462
[1mStep[0m  [14/21], [94mLoss[0m : 2.80900
[1mStep[0m  [16/21], [94mLoss[0m : 3.01745
[1mStep[0m  [18/21], [94mLoss[0m : 2.72160
[1mStep[0m  [20/21], [94mLoss[0m : 2.76206

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.918, [92mTest[0m: 2.827, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61775
[1mStep[0m  [2/21], [94mLoss[0m : 2.70510
[1mStep[0m  [4/21], [94mLoss[0m : 2.86620
[1mStep[0m  [6/21], [94mLoss[0m : 2.62233
[1mStep[0m  [8/21], [94mLoss[0m : 2.60149
[1mStep[0m  [10/21], [94mLoss[0m : 2.63234
[1mStep[0m  [12/21], [94mLoss[0m : 2.79407
[1mStep[0m  [14/21], [94mLoss[0m : 2.93025
[1mStep[0m  [16/21], [94mLoss[0m : 2.97214
[1mStep[0m  [18/21], [94mLoss[0m : 2.93467
[1mStep[0m  [20/21], [94mLoss[0m : 2.81358

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.766, [92mTest[0m: 2.568, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68468
[1mStep[0m  [2/21], [94mLoss[0m : 2.71765
[1mStep[0m  [4/21], [94mLoss[0m : 2.82016
[1mStep[0m  [6/21], [94mLoss[0m : 2.72666
[1mStep[0m  [8/21], [94mLoss[0m : 2.74315
[1mStep[0m  [10/21], [94mLoss[0m : 2.77832
[1mStep[0m  [12/21], [94mLoss[0m : 2.75230
[1mStep[0m  [14/21], [94mLoss[0m : 2.76581
[1mStep[0m  [16/21], [94mLoss[0m : 2.73028
[1mStep[0m  [18/21], [94mLoss[0m : 2.49043
[1mStep[0m  [20/21], [94mLoss[0m : 2.54344

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51266
[1mStep[0m  [2/21], [94mLoss[0m : 2.91272
[1mStep[0m  [4/21], [94mLoss[0m : 2.73683
[1mStep[0m  [6/21], [94mLoss[0m : 2.72445
[1mStep[0m  [8/21], [94mLoss[0m : 2.75226
[1mStep[0m  [10/21], [94mLoss[0m : 2.63567
[1mStep[0m  [12/21], [94mLoss[0m : 2.66071
[1mStep[0m  [14/21], [94mLoss[0m : 2.66530
[1mStep[0m  [16/21], [94mLoss[0m : 2.67137
[1mStep[0m  [18/21], [94mLoss[0m : 2.75097
[1mStep[0m  [20/21], [94mLoss[0m : 2.59386

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83753
[1mStep[0m  [2/21], [94mLoss[0m : 2.76016
[1mStep[0m  [4/21], [94mLoss[0m : 2.69788
[1mStep[0m  [6/21], [94mLoss[0m : 2.85263
[1mStep[0m  [8/21], [94mLoss[0m : 2.60643
[1mStep[0m  [10/21], [94mLoss[0m : 2.82028
[1mStep[0m  [12/21], [94mLoss[0m : 2.65968
[1mStep[0m  [14/21], [94mLoss[0m : 2.56012
[1mStep[0m  [16/21], [94mLoss[0m : 2.72015
[1mStep[0m  [18/21], [94mLoss[0m : 2.81611
[1mStep[0m  [20/21], [94mLoss[0m : 2.81252

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76873
[1mStep[0m  [2/21], [94mLoss[0m : 2.60947
[1mStep[0m  [4/21], [94mLoss[0m : 2.69176
[1mStep[0m  [6/21], [94mLoss[0m : 2.58047
[1mStep[0m  [8/21], [94mLoss[0m : 2.77686
[1mStep[0m  [10/21], [94mLoss[0m : 2.85595
[1mStep[0m  [12/21], [94mLoss[0m : 2.76482
[1mStep[0m  [14/21], [94mLoss[0m : 2.78796
[1mStep[0m  [16/21], [94mLoss[0m : 2.69519
[1mStep[0m  [18/21], [94mLoss[0m : 2.75028
[1mStep[0m  [20/21], [94mLoss[0m : 2.68611

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66131
[1mStep[0m  [2/21], [94mLoss[0m : 2.57630
[1mStep[0m  [4/21], [94mLoss[0m : 2.62739
[1mStep[0m  [6/21], [94mLoss[0m : 2.86803
[1mStep[0m  [8/21], [94mLoss[0m : 2.76354
[1mStep[0m  [10/21], [94mLoss[0m : 2.56020
[1mStep[0m  [12/21], [94mLoss[0m : 2.68527
[1mStep[0m  [14/21], [94mLoss[0m : 2.55343
[1mStep[0m  [16/21], [94mLoss[0m : 2.63519
[1mStep[0m  [18/21], [94mLoss[0m : 2.91604
[1mStep[0m  [20/21], [94mLoss[0m : 2.69129

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70622
[1mStep[0m  [2/21], [94mLoss[0m : 2.73531
[1mStep[0m  [4/21], [94mLoss[0m : 2.66221
[1mStep[0m  [6/21], [94mLoss[0m : 2.54960
[1mStep[0m  [8/21], [94mLoss[0m : 2.78479
[1mStep[0m  [10/21], [94mLoss[0m : 2.86935
[1mStep[0m  [12/21], [94mLoss[0m : 2.75175
[1mStep[0m  [14/21], [94mLoss[0m : 2.56606
[1mStep[0m  [16/21], [94mLoss[0m : 2.55367
[1mStep[0m  [18/21], [94mLoss[0m : 2.65031
[1mStep[0m  [20/21], [94mLoss[0m : 2.63191

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82763
[1mStep[0m  [2/21], [94mLoss[0m : 2.55017
[1mStep[0m  [4/21], [94mLoss[0m : 2.54747
[1mStep[0m  [6/21], [94mLoss[0m : 2.66492
[1mStep[0m  [8/21], [94mLoss[0m : 2.62861
[1mStep[0m  [10/21], [94mLoss[0m : 2.63691
[1mStep[0m  [12/21], [94mLoss[0m : 2.62779
[1mStep[0m  [14/21], [94mLoss[0m : 2.71641
[1mStep[0m  [16/21], [94mLoss[0m : 2.68060
[1mStep[0m  [18/21], [94mLoss[0m : 2.61133
[1mStep[0m  [20/21], [94mLoss[0m : 2.80120

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69373
[1mStep[0m  [2/21], [94mLoss[0m : 2.63043
[1mStep[0m  [4/21], [94mLoss[0m : 2.64928
[1mStep[0m  [6/21], [94mLoss[0m : 2.66093
[1mStep[0m  [8/21], [94mLoss[0m : 2.73362
[1mStep[0m  [10/21], [94mLoss[0m : 2.49223
[1mStep[0m  [12/21], [94mLoss[0m : 2.70516
[1mStep[0m  [14/21], [94mLoss[0m : 2.69838
[1mStep[0m  [16/21], [94mLoss[0m : 2.67560
[1mStep[0m  [18/21], [94mLoss[0m : 2.68111
[1mStep[0m  [20/21], [94mLoss[0m : 2.66622

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80912
[1mStep[0m  [2/21], [94mLoss[0m : 2.61716
[1mStep[0m  [4/21], [94mLoss[0m : 2.53285
[1mStep[0m  [6/21], [94mLoss[0m : 2.63805
[1mStep[0m  [8/21], [94mLoss[0m : 2.72162
[1mStep[0m  [10/21], [94mLoss[0m : 2.81403
[1mStep[0m  [12/21], [94mLoss[0m : 2.75112
[1mStep[0m  [14/21], [94mLoss[0m : 2.79454
[1mStep[0m  [16/21], [94mLoss[0m : 2.57057
[1mStep[0m  [18/21], [94mLoss[0m : 2.51942
[1mStep[0m  [20/21], [94mLoss[0m : 2.82330

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53690
[1mStep[0m  [2/21], [94mLoss[0m : 2.64086
[1mStep[0m  [4/21], [94mLoss[0m : 2.57117
[1mStep[0m  [6/21], [94mLoss[0m : 2.60804
[1mStep[0m  [8/21], [94mLoss[0m : 2.78530
[1mStep[0m  [10/21], [94mLoss[0m : 2.64954
[1mStep[0m  [12/21], [94mLoss[0m : 2.77934
[1mStep[0m  [14/21], [94mLoss[0m : 2.65490
[1mStep[0m  [16/21], [94mLoss[0m : 2.72613
[1mStep[0m  [18/21], [94mLoss[0m : 2.60412
[1mStep[0m  [20/21], [94mLoss[0m : 2.58773

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76401
[1mStep[0m  [2/21], [94mLoss[0m : 2.57392
[1mStep[0m  [4/21], [94mLoss[0m : 2.97667
[1mStep[0m  [6/21], [94mLoss[0m : 2.81550
[1mStep[0m  [8/21], [94mLoss[0m : 2.83167
[1mStep[0m  [10/21], [94mLoss[0m : 2.67274
[1mStep[0m  [12/21], [94mLoss[0m : 2.66596
[1mStep[0m  [14/21], [94mLoss[0m : 2.57376
[1mStep[0m  [16/21], [94mLoss[0m : 2.52154
[1mStep[0m  [18/21], [94mLoss[0m : 2.77643
[1mStep[0m  [20/21], [94mLoss[0m : 2.61287

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55560
[1mStep[0m  [2/21], [94mLoss[0m : 2.77451
[1mStep[0m  [4/21], [94mLoss[0m : 2.70827
[1mStep[0m  [6/21], [94mLoss[0m : 2.59551
[1mStep[0m  [8/21], [94mLoss[0m : 2.50466
[1mStep[0m  [10/21], [94mLoss[0m : 2.76760
[1mStep[0m  [12/21], [94mLoss[0m : 2.66236
[1mStep[0m  [14/21], [94mLoss[0m : 2.64095
[1mStep[0m  [16/21], [94mLoss[0m : 2.72178
[1mStep[0m  [18/21], [94mLoss[0m : 2.47333
[1mStep[0m  [20/21], [94mLoss[0m : 2.65351

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65692
[1mStep[0m  [2/21], [94mLoss[0m : 2.67207
[1mStep[0m  [4/21], [94mLoss[0m : 2.63061
[1mStep[0m  [6/21], [94mLoss[0m : 2.49887
[1mStep[0m  [8/21], [94mLoss[0m : 2.54452
[1mStep[0m  [10/21], [94mLoss[0m : 2.50694
[1mStep[0m  [12/21], [94mLoss[0m : 2.79070
[1mStep[0m  [14/21], [94mLoss[0m : 2.65071
[1mStep[0m  [16/21], [94mLoss[0m : 2.70974
[1mStep[0m  [18/21], [94mLoss[0m : 2.63648
[1mStep[0m  [20/21], [94mLoss[0m : 2.64178

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49726
[1mStep[0m  [2/21], [94mLoss[0m : 2.78251
[1mStep[0m  [4/21], [94mLoss[0m : 2.72868
[1mStep[0m  [6/21], [94mLoss[0m : 2.61003
[1mStep[0m  [8/21], [94mLoss[0m : 2.69380
[1mStep[0m  [10/21], [94mLoss[0m : 2.77119
[1mStep[0m  [12/21], [94mLoss[0m : 2.43898
[1mStep[0m  [14/21], [94mLoss[0m : 2.58619
[1mStep[0m  [16/21], [94mLoss[0m : 2.50025
[1mStep[0m  [18/21], [94mLoss[0m : 2.77159
[1mStep[0m  [20/21], [94mLoss[0m : 2.63842

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66181
[1mStep[0m  [2/21], [94mLoss[0m : 2.64470
[1mStep[0m  [4/21], [94mLoss[0m : 2.65170
[1mStep[0m  [6/21], [94mLoss[0m : 2.61317
[1mStep[0m  [8/21], [94mLoss[0m : 2.53018
[1mStep[0m  [10/21], [94mLoss[0m : 2.62867
[1mStep[0m  [12/21], [94mLoss[0m : 2.58350
[1mStep[0m  [14/21], [94mLoss[0m : 2.74828
[1mStep[0m  [16/21], [94mLoss[0m : 2.76639
[1mStep[0m  [18/21], [94mLoss[0m : 2.68041
[1mStep[0m  [20/21], [94mLoss[0m : 2.60442

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.377, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46284
[1mStep[0m  [2/21], [94mLoss[0m : 2.55779
[1mStep[0m  [4/21], [94mLoss[0m : 2.57702
[1mStep[0m  [6/21], [94mLoss[0m : 2.55443
[1mStep[0m  [8/21], [94mLoss[0m : 2.62124
[1mStep[0m  [10/21], [94mLoss[0m : 2.70564
[1mStep[0m  [12/21], [94mLoss[0m : 2.49064
[1mStep[0m  [14/21], [94mLoss[0m : 2.66506
[1mStep[0m  [16/21], [94mLoss[0m : 2.45699
[1mStep[0m  [18/21], [94mLoss[0m : 2.63218
[1mStep[0m  [20/21], [94mLoss[0m : 2.55904

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.378, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81150
[1mStep[0m  [2/21], [94mLoss[0m : 2.65301
[1mStep[0m  [4/21], [94mLoss[0m : 2.56196
[1mStep[0m  [6/21], [94mLoss[0m : 2.66573
[1mStep[0m  [8/21], [94mLoss[0m : 2.62667
[1mStep[0m  [10/21], [94mLoss[0m : 2.69938
[1mStep[0m  [12/21], [94mLoss[0m : 2.79350
[1mStep[0m  [14/21], [94mLoss[0m : 2.68609
[1mStep[0m  [16/21], [94mLoss[0m : 2.54661
[1mStep[0m  [18/21], [94mLoss[0m : 2.67609
[1mStep[0m  [20/21], [94mLoss[0m : 2.72198

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73499
[1mStep[0m  [2/21], [94mLoss[0m : 2.65985
[1mStep[0m  [4/21], [94mLoss[0m : 2.60631
[1mStep[0m  [6/21], [94mLoss[0m : 2.74812
[1mStep[0m  [8/21], [94mLoss[0m : 2.56964
[1mStep[0m  [10/21], [94mLoss[0m : 2.57111
[1mStep[0m  [12/21], [94mLoss[0m : 2.60094
[1mStep[0m  [14/21], [94mLoss[0m : 2.51010
[1mStep[0m  [16/21], [94mLoss[0m : 2.65797
[1mStep[0m  [18/21], [94mLoss[0m : 2.70349
[1mStep[0m  [20/21], [94mLoss[0m : 2.71817

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.374, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66143
[1mStep[0m  [2/21], [94mLoss[0m : 2.69530
[1mStep[0m  [4/21], [94mLoss[0m : 2.55273
[1mStep[0m  [6/21], [94mLoss[0m : 2.73473
[1mStep[0m  [8/21], [94mLoss[0m : 2.49913
[1mStep[0m  [10/21], [94mLoss[0m : 2.76745
[1mStep[0m  [12/21], [94mLoss[0m : 2.68809
[1mStep[0m  [14/21], [94mLoss[0m : 2.59101
[1mStep[0m  [16/21], [94mLoss[0m : 2.81331
[1mStep[0m  [18/21], [94mLoss[0m : 2.47719
[1mStep[0m  [20/21], [94mLoss[0m : 2.61943

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49630
[1mStep[0m  [2/21], [94mLoss[0m : 2.58742
[1mStep[0m  [4/21], [94mLoss[0m : 2.65491
[1mStep[0m  [6/21], [94mLoss[0m : 2.56892
[1mStep[0m  [8/21], [94mLoss[0m : 2.70378
[1mStep[0m  [10/21], [94mLoss[0m : 2.68694
[1mStep[0m  [12/21], [94mLoss[0m : 2.44910
[1mStep[0m  [14/21], [94mLoss[0m : 2.71566
[1mStep[0m  [16/21], [94mLoss[0m : 2.61289
[1mStep[0m  [18/21], [94mLoss[0m : 2.64897
[1mStep[0m  [20/21], [94mLoss[0m : 2.57094

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.369, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.86116
[1mStep[0m  [2/21], [94mLoss[0m : 2.53749
[1mStep[0m  [4/21], [94mLoss[0m : 2.59738
[1mStep[0m  [6/21], [94mLoss[0m : 2.84044
[1mStep[0m  [8/21], [94mLoss[0m : 2.50367
[1mStep[0m  [10/21], [94mLoss[0m : 2.64251
[1mStep[0m  [12/21], [94mLoss[0m : 2.65299
[1mStep[0m  [14/21], [94mLoss[0m : 2.57667
[1mStep[0m  [16/21], [94mLoss[0m : 2.65529
[1mStep[0m  [18/21], [94mLoss[0m : 2.82906
[1mStep[0m  [20/21], [94mLoss[0m : 2.66563

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.372, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40198
[1mStep[0m  [2/21], [94mLoss[0m : 2.51552
[1mStep[0m  [4/21], [94mLoss[0m : 2.71866
[1mStep[0m  [6/21], [94mLoss[0m : 2.72775
[1mStep[0m  [8/21], [94mLoss[0m : 2.50208
[1mStep[0m  [10/21], [94mLoss[0m : 2.54267
[1mStep[0m  [12/21], [94mLoss[0m : 2.59054
[1mStep[0m  [14/21], [94mLoss[0m : 2.58639
[1mStep[0m  [16/21], [94mLoss[0m : 2.77781
[1mStep[0m  [18/21], [94mLoss[0m : 2.70295
[1mStep[0m  [20/21], [94mLoss[0m : 2.64269

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.372, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50538
[1mStep[0m  [2/21], [94mLoss[0m : 2.64924
[1mStep[0m  [4/21], [94mLoss[0m : 2.63830
[1mStep[0m  [6/21], [94mLoss[0m : 2.63853
[1mStep[0m  [8/21], [94mLoss[0m : 2.72229
[1mStep[0m  [10/21], [94mLoss[0m : 2.62340
[1mStep[0m  [12/21], [94mLoss[0m : 2.57690
[1mStep[0m  [14/21], [94mLoss[0m : 2.58525
[1mStep[0m  [16/21], [94mLoss[0m : 2.50093
[1mStep[0m  [18/21], [94mLoss[0m : 2.47737
[1mStep[0m  [20/21], [94mLoss[0m : 2.71810

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74372
[1mStep[0m  [2/21], [94mLoss[0m : 2.50724
[1mStep[0m  [4/21], [94mLoss[0m : 2.57745
[1mStep[0m  [6/21], [94mLoss[0m : 2.59914
[1mStep[0m  [8/21], [94mLoss[0m : 2.72679
[1mStep[0m  [10/21], [94mLoss[0m : 2.59445
[1mStep[0m  [12/21], [94mLoss[0m : 2.64069
[1mStep[0m  [14/21], [94mLoss[0m : 2.63517
[1mStep[0m  [16/21], [94mLoss[0m : 2.62221
[1mStep[0m  [18/21], [94mLoss[0m : 2.59979
[1mStep[0m  [20/21], [94mLoss[0m : 2.75879

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.367, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.365
====================================

Phase 1 - Evaluation MAE:  2.36453914642334
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.65641
[1mStep[0m  [2/21], [94mLoss[0m : 2.74478
[1mStep[0m  [4/21], [94mLoss[0m : 2.51042
[1mStep[0m  [6/21], [94mLoss[0m : 2.77179
[1mStep[0m  [8/21], [94mLoss[0m : 2.65945
[1mStep[0m  [10/21], [94mLoss[0m : 2.72238
[1mStep[0m  [12/21], [94mLoss[0m : 2.74887
[1mStep[0m  [14/21], [94mLoss[0m : 2.63506
[1mStep[0m  [16/21], [94mLoss[0m : 2.65532
[1mStep[0m  [18/21], [94mLoss[0m : 2.73953
[1mStep[0m  [20/21], [94mLoss[0m : 2.59259

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50444
[1mStep[0m  [2/21], [94mLoss[0m : 2.57403
[1mStep[0m  [4/21], [94mLoss[0m : 2.65088
[1mStep[0m  [6/21], [94mLoss[0m : 2.55656
[1mStep[0m  [8/21], [94mLoss[0m : 2.57599
[1mStep[0m  [10/21], [94mLoss[0m : 2.60080
[1mStep[0m  [12/21], [94mLoss[0m : 2.55102
[1mStep[0m  [14/21], [94mLoss[0m : 2.64950
[1mStep[0m  [16/21], [94mLoss[0m : 2.54700
[1mStep[0m  [18/21], [94mLoss[0m : 2.69682
[1mStep[0m  [20/21], [94mLoss[0m : 2.57097

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72663
[1mStep[0m  [2/21], [94mLoss[0m : 2.59305
[1mStep[0m  [4/21], [94mLoss[0m : 2.59816
[1mStep[0m  [6/21], [94mLoss[0m : 2.54281
[1mStep[0m  [8/21], [94mLoss[0m : 2.58591
[1mStep[0m  [10/21], [94mLoss[0m : 2.57160
[1mStep[0m  [12/21], [94mLoss[0m : 2.64489
[1mStep[0m  [14/21], [94mLoss[0m : 2.63384
[1mStep[0m  [16/21], [94mLoss[0m : 2.61574
[1mStep[0m  [18/21], [94mLoss[0m : 2.61972
[1mStep[0m  [20/21], [94mLoss[0m : 2.57816

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59957
[1mStep[0m  [2/21], [94mLoss[0m : 2.55166
[1mStep[0m  [4/21], [94mLoss[0m : 2.59892
[1mStep[0m  [6/21], [94mLoss[0m : 2.78595
[1mStep[0m  [8/21], [94mLoss[0m : 2.40597
[1mStep[0m  [10/21], [94mLoss[0m : 2.46919
[1mStep[0m  [12/21], [94mLoss[0m : 2.71899
[1mStep[0m  [14/21], [94mLoss[0m : 2.64175
[1mStep[0m  [16/21], [94mLoss[0m : 2.44229
[1mStep[0m  [18/21], [94mLoss[0m : 2.67823
[1mStep[0m  [20/21], [94mLoss[0m : 2.55431

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72618
[1mStep[0m  [2/21], [94mLoss[0m : 2.46789
[1mStep[0m  [4/21], [94mLoss[0m : 2.55638
[1mStep[0m  [6/21], [94mLoss[0m : 2.65834
[1mStep[0m  [8/21], [94mLoss[0m : 2.61379
[1mStep[0m  [10/21], [94mLoss[0m : 2.61042
[1mStep[0m  [12/21], [94mLoss[0m : 2.68176
[1mStep[0m  [14/21], [94mLoss[0m : 2.66080
[1mStep[0m  [16/21], [94mLoss[0m : 2.62986
[1mStep[0m  [18/21], [94mLoss[0m : 2.66632
[1mStep[0m  [20/21], [94mLoss[0m : 2.71440

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68098
[1mStep[0m  [2/21], [94mLoss[0m : 2.59573
[1mStep[0m  [4/21], [94mLoss[0m : 2.49750
[1mStep[0m  [6/21], [94mLoss[0m : 2.61990
[1mStep[0m  [8/21], [94mLoss[0m : 2.54748
[1mStep[0m  [10/21], [94mLoss[0m : 2.66746
[1mStep[0m  [12/21], [94mLoss[0m : 2.68852
[1mStep[0m  [14/21], [94mLoss[0m : 2.52443
[1mStep[0m  [16/21], [94mLoss[0m : 2.65262
[1mStep[0m  [18/21], [94mLoss[0m : 2.59084
[1mStep[0m  [20/21], [94mLoss[0m : 2.59282

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58629
[1mStep[0m  [2/21], [94mLoss[0m : 2.67591
[1mStep[0m  [4/21], [94mLoss[0m : 2.53582
[1mStep[0m  [6/21], [94mLoss[0m : 2.46057
[1mStep[0m  [8/21], [94mLoss[0m : 2.64079
[1mStep[0m  [10/21], [94mLoss[0m : 2.39732
[1mStep[0m  [12/21], [94mLoss[0m : 2.58422
[1mStep[0m  [14/21], [94mLoss[0m : 2.52455
[1mStep[0m  [16/21], [94mLoss[0m : 2.46407
[1mStep[0m  [18/21], [94mLoss[0m : 2.60365
[1mStep[0m  [20/21], [94mLoss[0m : 2.54385

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59799
[1mStep[0m  [2/21], [94mLoss[0m : 2.39265
[1mStep[0m  [4/21], [94mLoss[0m : 2.57884
[1mStep[0m  [6/21], [94mLoss[0m : 2.56492
[1mStep[0m  [8/21], [94mLoss[0m : 2.58720
[1mStep[0m  [10/21], [94mLoss[0m : 2.47427
[1mStep[0m  [12/21], [94mLoss[0m : 2.59952
[1mStep[0m  [14/21], [94mLoss[0m : 2.57777
[1mStep[0m  [16/21], [94mLoss[0m : 2.54500
[1mStep[0m  [18/21], [94mLoss[0m : 2.55728
[1mStep[0m  [20/21], [94mLoss[0m : 2.63306

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40604
[1mStep[0m  [2/21], [94mLoss[0m : 2.72098
[1mStep[0m  [4/21], [94mLoss[0m : 2.57067
[1mStep[0m  [6/21], [94mLoss[0m : 2.62911
[1mStep[0m  [8/21], [94mLoss[0m : 2.60701
[1mStep[0m  [10/21], [94mLoss[0m : 2.70280
[1mStep[0m  [12/21], [94mLoss[0m : 2.71977
[1mStep[0m  [14/21], [94mLoss[0m : 2.54039
[1mStep[0m  [16/21], [94mLoss[0m : 2.42695
[1mStep[0m  [18/21], [94mLoss[0m : 2.67525
[1mStep[0m  [20/21], [94mLoss[0m : 2.44451

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45533
[1mStep[0m  [2/21], [94mLoss[0m : 2.59559
[1mStep[0m  [4/21], [94mLoss[0m : 2.53218
[1mStep[0m  [6/21], [94mLoss[0m : 2.57122
[1mStep[0m  [8/21], [94mLoss[0m : 2.56035
[1mStep[0m  [10/21], [94mLoss[0m : 2.52295
[1mStep[0m  [12/21], [94mLoss[0m : 2.44260
[1mStep[0m  [14/21], [94mLoss[0m : 2.57858
[1mStep[0m  [16/21], [94mLoss[0m : 2.39325
[1mStep[0m  [18/21], [94mLoss[0m : 2.68336
[1mStep[0m  [20/21], [94mLoss[0m : 2.62485

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58039
[1mStep[0m  [2/21], [94mLoss[0m : 2.56788
[1mStep[0m  [4/21], [94mLoss[0m : 2.62658
[1mStep[0m  [6/21], [94mLoss[0m : 2.66922
[1mStep[0m  [8/21], [94mLoss[0m : 2.65355
[1mStep[0m  [10/21], [94mLoss[0m : 2.60742
[1mStep[0m  [12/21], [94mLoss[0m : 2.53445
[1mStep[0m  [14/21], [94mLoss[0m : 2.56873
[1mStep[0m  [16/21], [94mLoss[0m : 2.49197
[1mStep[0m  [18/21], [94mLoss[0m : 2.56620
[1mStep[0m  [20/21], [94mLoss[0m : 2.60905

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35239
[1mStep[0m  [2/21], [94mLoss[0m : 2.44184
[1mStep[0m  [4/21], [94mLoss[0m : 2.41955
[1mStep[0m  [6/21], [94mLoss[0m : 2.56922
[1mStep[0m  [8/21], [94mLoss[0m : 2.54259
[1mStep[0m  [10/21], [94mLoss[0m : 2.59635
[1mStep[0m  [12/21], [94mLoss[0m : 2.43037
[1mStep[0m  [14/21], [94mLoss[0m : 2.50890
[1mStep[0m  [16/21], [94mLoss[0m : 2.49862
[1mStep[0m  [18/21], [94mLoss[0m : 2.48264
[1mStep[0m  [20/21], [94mLoss[0m : 2.46781

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48257
[1mStep[0m  [2/21], [94mLoss[0m : 2.68480
[1mStep[0m  [4/21], [94mLoss[0m : 2.30869
[1mStep[0m  [6/21], [94mLoss[0m : 2.51291
[1mStep[0m  [8/21], [94mLoss[0m : 2.47635
[1mStep[0m  [10/21], [94mLoss[0m : 2.59608
[1mStep[0m  [12/21], [94mLoss[0m : 2.48387
[1mStep[0m  [14/21], [94mLoss[0m : 2.45307
[1mStep[0m  [16/21], [94mLoss[0m : 2.50422
[1mStep[0m  [18/21], [94mLoss[0m : 2.60752
[1mStep[0m  [20/21], [94mLoss[0m : 2.50174

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.321, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42942
[1mStep[0m  [2/21], [94mLoss[0m : 2.58998
[1mStep[0m  [4/21], [94mLoss[0m : 2.65002
[1mStep[0m  [6/21], [94mLoss[0m : 2.56638
[1mStep[0m  [8/21], [94mLoss[0m : 2.49805
[1mStep[0m  [10/21], [94mLoss[0m : 2.41515
[1mStep[0m  [12/21], [94mLoss[0m : 2.59873
[1mStep[0m  [14/21], [94mLoss[0m : 2.51551
[1mStep[0m  [16/21], [94mLoss[0m : 2.54499
[1mStep[0m  [18/21], [94mLoss[0m : 2.57505
[1mStep[0m  [20/21], [94mLoss[0m : 2.51153

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.319, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49572
[1mStep[0m  [2/21], [94mLoss[0m : 2.37544
[1mStep[0m  [4/21], [94mLoss[0m : 2.63567
[1mStep[0m  [6/21], [94mLoss[0m : 2.50682
[1mStep[0m  [8/21], [94mLoss[0m : 2.55560
[1mStep[0m  [10/21], [94mLoss[0m : 2.48945
[1mStep[0m  [12/21], [94mLoss[0m : 2.48511
[1mStep[0m  [14/21], [94mLoss[0m : 2.60612
[1mStep[0m  [16/21], [94mLoss[0m : 2.48677
[1mStep[0m  [18/21], [94mLoss[0m : 2.46263
[1mStep[0m  [20/21], [94mLoss[0m : 2.43000

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.319, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58604
[1mStep[0m  [2/21], [94mLoss[0m : 2.40802
[1mStep[0m  [4/21], [94mLoss[0m : 2.49970
[1mStep[0m  [6/21], [94mLoss[0m : 2.46023
[1mStep[0m  [8/21], [94mLoss[0m : 2.58311
[1mStep[0m  [10/21], [94mLoss[0m : 2.55296
[1mStep[0m  [12/21], [94mLoss[0m : 2.53221
[1mStep[0m  [14/21], [94mLoss[0m : 2.49099
[1mStep[0m  [16/21], [94mLoss[0m : 2.53924
[1mStep[0m  [18/21], [94mLoss[0m : 2.57934
[1mStep[0m  [20/21], [94mLoss[0m : 2.65865

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49380
[1mStep[0m  [2/21], [94mLoss[0m : 2.57446
[1mStep[0m  [4/21], [94mLoss[0m : 2.48254
[1mStep[0m  [6/21], [94mLoss[0m : 2.50952
[1mStep[0m  [8/21], [94mLoss[0m : 2.50479
[1mStep[0m  [10/21], [94mLoss[0m : 2.40299
[1mStep[0m  [12/21], [94mLoss[0m : 2.37575
[1mStep[0m  [14/21], [94mLoss[0m : 2.49201
[1mStep[0m  [16/21], [94mLoss[0m : 2.53954
[1mStep[0m  [18/21], [94mLoss[0m : 2.42274
[1mStep[0m  [20/21], [94mLoss[0m : 2.46632

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.318, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46849
[1mStep[0m  [2/21], [94mLoss[0m : 2.46145
[1mStep[0m  [4/21], [94mLoss[0m : 2.47503
[1mStep[0m  [6/21], [94mLoss[0m : 2.53610
[1mStep[0m  [8/21], [94mLoss[0m : 2.58062
[1mStep[0m  [10/21], [94mLoss[0m : 2.49955
[1mStep[0m  [12/21], [94mLoss[0m : 2.63402
[1mStep[0m  [14/21], [94mLoss[0m : 2.41782
[1mStep[0m  [16/21], [94mLoss[0m : 2.53381
[1mStep[0m  [18/21], [94mLoss[0m : 2.51384
[1mStep[0m  [20/21], [94mLoss[0m : 2.36175

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38514
[1mStep[0m  [2/21], [94mLoss[0m : 2.46528
[1mStep[0m  [4/21], [94mLoss[0m : 2.41020
[1mStep[0m  [6/21], [94mLoss[0m : 2.44958
[1mStep[0m  [8/21], [94mLoss[0m : 2.61637
[1mStep[0m  [10/21], [94mLoss[0m : 2.39229
[1mStep[0m  [12/21], [94mLoss[0m : 2.48236
[1mStep[0m  [14/21], [94mLoss[0m : 2.52724
[1mStep[0m  [16/21], [94mLoss[0m : 2.44579
[1mStep[0m  [18/21], [94mLoss[0m : 2.43775
[1mStep[0m  [20/21], [94mLoss[0m : 2.57048

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55572
[1mStep[0m  [2/21], [94mLoss[0m : 2.49463
[1mStep[0m  [4/21], [94mLoss[0m : 2.39962
[1mStep[0m  [6/21], [94mLoss[0m : 2.44604
[1mStep[0m  [8/21], [94mLoss[0m : 2.41404
[1mStep[0m  [10/21], [94mLoss[0m : 2.40166
[1mStep[0m  [12/21], [94mLoss[0m : 2.35118
[1mStep[0m  [14/21], [94mLoss[0m : 2.44378
[1mStep[0m  [16/21], [94mLoss[0m : 2.44818
[1mStep[0m  [18/21], [94mLoss[0m : 2.52081
[1mStep[0m  [20/21], [94mLoss[0m : 2.40904

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.317, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45859
[1mStep[0m  [2/21], [94mLoss[0m : 2.40343
[1mStep[0m  [4/21], [94mLoss[0m : 2.43450
[1mStep[0m  [6/21], [94mLoss[0m : 2.54312
[1mStep[0m  [8/21], [94mLoss[0m : 2.55009
[1mStep[0m  [10/21], [94mLoss[0m : 2.42398
[1mStep[0m  [12/21], [94mLoss[0m : 2.45165
[1mStep[0m  [14/21], [94mLoss[0m : 2.41071
[1mStep[0m  [16/21], [94mLoss[0m : 2.57478
[1mStep[0m  [18/21], [94mLoss[0m : 2.37005
[1mStep[0m  [20/21], [94mLoss[0m : 2.32385

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.352, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39099
[1mStep[0m  [2/21], [94mLoss[0m : 2.21298
[1mStep[0m  [4/21], [94mLoss[0m : 2.48004
[1mStep[0m  [6/21], [94mLoss[0m : 2.41856
[1mStep[0m  [8/21], [94mLoss[0m : 2.47310
[1mStep[0m  [10/21], [94mLoss[0m : 2.35651
[1mStep[0m  [12/21], [94mLoss[0m : 2.41855
[1mStep[0m  [14/21], [94mLoss[0m : 2.65657
[1mStep[0m  [16/21], [94mLoss[0m : 2.47366
[1mStep[0m  [18/21], [94mLoss[0m : 2.44569
[1mStep[0m  [20/21], [94mLoss[0m : 2.46386

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34015
[1mStep[0m  [2/21], [94mLoss[0m : 2.38349
[1mStep[0m  [4/21], [94mLoss[0m : 2.50358
[1mStep[0m  [6/21], [94mLoss[0m : 2.27047
[1mStep[0m  [8/21], [94mLoss[0m : 2.46499
[1mStep[0m  [10/21], [94mLoss[0m : 2.43439
[1mStep[0m  [12/21], [94mLoss[0m : 2.60582
[1mStep[0m  [14/21], [94mLoss[0m : 2.38041
[1mStep[0m  [16/21], [94mLoss[0m : 2.29356
[1mStep[0m  [18/21], [94mLoss[0m : 2.48923
[1mStep[0m  [20/21], [94mLoss[0m : 2.27245

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39025
[1mStep[0m  [2/21], [94mLoss[0m : 2.55113
[1mStep[0m  [4/21], [94mLoss[0m : 2.40197
[1mStep[0m  [6/21], [94mLoss[0m : 2.44173
[1mStep[0m  [8/21], [94mLoss[0m : 2.45658
[1mStep[0m  [10/21], [94mLoss[0m : 2.44494
[1mStep[0m  [12/21], [94mLoss[0m : 2.32109
[1mStep[0m  [14/21], [94mLoss[0m : 2.55729
[1mStep[0m  [16/21], [94mLoss[0m : 2.42284
[1mStep[0m  [18/21], [94mLoss[0m : 2.33027
[1mStep[0m  [20/21], [94mLoss[0m : 2.43134

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37534
[1mStep[0m  [2/21], [94mLoss[0m : 2.40892
[1mStep[0m  [4/21], [94mLoss[0m : 2.46545
[1mStep[0m  [6/21], [94mLoss[0m : 2.48201
[1mStep[0m  [8/21], [94mLoss[0m : 2.44797
[1mStep[0m  [10/21], [94mLoss[0m : 2.26628
[1mStep[0m  [12/21], [94mLoss[0m : 2.25618
[1mStep[0m  [14/21], [94mLoss[0m : 2.39898
[1mStep[0m  [16/21], [94mLoss[0m : 2.39391
[1mStep[0m  [18/21], [94mLoss[0m : 2.34422
[1mStep[0m  [20/21], [94mLoss[0m : 2.39954

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38358
[1mStep[0m  [2/21], [94mLoss[0m : 2.58879
[1mStep[0m  [4/21], [94mLoss[0m : 2.23071
[1mStep[0m  [6/21], [94mLoss[0m : 2.32007
[1mStep[0m  [8/21], [94mLoss[0m : 2.31636
[1mStep[0m  [10/21], [94mLoss[0m : 2.36888
[1mStep[0m  [12/21], [94mLoss[0m : 2.38954
[1mStep[0m  [14/21], [94mLoss[0m : 2.56723
[1mStep[0m  [16/21], [94mLoss[0m : 2.35453
[1mStep[0m  [18/21], [94mLoss[0m : 2.37027
[1mStep[0m  [20/21], [94mLoss[0m : 2.30422

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29640
[1mStep[0m  [2/21], [94mLoss[0m : 2.37876
[1mStep[0m  [4/21], [94mLoss[0m : 2.43342
[1mStep[0m  [6/21], [94mLoss[0m : 2.22698
[1mStep[0m  [8/21], [94mLoss[0m : 2.41259
[1mStep[0m  [10/21], [94mLoss[0m : 2.46454
[1mStep[0m  [12/21], [94mLoss[0m : 2.32152
[1mStep[0m  [14/21], [94mLoss[0m : 2.40717
[1mStep[0m  [16/21], [94mLoss[0m : 2.24432
[1mStep[0m  [18/21], [94mLoss[0m : 2.19966
[1mStep[0m  [20/21], [94mLoss[0m : 2.44704

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31273
[1mStep[0m  [2/21], [94mLoss[0m : 2.47531
[1mStep[0m  [4/21], [94mLoss[0m : 2.30190
[1mStep[0m  [6/21], [94mLoss[0m : 2.19781
[1mStep[0m  [8/21], [94mLoss[0m : 2.36768
[1mStep[0m  [10/21], [94mLoss[0m : 2.42963
[1mStep[0m  [12/21], [94mLoss[0m : 2.37086
[1mStep[0m  [14/21], [94mLoss[0m : 2.29418
[1mStep[0m  [16/21], [94mLoss[0m : 2.23184
[1mStep[0m  [18/21], [94mLoss[0m : 2.35676
[1mStep[0m  [20/21], [94mLoss[0m : 2.24622

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.392, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26215
[1mStep[0m  [2/21], [94mLoss[0m : 2.35142
[1mStep[0m  [4/21], [94mLoss[0m : 2.18078
[1mStep[0m  [6/21], [94mLoss[0m : 2.31505
[1mStep[0m  [8/21], [94mLoss[0m : 2.29291
[1mStep[0m  [10/21], [94mLoss[0m : 2.32322
[1mStep[0m  [12/21], [94mLoss[0m : 2.31369
[1mStep[0m  [14/21], [94mLoss[0m : 2.15727
[1mStep[0m  [16/21], [94mLoss[0m : 2.35429
[1mStep[0m  [18/21], [94mLoss[0m : 2.39678
[1mStep[0m  [20/21], [94mLoss[0m : 2.33877

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45338
[1mStep[0m  [2/21], [94mLoss[0m : 2.25034
[1mStep[0m  [4/21], [94mLoss[0m : 2.20818
[1mStep[0m  [6/21], [94mLoss[0m : 2.28174
[1mStep[0m  [8/21], [94mLoss[0m : 2.21785
[1mStep[0m  [10/21], [94mLoss[0m : 2.31361
[1mStep[0m  [12/21], [94mLoss[0m : 2.33088
[1mStep[0m  [14/21], [94mLoss[0m : 2.27011
[1mStep[0m  [16/21], [94mLoss[0m : 2.28015
[1mStep[0m  [18/21], [94mLoss[0m : 2.33134
[1mStep[0m  [20/21], [94mLoss[0m : 2.29574

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.389, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.399
====================================

Phase 2 - Evaluation MAE:  2.398820161819458
MAE score P1       2.364539
MAE score P2        2.39882
loss               2.290978
learning_rate       0.00505
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 11.19357
[1mStep[0m  [33/339], [94mLoss[0m : 3.48362
[1mStep[0m  [66/339], [94mLoss[0m : 2.40382
[1mStep[0m  [99/339], [94mLoss[0m : 2.14065
[1mStep[0m  [132/339], [94mLoss[0m : 3.14455
[1mStep[0m  [165/339], [94mLoss[0m : 2.81516
[1mStep[0m  [198/339], [94mLoss[0m : 2.94451
[1mStep[0m  [231/339], [94mLoss[0m : 2.14144
[1mStep[0m  [264/339], [94mLoss[0m : 2.44880
[1mStep[0m  [297/339], [94mLoss[0m : 2.30433
[1mStep[0m  [330/339], [94mLoss[0m : 2.22382

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.989, [92mTest[0m: 11.003, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85927
[1mStep[0m  [33/339], [94mLoss[0m : 2.72453
[1mStep[0m  [66/339], [94mLoss[0m : 2.36784
[1mStep[0m  [99/339], [94mLoss[0m : 2.57195
[1mStep[0m  [132/339], [94mLoss[0m : 2.04594
[1mStep[0m  [165/339], [94mLoss[0m : 3.15174
[1mStep[0m  [198/339], [94mLoss[0m : 2.79505
[1mStep[0m  [231/339], [94mLoss[0m : 2.06766
[1mStep[0m  [264/339], [94mLoss[0m : 2.34899
[1mStep[0m  [297/339], [94mLoss[0m : 2.73279
[1mStep[0m  [330/339], [94mLoss[0m : 2.90149

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22197
[1mStep[0m  [33/339], [94mLoss[0m : 2.05637
[1mStep[0m  [66/339], [94mLoss[0m : 3.12539
[1mStep[0m  [99/339], [94mLoss[0m : 2.09500
[1mStep[0m  [132/339], [94mLoss[0m : 2.18219
[1mStep[0m  [165/339], [94mLoss[0m : 2.55081
[1mStep[0m  [198/339], [94mLoss[0m : 2.50458
[1mStep[0m  [231/339], [94mLoss[0m : 2.45157
[1mStep[0m  [264/339], [94mLoss[0m : 2.48372
[1mStep[0m  [297/339], [94mLoss[0m : 2.55316
[1mStep[0m  [330/339], [94mLoss[0m : 3.15785

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88463
[1mStep[0m  [33/339], [94mLoss[0m : 1.98605
[1mStep[0m  [66/339], [94mLoss[0m : 2.31181
[1mStep[0m  [99/339], [94mLoss[0m : 2.27965
[1mStep[0m  [132/339], [94mLoss[0m : 2.54116
[1mStep[0m  [165/339], [94mLoss[0m : 2.40046
[1mStep[0m  [198/339], [94mLoss[0m : 2.39304
[1mStep[0m  [231/339], [94mLoss[0m : 2.22329
[1mStep[0m  [264/339], [94mLoss[0m : 2.93518
[1mStep[0m  [297/339], [94mLoss[0m : 2.74387
[1mStep[0m  [330/339], [94mLoss[0m : 3.06153

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83385
[1mStep[0m  [33/339], [94mLoss[0m : 2.71011
[1mStep[0m  [66/339], [94mLoss[0m : 2.24660
[1mStep[0m  [99/339], [94mLoss[0m : 2.02403
[1mStep[0m  [132/339], [94mLoss[0m : 2.18487
[1mStep[0m  [165/339], [94mLoss[0m : 2.75634
[1mStep[0m  [198/339], [94mLoss[0m : 2.40361
[1mStep[0m  [231/339], [94mLoss[0m : 2.57573
[1mStep[0m  [264/339], [94mLoss[0m : 2.20964
[1mStep[0m  [297/339], [94mLoss[0m : 2.15819
[1mStep[0m  [330/339], [94mLoss[0m : 2.20285

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85208
[1mStep[0m  [33/339], [94mLoss[0m : 2.91680
[1mStep[0m  [66/339], [94mLoss[0m : 2.55494
[1mStep[0m  [99/339], [94mLoss[0m : 2.19544
[1mStep[0m  [132/339], [94mLoss[0m : 2.60671
[1mStep[0m  [165/339], [94mLoss[0m : 2.55027
[1mStep[0m  [198/339], [94mLoss[0m : 2.85380
[1mStep[0m  [231/339], [94mLoss[0m : 2.54625
[1mStep[0m  [264/339], [94mLoss[0m : 2.45639
[1mStep[0m  [297/339], [94mLoss[0m : 3.43031
[1mStep[0m  [330/339], [94mLoss[0m : 2.04737

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62967
[1mStep[0m  [33/339], [94mLoss[0m : 2.07572
[1mStep[0m  [66/339], [94mLoss[0m : 2.93785
[1mStep[0m  [99/339], [94mLoss[0m : 2.26088
[1mStep[0m  [132/339], [94mLoss[0m : 2.34481
[1mStep[0m  [165/339], [94mLoss[0m : 3.36366
[1mStep[0m  [198/339], [94mLoss[0m : 2.34158
[1mStep[0m  [231/339], [94mLoss[0m : 2.16021
[1mStep[0m  [264/339], [94mLoss[0m : 2.95166
[1mStep[0m  [297/339], [94mLoss[0m : 2.53143
[1mStep[0m  [330/339], [94mLoss[0m : 2.03089

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49301
[1mStep[0m  [33/339], [94mLoss[0m : 2.23406
[1mStep[0m  [66/339], [94mLoss[0m : 2.85467
[1mStep[0m  [99/339], [94mLoss[0m : 2.41403
[1mStep[0m  [132/339], [94mLoss[0m : 2.38849
[1mStep[0m  [165/339], [94mLoss[0m : 2.41169
[1mStep[0m  [198/339], [94mLoss[0m : 1.94956
[1mStep[0m  [231/339], [94mLoss[0m : 2.21370
[1mStep[0m  [264/339], [94mLoss[0m : 2.47719
[1mStep[0m  [297/339], [94mLoss[0m : 2.14437
[1mStep[0m  [330/339], [94mLoss[0m : 2.13088

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36563
[1mStep[0m  [33/339], [94mLoss[0m : 2.46606
[1mStep[0m  [66/339], [94mLoss[0m : 2.48232
[1mStep[0m  [99/339], [94mLoss[0m : 2.57819
[1mStep[0m  [132/339], [94mLoss[0m : 2.43922
[1mStep[0m  [165/339], [94mLoss[0m : 2.36050
[1mStep[0m  [198/339], [94mLoss[0m : 1.80821
[1mStep[0m  [231/339], [94mLoss[0m : 3.19025
[1mStep[0m  [264/339], [94mLoss[0m : 2.21397
[1mStep[0m  [297/339], [94mLoss[0m : 2.54694
[1mStep[0m  [330/339], [94mLoss[0m : 2.89731

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55642
[1mStep[0m  [33/339], [94mLoss[0m : 3.14453
[1mStep[0m  [66/339], [94mLoss[0m : 2.74595
[1mStep[0m  [99/339], [94mLoss[0m : 2.06237
[1mStep[0m  [132/339], [94mLoss[0m : 2.07948
[1mStep[0m  [165/339], [94mLoss[0m : 2.28339
[1mStep[0m  [198/339], [94mLoss[0m : 2.49942
[1mStep[0m  [231/339], [94mLoss[0m : 2.44830
[1mStep[0m  [264/339], [94mLoss[0m : 2.64614
[1mStep[0m  [297/339], [94mLoss[0m : 2.75454
[1mStep[0m  [330/339], [94mLoss[0m : 2.31456

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02194
[1mStep[0m  [33/339], [94mLoss[0m : 2.00524
[1mStep[0m  [66/339], [94mLoss[0m : 2.42266
[1mStep[0m  [99/339], [94mLoss[0m : 2.61478
[1mStep[0m  [132/339], [94mLoss[0m : 2.65008
[1mStep[0m  [165/339], [94mLoss[0m : 2.44911
[1mStep[0m  [198/339], [94mLoss[0m : 3.11318
[1mStep[0m  [231/339], [94mLoss[0m : 1.98961
[1mStep[0m  [264/339], [94mLoss[0m : 2.32974
[1mStep[0m  [297/339], [94mLoss[0m : 2.71334
[1mStep[0m  [330/339], [94mLoss[0m : 3.00384

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65116
[1mStep[0m  [33/339], [94mLoss[0m : 2.49971
[1mStep[0m  [66/339], [94mLoss[0m : 2.37861
[1mStep[0m  [99/339], [94mLoss[0m : 2.45516
[1mStep[0m  [132/339], [94mLoss[0m : 1.76859
[1mStep[0m  [165/339], [94mLoss[0m : 2.27732
[1mStep[0m  [198/339], [94mLoss[0m : 2.31478
[1mStep[0m  [231/339], [94mLoss[0m : 2.75185
[1mStep[0m  [264/339], [94mLoss[0m : 2.51380
[1mStep[0m  [297/339], [94mLoss[0m : 2.66758
[1mStep[0m  [330/339], [94mLoss[0m : 2.63111

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97250
[1mStep[0m  [33/339], [94mLoss[0m : 2.11441
[1mStep[0m  [66/339], [94mLoss[0m : 1.97365
[1mStep[0m  [99/339], [94mLoss[0m : 2.00740
[1mStep[0m  [132/339], [94mLoss[0m : 2.11687
[1mStep[0m  [165/339], [94mLoss[0m : 2.69743
[1mStep[0m  [198/339], [94mLoss[0m : 2.28733
[1mStep[0m  [231/339], [94mLoss[0m : 2.23409
[1mStep[0m  [264/339], [94mLoss[0m : 2.37799
[1mStep[0m  [297/339], [94mLoss[0m : 1.78187
[1mStep[0m  [330/339], [94mLoss[0m : 2.63225

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49243
[1mStep[0m  [33/339], [94mLoss[0m : 1.94092
[1mStep[0m  [66/339], [94mLoss[0m : 2.12475
[1mStep[0m  [99/339], [94mLoss[0m : 2.18330
[1mStep[0m  [132/339], [94mLoss[0m : 2.38254
[1mStep[0m  [165/339], [94mLoss[0m : 1.96263
[1mStep[0m  [198/339], [94mLoss[0m : 2.42682
[1mStep[0m  [231/339], [94mLoss[0m : 1.85299
[1mStep[0m  [264/339], [94mLoss[0m : 2.18426
[1mStep[0m  [297/339], [94mLoss[0m : 2.42852
[1mStep[0m  [330/339], [94mLoss[0m : 2.59147

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.315, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.86664
[1mStep[0m  [33/339], [94mLoss[0m : 2.95186
[1mStep[0m  [66/339], [94mLoss[0m : 2.06528
[1mStep[0m  [99/339], [94mLoss[0m : 1.78798
[1mStep[0m  [132/339], [94mLoss[0m : 2.24042
[1mStep[0m  [165/339], [94mLoss[0m : 2.69384
[1mStep[0m  [198/339], [94mLoss[0m : 2.56301
[1mStep[0m  [231/339], [94mLoss[0m : 2.17276
[1mStep[0m  [264/339], [94mLoss[0m : 2.27696
[1mStep[0m  [297/339], [94mLoss[0m : 2.44865
[1mStep[0m  [330/339], [94mLoss[0m : 2.21879

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.299, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54254
[1mStep[0m  [33/339], [94mLoss[0m : 1.72386
[1mStep[0m  [66/339], [94mLoss[0m : 2.39160
[1mStep[0m  [99/339], [94mLoss[0m : 2.22991
[1mStep[0m  [132/339], [94mLoss[0m : 2.87048
[1mStep[0m  [165/339], [94mLoss[0m : 2.47107
[1mStep[0m  [198/339], [94mLoss[0m : 2.02061
[1mStep[0m  [231/339], [94mLoss[0m : 2.65260
[1mStep[0m  [264/339], [94mLoss[0m : 2.41842
[1mStep[0m  [297/339], [94mLoss[0m : 2.02514
[1mStep[0m  [330/339], [94mLoss[0m : 3.19011

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00312
[1mStep[0m  [33/339], [94mLoss[0m : 2.53852
[1mStep[0m  [66/339], [94mLoss[0m : 2.57389
[1mStep[0m  [99/339], [94mLoss[0m : 2.37870
[1mStep[0m  [132/339], [94mLoss[0m : 2.17366
[1mStep[0m  [165/339], [94mLoss[0m : 2.14148
[1mStep[0m  [198/339], [94mLoss[0m : 2.36432
[1mStep[0m  [231/339], [94mLoss[0m : 2.57278
[1mStep[0m  [264/339], [94mLoss[0m : 2.41300
[1mStep[0m  [297/339], [94mLoss[0m : 2.03848
[1mStep[0m  [330/339], [94mLoss[0m : 2.86021

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.306, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63090
[1mStep[0m  [33/339], [94mLoss[0m : 2.92362
[1mStep[0m  [66/339], [94mLoss[0m : 2.01832
[1mStep[0m  [99/339], [94mLoss[0m : 1.79243
[1mStep[0m  [132/339], [94mLoss[0m : 2.26923
[1mStep[0m  [165/339], [94mLoss[0m : 2.35583
[1mStep[0m  [198/339], [94mLoss[0m : 2.19230
[1mStep[0m  [231/339], [94mLoss[0m : 2.83838
[1mStep[0m  [264/339], [94mLoss[0m : 2.17926
[1mStep[0m  [297/339], [94mLoss[0m : 2.32965
[1mStep[0m  [330/339], [94mLoss[0m : 1.99795

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34126
[1mStep[0m  [33/339], [94mLoss[0m : 2.18211
[1mStep[0m  [66/339], [94mLoss[0m : 2.77660
[1mStep[0m  [99/339], [94mLoss[0m : 1.80973
[1mStep[0m  [132/339], [94mLoss[0m : 2.42921
[1mStep[0m  [165/339], [94mLoss[0m : 1.90920
[1mStep[0m  [198/339], [94mLoss[0m : 2.33896
[1mStep[0m  [231/339], [94mLoss[0m : 3.24586
[1mStep[0m  [264/339], [94mLoss[0m : 1.92161
[1mStep[0m  [297/339], [94mLoss[0m : 2.58637
[1mStep[0m  [330/339], [94mLoss[0m : 2.00823

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.312, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76362
[1mStep[0m  [33/339], [94mLoss[0m : 2.01777
[1mStep[0m  [66/339], [94mLoss[0m : 2.69291
[1mStep[0m  [99/339], [94mLoss[0m : 2.42409
[1mStep[0m  [132/339], [94mLoss[0m : 1.65719
[1mStep[0m  [165/339], [94mLoss[0m : 1.76907
[1mStep[0m  [198/339], [94mLoss[0m : 1.98511
[1mStep[0m  [231/339], [94mLoss[0m : 2.45042
[1mStep[0m  [264/339], [94mLoss[0m : 2.37743
[1mStep[0m  [297/339], [94mLoss[0m : 2.39532
[1mStep[0m  [330/339], [94mLoss[0m : 2.43074

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38977
[1mStep[0m  [33/339], [94mLoss[0m : 2.65838
[1mStep[0m  [66/339], [94mLoss[0m : 2.86464
[1mStep[0m  [99/339], [94mLoss[0m : 2.02865
[1mStep[0m  [132/339], [94mLoss[0m : 2.49387
[1mStep[0m  [165/339], [94mLoss[0m : 2.64912
[1mStep[0m  [198/339], [94mLoss[0m : 2.12649
[1mStep[0m  [231/339], [94mLoss[0m : 2.23872
[1mStep[0m  [264/339], [94mLoss[0m : 2.21438
[1mStep[0m  [297/339], [94mLoss[0m : 2.82403
[1mStep[0m  [330/339], [94mLoss[0m : 1.75825

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.310, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62985
[1mStep[0m  [33/339], [94mLoss[0m : 2.10162
[1mStep[0m  [66/339], [94mLoss[0m : 2.10863
[1mStep[0m  [99/339], [94mLoss[0m : 2.39364
[1mStep[0m  [132/339], [94mLoss[0m : 2.93665
[1mStep[0m  [165/339], [94mLoss[0m : 2.25797
[1mStep[0m  [198/339], [94mLoss[0m : 2.81771
[1mStep[0m  [231/339], [94mLoss[0m : 2.15566
[1mStep[0m  [264/339], [94mLoss[0m : 2.63694
[1mStep[0m  [297/339], [94mLoss[0m : 2.37971
[1mStep[0m  [330/339], [94mLoss[0m : 2.13018

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28667
[1mStep[0m  [33/339], [94mLoss[0m : 1.96975
[1mStep[0m  [66/339], [94mLoss[0m : 2.44827
[1mStep[0m  [99/339], [94mLoss[0m : 2.59455
[1mStep[0m  [132/339], [94mLoss[0m : 2.24312
[1mStep[0m  [165/339], [94mLoss[0m : 1.86997
[1mStep[0m  [198/339], [94mLoss[0m : 2.18596
[1mStep[0m  [231/339], [94mLoss[0m : 1.99244
[1mStep[0m  [264/339], [94mLoss[0m : 2.02827
[1mStep[0m  [297/339], [94mLoss[0m : 2.56749
[1mStep[0m  [330/339], [94mLoss[0m : 1.78618

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08311
[1mStep[0m  [33/339], [94mLoss[0m : 2.57651
[1mStep[0m  [66/339], [94mLoss[0m : 2.86744
[1mStep[0m  [99/339], [94mLoss[0m : 2.51436
[1mStep[0m  [132/339], [94mLoss[0m : 2.24730
[1mStep[0m  [165/339], [94mLoss[0m : 2.15529
[1mStep[0m  [198/339], [94mLoss[0m : 2.50660
[1mStep[0m  [231/339], [94mLoss[0m : 2.03963
[1mStep[0m  [264/339], [94mLoss[0m : 2.86288
[1mStep[0m  [297/339], [94mLoss[0m : 2.52990
[1mStep[0m  [330/339], [94mLoss[0m : 2.15617

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72537
[1mStep[0m  [33/339], [94mLoss[0m : 2.14339
[1mStep[0m  [66/339], [94mLoss[0m : 2.41312
[1mStep[0m  [99/339], [94mLoss[0m : 2.15953
[1mStep[0m  [132/339], [94mLoss[0m : 2.98515
[1mStep[0m  [165/339], [94mLoss[0m : 1.88542
[1mStep[0m  [198/339], [94mLoss[0m : 2.70074
[1mStep[0m  [231/339], [94mLoss[0m : 2.65055
[1mStep[0m  [264/339], [94mLoss[0m : 2.11557
[1mStep[0m  [297/339], [94mLoss[0m : 2.03558
[1mStep[0m  [330/339], [94mLoss[0m : 2.98313

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.315, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88038
[1mStep[0m  [33/339], [94mLoss[0m : 2.40390
[1mStep[0m  [66/339], [94mLoss[0m : 2.41507
[1mStep[0m  [99/339], [94mLoss[0m : 2.57128
[1mStep[0m  [132/339], [94mLoss[0m : 2.57066
[1mStep[0m  [165/339], [94mLoss[0m : 2.42183
[1mStep[0m  [198/339], [94mLoss[0m : 1.67165
[1mStep[0m  [231/339], [94mLoss[0m : 3.04536
[1mStep[0m  [264/339], [94mLoss[0m : 2.29391
[1mStep[0m  [297/339], [94mLoss[0m : 1.49085
[1mStep[0m  [330/339], [94mLoss[0m : 2.40294

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65852
[1mStep[0m  [33/339], [94mLoss[0m : 1.97574
[1mStep[0m  [66/339], [94mLoss[0m : 1.94690
[1mStep[0m  [99/339], [94mLoss[0m : 2.28260
[1mStep[0m  [132/339], [94mLoss[0m : 2.59747
[1mStep[0m  [165/339], [94mLoss[0m : 2.29799
[1mStep[0m  [198/339], [94mLoss[0m : 2.39698
[1mStep[0m  [231/339], [94mLoss[0m : 2.28930
[1mStep[0m  [264/339], [94mLoss[0m : 2.63234
[1mStep[0m  [297/339], [94mLoss[0m : 2.48270
[1mStep[0m  [330/339], [94mLoss[0m : 2.19365

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.318, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70739
[1mStep[0m  [33/339], [94mLoss[0m : 2.11765
[1mStep[0m  [66/339], [94mLoss[0m : 2.37005
[1mStep[0m  [99/339], [94mLoss[0m : 2.45847
[1mStep[0m  [132/339], [94mLoss[0m : 2.24405
[1mStep[0m  [165/339], [94mLoss[0m : 3.06331
[1mStep[0m  [198/339], [94mLoss[0m : 2.55828
[1mStep[0m  [231/339], [94mLoss[0m : 1.86807
[1mStep[0m  [264/339], [94mLoss[0m : 2.34743
[1mStep[0m  [297/339], [94mLoss[0m : 2.10187
[1mStep[0m  [330/339], [94mLoss[0m : 2.53431

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91059
[1mStep[0m  [33/339], [94mLoss[0m : 3.44996
[1mStep[0m  [66/339], [94mLoss[0m : 1.81834
[1mStep[0m  [99/339], [94mLoss[0m : 2.60004
[1mStep[0m  [132/339], [94mLoss[0m : 2.51305
[1mStep[0m  [165/339], [94mLoss[0m : 1.90458
[1mStep[0m  [198/339], [94mLoss[0m : 2.19962
[1mStep[0m  [231/339], [94mLoss[0m : 2.37883
[1mStep[0m  [264/339], [94mLoss[0m : 2.44921
[1mStep[0m  [297/339], [94mLoss[0m : 2.13628
[1mStep[0m  [330/339], [94mLoss[0m : 2.25359

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.315, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29867
[1mStep[0m  [33/339], [94mLoss[0m : 2.49854
[1mStep[0m  [66/339], [94mLoss[0m : 2.12121
[1mStep[0m  [99/339], [94mLoss[0m : 3.22308
[1mStep[0m  [132/339], [94mLoss[0m : 2.68366
[1mStep[0m  [165/339], [94mLoss[0m : 2.48326
[1mStep[0m  [198/339], [94mLoss[0m : 2.18658
[1mStep[0m  [231/339], [94mLoss[0m : 2.01032
[1mStep[0m  [264/339], [94mLoss[0m : 2.48355
[1mStep[0m  [297/339], [94mLoss[0m : 2.09975
[1mStep[0m  [330/339], [94mLoss[0m : 2.18372

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.322
====================================

Phase 1 - Evaluation MAE:  2.321790503189627
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.27887
[1mStep[0m  [33/339], [94mLoss[0m : 3.22355
[1mStep[0m  [66/339], [94mLoss[0m : 2.47371
[1mStep[0m  [99/339], [94mLoss[0m : 2.75499
[1mStep[0m  [132/339], [94mLoss[0m : 2.39061
[1mStep[0m  [165/339], [94mLoss[0m : 2.63199
[1mStep[0m  [198/339], [94mLoss[0m : 2.42566
[1mStep[0m  [231/339], [94mLoss[0m : 2.49031
[1mStep[0m  [264/339], [94mLoss[0m : 2.44705
[1mStep[0m  [297/339], [94mLoss[0m : 2.10476
[1mStep[0m  [330/339], [94mLoss[0m : 2.65385

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.322, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85499
[1mStep[0m  [33/339], [94mLoss[0m : 2.34495
[1mStep[0m  [66/339], [94mLoss[0m : 2.15488
[1mStep[0m  [99/339], [94mLoss[0m : 2.73619
[1mStep[0m  [132/339], [94mLoss[0m : 2.20903
[1mStep[0m  [165/339], [94mLoss[0m : 2.11468
[1mStep[0m  [198/339], [94mLoss[0m : 2.24188
[1mStep[0m  [231/339], [94mLoss[0m : 2.23790
[1mStep[0m  [264/339], [94mLoss[0m : 2.26968
[1mStep[0m  [297/339], [94mLoss[0m : 2.86475
[1mStep[0m  [330/339], [94mLoss[0m : 2.73816

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.864, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50357
[1mStep[0m  [33/339], [94mLoss[0m : 2.24434
[1mStep[0m  [66/339], [94mLoss[0m : 2.27973
[1mStep[0m  [99/339], [94mLoss[0m : 2.97631
[1mStep[0m  [132/339], [94mLoss[0m : 2.41956
[1mStep[0m  [165/339], [94mLoss[0m : 2.32240
[1mStep[0m  [198/339], [94mLoss[0m : 2.29002
[1mStep[0m  [231/339], [94mLoss[0m : 2.28770
[1mStep[0m  [264/339], [94mLoss[0m : 2.76265
[1mStep[0m  [297/339], [94mLoss[0m : 2.58140
[1mStep[0m  [330/339], [94mLoss[0m : 2.06671

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21688
[1mStep[0m  [33/339], [94mLoss[0m : 2.52457
[1mStep[0m  [66/339], [94mLoss[0m : 1.94386
[1mStep[0m  [99/339], [94mLoss[0m : 1.99715
[1mStep[0m  [132/339], [94mLoss[0m : 2.34766
[1mStep[0m  [165/339], [94mLoss[0m : 2.32187
[1mStep[0m  [198/339], [94mLoss[0m : 2.02903
[1mStep[0m  [231/339], [94mLoss[0m : 2.40247
[1mStep[0m  [264/339], [94mLoss[0m : 2.24390
[1mStep[0m  [297/339], [94mLoss[0m : 2.04986
[1mStep[0m  [330/339], [94mLoss[0m : 2.91196

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05825
[1mStep[0m  [33/339], [94mLoss[0m : 2.12221
[1mStep[0m  [66/339], [94mLoss[0m : 2.37587
[1mStep[0m  [99/339], [94mLoss[0m : 1.85135
[1mStep[0m  [132/339], [94mLoss[0m : 1.86380
[1mStep[0m  [165/339], [94mLoss[0m : 2.71004
[1mStep[0m  [198/339], [94mLoss[0m : 1.76792
[1mStep[0m  [231/339], [94mLoss[0m : 1.74625
[1mStep[0m  [264/339], [94mLoss[0m : 2.41539
[1mStep[0m  [297/339], [94mLoss[0m : 2.59508
[1mStep[0m  [330/339], [94mLoss[0m : 2.28713

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.152, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10974
[1mStep[0m  [33/339], [94mLoss[0m : 1.76366
[1mStep[0m  [66/339], [94mLoss[0m : 2.03029
[1mStep[0m  [99/339], [94mLoss[0m : 1.68688
[1mStep[0m  [132/339], [94mLoss[0m : 2.22527
[1mStep[0m  [165/339], [94mLoss[0m : 1.80038
[1mStep[0m  [198/339], [94mLoss[0m : 2.11253
[1mStep[0m  [231/339], [94mLoss[0m : 2.58963
[1mStep[0m  [264/339], [94mLoss[0m : 2.21863
[1mStep[0m  [297/339], [94mLoss[0m : 1.83544
[1mStep[0m  [330/339], [94mLoss[0m : 1.99550

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45492
[1mStep[0m  [33/339], [94mLoss[0m : 1.81627
[1mStep[0m  [66/339], [94mLoss[0m : 2.32713
[1mStep[0m  [99/339], [94mLoss[0m : 2.04826
[1mStep[0m  [132/339], [94mLoss[0m : 2.00439
[1mStep[0m  [165/339], [94mLoss[0m : 1.93629
[1mStep[0m  [198/339], [94mLoss[0m : 2.14127
[1mStep[0m  [231/339], [94mLoss[0m : 1.93986
[1mStep[0m  [264/339], [94mLoss[0m : 2.08292
[1mStep[0m  [297/339], [94mLoss[0m : 1.62942
[1mStep[0m  [330/339], [94mLoss[0m : 2.18084

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56989
[1mStep[0m  [33/339], [94mLoss[0m : 1.81999
[1mStep[0m  [66/339], [94mLoss[0m : 2.14818
[1mStep[0m  [99/339], [94mLoss[0m : 1.79456
[1mStep[0m  [132/339], [94mLoss[0m : 1.80417
[1mStep[0m  [165/339], [94mLoss[0m : 1.97957
[1mStep[0m  [198/339], [94mLoss[0m : 1.41194
[1mStep[0m  [231/339], [94mLoss[0m : 1.69017
[1mStep[0m  [264/339], [94mLoss[0m : 2.28466
[1mStep[0m  [297/339], [94mLoss[0m : 1.70852
[1mStep[0m  [330/339], [94mLoss[0m : 1.79298

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14143
[1mStep[0m  [33/339], [94mLoss[0m : 2.48941
[1mStep[0m  [66/339], [94mLoss[0m : 2.28683
[1mStep[0m  [99/339], [94mLoss[0m : 1.78641
[1mStep[0m  [132/339], [94mLoss[0m : 1.61871
[1mStep[0m  [165/339], [94mLoss[0m : 1.56257
[1mStep[0m  [198/339], [94mLoss[0m : 1.70536
[1mStep[0m  [231/339], [94mLoss[0m : 1.91970
[1mStep[0m  [264/339], [94mLoss[0m : 1.76134
[1mStep[0m  [297/339], [94mLoss[0m : 2.13646
[1mStep[0m  [330/339], [94mLoss[0m : 2.08617

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19555
[1mStep[0m  [33/339], [94mLoss[0m : 1.54397
[1mStep[0m  [66/339], [94mLoss[0m : 2.57770
[1mStep[0m  [99/339], [94mLoss[0m : 2.10147
[1mStep[0m  [132/339], [94mLoss[0m : 1.85922
[1mStep[0m  [165/339], [94mLoss[0m : 1.68410
[1mStep[0m  [198/339], [94mLoss[0m : 1.61735
[1mStep[0m  [231/339], [94mLoss[0m : 2.23553
[1mStep[0m  [264/339], [94mLoss[0m : 1.90000
[1mStep[0m  [297/339], [94mLoss[0m : 1.70383
[1mStep[0m  [330/339], [94mLoss[0m : 2.12874

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20820
[1mStep[0m  [33/339], [94mLoss[0m : 1.70326
[1mStep[0m  [66/339], [94mLoss[0m : 2.09456
[1mStep[0m  [99/339], [94mLoss[0m : 1.78990
[1mStep[0m  [132/339], [94mLoss[0m : 1.55365
[1mStep[0m  [165/339], [94mLoss[0m : 1.65938
[1mStep[0m  [198/339], [94mLoss[0m : 2.04856
[1mStep[0m  [231/339], [94mLoss[0m : 2.50449
[1mStep[0m  [264/339], [94mLoss[0m : 1.88580
[1mStep[0m  [297/339], [94mLoss[0m : 1.62728
[1mStep[0m  [330/339], [94mLoss[0m : 2.32224

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42368
[1mStep[0m  [33/339], [94mLoss[0m : 1.80028
[1mStep[0m  [66/339], [94mLoss[0m : 2.11961
[1mStep[0m  [99/339], [94mLoss[0m : 1.34006
[1mStep[0m  [132/339], [94mLoss[0m : 2.10232
[1mStep[0m  [165/339], [94mLoss[0m : 2.05597
[1mStep[0m  [198/339], [94mLoss[0m : 1.88617
[1mStep[0m  [231/339], [94mLoss[0m : 1.45843
[1mStep[0m  [264/339], [94mLoss[0m : 1.74681
[1mStep[0m  [297/339], [94mLoss[0m : 1.92366
[1mStep[0m  [330/339], [94mLoss[0m : 1.74233

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.482, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46497
[1mStep[0m  [33/339], [94mLoss[0m : 1.59454
[1mStep[0m  [66/339], [94mLoss[0m : 1.69748
[1mStep[0m  [99/339], [94mLoss[0m : 1.71591
[1mStep[0m  [132/339], [94mLoss[0m : 1.76749
[1mStep[0m  [165/339], [94mLoss[0m : 1.90215
[1mStep[0m  [198/339], [94mLoss[0m : 1.87367
[1mStep[0m  [231/339], [94mLoss[0m : 2.06994
[1mStep[0m  [264/339], [94mLoss[0m : 2.51648
[1mStep[0m  [297/339], [94mLoss[0m : 2.20295
[1mStep[0m  [330/339], [94mLoss[0m : 2.28027

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97240
[1mStep[0m  [33/339], [94mLoss[0m : 1.55071
[1mStep[0m  [66/339], [94mLoss[0m : 1.68836
[1mStep[0m  [99/339], [94mLoss[0m : 1.73745
[1mStep[0m  [132/339], [94mLoss[0m : 1.85191
[1mStep[0m  [165/339], [94mLoss[0m : 1.83946
[1mStep[0m  [198/339], [94mLoss[0m : 1.57475
[1mStep[0m  [231/339], [94mLoss[0m : 2.01543
[1mStep[0m  [264/339], [94mLoss[0m : 1.62680
[1mStep[0m  [297/339], [94mLoss[0m : 1.47481
[1mStep[0m  [330/339], [94mLoss[0m : 2.15068

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89534
[1mStep[0m  [33/339], [94mLoss[0m : 2.37053
[1mStep[0m  [66/339], [94mLoss[0m : 1.62915
[1mStep[0m  [99/339], [94mLoss[0m : 1.41002
[1mStep[0m  [132/339], [94mLoss[0m : 1.80459
[1mStep[0m  [165/339], [94mLoss[0m : 1.76795
[1mStep[0m  [198/339], [94mLoss[0m : 1.92481
[1mStep[0m  [231/339], [94mLoss[0m : 1.63541
[1mStep[0m  [264/339], [94mLoss[0m : 1.62724
[1mStep[0m  [297/339], [94mLoss[0m : 1.96331
[1mStep[0m  [330/339], [94mLoss[0m : 2.16008

====================================
