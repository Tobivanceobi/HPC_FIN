no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  9
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.84076
[1mStep[0m  [2/26], [94mLoss[0m : 10.60664
[1mStep[0m  [4/26], [94mLoss[0m : 10.75154
[1mStep[0m  [6/26], [94mLoss[0m : 10.59985
[1mStep[0m  [8/26], [94mLoss[0m : 10.40932
[1mStep[0m  [10/26], [94mLoss[0m : 10.19515
[1mStep[0m  [12/26], [94mLoss[0m : 10.36801
[1mStep[0m  [14/26], [94mLoss[0m : 10.19621
[1mStep[0m  [16/26], [94mLoss[0m : 10.06787
[1mStep[0m  [18/26], [94mLoss[0m : 10.13247
[1mStep[0m  [20/26], [94mLoss[0m : 9.93745
[1mStep[0m  [22/26], [94mLoss[0m : 9.82133
[1mStep[0m  [24/26], [94mLoss[0m : 9.30063

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.254, [92mTest[0m: 10.849, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.31682
[1mStep[0m  [2/26], [94mLoss[0m : 9.76970
[1mStep[0m  [4/26], [94mLoss[0m : 9.44693
[1mStep[0m  [6/26], [94mLoss[0m : 9.14934
[1mStep[0m  [8/26], [94mLoss[0m : 9.02082
[1mStep[0m  [10/26], [94mLoss[0m : 9.25005
[1mStep[0m  [12/26], [94mLoss[0m : 9.15122
[1mStep[0m  [14/26], [94mLoss[0m : 8.79534
[1mStep[0m  [16/26], [94mLoss[0m : 9.01979
[1mStep[0m  [18/26], [94mLoss[0m : 8.71532
[1mStep[0m  [20/26], [94mLoss[0m : 8.86780
[1mStep[0m  [22/26], [94mLoss[0m : 8.49813
[1mStep[0m  [24/26], [94mLoss[0m : 8.70708

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.998, [92mTest[0m: 10.159, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.11040
[1mStep[0m  [2/26], [94mLoss[0m : 8.18926
[1mStep[0m  [4/26], [94mLoss[0m : 8.32610
[1mStep[0m  [6/26], [94mLoss[0m : 8.01271
[1mStep[0m  [8/26], [94mLoss[0m : 7.68102
[1mStep[0m  [10/26], [94mLoss[0m : 7.82314
[1mStep[0m  [12/26], [94mLoss[0m : 7.39709
[1mStep[0m  [14/26], [94mLoss[0m : 7.65652
[1mStep[0m  [16/26], [94mLoss[0m : 7.33280
[1mStep[0m  [18/26], [94mLoss[0m : 7.50933
[1mStep[0m  [20/26], [94mLoss[0m : 7.58426
[1mStep[0m  [22/26], [94mLoss[0m : 7.28206
[1mStep[0m  [24/26], [94mLoss[0m : 7.02998

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.685, [92mTest[0m: 9.055, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.91494
[1mStep[0m  [2/26], [94mLoss[0m : 7.08288
[1mStep[0m  [4/26], [94mLoss[0m : 6.51852
[1mStep[0m  [6/26], [94mLoss[0m : 6.83178
[1mStep[0m  [8/26], [94mLoss[0m : 6.80738
[1mStep[0m  [10/26], [94mLoss[0m : 6.51604
[1mStep[0m  [12/26], [94mLoss[0m : 6.20158
[1mStep[0m  [14/26], [94mLoss[0m : 6.33586
[1mStep[0m  [16/26], [94mLoss[0m : 6.51636
[1mStep[0m  [18/26], [94mLoss[0m : 6.01727
[1mStep[0m  [20/26], [94mLoss[0m : 6.08010
[1mStep[0m  [22/26], [94mLoss[0m : 5.72055
[1mStep[0m  [24/26], [94mLoss[0m : 5.83231

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.343, [92mTest[0m: 8.034, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.70815
[1mStep[0m  [2/26], [94mLoss[0m : 5.51372
[1mStep[0m  [4/26], [94mLoss[0m : 5.65101
[1mStep[0m  [6/26], [94mLoss[0m : 5.70486
[1mStep[0m  [8/26], [94mLoss[0m : 5.22980
[1mStep[0m  [10/26], [94mLoss[0m : 4.87312
[1mStep[0m  [12/26], [94mLoss[0m : 5.01275
[1mStep[0m  [14/26], [94mLoss[0m : 4.92396
[1mStep[0m  [16/26], [94mLoss[0m : 4.82150
[1mStep[0m  [18/26], [94mLoss[0m : 4.95071
[1mStep[0m  [20/26], [94mLoss[0m : 4.59558
[1mStep[0m  [22/26], [94mLoss[0m : 4.75671
[1mStep[0m  [24/26], [94mLoss[0m : 4.67010

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.058, [92mTest[0m: 6.914, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.52937
[1mStep[0m  [2/26], [94mLoss[0m : 4.74338
[1mStep[0m  [4/26], [94mLoss[0m : 4.30766
[1mStep[0m  [6/26], [94mLoss[0m : 4.22533
[1mStep[0m  [8/26], [94mLoss[0m : 4.11765
[1mStep[0m  [10/26], [94mLoss[0m : 4.19053
[1mStep[0m  [12/26], [94mLoss[0m : 4.15201
[1mStep[0m  [14/26], [94mLoss[0m : 3.88359
[1mStep[0m  [16/26], [94mLoss[0m : 3.90930
[1mStep[0m  [18/26], [94mLoss[0m : 4.02430
[1mStep[0m  [20/26], [94mLoss[0m : 3.73719
[1mStep[0m  [22/26], [94mLoss[0m : 3.54089
[1mStep[0m  [24/26], [94mLoss[0m : 3.84340

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.051, [92mTest[0m: 5.602, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.42149
[1mStep[0m  [2/26], [94mLoss[0m : 3.81292
[1mStep[0m  [4/26], [94mLoss[0m : 3.47263
[1mStep[0m  [6/26], [94mLoss[0m : 3.38915
[1mStep[0m  [8/26], [94mLoss[0m : 3.59666
[1mStep[0m  [10/26], [94mLoss[0m : 3.49022
[1mStep[0m  [12/26], [94mLoss[0m : 3.18695
[1mStep[0m  [14/26], [94mLoss[0m : 3.41439
[1mStep[0m  [16/26], [94mLoss[0m : 3.67033
[1mStep[0m  [18/26], [94mLoss[0m : 3.39797
[1mStep[0m  [20/26], [94mLoss[0m : 3.34580
[1mStep[0m  [22/26], [94mLoss[0m : 3.27443
[1mStep[0m  [24/26], [94mLoss[0m : 3.37054

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.426, [92mTest[0m: 4.610, [96mlr[0m: 0.005050000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 6 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.891
====================================

Phase 1 - Evaluation MAE:  3.8910889625549316
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 3.31882
[1mStep[0m  [2/26], [94mLoss[0m : 3.16617
[1mStep[0m  [4/26], [94mLoss[0m : 3.21723
[1mStep[0m  [6/26], [94mLoss[0m : 3.13660
[1mStep[0m  [8/26], [94mLoss[0m : 3.11634
[1mStep[0m  [10/26], [94mLoss[0m : 3.23687
[1mStep[0m  [12/26], [94mLoss[0m : 3.24807
[1mStep[0m  [14/26], [94mLoss[0m : 3.12787
[1mStep[0m  [16/26], [94mLoss[0m : 2.91642
[1mStep[0m  [18/26], [94mLoss[0m : 3.16352
[1mStep[0m  [20/26], [94mLoss[0m : 2.92149
[1mStep[0m  [22/26], [94mLoss[0m : 3.04919
[1mStep[0m  [24/26], [94mLoss[0m : 2.97237

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.126, [92mTest[0m: 3.904, [96mlr[0m: 0.005050000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.039
====================================

Phase 2 - Evaluation MAE:  4.0385314501248875
MAE score P1       3.891089
MAE score P2       4.038531
loss               3.125699
learning_rate       0.00505
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.1
weight_decay           0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.42969
[1mStep[0m  [2/26], [94mLoss[0m : 10.29695
[1mStep[0m  [4/26], [94mLoss[0m : 10.06073
[1mStep[0m  [6/26], [94mLoss[0m : 9.81207
[1mStep[0m  [8/26], [94mLoss[0m : 9.62958
[1mStep[0m  [10/26], [94mLoss[0m : 9.13898
[1mStep[0m  [12/26], [94mLoss[0m : 9.04010
[1mStep[0m  [14/26], [94mLoss[0m : 8.81581
[1mStep[0m  [16/26], [94mLoss[0m : 8.60584
[1mStep[0m  [18/26], [94mLoss[0m : 8.01185
[1mStep[0m  [20/26], [94mLoss[0m : 7.65104
[1mStep[0m  [22/26], [94mLoss[0m : 7.43906
[1mStep[0m  [24/26], [94mLoss[0m : 7.20923

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.880, [92mTest[0m: 10.819, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.82619
[1mStep[0m  [2/26], [94mLoss[0m : 6.05031
[1mStep[0m  [4/26], [94mLoss[0m : 6.04237
[1mStep[0m  [6/26], [94mLoss[0m : 5.75581
[1mStep[0m  [8/26], [94mLoss[0m : 5.53417
[1mStep[0m  [10/26], [94mLoss[0m : 5.34877
[1mStep[0m  [12/26], [94mLoss[0m : 5.29113
[1mStep[0m  [14/26], [94mLoss[0m : 4.56662
[1mStep[0m  [16/26], [94mLoss[0m : 4.47063
[1mStep[0m  [18/26], [94mLoss[0m : 4.30857
[1mStep[0m  [20/26], [94mLoss[0m : 3.95447
[1mStep[0m  [22/26], [94mLoss[0m : 3.83635
[1mStep[0m  [24/26], [94mLoss[0m : 3.82643

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.035, [92mTest[0m: 8.060, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.66825
[1mStep[0m  [2/26], [94mLoss[0m : 3.22683
[1mStep[0m  [4/26], [94mLoss[0m : 3.07090
[1mStep[0m  [6/26], [94mLoss[0m : 3.00304
[1mStep[0m  [8/26], [94mLoss[0m : 3.28576
[1mStep[0m  [10/26], [94mLoss[0m : 3.14955
[1mStep[0m  [12/26], [94mLoss[0m : 2.91434
[1mStep[0m  [14/26], [94mLoss[0m : 2.61347
[1mStep[0m  [16/26], [94mLoss[0m : 2.99596
[1mStep[0m  [18/26], [94mLoss[0m : 2.92310
[1mStep[0m  [20/26], [94mLoss[0m : 2.78708
[1mStep[0m  [22/26], [94mLoss[0m : 2.86576
[1mStep[0m  [24/26], [94mLoss[0m : 2.77205

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.001, [92mTest[0m: 4.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64810
[1mStep[0m  [2/26], [94mLoss[0m : 2.65472
[1mStep[0m  [4/26], [94mLoss[0m : 2.80961
[1mStep[0m  [6/26], [94mLoss[0m : 2.53906
[1mStep[0m  [8/26], [94mLoss[0m : 2.48484
[1mStep[0m  [10/26], [94mLoss[0m : 2.48712
[1mStep[0m  [12/26], [94mLoss[0m : 2.58434
[1mStep[0m  [14/26], [94mLoss[0m : 2.69643
[1mStep[0m  [16/26], [94mLoss[0m : 2.67593
[1mStep[0m  [18/26], [94mLoss[0m : 2.64939
[1mStep[0m  [20/26], [94mLoss[0m : 2.62974
[1mStep[0m  [22/26], [94mLoss[0m : 2.53907
[1mStep[0m  [24/26], [94mLoss[0m : 2.59288

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.641, [92mTest[0m: 3.087, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59741
[1mStep[0m  [2/26], [94mLoss[0m : 2.51128
[1mStep[0m  [4/26], [94mLoss[0m : 2.61432
[1mStep[0m  [6/26], [94mLoss[0m : 2.47011
[1mStep[0m  [8/26], [94mLoss[0m : 2.59668
[1mStep[0m  [10/26], [94mLoss[0m : 2.55171
[1mStep[0m  [12/26], [94mLoss[0m : 2.60267
[1mStep[0m  [14/26], [94mLoss[0m : 2.75883
[1mStep[0m  [16/26], [94mLoss[0m : 2.65652
[1mStep[0m  [18/26], [94mLoss[0m : 2.73820
[1mStep[0m  [20/26], [94mLoss[0m : 2.56295
[1mStep[0m  [22/26], [94mLoss[0m : 2.71197
[1mStep[0m  [24/26], [94mLoss[0m : 2.49365

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.745, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50811
[1mStep[0m  [2/26], [94mLoss[0m : 2.62424
[1mStep[0m  [4/26], [94mLoss[0m : 2.58652
[1mStep[0m  [6/26], [94mLoss[0m : 2.52841
[1mStep[0m  [8/26], [94mLoss[0m : 2.61835
[1mStep[0m  [10/26], [94mLoss[0m : 2.47117
[1mStep[0m  [12/26], [94mLoss[0m : 2.60882
[1mStep[0m  [14/26], [94mLoss[0m : 2.53088
[1mStep[0m  [16/26], [94mLoss[0m : 2.52167
[1mStep[0m  [18/26], [94mLoss[0m : 2.48824
[1mStep[0m  [20/26], [94mLoss[0m : 2.60106
[1mStep[0m  [22/26], [94mLoss[0m : 2.50588
[1mStep[0m  [24/26], [94mLoss[0m : 2.67780

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.667, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40519
[1mStep[0m  [2/26], [94mLoss[0m : 2.73921
[1mStep[0m  [4/26], [94mLoss[0m : 2.62554
[1mStep[0m  [6/26], [94mLoss[0m : 2.60119
[1mStep[0m  [8/26], [94mLoss[0m : 2.46243
[1mStep[0m  [10/26], [94mLoss[0m : 2.61569
[1mStep[0m  [12/26], [94mLoss[0m : 2.55152
[1mStep[0m  [14/26], [94mLoss[0m : 2.39912
[1mStep[0m  [16/26], [94mLoss[0m : 2.56514
[1mStep[0m  [18/26], [94mLoss[0m : 2.51890
[1mStep[0m  [20/26], [94mLoss[0m : 2.66587
[1mStep[0m  [22/26], [94mLoss[0m : 2.56135
[1mStep[0m  [24/26], [94mLoss[0m : 2.49020

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.664, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49621
[1mStep[0m  [2/26], [94mLoss[0m : 2.45674
[1mStep[0m  [4/26], [94mLoss[0m : 2.62889
[1mStep[0m  [6/26], [94mLoss[0m : 2.51241
[1mStep[0m  [8/26], [94mLoss[0m : 2.47727
[1mStep[0m  [10/26], [94mLoss[0m : 2.51598
[1mStep[0m  [12/26], [94mLoss[0m : 2.51449
[1mStep[0m  [14/26], [94mLoss[0m : 2.43820
[1mStep[0m  [16/26], [94mLoss[0m : 2.56402
[1mStep[0m  [18/26], [94mLoss[0m : 2.39982
[1mStep[0m  [20/26], [94mLoss[0m : 2.70948
[1mStep[0m  [22/26], [94mLoss[0m : 2.47087
[1mStep[0m  [24/26], [94mLoss[0m : 2.48999

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.560, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47408
[1mStep[0m  [2/26], [94mLoss[0m : 2.65068
[1mStep[0m  [4/26], [94mLoss[0m : 2.58158
[1mStep[0m  [6/26], [94mLoss[0m : 2.52009
[1mStep[0m  [8/26], [94mLoss[0m : 2.50328
[1mStep[0m  [10/26], [94mLoss[0m : 2.44844
[1mStep[0m  [12/26], [94mLoss[0m : 2.42132
[1mStep[0m  [14/26], [94mLoss[0m : 2.47219
[1mStep[0m  [16/26], [94mLoss[0m : 2.43833
[1mStep[0m  [18/26], [94mLoss[0m : 2.53144
[1mStep[0m  [20/26], [94mLoss[0m : 2.62913
[1mStep[0m  [22/26], [94mLoss[0m : 2.41130
[1mStep[0m  [24/26], [94mLoss[0m : 2.65455

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.593, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44463
[1mStep[0m  [2/26], [94mLoss[0m : 2.32941
[1mStep[0m  [4/26], [94mLoss[0m : 2.52257
[1mStep[0m  [6/26], [94mLoss[0m : 2.62177
[1mStep[0m  [8/26], [94mLoss[0m : 2.42683
[1mStep[0m  [10/26], [94mLoss[0m : 2.47338
[1mStep[0m  [12/26], [94mLoss[0m : 2.52578
[1mStep[0m  [14/26], [94mLoss[0m : 2.63513
[1mStep[0m  [16/26], [94mLoss[0m : 2.65377
[1mStep[0m  [18/26], [94mLoss[0m : 2.53742
[1mStep[0m  [20/26], [94mLoss[0m : 2.62795
[1mStep[0m  [22/26], [94mLoss[0m : 2.39747
[1mStep[0m  [24/26], [94mLoss[0m : 2.60026

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.562, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55943
[1mStep[0m  [2/26], [94mLoss[0m : 2.50359
[1mStep[0m  [4/26], [94mLoss[0m : 2.46946
[1mStep[0m  [6/26], [94mLoss[0m : 2.49199
[1mStep[0m  [8/26], [94mLoss[0m : 2.31198
[1mStep[0m  [10/26], [94mLoss[0m : 2.45640
[1mStep[0m  [12/26], [94mLoss[0m : 2.53019
[1mStep[0m  [14/26], [94mLoss[0m : 2.47260
[1mStep[0m  [16/26], [94mLoss[0m : 2.64648
[1mStep[0m  [18/26], [94mLoss[0m : 2.45624
[1mStep[0m  [20/26], [94mLoss[0m : 2.51162
[1mStep[0m  [22/26], [94mLoss[0m : 2.44053
[1mStep[0m  [24/26], [94mLoss[0m : 2.52252

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.581, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52536
[1mStep[0m  [2/26], [94mLoss[0m : 2.44568
[1mStep[0m  [4/26], [94mLoss[0m : 2.60504
[1mStep[0m  [6/26], [94mLoss[0m : 2.41923
[1mStep[0m  [8/26], [94mLoss[0m : 2.49204
[1mStep[0m  [10/26], [94mLoss[0m : 2.54104
[1mStep[0m  [12/26], [94mLoss[0m : 2.45148
[1mStep[0m  [14/26], [94mLoss[0m : 2.37009
[1mStep[0m  [16/26], [94mLoss[0m : 2.57686
[1mStep[0m  [18/26], [94mLoss[0m : 2.35868
[1mStep[0m  [20/26], [94mLoss[0m : 2.64319
[1mStep[0m  [22/26], [94mLoss[0m : 2.58792
[1mStep[0m  [24/26], [94mLoss[0m : 2.57931

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58748
[1mStep[0m  [2/26], [94mLoss[0m : 2.36641
[1mStep[0m  [4/26], [94mLoss[0m : 2.38134
[1mStep[0m  [6/26], [94mLoss[0m : 2.44894
[1mStep[0m  [8/26], [94mLoss[0m : 2.66652
[1mStep[0m  [10/26], [94mLoss[0m : 2.49558
[1mStep[0m  [12/26], [94mLoss[0m : 2.53370
[1mStep[0m  [14/26], [94mLoss[0m : 2.42119
[1mStep[0m  [16/26], [94mLoss[0m : 2.38634
[1mStep[0m  [18/26], [94mLoss[0m : 2.52048
[1mStep[0m  [20/26], [94mLoss[0m : 2.49978
[1mStep[0m  [22/26], [94mLoss[0m : 2.39692
[1mStep[0m  [24/26], [94mLoss[0m : 2.45943

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.578, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46520
[1mStep[0m  [2/26], [94mLoss[0m : 2.31562
[1mStep[0m  [4/26], [94mLoss[0m : 2.47759
[1mStep[0m  [6/26], [94mLoss[0m : 2.45053
[1mStep[0m  [8/26], [94mLoss[0m : 2.62964
[1mStep[0m  [10/26], [94mLoss[0m : 2.39803
[1mStep[0m  [12/26], [94mLoss[0m : 2.48181
[1mStep[0m  [14/26], [94mLoss[0m : 2.48661
[1mStep[0m  [16/26], [94mLoss[0m : 2.48290
[1mStep[0m  [18/26], [94mLoss[0m : 2.57071
[1mStep[0m  [20/26], [94mLoss[0m : 2.48573
[1mStep[0m  [22/26], [94mLoss[0m : 2.53878
[1mStep[0m  [24/26], [94mLoss[0m : 2.38674

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39594
[1mStep[0m  [2/26], [94mLoss[0m : 2.43238
[1mStep[0m  [4/26], [94mLoss[0m : 2.53835
[1mStep[0m  [6/26], [94mLoss[0m : 2.36188
[1mStep[0m  [8/26], [94mLoss[0m : 2.48556
[1mStep[0m  [10/26], [94mLoss[0m : 2.40209
[1mStep[0m  [12/26], [94mLoss[0m : 2.56520
[1mStep[0m  [14/26], [94mLoss[0m : 2.44578
[1mStep[0m  [16/26], [94mLoss[0m : 2.44547
[1mStep[0m  [18/26], [94mLoss[0m : 2.49091
[1mStep[0m  [20/26], [94mLoss[0m : 2.45875
[1mStep[0m  [22/26], [94mLoss[0m : 2.45878
[1mStep[0m  [24/26], [94mLoss[0m : 2.35852

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.525, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53340
[1mStep[0m  [2/26], [94mLoss[0m : 2.28634
[1mStep[0m  [4/26], [94mLoss[0m : 2.46000
[1mStep[0m  [6/26], [94mLoss[0m : 2.57976
[1mStep[0m  [8/26], [94mLoss[0m : 2.47011
[1mStep[0m  [10/26], [94mLoss[0m : 2.44565
[1mStep[0m  [12/26], [94mLoss[0m : 2.47961
[1mStep[0m  [14/26], [94mLoss[0m : 2.28342
[1mStep[0m  [16/26], [94mLoss[0m : 2.42095
[1mStep[0m  [18/26], [94mLoss[0m : 2.48437
[1mStep[0m  [20/26], [94mLoss[0m : 2.54142
[1mStep[0m  [22/26], [94mLoss[0m : 2.49048
[1mStep[0m  [24/26], [94mLoss[0m : 2.50451

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47971
[1mStep[0m  [2/26], [94mLoss[0m : 2.43304
[1mStep[0m  [4/26], [94mLoss[0m : 2.56152
[1mStep[0m  [6/26], [94mLoss[0m : 2.66274
[1mStep[0m  [8/26], [94mLoss[0m : 2.56504
[1mStep[0m  [10/26], [94mLoss[0m : 2.37332
[1mStep[0m  [12/26], [94mLoss[0m : 2.54388
[1mStep[0m  [14/26], [94mLoss[0m : 2.48552
[1mStep[0m  [16/26], [94mLoss[0m : 2.50639
[1mStep[0m  [18/26], [94mLoss[0m : 2.50915
[1mStep[0m  [20/26], [94mLoss[0m : 2.39014
[1mStep[0m  [22/26], [94mLoss[0m : 2.49312
[1mStep[0m  [24/26], [94mLoss[0m : 2.44999

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37665
[1mStep[0m  [2/26], [94mLoss[0m : 2.57490
[1mStep[0m  [4/26], [94mLoss[0m : 2.54509
[1mStep[0m  [6/26], [94mLoss[0m : 2.59824
[1mStep[0m  [8/26], [94mLoss[0m : 2.57265
[1mStep[0m  [10/26], [94mLoss[0m : 2.46465
[1mStep[0m  [12/26], [94mLoss[0m : 2.51393
[1mStep[0m  [14/26], [94mLoss[0m : 2.33172
[1mStep[0m  [16/26], [94mLoss[0m : 2.32693
[1mStep[0m  [18/26], [94mLoss[0m : 2.44244
[1mStep[0m  [20/26], [94mLoss[0m : 2.58455
[1mStep[0m  [22/26], [94mLoss[0m : 2.49158
[1mStep[0m  [24/26], [94mLoss[0m : 2.54857

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68990
[1mStep[0m  [2/26], [94mLoss[0m : 2.26799
[1mStep[0m  [4/26], [94mLoss[0m : 2.42963
[1mStep[0m  [6/26], [94mLoss[0m : 2.29685
[1mStep[0m  [8/26], [94mLoss[0m : 2.30253
[1mStep[0m  [10/26], [94mLoss[0m : 2.28611
[1mStep[0m  [12/26], [94mLoss[0m : 2.39683
[1mStep[0m  [14/26], [94mLoss[0m : 2.49098
[1mStep[0m  [16/26], [94mLoss[0m : 2.40612
[1mStep[0m  [18/26], [94mLoss[0m : 2.45857
[1mStep[0m  [20/26], [94mLoss[0m : 2.44944
[1mStep[0m  [22/26], [94mLoss[0m : 2.39362
[1mStep[0m  [24/26], [94mLoss[0m : 2.42263

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.531, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60592
[1mStep[0m  [2/26], [94mLoss[0m : 2.43761
[1mStep[0m  [4/26], [94mLoss[0m : 2.48947
[1mStep[0m  [6/26], [94mLoss[0m : 2.40979
[1mStep[0m  [8/26], [94mLoss[0m : 2.55375
[1mStep[0m  [10/26], [94mLoss[0m : 2.35958
[1mStep[0m  [12/26], [94mLoss[0m : 2.53527
[1mStep[0m  [14/26], [94mLoss[0m : 2.47235
[1mStep[0m  [16/26], [94mLoss[0m : 2.72038
[1mStep[0m  [18/26], [94mLoss[0m : 2.44300
[1mStep[0m  [20/26], [94mLoss[0m : 2.40246
[1mStep[0m  [22/26], [94mLoss[0m : 2.52344
[1mStep[0m  [24/26], [94mLoss[0m : 2.53858

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47685
[1mStep[0m  [2/26], [94mLoss[0m : 2.47701
[1mStep[0m  [4/26], [94mLoss[0m : 2.47003
[1mStep[0m  [6/26], [94mLoss[0m : 2.28166
[1mStep[0m  [8/26], [94mLoss[0m : 2.46779
[1mStep[0m  [10/26], [94mLoss[0m : 2.37117
[1mStep[0m  [12/26], [94mLoss[0m : 2.44137
[1mStep[0m  [14/26], [94mLoss[0m : 2.41065
[1mStep[0m  [16/26], [94mLoss[0m : 2.47856
[1mStep[0m  [18/26], [94mLoss[0m : 2.58248
[1mStep[0m  [20/26], [94mLoss[0m : 2.43055
[1mStep[0m  [22/26], [94mLoss[0m : 2.47998
[1mStep[0m  [24/26], [94mLoss[0m : 2.43169

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19269
[1mStep[0m  [2/26], [94mLoss[0m : 2.33231
[1mStep[0m  [4/26], [94mLoss[0m : 2.50867
[1mStep[0m  [6/26], [94mLoss[0m : 2.36224
[1mStep[0m  [8/26], [94mLoss[0m : 2.57569
[1mStep[0m  [10/26], [94mLoss[0m : 2.57237
[1mStep[0m  [12/26], [94mLoss[0m : 2.43619
[1mStep[0m  [14/26], [94mLoss[0m : 2.60301
[1mStep[0m  [16/26], [94mLoss[0m : 2.22685
[1mStep[0m  [18/26], [94mLoss[0m : 2.45548
[1mStep[0m  [20/26], [94mLoss[0m : 2.43777
[1mStep[0m  [22/26], [94mLoss[0m : 2.40768
[1mStep[0m  [24/26], [94mLoss[0m : 2.31752

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55952
[1mStep[0m  [2/26], [94mLoss[0m : 2.34825
[1mStep[0m  [4/26], [94mLoss[0m : 2.55557
[1mStep[0m  [6/26], [94mLoss[0m : 2.41824
[1mStep[0m  [8/26], [94mLoss[0m : 2.49580
[1mStep[0m  [10/26], [94mLoss[0m : 2.35293
[1mStep[0m  [12/26], [94mLoss[0m : 2.40223
[1mStep[0m  [14/26], [94mLoss[0m : 2.37981
[1mStep[0m  [16/26], [94mLoss[0m : 2.34347
[1mStep[0m  [18/26], [94mLoss[0m : 2.41911
[1mStep[0m  [20/26], [94mLoss[0m : 2.55053
[1mStep[0m  [22/26], [94mLoss[0m : 2.39004
[1mStep[0m  [24/26], [94mLoss[0m : 2.37786

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.525, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44500
[1mStep[0m  [2/26], [94mLoss[0m : 2.35945
[1mStep[0m  [4/26], [94mLoss[0m : 2.33387
[1mStep[0m  [6/26], [94mLoss[0m : 2.43553
[1mStep[0m  [8/26], [94mLoss[0m : 2.53527
[1mStep[0m  [10/26], [94mLoss[0m : 2.45907
[1mStep[0m  [12/26], [94mLoss[0m : 2.52647
[1mStep[0m  [14/26], [94mLoss[0m : 2.39508
[1mStep[0m  [16/26], [94mLoss[0m : 2.44920
[1mStep[0m  [18/26], [94mLoss[0m : 2.38796
[1mStep[0m  [20/26], [94mLoss[0m : 2.45687
[1mStep[0m  [22/26], [94mLoss[0m : 2.52896
[1mStep[0m  [24/26], [94mLoss[0m : 2.46659

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26540
[1mStep[0m  [2/26], [94mLoss[0m : 2.46889
[1mStep[0m  [4/26], [94mLoss[0m : 2.48323
[1mStep[0m  [6/26], [94mLoss[0m : 2.45908
[1mStep[0m  [8/26], [94mLoss[0m : 2.40221
[1mStep[0m  [10/26], [94mLoss[0m : 2.44552
[1mStep[0m  [12/26], [94mLoss[0m : 2.33663
[1mStep[0m  [14/26], [94mLoss[0m : 2.48437
[1mStep[0m  [16/26], [94mLoss[0m : 2.36211
[1mStep[0m  [18/26], [94mLoss[0m : 2.49091
[1mStep[0m  [20/26], [94mLoss[0m : 2.47726
[1mStep[0m  [22/26], [94mLoss[0m : 2.41097
[1mStep[0m  [24/26], [94mLoss[0m : 2.50978

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54483
[1mStep[0m  [2/26], [94mLoss[0m : 2.23696
[1mStep[0m  [4/26], [94mLoss[0m : 2.45555
[1mStep[0m  [6/26], [94mLoss[0m : 2.43539
[1mStep[0m  [8/26], [94mLoss[0m : 2.51591
[1mStep[0m  [10/26], [94mLoss[0m : 2.40423
[1mStep[0m  [12/26], [94mLoss[0m : 2.39621
[1mStep[0m  [14/26], [94mLoss[0m : 2.25888
[1mStep[0m  [16/26], [94mLoss[0m : 2.71272
[1mStep[0m  [18/26], [94mLoss[0m : 2.41600
[1mStep[0m  [20/26], [94mLoss[0m : 2.40276
[1mStep[0m  [22/26], [94mLoss[0m : 2.39110
[1mStep[0m  [24/26], [94mLoss[0m : 2.50847

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44857
[1mStep[0m  [2/26], [94mLoss[0m : 2.40677
[1mStep[0m  [4/26], [94mLoss[0m : 2.33821
[1mStep[0m  [6/26], [94mLoss[0m : 2.40676
[1mStep[0m  [8/26], [94mLoss[0m : 2.53981
[1mStep[0m  [10/26], [94mLoss[0m : 2.37115
[1mStep[0m  [12/26], [94mLoss[0m : 2.48781
[1mStep[0m  [14/26], [94mLoss[0m : 2.36133
[1mStep[0m  [16/26], [94mLoss[0m : 2.35207
[1mStep[0m  [18/26], [94mLoss[0m : 2.55808
[1mStep[0m  [20/26], [94mLoss[0m : 2.45386
[1mStep[0m  [22/26], [94mLoss[0m : 2.54564
[1mStep[0m  [24/26], [94mLoss[0m : 2.35369

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46728
[1mStep[0m  [2/26], [94mLoss[0m : 2.33152
[1mStep[0m  [4/26], [94mLoss[0m : 2.30783
[1mStep[0m  [6/26], [94mLoss[0m : 2.32108
[1mStep[0m  [8/26], [94mLoss[0m : 2.37585
[1mStep[0m  [10/26], [94mLoss[0m : 2.50573
[1mStep[0m  [12/26], [94mLoss[0m : 2.43532
[1mStep[0m  [14/26], [94mLoss[0m : 2.37831
[1mStep[0m  [16/26], [94mLoss[0m : 2.28168
[1mStep[0m  [18/26], [94mLoss[0m : 2.32516
[1mStep[0m  [20/26], [94mLoss[0m : 2.56374
[1mStep[0m  [22/26], [94mLoss[0m : 2.49723
[1mStep[0m  [24/26], [94mLoss[0m : 2.37082

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56272
[1mStep[0m  [2/26], [94mLoss[0m : 2.46578
[1mStep[0m  [4/26], [94mLoss[0m : 2.43887
[1mStep[0m  [6/26], [94mLoss[0m : 2.46612
[1mStep[0m  [8/26], [94mLoss[0m : 2.47156
[1mStep[0m  [10/26], [94mLoss[0m : 2.61197
[1mStep[0m  [12/26], [94mLoss[0m : 2.44336
[1mStep[0m  [14/26], [94mLoss[0m : 2.41119
[1mStep[0m  [16/26], [94mLoss[0m : 2.34838
[1mStep[0m  [18/26], [94mLoss[0m : 2.46939
[1mStep[0m  [20/26], [94mLoss[0m : 2.49372
[1mStep[0m  [22/26], [94mLoss[0m : 2.58148
[1mStep[0m  [24/26], [94mLoss[0m : 2.44786

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42828
[1mStep[0m  [2/26], [94mLoss[0m : 2.53419
[1mStep[0m  [4/26], [94mLoss[0m : 2.43780
[1mStep[0m  [6/26], [94mLoss[0m : 2.50339
[1mStep[0m  [8/26], [94mLoss[0m : 2.48460
[1mStep[0m  [10/26], [94mLoss[0m : 2.32874
[1mStep[0m  [12/26], [94mLoss[0m : 2.52306
[1mStep[0m  [14/26], [94mLoss[0m : 2.43841
[1mStep[0m  [16/26], [94mLoss[0m : 2.51099
[1mStep[0m  [18/26], [94mLoss[0m : 2.46729
[1mStep[0m  [20/26], [94mLoss[0m : 2.40522
[1mStep[0m  [22/26], [94mLoss[0m : 2.58861
[1mStep[0m  [24/26], [94mLoss[0m : 2.34815

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.480, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.452
====================================

Phase 1 - Evaluation MAE:  2.4523246104900656
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.50919
[1mStep[0m  [2/26], [94mLoss[0m : 2.47298
[1mStep[0m  [4/26], [94mLoss[0m : 2.45283
[1mStep[0m  [6/26], [94mLoss[0m : 2.45981
[1mStep[0m  [8/26], [94mLoss[0m : 2.52572
[1mStep[0m  [10/26], [94mLoss[0m : 2.51409
[1mStep[0m  [12/26], [94mLoss[0m : 2.49872
[1mStep[0m  [14/26], [94mLoss[0m : 2.43856
[1mStep[0m  [16/26], [94mLoss[0m : 2.64808
[1mStep[0m  [18/26], [94mLoss[0m : 2.37803
[1mStep[0m  [20/26], [94mLoss[0m : 2.52335
[1mStep[0m  [22/26], [94mLoss[0m : 2.55590
[1mStep[0m  [24/26], [94mLoss[0m : 2.50939

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30058
[1mStep[0m  [2/26], [94mLoss[0m : 2.49642
[1mStep[0m  [4/26], [94mLoss[0m : 2.38350
[1mStep[0m  [6/26], [94mLoss[0m : 2.53424
[1mStep[0m  [8/26], [94mLoss[0m : 2.32729
[1mStep[0m  [10/26], [94mLoss[0m : 2.60633
[1mStep[0m  [12/26], [94mLoss[0m : 2.53625
[1mStep[0m  [14/26], [94mLoss[0m : 2.49061
[1mStep[0m  [16/26], [94mLoss[0m : 2.43965
[1mStep[0m  [18/26], [94mLoss[0m : 2.45141
[1mStep[0m  [20/26], [94mLoss[0m : 2.45386
[1mStep[0m  [22/26], [94mLoss[0m : 2.52917
[1mStep[0m  [24/26], [94mLoss[0m : 2.45185

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.651, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37010
[1mStep[0m  [2/26], [94mLoss[0m : 2.46571
[1mStep[0m  [4/26], [94mLoss[0m : 2.42173
[1mStep[0m  [6/26], [94mLoss[0m : 2.45024
[1mStep[0m  [8/26], [94mLoss[0m : 2.60903
[1mStep[0m  [10/26], [94mLoss[0m : 2.40981
[1mStep[0m  [12/26], [94mLoss[0m : 2.41876
[1mStep[0m  [14/26], [94mLoss[0m : 2.45801
[1mStep[0m  [16/26], [94mLoss[0m : 2.37234
[1mStep[0m  [18/26], [94mLoss[0m : 2.36512
[1mStep[0m  [20/26], [94mLoss[0m : 2.34207
[1mStep[0m  [22/26], [94mLoss[0m : 2.39112
[1mStep[0m  [24/26], [94mLoss[0m : 2.22976

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39093
[1mStep[0m  [2/26], [94mLoss[0m : 2.46899
[1mStep[0m  [4/26], [94mLoss[0m : 2.32853
[1mStep[0m  [6/26], [94mLoss[0m : 2.38895
[1mStep[0m  [8/26], [94mLoss[0m : 2.40696
[1mStep[0m  [10/26], [94mLoss[0m : 2.34572
[1mStep[0m  [12/26], [94mLoss[0m : 2.49511
[1mStep[0m  [14/26], [94mLoss[0m : 2.20389
[1mStep[0m  [16/26], [94mLoss[0m : 2.56212
[1mStep[0m  [18/26], [94mLoss[0m : 2.35820
[1mStep[0m  [20/26], [94mLoss[0m : 2.41060
[1mStep[0m  [22/26], [94mLoss[0m : 2.24007
[1mStep[0m  [24/26], [94mLoss[0m : 2.60248

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34487
[1mStep[0m  [2/26], [94mLoss[0m : 2.32822
[1mStep[0m  [4/26], [94mLoss[0m : 2.49462
[1mStep[0m  [6/26], [94mLoss[0m : 2.25433
[1mStep[0m  [8/26], [94mLoss[0m : 2.45333
[1mStep[0m  [10/26], [94mLoss[0m : 2.33217
[1mStep[0m  [12/26], [94mLoss[0m : 2.31147
[1mStep[0m  [14/26], [94mLoss[0m : 2.32644
[1mStep[0m  [16/26], [94mLoss[0m : 2.32935
[1mStep[0m  [18/26], [94mLoss[0m : 2.30741
[1mStep[0m  [20/26], [94mLoss[0m : 2.32917
[1mStep[0m  [22/26], [94mLoss[0m : 2.33885
[1mStep[0m  [24/26], [94mLoss[0m : 2.18935

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30512
[1mStep[0m  [2/26], [94mLoss[0m : 2.31959
[1mStep[0m  [4/26], [94mLoss[0m : 2.33471
[1mStep[0m  [6/26], [94mLoss[0m : 2.43081
[1mStep[0m  [8/26], [94mLoss[0m : 2.24285
[1mStep[0m  [10/26], [94mLoss[0m : 2.59469
[1mStep[0m  [12/26], [94mLoss[0m : 2.27694
[1mStep[0m  [14/26], [94mLoss[0m : 2.43612
[1mStep[0m  [16/26], [94mLoss[0m : 2.38874
[1mStep[0m  [18/26], [94mLoss[0m : 2.19461
[1mStep[0m  [20/26], [94mLoss[0m : 2.36624
[1mStep[0m  [22/26], [94mLoss[0m : 2.35056
[1mStep[0m  [24/26], [94mLoss[0m : 2.39892

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34365
[1mStep[0m  [2/26], [94mLoss[0m : 2.36227
[1mStep[0m  [4/26], [94mLoss[0m : 2.39759
[1mStep[0m  [6/26], [94mLoss[0m : 2.46872
[1mStep[0m  [8/26], [94mLoss[0m : 2.21537
[1mStep[0m  [10/26], [94mLoss[0m : 2.28280
[1mStep[0m  [12/26], [94mLoss[0m : 2.11319
[1mStep[0m  [14/26], [94mLoss[0m : 2.29700
[1mStep[0m  [16/26], [94mLoss[0m : 2.27588
[1mStep[0m  [18/26], [94mLoss[0m : 2.37047
[1mStep[0m  [20/26], [94mLoss[0m : 2.32505
[1mStep[0m  [22/26], [94mLoss[0m : 2.57444
[1mStep[0m  [24/26], [94mLoss[0m : 2.33482

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26885
[1mStep[0m  [2/26], [94mLoss[0m : 2.39809
[1mStep[0m  [4/26], [94mLoss[0m : 2.16599
[1mStep[0m  [6/26], [94mLoss[0m : 2.27012
[1mStep[0m  [8/26], [94mLoss[0m : 2.14678
[1mStep[0m  [10/26], [94mLoss[0m : 2.24588
[1mStep[0m  [12/26], [94mLoss[0m : 2.31159
[1mStep[0m  [14/26], [94mLoss[0m : 2.18636
[1mStep[0m  [16/26], [94mLoss[0m : 2.24388
[1mStep[0m  [18/26], [94mLoss[0m : 2.28379
[1mStep[0m  [20/26], [94mLoss[0m : 2.48965
[1mStep[0m  [22/26], [94mLoss[0m : 2.42396
[1mStep[0m  [24/26], [94mLoss[0m : 2.29874

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26911
[1mStep[0m  [2/26], [94mLoss[0m : 2.36103
[1mStep[0m  [4/26], [94mLoss[0m : 2.33005
[1mStep[0m  [6/26], [94mLoss[0m : 2.25606
[1mStep[0m  [8/26], [94mLoss[0m : 2.28018
[1mStep[0m  [10/26], [94mLoss[0m : 2.26530
[1mStep[0m  [12/26], [94mLoss[0m : 2.12523
[1mStep[0m  [14/26], [94mLoss[0m : 2.28323
[1mStep[0m  [16/26], [94mLoss[0m : 2.20091
[1mStep[0m  [18/26], [94mLoss[0m : 2.39917
[1mStep[0m  [20/26], [94mLoss[0m : 2.20351
[1mStep[0m  [22/26], [94mLoss[0m : 2.32095
[1mStep[0m  [24/26], [94mLoss[0m : 2.31108

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26841
[1mStep[0m  [2/26], [94mLoss[0m : 2.22712
[1mStep[0m  [4/26], [94mLoss[0m : 2.21530
[1mStep[0m  [6/26], [94mLoss[0m : 2.22904
[1mStep[0m  [8/26], [94mLoss[0m : 2.20811
[1mStep[0m  [10/26], [94mLoss[0m : 2.19377
[1mStep[0m  [12/26], [94mLoss[0m : 2.23477
[1mStep[0m  [14/26], [94mLoss[0m : 2.22443
[1mStep[0m  [16/26], [94mLoss[0m : 2.32898
[1mStep[0m  [18/26], [94mLoss[0m : 2.23465
[1mStep[0m  [20/26], [94mLoss[0m : 2.22938
[1mStep[0m  [22/26], [94mLoss[0m : 2.29536
[1mStep[0m  [24/26], [94mLoss[0m : 2.29811

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24255
[1mStep[0m  [2/26], [94mLoss[0m : 2.16458
[1mStep[0m  [4/26], [94mLoss[0m : 2.09902
[1mStep[0m  [6/26], [94mLoss[0m : 2.20914
[1mStep[0m  [8/26], [94mLoss[0m : 2.25468
[1mStep[0m  [10/26], [94mLoss[0m : 2.37703
[1mStep[0m  [12/26], [94mLoss[0m : 2.11666
[1mStep[0m  [14/26], [94mLoss[0m : 2.31725
[1mStep[0m  [16/26], [94mLoss[0m : 2.30618
[1mStep[0m  [18/26], [94mLoss[0m : 2.07377
[1mStep[0m  [20/26], [94mLoss[0m : 2.20957
[1mStep[0m  [22/26], [94mLoss[0m : 2.21117
[1mStep[0m  [24/26], [94mLoss[0m : 2.09445

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22299
[1mStep[0m  [2/26], [94mLoss[0m : 2.09074
[1mStep[0m  [4/26], [94mLoss[0m : 2.12151
[1mStep[0m  [6/26], [94mLoss[0m : 2.32338
[1mStep[0m  [8/26], [94mLoss[0m : 2.17304
[1mStep[0m  [10/26], [94mLoss[0m : 2.17389
[1mStep[0m  [12/26], [94mLoss[0m : 2.22965
[1mStep[0m  [14/26], [94mLoss[0m : 2.18969
[1mStep[0m  [16/26], [94mLoss[0m : 2.10138
[1mStep[0m  [18/26], [94mLoss[0m : 2.23914
[1mStep[0m  [20/26], [94mLoss[0m : 2.17187
[1mStep[0m  [22/26], [94mLoss[0m : 2.21102
[1mStep[0m  [24/26], [94mLoss[0m : 2.11168

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21798
[1mStep[0m  [2/26], [94mLoss[0m : 2.11546
[1mStep[0m  [4/26], [94mLoss[0m : 2.08531
[1mStep[0m  [6/26], [94mLoss[0m : 2.37267
[1mStep[0m  [8/26], [94mLoss[0m : 2.16396
[1mStep[0m  [10/26], [94mLoss[0m : 2.15138
[1mStep[0m  [12/26], [94mLoss[0m : 2.17087
[1mStep[0m  [14/26], [94mLoss[0m : 2.20250
[1mStep[0m  [16/26], [94mLoss[0m : 2.17498
[1mStep[0m  [18/26], [94mLoss[0m : 2.14815
[1mStep[0m  [20/26], [94mLoss[0m : 2.26506
[1mStep[0m  [22/26], [94mLoss[0m : 2.19495
[1mStep[0m  [24/26], [94mLoss[0m : 1.97784

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.05514
[1mStep[0m  [2/26], [94mLoss[0m : 2.17250
[1mStep[0m  [4/26], [94mLoss[0m : 2.05392
[1mStep[0m  [6/26], [94mLoss[0m : 1.96887
[1mStep[0m  [8/26], [94mLoss[0m : 2.01005
[1mStep[0m  [10/26], [94mLoss[0m : 2.13794
[1mStep[0m  [12/26], [94mLoss[0m : 2.13970
[1mStep[0m  [14/26], [94mLoss[0m : 2.18028
[1mStep[0m  [16/26], [94mLoss[0m : 1.93482
[1mStep[0m  [18/26], [94mLoss[0m : 2.14220
[1mStep[0m  [20/26], [94mLoss[0m : 2.06563
[1mStep[0m  [22/26], [94mLoss[0m : 2.22447
[1mStep[0m  [24/26], [94mLoss[0m : 2.16386

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.01447
[1mStep[0m  [2/26], [94mLoss[0m : 2.05727
[1mStep[0m  [4/26], [94mLoss[0m : 2.01678
[1mStep[0m  [6/26], [94mLoss[0m : 2.09933
[1mStep[0m  [8/26], [94mLoss[0m : 1.98525
[1mStep[0m  [10/26], [94mLoss[0m : 1.96440
[1mStep[0m  [12/26], [94mLoss[0m : 2.14065
[1mStep[0m  [14/26], [94mLoss[0m : 2.19744
[1mStep[0m  [16/26], [94mLoss[0m : 2.19107
[1mStep[0m  [18/26], [94mLoss[0m : 2.17586
[1mStep[0m  [20/26], [94mLoss[0m : 2.01849
[1mStep[0m  [22/26], [94mLoss[0m : 2.16240
[1mStep[0m  [24/26], [94mLoss[0m : 2.16231

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96556
[1mStep[0m  [2/26], [94mLoss[0m : 1.95686
[1mStep[0m  [4/26], [94mLoss[0m : 1.90297
[1mStep[0m  [6/26], [94mLoss[0m : 2.01425
[1mStep[0m  [8/26], [94mLoss[0m : 2.10825
[1mStep[0m  [10/26], [94mLoss[0m : 1.99511
[1mStep[0m  [12/26], [94mLoss[0m : 1.94540
[1mStep[0m  [14/26], [94mLoss[0m : 2.16051
[1mStep[0m  [16/26], [94mLoss[0m : 2.09640
[1mStep[0m  [18/26], [94mLoss[0m : 2.02321
[1mStep[0m  [20/26], [94mLoss[0m : 2.09594
[1mStep[0m  [22/26], [94mLoss[0m : 1.94210
[1mStep[0m  [24/26], [94mLoss[0m : 2.20504

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.94085
[1mStep[0m  [2/26], [94mLoss[0m : 1.92791
[1mStep[0m  [4/26], [94mLoss[0m : 2.12288
[1mStep[0m  [6/26], [94mLoss[0m : 1.99951
[1mStep[0m  [8/26], [94mLoss[0m : 1.99376
[1mStep[0m  [10/26], [94mLoss[0m : 1.97271
[1mStep[0m  [12/26], [94mLoss[0m : 1.98778
[1mStep[0m  [14/26], [94mLoss[0m : 2.02597
[1mStep[0m  [16/26], [94mLoss[0m : 2.10178
[1mStep[0m  [18/26], [94mLoss[0m : 1.88705
[1mStep[0m  [20/26], [94mLoss[0m : 1.98015
[1mStep[0m  [22/26], [94mLoss[0m : 2.11285
[1mStep[0m  [24/26], [94mLoss[0m : 1.96627

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97521
[1mStep[0m  [2/26], [94mLoss[0m : 1.97542
[1mStep[0m  [4/26], [94mLoss[0m : 2.01297
[1mStep[0m  [6/26], [94mLoss[0m : 1.98912
[1mStep[0m  [8/26], [94mLoss[0m : 2.00071
[1mStep[0m  [10/26], [94mLoss[0m : 2.03755
[1mStep[0m  [12/26], [94mLoss[0m : 2.02911
[1mStep[0m  [14/26], [94mLoss[0m : 2.03179
[1mStep[0m  [16/26], [94mLoss[0m : 2.03433
[1mStep[0m  [18/26], [94mLoss[0m : 2.05440
[1mStep[0m  [20/26], [94mLoss[0m : 2.14276
[1mStep[0m  [22/26], [94mLoss[0m : 1.88218
[1mStep[0m  [24/26], [94mLoss[0m : 2.02597

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.95313
[1mStep[0m  [2/26], [94mLoss[0m : 1.94256
[1mStep[0m  [4/26], [94mLoss[0m : 1.94074
[1mStep[0m  [6/26], [94mLoss[0m : 1.98458
[1mStep[0m  [8/26], [94mLoss[0m : 1.99848
[1mStep[0m  [10/26], [94mLoss[0m : 1.95793
[1mStep[0m  [12/26], [94mLoss[0m : 1.85171
[1mStep[0m  [14/26], [94mLoss[0m : 1.89039
[1mStep[0m  [16/26], [94mLoss[0m : 1.88094
[1mStep[0m  [18/26], [94mLoss[0m : 2.03879
[1mStep[0m  [20/26], [94mLoss[0m : 1.88114
[1mStep[0m  [22/26], [94mLoss[0m : 1.95073
[1mStep[0m  [24/26], [94mLoss[0m : 1.98162

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82688
[1mStep[0m  [2/26], [94mLoss[0m : 1.88174
[1mStep[0m  [4/26], [94mLoss[0m : 1.91215
[1mStep[0m  [6/26], [94mLoss[0m : 1.91211
[1mStep[0m  [8/26], [94mLoss[0m : 1.89645
[1mStep[0m  [10/26], [94mLoss[0m : 2.02460
[1mStep[0m  [12/26], [94mLoss[0m : 1.94589
[1mStep[0m  [14/26], [94mLoss[0m : 1.86051
[1mStep[0m  [16/26], [94mLoss[0m : 1.88695
[1mStep[0m  [18/26], [94mLoss[0m : 1.94900
[1mStep[0m  [20/26], [94mLoss[0m : 2.04981
[1mStep[0m  [22/26], [94mLoss[0m : 1.95019
[1mStep[0m  [24/26], [94mLoss[0m : 1.92606

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.437, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.86711
[1mStep[0m  [2/26], [94mLoss[0m : 1.89488
[1mStep[0m  [4/26], [94mLoss[0m : 1.94135
[1mStep[0m  [6/26], [94mLoss[0m : 1.91480
[1mStep[0m  [8/26], [94mLoss[0m : 1.97287
[1mStep[0m  [10/26], [94mLoss[0m : 1.73572
[1mStep[0m  [12/26], [94mLoss[0m : 1.89943
[1mStep[0m  [14/26], [94mLoss[0m : 1.90977
[1mStep[0m  [16/26], [94mLoss[0m : 1.95508
[1mStep[0m  [18/26], [94mLoss[0m : 1.98367
[1mStep[0m  [20/26], [94mLoss[0m : 1.94188
[1mStep[0m  [22/26], [94mLoss[0m : 1.97425
[1mStep[0m  [24/26], [94mLoss[0m : 1.88034

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.908, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.94071
[1mStep[0m  [2/26], [94mLoss[0m : 1.85317
[1mStep[0m  [4/26], [94mLoss[0m : 1.81834
[1mStep[0m  [6/26], [94mLoss[0m : 1.76666
[1mStep[0m  [8/26], [94mLoss[0m : 1.86511
[1mStep[0m  [10/26], [94mLoss[0m : 1.78122
[1mStep[0m  [12/26], [94mLoss[0m : 2.01518
[1mStep[0m  [14/26], [94mLoss[0m : 1.84495
[1mStep[0m  [16/26], [94mLoss[0m : 1.87202
[1mStep[0m  [18/26], [94mLoss[0m : 1.85625
[1mStep[0m  [20/26], [94mLoss[0m : 1.91460
[1mStep[0m  [22/26], [94mLoss[0m : 1.94542
[1mStep[0m  [24/26], [94mLoss[0m : 1.89933

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.68931
[1mStep[0m  [2/26], [94mLoss[0m : 1.83539
[1mStep[0m  [4/26], [94mLoss[0m : 1.85207
[1mStep[0m  [6/26], [94mLoss[0m : 1.74139
[1mStep[0m  [8/26], [94mLoss[0m : 1.78079
[1mStep[0m  [10/26], [94mLoss[0m : 1.89287
[1mStep[0m  [12/26], [94mLoss[0m : 1.82028
[1mStep[0m  [14/26], [94mLoss[0m : 1.86780
[1mStep[0m  [16/26], [94mLoss[0m : 1.75794
[1mStep[0m  [18/26], [94mLoss[0m : 1.77081
[1mStep[0m  [20/26], [94mLoss[0m : 1.94673
[1mStep[0m  [22/26], [94mLoss[0m : 1.89678
[1mStep[0m  [24/26], [94mLoss[0m : 1.84954

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73795
[1mStep[0m  [2/26], [94mLoss[0m : 1.87362
[1mStep[0m  [4/26], [94mLoss[0m : 1.72870
[1mStep[0m  [6/26], [94mLoss[0m : 1.80887
[1mStep[0m  [8/26], [94mLoss[0m : 1.80836
[1mStep[0m  [10/26], [94mLoss[0m : 1.76110
[1mStep[0m  [12/26], [94mLoss[0m : 1.86725
[1mStep[0m  [14/26], [94mLoss[0m : 1.85907
[1mStep[0m  [16/26], [94mLoss[0m : 1.85696
[1mStep[0m  [18/26], [94mLoss[0m : 1.86870
[1mStep[0m  [20/26], [94mLoss[0m : 1.80603
[1mStep[0m  [22/26], [94mLoss[0m : 1.90200
[1mStep[0m  [24/26], [94mLoss[0m : 1.77915

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93556
[1mStep[0m  [2/26], [94mLoss[0m : 1.77825
[1mStep[0m  [4/26], [94mLoss[0m : 1.73974
[1mStep[0m  [6/26], [94mLoss[0m : 1.64099
[1mStep[0m  [8/26], [94mLoss[0m : 1.73139
[1mStep[0m  [10/26], [94mLoss[0m : 1.77534
[1mStep[0m  [12/26], [94mLoss[0m : 1.80863
[1mStep[0m  [14/26], [94mLoss[0m : 1.87536
[1mStep[0m  [16/26], [94mLoss[0m : 1.76599
[1mStep[0m  [18/26], [94mLoss[0m : 1.77256
[1mStep[0m  [20/26], [94mLoss[0m : 1.93532
[1mStep[0m  [22/26], [94mLoss[0m : 1.78997
[1mStep[0m  [24/26], [94mLoss[0m : 1.82688

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.67440
[1mStep[0m  [2/26], [94mLoss[0m : 1.87649
[1mStep[0m  [4/26], [94mLoss[0m : 1.76482
[1mStep[0m  [6/26], [94mLoss[0m : 1.77860
[1mStep[0m  [8/26], [94mLoss[0m : 1.76357
[1mStep[0m  [10/26], [94mLoss[0m : 1.89707
[1mStep[0m  [12/26], [94mLoss[0m : 1.75022
[1mStep[0m  [14/26], [94mLoss[0m : 1.67378
[1mStep[0m  [16/26], [94mLoss[0m : 1.79020
[1mStep[0m  [18/26], [94mLoss[0m : 1.93758
[1mStep[0m  [20/26], [94mLoss[0m : 1.68592
[1mStep[0m  [22/26], [94mLoss[0m : 1.74160
[1mStep[0m  [24/26], [94mLoss[0m : 1.85531

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.68853
[1mStep[0m  [2/26], [94mLoss[0m : 1.69627
[1mStep[0m  [4/26], [94mLoss[0m : 1.64514
[1mStep[0m  [6/26], [94mLoss[0m : 1.76512
[1mStep[0m  [8/26], [94mLoss[0m : 1.69577
[1mStep[0m  [10/26], [94mLoss[0m : 1.85647
[1mStep[0m  [12/26], [94mLoss[0m : 1.69885
[1mStep[0m  [14/26], [94mLoss[0m : 1.72299
[1mStep[0m  [16/26], [94mLoss[0m : 1.68774
[1mStep[0m  [18/26], [94mLoss[0m : 1.82823
[1mStep[0m  [20/26], [94mLoss[0m : 1.85408
[1mStep[0m  [22/26], [94mLoss[0m : 1.70283
[1mStep[0m  [24/26], [94mLoss[0m : 1.73325

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.71581
[1mStep[0m  [2/26], [94mLoss[0m : 1.79353
[1mStep[0m  [4/26], [94mLoss[0m : 1.75050
[1mStep[0m  [6/26], [94mLoss[0m : 1.78830
[1mStep[0m  [8/26], [94mLoss[0m : 1.75542
[1mStep[0m  [10/26], [94mLoss[0m : 1.70672
[1mStep[0m  [12/26], [94mLoss[0m : 1.82578
[1mStep[0m  [14/26], [94mLoss[0m : 1.72841
[1mStep[0m  [16/26], [94mLoss[0m : 1.76883
[1mStep[0m  [18/26], [94mLoss[0m : 1.68137
[1mStep[0m  [20/26], [94mLoss[0m : 1.71259
[1mStep[0m  [22/26], [94mLoss[0m : 1.81285
[1mStep[0m  [24/26], [94mLoss[0m : 1.81843

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.65980
[1mStep[0m  [2/26], [94mLoss[0m : 1.76449
[1mStep[0m  [4/26], [94mLoss[0m : 1.73725
[1mStep[0m  [6/26], [94mLoss[0m : 1.70207
[1mStep[0m  [8/26], [94mLoss[0m : 1.80867
[1mStep[0m  [10/26], [94mLoss[0m : 1.74282
[1mStep[0m  [12/26], [94mLoss[0m : 1.67880
[1mStep[0m  [14/26], [94mLoss[0m : 1.73763
[1mStep[0m  [16/26], [94mLoss[0m : 1.74786
[1mStep[0m  [18/26], [94mLoss[0m : 1.54104
[1mStep[0m  [20/26], [94mLoss[0m : 1.71418
[1mStep[0m  [22/26], [94mLoss[0m : 1.79078
[1mStep[0m  [24/26], [94mLoss[0m : 1.60180

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.703, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.66782
[1mStep[0m  [2/26], [94mLoss[0m : 1.78454
[1mStep[0m  [4/26], [94mLoss[0m : 1.55849
[1mStep[0m  [6/26], [94mLoss[0m : 1.61992
[1mStep[0m  [8/26], [94mLoss[0m : 1.74419
[1mStep[0m  [10/26], [94mLoss[0m : 1.63611
[1mStep[0m  [12/26], [94mLoss[0m : 1.72285
[1mStep[0m  [14/26], [94mLoss[0m : 1.78119
[1mStep[0m  [16/26], [94mLoss[0m : 1.64253
[1mStep[0m  [18/26], [94mLoss[0m : 1.74051
[1mStep[0m  [20/26], [94mLoss[0m : 1.78673
[1mStep[0m  [22/26], [94mLoss[0m : 1.63817
[1mStep[0m  [24/26], [94mLoss[0m : 1.69118

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.456
====================================

Phase 2 - Evaluation MAE:  2.456200489631066
MAE score P1      2.452325
MAE score P2        2.4562
loss               1.70322
learning_rate      0.00505
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.43617
[1mStep[0m  [2/26], [94mLoss[0m : 10.53575
[1mStep[0m  [4/26], [94mLoss[0m : 9.72820
[1mStep[0m  [6/26], [94mLoss[0m : 9.39170
[1mStep[0m  [8/26], [94mLoss[0m : 8.41454
[1mStep[0m  [10/26], [94mLoss[0m : 7.66518
[1mStep[0m  [12/26], [94mLoss[0m : 6.60360
[1mStep[0m  [14/26], [94mLoss[0m : 5.66258
[1mStep[0m  [16/26], [94mLoss[0m : 4.62456
[1mStep[0m  [18/26], [94mLoss[0m : 3.98457
[1mStep[0m  [20/26], [94mLoss[0m : 3.32029
[1mStep[0m  [22/26], [94mLoss[0m : 3.08360
[1mStep[0m  [24/26], [94mLoss[0m : 3.00860

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.562, [92mTest[0m: 10.541, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.96345
[1mStep[0m  [2/26], [94mLoss[0m : 2.75448
[1mStep[0m  [4/26], [94mLoss[0m : 2.87133
[1mStep[0m  [6/26], [94mLoss[0m : 2.97031
[1mStep[0m  [8/26], [94mLoss[0m : 2.81719
[1mStep[0m  [10/26], [94mLoss[0m : 2.97925
[1mStep[0m  [12/26], [94mLoss[0m : 2.97563
[1mStep[0m  [14/26], [94mLoss[0m : 2.76471
[1mStep[0m  [16/26], [94mLoss[0m : 2.70412
[1mStep[0m  [18/26], [94mLoss[0m : 2.76190
[1mStep[0m  [20/26], [94mLoss[0m : 2.72844
[1mStep[0m  [22/26], [94mLoss[0m : 2.66162
[1mStep[0m  [24/26], [94mLoss[0m : 2.66465

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.825, [92mTest[0m: 2.777, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.80720
[1mStep[0m  [2/26], [94mLoss[0m : 2.62677
[1mStep[0m  [4/26], [94mLoss[0m : 2.67993
[1mStep[0m  [6/26], [94mLoss[0m : 2.46660
[1mStep[0m  [8/26], [94mLoss[0m : 2.66027
[1mStep[0m  [10/26], [94mLoss[0m : 2.56914
[1mStep[0m  [12/26], [94mLoss[0m : 2.49419
[1mStep[0m  [14/26], [94mLoss[0m : 2.64913
[1mStep[0m  [16/26], [94mLoss[0m : 2.80790
[1mStep[0m  [18/26], [94mLoss[0m : 2.60311
[1mStep[0m  [20/26], [94mLoss[0m : 2.51980
[1mStep[0m  [22/26], [94mLoss[0m : 2.53588
[1mStep[0m  [24/26], [94mLoss[0m : 2.58904

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60256
[1mStep[0m  [2/26], [94mLoss[0m : 2.62680
[1mStep[0m  [4/26], [94mLoss[0m : 2.59651
[1mStep[0m  [6/26], [94mLoss[0m : 2.62667
[1mStep[0m  [8/26], [94mLoss[0m : 2.44725
[1mStep[0m  [10/26], [94mLoss[0m : 2.54347
[1mStep[0m  [12/26], [94mLoss[0m : 2.65168
[1mStep[0m  [14/26], [94mLoss[0m : 2.49220
[1mStep[0m  [16/26], [94mLoss[0m : 2.57370
[1mStep[0m  [18/26], [94mLoss[0m : 2.58652
[1mStep[0m  [20/26], [94mLoss[0m : 2.53412
[1mStep[0m  [22/26], [94mLoss[0m : 2.50025
[1mStep[0m  [24/26], [94mLoss[0m : 2.55363

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54260
[1mStep[0m  [2/26], [94mLoss[0m : 2.58874
[1mStep[0m  [4/26], [94mLoss[0m : 2.77425
[1mStep[0m  [6/26], [94mLoss[0m : 2.47040
[1mStep[0m  [8/26], [94mLoss[0m : 2.52748
[1mStep[0m  [10/26], [94mLoss[0m : 2.54024
[1mStep[0m  [12/26], [94mLoss[0m : 2.41888
[1mStep[0m  [14/26], [94mLoss[0m : 2.53551
[1mStep[0m  [16/26], [94mLoss[0m : 2.51490
[1mStep[0m  [18/26], [94mLoss[0m : 2.57541
[1mStep[0m  [20/26], [94mLoss[0m : 2.68101
[1mStep[0m  [22/26], [94mLoss[0m : 2.61437
[1mStep[0m  [24/26], [94mLoss[0m : 2.66096

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57542
[1mStep[0m  [2/26], [94mLoss[0m : 2.59323
[1mStep[0m  [4/26], [94mLoss[0m : 2.50398
[1mStep[0m  [6/26], [94mLoss[0m : 2.63118
[1mStep[0m  [8/26], [94mLoss[0m : 2.49805
[1mStep[0m  [10/26], [94mLoss[0m : 2.60552
[1mStep[0m  [12/26], [94mLoss[0m : 2.38467
[1mStep[0m  [14/26], [94mLoss[0m : 2.32258
[1mStep[0m  [16/26], [94mLoss[0m : 2.59069
[1mStep[0m  [18/26], [94mLoss[0m : 2.69723
[1mStep[0m  [20/26], [94mLoss[0m : 2.51462
[1mStep[0m  [22/26], [94mLoss[0m : 2.58884
[1mStep[0m  [24/26], [94mLoss[0m : 2.61146

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46096
[1mStep[0m  [2/26], [94mLoss[0m : 2.64448
[1mStep[0m  [4/26], [94mLoss[0m : 2.62238
[1mStep[0m  [6/26], [94mLoss[0m : 2.57987
[1mStep[0m  [8/26], [94mLoss[0m : 2.60576
[1mStep[0m  [10/26], [94mLoss[0m : 2.64055
[1mStep[0m  [12/26], [94mLoss[0m : 2.49451
[1mStep[0m  [14/26], [94mLoss[0m : 2.52381
[1mStep[0m  [16/26], [94mLoss[0m : 2.38364
[1mStep[0m  [18/26], [94mLoss[0m : 2.57741
[1mStep[0m  [20/26], [94mLoss[0m : 2.28576
[1mStep[0m  [22/26], [94mLoss[0m : 2.53818
[1mStep[0m  [24/26], [94mLoss[0m : 2.57649

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53215
[1mStep[0m  [2/26], [94mLoss[0m : 2.66668
[1mStep[0m  [4/26], [94mLoss[0m : 2.58096
[1mStep[0m  [6/26], [94mLoss[0m : 2.55230
[1mStep[0m  [8/26], [94mLoss[0m : 2.72596
[1mStep[0m  [10/26], [94mLoss[0m : 2.53679
[1mStep[0m  [12/26], [94mLoss[0m : 2.43678
[1mStep[0m  [14/26], [94mLoss[0m : 2.33570
[1mStep[0m  [16/26], [94mLoss[0m : 2.38263
[1mStep[0m  [18/26], [94mLoss[0m : 2.55641
[1mStep[0m  [20/26], [94mLoss[0m : 2.46839
[1mStep[0m  [22/26], [94mLoss[0m : 2.38861
[1mStep[0m  [24/26], [94mLoss[0m : 2.55870

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53138
[1mStep[0m  [2/26], [94mLoss[0m : 2.49812
[1mStep[0m  [4/26], [94mLoss[0m : 2.54774
[1mStep[0m  [6/26], [94mLoss[0m : 2.42691
[1mStep[0m  [8/26], [94mLoss[0m : 2.60021
[1mStep[0m  [10/26], [94mLoss[0m : 2.49532
[1mStep[0m  [12/26], [94mLoss[0m : 2.67886
[1mStep[0m  [14/26], [94mLoss[0m : 2.52554
[1mStep[0m  [16/26], [94mLoss[0m : 2.54795
[1mStep[0m  [18/26], [94mLoss[0m : 2.44603
[1mStep[0m  [20/26], [94mLoss[0m : 2.65402
[1mStep[0m  [22/26], [94mLoss[0m : 2.46961
[1mStep[0m  [24/26], [94mLoss[0m : 2.70504

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60073
[1mStep[0m  [2/26], [94mLoss[0m : 2.68527
[1mStep[0m  [4/26], [94mLoss[0m : 2.51488
[1mStep[0m  [6/26], [94mLoss[0m : 2.51606
[1mStep[0m  [8/26], [94mLoss[0m : 2.55701
[1mStep[0m  [10/26], [94mLoss[0m : 2.53045
[1mStep[0m  [12/26], [94mLoss[0m : 2.65539
[1mStep[0m  [14/26], [94mLoss[0m : 2.62491
[1mStep[0m  [16/26], [94mLoss[0m : 2.55829
[1mStep[0m  [18/26], [94mLoss[0m : 2.58053
[1mStep[0m  [20/26], [94mLoss[0m : 2.51483
[1mStep[0m  [22/26], [94mLoss[0m : 2.62563
[1mStep[0m  [24/26], [94mLoss[0m : 2.55819

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48918
[1mStep[0m  [2/26], [94mLoss[0m : 2.72139
[1mStep[0m  [4/26], [94mLoss[0m : 2.78258
[1mStep[0m  [6/26], [94mLoss[0m : 2.49293
[1mStep[0m  [8/26], [94mLoss[0m : 2.51561
[1mStep[0m  [10/26], [94mLoss[0m : 2.56064
[1mStep[0m  [12/26], [94mLoss[0m : 2.45037
[1mStep[0m  [14/26], [94mLoss[0m : 2.57499
[1mStep[0m  [16/26], [94mLoss[0m : 2.64920
[1mStep[0m  [18/26], [94mLoss[0m : 2.43802
[1mStep[0m  [20/26], [94mLoss[0m : 2.46351
[1mStep[0m  [22/26], [94mLoss[0m : 2.37508
[1mStep[0m  [24/26], [94mLoss[0m : 2.50443

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35829
[1mStep[0m  [2/26], [94mLoss[0m : 2.51877
[1mStep[0m  [4/26], [94mLoss[0m : 2.61197
[1mStep[0m  [6/26], [94mLoss[0m : 2.50673
[1mStep[0m  [8/26], [94mLoss[0m : 2.43485
[1mStep[0m  [10/26], [94mLoss[0m : 2.55177
[1mStep[0m  [12/26], [94mLoss[0m : 2.43341
[1mStep[0m  [14/26], [94mLoss[0m : 2.58621
[1mStep[0m  [16/26], [94mLoss[0m : 2.43514
[1mStep[0m  [18/26], [94mLoss[0m : 2.42344
[1mStep[0m  [20/26], [94mLoss[0m : 2.51154
[1mStep[0m  [22/26], [94mLoss[0m : 2.57064
[1mStep[0m  [24/26], [94mLoss[0m : 2.67015

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57050
[1mStep[0m  [2/26], [94mLoss[0m : 2.51660
[1mStep[0m  [4/26], [94mLoss[0m : 2.45306
[1mStep[0m  [6/26], [94mLoss[0m : 2.35765
[1mStep[0m  [8/26], [94mLoss[0m : 2.60531
[1mStep[0m  [10/26], [94mLoss[0m : 2.53931
[1mStep[0m  [12/26], [94mLoss[0m : 2.57229
[1mStep[0m  [14/26], [94mLoss[0m : 2.57803
[1mStep[0m  [16/26], [94mLoss[0m : 2.58442
[1mStep[0m  [18/26], [94mLoss[0m : 2.41335
[1mStep[0m  [20/26], [94mLoss[0m : 2.48582
[1mStep[0m  [22/26], [94mLoss[0m : 2.63829
[1mStep[0m  [24/26], [94mLoss[0m : 2.59015

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47900
[1mStep[0m  [2/26], [94mLoss[0m : 2.60511
[1mStep[0m  [4/26], [94mLoss[0m : 2.49105
[1mStep[0m  [6/26], [94mLoss[0m : 2.55833
[1mStep[0m  [8/26], [94mLoss[0m : 2.51881
[1mStep[0m  [10/26], [94mLoss[0m : 2.57804
[1mStep[0m  [12/26], [94mLoss[0m : 2.48970
[1mStep[0m  [14/26], [94mLoss[0m : 2.43623
[1mStep[0m  [16/26], [94mLoss[0m : 2.51906
[1mStep[0m  [18/26], [94mLoss[0m : 2.61701
[1mStep[0m  [20/26], [94mLoss[0m : 2.50672
[1mStep[0m  [22/26], [94mLoss[0m : 2.66388
[1mStep[0m  [24/26], [94mLoss[0m : 2.40896

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63916
[1mStep[0m  [2/26], [94mLoss[0m : 2.56738
[1mStep[0m  [4/26], [94mLoss[0m : 2.45477
[1mStep[0m  [6/26], [94mLoss[0m : 2.38037
[1mStep[0m  [8/26], [94mLoss[0m : 2.68444
[1mStep[0m  [10/26], [94mLoss[0m : 2.43799
[1mStep[0m  [12/26], [94mLoss[0m : 2.45699
[1mStep[0m  [14/26], [94mLoss[0m : 2.46669
[1mStep[0m  [16/26], [94mLoss[0m : 2.62406
[1mStep[0m  [18/26], [94mLoss[0m : 2.46329
[1mStep[0m  [20/26], [94mLoss[0m : 2.58110
[1mStep[0m  [22/26], [94mLoss[0m : 2.49616
[1mStep[0m  [24/26], [94mLoss[0m : 2.55675

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69485
[1mStep[0m  [2/26], [94mLoss[0m : 2.58046
[1mStep[0m  [4/26], [94mLoss[0m : 2.54347
[1mStep[0m  [6/26], [94mLoss[0m : 2.45804
[1mStep[0m  [8/26], [94mLoss[0m : 2.50925
[1mStep[0m  [10/26], [94mLoss[0m : 2.64349
[1mStep[0m  [12/26], [94mLoss[0m : 2.49303
[1mStep[0m  [14/26], [94mLoss[0m : 2.53857
[1mStep[0m  [16/26], [94mLoss[0m : 2.56457
[1mStep[0m  [18/26], [94mLoss[0m : 2.61594
[1mStep[0m  [20/26], [94mLoss[0m : 2.63073
[1mStep[0m  [22/26], [94mLoss[0m : 2.44560
[1mStep[0m  [24/26], [94mLoss[0m : 2.50269

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63906
[1mStep[0m  [2/26], [94mLoss[0m : 2.68780
[1mStep[0m  [4/26], [94mLoss[0m : 2.71663
[1mStep[0m  [6/26], [94mLoss[0m : 2.66271
[1mStep[0m  [8/26], [94mLoss[0m : 2.60641
[1mStep[0m  [10/26], [94mLoss[0m : 2.57698
[1mStep[0m  [12/26], [94mLoss[0m : 2.58928
[1mStep[0m  [14/26], [94mLoss[0m : 2.59211
[1mStep[0m  [16/26], [94mLoss[0m : 2.65322
[1mStep[0m  [18/26], [94mLoss[0m : 2.62921
[1mStep[0m  [20/26], [94mLoss[0m : 2.50291
[1mStep[0m  [22/26], [94mLoss[0m : 2.53244
[1mStep[0m  [24/26], [94mLoss[0m : 2.45159

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54198
[1mStep[0m  [2/26], [94mLoss[0m : 2.53081
[1mStep[0m  [4/26], [94mLoss[0m : 2.52574
[1mStep[0m  [6/26], [94mLoss[0m : 2.70134
[1mStep[0m  [8/26], [94mLoss[0m : 2.48641
[1mStep[0m  [10/26], [94mLoss[0m : 2.45862
[1mStep[0m  [12/26], [94mLoss[0m : 2.43659
[1mStep[0m  [14/26], [94mLoss[0m : 2.39352
[1mStep[0m  [16/26], [94mLoss[0m : 2.62063
[1mStep[0m  [18/26], [94mLoss[0m : 2.50333
[1mStep[0m  [20/26], [94mLoss[0m : 2.50874
[1mStep[0m  [22/26], [94mLoss[0m : 2.71316
[1mStep[0m  [24/26], [94mLoss[0m : 2.50503

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62001
[1mStep[0m  [2/26], [94mLoss[0m : 2.54195
[1mStep[0m  [4/26], [94mLoss[0m : 2.51438
[1mStep[0m  [6/26], [94mLoss[0m : 2.53122
[1mStep[0m  [8/26], [94mLoss[0m : 2.45139
[1mStep[0m  [10/26], [94mLoss[0m : 2.50438
[1mStep[0m  [12/26], [94mLoss[0m : 2.61606
[1mStep[0m  [14/26], [94mLoss[0m : 2.51429
[1mStep[0m  [16/26], [94mLoss[0m : 2.60299
[1mStep[0m  [18/26], [94mLoss[0m : 2.48194
[1mStep[0m  [20/26], [94mLoss[0m : 2.54773
[1mStep[0m  [22/26], [94mLoss[0m : 2.49184
[1mStep[0m  [24/26], [94mLoss[0m : 2.44579

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59037
[1mStep[0m  [2/26], [94mLoss[0m : 2.40783
[1mStep[0m  [4/26], [94mLoss[0m : 2.50270
[1mStep[0m  [6/26], [94mLoss[0m : 2.48385
[1mStep[0m  [8/26], [94mLoss[0m : 2.57731
[1mStep[0m  [10/26], [94mLoss[0m : 2.55464
[1mStep[0m  [12/26], [94mLoss[0m : 2.48447
[1mStep[0m  [14/26], [94mLoss[0m : 2.48518
[1mStep[0m  [16/26], [94mLoss[0m : 2.44848
[1mStep[0m  [18/26], [94mLoss[0m : 2.52760
[1mStep[0m  [20/26], [94mLoss[0m : 2.52866
[1mStep[0m  [22/26], [94mLoss[0m : 2.39304
[1mStep[0m  [24/26], [94mLoss[0m : 2.56398

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53562
[1mStep[0m  [2/26], [94mLoss[0m : 2.57869
[1mStep[0m  [4/26], [94mLoss[0m : 2.53193
[1mStep[0m  [6/26], [94mLoss[0m : 2.62688
[1mStep[0m  [8/26], [94mLoss[0m : 2.43929
[1mStep[0m  [10/26], [94mLoss[0m : 2.47035
[1mStep[0m  [12/26], [94mLoss[0m : 2.64447
[1mStep[0m  [14/26], [94mLoss[0m : 2.52537
[1mStep[0m  [16/26], [94mLoss[0m : 2.40768
[1mStep[0m  [18/26], [94mLoss[0m : 2.61658
[1mStep[0m  [20/26], [94mLoss[0m : 2.33526
[1mStep[0m  [22/26], [94mLoss[0m : 2.62522
[1mStep[0m  [24/26], [94mLoss[0m : 2.49546

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64028
[1mStep[0m  [2/26], [94mLoss[0m : 2.45472
[1mStep[0m  [4/26], [94mLoss[0m : 2.59306
[1mStep[0m  [6/26], [94mLoss[0m : 2.54296
[1mStep[0m  [8/26], [94mLoss[0m : 2.46774
[1mStep[0m  [10/26], [94mLoss[0m : 2.41679
[1mStep[0m  [12/26], [94mLoss[0m : 2.39615
[1mStep[0m  [14/26], [94mLoss[0m : 2.56155
[1mStep[0m  [16/26], [94mLoss[0m : 2.33227
[1mStep[0m  [18/26], [94mLoss[0m : 2.42205
[1mStep[0m  [20/26], [94mLoss[0m : 2.68387
[1mStep[0m  [22/26], [94mLoss[0m : 2.46511
[1mStep[0m  [24/26], [94mLoss[0m : 2.40664

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56206
[1mStep[0m  [2/26], [94mLoss[0m : 2.31632
[1mStep[0m  [4/26], [94mLoss[0m : 2.45359
[1mStep[0m  [6/26], [94mLoss[0m : 2.49769
[1mStep[0m  [8/26], [94mLoss[0m : 2.53559
[1mStep[0m  [10/26], [94mLoss[0m : 2.56885
[1mStep[0m  [12/26], [94mLoss[0m : 2.62810
[1mStep[0m  [14/26], [94mLoss[0m : 2.46075
[1mStep[0m  [16/26], [94mLoss[0m : 2.58112
[1mStep[0m  [18/26], [94mLoss[0m : 2.38509
[1mStep[0m  [20/26], [94mLoss[0m : 2.46790
[1mStep[0m  [22/26], [94mLoss[0m : 2.59039
[1mStep[0m  [24/26], [94mLoss[0m : 2.49591

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38805
[1mStep[0m  [2/26], [94mLoss[0m : 2.44547
[1mStep[0m  [4/26], [94mLoss[0m : 2.38257
[1mStep[0m  [6/26], [94mLoss[0m : 2.50899
[1mStep[0m  [8/26], [94mLoss[0m : 2.48018
[1mStep[0m  [10/26], [94mLoss[0m : 2.51635
[1mStep[0m  [12/26], [94mLoss[0m : 2.62063
[1mStep[0m  [14/26], [94mLoss[0m : 2.66640
[1mStep[0m  [16/26], [94mLoss[0m : 2.56110
[1mStep[0m  [18/26], [94mLoss[0m : 2.44552
[1mStep[0m  [20/26], [94mLoss[0m : 2.37537
[1mStep[0m  [22/26], [94mLoss[0m : 2.58705
[1mStep[0m  [24/26], [94mLoss[0m : 2.49683

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70382
[1mStep[0m  [2/26], [94mLoss[0m : 2.56850
[1mStep[0m  [4/26], [94mLoss[0m : 2.59637
[1mStep[0m  [6/26], [94mLoss[0m : 2.68643
[1mStep[0m  [8/26], [94mLoss[0m : 2.47825
[1mStep[0m  [10/26], [94mLoss[0m : 2.53080
[1mStep[0m  [12/26], [94mLoss[0m : 2.43212
[1mStep[0m  [14/26], [94mLoss[0m : 2.47921
[1mStep[0m  [16/26], [94mLoss[0m : 2.37864
[1mStep[0m  [18/26], [94mLoss[0m : 2.43042
[1mStep[0m  [20/26], [94mLoss[0m : 2.39325
[1mStep[0m  [22/26], [94mLoss[0m : 2.66085
[1mStep[0m  [24/26], [94mLoss[0m : 2.41252

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57568
[1mStep[0m  [2/26], [94mLoss[0m : 2.48243
[1mStep[0m  [4/26], [94mLoss[0m : 2.48352
[1mStep[0m  [6/26], [94mLoss[0m : 2.50943
[1mStep[0m  [8/26], [94mLoss[0m : 2.48774
[1mStep[0m  [10/26], [94mLoss[0m : 2.57613
[1mStep[0m  [12/26], [94mLoss[0m : 2.53681
[1mStep[0m  [14/26], [94mLoss[0m : 2.51281
[1mStep[0m  [16/26], [94mLoss[0m : 2.60193
[1mStep[0m  [18/26], [94mLoss[0m : 2.44902
[1mStep[0m  [20/26], [94mLoss[0m : 2.50284
[1mStep[0m  [22/26], [94mLoss[0m : 2.41944
[1mStep[0m  [24/26], [94mLoss[0m : 2.57744

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50175
[1mStep[0m  [2/26], [94mLoss[0m : 2.44454
[1mStep[0m  [4/26], [94mLoss[0m : 2.47388
[1mStep[0m  [6/26], [94mLoss[0m : 2.45000
[1mStep[0m  [8/26], [94mLoss[0m : 2.55459
[1mStep[0m  [10/26], [94mLoss[0m : 2.39623
[1mStep[0m  [12/26], [94mLoss[0m : 2.58940
[1mStep[0m  [14/26], [94mLoss[0m : 2.44280
[1mStep[0m  [16/26], [94mLoss[0m : 2.52397
[1mStep[0m  [18/26], [94mLoss[0m : 2.44224
[1mStep[0m  [20/26], [94mLoss[0m : 2.56624
[1mStep[0m  [22/26], [94mLoss[0m : 2.49062
[1mStep[0m  [24/26], [94mLoss[0m : 2.66156

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60716
[1mStep[0m  [2/26], [94mLoss[0m : 2.47647
[1mStep[0m  [4/26], [94mLoss[0m : 2.44561
[1mStep[0m  [6/26], [94mLoss[0m : 2.39634
[1mStep[0m  [8/26], [94mLoss[0m : 2.51709
[1mStep[0m  [10/26], [94mLoss[0m : 2.58807
[1mStep[0m  [12/26], [94mLoss[0m : 2.53962
[1mStep[0m  [14/26], [94mLoss[0m : 2.56437
[1mStep[0m  [16/26], [94mLoss[0m : 2.40446
[1mStep[0m  [18/26], [94mLoss[0m : 2.63022
[1mStep[0m  [20/26], [94mLoss[0m : 2.55005
[1mStep[0m  [22/26], [94mLoss[0m : 2.36029
[1mStep[0m  [24/26], [94mLoss[0m : 2.53621

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65511
[1mStep[0m  [2/26], [94mLoss[0m : 2.31387
[1mStep[0m  [4/26], [94mLoss[0m : 2.53235
[1mStep[0m  [6/26], [94mLoss[0m : 2.43384
[1mStep[0m  [8/26], [94mLoss[0m : 2.52717
[1mStep[0m  [10/26], [94mLoss[0m : 2.53126
[1mStep[0m  [12/26], [94mLoss[0m : 2.59527
[1mStep[0m  [14/26], [94mLoss[0m : 2.70941
[1mStep[0m  [16/26], [94mLoss[0m : 2.47713
[1mStep[0m  [18/26], [94mLoss[0m : 2.45192
[1mStep[0m  [20/26], [94mLoss[0m : 2.42927
[1mStep[0m  [22/26], [94mLoss[0m : 2.61682
[1mStep[0m  [24/26], [94mLoss[0m : 2.55591

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50274
[1mStep[0m  [2/26], [94mLoss[0m : 2.44849
[1mStep[0m  [4/26], [94mLoss[0m : 2.50954
[1mStep[0m  [6/26], [94mLoss[0m : 2.39775
[1mStep[0m  [8/26], [94mLoss[0m : 2.47531
[1mStep[0m  [10/26], [94mLoss[0m : 2.51963
[1mStep[0m  [12/26], [94mLoss[0m : 2.54866
[1mStep[0m  [14/26], [94mLoss[0m : 2.37067
[1mStep[0m  [16/26], [94mLoss[0m : 2.45529
[1mStep[0m  [18/26], [94mLoss[0m : 2.65226
[1mStep[0m  [20/26], [94mLoss[0m : 2.56496
[1mStep[0m  [22/26], [94mLoss[0m : 2.53796
[1mStep[0m  [24/26], [94mLoss[0m : 2.42920

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.398
====================================

Phase 1 - Evaluation MAE:  2.397514031483577
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 2.56777
[1mStep[0m  [2/26], [94mLoss[0m : 2.38150
[1mStep[0m  [4/26], [94mLoss[0m : 2.61995
[1mStep[0m  [6/26], [94mLoss[0m : 2.66897
[1mStep[0m  [8/26], [94mLoss[0m : 2.44458
[1mStep[0m  [10/26], [94mLoss[0m : 2.66191
[1mStep[0m  [12/26], [94mLoss[0m : 2.50079
[1mStep[0m  [14/26], [94mLoss[0m : 2.63471
[1mStep[0m  [16/26], [94mLoss[0m : 2.47892
[1mStep[0m  [18/26], [94mLoss[0m : 2.43752
[1mStep[0m  [20/26], [94mLoss[0m : 2.57726
[1mStep[0m  [22/26], [94mLoss[0m : 2.70173
[1mStep[0m  [24/26], [94mLoss[0m : 2.51121

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47324
[1mStep[0m  [2/26], [94mLoss[0m : 2.53246
[1mStep[0m  [4/26], [94mLoss[0m : 2.63882
[1mStep[0m  [6/26], [94mLoss[0m : 2.43218
[1mStep[0m  [8/26], [94mLoss[0m : 2.59122
[1mStep[0m  [10/26], [94mLoss[0m : 2.41307
[1mStep[0m  [12/26], [94mLoss[0m : 2.36355
[1mStep[0m  [14/26], [94mLoss[0m : 2.56776
[1mStep[0m  [16/26], [94mLoss[0m : 2.57799
[1mStep[0m  [18/26], [94mLoss[0m : 2.60859
[1mStep[0m  [20/26], [94mLoss[0m : 2.50961
[1mStep[0m  [22/26], [94mLoss[0m : 2.36276
[1mStep[0m  [24/26], [94mLoss[0m : 2.56954

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41089
[1mStep[0m  [2/26], [94mLoss[0m : 2.33562
[1mStep[0m  [4/26], [94mLoss[0m : 2.26538
[1mStep[0m  [6/26], [94mLoss[0m : 2.55679
[1mStep[0m  [8/26], [94mLoss[0m : 2.43478
[1mStep[0m  [10/26], [94mLoss[0m : 2.38942
[1mStep[0m  [12/26], [94mLoss[0m : 2.38800
[1mStep[0m  [14/26], [94mLoss[0m : 2.52158
[1mStep[0m  [16/26], [94mLoss[0m : 2.36620
[1mStep[0m  [18/26], [94mLoss[0m : 2.40499
[1mStep[0m  [20/26], [94mLoss[0m : 2.50604
[1mStep[0m  [22/26], [94mLoss[0m : 2.35773
[1mStep[0m  [24/26], [94mLoss[0m : 2.61183

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.701, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32947
[1mStep[0m  [2/26], [94mLoss[0m : 2.46262
[1mStep[0m  [4/26], [94mLoss[0m : 2.42236
[1mStep[0m  [6/26], [94mLoss[0m : 2.41238
[1mStep[0m  [8/26], [94mLoss[0m : 2.38175
[1mStep[0m  [10/26], [94mLoss[0m : 2.42149
[1mStep[0m  [12/26], [94mLoss[0m : 2.33666
[1mStep[0m  [14/26], [94mLoss[0m : 2.33739
[1mStep[0m  [16/26], [94mLoss[0m : 2.37341
[1mStep[0m  [18/26], [94mLoss[0m : 2.33934
[1mStep[0m  [20/26], [94mLoss[0m : 2.28729
[1mStep[0m  [22/26], [94mLoss[0m : 2.21640
[1mStep[0m  [24/26], [94mLoss[0m : 2.46448

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.695, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17616
[1mStep[0m  [2/26], [94mLoss[0m : 2.30119
[1mStep[0m  [4/26], [94mLoss[0m : 2.48192
[1mStep[0m  [6/26], [94mLoss[0m : 2.36964
[1mStep[0m  [8/26], [94mLoss[0m : 2.32701
[1mStep[0m  [10/26], [94mLoss[0m : 2.29263
[1mStep[0m  [12/26], [94mLoss[0m : 2.28554
[1mStep[0m  [14/26], [94mLoss[0m : 2.22346
[1mStep[0m  [16/26], [94mLoss[0m : 2.24567
[1mStep[0m  [18/26], [94mLoss[0m : 2.36319
[1mStep[0m  [20/26], [94mLoss[0m : 2.36118
[1mStep[0m  [22/26], [94mLoss[0m : 2.48373
[1mStep[0m  [24/26], [94mLoss[0m : 2.28246

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.665, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30191
[1mStep[0m  [2/26], [94mLoss[0m : 2.23039
[1mStep[0m  [4/26], [94mLoss[0m : 2.27112
[1mStep[0m  [6/26], [94mLoss[0m : 2.37963
[1mStep[0m  [8/26], [94mLoss[0m : 2.28640
[1mStep[0m  [10/26], [94mLoss[0m : 2.15614
[1mStep[0m  [12/26], [94mLoss[0m : 2.30683
[1mStep[0m  [14/26], [94mLoss[0m : 2.18205
[1mStep[0m  [16/26], [94mLoss[0m : 2.41220
[1mStep[0m  [18/26], [94mLoss[0m : 2.15950
[1mStep[0m  [20/26], [94mLoss[0m : 2.29745
[1mStep[0m  [22/26], [94mLoss[0m : 2.34956
[1mStep[0m  [24/26], [94mLoss[0m : 2.17859

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.731, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26167
[1mStep[0m  [2/26], [94mLoss[0m : 2.38623
[1mStep[0m  [4/26], [94mLoss[0m : 2.16815
[1mStep[0m  [6/26], [94mLoss[0m : 2.21581
[1mStep[0m  [8/26], [94mLoss[0m : 2.32644
[1mStep[0m  [10/26], [94mLoss[0m : 2.24146
[1mStep[0m  [12/26], [94mLoss[0m : 2.18380
[1mStep[0m  [14/26], [94mLoss[0m : 2.21282
[1mStep[0m  [16/26], [94mLoss[0m : 2.38237
[1mStep[0m  [18/26], [94mLoss[0m : 2.44246
[1mStep[0m  [20/26], [94mLoss[0m : 2.17450
[1mStep[0m  [22/26], [94mLoss[0m : 2.28633
[1mStep[0m  [24/26], [94mLoss[0m : 2.28007

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.589, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19181
[1mStep[0m  [2/26], [94mLoss[0m : 2.10299
[1mStep[0m  [4/26], [94mLoss[0m : 2.20776
[1mStep[0m  [6/26], [94mLoss[0m : 2.22550
[1mStep[0m  [8/26], [94mLoss[0m : 2.22104
[1mStep[0m  [10/26], [94mLoss[0m : 2.17416
[1mStep[0m  [12/26], [94mLoss[0m : 2.28318
[1mStep[0m  [14/26], [94mLoss[0m : 2.23963
[1mStep[0m  [16/26], [94mLoss[0m : 2.22551
[1mStep[0m  [18/26], [94mLoss[0m : 2.15101
[1mStep[0m  [20/26], [94mLoss[0m : 2.03757
[1mStep[0m  [22/26], [94mLoss[0m : 2.33094
[1mStep[0m  [24/26], [94mLoss[0m : 2.16688

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08304
[1mStep[0m  [2/26], [94mLoss[0m : 2.19511
[1mStep[0m  [4/26], [94mLoss[0m : 2.11486
[1mStep[0m  [6/26], [94mLoss[0m : 2.24560
[1mStep[0m  [8/26], [94mLoss[0m : 2.07951
[1mStep[0m  [10/26], [94mLoss[0m : 1.98882
[1mStep[0m  [12/26], [94mLoss[0m : 2.13264
[1mStep[0m  [14/26], [94mLoss[0m : 2.14732
[1mStep[0m  [16/26], [94mLoss[0m : 2.06293
[1mStep[0m  [18/26], [94mLoss[0m : 2.17127
[1mStep[0m  [20/26], [94mLoss[0m : 2.15788
[1mStep[0m  [22/26], [94mLoss[0m : 2.12321
[1mStep[0m  [24/26], [94mLoss[0m : 2.13568

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.91949
[1mStep[0m  [2/26], [94mLoss[0m : 2.08468
[1mStep[0m  [4/26], [94mLoss[0m : 2.05405
[1mStep[0m  [6/26], [94mLoss[0m : 1.98835
[1mStep[0m  [8/26], [94mLoss[0m : 1.98668
[1mStep[0m  [10/26], [94mLoss[0m : 2.10115
[1mStep[0m  [12/26], [94mLoss[0m : 1.97732
[1mStep[0m  [14/26], [94mLoss[0m : 2.11300
[1mStep[0m  [16/26], [94mLoss[0m : 1.88509
[1mStep[0m  [18/26], [94mLoss[0m : 2.10737
[1mStep[0m  [20/26], [94mLoss[0m : 2.15484
[1mStep[0m  [22/26], [94mLoss[0m : 2.17605
[1mStep[0m  [24/26], [94mLoss[0m : 2.01989

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.062, [92mTest[0m: 2.525, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.13237
[1mStep[0m  [2/26], [94mLoss[0m : 2.03737
[1mStep[0m  [4/26], [94mLoss[0m : 2.07017
[1mStep[0m  [6/26], [94mLoss[0m : 1.96528
[1mStep[0m  [8/26], [94mLoss[0m : 2.15281
[1mStep[0m  [10/26], [94mLoss[0m : 2.17824
[1mStep[0m  [12/26], [94mLoss[0m : 2.06118
[1mStep[0m  [14/26], [94mLoss[0m : 1.93401
[1mStep[0m  [16/26], [94mLoss[0m : 2.10111
[1mStep[0m  [18/26], [94mLoss[0m : 2.13167
[1mStep[0m  [20/26], [94mLoss[0m : 2.04899
[1mStep[0m  [22/26], [94mLoss[0m : 2.13538
[1mStep[0m  [24/26], [94mLoss[0m : 1.94804

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.13164
[1mStep[0m  [2/26], [94mLoss[0m : 1.80599
[1mStep[0m  [4/26], [94mLoss[0m : 2.01069
[1mStep[0m  [6/26], [94mLoss[0m : 1.86249
[1mStep[0m  [8/26], [94mLoss[0m : 1.90573
[1mStep[0m  [10/26], [94mLoss[0m : 2.09143
[1mStep[0m  [12/26], [94mLoss[0m : 1.97556
[1mStep[0m  [14/26], [94mLoss[0m : 1.98767
[1mStep[0m  [16/26], [94mLoss[0m : 1.92900
[1mStep[0m  [18/26], [94mLoss[0m : 2.01259
[1mStep[0m  [20/26], [94mLoss[0m : 1.98744
[1mStep[0m  [22/26], [94mLoss[0m : 2.06763
[1mStep[0m  [24/26], [94mLoss[0m : 1.89693

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.99522
[1mStep[0m  [2/26], [94mLoss[0m : 1.87386
[1mStep[0m  [4/26], [94mLoss[0m : 1.84239
[1mStep[0m  [6/26], [94mLoss[0m : 1.96454
[1mStep[0m  [8/26], [94mLoss[0m : 1.95811
[1mStep[0m  [10/26], [94mLoss[0m : 1.74520
[1mStep[0m  [12/26], [94mLoss[0m : 1.96823
[1mStep[0m  [14/26], [94mLoss[0m : 2.01060
[1mStep[0m  [16/26], [94mLoss[0m : 1.99100
[1mStep[0m  [18/26], [94mLoss[0m : 1.82554
[1mStep[0m  [20/26], [94mLoss[0m : 1.95655
[1mStep[0m  [22/26], [94mLoss[0m : 1.88027
[1mStep[0m  [24/26], [94mLoss[0m : 2.01526

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.86705
[1mStep[0m  [2/26], [94mLoss[0m : 1.95841
[1mStep[0m  [4/26], [94mLoss[0m : 1.85083
[1mStep[0m  [6/26], [94mLoss[0m : 1.85597
[1mStep[0m  [8/26], [94mLoss[0m : 1.89729
[1mStep[0m  [10/26], [94mLoss[0m : 1.96658
[1mStep[0m  [12/26], [94mLoss[0m : 1.80005
[1mStep[0m  [14/26], [94mLoss[0m : 1.93327
[1mStep[0m  [16/26], [94mLoss[0m : 2.00834
[1mStep[0m  [18/26], [94mLoss[0m : 1.85490
[1mStep[0m  [20/26], [94mLoss[0m : 1.93887
[1mStep[0m  [22/26], [94mLoss[0m : 1.86363
[1mStep[0m  [24/26], [94mLoss[0m : 2.00082

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81819
[1mStep[0m  [2/26], [94mLoss[0m : 1.89464
[1mStep[0m  [4/26], [94mLoss[0m : 1.88560
[1mStep[0m  [6/26], [94mLoss[0m : 1.82735
[1mStep[0m  [8/26], [94mLoss[0m : 1.85062
[1mStep[0m  [10/26], [94mLoss[0m : 1.84959
[1mStep[0m  [12/26], [94mLoss[0m : 1.85200
[1mStep[0m  [14/26], [94mLoss[0m : 1.86420
[1mStep[0m  [16/26], [94mLoss[0m : 2.00013
[1mStep[0m  [18/26], [94mLoss[0m : 1.84981
[1mStep[0m  [20/26], [94mLoss[0m : 1.79489
[1mStep[0m  [22/26], [94mLoss[0m : 1.84577
[1mStep[0m  [24/26], [94mLoss[0m : 1.83275

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.78198
[1mStep[0m  [2/26], [94mLoss[0m : 1.78651
[1mStep[0m  [4/26], [94mLoss[0m : 1.85495
[1mStep[0m  [6/26], [94mLoss[0m : 1.98545
[1mStep[0m  [8/26], [94mLoss[0m : 1.76851
[1mStep[0m  [10/26], [94mLoss[0m : 1.95529
[1mStep[0m  [12/26], [94mLoss[0m : 1.85165
[1mStep[0m  [14/26], [94mLoss[0m : 1.86030
[1mStep[0m  [16/26], [94mLoss[0m : 1.98122
[1mStep[0m  [18/26], [94mLoss[0m : 1.87166
[1mStep[0m  [20/26], [94mLoss[0m : 1.91558
[1mStep[0m  [22/26], [94mLoss[0m : 1.91167
[1mStep[0m  [24/26], [94mLoss[0m : 1.96866

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.79215
[1mStep[0m  [2/26], [94mLoss[0m : 1.77529
[1mStep[0m  [4/26], [94mLoss[0m : 1.92560
[1mStep[0m  [6/26], [94mLoss[0m : 1.80417
[1mStep[0m  [8/26], [94mLoss[0m : 1.78388
[1mStep[0m  [10/26], [94mLoss[0m : 1.74073
[1mStep[0m  [12/26], [94mLoss[0m : 1.74391
[1mStep[0m  [14/26], [94mLoss[0m : 1.83243
[1mStep[0m  [16/26], [94mLoss[0m : 1.85616
[1mStep[0m  [18/26], [94mLoss[0m : 1.70105
[1mStep[0m  [20/26], [94mLoss[0m : 1.82225
[1mStep[0m  [22/26], [94mLoss[0m : 1.93191
[1mStep[0m  [24/26], [94mLoss[0m : 1.79927

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.70452
[1mStep[0m  [2/26], [94mLoss[0m : 1.79753
[1mStep[0m  [4/26], [94mLoss[0m : 1.83788
[1mStep[0m  [6/26], [94mLoss[0m : 1.71074
[1mStep[0m  [8/26], [94mLoss[0m : 1.87142
[1mStep[0m  [10/26], [94mLoss[0m : 1.75037
[1mStep[0m  [12/26], [94mLoss[0m : 1.73495
[1mStep[0m  [14/26], [94mLoss[0m : 1.80736
[1mStep[0m  [16/26], [94mLoss[0m : 1.81720
[1mStep[0m  [18/26], [94mLoss[0m : 1.78383
[1mStep[0m  [20/26], [94mLoss[0m : 1.78508
[1mStep[0m  [22/26], [94mLoss[0m : 1.66975
[1mStep[0m  [24/26], [94mLoss[0m : 1.79944

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.543, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.75905
[1mStep[0m  [2/26], [94mLoss[0m : 1.60526
[1mStep[0m  [4/26], [94mLoss[0m : 1.84383
[1mStep[0m  [6/26], [94mLoss[0m : 1.66382
[1mStep[0m  [8/26], [94mLoss[0m : 1.92008
[1mStep[0m  [10/26], [94mLoss[0m : 1.70357
[1mStep[0m  [12/26], [94mLoss[0m : 1.67583
[1mStep[0m  [14/26], [94mLoss[0m : 1.79147
[1mStep[0m  [16/26], [94mLoss[0m : 1.69591
[1mStep[0m  [18/26], [94mLoss[0m : 1.70552
[1mStep[0m  [20/26], [94mLoss[0m : 1.76796
[1mStep[0m  [22/26], [94mLoss[0m : 1.85478
[1mStep[0m  [24/26], [94mLoss[0m : 1.65674

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.605, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.65497
[1mStep[0m  [2/26], [94mLoss[0m : 1.66601
[1mStep[0m  [4/26], [94mLoss[0m : 1.69560
[1mStep[0m  [6/26], [94mLoss[0m : 1.69236
[1mStep[0m  [8/26], [94mLoss[0m : 1.71938
[1mStep[0m  [10/26], [94mLoss[0m : 1.75511
[1mStep[0m  [12/26], [94mLoss[0m : 1.82887
[1mStep[0m  [14/26], [94mLoss[0m : 1.73824
[1mStep[0m  [16/26], [94mLoss[0m : 1.69678
[1mStep[0m  [18/26], [94mLoss[0m : 1.80797
[1mStep[0m  [20/26], [94mLoss[0m : 1.67775
[1mStep[0m  [22/26], [94mLoss[0m : 1.81284
[1mStep[0m  [24/26], [94mLoss[0m : 1.78174

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.537, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73699
[1mStep[0m  [2/26], [94mLoss[0m : 1.72199
[1mStep[0m  [4/26], [94mLoss[0m : 1.61388
[1mStep[0m  [6/26], [94mLoss[0m : 1.60507
[1mStep[0m  [8/26], [94mLoss[0m : 1.64305
[1mStep[0m  [10/26], [94mLoss[0m : 1.71400
[1mStep[0m  [12/26], [94mLoss[0m : 1.71146
[1mStep[0m  [14/26], [94mLoss[0m : 1.64293
[1mStep[0m  [16/26], [94mLoss[0m : 1.73514
[1mStep[0m  [18/26], [94mLoss[0m : 1.72500
[1mStep[0m  [20/26], [94mLoss[0m : 1.68665
[1mStep[0m  [22/26], [94mLoss[0m : 1.68817
[1mStep[0m  [24/26], [94mLoss[0m : 1.56763

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.483, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.55600
[1mStep[0m  [2/26], [94mLoss[0m : 1.66509
[1mStep[0m  [4/26], [94mLoss[0m : 1.81183
[1mStep[0m  [6/26], [94mLoss[0m : 1.47144
[1mStep[0m  [8/26], [94mLoss[0m : 1.80246
[1mStep[0m  [10/26], [94mLoss[0m : 1.55429
[1mStep[0m  [12/26], [94mLoss[0m : 1.70229
[1mStep[0m  [14/26], [94mLoss[0m : 1.63094
[1mStep[0m  [16/26], [94mLoss[0m : 1.75061
[1mStep[0m  [18/26], [94mLoss[0m : 1.61012
[1mStep[0m  [20/26], [94mLoss[0m : 1.61368
[1mStep[0m  [22/26], [94mLoss[0m : 1.62062
[1mStep[0m  [24/26], [94mLoss[0m : 1.83359

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.561, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54697
[1mStep[0m  [2/26], [94mLoss[0m : 1.48538
[1mStep[0m  [4/26], [94mLoss[0m : 1.56947
[1mStep[0m  [6/26], [94mLoss[0m : 1.69879
[1mStep[0m  [8/26], [94mLoss[0m : 1.65868
[1mStep[0m  [10/26], [94mLoss[0m : 1.65011
[1mStep[0m  [12/26], [94mLoss[0m : 1.64190
[1mStep[0m  [14/26], [94mLoss[0m : 1.59310
[1mStep[0m  [16/26], [94mLoss[0m : 1.54029
[1mStep[0m  [18/26], [94mLoss[0m : 1.67505
[1mStep[0m  [20/26], [94mLoss[0m : 1.57150
[1mStep[0m  [22/26], [94mLoss[0m : 1.71070
[1mStep[0m  [24/26], [94mLoss[0m : 1.61402

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.634, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.63094
[1mStep[0m  [2/26], [94mLoss[0m : 1.55147
[1mStep[0m  [4/26], [94mLoss[0m : 1.66781
[1mStep[0m  [6/26], [94mLoss[0m : 1.53519
[1mStep[0m  [8/26], [94mLoss[0m : 1.53952
[1mStep[0m  [10/26], [94mLoss[0m : 1.58105
[1mStep[0m  [12/26], [94mLoss[0m : 1.62256
[1mStep[0m  [14/26], [94mLoss[0m : 1.64147
[1mStep[0m  [16/26], [94mLoss[0m : 1.70980
[1mStep[0m  [18/26], [94mLoss[0m : 1.53023
[1mStep[0m  [20/26], [94mLoss[0m : 1.73000
[1mStep[0m  [22/26], [94mLoss[0m : 1.60412
[1mStep[0m  [24/26], [94mLoss[0m : 1.65845

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.66329
[1mStep[0m  [2/26], [94mLoss[0m : 1.56970
[1mStep[0m  [4/26], [94mLoss[0m : 1.49543
[1mStep[0m  [6/26], [94mLoss[0m : 1.57338
[1mStep[0m  [8/26], [94mLoss[0m : 1.54478
[1mStep[0m  [10/26], [94mLoss[0m : 1.61260
[1mStep[0m  [12/26], [94mLoss[0m : 1.54572
[1mStep[0m  [14/26], [94mLoss[0m : 1.67225
[1mStep[0m  [16/26], [94mLoss[0m : 1.45823
[1mStep[0m  [18/26], [94mLoss[0m : 1.72373
[1mStep[0m  [20/26], [94mLoss[0m : 1.65232
[1mStep[0m  [22/26], [94mLoss[0m : 1.58045
[1mStep[0m  [24/26], [94mLoss[0m : 1.66432

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.511, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52388
[1mStep[0m  [2/26], [94mLoss[0m : 1.60364
[1mStep[0m  [4/26], [94mLoss[0m : 1.68187
[1mStep[0m  [6/26], [94mLoss[0m : 1.65152
[1mStep[0m  [8/26], [94mLoss[0m : 1.55295
[1mStep[0m  [10/26], [94mLoss[0m : 1.52353
[1mStep[0m  [12/26], [94mLoss[0m : 1.50178
[1mStep[0m  [14/26], [94mLoss[0m : 1.70313
[1mStep[0m  [16/26], [94mLoss[0m : 1.68965
[1mStep[0m  [18/26], [94mLoss[0m : 1.49255
[1mStep[0m  [20/26], [94mLoss[0m : 1.67810
[1mStep[0m  [22/26], [94mLoss[0m : 1.54204
[1mStep[0m  [24/26], [94mLoss[0m : 1.71411

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.573, [92mTest[0m: 2.662, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.55102
[1mStep[0m  [2/26], [94mLoss[0m : 1.49561
[1mStep[0m  [4/26], [94mLoss[0m : 1.46727
[1mStep[0m  [6/26], [94mLoss[0m : 1.52696
[1mStep[0m  [8/26], [94mLoss[0m : 1.57440
[1mStep[0m  [10/26], [94mLoss[0m : 1.48294
[1mStep[0m  [12/26], [94mLoss[0m : 1.49387
[1mStep[0m  [14/26], [94mLoss[0m : 1.45710
[1mStep[0m  [16/26], [94mLoss[0m : 1.51297
[1mStep[0m  [18/26], [94mLoss[0m : 1.52946
[1mStep[0m  [20/26], [94mLoss[0m : 1.65403
[1mStep[0m  [22/26], [94mLoss[0m : 1.56179
[1mStep[0m  [24/26], [94mLoss[0m : 1.56043

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49640
[1mStep[0m  [2/26], [94mLoss[0m : 1.46452
[1mStep[0m  [4/26], [94mLoss[0m : 1.40525
[1mStep[0m  [6/26], [94mLoss[0m : 1.53341
[1mStep[0m  [8/26], [94mLoss[0m : 1.53444
[1mStep[0m  [10/26], [94mLoss[0m : 1.62599
[1mStep[0m  [12/26], [94mLoss[0m : 1.47998
[1mStep[0m  [14/26], [94mLoss[0m : 1.49922
[1mStep[0m  [16/26], [94mLoss[0m : 1.53755
[1mStep[0m  [18/26], [94mLoss[0m : 1.63015
[1mStep[0m  [20/26], [94mLoss[0m : 1.51516
[1mStep[0m  [22/26], [94mLoss[0m : 1.52035
[1mStep[0m  [24/26], [94mLoss[0m : 1.49906

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.758, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52957
[1mStep[0m  [2/26], [94mLoss[0m : 1.40035
[1mStep[0m  [4/26], [94mLoss[0m : 1.53785
[1mStep[0m  [6/26], [94mLoss[0m : 1.49586
[1mStep[0m  [8/26], [94mLoss[0m : 1.38376
[1mStep[0m  [10/26], [94mLoss[0m : 1.48287
[1mStep[0m  [12/26], [94mLoss[0m : 1.45616
[1mStep[0m  [14/26], [94mLoss[0m : 1.44213
[1mStep[0m  [16/26], [94mLoss[0m : 1.52115
[1mStep[0m  [18/26], [94mLoss[0m : 1.46903
[1mStep[0m  [20/26], [94mLoss[0m : 1.46230
[1mStep[0m  [22/26], [94mLoss[0m : 1.51398
[1mStep[0m  [24/26], [94mLoss[0m : 1.52066

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49680
[1mStep[0m  [2/26], [94mLoss[0m : 1.47805
[1mStep[0m  [4/26], [94mLoss[0m : 1.49034
[1mStep[0m  [6/26], [94mLoss[0m : 1.46815
[1mStep[0m  [8/26], [94mLoss[0m : 1.50095
[1mStep[0m  [10/26], [94mLoss[0m : 1.50744
[1mStep[0m  [12/26], [94mLoss[0m : 1.55166
[1mStep[0m  [14/26], [94mLoss[0m : 1.46767
[1mStep[0m  [16/26], [94mLoss[0m : 1.47137
[1mStep[0m  [18/26], [94mLoss[0m : 1.50997
[1mStep[0m  [20/26], [94mLoss[0m : 1.46080
[1mStep[0m  [22/26], [94mLoss[0m : 1.56578
[1mStep[0m  [24/26], [94mLoss[0m : 1.46614

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.494, [92mTest[0m: 2.511, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.562
====================================

Phase 2 - Evaluation MAE:  2.561865678200355
MAE score P1       2.397514
MAE score P2       2.561866
loss               1.488038
learning_rate       0.00505
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay         0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.40755
[1mStep[0m  [2/26], [94mLoss[0m : 9.67805
[1mStep[0m  [4/26], [94mLoss[0m : 7.98224
[1mStep[0m  [6/26], [94mLoss[0m : 6.45667
[1mStep[0m  [8/26], [94mLoss[0m : 5.28216
[1mStep[0m  [10/26], [94mLoss[0m : 3.65731
[1mStep[0m  [12/26], [94mLoss[0m : 3.38977
[1mStep[0m  [14/26], [94mLoss[0m : 2.93717
[1mStep[0m  [16/26], [94mLoss[0m : 2.87005
[1mStep[0m  [18/26], [94mLoss[0m : 2.87220
[1mStep[0m  [20/26], [94mLoss[0m : 2.95947
[1mStep[0m  [22/26], [94mLoss[0m : 2.76944
[1mStep[0m  [24/26], [94mLoss[0m : 2.72754

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.793, [92mTest[0m: 10.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.01205
[1mStep[0m  [2/26], [94mLoss[0m : 2.67889
[1mStep[0m  [4/26], [94mLoss[0m : 2.48814
[1mStep[0m  [6/26], [94mLoss[0m : 2.74600
[1mStep[0m  [8/26], [94mLoss[0m : 2.63423
[1mStep[0m  [10/26], [94mLoss[0m : 2.70258
[1mStep[0m  [12/26], [94mLoss[0m : 2.65886
[1mStep[0m  [14/26], [94mLoss[0m : 2.74825
[1mStep[0m  [16/26], [94mLoss[0m : 2.64710
[1mStep[0m  [18/26], [94mLoss[0m : 2.61556
[1mStep[0m  [20/26], [94mLoss[0m : 2.52360
[1mStep[0m  [22/26], [94mLoss[0m : 2.48026
[1mStep[0m  [24/26], [94mLoss[0m : 2.49197

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.733, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67909
[1mStep[0m  [2/26], [94mLoss[0m : 2.56215
[1mStep[0m  [4/26], [94mLoss[0m : 2.59065
[1mStep[0m  [6/26], [94mLoss[0m : 2.63021
[1mStep[0m  [8/26], [94mLoss[0m : 2.75341
[1mStep[0m  [10/26], [94mLoss[0m : 2.63432
[1mStep[0m  [12/26], [94mLoss[0m : 2.44382
[1mStep[0m  [14/26], [94mLoss[0m : 2.58759
[1mStep[0m  [16/26], [94mLoss[0m : 2.61163
[1mStep[0m  [18/26], [94mLoss[0m : 2.59021
[1mStep[0m  [20/26], [94mLoss[0m : 2.38443
[1mStep[0m  [22/26], [94mLoss[0m : 2.45550
[1mStep[0m  [24/26], [94mLoss[0m : 2.59031

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.557, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43968
[1mStep[0m  [2/26], [94mLoss[0m : 2.35909
[1mStep[0m  [4/26], [94mLoss[0m : 2.51003
[1mStep[0m  [6/26], [94mLoss[0m : 2.49641
[1mStep[0m  [8/26], [94mLoss[0m : 2.63854
[1mStep[0m  [10/26], [94mLoss[0m : 2.41125
[1mStep[0m  [12/26], [94mLoss[0m : 2.56593
[1mStep[0m  [14/26], [94mLoss[0m : 2.48868
[1mStep[0m  [16/26], [94mLoss[0m : 2.49645
[1mStep[0m  [18/26], [94mLoss[0m : 2.40324
[1mStep[0m  [20/26], [94mLoss[0m : 2.66227
[1mStep[0m  [22/26], [94mLoss[0m : 2.42248
[1mStep[0m  [24/26], [94mLoss[0m : 2.50879

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38332
[1mStep[0m  [2/26], [94mLoss[0m : 2.47863
[1mStep[0m  [4/26], [94mLoss[0m : 2.46998
[1mStep[0m  [6/26], [94mLoss[0m : 2.42023
[1mStep[0m  [8/26], [94mLoss[0m : 2.41020
[1mStep[0m  [10/26], [94mLoss[0m : 2.44632
[1mStep[0m  [12/26], [94mLoss[0m : 2.48356
[1mStep[0m  [14/26], [94mLoss[0m : 2.46635
[1mStep[0m  [16/26], [94mLoss[0m : 2.51824
[1mStep[0m  [18/26], [94mLoss[0m : 2.53466
[1mStep[0m  [20/26], [94mLoss[0m : 2.60668
[1mStep[0m  [22/26], [94mLoss[0m : 2.64393
[1mStep[0m  [24/26], [94mLoss[0m : 2.62465

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40472
[1mStep[0m  [2/26], [94mLoss[0m : 2.50448
[1mStep[0m  [4/26], [94mLoss[0m : 2.44686
[1mStep[0m  [6/26], [94mLoss[0m : 2.45883
[1mStep[0m  [8/26], [94mLoss[0m : 2.30719
[1mStep[0m  [10/26], [94mLoss[0m : 2.57824
[1mStep[0m  [12/26], [94mLoss[0m : 2.58094
[1mStep[0m  [14/26], [94mLoss[0m : 2.48470
[1mStep[0m  [16/26], [94mLoss[0m : 2.36872
[1mStep[0m  [18/26], [94mLoss[0m : 2.41103
[1mStep[0m  [20/26], [94mLoss[0m : 2.58341
[1mStep[0m  [22/26], [94mLoss[0m : 2.44650
[1mStep[0m  [24/26], [94mLoss[0m : 2.72536

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37842
[1mStep[0m  [2/26], [94mLoss[0m : 2.44049
[1mStep[0m  [4/26], [94mLoss[0m : 2.45459
[1mStep[0m  [6/26], [94mLoss[0m : 2.36336
[1mStep[0m  [8/26], [94mLoss[0m : 2.39762
[1mStep[0m  [10/26], [94mLoss[0m : 2.61097
[1mStep[0m  [12/26], [94mLoss[0m : 2.46210
[1mStep[0m  [14/26], [94mLoss[0m : 2.41385
[1mStep[0m  [16/26], [94mLoss[0m : 2.49799
[1mStep[0m  [18/26], [94mLoss[0m : 2.47347
[1mStep[0m  [20/26], [94mLoss[0m : 2.56540
[1mStep[0m  [22/26], [94mLoss[0m : 2.45729
[1mStep[0m  [24/26], [94mLoss[0m : 2.54898

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41921
[1mStep[0m  [2/26], [94mLoss[0m : 2.36420
[1mStep[0m  [4/26], [94mLoss[0m : 2.50142
[1mStep[0m  [6/26], [94mLoss[0m : 2.48163
[1mStep[0m  [8/26], [94mLoss[0m : 2.55488
[1mStep[0m  [10/26], [94mLoss[0m : 2.34016
[1mStep[0m  [12/26], [94mLoss[0m : 2.52730
[1mStep[0m  [14/26], [94mLoss[0m : 2.39817
[1mStep[0m  [16/26], [94mLoss[0m : 2.48073
[1mStep[0m  [18/26], [94mLoss[0m : 2.37123
[1mStep[0m  [20/26], [94mLoss[0m : 2.58194
[1mStep[0m  [22/26], [94mLoss[0m : 2.43969
[1mStep[0m  [24/26], [94mLoss[0m : 2.54725

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47404
[1mStep[0m  [2/26], [94mLoss[0m : 2.40319
[1mStep[0m  [4/26], [94mLoss[0m : 2.39642
[1mStep[0m  [6/26], [94mLoss[0m : 2.43875
[1mStep[0m  [8/26], [94mLoss[0m : 2.56955
[1mStep[0m  [10/26], [94mLoss[0m : 2.58230
[1mStep[0m  [12/26], [94mLoss[0m : 2.32883
[1mStep[0m  [14/26], [94mLoss[0m : 2.47670
[1mStep[0m  [16/26], [94mLoss[0m : 2.41572
[1mStep[0m  [18/26], [94mLoss[0m : 2.58389
[1mStep[0m  [20/26], [94mLoss[0m : 2.38627
[1mStep[0m  [22/26], [94mLoss[0m : 2.45371
[1mStep[0m  [24/26], [94mLoss[0m : 2.43128

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48825
[1mStep[0m  [2/26], [94mLoss[0m : 2.46821
[1mStep[0m  [4/26], [94mLoss[0m : 2.33741
[1mStep[0m  [6/26], [94mLoss[0m : 2.44690
[1mStep[0m  [8/26], [94mLoss[0m : 2.52809
[1mStep[0m  [10/26], [94mLoss[0m : 2.46961
[1mStep[0m  [12/26], [94mLoss[0m : 2.30332
[1mStep[0m  [14/26], [94mLoss[0m : 2.54210
[1mStep[0m  [16/26], [94mLoss[0m : 2.44527
[1mStep[0m  [18/26], [94mLoss[0m : 2.38107
[1mStep[0m  [20/26], [94mLoss[0m : 2.48353
[1mStep[0m  [22/26], [94mLoss[0m : 2.50844
[1mStep[0m  [24/26], [94mLoss[0m : 2.33188

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48307
[1mStep[0m  [2/26], [94mLoss[0m : 2.29653
[1mStep[0m  [4/26], [94mLoss[0m : 2.50999
[1mStep[0m  [6/26], [94mLoss[0m : 2.38727
[1mStep[0m  [8/26], [94mLoss[0m : 2.49150
[1mStep[0m  [10/26], [94mLoss[0m : 2.56630
[1mStep[0m  [12/26], [94mLoss[0m : 2.48265
[1mStep[0m  [14/26], [94mLoss[0m : 2.47680
[1mStep[0m  [16/26], [94mLoss[0m : 2.36548
[1mStep[0m  [18/26], [94mLoss[0m : 2.50547
[1mStep[0m  [20/26], [94mLoss[0m : 2.58320
[1mStep[0m  [22/26], [94mLoss[0m : 2.55782
[1mStep[0m  [24/26], [94mLoss[0m : 2.32703

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53142
[1mStep[0m  [2/26], [94mLoss[0m : 2.42463
[1mStep[0m  [4/26], [94mLoss[0m : 2.45502
[1mStep[0m  [6/26], [94mLoss[0m : 2.41210
[1mStep[0m  [8/26], [94mLoss[0m : 2.55521
[1mStep[0m  [10/26], [94mLoss[0m : 2.38821
[1mStep[0m  [12/26], [94mLoss[0m : 2.36262
[1mStep[0m  [14/26], [94mLoss[0m : 2.53477
[1mStep[0m  [16/26], [94mLoss[0m : 2.32699
[1mStep[0m  [18/26], [94mLoss[0m : 2.46061
[1mStep[0m  [20/26], [94mLoss[0m : 2.53274
[1mStep[0m  [22/26], [94mLoss[0m : 2.42358
[1mStep[0m  [24/26], [94mLoss[0m : 2.43962

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38002
[1mStep[0m  [2/26], [94mLoss[0m : 2.51664
[1mStep[0m  [4/26], [94mLoss[0m : 2.51716
[1mStep[0m  [6/26], [94mLoss[0m : 2.32657
[1mStep[0m  [8/26], [94mLoss[0m : 2.42436
[1mStep[0m  [10/26], [94mLoss[0m : 2.44452
[1mStep[0m  [12/26], [94mLoss[0m : 2.45305
[1mStep[0m  [14/26], [94mLoss[0m : 2.25917
[1mStep[0m  [16/26], [94mLoss[0m : 2.43715
[1mStep[0m  [18/26], [94mLoss[0m : 2.48067
[1mStep[0m  [20/26], [94mLoss[0m : 2.47576
[1mStep[0m  [22/26], [94mLoss[0m : 2.61691
[1mStep[0m  [24/26], [94mLoss[0m : 2.44985

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37307
[1mStep[0m  [2/26], [94mLoss[0m : 2.46189
[1mStep[0m  [4/26], [94mLoss[0m : 2.43747
[1mStep[0m  [6/26], [94mLoss[0m : 2.46433
[1mStep[0m  [8/26], [94mLoss[0m : 2.55151
[1mStep[0m  [10/26], [94mLoss[0m : 2.58099
[1mStep[0m  [12/26], [94mLoss[0m : 2.36325
[1mStep[0m  [14/26], [94mLoss[0m : 2.28320
[1mStep[0m  [16/26], [94mLoss[0m : 2.39206
[1mStep[0m  [18/26], [94mLoss[0m : 2.54425
[1mStep[0m  [20/26], [94mLoss[0m : 2.54777
[1mStep[0m  [22/26], [94mLoss[0m : 2.53782
[1mStep[0m  [24/26], [94mLoss[0m : 2.35020

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32887
[1mStep[0m  [2/26], [94mLoss[0m : 2.36477
[1mStep[0m  [4/26], [94mLoss[0m : 2.46994
[1mStep[0m  [6/26], [94mLoss[0m : 2.52308
[1mStep[0m  [8/26], [94mLoss[0m : 2.57170
[1mStep[0m  [10/26], [94mLoss[0m : 2.56662
[1mStep[0m  [12/26], [94mLoss[0m : 2.36793
[1mStep[0m  [14/26], [94mLoss[0m : 2.54495
[1mStep[0m  [16/26], [94mLoss[0m : 2.31244
[1mStep[0m  [18/26], [94mLoss[0m : 2.34509
[1mStep[0m  [20/26], [94mLoss[0m : 2.43386
[1mStep[0m  [22/26], [94mLoss[0m : 2.56497
[1mStep[0m  [24/26], [94mLoss[0m : 2.39088

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38311
[1mStep[0m  [2/26], [94mLoss[0m : 2.28538
[1mStep[0m  [4/26], [94mLoss[0m : 2.53307
[1mStep[0m  [6/26], [94mLoss[0m : 2.50237
[1mStep[0m  [8/26], [94mLoss[0m : 2.51983
[1mStep[0m  [10/26], [94mLoss[0m : 2.37342
[1mStep[0m  [12/26], [94mLoss[0m : 2.44615
[1mStep[0m  [14/26], [94mLoss[0m : 2.39418
[1mStep[0m  [16/26], [94mLoss[0m : 2.47944
[1mStep[0m  [18/26], [94mLoss[0m : 2.54063
[1mStep[0m  [20/26], [94mLoss[0m : 2.47915
[1mStep[0m  [22/26], [94mLoss[0m : 2.56004
[1mStep[0m  [24/26], [94mLoss[0m : 2.41825

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35176
[1mStep[0m  [2/26], [94mLoss[0m : 2.37352
[1mStep[0m  [4/26], [94mLoss[0m : 2.52787
[1mStep[0m  [6/26], [94mLoss[0m : 2.53371
[1mStep[0m  [8/26], [94mLoss[0m : 2.42386
[1mStep[0m  [10/26], [94mLoss[0m : 2.30135
[1mStep[0m  [12/26], [94mLoss[0m : 2.60119
[1mStep[0m  [14/26], [94mLoss[0m : 2.36522
[1mStep[0m  [16/26], [94mLoss[0m : 2.37998
[1mStep[0m  [18/26], [94mLoss[0m : 2.42563
[1mStep[0m  [20/26], [94mLoss[0m : 2.50178
[1mStep[0m  [22/26], [94mLoss[0m : 2.57613
[1mStep[0m  [24/26], [94mLoss[0m : 2.61123

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40549
[1mStep[0m  [2/26], [94mLoss[0m : 2.55351
[1mStep[0m  [4/26], [94mLoss[0m : 2.42842
[1mStep[0m  [6/26], [94mLoss[0m : 2.29707
[1mStep[0m  [8/26], [94mLoss[0m : 2.42132
[1mStep[0m  [10/26], [94mLoss[0m : 2.38270
[1mStep[0m  [12/26], [94mLoss[0m : 2.35925
[1mStep[0m  [14/26], [94mLoss[0m : 2.47601
[1mStep[0m  [16/26], [94mLoss[0m : 2.48417
[1mStep[0m  [18/26], [94mLoss[0m : 2.28417
[1mStep[0m  [20/26], [94mLoss[0m : 2.37118
[1mStep[0m  [22/26], [94mLoss[0m : 2.44251
[1mStep[0m  [24/26], [94mLoss[0m : 2.36729

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37756
[1mStep[0m  [2/26], [94mLoss[0m : 2.37430
[1mStep[0m  [4/26], [94mLoss[0m : 2.30971
[1mStep[0m  [6/26], [94mLoss[0m : 2.49516
[1mStep[0m  [8/26], [94mLoss[0m : 2.40659
[1mStep[0m  [10/26], [94mLoss[0m : 2.60068
[1mStep[0m  [12/26], [94mLoss[0m : 2.39973
[1mStep[0m  [14/26], [94mLoss[0m : 2.36170
[1mStep[0m  [16/26], [94mLoss[0m : 2.47057
[1mStep[0m  [18/26], [94mLoss[0m : 2.33090
[1mStep[0m  [20/26], [94mLoss[0m : 2.50637
[1mStep[0m  [22/26], [94mLoss[0m : 2.51867
[1mStep[0m  [24/26], [94mLoss[0m : 2.50671

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38780
[1mStep[0m  [2/26], [94mLoss[0m : 2.43779
[1mStep[0m  [4/26], [94mLoss[0m : 2.30722
[1mStep[0m  [6/26], [94mLoss[0m : 2.41583
[1mStep[0m  [8/26], [94mLoss[0m : 2.30605
[1mStep[0m  [10/26], [94mLoss[0m : 2.38520
[1mStep[0m  [12/26], [94mLoss[0m : 2.45320
[1mStep[0m  [14/26], [94mLoss[0m : 2.36025
[1mStep[0m  [16/26], [94mLoss[0m : 2.50200
[1mStep[0m  [18/26], [94mLoss[0m : 2.41102
[1mStep[0m  [20/26], [94mLoss[0m : 2.53738
[1mStep[0m  [22/26], [94mLoss[0m : 2.52794
[1mStep[0m  [24/26], [94mLoss[0m : 2.45412

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50788
[1mStep[0m  [2/26], [94mLoss[0m : 2.44865
[1mStep[0m  [4/26], [94mLoss[0m : 2.49158
[1mStep[0m  [6/26], [94mLoss[0m : 2.33073
[1mStep[0m  [8/26], [94mLoss[0m : 2.61563
[1mStep[0m  [10/26], [94mLoss[0m : 2.46286
[1mStep[0m  [12/26], [94mLoss[0m : 2.46745
[1mStep[0m  [14/26], [94mLoss[0m : 2.35537
[1mStep[0m  [16/26], [94mLoss[0m : 2.43394
[1mStep[0m  [18/26], [94mLoss[0m : 2.38055
[1mStep[0m  [20/26], [94mLoss[0m : 2.53890
[1mStep[0m  [22/26], [94mLoss[0m : 2.45705
[1mStep[0m  [24/26], [94mLoss[0m : 2.50593

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.437, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40207
[1mStep[0m  [2/26], [94mLoss[0m : 2.49849
[1mStep[0m  [4/26], [94mLoss[0m : 2.38729
[1mStep[0m  [6/26], [94mLoss[0m : 2.52737
[1mStep[0m  [8/26], [94mLoss[0m : 2.35602
[1mStep[0m  [10/26], [94mLoss[0m : 2.34226
[1mStep[0m  [12/26], [94mLoss[0m : 2.45846
[1mStep[0m  [14/26], [94mLoss[0m : 2.43979
[1mStep[0m  [16/26], [94mLoss[0m : 2.26164
[1mStep[0m  [18/26], [94mLoss[0m : 2.55198
[1mStep[0m  [20/26], [94mLoss[0m : 2.41779
[1mStep[0m  [22/26], [94mLoss[0m : 2.42443
[1mStep[0m  [24/26], [94mLoss[0m : 2.54820

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.441, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43600
[1mStep[0m  [2/26], [94mLoss[0m : 2.31786
[1mStep[0m  [4/26], [94mLoss[0m : 2.44940
[1mStep[0m  [6/26], [94mLoss[0m : 2.42119
[1mStep[0m  [8/26], [94mLoss[0m : 2.37988
[1mStep[0m  [10/26], [94mLoss[0m : 2.59350
[1mStep[0m  [12/26], [94mLoss[0m : 2.38280
[1mStep[0m  [14/26], [94mLoss[0m : 2.37353
[1mStep[0m  [16/26], [94mLoss[0m : 2.31226
[1mStep[0m  [18/26], [94mLoss[0m : 2.51675
[1mStep[0m  [20/26], [94mLoss[0m : 2.29911
[1mStep[0m  [22/26], [94mLoss[0m : 2.57453
[1mStep[0m  [24/26], [94mLoss[0m : 2.35146

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37913
[1mStep[0m  [2/26], [94mLoss[0m : 2.33724
[1mStep[0m  [4/26], [94mLoss[0m : 2.29075
[1mStep[0m  [6/26], [94mLoss[0m : 2.38534
[1mStep[0m  [8/26], [94mLoss[0m : 2.48996
[1mStep[0m  [10/26], [94mLoss[0m : 2.38636
[1mStep[0m  [12/26], [94mLoss[0m : 2.46485
[1mStep[0m  [14/26], [94mLoss[0m : 2.30727
[1mStep[0m  [16/26], [94mLoss[0m : 2.46663
[1mStep[0m  [18/26], [94mLoss[0m : 2.47132
[1mStep[0m  [20/26], [94mLoss[0m : 2.46730
[1mStep[0m  [22/26], [94mLoss[0m : 2.39068
[1mStep[0m  [24/26], [94mLoss[0m : 2.34611

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60952
[1mStep[0m  [2/26], [94mLoss[0m : 2.29867
[1mStep[0m  [4/26], [94mLoss[0m : 2.50601
[1mStep[0m  [6/26], [94mLoss[0m : 2.44997
[1mStep[0m  [8/26], [94mLoss[0m : 2.42938
[1mStep[0m  [10/26], [94mLoss[0m : 2.40111
[1mStep[0m  [12/26], [94mLoss[0m : 2.39694
[1mStep[0m  [14/26], [94mLoss[0m : 2.39998
[1mStep[0m  [16/26], [94mLoss[0m : 2.55367
[1mStep[0m  [18/26], [94mLoss[0m : 2.49617
[1mStep[0m  [20/26], [94mLoss[0m : 2.39174
[1mStep[0m  [22/26], [94mLoss[0m : 2.24548
[1mStep[0m  [24/26], [94mLoss[0m : 2.49742

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48766
[1mStep[0m  [2/26], [94mLoss[0m : 2.46453
[1mStep[0m  [4/26], [94mLoss[0m : 2.37055
[1mStep[0m  [6/26], [94mLoss[0m : 2.57367
[1mStep[0m  [8/26], [94mLoss[0m : 2.44836
[1mStep[0m  [10/26], [94mLoss[0m : 2.50589
[1mStep[0m  [12/26], [94mLoss[0m : 2.47129
[1mStep[0m  [14/26], [94mLoss[0m : 2.23888
[1mStep[0m  [16/26], [94mLoss[0m : 2.52208
[1mStep[0m  [18/26], [94mLoss[0m : 2.48061
[1mStep[0m  [20/26], [94mLoss[0m : 2.44924
[1mStep[0m  [22/26], [94mLoss[0m : 2.37695
[1mStep[0m  [24/26], [94mLoss[0m : 2.52012

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37600
[1mStep[0m  [2/26], [94mLoss[0m : 2.35084
[1mStep[0m  [4/26], [94mLoss[0m : 2.44837
[1mStep[0m  [6/26], [94mLoss[0m : 2.40913
[1mStep[0m  [8/26], [94mLoss[0m : 2.29995
[1mStep[0m  [10/26], [94mLoss[0m : 2.40526
[1mStep[0m  [12/26], [94mLoss[0m : 2.53749
[1mStep[0m  [14/26], [94mLoss[0m : 2.35944
[1mStep[0m  [16/26], [94mLoss[0m : 2.32867
[1mStep[0m  [18/26], [94mLoss[0m : 2.50160
[1mStep[0m  [20/26], [94mLoss[0m : 2.39722
[1mStep[0m  [22/26], [94mLoss[0m : 2.49729
[1mStep[0m  [24/26], [94mLoss[0m : 2.42382

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46076
[1mStep[0m  [2/26], [94mLoss[0m : 2.43564
[1mStep[0m  [4/26], [94mLoss[0m : 2.40227
[1mStep[0m  [6/26], [94mLoss[0m : 2.48451
[1mStep[0m  [8/26], [94mLoss[0m : 2.42738
[1mStep[0m  [10/26], [94mLoss[0m : 2.34612
[1mStep[0m  [12/26], [94mLoss[0m : 2.42088
[1mStep[0m  [14/26], [94mLoss[0m : 2.38494
[1mStep[0m  [16/26], [94mLoss[0m : 2.44865
[1mStep[0m  [18/26], [94mLoss[0m : 2.36132
[1mStep[0m  [20/26], [94mLoss[0m : 2.33085
[1mStep[0m  [22/26], [94mLoss[0m : 2.41383
[1mStep[0m  [24/26], [94mLoss[0m : 2.29733

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40325
[1mStep[0m  [2/26], [94mLoss[0m : 2.61070
[1mStep[0m  [4/26], [94mLoss[0m : 2.44116
[1mStep[0m  [6/26], [94mLoss[0m : 2.44149
[1mStep[0m  [8/26], [94mLoss[0m : 2.56668
[1mStep[0m  [10/26], [94mLoss[0m : 2.32094
[1mStep[0m  [12/26], [94mLoss[0m : 2.47326
[1mStep[0m  [14/26], [94mLoss[0m : 2.36028
[1mStep[0m  [16/26], [94mLoss[0m : 2.38963
[1mStep[0m  [18/26], [94mLoss[0m : 2.49373
[1mStep[0m  [20/26], [94mLoss[0m : 2.36608
[1mStep[0m  [22/26], [94mLoss[0m : 2.42168
[1mStep[0m  [24/26], [94mLoss[0m : 2.54329

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41225
[1mStep[0m  [2/26], [94mLoss[0m : 2.51976
[1mStep[0m  [4/26], [94mLoss[0m : 2.41687
[1mStep[0m  [6/26], [94mLoss[0m : 2.36903
[1mStep[0m  [8/26], [94mLoss[0m : 2.31105
[1mStep[0m  [10/26], [94mLoss[0m : 2.40794
[1mStep[0m  [12/26], [94mLoss[0m : 2.31411
[1mStep[0m  [14/26], [94mLoss[0m : 2.44851
[1mStep[0m  [16/26], [94mLoss[0m : 2.44416
[1mStep[0m  [18/26], [94mLoss[0m : 2.44803
[1mStep[0m  [20/26], [94mLoss[0m : 2.32649
[1mStep[0m  [22/26], [94mLoss[0m : 2.47005
[1mStep[0m  [24/26], [94mLoss[0m : 2.31075

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.419
====================================

Phase 1 - Evaluation MAE:  2.418799822147076
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.36815
[1mStep[0m  [2/26], [94mLoss[0m : 2.45761
[1mStep[0m  [4/26], [94mLoss[0m : 2.43958
[1mStep[0m  [6/26], [94mLoss[0m : 2.44832
[1mStep[0m  [8/26], [94mLoss[0m : 2.42029
[1mStep[0m  [10/26], [94mLoss[0m : 2.50006
[1mStep[0m  [12/26], [94mLoss[0m : 2.48880
[1mStep[0m  [14/26], [94mLoss[0m : 2.38946
[1mStep[0m  [16/26], [94mLoss[0m : 2.45015
[1mStep[0m  [18/26], [94mLoss[0m : 2.49248
[1mStep[0m  [20/26], [94mLoss[0m : 2.53973
[1mStep[0m  [22/26], [94mLoss[0m : 2.23111
[1mStep[0m  [24/26], [94mLoss[0m : 2.49323

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50513
[1mStep[0m  [2/26], [94mLoss[0m : 2.44746
[1mStep[0m  [4/26], [94mLoss[0m : 2.38577
[1mStep[0m  [6/26], [94mLoss[0m : 2.34415
[1mStep[0m  [8/26], [94mLoss[0m : 2.56072
[1mStep[0m  [10/26], [94mLoss[0m : 2.46207
[1mStep[0m  [12/26], [94mLoss[0m : 2.49764
[1mStep[0m  [14/26], [94mLoss[0m : 2.45953
[1mStep[0m  [16/26], [94mLoss[0m : 2.35264
[1mStep[0m  [18/26], [94mLoss[0m : 2.34672
[1mStep[0m  [20/26], [94mLoss[0m : 2.38534
[1mStep[0m  [22/26], [94mLoss[0m : 2.23929
[1mStep[0m  [24/26], [94mLoss[0m : 2.37444

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45448
[1mStep[0m  [2/26], [94mLoss[0m : 2.30061
[1mStep[0m  [4/26], [94mLoss[0m : 2.39516
[1mStep[0m  [6/26], [94mLoss[0m : 2.36731
[1mStep[0m  [8/26], [94mLoss[0m : 2.60801
[1mStep[0m  [10/26], [94mLoss[0m : 2.18835
[1mStep[0m  [12/26], [94mLoss[0m : 2.52106
[1mStep[0m  [14/26], [94mLoss[0m : 2.41196
[1mStep[0m  [16/26], [94mLoss[0m : 2.40546
[1mStep[0m  [18/26], [94mLoss[0m : 2.36818
[1mStep[0m  [20/26], [94mLoss[0m : 2.40101
[1mStep[0m  [22/26], [94mLoss[0m : 2.43367
[1mStep[0m  [24/26], [94mLoss[0m : 2.45815

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28913
[1mStep[0m  [2/26], [94mLoss[0m : 2.43018
[1mStep[0m  [4/26], [94mLoss[0m : 2.40541
[1mStep[0m  [6/26], [94mLoss[0m : 2.37155
[1mStep[0m  [8/26], [94mLoss[0m : 2.42097
[1mStep[0m  [10/26], [94mLoss[0m : 2.40766
[1mStep[0m  [12/26], [94mLoss[0m : 2.41583
[1mStep[0m  [14/26], [94mLoss[0m : 2.31107
[1mStep[0m  [16/26], [94mLoss[0m : 2.52216
[1mStep[0m  [18/26], [94mLoss[0m : 2.40406
[1mStep[0m  [20/26], [94mLoss[0m : 2.38657
[1mStep[0m  [22/26], [94mLoss[0m : 2.29973
[1mStep[0m  [24/26], [94mLoss[0m : 2.39208

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34732
[1mStep[0m  [2/26], [94mLoss[0m : 2.42163
[1mStep[0m  [4/26], [94mLoss[0m : 2.57955
[1mStep[0m  [6/26], [94mLoss[0m : 2.29755
[1mStep[0m  [8/26], [94mLoss[0m : 2.36277
[1mStep[0m  [10/26], [94mLoss[0m : 2.33485
[1mStep[0m  [12/26], [94mLoss[0m : 2.35100
[1mStep[0m  [14/26], [94mLoss[0m : 2.40602
[1mStep[0m  [16/26], [94mLoss[0m : 2.43851
[1mStep[0m  [18/26], [94mLoss[0m : 2.44364
[1mStep[0m  [20/26], [94mLoss[0m : 2.44990
[1mStep[0m  [22/26], [94mLoss[0m : 2.39290
[1mStep[0m  [24/26], [94mLoss[0m : 2.32124

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22821
[1mStep[0m  [2/26], [94mLoss[0m : 2.41023
[1mStep[0m  [4/26], [94mLoss[0m : 2.32540
[1mStep[0m  [6/26], [94mLoss[0m : 2.40604
[1mStep[0m  [8/26], [94mLoss[0m : 2.26666
[1mStep[0m  [10/26], [94mLoss[0m : 2.41994
[1mStep[0m  [12/26], [94mLoss[0m : 2.43856
[1mStep[0m  [14/26], [94mLoss[0m : 2.33198
[1mStep[0m  [16/26], [94mLoss[0m : 2.40109
[1mStep[0m  [18/26], [94mLoss[0m : 2.45927
[1mStep[0m  [20/26], [94mLoss[0m : 2.47166
[1mStep[0m  [22/26], [94mLoss[0m : 2.38700
[1mStep[0m  [24/26], [94mLoss[0m : 2.43011

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35613
[1mStep[0m  [2/26], [94mLoss[0m : 2.30538
[1mStep[0m  [4/26], [94mLoss[0m : 2.26157
[1mStep[0m  [6/26], [94mLoss[0m : 2.21815
[1mStep[0m  [8/26], [94mLoss[0m : 2.43592
[1mStep[0m  [10/26], [94mLoss[0m : 2.24287
[1mStep[0m  [12/26], [94mLoss[0m : 2.51000
[1mStep[0m  [14/26], [94mLoss[0m : 2.30611
[1mStep[0m  [16/26], [94mLoss[0m : 2.36932
[1mStep[0m  [18/26], [94mLoss[0m : 2.40220
[1mStep[0m  [20/26], [94mLoss[0m : 2.36700
[1mStep[0m  [22/26], [94mLoss[0m : 2.42499
[1mStep[0m  [24/26], [94mLoss[0m : 2.41453

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30406
[1mStep[0m  [2/26], [94mLoss[0m : 2.29434
[1mStep[0m  [4/26], [94mLoss[0m : 2.20562
[1mStep[0m  [6/26], [94mLoss[0m : 2.44045
[1mStep[0m  [8/26], [94mLoss[0m : 2.29371
[1mStep[0m  [10/26], [94mLoss[0m : 2.29386
[1mStep[0m  [12/26], [94mLoss[0m : 2.16670
[1mStep[0m  [14/26], [94mLoss[0m : 2.38739
[1mStep[0m  [16/26], [94mLoss[0m : 2.36540
[1mStep[0m  [18/26], [94mLoss[0m : 2.36814
[1mStep[0m  [20/26], [94mLoss[0m : 2.26398
[1mStep[0m  [22/26], [94mLoss[0m : 2.33293
[1mStep[0m  [24/26], [94mLoss[0m : 2.43156

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.509, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27062
[1mStep[0m  [2/26], [94mLoss[0m : 2.15347
[1mStep[0m  [4/26], [94mLoss[0m : 2.31784
[1mStep[0m  [6/26], [94mLoss[0m : 2.42488
[1mStep[0m  [8/26], [94mLoss[0m : 2.38408
[1mStep[0m  [10/26], [94mLoss[0m : 2.31267
[1mStep[0m  [12/26], [94mLoss[0m : 2.22820
[1mStep[0m  [14/26], [94mLoss[0m : 2.23435
[1mStep[0m  [16/26], [94mLoss[0m : 2.14038
[1mStep[0m  [18/26], [94mLoss[0m : 2.21530
[1mStep[0m  [20/26], [94mLoss[0m : 2.33921
[1mStep[0m  [22/26], [94mLoss[0m : 2.43062
[1mStep[0m  [24/26], [94mLoss[0m : 2.29232

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.501, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29849
[1mStep[0m  [2/26], [94mLoss[0m : 2.22402
[1mStep[0m  [4/26], [94mLoss[0m : 2.34481
[1mStep[0m  [6/26], [94mLoss[0m : 2.22517
[1mStep[0m  [8/26], [94mLoss[0m : 2.31397
[1mStep[0m  [10/26], [94mLoss[0m : 2.29642
[1mStep[0m  [12/26], [94mLoss[0m : 2.36718
[1mStep[0m  [14/26], [94mLoss[0m : 2.37777
[1mStep[0m  [16/26], [94mLoss[0m : 2.27367
[1mStep[0m  [18/26], [94mLoss[0m : 2.36946
[1mStep[0m  [20/26], [94mLoss[0m : 2.41854
[1mStep[0m  [22/26], [94mLoss[0m : 2.26078
[1mStep[0m  [24/26], [94mLoss[0m : 2.32182

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38053
[1mStep[0m  [2/26], [94mLoss[0m : 2.18234
[1mStep[0m  [4/26], [94mLoss[0m : 2.35015
[1mStep[0m  [6/26], [94mLoss[0m : 2.21040
[1mStep[0m  [8/26], [94mLoss[0m : 2.31276
[1mStep[0m  [10/26], [94mLoss[0m : 2.31123
[1mStep[0m  [12/26], [94mLoss[0m : 2.37041
[1mStep[0m  [14/26], [94mLoss[0m : 2.19907
[1mStep[0m  [16/26], [94mLoss[0m : 2.33544
[1mStep[0m  [18/26], [94mLoss[0m : 2.38546
[1mStep[0m  [20/26], [94mLoss[0m : 2.25850
[1mStep[0m  [22/26], [94mLoss[0m : 2.19657
[1mStep[0m  [24/26], [94mLoss[0m : 2.25082

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20569
[1mStep[0m  [2/26], [94mLoss[0m : 2.15423
[1mStep[0m  [4/26], [94mLoss[0m : 2.39329
[1mStep[0m  [6/26], [94mLoss[0m : 2.25902
[1mStep[0m  [8/26], [94mLoss[0m : 2.35522
[1mStep[0m  [10/26], [94mLoss[0m : 2.41968
[1mStep[0m  [12/26], [94mLoss[0m : 2.36218
[1mStep[0m  [14/26], [94mLoss[0m : 2.25506
[1mStep[0m  [16/26], [94mLoss[0m : 2.16340
[1mStep[0m  [18/26], [94mLoss[0m : 2.16621
[1mStep[0m  [20/26], [94mLoss[0m : 2.40973
[1mStep[0m  [22/26], [94mLoss[0m : 2.11780
[1mStep[0m  [24/26], [94mLoss[0m : 2.35242

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23049
[1mStep[0m  [2/26], [94mLoss[0m : 2.24084
[1mStep[0m  [4/26], [94mLoss[0m : 2.30840
[1mStep[0m  [6/26], [94mLoss[0m : 2.27145
[1mStep[0m  [8/26], [94mLoss[0m : 2.35135
[1mStep[0m  [10/26], [94mLoss[0m : 2.11107
[1mStep[0m  [12/26], [94mLoss[0m : 2.22505
[1mStep[0m  [14/26], [94mLoss[0m : 2.26984
[1mStep[0m  [16/26], [94mLoss[0m : 2.10236
[1mStep[0m  [18/26], [94mLoss[0m : 2.20846
[1mStep[0m  [20/26], [94mLoss[0m : 2.27669
[1mStep[0m  [22/26], [94mLoss[0m : 2.30521
[1mStep[0m  [24/26], [94mLoss[0m : 2.31260

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20281
[1mStep[0m  [2/26], [94mLoss[0m : 2.25589
[1mStep[0m  [4/26], [94mLoss[0m : 2.28989
[1mStep[0m  [6/26], [94mLoss[0m : 2.27986
[1mStep[0m  [8/26], [94mLoss[0m : 2.35448
[1mStep[0m  [10/26], [94mLoss[0m : 2.34728
[1mStep[0m  [12/26], [94mLoss[0m : 2.20778
[1mStep[0m  [14/26], [94mLoss[0m : 2.19314
[1mStep[0m  [16/26], [94mLoss[0m : 2.19304
[1mStep[0m  [18/26], [94mLoss[0m : 2.18522
[1mStep[0m  [20/26], [94mLoss[0m : 2.34800
[1mStep[0m  [22/26], [94mLoss[0m : 2.31641
[1mStep[0m  [24/26], [94mLoss[0m : 2.23500

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.233, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21502
[1mStep[0m  [2/26], [94mLoss[0m : 2.33076
[1mStep[0m  [4/26], [94mLoss[0m : 2.19707
[1mStep[0m  [6/26], [94mLoss[0m : 2.18166
[1mStep[0m  [8/26], [94mLoss[0m : 2.15122
[1mStep[0m  [10/26], [94mLoss[0m : 2.22652
[1mStep[0m  [12/26], [94mLoss[0m : 2.16339
[1mStep[0m  [14/26], [94mLoss[0m : 2.26091
[1mStep[0m  [16/26], [94mLoss[0m : 2.09111
[1mStep[0m  [18/26], [94mLoss[0m : 2.41121
[1mStep[0m  [20/26], [94mLoss[0m : 2.11763
[1mStep[0m  [22/26], [94mLoss[0m : 2.17878
[1mStep[0m  [24/26], [94mLoss[0m : 2.26471

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.503, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24884
[1mStep[0m  [2/26], [94mLoss[0m : 2.23534
[1mStep[0m  [4/26], [94mLoss[0m : 2.10491
[1mStep[0m  [6/26], [94mLoss[0m : 2.17602
[1mStep[0m  [8/26], [94mLoss[0m : 2.12883
[1mStep[0m  [10/26], [94mLoss[0m : 2.13704
[1mStep[0m  [12/26], [94mLoss[0m : 2.21130
[1mStep[0m  [14/26], [94mLoss[0m : 2.14470
[1mStep[0m  [16/26], [94mLoss[0m : 2.16979
[1mStep[0m  [18/26], [94mLoss[0m : 2.20375
[1mStep[0m  [20/26], [94mLoss[0m : 2.30218
[1mStep[0m  [22/26], [94mLoss[0m : 2.25145
[1mStep[0m  [24/26], [94mLoss[0m : 2.08350

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.95081
[1mStep[0m  [2/26], [94mLoss[0m : 2.01439
[1mStep[0m  [4/26], [94mLoss[0m : 2.23945
[1mStep[0m  [6/26], [94mLoss[0m : 2.21724
[1mStep[0m  [8/26], [94mLoss[0m : 2.07143
[1mStep[0m  [10/26], [94mLoss[0m : 2.16806
[1mStep[0m  [12/26], [94mLoss[0m : 2.16775
[1mStep[0m  [14/26], [94mLoss[0m : 2.06168
[1mStep[0m  [16/26], [94mLoss[0m : 2.16197
[1mStep[0m  [18/26], [94mLoss[0m : 2.14125
[1mStep[0m  [20/26], [94mLoss[0m : 2.13478
[1mStep[0m  [22/26], [94mLoss[0m : 2.15160
[1mStep[0m  [24/26], [94mLoss[0m : 2.17206

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07444
[1mStep[0m  [2/26], [94mLoss[0m : 2.18655
[1mStep[0m  [4/26], [94mLoss[0m : 2.16836
[1mStep[0m  [6/26], [94mLoss[0m : 2.20937
[1mStep[0m  [8/26], [94mLoss[0m : 2.20531
[1mStep[0m  [10/26], [94mLoss[0m : 2.08171
[1mStep[0m  [12/26], [94mLoss[0m : 2.17535
[1mStep[0m  [14/26], [94mLoss[0m : 2.14508
[1mStep[0m  [16/26], [94mLoss[0m : 2.16222
[1mStep[0m  [18/26], [94mLoss[0m : 2.20544
[1mStep[0m  [20/26], [94mLoss[0m : 2.08653
[1mStep[0m  [22/26], [94mLoss[0m : 2.10288
[1mStep[0m  [24/26], [94mLoss[0m : 2.10476

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.16685
[1mStep[0m  [2/26], [94mLoss[0m : 2.14527
[1mStep[0m  [4/26], [94mLoss[0m : 2.14963
[1mStep[0m  [6/26], [94mLoss[0m : 2.05458
[1mStep[0m  [8/26], [94mLoss[0m : 2.20764
[1mStep[0m  [10/26], [94mLoss[0m : 2.16647
[1mStep[0m  [12/26], [94mLoss[0m : 2.03373
[1mStep[0m  [14/26], [94mLoss[0m : 2.19198
[1mStep[0m  [16/26], [94mLoss[0m : 2.08933
[1mStep[0m  [18/26], [94mLoss[0m : 2.17555
[1mStep[0m  [20/26], [94mLoss[0m : 2.04381
[1mStep[0m  [22/26], [94mLoss[0m : 2.03948
[1mStep[0m  [24/26], [94mLoss[0m : 2.15104

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.05620
[1mStep[0m  [2/26], [94mLoss[0m : 2.14693
[1mStep[0m  [4/26], [94mLoss[0m : 2.07928
[1mStep[0m  [6/26], [94mLoss[0m : 2.19101
[1mStep[0m  [8/26], [94mLoss[0m : 2.13320
[1mStep[0m  [10/26], [94mLoss[0m : 2.12115
[1mStep[0m  [12/26], [94mLoss[0m : 2.08806
[1mStep[0m  [14/26], [94mLoss[0m : 2.13311
[1mStep[0m  [16/26], [94mLoss[0m : 2.12086
[1mStep[0m  [18/26], [94mLoss[0m : 2.15797
[1mStep[0m  [20/26], [94mLoss[0m : 2.02678
[1mStep[0m  [22/26], [94mLoss[0m : 2.04921
[1mStep[0m  [24/26], [94mLoss[0m : 2.09636

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.11671
[1mStep[0m  [2/26], [94mLoss[0m : 1.90683
[1mStep[0m  [4/26], [94mLoss[0m : 2.01877
[1mStep[0m  [6/26], [94mLoss[0m : 2.08339
[1mStep[0m  [8/26], [94mLoss[0m : 2.20857
[1mStep[0m  [10/26], [94mLoss[0m : 1.87956
[1mStep[0m  [12/26], [94mLoss[0m : 2.10524
[1mStep[0m  [14/26], [94mLoss[0m : 2.02756
[1mStep[0m  [16/26], [94mLoss[0m : 2.11163
[1mStep[0m  [18/26], [94mLoss[0m : 2.05373
[1mStep[0m  [20/26], [94mLoss[0m : 2.11788
[1mStep[0m  [22/26], [94mLoss[0m : 2.15110
[1mStep[0m  [24/26], [94mLoss[0m : 2.01160

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.568, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97642
[1mStep[0m  [2/26], [94mLoss[0m : 2.14194
[1mStep[0m  [4/26], [94mLoss[0m : 1.95794
[1mStep[0m  [6/26], [94mLoss[0m : 2.17406
[1mStep[0m  [8/26], [94mLoss[0m : 2.11284
[1mStep[0m  [10/26], [94mLoss[0m : 2.01152
[1mStep[0m  [12/26], [94mLoss[0m : 2.06668
[1mStep[0m  [14/26], [94mLoss[0m : 2.07263
[1mStep[0m  [16/26], [94mLoss[0m : 2.05419
[1mStep[0m  [18/26], [94mLoss[0m : 1.87569
[1mStep[0m  [20/26], [94mLoss[0m : 2.08364
[1mStep[0m  [22/26], [94mLoss[0m : 1.98830
[1mStep[0m  [24/26], [94mLoss[0m : 2.09509

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.06701
[1mStep[0m  [2/26], [94mLoss[0m : 1.99569
[1mStep[0m  [4/26], [94mLoss[0m : 2.02289
[1mStep[0m  [6/26], [94mLoss[0m : 2.02836
[1mStep[0m  [8/26], [94mLoss[0m : 1.93698
[1mStep[0m  [10/26], [94mLoss[0m : 1.96578
[1mStep[0m  [12/26], [94mLoss[0m : 2.00315
[1mStep[0m  [14/26], [94mLoss[0m : 1.87145
[1mStep[0m  [16/26], [94mLoss[0m : 2.04568
[1mStep[0m  [18/26], [94mLoss[0m : 2.08389
[1mStep[0m  [20/26], [94mLoss[0m : 2.04998
[1mStep[0m  [22/26], [94mLoss[0m : 2.12308
[1mStep[0m  [24/26], [94mLoss[0m : 1.95863

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98816
[1mStep[0m  [2/26], [94mLoss[0m : 2.05432
[1mStep[0m  [4/26], [94mLoss[0m : 1.94890
[1mStep[0m  [6/26], [94mLoss[0m : 2.07413
[1mStep[0m  [8/26], [94mLoss[0m : 1.94679
[1mStep[0m  [10/26], [94mLoss[0m : 2.14094
[1mStep[0m  [12/26], [94mLoss[0m : 2.09100
[1mStep[0m  [14/26], [94mLoss[0m : 1.88462
[1mStep[0m  [16/26], [94mLoss[0m : 2.13316
[1mStep[0m  [18/26], [94mLoss[0m : 1.95666
[1mStep[0m  [20/26], [94mLoss[0m : 2.08875
[1mStep[0m  [22/26], [94mLoss[0m : 1.98247
[1mStep[0m  [24/26], [94mLoss[0m : 2.13843

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.528, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.75364
[1mStep[0m  [2/26], [94mLoss[0m : 1.90664
[1mStep[0m  [4/26], [94mLoss[0m : 1.89751
[1mStep[0m  [6/26], [94mLoss[0m : 1.94728
[1mStep[0m  [8/26], [94mLoss[0m : 1.97083
[1mStep[0m  [10/26], [94mLoss[0m : 1.94366
[1mStep[0m  [12/26], [94mLoss[0m : 2.00009
[1mStep[0m  [14/26], [94mLoss[0m : 1.87837
[1mStep[0m  [16/26], [94mLoss[0m : 2.11296
[1mStep[0m  [18/26], [94mLoss[0m : 2.11133
[1mStep[0m  [20/26], [94mLoss[0m : 2.00572
[1mStep[0m  [22/26], [94mLoss[0m : 1.88320
[1mStep[0m  [24/26], [94mLoss[0m : 2.08091

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.454, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98233
[1mStep[0m  [2/26], [94mLoss[0m : 1.98090
[1mStep[0m  [4/26], [94mLoss[0m : 1.95119
[1mStep[0m  [6/26], [94mLoss[0m : 1.89085
[1mStep[0m  [8/26], [94mLoss[0m : 2.00085
[1mStep[0m  [10/26], [94mLoss[0m : 1.87859
[1mStep[0m  [12/26], [94mLoss[0m : 2.05279
[1mStep[0m  [14/26], [94mLoss[0m : 1.86755
[1mStep[0m  [16/26], [94mLoss[0m : 1.83784
[1mStep[0m  [18/26], [94mLoss[0m : 1.96941
[1mStep[0m  [20/26], [94mLoss[0m : 2.02624
[1mStep[0m  [22/26], [94mLoss[0m : 1.86746
[1mStep[0m  [24/26], [94mLoss[0m : 2.01225

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.961, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.06977
[1mStep[0m  [2/26], [94mLoss[0m : 2.00182
[1mStep[0m  [4/26], [94mLoss[0m : 1.95493
[1mStep[0m  [6/26], [94mLoss[0m : 1.94466
[1mStep[0m  [8/26], [94mLoss[0m : 1.96816
[1mStep[0m  [10/26], [94mLoss[0m : 1.82417
[1mStep[0m  [12/26], [94mLoss[0m : 1.81821
[1mStep[0m  [14/26], [94mLoss[0m : 1.92683
[1mStep[0m  [16/26], [94mLoss[0m : 1.88348
[1mStep[0m  [18/26], [94mLoss[0m : 1.84892
[1mStep[0m  [20/26], [94mLoss[0m : 1.94168
[1mStep[0m  [22/26], [94mLoss[0m : 2.07849
[1mStep[0m  [24/26], [94mLoss[0m : 1.77710

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.90065
[1mStep[0m  [2/26], [94mLoss[0m : 1.86259
[1mStep[0m  [4/26], [94mLoss[0m : 1.88401
[1mStep[0m  [6/26], [94mLoss[0m : 1.88076
[1mStep[0m  [8/26], [94mLoss[0m : 1.98112
[1mStep[0m  [10/26], [94mLoss[0m : 1.93641
[1mStep[0m  [12/26], [94mLoss[0m : 1.89815
[1mStep[0m  [14/26], [94mLoss[0m : 1.79676
[1mStep[0m  [16/26], [94mLoss[0m : 1.77715
[1mStep[0m  [18/26], [94mLoss[0m : 1.91991
[1mStep[0m  [20/26], [94mLoss[0m : 1.98510
[1mStep[0m  [22/26], [94mLoss[0m : 1.94267
[1mStep[0m  [24/26], [94mLoss[0m : 1.93639

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.89310
[1mStep[0m  [2/26], [94mLoss[0m : 1.96002
[1mStep[0m  [4/26], [94mLoss[0m : 1.86512
[1mStep[0m  [6/26], [94mLoss[0m : 1.86305
[1mStep[0m  [8/26], [94mLoss[0m : 1.96388
[1mStep[0m  [10/26], [94mLoss[0m : 1.83335
[1mStep[0m  [12/26], [94mLoss[0m : 1.95209
[1mStep[0m  [14/26], [94mLoss[0m : 1.89317
[1mStep[0m  [16/26], [94mLoss[0m : 1.78145
[1mStep[0m  [18/26], [94mLoss[0m : 1.80189
[1mStep[0m  [20/26], [94mLoss[0m : 1.90149
[1mStep[0m  [22/26], [94mLoss[0m : 1.77932
[1mStep[0m  [24/26], [94mLoss[0m : 1.85686

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84615
[1mStep[0m  [2/26], [94mLoss[0m : 1.92810
[1mStep[0m  [4/26], [94mLoss[0m : 1.96594
[1mStep[0m  [6/26], [94mLoss[0m : 1.91064
[1mStep[0m  [8/26], [94mLoss[0m : 1.91176
[1mStep[0m  [10/26], [94mLoss[0m : 2.00967
[1mStep[0m  [12/26], [94mLoss[0m : 1.88440
[1mStep[0m  [14/26], [94mLoss[0m : 1.69964
[1mStep[0m  [16/26], [94mLoss[0m : 1.87223
[1mStep[0m  [18/26], [94mLoss[0m : 1.96013
[1mStep[0m  [20/26], [94mLoss[0m : 1.83213
[1mStep[0m  [22/26], [94mLoss[0m : 1.77576
[1mStep[0m  [24/26], [94mLoss[0m : 1.79304

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.510
====================================

Phase 2 - Evaluation MAE:  2.5097984167245717
MAE score P1        2.4188
MAE score P2      2.509798
loss              1.864565
learning_rate      0.00505
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/427], [94mLoss[0m : 10.68782
[1mStep[0m  [42/427], [94mLoss[0m : 2.22962
[1mStep[0m  [84/427], [94mLoss[0m : 2.20167
[1mStep[0m  [126/427], [94mLoss[0m : 2.61280
[1mStep[0m  [168/427], [94mLoss[0m : 2.61868
[1mStep[0m  [210/427], [94mLoss[0m : 2.23916
[1mStep[0m  [252/427], [94mLoss[0m : 2.45772
[1mStep[0m  [294/427], [94mLoss[0m : 2.86459
[1mStep[0m  [336/427], [94mLoss[0m : 2.34412
[1mStep[0m  [378/427], [94mLoss[0m : 2.26075
[1mStep[0m  [420/427], [94mLoss[0m : 2.90744

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.667, [92mTest[0m: 10.883, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32866
[1mStep[0m  [42/427], [94mLoss[0m : 2.40253
[1mStep[0m  [84/427], [94mLoss[0m : 2.55381
[1mStep[0m  [126/427], [94mLoss[0m : 2.54251
[1mStep[0m  [168/427], [94mLoss[0m : 2.54025
[1mStep[0m  [210/427], [94mLoss[0m : 2.29044
[1mStep[0m  [252/427], [94mLoss[0m : 2.40635
[1mStep[0m  [294/427], [94mLoss[0m : 2.64818
[1mStep[0m  [336/427], [94mLoss[0m : 2.06484
[1mStep[0m  [378/427], [94mLoss[0m : 1.87373
[1mStep[0m  [420/427], [94mLoss[0m : 2.70715

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.23043
[1mStep[0m  [42/427], [94mLoss[0m : 2.24636
[1mStep[0m  [84/427], [94mLoss[0m : 2.84782
[1mStep[0m  [126/427], [94mLoss[0m : 1.75189
[1mStep[0m  [168/427], [94mLoss[0m : 1.93653
[1mStep[0m  [210/427], [94mLoss[0m : 2.21818
[1mStep[0m  [252/427], [94mLoss[0m : 2.51417
[1mStep[0m  [294/427], [94mLoss[0m : 2.64819
[1mStep[0m  [336/427], [94mLoss[0m : 2.47684
[1mStep[0m  [378/427], [94mLoss[0m : 2.83885
[1mStep[0m  [420/427], [94mLoss[0m : 2.82186

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.15202
[1mStep[0m  [42/427], [94mLoss[0m : 2.65521
[1mStep[0m  [84/427], [94mLoss[0m : 2.17370
[1mStep[0m  [126/427], [94mLoss[0m : 2.54789
[1mStep[0m  [168/427], [94mLoss[0m : 2.37538
[1mStep[0m  [210/427], [94mLoss[0m : 2.79766
[1mStep[0m  [252/427], [94mLoss[0m : 2.28900
[1mStep[0m  [294/427], [94mLoss[0m : 2.09743
[1mStep[0m  [336/427], [94mLoss[0m : 2.28832
[1mStep[0m  [378/427], [94mLoss[0m : 2.61868
[1mStep[0m  [420/427], [94mLoss[0m : 2.48652

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.72355
[1mStep[0m  [42/427], [94mLoss[0m : 3.14945
[1mStep[0m  [84/427], [94mLoss[0m : 1.85403
[1mStep[0m  [126/427], [94mLoss[0m : 2.30647
[1mStep[0m  [168/427], [94mLoss[0m : 2.20767
[1mStep[0m  [210/427], [94mLoss[0m : 2.42997
[1mStep[0m  [252/427], [94mLoss[0m : 2.87737
[1mStep[0m  [294/427], [94mLoss[0m : 2.53869
[1mStep[0m  [336/427], [94mLoss[0m : 2.56610
[1mStep[0m  [378/427], [94mLoss[0m : 2.19511
[1mStep[0m  [420/427], [94mLoss[0m : 2.33937

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.94801
[1mStep[0m  [42/427], [94mLoss[0m : 2.87552
[1mStep[0m  [84/427], [94mLoss[0m : 3.07639
[1mStep[0m  [126/427], [94mLoss[0m : 2.26200
[1mStep[0m  [168/427], [94mLoss[0m : 2.39944
[1mStep[0m  [210/427], [94mLoss[0m : 2.04569
[1mStep[0m  [252/427], [94mLoss[0m : 2.52556
[1mStep[0m  [294/427], [94mLoss[0m : 2.40777
[1mStep[0m  [336/427], [94mLoss[0m : 2.21268
[1mStep[0m  [378/427], [94mLoss[0m : 1.89716
[1mStep[0m  [420/427], [94mLoss[0m : 2.39569

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.26011
[1mStep[0m  [42/427], [94mLoss[0m : 2.73263
[1mStep[0m  [84/427], [94mLoss[0m : 2.69282
[1mStep[0m  [126/427], [94mLoss[0m : 3.05705
[1mStep[0m  [168/427], [94mLoss[0m : 2.48567
[1mStep[0m  [210/427], [94mLoss[0m : 3.06134
[1mStep[0m  [252/427], [94mLoss[0m : 2.31687
[1mStep[0m  [294/427], [94mLoss[0m : 2.53220
[1mStep[0m  [336/427], [94mLoss[0m : 1.97928
[1mStep[0m  [378/427], [94mLoss[0m : 2.46916
[1mStep[0m  [420/427], [94mLoss[0m : 2.72516

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.86626
[1mStep[0m  [42/427], [94mLoss[0m : 2.65577
[1mStep[0m  [84/427], [94mLoss[0m : 2.56532
[1mStep[0m  [126/427], [94mLoss[0m : 2.49724
[1mStep[0m  [168/427], [94mLoss[0m : 2.73540
[1mStep[0m  [210/427], [94mLoss[0m : 2.17167
[1mStep[0m  [252/427], [94mLoss[0m : 2.89082
[1mStep[0m  [294/427], [94mLoss[0m : 3.33248
[1mStep[0m  [336/427], [94mLoss[0m : 2.22933
[1mStep[0m  [378/427], [94mLoss[0m : 2.48396
[1mStep[0m  [420/427], [94mLoss[0m : 2.38261

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.45418
[1mStep[0m  [42/427], [94mLoss[0m : 2.41711
[1mStep[0m  [84/427], [94mLoss[0m : 2.12031
[1mStep[0m  [126/427], [94mLoss[0m : 1.94352
[1mStep[0m  [168/427], [94mLoss[0m : 2.45436
[1mStep[0m  [210/427], [94mLoss[0m : 3.27722
[1mStep[0m  [252/427], [94mLoss[0m : 2.91793
[1mStep[0m  [294/427], [94mLoss[0m : 1.95037
[1mStep[0m  [336/427], [94mLoss[0m : 2.15946
[1mStep[0m  [378/427], [94mLoss[0m : 2.56349
[1mStep[0m  [420/427], [94mLoss[0m : 2.36735

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.19805
[1mStep[0m  [42/427], [94mLoss[0m : 2.74067
[1mStep[0m  [84/427], [94mLoss[0m : 1.93745
[1mStep[0m  [126/427], [94mLoss[0m : 1.94651
[1mStep[0m  [168/427], [94mLoss[0m : 2.41908
[1mStep[0m  [210/427], [94mLoss[0m : 2.02559
[1mStep[0m  [252/427], [94mLoss[0m : 2.75235
[1mStep[0m  [294/427], [94mLoss[0m : 2.70155
[1mStep[0m  [336/427], [94mLoss[0m : 2.25532
[1mStep[0m  [378/427], [94mLoss[0m : 2.25898
[1mStep[0m  [420/427], [94mLoss[0m : 2.75961

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.18460
[1mStep[0m  [42/427], [94mLoss[0m : 2.41973
[1mStep[0m  [84/427], [94mLoss[0m : 2.76379
[1mStep[0m  [126/427], [94mLoss[0m : 2.70417
[1mStep[0m  [168/427], [94mLoss[0m : 2.71367
[1mStep[0m  [210/427], [94mLoss[0m : 2.40556
[1mStep[0m  [252/427], [94mLoss[0m : 2.12585
[1mStep[0m  [294/427], [94mLoss[0m : 2.04260
[1mStep[0m  [336/427], [94mLoss[0m : 2.62495
[1mStep[0m  [378/427], [94mLoss[0m : 2.14016
[1mStep[0m  [420/427], [94mLoss[0m : 1.79988

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.66269
[1mStep[0m  [42/427], [94mLoss[0m : 2.40211
[1mStep[0m  [84/427], [94mLoss[0m : 3.28206
[1mStep[0m  [126/427], [94mLoss[0m : 1.98673
[1mStep[0m  [168/427], [94mLoss[0m : 2.52728
[1mStep[0m  [210/427], [94mLoss[0m : 2.18154
[1mStep[0m  [252/427], [94mLoss[0m : 2.96481
[1mStep[0m  [294/427], [94mLoss[0m : 2.06279
[1mStep[0m  [336/427], [94mLoss[0m : 2.12767
[1mStep[0m  [378/427], [94mLoss[0m : 2.60829
[1mStep[0m  [420/427], [94mLoss[0m : 2.46208

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.81417
[1mStep[0m  [42/427], [94mLoss[0m : 1.76756
[1mStep[0m  [84/427], [94mLoss[0m : 2.73808
[1mStep[0m  [126/427], [94mLoss[0m : 2.44462
[1mStep[0m  [168/427], [94mLoss[0m : 2.89176
[1mStep[0m  [210/427], [94mLoss[0m : 2.81824
[1mStep[0m  [252/427], [94mLoss[0m : 2.10872
[1mStep[0m  [294/427], [94mLoss[0m : 2.90917
[1mStep[0m  [336/427], [94mLoss[0m : 2.46942
[1mStep[0m  [378/427], [94mLoss[0m : 2.58233
[1mStep[0m  [420/427], [94mLoss[0m : 2.95028

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.96926
[1mStep[0m  [42/427], [94mLoss[0m : 1.81767
[1mStep[0m  [84/427], [94mLoss[0m : 2.30186
[1mStep[0m  [126/427], [94mLoss[0m : 3.77327
[1mStep[0m  [168/427], [94mLoss[0m : 2.16342
[1mStep[0m  [210/427], [94mLoss[0m : 2.09144
[1mStep[0m  [252/427], [94mLoss[0m : 2.50367
[1mStep[0m  [294/427], [94mLoss[0m : 2.46369
[1mStep[0m  [336/427], [94mLoss[0m : 2.61353
[1mStep[0m  [378/427], [94mLoss[0m : 2.17557
[1mStep[0m  [420/427], [94mLoss[0m : 2.80164

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.28122
[1mStep[0m  [42/427], [94mLoss[0m : 2.14175
[1mStep[0m  [84/427], [94mLoss[0m : 2.21564
[1mStep[0m  [126/427], [94mLoss[0m : 2.39905
[1mStep[0m  [168/427], [94mLoss[0m : 2.04423
[1mStep[0m  [210/427], [94mLoss[0m : 2.40761
[1mStep[0m  [252/427], [94mLoss[0m : 2.60428
[1mStep[0m  [294/427], [94mLoss[0m : 2.80311
[1mStep[0m  [336/427], [94mLoss[0m : 2.48912
[1mStep[0m  [378/427], [94mLoss[0m : 2.09571
[1mStep[0m  [420/427], [94mLoss[0m : 2.05707

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.38461
[1mStep[0m  [42/427], [94mLoss[0m : 2.35176
[1mStep[0m  [84/427], [94mLoss[0m : 2.65078
[1mStep[0m  [126/427], [94mLoss[0m : 2.11513
[1mStep[0m  [168/427], [94mLoss[0m : 2.31892
[1mStep[0m  [210/427], [94mLoss[0m : 3.04355
[1mStep[0m  [252/427], [94mLoss[0m : 2.97787
[1mStep[0m  [294/427], [94mLoss[0m : 1.92574
[1mStep[0m  [336/427], [94mLoss[0m : 2.27037
[1mStep[0m  [378/427], [94mLoss[0m : 2.91387
[1mStep[0m  [420/427], [94mLoss[0m : 2.21430

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.23421
[1mStep[0m  [42/427], [94mLoss[0m : 2.81446
[1mStep[0m  [84/427], [94mLoss[0m : 2.50292
[1mStep[0m  [126/427], [94mLoss[0m : 2.05645
[1mStep[0m  [168/427], [94mLoss[0m : 2.38992
[1mStep[0m  [210/427], [94mLoss[0m : 2.77927
[1mStep[0m  [252/427], [94mLoss[0m : 2.71624
[1mStep[0m  [294/427], [94mLoss[0m : 2.55528
[1mStep[0m  [336/427], [94mLoss[0m : 2.00372
[1mStep[0m  [378/427], [94mLoss[0m : 2.80226
[1mStep[0m  [420/427], [94mLoss[0m : 2.92369

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.59893
[1mStep[0m  [42/427], [94mLoss[0m : 1.95337
[1mStep[0m  [84/427], [94mLoss[0m : 2.10869
[1mStep[0m  [126/427], [94mLoss[0m : 2.08648
[1mStep[0m  [168/427], [94mLoss[0m : 2.44157
[1mStep[0m  [210/427], [94mLoss[0m : 2.78303
[1mStep[0m  [252/427], [94mLoss[0m : 2.18885
[1mStep[0m  [294/427], [94mLoss[0m : 2.37687
[1mStep[0m  [336/427], [94mLoss[0m : 2.64777
[1mStep[0m  [378/427], [94mLoss[0m : 2.59254
[1mStep[0m  [420/427], [94mLoss[0m : 2.51962

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.13232
[1mStep[0m  [42/427], [94mLoss[0m : 2.30189
[1mStep[0m  [84/427], [94mLoss[0m : 2.71735
[1mStep[0m  [126/427], [94mLoss[0m : 1.95734
[1mStep[0m  [168/427], [94mLoss[0m : 2.44948
[1mStep[0m  [210/427], [94mLoss[0m : 2.81629
[1mStep[0m  [252/427], [94mLoss[0m : 2.42774
[1mStep[0m  [294/427], [94mLoss[0m : 2.18556
[1mStep[0m  [336/427], [94mLoss[0m : 2.09034
[1mStep[0m  [378/427], [94mLoss[0m : 2.40322
[1mStep[0m  [420/427], [94mLoss[0m : 2.11065

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.26268
[1mStep[0m  [42/427], [94mLoss[0m : 2.56889
[1mStep[0m  [84/427], [94mLoss[0m : 2.48670
[1mStep[0m  [126/427], [94mLoss[0m : 1.82163
[1mStep[0m  [168/427], [94mLoss[0m : 2.49954
[1mStep[0m  [210/427], [94mLoss[0m : 2.30319
[1mStep[0m  [252/427], [94mLoss[0m : 2.81477
[1mStep[0m  [294/427], [94mLoss[0m : 2.56515
[1mStep[0m  [336/427], [94mLoss[0m : 2.61672
[1mStep[0m  [378/427], [94mLoss[0m : 2.38454
[1mStep[0m  [420/427], [94mLoss[0m : 2.29884

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.10315
[1mStep[0m  [42/427], [94mLoss[0m : 2.17356
[1mStep[0m  [84/427], [94mLoss[0m : 2.62712
[1mStep[0m  [126/427], [94mLoss[0m : 3.04291
[1mStep[0m  [168/427], [94mLoss[0m : 2.47111
[1mStep[0m  [210/427], [94mLoss[0m : 2.24271
[1mStep[0m  [252/427], [94mLoss[0m : 1.84440
[1mStep[0m  [294/427], [94mLoss[0m : 2.82968
[1mStep[0m  [336/427], [94mLoss[0m : 2.40017
[1mStep[0m  [378/427], [94mLoss[0m : 2.13135
[1mStep[0m  [420/427], [94mLoss[0m : 2.60538

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.390, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.68265
[1mStep[0m  [42/427], [94mLoss[0m : 2.39518
[1mStep[0m  [84/427], [94mLoss[0m : 2.12505
[1mStep[0m  [126/427], [94mLoss[0m : 2.41994
[1mStep[0m  [168/427], [94mLoss[0m : 2.28465
[1mStep[0m  [210/427], [94mLoss[0m : 2.66510
[1mStep[0m  [252/427], [94mLoss[0m : 2.34623
[1mStep[0m  [294/427], [94mLoss[0m : 2.43703
[1mStep[0m  [336/427], [94mLoss[0m : 2.08780
[1mStep[0m  [378/427], [94mLoss[0m : 2.38445
[1mStep[0m  [420/427], [94mLoss[0m : 2.79694

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.392, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.10739
[1mStep[0m  [42/427], [94mLoss[0m : 3.57611
[1mStep[0m  [84/427], [94mLoss[0m : 2.07969
[1mStep[0m  [126/427], [94mLoss[0m : 2.15317
[1mStep[0m  [168/427], [94mLoss[0m : 2.13314
[1mStep[0m  [210/427], [94mLoss[0m : 2.67040
[1mStep[0m  [252/427], [94mLoss[0m : 2.48826
[1mStep[0m  [294/427], [94mLoss[0m : 2.24653
[1mStep[0m  [336/427], [94mLoss[0m : 2.67887
[1mStep[0m  [378/427], [94mLoss[0m : 2.31287
[1mStep[0m  [420/427], [94mLoss[0m : 2.48789

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.422, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.28148
[1mStep[0m  [42/427], [94mLoss[0m : 2.18267
[1mStep[0m  [84/427], [94mLoss[0m : 2.78966
[1mStep[0m  [126/427], [94mLoss[0m : 2.20039
[1mStep[0m  [168/427], [94mLoss[0m : 2.54700
[1mStep[0m  [210/427], [94mLoss[0m : 2.07489
[1mStep[0m  [252/427], [94mLoss[0m : 2.66040
[1mStep[0m  [294/427], [94mLoss[0m : 3.00443
[1mStep[0m  [336/427], [94mLoss[0m : 2.72621
[1mStep[0m  [378/427], [94mLoss[0m : 2.39886
[1mStep[0m  [420/427], [94mLoss[0m : 2.56406

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.392, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.19434
[1mStep[0m  [42/427], [94mLoss[0m : 2.03499
[1mStep[0m  [84/427], [94mLoss[0m : 2.72614
[1mStep[0m  [126/427], [94mLoss[0m : 2.70074
[1mStep[0m  [168/427], [94mLoss[0m : 2.33551
[1mStep[0m  [210/427], [94mLoss[0m : 2.89582
[1mStep[0m  [252/427], [94mLoss[0m : 2.61644
[1mStep[0m  [294/427], [94mLoss[0m : 2.25999
[1mStep[0m  [336/427], [94mLoss[0m : 2.36907
[1mStep[0m  [378/427], [94mLoss[0m : 1.61028
[1mStep[0m  [420/427], [94mLoss[0m : 2.86252

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.404, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.23678
[1mStep[0m  [42/427], [94mLoss[0m : 2.88079
[1mStep[0m  [84/427], [94mLoss[0m : 2.26117
[1mStep[0m  [126/427], [94mLoss[0m : 2.73541
[1mStep[0m  [168/427], [94mLoss[0m : 2.86645
[1mStep[0m  [210/427], [94mLoss[0m : 2.41223
[1mStep[0m  [252/427], [94mLoss[0m : 2.58521
[1mStep[0m  [294/427], [94mLoss[0m : 1.84840
[1mStep[0m  [336/427], [94mLoss[0m : 1.99627
[1mStep[0m  [378/427], [94mLoss[0m : 2.36731
[1mStep[0m  [420/427], [94mLoss[0m : 2.26310

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.396, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.57631
[1mStep[0m  [42/427], [94mLoss[0m : 2.36779
[1mStep[0m  [84/427], [94mLoss[0m : 2.63293
[1mStep[0m  [126/427], [94mLoss[0m : 2.22544
[1mStep[0m  [168/427], [94mLoss[0m : 2.93078
[1mStep[0m  [210/427], [94mLoss[0m : 2.24303
[1mStep[0m  [252/427], [94mLoss[0m : 2.30457
[1mStep[0m  [294/427], [94mLoss[0m : 2.82537
[1mStep[0m  [336/427], [94mLoss[0m : 2.70929
[1mStep[0m  [378/427], [94mLoss[0m : 2.65134
[1mStep[0m  [420/427], [94mLoss[0m : 1.63556

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.408, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.14539
[1mStep[0m  [42/427], [94mLoss[0m : 2.38076
[1mStep[0m  [84/427], [94mLoss[0m : 3.22687
[1mStep[0m  [126/427], [94mLoss[0m : 2.34841
[1mStep[0m  [168/427], [94mLoss[0m : 2.62911
[1mStep[0m  [210/427], [94mLoss[0m : 2.18910
[1mStep[0m  [252/427], [94mLoss[0m : 2.92568
[1mStep[0m  [294/427], [94mLoss[0m : 2.06827
[1mStep[0m  [336/427], [94mLoss[0m : 2.80653
[1mStep[0m  [378/427], [94mLoss[0m : 1.97695
[1mStep[0m  [420/427], [94mLoss[0m : 2.52158

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.388, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.47227
[1mStep[0m  [42/427], [94mLoss[0m : 2.16155
[1mStep[0m  [84/427], [94mLoss[0m : 2.25445
[1mStep[0m  [126/427], [94mLoss[0m : 2.09693
[1mStep[0m  [168/427], [94mLoss[0m : 2.40677
[1mStep[0m  [210/427], [94mLoss[0m : 2.25231
[1mStep[0m  [252/427], [94mLoss[0m : 2.37447
[1mStep[0m  [294/427], [94mLoss[0m : 2.69420
[1mStep[0m  [336/427], [94mLoss[0m : 2.32501
[1mStep[0m  [378/427], [94mLoss[0m : 2.70959
[1mStep[0m  [420/427], [94mLoss[0m : 2.07369

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.388, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.10662
[1mStep[0m  [42/427], [94mLoss[0m : 2.39312
[1mStep[0m  [84/427], [94mLoss[0m : 2.03709
[1mStep[0m  [126/427], [94mLoss[0m : 2.29816
[1mStep[0m  [168/427], [94mLoss[0m : 2.09672
[1mStep[0m  [210/427], [94mLoss[0m : 2.17998
[1mStep[0m  [252/427], [94mLoss[0m : 1.98598
[1mStep[0m  [294/427], [94mLoss[0m : 2.52006
[1mStep[0m  [336/427], [94mLoss[0m : 2.13378
[1mStep[0m  [378/427], [94mLoss[0m : 2.94326
[1mStep[0m  [420/427], [94mLoss[0m : 2.52295

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.392773728975108
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/427], [94mLoss[0m : 2.76164
[1mStep[0m  [42/427], [94mLoss[0m : 2.33658
[1mStep[0m  [84/427], [94mLoss[0m : 3.09920
[1mStep[0m  [126/427], [94mLoss[0m : 2.02699
[1mStep[0m  [168/427], [94mLoss[0m : 2.25937
[1mStep[0m  [210/427], [94mLoss[0m : 2.72970
[1mStep[0m  [252/427], [94mLoss[0m : 2.43061
[1mStep[0m  [294/427], [94mLoss[0m : 2.45471
[1mStep[0m  [336/427], [94mLoss[0m : 2.22556
[1mStep[0m  [378/427], [94mLoss[0m : 2.81353
[1mStep[0m  [420/427], [94mLoss[0m : 2.97349

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.92204
[1mStep[0m  [42/427], [94mLoss[0m : 2.34338
[1mStep[0m  [84/427], [94mLoss[0m : 2.59355
[1mStep[0m  [126/427], [94mLoss[0m : 2.68638
[1mStep[0m  [168/427], [94mLoss[0m : 2.46544
[1mStep[0m  [210/427], [94mLoss[0m : 2.14920
[1mStep[0m  [252/427], [94mLoss[0m : 2.59186
[1mStep[0m  [294/427], [94mLoss[0m : 2.42654
[1mStep[0m  [336/427], [94mLoss[0m : 2.27502
[1mStep[0m  [378/427], [94mLoss[0m : 2.18509
[1mStep[0m  [420/427], [94mLoss[0m : 2.29131

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.93378
[1mStep[0m  [42/427], [94mLoss[0m : 2.66600
[1mStep[0m  [84/427], [94mLoss[0m : 2.68381
[1mStep[0m  [126/427], [94mLoss[0m : 2.19337
[1mStep[0m  [168/427], [94mLoss[0m : 2.18463
[1mStep[0m  [210/427], [94mLoss[0m : 2.14259
[1mStep[0m  [252/427], [94mLoss[0m : 2.24986
[1mStep[0m  [294/427], [94mLoss[0m : 2.10939
[1mStep[0m  [336/427], [94mLoss[0m : 2.54885
[1mStep[0m  [378/427], [94mLoss[0m : 2.55796
[1mStep[0m  [420/427], [94mLoss[0m : 3.57949

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.81524
[1mStep[0m  [42/427], [94mLoss[0m : 2.93576
[1mStep[0m  [84/427], [94mLoss[0m : 1.82936
[1mStep[0m  [126/427], [94mLoss[0m : 2.34837
[1mStep[0m  [168/427], [94mLoss[0m : 2.32550
[1mStep[0m  [210/427], [94mLoss[0m : 2.69713
[1mStep[0m  [252/427], [94mLoss[0m : 2.34162
[1mStep[0m  [294/427], [94mLoss[0m : 2.20753
[1mStep[0m  [336/427], [94mLoss[0m : 1.96170
[1mStep[0m  [378/427], [94mLoss[0m : 2.19220
[1mStep[0m  [420/427], [94mLoss[0m : 2.74799

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.88262
[1mStep[0m  [42/427], [94mLoss[0m : 1.88686
[1mStep[0m  [84/427], [94mLoss[0m : 2.57998
[1mStep[0m  [126/427], [94mLoss[0m : 2.26374
[1mStep[0m  [168/427], [94mLoss[0m : 1.99015
[1mStep[0m  [210/427], [94mLoss[0m : 2.82676
[1mStep[0m  [252/427], [94mLoss[0m : 2.51888
[1mStep[0m  [294/427], [94mLoss[0m : 2.15400
[1mStep[0m  [336/427], [94mLoss[0m : 2.43903
[1mStep[0m  [378/427], [94mLoss[0m : 1.92940
[1mStep[0m  [420/427], [94mLoss[0m : 2.25071

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.45082
[1mStep[0m  [42/427], [94mLoss[0m : 2.64337
[1mStep[0m  [84/427], [94mLoss[0m : 1.71156
[1mStep[0m  [126/427], [94mLoss[0m : 2.18463
[1mStep[0m  [168/427], [94mLoss[0m : 2.05950
[1mStep[0m  [210/427], [94mLoss[0m : 2.44071
[1mStep[0m  [252/427], [94mLoss[0m : 2.22846
[1mStep[0m  [294/427], [94mLoss[0m : 2.15592
[1mStep[0m  [336/427], [94mLoss[0m : 2.18846
[1mStep[0m  [378/427], [94mLoss[0m : 2.84652
[1mStep[0m  [420/427], [94mLoss[0m : 2.39912

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.96330
[1mStep[0m  [42/427], [94mLoss[0m : 2.12752
[1mStep[0m  [84/427], [94mLoss[0m : 2.09372
[1mStep[0m  [126/427], [94mLoss[0m : 2.70549
[1mStep[0m  [168/427], [94mLoss[0m : 2.26200
[1mStep[0m  [210/427], [94mLoss[0m : 1.78493
[1mStep[0m  [252/427], [94mLoss[0m : 2.13818
[1mStep[0m  [294/427], [94mLoss[0m : 2.63461
[1mStep[0m  [336/427], [94mLoss[0m : 1.76508
[1mStep[0m  [378/427], [94mLoss[0m : 2.46608
[1mStep[0m  [420/427], [94mLoss[0m : 2.22538

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.77319
[1mStep[0m  [42/427], [94mLoss[0m : 2.01220
[1mStep[0m  [84/427], [94mLoss[0m : 2.61350
[1mStep[0m  [126/427], [94mLoss[0m : 1.79994
[1mStep[0m  [168/427], [94mLoss[0m : 1.85963
[1mStep[0m  [210/427], [94mLoss[0m : 1.75287
[1mStep[0m  [252/427], [94mLoss[0m : 2.12079
[1mStep[0m  [294/427], [94mLoss[0m : 2.08922
[1mStep[0m  [336/427], [94mLoss[0m : 2.65223
[1mStep[0m  [378/427], [94mLoss[0m : 1.36308
[1mStep[0m  [420/427], [94mLoss[0m : 1.70802

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.79373
[1mStep[0m  [42/427], [94mLoss[0m : 1.55907
[1mStep[0m  [84/427], [94mLoss[0m : 1.81027
[1mStep[0m  [126/427], [94mLoss[0m : 1.76944
[1mStep[0m  [168/427], [94mLoss[0m : 2.11864
[1mStep[0m  [210/427], [94mLoss[0m : 1.93760
[1mStep[0m  [252/427], [94mLoss[0m : 1.78522
[1mStep[0m  [294/427], [94mLoss[0m : 1.52306
[1mStep[0m  [336/427], [94mLoss[0m : 1.77598
[1mStep[0m  [378/427], [94mLoss[0m : 2.03657
[1mStep[0m  [420/427], [94mLoss[0m : 1.91532

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.04791
[1mStep[0m  [42/427], [94mLoss[0m : 2.06361
[1mStep[0m  [84/427], [94mLoss[0m : 1.32686
[1mStep[0m  [126/427], [94mLoss[0m : 2.14480
[1mStep[0m  [168/427], [94mLoss[0m : 2.24578
[1mStep[0m  [210/427], [94mLoss[0m : 2.13155
[1mStep[0m  [252/427], [94mLoss[0m : 1.74974
[1mStep[0m  [294/427], [94mLoss[0m : 2.26546
[1mStep[0m  [336/427], [94mLoss[0m : 1.73490
[1mStep[0m  [378/427], [94mLoss[0m : 2.20570
[1mStep[0m  [420/427], [94mLoss[0m : 1.73248

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.83968
[1mStep[0m  [42/427], [94mLoss[0m : 1.92596
[1mStep[0m  [84/427], [94mLoss[0m : 1.94470
[1mStep[0m  [126/427], [94mLoss[0m : 1.79058
[1mStep[0m  [168/427], [94mLoss[0m : 1.73237
[1mStep[0m  [210/427], [94mLoss[0m : 1.67031
[1mStep[0m  [252/427], [94mLoss[0m : 2.25727
[1mStep[0m  [294/427], [94mLoss[0m : 1.61000
[1mStep[0m  [336/427], [94mLoss[0m : 1.84130
[1mStep[0m  [378/427], [94mLoss[0m : 2.26430
[1mStep[0m  [420/427], [94mLoss[0m : 2.25826

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.552, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.93007
[1mStep[0m  [42/427], [94mLoss[0m : 1.45896
[1mStep[0m  [84/427], [94mLoss[0m : 1.54797
[1mStep[0m  [126/427], [94mLoss[0m : 1.49329
[1mStep[0m  [168/427], [94mLoss[0m : 2.38829
[1mStep[0m  [210/427], [94mLoss[0m : 2.03008
[1mStep[0m  [252/427], [94mLoss[0m : 1.54307
[1mStep[0m  [294/427], [94mLoss[0m : 1.43822
[1mStep[0m  [336/427], [94mLoss[0m : 2.14468
[1mStep[0m  [378/427], [94mLoss[0m : 2.65222
[1mStep[0m  [420/427], [94mLoss[0m : 1.83654

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.60913
[1mStep[0m  [42/427], [94mLoss[0m : 1.82282
[1mStep[0m  [84/427], [94mLoss[0m : 2.27901
[1mStep[0m  [126/427], [94mLoss[0m : 2.04878
[1mStep[0m  [168/427], [94mLoss[0m : 1.94946
[1mStep[0m  [210/427], [94mLoss[0m : 1.56100
[1mStep[0m  [252/427], [94mLoss[0m : 1.93247
[1mStep[0m  [294/427], [94mLoss[0m : 2.17511
[1mStep[0m  [336/427], [94mLoss[0m : 2.13948
[1mStep[0m  [378/427], [94mLoss[0m : 1.24755
[1mStep[0m  [420/427], [94mLoss[0m : 1.90734

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.42088
[1mStep[0m  [42/427], [94mLoss[0m : 2.00050
[1mStep[0m  [84/427], [94mLoss[0m : 1.91106
[1mStep[0m  [126/427], [94mLoss[0m : 1.59211
[1mStep[0m  [168/427], [94mLoss[0m : 1.70032
[1mStep[0m  [210/427], [94mLoss[0m : 1.74264
[1mStep[0m  [252/427], [94mLoss[0m : 2.05591
[1mStep[0m  [294/427], [94mLoss[0m : 1.93288
[1mStep[0m  [336/427], [94mLoss[0m : 1.73484
[1mStep[0m  [378/427], [94mLoss[0m : 1.59870
[1mStep[0m  [420/427], [94mLoss[0m : 2.24482

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.74361
[1mStep[0m  [42/427], [94mLoss[0m : 1.68536
[1mStep[0m  [84/427], [94mLoss[0m : 1.25191
[1mStep[0m  [126/427], [94mLoss[0m : 1.41655
[1mStep[0m  [168/427], [94mLoss[0m : 1.82662
[1mStep[0m  [210/427], [94mLoss[0m : 2.07409
[1mStep[0m  [252/427], [94mLoss[0m : 1.99530
[1mStep[0m  [294/427], [94mLoss[0m : 1.95621
[1mStep[0m  [336/427], [94mLoss[0m : 1.69874
[1mStep[0m  [378/427], [94mLoss[0m : 1.99704
[1mStep[0m  [420/427], [94mLoss[0m : 2.06491

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.60342
[1mStep[0m  [42/427], [94mLoss[0m : 2.31466
[1mStep[0m  [84/427], [94mLoss[0m : 1.70563
[1mStep[0m  [126/427], [94mLoss[0m : 2.25304
[1mStep[0m  [168/427], [94mLoss[0m : 1.98719
[1mStep[0m  [210/427], [94mLoss[0m : 2.04006
[1mStep[0m  [252/427], [94mLoss[0m : 1.60507
[1mStep[0m  [294/427], [94mLoss[0m : 1.62914
[1mStep[0m  [336/427], [94mLoss[0m : 1.87827
[1mStep[0m  [378/427], [94mLoss[0m : 1.63926
[1mStep[0m  [420/427], [94mLoss[0m : 1.65189

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.529, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.64599
[1mStep[0m  [42/427], [94mLoss[0m : 1.41609
[1mStep[0m  [84/427], [94mLoss[0m : 1.96253
[1mStep[0m  [126/427], [94mLoss[0m : 1.69587
[1mStep[0m  [168/427], [94mLoss[0m : 1.34720
[1mStep[0m  [210/427], [94mLoss[0m : 1.62261
[1mStep[0m  [252/427], [94mLoss[0m : 2.02373
[1mStep[0m  [294/427], [94mLoss[0m : 1.97327
[1mStep[0m  [336/427], [94mLoss[0m : 2.51984
[1mStep[0m  [378/427], [94mLoss[0m : 1.65171
[1mStep[0m  [420/427], [94mLoss[0m : 2.04165

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.67832
[1mStep[0m  [42/427], [94mLoss[0m : 1.46600
[1mStep[0m  [84/427], [94mLoss[0m : 1.74166
[1mStep[0m  [126/427], [94mLoss[0m : 1.68187
[1mStep[0m  [168/427], [94mLoss[0m : 1.68273
[1mStep[0m  [210/427], [94mLoss[0m : 1.60089
[1mStep[0m  [252/427], [94mLoss[0m : 1.58437
[1mStep[0m  [294/427], [94mLoss[0m : 1.91198
[1mStep[0m  [336/427], [94mLoss[0m : 1.44559
[1mStep[0m  [378/427], [94mLoss[0m : 1.54192
[1mStep[0m  [420/427], [94mLoss[0m : 1.80174

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.00965
[1mStep[0m  [42/427], [94mLoss[0m : 2.41402
[1mStep[0m  [84/427], [94mLoss[0m : 1.61672
[1mStep[0m  [126/427], [94mLoss[0m : 1.87015
[1mStep[0m  [168/427], [94mLoss[0m : 1.56083
[1mStep[0m  [210/427], [94mLoss[0m : 1.66662
[1mStep[0m  [252/427], [94mLoss[0m : 1.52484
[1mStep[0m  [294/427], [94mLoss[0m : 1.73268
[1mStep[0m  [336/427], [94mLoss[0m : 1.84639
[1mStep[0m  [378/427], [94mLoss[0m : 1.86631
[1mStep[0m  [420/427], [94mLoss[0m : 1.75965

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.724, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.52081
[1mStep[0m  [42/427], [94mLoss[0m : 1.67118
[1mStep[0m  [84/427], [94mLoss[0m : 1.52211
[1mStep[0m  [126/427], [94mLoss[0m : 1.34836
[1mStep[0m  [168/427], [94mLoss[0m : 1.86915
[1mStep[0m  [210/427], [94mLoss[0m : 1.74231
[1mStep[0m  [252/427], [94mLoss[0m : 1.61153
[1mStep[0m  [294/427], [94mLoss[0m : 1.56169
[1mStep[0m  [336/427], [94mLoss[0m : 1.73630
[1mStep[0m  [378/427], [94mLoss[0m : 1.69810
[1mStep[0m  [420/427], [94mLoss[0m : 1.78456

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.58406
[1mStep[0m  [42/427], [94mLoss[0m : 2.05905
[1mStep[0m  [84/427], [94mLoss[0m : 1.73590
[1mStep[0m  [126/427], [94mLoss[0m : 1.67922
[1mStep[0m  [168/427], [94mLoss[0m : 1.54710
[1mStep[0m  [210/427], [94mLoss[0m : 1.47212
[1mStep[0m  [252/427], [94mLoss[0m : 1.51188
[1mStep[0m  [294/427], [94mLoss[0m : 1.72891
[1mStep[0m  [336/427], [94mLoss[0m : 1.58864
[1mStep[0m  [378/427], [94mLoss[0m : 1.69936
[1mStep[0m  [420/427], [94mLoss[0m : 1.44233

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.541, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.67949
[1mStep[0m  [42/427], [94mLoss[0m : 2.04534
[1mStep[0m  [84/427], [94mLoss[0m : 1.78124
[1mStep[0m  [126/427], [94mLoss[0m : 1.54398
[1mStep[0m  [168/427], [94mLoss[0m : 1.60511
[1mStep[0m  [210/427], [94mLoss[0m : 2.14048
[1mStep[0m  [252/427], [94mLoss[0m : 1.40245
[1mStep[0m  [294/427], [94mLoss[0m : 1.73272
[1mStep[0m  [336/427], [94mLoss[0m : 1.46860
[1mStep[0m  [378/427], [94mLoss[0m : 1.79104
[1mStep[0m  [420/427], [94mLoss[0m : 1.23315

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.446, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.87923
[1mStep[0m  [42/427], [94mLoss[0m : 1.46747
[1mStep[0m  [84/427], [94mLoss[0m : 1.94360
[1mStep[0m  [126/427], [94mLoss[0m : 2.13975
[1mStep[0m  [168/427], [94mLoss[0m : 2.00139
[1mStep[0m  [210/427], [94mLoss[0m : 1.48128
[1mStep[0m  [252/427], [94mLoss[0m : 1.18325
[1mStep[0m  [294/427], [94mLoss[0m : 2.37276
[1mStep[0m  [336/427], [94mLoss[0m : 1.42117
[1mStep[0m  [378/427], [94mLoss[0m : 1.45039
[1mStep[0m  [420/427], [94mLoss[0m : 1.49911

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.19618
[1mStep[0m  [42/427], [94mLoss[0m : 1.27311
[1mStep[0m  [84/427], [94mLoss[0m : 1.59615
[1mStep[0m  [126/427], [94mLoss[0m : 1.63559
[1mStep[0m  [168/427], [94mLoss[0m : 1.85002
[1mStep[0m  [210/427], [94mLoss[0m : 2.56368
[1mStep[0m  [252/427], [94mLoss[0m : 2.20066
[1mStep[0m  [294/427], [94mLoss[0m : 1.73931
[1mStep[0m  [336/427], [94mLoss[0m : 1.46581
[1mStep[0m  [378/427], [94mLoss[0m : 1.21682
[1mStep[0m  [420/427], [94mLoss[0m : 1.29734

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.31618
[1mStep[0m  [42/427], [94mLoss[0m : 1.65335
[1mStep[0m  [84/427], [94mLoss[0m : 1.24755
[1mStep[0m  [126/427], [94mLoss[0m : 1.38813
[1mStep[0m  [168/427], [94mLoss[0m : 1.43896
[1mStep[0m  [210/427], [94mLoss[0m : 1.38015
[1mStep[0m  [252/427], [94mLoss[0m : 1.73174
[1mStep[0m  [294/427], [94mLoss[0m : 1.39932
[1mStep[0m  [336/427], [94mLoss[0m : 1.55626
[1mStep[0m  [378/427], [94mLoss[0m : 1.66670
[1mStep[0m  [420/427], [94mLoss[0m : 1.61895

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.34063
[1mStep[0m  [42/427], [94mLoss[0m : 1.43118
[1mStep[0m  [84/427], [94mLoss[0m : 1.36059
[1mStep[0m  [126/427], [94mLoss[0m : 1.27776
[1mStep[0m  [168/427], [94mLoss[0m : 1.96549
[1mStep[0m  [210/427], [94mLoss[0m : 1.22207
[1mStep[0m  [252/427], [94mLoss[0m : 1.53969
[1mStep[0m  [294/427], [94mLoss[0m : 1.67731
[1mStep[0m  [336/427], [94mLoss[0m : 1.06377
[1mStep[0m  [378/427], [94mLoss[0m : 1.62953
[1mStep[0m  [420/427], [94mLoss[0m : 1.22845

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.46739
[1mStep[0m  [42/427], [94mLoss[0m : 1.74862
[1mStep[0m  [84/427], [94mLoss[0m : 1.32931
[1mStep[0m  [126/427], [94mLoss[0m : 1.69013
[1mStep[0m  [168/427], [94mLoss[0m : 1.56317
[1mStep[0m  [210/427], [94mLoss[0m : 1.77556
[1mStep[0m  [252/427], [94mLoss[0m : 1.54611
[1mStep[0m  [294/427], [94mLoss[0m : 1.79992
[1mStep[0m  [336/427], [94mLoss[0m : 1.87886
[1mStep[0m  [378/427], [94mLoss[0m : 1.51810
[1mStep[0m  [420/427], [94mLoss[0m : 1.66050

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.10194
[1mStep[0m  [42/427], [94mLoss[0m : 1.95663
[1mStep[0m  [84/427], [94mLoss[0m : 1.23055
[1mStep[0m  [126/427], [94mLoss[0m : 1.93153
[1mStep[0m  [168/427], [94mLoss[0m : 1.58050
[1mStep[0m  [210/427], [94mLoss[0m : 1.56010
[1mStep[0m  [252/427], [94mLoss[0m : 1.25124
[1mStep[0m  [294/427], [94mLoss[0m : 1.60144
[1mStep[0m  [336/427], [94mLoss[0m : 1.15200
[1mStep[0m  [378/427], [94mLoss[0m : 1.57936
[1mStep[0m  [420/427], [94mLoss[0m : 1.34107

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.31647
[1mStep[0m  [42/427], [94mLoss[0m : 1.85323
[1mStep[0m  [84/427], [94mLoss[0m : 1.54389
[1mStep[0m  [126/427], [94mLoss[0m : 1.41012
[1mStep[0m  [168/427], [94mLoss[0m : 1.19670
[1mStep[0m  [210/427], [94mLoss[0m : 1.51283
[1mStep[0m  [252/427], [94mLoss[0m : 1.45126
[1mStep[0m  [294/427], [94mLoss[0m : 1.33549
[1mStep[0m  [336/427], [94mLoss[0m : 1.41673
[1mStep[0m  [378/427], [94mLoss[0m : 1.40422
[1mStep[0m  [420/427], [94mLoss[0m : 1.43353

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.635, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.18907
[1mStep[0m  [42/427], [94mLoss[0m : 1.67315
[1mStep[0m  [84/427], [94mLoss[0m : 1.29186
[1mStep[0m  [126/427], [94mLoss[0m : 0.88916
[1mStep[0m  [168/427], [94mLoss[0m : 1.45340
[1mStep[0m  [210/427], [94mLoss[0m : 1.21908
[1mStep[0m  [252/427], [94mLoss[0m : 1.40958
[1mStep[0m  [294/427], [94mLoss[0m : 1.57519
[1mStep[0m  [336/427], [94mLoss[0m : 1.27405
[1mStep[0m  [378/427], [94mLoss[0m : 1.53303
[1mStep[0m  [420/427], [94mLoss[0m : 1.20408

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.459
====================================

Phase 2 - Evaluation MAE:  2.459446814138565
MAE score P1      2.392774
MAE score P2      2.459447
loss              1.489732
learning_rate     0.007525
batch_size              32
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 11.01204
[1mStep[0m  [2/26], [94mLoss[0m : 10.85530
[1mStep[0m  [4/26], [94mLoss[0m : 10.81739
[1mStep[0m  [6/26], [94mLoss[0m : 10.79321
[1mStep[0m  [8/26], [94mLoss[0m : 10.49584
[1mStep[0m  [10/26], [94mLoss[0m : 10.79394
[1mStep[0m  [12/26], [94mLoss[0m : 10.48657
[1mStep[0m  [14/26], [94mLoss[0m : 10.36184
[1mStep[0m  [16/26], [94mLoss[0m : 10.48049
[1mStep[0m  [18/26], [94mLoss[0m : 10.17640
[1mStep[0m  [20/26], [94mLoss[0m : 10.19360
[1mStep[0m  [22/26], [94mLoss[0m : 9.93948
[1mStep[0m  [24/26], [94mLoss[0m : 10.13040

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.517, [92mTest[0m: 11.047, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.86147
[1mStep[0m  [2/26], [94mLoss[0m : 9.65994
[1mStep[0m  [4/26], [94mLoss[0m : 10.05782
[1mStep[0m  [6/26], [94mLoss[0m : 9.41635
[1mStep[0m  [8/26], [94mLoss[0m : 9.49662
[1mStep[0m  [10/26], [94mLoss[0m : 9.35885
[1mStep[0m  [12/26], [94mLoss[0m : 9.45414
[1mStep[0m  [14/26], [94mLoss[0m : 9.29829
[1mStep[0m  [16/26], [94mLoss[0m : 9.01541
[1mStep[0m  [18/26], [94mLoss[0m : 9.14510
[1mStep[0m  [20/26], [94mLoss[0m : 8.67389
[1mStep[0m  [22/26], [94mLoss[0m : 8.59506
[1mStep[0m  [24/26], [94mLoss[0m : 8.41395

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.275, [92mTest[0m: 10.263, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.56825
[1mStep[0m  [2/26], [94mLoss[0m : 8.44836
[1mStep[0m  [4/26], [94mLoss[0m : 8.31143
[1mStep[0m  [6/26], [94mLoss[0m : 8.43369
[1mStep[0m  [8/26], [94mLoss[0m : 8.39310
[1mStep[0m  [10/26], [94mLoss[0m : 7.87636
[1mStep[0m  [12/26], [94mLoss[0m : 7.92325
[1mStep[0m  [14/26], [94mLoss[0m : 7.57633
[1mStep[0m  [16/26], [94mLoss[0m : 7.93821
[1mStep[0m  [18/26], [94mLoss[0m : 7.81189
[1mStep[0m  [20/26], [94mLoss[0m : 7.70798
[1mStep[0m  [22/26], [94mLoss[0m : 7.40332
[1mStep[0m  [24/26], [94mLoss[0m : 7.28235

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.975, [92mTest[0m: 9.084, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.08413
[1mStep[0m  [2/26], [94mLoss[0m : 7.06086
[1mStep[0m  [4/26], [94mLoss[0m : 7.02909
[1mStep[0m  [6/26], [94mLoss[0m : 6.99896
[1mStep[0m  [8/26], [94mLoss[0m : 6.66900
[1mStep[0m  [10/26], [94mLoss[0m : 6.67894
[1mStep[0m  [12/26], [94mLoss[0m : 6.61928
[1mStep[0m  [14/26], [94mLoss[0m : 6.31187
[1mStep[0m  [16/26], [94mLoss[0m : 6.34801
[1mStep[0m  [18/26], [94mLoss[0m : 6.31675
[1mStep[0m  [20/26], [94mLoss[0m : 6.30575
[1mStep[0m  [22/26], [94mLoss[0m : 6.30125
[1mStep[0m  [24/26], [94mLoss[0m : 6.16600

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.606, [92mTest[0m: 7.938, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.14968
[1mStep[0m  [2/26], [94mLoss[0m : 5.90108
[1mStep[0m  [4/26], [94mLoss[0m : 5.68630
[1mStep[0m  [6/26], [94mLoss[0m : 5.55016
[1mStep[0m  [8/26], [94mLoss[0m : 5.56388
[1mStep[0m  [10/26], [94mLoss[0m : 5.35471
[1mStep[0m  [12/26], [94mLoss[0m : 5.33720
[1mStep[0m  [14/26], [94mLoss[0m : 5.19368
[1mStep[0m  [16/26], [94mLoss[0m : 4.83891
[1mStep[0m  [18/26], [94mLoss[0m : 4.94174
[1mStep[0m  [20/26], [94mLoss[0m : 5.10579
[1mStep[0m  [22/26], [94mLoss[0m : 4.60788
[1mStep[0m  [24/26], [94mLoss[0m : 4.72964

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.250, [92mTest[0m: 6.760, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.59067
[1mStep[0m  [2/26], [94mLoss[0m : 4.26017
[1mStep[0m  [4/26], [94mLoss[0m : 4.41419
[1mStep[0m  [6/26], [94mLoss[0m : 4.11194
[1mStep[0m  [8/26], [94mLoss[0m : 4.20448
[1mStep[0m  [10/26], [94mLoss[0m : 4.34650
[1mStep[0m  [12/26], [94mLoss[0m : 4.12681
[1mStep[0m  [14/26], [94mLoss[0m : 3.72501
[1mStep[0m  [16/26], [94mLoss[0m : 4.19164
[1mStep[0m  [18/26], [94mLoss[0m : 3.92830
[1mStep[0m  [20/26], [94mLoss[0m : 3.97734
[1mStep[0m  [22/26], [94mLoss[0m : 3.81630
[1mStep[0m  [24/26], [94mLoss[0m : 3.43611

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.100, [92mTest[0m: 5.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.83247
[1mStep[0m  [2/26], [94mLoss[0m : 3.68714
[1mStep[0m  [4/26], [94mLoss[0m : 3.60570
[1mStep[0m  [6/26], [94mLoss[0m : 3.40747
[1mStep[0m  [8/26], [94mLoss[0m : 3.30135
[1mStep[0m  [10/26], [94mLoss[0m : 3.61002
[1mStep[0m  [12/26], [94mLoss[0m : 3.31318
[1mStep[0m  [14/26], [94mLoss[0m : 3.20028
[1mStep[0m  [16/26], [94mLoss[0m : 3.28081
[1mStep[0m  [18/26], [94mLoss[0m : 3.27939
[1mStep[0m  [20/26], [94mLoss[0m : 3.06825
[1mStep[0m  [22/26], [94mLoss[0m : 3.29710
[1mStep[0m  [24/26], [94mLoss[0m : 3.33601

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.398, [92mTest[0m: 4.155, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.26049
[1mStep[0m  [2/26], [94mLoss[0m : 3.14458
[1mStep[0m  [4/26], [94mLoss[0m : 3.03187
[1mStep[0m  [6/26], [94mLoss[0m : 3.09761
[1mStep[0m  [8/26], [94mLoss[0m : 2.94379
[1mStep[0m  [10/26], [94mLoss[0m : 3.07129
[1mStep[0m  [12/26], [94mLoss[0m : 3.02040
[1mStep[0m  [14/26], [94mLoss[0m : 2.86517
[1mStep[0m  [16/26], [94mLoss[0m : 2.95077
[1mStep[0m  [18/26], [94mLoss[0m : 2.95137
[1mStep[0m  [20/26], [94mLoss[0m : 2.73779
[1mStep[0m  [22/26], [94mLoss[0m : 2.84162
[1mStep[0m  [24/26], [94mLoss[0m : 2.75089

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.980, [92mTest[0m: 3.542, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.95360
[1mStep[0m  [2/26], [94mLoss[0m : 3.01763
[1mStep[0m  [4/26], [94mLoss[0m : 2.91623
[1mStep[0m  [6/26], [94mLoss[0m : 2.99415
[1mStep[0m  [8/26], [94mLoss[0m : 3.01368
[1mStep[0m  [10/26], [94mLoss[0m : 2.95072
[1mStep[0m  [12/26], [94mLoss[0m : 2.59146
[1mStep[0m  [14/26], [94mLoss[0m : 2.72957
[1mStep[0m  [16/26], [94mLoss[0m : 2.67665
[1mStep[0m  [18/26], [94mLoss[0m : 2.91058
[1mStep[0m  [20/26], [94mLoss[0m : 2.79563
[1mStep[0m  [22/26], [94mLoss[0m : 2.66477
[1mStep[0m  [24/26], [94mLoss[0m : 2.76136

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.831, [92mTest[0m: 3.081, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.81265
[1mStep[0m  [2/26], [94mLoss[0m : 2.87562
[1mStep[0m  [4/26], [94mLoss[0m : 2.80469
[1mStep[0m  [6/26], [94mLoss[0m : 2.97698
[1mStep[0m  [8/26], [94mLoss[0m : 2.78001
[1mStep[0m  [10/26], [94mLoss[0m : 2.71216
[1mStep[0m  [12/26], [94mLoss[0m : 2.74559
[1mStep[0m  [14/26], [94mLoss[0m : 2.76790
[1mStep[0m  [16/26], [94mLoss[0m : 2.81275
[1mStep[0m  [18/26], [94mLoss[0m : 2.83571
[1mStep[0m  [20/26], [94mLoss[0m : 2.62786
[1mStep[0m  [22/26], [94mLoss[0m : 2.68555
[1mStep[0m  [24/26], [94mLoss[0m : 2.71798

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.841, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67580
[1mStep[0m  [2/26], [94mLoss[0m : 2.63292
[1mStep[0m  [4/26], [94mLoss[0m : 2.80956
[1mStep[0m  [6/26], [94mLoss[0m : 2.60125
[1mStep[0m  [8/26], [94mLoss[0m : 2.46852
[1mStep[0m  [10/26], [94mLoss[0m : 2.78234
[1mStep[0m  [12/26], [94mLoss[0m : 2.71406
[1mStep[0m  [14/26], [94mLoss[0m : 2.93094
[1mStep[0m  [16/26], [94mLoss[0m : 2.73792
[1mStep[0m  [18/26], [94mLoss[0m : 2.59320
[1mStep[0m  [20/26], [94mLoss[0m : 2.74759
[1mStep[0m  [22/26], [94mLoss[0m : 2.82924
[1mStep[0m  [24/26], [94mLoss[0m : 2.62828

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.704, [92mTest[0m: 2.741, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.73225
[1mStep[0m  [2/26], [94mLoss[0m : 2.69922
[1mStep[0m  [4/26], [94mLoss[0m : 2.66838
[1mStep[0m  [6/26], [94mLoss[0m : 2.72644
[1mStep[0m  [8/26], [94mLoss[0m : 2.74477
[1mStep[0m  [10/26], [94mLoss[0m : 2.60826
[1mStep[0m  [12/26], [94mLoss[0m : 2.63704
[1mStep[0m  [14/26], [94mLoss[0m : 2.61936
[1mStep[0m  [16/26], [94mLoss[0m : 2.49616
[1mStep[0m  [18/26], [94mLoss[0m : 2.76843
[1mStep[0m  [20/26], [94mLoss[0m : 2.60589
[1mStep[0m  [22/26], [94mLoss[0m : 2.63743
[1mStep[0m  [24/26], [94mLoss[0m : 2.73057

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.631, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54732
[1mStep[0m  [2/26], [94mLoss[0m : 2.59510
[1mStep[0m  [4/26], [94mLoss[0m : 2.73651
[1mStep[0m  [6/26], [94mLoss[0m : 2.65555
[1mStep[0m  [8/26], [94mLoss[0m : 2.78648
[1mStep[0m  [10/26], [94mLoss[0m : 2.67730
[1mStep[0m  [12/26], [94mLoss[0m : 2.68219
[1mStep[0m  [14/26], [94mLoss[0m : 2.70733
[1mStep[0m  [16/26], [94mLoss[0m : 2.68950
[1mStep[0m  [18/26], [94mLoss[0m : 2.55203
[1mStep[0m  [20/26], [94mLoss[0m : 2.82267
[1mStep[0m  [22/26], [94mLoss[0m : 2.55487
[1mStep[0m  [24/26], [94mLoss[0m : 2.80064

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.587, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74070
[1mStep[0m  [2/26], [94mLoss[0m : 2.61059
[1mStep[0m  [4/26], [94mLoss[0m : 2.72040
[1mStep[0m  [6/26], [94mLoss[0m : 2.77765
[1mStep[0m  [8/26], [94mLoss[0m : 2.72228
[1mStep[0m  [10/26], [94mLoss[0m : 2.66155
[1mStep[0m  [12/26], [94mLoss[0m : 2.57659
[1mStep[0m  [14/26], [94mLoss[0m : 2.72935
[1mStep[0m  [16/26], [94mLoss[0m : 2.70129
[1mStep[0m  [18/26], [94mLoss[0m : 2.52318
[1mStep[0m  [20/26], [94mLoss[0m : 2.81834
[1mStep[0m  [22/26], [94mLoss[0m : 2.59002
[1mStep[0m  [24/26], [94mLoss[0m : 2.72743

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.569, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58387
[1mStep[0m  [2/26], [94mLoss[0m : 2.68606
[1mStep[0m  [4/26], [94mLoss[0m : 2.78114
[1mStep[0m  [6/26], [94mLoss[0m : 2.62776
[1mStep[0m  [8/26], [94mLoss[0m : 2.64007
[1mStep[0m  [10/26], [94mLoss[0m : 2.83363
[1mStep[0m  [12/26], [94mLoss[0m : 2.70656
[1mStep[0m  [14/26], [94mLoss[0m : 2.51606
[1mStep[0m  [16/26], [94mLoss[0m : 2.70020
[1mStep[0m  [18/26], [94mLoss[0m : 2.56460
[1mStep[0m  [20/26], [94mLoss[0m : 2.52103
[1mStep[0m  [22/26], [94mLoss[0m : 2.72902
[1mStep[0m  [24/26], [94mLoss[0m : 2.58923

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.526, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58254
[1mStep[0m  [2/26], [94mLoss[0m : 2.51204
[1mStep[0m  [4/26], [94mLoss[0m : 2.66853
[1mStep[0m  [6/26], [94mLoss[0m : 2.71290
[1mStep[0m  [8/26], [94mLoss[0m : 2.58761
[1mStep[0m  [10/26], [94mLoss[0m : 2.59227
[1mStep[0m  [12/26], [94mLoss[0m : 2.51179
[1mStep[0m  [14/26], [94mLoss[0m : 2.76900
[1mStep[0m  [16/26], [94mLoss[0m : 2.79242
[1mStep[0m  [18/26], [94mLoss[0m : 2.54516
[1mStep[0m  [20/26], [94mLoss[0m : 2.55896
[1mStep[0m  [22/26], [94mLoss[0m : 2.71236
[1mStep[0m  [24/26], [94mLoss[0m : 2.60299

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69009
[1mStep[0m  [2/26], [94mLoss[0m : 2.64274
[1mStep[0m  [4/26], [94mLoss[0m : 2.70475
[1mStep[0m  [6/26], [94mLoss[0m : 2.69750
[1mStep[0m  [8/26], [94mLoss[0m : 2.67977
[1mStep[0m  [10/26], [94mLoss[0m : 2.68657
[1mStep[0m  [12/26], [94mLoss[0m : 2.56654
[1mStep[0m  [14/26], [94mLoss[0m : 2.69691
[1mStep[0m  [16/26], [94mLoss[0m : 2.50491
[1mStep[0m  [18/26], [94mLoss[0m : 2.69442
[1mStep[0m  [20/26], [94mLoss[0m : 2.62641
[1mStep[0m  [22/26], [94mLoss[0m : 2.71594
[1mStep[0m  [24/26], [94mLoss[0m : 2.63294

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56233
[1mStep[0m  [2/26], [94mLoss[0m : 2.65022
[1mStep[0m  [4/26], [94mLoss[0m : 2.55520
[1mStep[0m  [6/26], [94mLoss[0m : 2.64565
[1mStep[0m  [8/26], [94mLoss[0m : 2.66191
[1mStep[0m  [10/26], [94mLoss[0m : 2.57190
[1mStep[0m  [12/26], [94mLoss[0m : 2.48093
[1mStep[0m  [14/26], [94mLoss[0m : 2.79819
[1mStep[0m  [16/26], [94mLoss[0m : 2.65437
[1mStep[0m  [18/26], [94mLoss[0m : 2.64668
[1mStep[0m  [20/26], [94mLoss[0m : 2.73019
[1mStep[0m  [22/26], [94mLoss[0m : 2.59250
[1mStep[0m  [24/26], [94mLoss[0m : 2.49767

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59595
[1mStep[0m  [2/26], [94mLoss[0m : 2.59539
[1mStep[0m  [4/26], [94mLoss[0m : 2.47570
[1mStep[0m  [6/26], [94mLoss[0m : 2.51440
[1mStep[0m  [8/26], [94mLoss[0m : 2.49644
[1mStep[0m  [10/26], [94mLoss[0m : 2.52072
[1mStep[0m  [12/26], [94mLoss[0m : 2.63802
[1mStep[0m  [14/26], [94mLoss[0m : 2.46271
[1mStep[0m  [16/26], [94mLoss[0m : 2.68116
[1mStep[0m  [18/26], [94mLoss[0m : 2.64793
[1mStep[0m  [20/26], [94mLoss[0m : 2.66888
[1mStep[0m  [22/26], [94mLoss[0m : 2.58667
[1mStep[0m  [24/26], [94mLoss[0m : 2.75601

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75250
[1mStep[0m  [2/26], [94mLoss[0m : 2.49356
[1mStep[0m  [4/26], [94mLoss[0m : 2.55003
[1mStep[0m  [6/26], [94mLoss[0m : 2.73362
[1mStep[0m  [8/26], [94mLoss[0m : 2.57266
[1mStep[0m  [10/26], [94mLoss[0m : 2.69259
[1mStep[0m  [12/26], [94mLoss[0m : 2.54656
[1mStep[0m  [14/26], [94mLoss[0m : 2.57490
[1mStep[0m  [16/26], [94mLoss[0m : 2.49512
[1mStep[0m  [18/26], [94mLoss[0m : 2.54197
[1mStep[0m  [20/26], [94mLoss[0m : 2.53677
[1mStep[0m  [22/26], [94mLoss[0m : 2.71278
[1mStep[0m  [24/26], [94mLoss[0m : 2.67496

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54496
[1mStep[0m  [2/26], [94mLoss[0m : 2.58683
[1mStep[0m  [4/26], [94mLoss[0m : 2.60163
[1mStep[0m  [6/26], [94mLoss[0m : 2.65381
[1mStep[0m  [8/26], [94mLoss[0m : 2.64271
[1mStep[0m  [10/26], [94mLoss[0m : 2.74243
[1mStep[0m  [12/26], [94mLoss[0m : 2.71394
[1mStep[0m  [14/26], [94mLoss[0m : 2.69441
[1mStep[0m  [16/26], [94mLoss[0m : 2.67130
[1mStep[0m  [18/26], [94mLoss[0m : 2.47085
[1mStep[0m  [20/26], [94mLoss[0m : 2.65376
[1mStep[0m  [22/26], [94mLoss[0m : 2.46420
[1mStep[0m  [24/26], [94mLoss[0m : 2.56658

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57317
[1mStep[0m  [2/26], [94mLoss[0m : 2.53104
[1mStep[0m  [4/26], [94mLoss[0m : 2.52777
[1mStep[0m  [6/26], [94mLoss[0m : 2.47817
[1mStep[0m  [8/26], [94mLoss[0m : 2.57235
[1mStep[0m  [10/26], [94mLoss[0m : 2.67446
[1mStep[0m  [12/26], [94mLoss[0m : 2.63279
[1mStep[0m  [14/26], [94mLoss[0m : 2.67867
[1mStep[0m  [16/26], [94mLoss[0m : 2.65941
[1mStep[0m  [18/26], [94mLoss[0m : 2.61128
[1mStep[0m  [20/26], [94mLoss[0m : 2.60075
[1mStep[0m  [22/26], [94mLoss[0m : 2.64892
[1mStep[0m  [24/26], [94mLoss[0m : 2.56008

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64742
[1mStep[0m  [2/26], [94mLoss[0m : 2.64823
[1mStep[0m  [4/26], [94mLoss[0m : 2.60848
[1mStep[0m  [6/26], [94mLoss[0m : 2.52686
[1mStep[0m  [8/26], [94mLoss[0m : 2.46355
[1mStep[0m  [10/26], [94mLoss[0m : 2.69454
[1mStep[0m  [12/26], [94mLoss[0m : 2.61068
[1mStep[0m  [14/26], [94mLoss[0m : 2.63010
[1mStep[0m  [16/26], [94mLoss[0m : 2.55909
[1mStep[0m  [18/26], [94mLoss[0m : 2.40814
[1mStep[0m  [20/26], [94mLoss[0m : 2.63504
[1mStep[0m  [22/26], [94mLoss[0m : 2.53700
[1mStep[0m  [24/26], [94mLoss[0m : 2.59877

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62149
[1mStep[0m  [2/26], [94mLoss[0m : 2.71693
[1mStep[0m  [4/26], [94mLoss[0m : 2.58732
[1mStep[0m  [6/26], [94mLoss[0m : 2.43635
[1mStep[0m  [8/26], [94mLoss[0m : 2.59306
[1mStep[0m  [10/26], [94mLoss[0m : 2.59179
[1mStep[0m  [12/26], [94mLoss[0m : 2.58928
[1mStep[0m  [14/26], [94mLoss[0m : 2.57818
[1mStep[0m  [16/26], [94mLoss[0m : 2.62839
[1mStep[0m  [18/26], [94mLoss[0m : 2.60663
[1mStep[0m  [20/26], [94mLoss[0m : 2.49089
[1mStep[0m  [22/26], [94mLoss[0m : 2.42461
[1mStep[0m  [24/26], [94mLoss[0m : 2.54552

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50553
[1mStep[0m  [2/26], [94mLoss[0m : 2.22883
[1mStep[0m  [4/26], [94mLoss[0m : 2.52515
[1mStep[0m  [6/26], [94mLoss[0m : 2.65466
[1mStep[0m  [8/26], [94mLoss[0m : 2.63333
[1mStep[0m  [10/26], [94mLoss[0m : 2.68208
[1mStep[0m  [12/26], [94mLoss[0m : 2.56419
[1mStep[0m  [14/26], [94mLoss[0m : 2.53014
[1mStep[0m  [16/26], [94mLoss[0m : 2.56568
[1mStep[0m  [18/26], [94mLoss[0m : 2.46703
[1mStep[0m  [20/26], [94mLoss[0m : 2.57274
[1mStep[0m  [22/26], [94mLoss[0m : 2.57483
[1mStep[0m  [24/26], [94mLoss[0m : 2.51847

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.466, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75232
[1mStep[0m  [2/26], [94mLoss[0m : 2.68515
[1mStep[0m  [4/26], [94mLoss[0m : 2.47056
[1mStep[0m  [6/26], [94mLoss[0m : 2.63880
[1mStep[0m  [8/26], [94mLoss[0m : 2.67568
[1mStep[0m  [10/26], [94mLoss[0m : 2.62355
[1mStep[0m  [12/26], [94mLoss[0m : 2.38536
[1mStep[0m  [14/26], [94mLoss[0m : 2.55184
[1mStep[0m  [16/26], [94mLoss[0m : 2.48053
[1mStep[0m  [18/26], [94mLoss[0m : 2.56149
[1mStep[0m  [20/26], [94mLoss[0m : 2.51651
[1mStep[0m  [22/26], [94mLoss[0m : 2.65010
[1mStep[0m  [24/26], [94mLoss[0m : 2.58011

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47220
[1mStep[0m  [2/26], [94mLoss[0m : 2.64121
[1mStep[0m  [4/26], [94mLoss[0m : 2.48819
[1mStep[0m  [6/26], [94mLoss[0m : 2.57209
[1mStep[0m  [8/26], [94mLoss[0m : 2.45825
[1mStep[0m  [10/26], [94mLoss[0m : 2.63621
[1mStep[0m  [12/26], [94mLoss[0m : 2.58726
[1mStep[0m  [14/26], [94mLoss[0m : 2.49791
[1mStep[0m  [16/26], [94mLoss[0m : 2.59790
[1mStep[0m  [18/26], [94mLoss[0m : 2.49680
[1mStep[0m  [20/26], [94mLoss[0m : 2.58817
[1mStep[0m  [22/26], [94mLoss[0m : 2.44593
[1mStep[0m  [24/26], [94mLoss[0m : 2.55259

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63823
[1mStep[0m  [2/26], [94mLoss[0m : 2.70100
[1mStep[0m  [4/26], [94mLoss[0m : 2.56259
[1mStep[0m  [6/26], [94mLoss[0m : 2.63503
[1mStep[0m  [8/26], [94mLoss[0m : 2.52022
[1mStep[0m  [10/26], [94mLoss[0m : 2.50534
[1mStep[0m  [12/26], [94mLoss[0m : 2.67835
[1mStep[0m  [14/26], [94mLoss[0m : 2.62849
[1mStep[0m  [16/26], [94mLoss[0m : 2.44801
[1mStep[0m  [18/26], [94mLoss[0m : 2.65104
[1mStep[0m  [20/26], [94mLoss[0m : 2.51748
[1mStep[0m  [22/26], [94mLoss[0m : 2.60912
[1mStep[0m  [24/26], [94mLoss[0m : 2.46874

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49025
[1mStep[0m  [2/26], [94mLoss[0m : 2.66354
[1mStep[0m  [4/26], [94mLoss[0m : 2.47133
[1mStep[0m  [6/26], [94mLoss[0m : 2.61098
[1mStep[0m  [8/26], [94mLoss[0m : 2.49076
[1mStep[0m  [10/26], [94mLoss[0m : 2.56347
[1mStep[0m  [12/26], [94mLoss[0m : 2.59367
[1mStep[0m  [14/26], [94mLoss[0m : 2.60656
[1mStep[0m  [16/26], [94mLoss[0m : 2.51623
[1mStep[0m  [18/26], [94mLoss[0m : 2.54136
[1mStep[0m  [20/26], [94mLoss[0m : 2.64102
[1mStep[0m  [22/26], [94mLoss[0m : 2.57090
[1mStep[0m  [24/26], [94mLoss[0m : 2.64710

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61399
[1mStep[0m  [2/26], [94mLoss[0m : 2.60753
[1mStep[0m  [4/26], [94mLoss[0m : 2.68510
[1mStep[0m  [6/26], [94mLoss[0m : 2.42792
[1mStep[0m  [8/26], [94mLoss[0m : 2.46429
[1mStep[0m  [10/26], [94mLoss[0m : 2.54658
[1mStep[0m  [12/26], [94mLoss[0m : 2.47975
[1mStep[0m  [14/26], [94mLoss[0m : 2.63940
[1mStep[0m  [16/26], [94mLoss[0m : 2.52877
[1mStep[0m  [18/26], [94mLoss[0m : 2.66781
[1mStep[0m  [20/26], [94mLoss[0m : 2.47486
[1mStep[0m  [22/26], [94mLoss[0m : 2.53522
[1mStep[0m  [24/26], [94mLoss[0m : 2.53202

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.428
====================================

Phase 1 - Evaluation MAE:  2.427636531683115
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.49238
[1mStep[0m  [2/26], [94mLoss[0m : 2.72022
[1mStep[0m  [4/26], [94mLoss[0m : 2.92495
[1mStep[0m  [6/26], [94mLoss[0m : 2.59884
[1mStep[0m  [8/26], [94mLoss[0m : 2.61926
[1mStep[0m  [10/26], [94mLoss[0m : 2.56375
[1mStep[0m  [12/26], [94mLoss[0m : 2.71073
[1mStep[0m  [14/26], [94mLoss[0m : 2.59267
[1mStep[0m  [16/26], [94mLoss[0m : 2.60681
[1mStep[0m  [18/26], [94mLoss[0m : 2.54694
[1mStep[0m  [20/26], [94mLoss[0m : 2.61684
[1mStep[0m  [22/26], [94mLoss[0m : 2.57956
[1mStep[0m  [24/26], [94mLoss[0m : 2.45733

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.73205
[1mStep[0m  [2/26], [94mLoss[0m : 2.55495
[1mStep[0m  [4/26], [94mLoss[0m : 2.63799
[1mStep[0m  [6/26], [94mLoss[0m : 2.61819
[1mStep[0m  [8/26], [94mLoss[0m : 2.66494
[1mStep[0m  [10/26], [94mLoss[0m : 2.72683
[1mStep[0m  [12/26], [94mLoss[0m : 2.61824
[1mStep[0m  [14/26], [94mLoss[0m : 2.57692
[1mStep[0m  [16/26], [94mLoss[0m : 2.54769
[1mStep[0m  [18/26], [94mLoss[0m : 2.79115
[1mStep[0m  [20/26], [94mLoss[0m : 2.52351
[1mStep[0m  [22/26], [94mLoss[0m : 2.73729
[1mStep[0m  [24/26], [94mLoss[0m : 2.65308

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.662, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62938
[1mStep[0m  [2/26], [94mLoss[0m : 2.70400
[1mStep[0m  [4/26], [94mLoss[0m : 2.63103
[1mStep[0m  [6/26], [94mLoss[0m : 2.47150
[1mStep[0m  [8/26], [94mLoss[0m : 2.59123
[1mStep[0m  [10/26], [94mLoss[0m : 2.53068
[1mStep[0m  [12/26], [94mLoss[0m : 2.50329
[1mStep[0m  [14/26], [94mLoss[0m : 2.60404
[1mStep[0m  [16/26], [94mLoss[0m : 2.52146
[1mStep[0m  [18/26], [94mLoss[0m : 2.59801
[1mStep[0m  [20/26], [94mLoss[0m : 2.56111
[1mStep[0m  [22/26], [94mLoss[0m : 2.57720
[1mStep[0m  [24/26], [94mLoss[0m : 2.73414

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64391
[1mStep[0m  [2/26], [94mLoss[0m : 2.80009
[1mStep[0m  [4/26], [94mLoss[0m : 2.56281
[1mStep[0m  [6/26], [94mLoss[0m : 2.62213
[1mStep[0m  [8/26], [94mLoss[0m : 2.45077
[1mStep[0m  [10/26], [94mLoss[0m : 2.63040
[1mStep[0m  [12/26], [94mLoss[0m : 2.44488
[1mStep[0m  [14/26], [94mLoss[0m : 2.58423
[1mStep[0m  [16/26], [94mLoss[0m : 2.73078
[1mStep[0m  [18/26], [94mLoss[0m : 2.58033
[1mStep[0m  [20/26], [94mLoss[0m : 2.57609
[1mStep[0m  [22/26], [94mLoss[0m : 2.47759
[1mStep[0m  [24/26], [94mLoss[0m : 2.67012

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.558, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58158
[1mStep[0m  [2/26], [94mLoss[0m : 2.54425
[1mStep[0m  [4/26], [94mLoss[0m : 2.63943
[1mStep[0m  [6/26], [94mLoss[0m : 2.54051
[1mStep[0m  [8/26], [94mLoss[0m : 2.48916
[1mStep[0m  [10/26], [94mLoss[0m : 2.59461
[1mStep[0m  [12/26], [94mLoss[0m : 2.53195
[1mStep[0m  [14/26], [94mLoss[0m : 2.56229
[1mStep[0m  [16/26], [94mLoss[0m : 2.70125
[1mStep[0m  [18/26], [94mLoss[0m : 2.55057
[1mStep[0m  [20/26], [94mLoss[0m : 2.71682
[1mStep[0m  [22/26], [94mLoss[0m : 2.71423
[1mStep[0m  [24/26], [94mLoss[0m : 2.48505

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61239
[1mStep[0m  [2/26], [94mLoss[0m : 2.45439
[1mStep[0m  [4/26], [94mLoss[0m : 2.48548
[1mStep[0m  [6/26], [94mLoss[0m : 2.59963
[1mStep[0m  [8/26], [94mLoss[0m : 2.66737
[1mStep[0m  [10/26], [94mLoss[0m : 2.59434
[1mStep[0m  [12/26], [94mLoss[0m : 2.58764
[1mStep[0m  [14/26], [94mLoss[0m : 2.57154
[1mStep[0m  [16/26], [94mLoss[0m : 2.60695
[1mStep[0m  [18/26], [94mLoss[0m : 2.53389
[1mStep[0m  [20/26], [94mLoss[0m : 2.71093
[1mStep[0m  [22/26], [94mLoss[0m : 2.44394
[1mStep[0m  [24/26], [94mLoss[0m : 2.41223

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.543, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68349
[1mStep[0m  [2/26], [94mLoss[0m : 2.53259
[1mStep[0m  [4/26], [94mLoss[0m : 2.54884
[1mStep[0m  [6/26], [94mLoss[0m : 2.44612
[1mStep[0m  [8/26], [94mLoss[0m : 2.61403
[1mStep[0m  [10/26], [94mLoss[0m : 2.60311
[1mStep[0m  [12/26], [94mLoss[0m : 2.47014
[1mStep[0m  [14/26], [94mLoss[0m : 2.44015
[1mStep[0m  [16/26], [94mLoss[0m : 2.62781
[1mStep[0m  [18/26], [94mLoss[0m : 2.59898
[1mStep[0m  [20/26], [94mLoss[0m : 2.52081
[1mStep[0m  [22/26], [94mLoss[0m : 2.60560
[1mStep[0m  [24/26], [94mLoss[0m : 2.61053

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.534, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54391
[1mStep[0m  [2/26], [94mLoss[0m : 2.69638
[1mStep[0m  [4/26], [94mLoss[0m : 2.68476
[1mStep[0m  [6/26], [94mLoss[0m : 2.59843
[1mStep[0m  [8/26], [94mLoss[0m : 2.58195
[1mStep[0m  [10/26], [94mLoss[0m : 2.50307
[1mStep[0m  [12/26], [94mLoss[0m : 2.52538
[1mStep[0m  [14/26], [94mLoss[0m : 2.42312
[1mStep[0m  [16/26], [94mLoss[0m : 2.49900
[1mStep[0m  [18/26], [94mLoss[0m : 2.41058
[1mStep[0m  [20/26], [94mLoss[0m : 2.49207
[1mStep[0m  [22/26], [94mLoss[0m : 2.52877
[1mStep[0m  [24/26], [94mLoss[0m : 2.50724

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.568, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41839
[1mStep[0m  [2/26], [94mLoss[0m : 2.73647
[1mStep[0m  [4/26], [94mLoss[0m : 2.40334
[1mStep[0m  [6/26], [94mLoss[0m : 2.43863
[1mStep[0m  [8/26], [94mLoss[0m : 2.53409
[1mStep[0m  [10/26], [94mLoss[0m : 2.48283
[1mStep[0m  [12/26], [94mLoss[0m : 2.55049
[1mStep[0m  [14/26], [94mLoss[0m : 2.59030
[1mStep[0m  [16/26], [94mLoss[0m : 2.53300
[1mStep[0m  [18/26], [94mLoss[0m : 2.56916
[1mStep[0m  [20/26], [94mLoss[0m : 2.52955
[1mStep[0m  [22/26], [94mLoss[0m : 2.42017
[1mStep[0m  [24/26], [94mLoss[0m : 2.57669

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53605
[1mStep[0m  [2/26], [94mLoss[0m : 2.42572
[1mStep[0m  [4/26], [94mLoss[0m : 2.44224
[1mStep[0m  [6/26], [94mLoss[0m : 2.45495
[1mStep[0m  [8/26], [94mLoss[0m : 2.59714
[1mStep[0m  [10/26], [94mLoss[0m : 2.61350
[1mStep[0m  [12/26], [94mLoss[0m : 2.46954
[1mStep[0m  [14/26], [94mLoss[0m : 2.57364
[1mStep[0m  [16/26], [94mLoss[0m : 2.50656
[1mStep[0m  [18/26], [94mLoss[0m : 2.61372
[1mStep[0m  [20/26], [94mLoss[0m : 2.72758
[1mStep[0m  [22/26], [94mLoss[0m : 2.38305
[1mStep[0m  [24/26], [94mLoss[0m : 2.65988

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43306
[1mStep[0m  [2/26], [94mLoss[0m : 2.48186
[1mStep[0m  [4/26], [94mLoss[0m : 2.50160
[1mStep[0m  [6/26], [94mLoss[0m : 2.36007
[1mStep[0m  [8/26], [94mLoss[0m : 2.60822
[1mStep[0m  [10/26], [94mLoss[0m : 2.58396
[1mStep[0m  [12/26], [94mLoss[0m : 2.46888
[1mStep[0m  [14/26], [94mLoss[0m : 2.49820
[1mStep[0m  [16/26], [94mLoss[0m : 2.36705
[1mStep[0m  [18/26], [94mLoss[0m : 2.57562
[1mStep[0m  [20/26], [94mLoss[0m : 2.46615
[1mStep[0m  [22/26], [94mLoss[0m : 2.58734
[1mStep[0m  [24/26], [94mLoss[0m : 2.32882

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51193
[1mStep[0m  [2/26], [94mLoss[0m : 2.54239
[1mStep[0m  [4/26], [94mLoss[0m : 2.59877
[1mStep[0m  [6/26], [94mLoss[0m : 2.49738
[1mStep[0m  [8/26], [94mLoss[0m : 2.54966
[1mStep[0m  [10/26], [94mLoss[0m : 2.59665
[1mStep[0m  [12/26], [94mLoss[0m : 2.48783
[1mStep[0m  [14/26], [94mLoss[0m : 2.60215
[1mStep[0m  [16/26], [94mLoss[0m : 2.49767
[1mStep[0m  [18/26], [94mLoss[0m : 2.53303
[1mStep[0m  [20/26], [94mLoss[0m : 2.53121
[1mStep[0m  [22/26], [94mLoss[0m : 2.45096
[1mStep[0m  [24/26], [94mLoss[0m : 2.27356

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48433
[1mStep[0m  [2/26], [94mLoss[0m : 2.26361
[1mStep[0m  [4/26], [94mLoss[0m : 2.46050
[1mStep[0m  [6/26], [94mLoss[0m : 2.54853
[1mStep[0m  [8/26], [94mLoss[0m : 2.37905
[1mStep[0m  [10/26], [94mLoss[0m : 2.51068
[1mStep[0m  [12/26], [94mLoss[0m : 2.34803
[1mStep[0m  [14/26], [94mLoss[0m : 2.58771
[1mStep[0m  [16/26], [94mLoss[0m : 2.65964
[1mStep[0m  [18/26], [94mLoss[0m : 2.51897
[1mStep[0m  [20/26], [94mLoss[0m : 2.50580
[1mStep[0m  [22/26], [94mLoss[0m : 2.40554
[1mStep[0m  [24/26], [94mLoss[0m : 2.48714

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46352
[1mStep[0m  [2/26], [94mLoss[0m : 2.35349
[1mStep[0m  [4/26], [94mLoss[0m : 2.53445
[1mStep[0m  [6/26], [94mLoss[0m : 2.45164
[1mStep[0m  [8/26], [94mLoss[0m : 2.57710
[1mStep[0m  [10/26], [94mLoss[0m : 2.35631
[1mStep[0m  [12/26], [94mLoss[0m : 2.44862
[1mStep[0m  [14/26], [94mLoss[0m : 2.47161
[1mStep[0m  [16/26], [94mLoss[0m : 2.46495
[1mStep[0m  [18/26], [94mLoss[0m : 2.44623
[1mStep[0m  [20/26], [94mLoss[0m : 2.47818
[1mStep[0m  [22/26], [94mLoss[0m : 2.44300
[1mStep[0m  [24/26], [94mLoss[0m : 2.44033

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47691
[1mStep[0m  [2/26], [94mLoss[0m : 2.40189
[1mStep[0m  [4/26], [94mLoss[0m : 2.36261
[1mStep[0m  [6/26], [94mLoss[0m : 2.46778
[1mStep[0m  [8/26], [94mLoss[0m : 2.34262
[1mStep[0m  [10/26], [94mLoss[0m : 2.44778
[1mStep[0m  [12/26], [94mLoss[0m : 2.41950
[1mStep[0m  [14/26], [94mLoss[0m : 2.41419
[1mStep[0m  [16/26], [94mLoss[0m : 2.42076
[1mStep[0m  [18/26], [94mLoss[0m : 2.53485
[1mStep[0m  [20/26], [94mLoss[0m : 2.51466
[1mStep[0m  [22/26], [94mLoss[0m : 2.39473
[1mStep[0m  [24/26], [94mLoss[0m : 2.47768

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55620
[1mStep[0m  [2/26], [94mLoss[0m : 2.34293
[1mStep[0m  [4/26], [94mLoss[0m : 2.45717
[1mStep[0m  [6/26], [94mLoss[0m : 2.36244
[1mStep[0m  [8/26], [94mLoss[0m : 2.39024
[1mStep[0m  [10/26], [94mLoss[0m : 2.43373
[1mStep[0m  [12/26], [94mLoss[0m : 2.57822
[1mStep[0m  [14/26], [94mLoss[0m : 2.40664
[1mStep[0m  [16/26], [94mLoss[0m : 2.42183
[1mStep[0m  [18/26], [94mLoss[0m : 2.48313
[1mStep[0m  [20/26], [94mLoss[0m : 2.50298
[1mStep[0m  [22/26], [94mLoss[0m : 2.42448
[1mStep[0m  [24/26], [94mLoss[0m : 2.38568

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45336
[1mStep[0m  [2/26], [94mLoss[0m : 2.42961
[1mStep[0m  [4/26], [94mLoss[0m : 2.47200
[1mStep[0m  [6/26], [94mLoss[0m : 2.28418
[1mStep[0m  [8/26], [94mLoss[0m : 2.26175
[1mStep[0m  [10/26], [94mLoss[0m : 2.27341
[1mStep[0m  [12/26], [94mLoss[0m : 2.47266
[1mStep[0m  [14/26], [94mLoss[0m : 2.52409
[1mStep[0m  [16/26], [94mLoss[0m : 2.40193
[1mStep[0m  [18/26], [94mLoss[0m : 2.48356
[1mStep[0m  [20/26], [94mLoss[0m : 2.43258
[1mStep[0m  [22/26], [94mLoss[0m : 2.40022
[1mStep[0m  [24/26], [94mLoss[0m : 2.38152

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48209
[1mStep[0m  [2/26], [94mLoss[0m : 2.37312
[1mStep[0m  [4/26], [94mLoss[0m : 2.41745
[1mStep[0m  [6/26], [94mLoss[0m : 2.39740
[1mStep[0m  [8/26], [94mLoss[0m : 2.39341
[1mStep[0m  [10/26], [94mLoss[0m : 2.39885
[1mStep[0m  [12/26], [94mLoss[0m : 2.46482
[1mStep[0m  [14/26], [94mLoss[0m : 2.42505
[1mStep[0m  [16/26], [94mLoss[0m : 2.48054
[1mStep[0m  [18/26], [94mLoss[0m : 2.41943
[1mStep[0m  [20/26], [94mLoss[0m : 2.30661
[1mStep[0m  [22/26], [94mLoss[0m : 2.47539
[1mStep[0m  [24/26], [94mLoss[0m : 2.46010

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30050
[1mStep[0m  [2/26], [94mLoss[0m : 2.66523
[1mStep[0m  [4/26], [94mLoss[0m : 2.45479
[1mStep[0m  [6/26], [94mLoss[0m : 2.36193
[1mStep[0m  [8/26], [94mLoss[0m : 2.38223
[1mStep[0m  [10/26], [94mLoss[0m : 2.67075
[1mStep[0m  [12/26], [94mLoss[0m : 2.34200
[1mStep[0m  [14/26], [94mLoss[0m : 2.32931
[1mStep[0m  [16/26], [94mLoss[0m : 2.49169
[1mStep[0m  [18/26], [94mLoss[0m : 2.35902
[1mStep[0m  [20/26], [94mLoss[0m : 2.40075
[1mStep[0m  [22/26], [94mLoss[0m : 2.27758
[1mStep[0m  [24/26], [94mLoss[0m : 2.36627

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29152
[1mStep[0m  [2/26], [94mLoss[0m : 2.28825
[1mStep[0m  [4/26], [94mLoss[0m : 2.46416
[1mStep[0m  [6/26], [94mLoss[0m : 2.48478
[1mStep[0m  [8/26], [94mLoss[0m : 2.37536
[1mStep[0m  [10/26], [94mLoss[0m : 2.58932
[1mStep[0m  [12/26], [94mLoss[0m : 2.47842
[1mStep[0m  [14/26], [94mLoss[0m : 2.48720
[1mStep[0m  [16/26], [94mLoss[0m : 2.45047
[1mStep[0m  [18/26], [94mLoss[0m : 2.36874
[1mStep[0m  [20/26], [94mLoss[0m : 2.32075
[1mStep[0m  [22/26], [94mLoss[0m : 2.31201
[1mStep[0m  [24/26], [94mLoss[0m : 2.38792

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.522, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45853
[1mStep[0m  [2/26], [94mLoss[0m : 2.34554
[1mStep[0m  [4/26], [94mLoss[0m : 2.43984
[1mStep[0m  [6/26], [94mLoss[0m : 2.44561
[1mStep[0m  [8/26], [94mLoss[0m : 2.33684
[1mStep[0m  [10/26], [94mLoss[0m : 2.36391
[1mStep[0m  [12/26], [94mLoss[0m : 2.39226
[1mStep[0m  [14/26], [94mLoss[0m : 2.33083
[1mStep[0m  [16/26], [94mLoss[0m : 2.42170
[1mStep[0m  [18/26], [94mLoss[0m : 2.18444
[1mStep[0m  [20/26], [94mLoss[0m : 2.32572
[1mStep[0m  [22/26], [94mLoss[0m : 2.43253
[1mStep[0m  [24/26], [94mLoss[0m : 2.45664

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44848
[1mStep[0m  [2/26], [94mLoss[0m : 2.42564
[1mStep[0m  [4/26], [94mLoss[0m : 2.31057
[1mStep[0m  [6/26], [94mLoss[0m : 2.36965
[1mStep[0m  [8/26], [94mLoss[0m : 2.35971
[1mStep[0m  [10/26], [94mLoss[0m : 2.28412
[1mStep[0m  [12/26], [94mLoss[0m : 2.53198
[1mStep[0m  [14/26], [94mLoss[0m : 2.34861
[1mStep[0m  [16/26], [94mLoss[0m : 2.36901
[1mStep[0m  [18/26], [94mLoss[0m : 2.40025
[1mStep[0m  [20/26], [94mLoss[0m : 2.29749
[1mStep[0m  [22/26], [94mLoss[0m : 2.40608
[1mStep[0m  [24/26], [94mLoss[0m : 2.28951

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.16575
[1mStep[0m  [2/26], [94mLoss[0m : 2.33839
[1mStep[0m  [4/26], [94mLoss[0m : 2.37575
[1mStep[0m  [6/26], [94mLoss[0m : 2.41329
[1mStep[0m  [8/26], [94mLoss[0m : 2.35797
[1mStep[0m  [10/26], [94mLoss[0m : 2.27151
[1mStep[0m  [12/26], [94mLoss[0m : 2.31138
[1mStep[0m  [14/26], [94mLoss[0m : 2.41903
[1mStep[0m  [16/26], [94mLoss[0m : 2.25829
[1mStep[0m  [18/26], [94mLoss[0m : 2.36457
[1mStep[0m  [20/26], [94mLoss[0m : 2.40376
[1mStep[0m  [22/26], [94mLoss[0m : 2.30451
[1mStep[0m  [24/26], [94mLoss[0m : 2.39246

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28031
[1mStep[0m  [2/26], [94mLoss[0m : 2.53921
[1mStep[0m  [4/26], [94mLoss[0m : 2.30051
[1mStep[0m  [6/26], [94mLoss[0m : 2.33660
[1mStep[0m  [8/26], [94mLoss[0m : 2.42710
[1mStep[0m  [10/26], [94mLoss[0m : 2.47283
[1mStep[0m  [12/26], [94mLoss[0m : 2.33809
[1mStep[0m  [14/26], [94mLoss[0m : 2.34760
[1mStep[0m  [16/26], [94mLoss[0m : 2.29421
[1mStep[0m  [18/26], [94mLoss[0m : 2.39390
[1mStep[0m  [20/26], [94mLoss[0m : 2.39003
[1mStep[0m  [22/26], [94mLoss[0m : 2.29815
[1mStep[0m  [24/26], [94mLoss[0m : 2.34716

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43758
[1mStep[0m  [2/26], [94mLoss[0m : 2.32428
[1mStep[0m  [4/26], [94mLoss[0m : 2.27432
[1mStep[0m  [6/26], [94mLoss[0m : 2.38937
[1mStep[0m  [8/26], [94mLoss[0m : 2.48260
[1mStep[0m  [10/26], [94mLoss[0m : 2.25924
[1mStep[0m  [12/26], [94mLoss[0m : 2.27706
[1mStep[0m  [14/26], [94mLoss[0m : 2.27571
[1mStep[0m  [16/26], [94mLoss[0m : 2.30366
[1mStep[0m  [18/26], [94mLoss[0m : 2.37151
[1mStep[0m  [20/26], [94mLoss[0m : 2.44408
[1mStep[0m  [22/26], [94mLoss[0m : 2.16533
[1mStep[0m  [24/26], [94mLoss[0m : 2.20033

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39443
[1mStep[0m  [2/26], [94mLoss[0m : 2.33985
[1mStep[0m  [4/26], [94mLoss[0m : 2.25559
[1mStep[0m  [6/26], [94mLoss[0m : 2.31060
[1mStep[0m  [8/26], [94mLoss[0m : 2.24237
[1mStep[0m  [10/26], [94mLoss[0m : 2.40955
[1mStep[0m  [12/26], [94mLoss[0m : 2.20881
[1mStep[0m  [14/26], [94mLoss[0m : 2.30104
[1mStep[0m  [16/26], [94mLoss[0m : 2.36519
[1mStep[0m  [18/26], [94mLoss[0m : 2.35366
[1mStep[0m  [20/26], [94mLoss[0m : 2.40345
[1mStep[0m  [22/26], [94mLoss[0m : 2.31414
[1mStep[0m  [24/26], [94mLoss[0m : 2.15415

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.441, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34058
[1mStep[0m  [2/26], [94mLoss[0m : 2.25597
[1mStep[0m  [4/26], [94mLoss[0m : 2.35115
[1mStep[0m  [6/26], [94mLoss[0m : 2.32195
[1mStep[0m  [8/26], [94mLoss[0m : 2.31102
[1mStep[0m  [10/26], [94mLoss[0m : 2.31717
[1mStep[0m  [12/26], [94mLoss[0m : 2.25755
[1mStep[0m  [14/26], [94mLoss[0m : 2.28709
[1mStep[0m  [16/26], [94mLoss[0m : 2.27952
[1mStep[0m  [18/26], [94mLoss[0m : 2.30992
[1mStep[0m  [20/26], [94mLoss[0m : 2.29187
[1mStep[0m  [22/26], [94mLoss[0m : 2.25709
[1mStep[0m  [24/26], [94mLoss[0m : 2.31530

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26126
[1mStep[0m  [2/26], [94mLoss[0m : 2.31654
[1mStep[0m  [4/26], [94mLoss[0m : 2.24850
[1mStep[0m  [6/26], [94mLoss[0m : 2.35466
[1mStep[0m  [8/26], [94mLoss[0m : 2.23640
[1mStep[0m  [10/26], [94mLoss[0m : 2.41177
[1mStep[0m  [12/26], [94mLoss[0m : 2.27716
[1mStep[0m  [14/26], [94mLoss[0m : 2.29703
[1mStep[0m  [16/26], [94mLoss[0m : 2.42304
[1mStep[0m  [18/26], [94mLoss[0m : 2.26549
[1mStep[0m  [20/26], [94mLoss[0m : 2.21154
[1mStep[0m  [22/26], [94mLoss[0m : 2.32080
[1mStep[0m  [24/26], [94mLoss[0m : 2.21087

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37948
[1mStep[0m  [2/26], [94mLoss[0m : 2.29428
[1mStep[0m  [4/26], [94mLoss[0m : 2.22766
[1mStep[0m  [6/26], [94mLoss[0m : 2.28509
[1mStep[0m  [8/26], [94mLoss[0m : 2.26738
[1mStep[0m  [10/26], [94mLoss[0m : 2.47319
[1mStep[0m  [12/26], [94mLoss[0m : 2.19000
[1mStep[0m  [14/26], [94mLoss[0m : 2.26736
[1mStep[0m  [16/26], [94mLoss[0m : 2.28354
[1mStep[0m  [18/26], [94mLoss[0m : 2.15719
[1mStep[0m  [20/26], [94mLoss[0m : 2.34444
[1mStep[0m  [22/26], [94mLoss[0m : 2.25179
[1mStep[0m  [24/26], [94mLoss[0m : 2.40652

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40367
[1mStep[0m  [2/26], [94mLoss[0m : 2.28597
[1mStep[0m  [4/26], [94mLoss[0m : 2.21616
[1mStep[0m  [6/26], [94mLoss[0m : 2.24616
[1mStep[0m  [8/26], [94mLoss[0m : 2.27686
[1mStep[0m  [10/26], [94mLoss[0m : 2.37314
[1mStep[0m  [12/26], [94mLoss[0m : 2.28630
[1mStep[0m  [14/26], [94mLoss[0m : 2.35854
[1mStep[0m  [16/26], [94mLoss[0m : 2.33336
[1mStep[0m  [18/26], [94mLoss[0m : 2.15516
[1mStep[0m  [20/26], [94mLoss[0m : 2.30651
[1mStep[0m  [22/26], [94mLoss[0m : 2.31834
[1mStep[0m  [24/26], [94mLoss[0m : 2.09617

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.406
====================================

Phase 2 - Evaluation MAE:  2.4063999286064734
MAE score P1       2.427637
MAE score P2         2.4064
loss               2.268773
learning_rate       0.00505
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/427], [94mLoss[0m : 10.96347
[1mStep[0m  [42/427], [94mLoss[0m : 5.16674
[1mStep[0m  [84/427], [94mLoss[0m : 2.50740
[1mStep[0m  [126/427], [94mLoss[0m : 2.66527
[1mStep[0m  [168/427], [94mLoss[0m : 2.64189
[1mStep[0m  [210/427], [94mLoss[0m : 3.13797
[1mStep[0m  [252/427], [94mLoss[0m : 3.29816
[1mStep[0m  [294/427], [94mLoss[0m : 2.45110
[1mStep[0m  [336/427], [94mLoss[0m : 2.58305
[1mStep[0m  [378/427], [94mLoss[0m : 2.76940
[1mStep[0m  [420/427], [94mLoss[0m : 3.06937

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.571, [92mTest[0m: 10.858, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.38599
[1mStep[0m  [42/427], [94mLoss[0m : 2.35529
[1mStep[0m  [84/427], [94mLoss[0m : 3.09481
[1mStep[0m  [126/427], [94mLoss[0m : 2.67894
[1mStep[0m  [168/427], [94mLoss[0m : 2.73966
[1mStep[0m  [210/427], [94mLoss[0m : 2.38557
[1mStep[0m  [252/427], [94mLoss[0m : 2.17960
[1mStep[0m  [294/427], [94mLoss[0m : 2.58296
[1mStep[0m  [336/427], [94mLoss[0m : 2.27175
[1mStep[0m  [378/427], [94mLoss[0m : 2.07833
[1mStep[0m  [420/427], [94mLoss[0m : 2.59717

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.07589
[1mStep[0m  [42/427], [94mLoss[0m : 3.15065
[1mStep[0m  [84/427], [94mLoss[0m : 2.58674
[1mStep[0m  [126/427], [94mLoss[0m : 2.79858
[1mStep[0m  [168/427], [94mLoss[0m : 2.80883
[1mStep[0m  [210/427], [94mLoss[0m : 2.81333
[1mStep[0m  [252/427], [94mLoss[0m : 2.72441
[1mStep[0m  [294/427], [94mLoss[0m : 2.60987
[1mStep[0m  [336/427], [94mLoss[0m : 2.69635
[1mStep[0m  [378/427], [94mLoss[0m : 2.24947
[1mStep[0m  [420/427], [94mLoss[0m : 2.54390

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.47145
[1mStep[0m  [42/427], [94mLoss[0m : 2.63748
[1mStep[0m  [84/427], [94mLoss[0m : 2.26433
[1mStep[0m  [126/427], [94mLoss[0m : 2.14690
[1mStep[0m  [168/427], [94mLoss[0m : 2.61632
[1mStep[0m  [210/427], [94mLoss[0m : 2.46332
[1mStep[0m  [252/427], [94mLoss[0m : 2.34527
[1mStep[0m  [294/427], [94mLoss[0m : 2.37088
[1mStep[0m  [336/427], [94mLoss[0m : 2.22219
[1mStep[0m  [378/427], [94mLoss[0m : 2.10633
[1mStep[0m  [420/427], [94mLoss[0m : 2.62721

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.582, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.53046
[1mStep[0m  [42/427], [94mLoss[0m : 2.54596
[1mStep[0m  [84/427], [94mLoss[0m : 2.63361
[1mStep[0m  [126/427], [94mLoss[0m : 2.71381
[1mStep[0m  [168/427], [94mLoss[0m : 1.98605
[1mStep[0m  [210/427], [94mLoss[0m : 2.28241
[1mStep[0m  [252/427], [94mLoss[0m : 3.13904
[1mStep[0m  [294/427], [94mLoss[0m : 2.21152
[1mStep[0m  [336/427], [94mLoss[0m : 2.57671
[1mStep[0m  [378/427], [94mLoss[0m : 3.07257
[1mStep[0m  [420/427], [94mLoss[0m : 2.49220

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.52663
[1mStep[0m  [42/427], [94mLoss[0m : 2.37949
[1mStep[0m  [84/427], [94mLoss[0m : 2.57376
[1mStep[0m  [126/427], [94mLoss[0m : 2.88979
[1mStep[0m  [168/427], [94mLoss[0m : 2.39750
[1mStep[0m  [210/427], [94mLoss[0m : 2.75817
[1mStep[0m  [252/427], [94mLoss[0m : 2.98179
[1mStep[0m  [294/427], [94mLoss[0m : 2.48492
[1mStep[0m  [336/427], [94mLoss[0m : 2.51589
[1mStep[0m  [378/427], [94mLoss[0m : 2.35428
[1mStep[0m  [420/427], [94mLoss[0m : 2.70860

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.44294
[1mStep[0m  [42/427], [94mLoss[0m : 2.00946
[1mStep[0m  [84/427], [94mLoss[0m : 2.39655
[1mStep[0m  [126/427], [94mLoss[0m : 2.44914
[1mStep[0m  [168/427], [94mLoss[0m : 2.23946
[1mStep[0m  [210/427], [94mLoss[0m : 2.21487
[1mStep[0m  [252/427], [94mLoss[0m : 2.43717
[1mStep[0m  [294/427], [94mLoss[0m : 3.00865
[1mStep[0m  [336/427], [94mLoss[0m : 2.87654
[1mStep[0m  [378/427], [94mLoss[0m : 2.39188
[1mStep[0m  [420/427], [94mLoss[0m : 2.41656

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.579, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.03396
[1mStep[0m  [42/427], [94mLoss[0m : 2.77655
[1mStep[0m  [84/427], [94mLoss[0m : 2.43313
[1mStep[0m  [126/427], [94mLoss[0m : 2.81625
[1mStep[0m  [168/427], [94mLoss[0m : 3.05086
[1mStep[0m  [210/427], [94mLoss[0m : 2.63924
[1mStep[0m  [252/427], [94mLoss[0m : 2.89423
[1mStep[0m  [294/427], [94mLoss[0m : 2.09223
[1mStep[0m  [336/427], [94mLoss[0m : 2.78190
[1mStep[0m  [378/427], [94mLoss[0m : 2.11508
[1mStep[0m  [420/427], [94mLoss[0m : 2.94058

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.561, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.20992
[1mStep[0m  [42/427], [94mLoss[0m : 2.14678
[1mStep[0m  [84/427], [94mLoss[0m : 1.78507
[1mStep[0m  [126/427], [94mLoss[0m : 2.04491
[1mStep[0m  [168/427], [94mLoss[0m : 2.55646
[1mStep[0m  [210/427], [94mLoss[0m : 3.23835
[1mStep[0m  [252/427], [94mLoss[0m : 2.19556
[1mStep[0m  [294/427], [94mLoss[0m : 2.36024
[1mStep[0m  [336/427], [94mLoss[0m : 2.08414
[1mStep[0m  [378/427], [94mLoss[0m : 2.27876
[1mStep[0m  [420/427], [94mLoss[0m : 2.08817

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.56165
[1mStep[0m  [42/427], [94mLoss[0m : 2.28172
[1mStep[0m  [84/427], [94mLoss[0m : 2.56890
[1mStep[0m  [126/427], [94mLoss[0m : 2.29587
[1mStep[0m  [168/427], [94mLoss[0m : 1.89376
[1mStep[0m  [210/427], [94mLoss[0m : 2.36161
[1mStep[0m  [252/427], [94mLoss[0m : 2.37079
[1mStep[0m  [294/427], [94mLoss[0m : 1.91237
[1mStep[0m  [336/427], [94mLoss[0m : 2.58953
[1mStep[0m  [378/427], [94mLoss[0m : 2.63818
[1mStep[0m  [420/427], [94mLoss[0m : 1.97660

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.64342
[1mStep[0m  [42/427], [94mLoss[0m : 2.14106
[1mStep[0m  [84/427], [94mLoss[0m : 2.81561
[1mStep[0m  [126/427], [94mLoss[0m : 2.52356
[1mStep[0m  [168/427], [94mLoss[0m : 2.02799
[1mStep[0m  [210/427], [94mLoss[0m : 2.72410
[1mStep[0m  [252/427], [94mLoss[0m : 2.17216
[1mStep[0m  [294/427], [94mLoss[0m : 2.82550
[1mStep[0m  [336/427], [94mLoss[0m : 2.80617
[1mStep[0m  [378/427], [94mLoss[0m : 2.66824
[1mStep[0m  [420/427], [94mLoss[0m : 2.15867

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.87440
[1mStep[0m  [42/427], [94mLoss[0m : 2.47141
[1mStep[0m  [84/427], [94mLoss[0m : 2.32473
[1mStep[0m  [126/427], [94mLoss[0m : 2.20179
[1mStep[0m  [168/427], [94mLoss[0m : 2.90412
[1mStep[0m  [210/427], [94mLoss[0m : 2.28344
[1mStep[0m  [252/427], [94mLoss[0m : 2.35507
[1mStep[0m  [294/427], [94mLoss[0m : 2.46139
[1mStep[0m  [336/427], [94mLoss[0m : 2.55160
[1mStep[0m  [378/427], [94mLoss[0m : 2.70093
[1mStep[0m  [420/427], [94mLoss[0m : 1.71930

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.76735
[1mStep[0m  [42/427], [94mLoss[0m : 2.85704
[1mStep[0m  [84/427], [94mLoss[0m : 2.74696
[1mStep[0m  [126/427], [94mLoss[0m : 2.38675
[1mStep[0m  [168/427], [94mLoss[0m : 2.21124
[1mStep[0m  [210/427], [94mLoss[0m : 2.35731
[1mStep[0m  [252/427], [94mLoss[0m : 2.62848
[1mStep[0m  [294/427], [94mLoss[0m : 2.81636
[1mStep[0m  [336/427], [94mLoss[0m : 2.77416
[1mStep[0m  [378/427], [94mLoss[0m : 2.69697
[1mStep[0m  [420/427], [94mLoss[0m : 2.82979

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.47022
[1mStep[0m  [42/427], [94mLoss[0m : 2.96948
[1mStep[0m  [84/427], [94mLoss[0m : 2.59441
[1mStep[0m  [126/427], [94mLoss[0m : 2.37686
[1mStep[0m  [168/427], [94mLoss[0m : 1.70820
[1mStep[0m  [210/427], [94mLoss[0m : 2.18970
[1mStep[0m  [252/427], [94mLoss[0m : 2.40156
[1mStep[0m  [294/427], [94mLoss[0m : 3.35222
[1mStep[0m  [336/427], [94mLoss[0m : 2.34317
[1mStep[0m  [378/427], [94mLoss[0m : 1.83460
[1mStep[0m  [420/427], [94mLoss[0m : 2.05146

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.66358
[1mStep[0m  [42/427], [94mLoss[0m : 2.09894
[1mStep[0m  [84/427], [94mLoss[0m : 2.93212
[1mStep[0m  [126/427], [94mLoss[0m : 2.95014
[1mStep[0m  [168/427], [94mLoss[0m : 2.65606
[1mStep[0m  [210/427], [94mLoss[0m : 2.25207
[1mStep[0m  [252/427], [94mLoss[0m : 3.19577
[1mStep[0m  [294/427], [94mLoss[0m : 3.43336
[1mStep[0m  [336/427], [94mLoss[0m : 2.17206
[1mStep[0m  [378/427], [94mLoss[0m : 3.23605
[1mStep[0m  [420/427], [94mLoss[0m : 2.47645

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.09258
[1mStep[0m  [42/427], [94mLoss[0m : 3.07591
[1mStep[0m  [84/427], [94mLoss[0m : 2.44482
[1mStep[0m  [126/427], [94mLoss[0m : 2.35617
[1mStep[0m  [168/427], [94mLoss[0m : 2.27171
[1mStep[0m  [210/427], [94mLoss[0m : 2.80127
[1mStep[0m  [252/427], [94mLoss[0m : 2.38982
[1mStep[0m  [294/427], [94mLoss[0m : 2.84048
[1mStep[0m  [336/427], [94mLoss[0m : 1.95253
[1mStep[0m  [378/427], [94mLoss[0m : 2.53096
[1mStep[0m  [420/427], [94mLoss[0m : 2.66000

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.10639
[1mStep[0m  [42/427], [94mLoss[0m : 2.24908
[1mStep[0m  [84/427], [94mLoss[0m : 2.72595
[1mStep[0m  [126/427], [94mLoss[0m : 2.51170
[1mStep[0m  [168/427], [94mLoss[0m : 2.03059
[1mStep[0m  [210/427], [94mLoss[0m : 2.14023
[1mStep[0m  [252/427], [94mLoss[0m : 3.41087
[1mStep[0m  [294/427], [94mLoss[0m : 2.59363
[1mStep[0m  [336/427], [94mLoss[0m : 2.16030
[1mStep[0m  [378/427], [94mLoss[0m : 2.61732
[1mStep[0m  [420/427], [94mLoss[0m : 2.29981

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.73545
[1mStep[0m  [42/427], [94mLoss[0m : 2.26611
[1mStep[0m  [84/427], [94mLoss[0m : 2.04234
[1mStep[0m  [126/427], [94mLoss[0m : 2.85745
[1mStep[0m  [168/427], [94mLoss[0m : 2.80574
[1mStep[0m  [210/427], [94mLoss[0m : 2.67255
[1mStep[0m  [252/427], [94mLoss[0m : 2.75922
[1mStep[0m  [294/427], [94mLoss[0m : 1.95858
[1mStep[0m  [336/427], [94mLoss[0m : 2.04039
[1mStep[0m  [378/427], [94mLoss[0m : 2.95030
[1mStep[0m  [420/427], [94mLoss[0m : 2.31136

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.12481
[1mStep[0m  [42/427], [94mLoss[0m : 2.05951
[1mStep[0m  [84/427], [94mLoss[0m : 1.90973
[1mStep[0m  [126/427], [94mLoss[0m : 2.32490
[1mStep[0m  [168/427], [94mLoss[0m : 2.45161
[1mStep[0m  [210/427], [94mLoss[0m : 2.59134
[1mStep[0m  [252/427], [94mLoss[0m : 2.95397
[1mStep[0m  [294/427], [94mLoss[0m : 2.37566
[1mStep[0m  [336/427], [94mLoss[0m : 1.94064
[1mStep[0m  [378/427], [94mLoss[0m : 2.88550
[1mStep[0m  [420/427], [94mLoss[0m : 2.55588

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.09481
[1mStep[0m  [42/427], [94mLoss[0m : 1.91900
[1mStep[0m  [84/427], [94mLoss[0m : 2.57485
[1mStep[0m  [126/427], [94mLoss[0m : 1.83752
[1mStep[0m  [168/427], [94mLoss[0m : 2.54656
[1mStep[0m  [210/427], [94mLoss[0m : 2.45529
[1mStep[0m  [252/427], [94mLoss[0m : 2.71707
[1mStep[0m  [294/427], [94mLoss[0m : 2.90214
[1mStep[0m  [336/427], [94mLoss[0m : 1.60775
[1mStep[0m  [378/427], [94mLoss[0m : 2.70982
[1mStep[0m  [420/427], [94mLoss[0m : 2.61351

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.437, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.35670
[1mStep[0m  [42/427], [94mLoss[0m : 2.61194
[1mStep[0m  [84/427], [94mLoss[0m : 2.59421
[1mStep[0m  [126/427], [94mLoss[0m : 2.19001
[1mStep[0m  [168/427], [94mLoss[0m : 2.83823
[1mStep[0m  [210/427], [94mLoss[0m : 2.46046
[1mStep[0m  [252/427], [94mLoss[0m : 2.61709
[1mStep[0m  [294/427], [94mLoss[0m : 2.45816
[1mStep[0m  [336/427], [94mLoss[0m : 3.42167
[1mStep[0m  [378/427], [94mLoss[0m : 2.20820
[1mStep[0m  [420/427], [94mLoss[0m : 1.87100

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.417, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.35794
[1mStep[0m  [42/427], [94mLoss[0m : 2.46750
[1mStep[0m  [84/427], [94mLoss[0m : 2.79473
[1mStep[0m  [126/427], [94mLoss[0m : 2.73693
[1mStep[0m  [168/427], [94mLoss[0m : 2.57077
[1mStep[0m  [210/427], [94mLoss[0m : 2.15398
[1mStep[0m  [252/427], [94mLoss[0m : 2.20268
[1mStep[0m  [294/427], [94mLoss[0m : 2.95835
[1mStep[0m  [336/427], [94mLoss[0m : 2.81949
[1mStep[0m  [378/427], [94mLoss[0m : 2.63816
[1mStep[0m  [420/427], [94mLoss[0m : 2.63975

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.405, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.75125
[1mStep[0m  [42/427], [94mLoss[0m : 2.57599
[1mStep[0m  [84/427], [94mLoss[0m : 2.53462
[1mStep[0m  [126/427], [94mLoss[0m : 2.57812
[1mStep[0m  [168/427], [94mLoss[0m : 2.24815
[1mStep[0m  [210/427], [94mLoss[0m : 2.08993
[1mStep[0m  [252/427], [94mLoss[0m : 1.83593
[1mStep[0m  [294/427], [94mLoss[0m : 2.10764
[1mStep[0m  [336/427], [94mLoss[0m : 2.93945
[1mStep[0m  [378/427], [94mLoss[0m : 2.90659
[1mStep[0m  [420/427], [94mLoss[0m : 2.41906

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.394, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.50837
[1mStep[0m  [42/427], [94mLoss[0m : 2.17025
[1mStep[0m  [84/427], [94mLoss[0m : 3.21634
[1mStep[0m  [126/427], [94mLoss[0m : 1.65830
[1mStep[0m  [168/427], [94mLoss[0m : 2.49480
[1mStep[0m  [210/427], [94mLoss[0m : 2.55904
[1mStep[0m  [252/427], [94mLoss[0m : 2.83677
[1mStep[0m  [294/427], [94mLoss[0m : 2.31422
[1mStep[0m  [336/427], [94mLoss[0m : 1.92776
[1mStep[0m  [378/427], [94mLoss[0m : 3.06868
[1mStep[0m  [420/427], [94mLoss[0m : 2.51405

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.394, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.87259
[1mStep[0m  [42/427], [94mLoss[0m : 1.99350
[1mStep[0m  [84/427], [94mLoss[0m : 2.72373
[1mStep[0m  [126/427], [94mLoss[0m : 2.56374
[1mStep[0m  [168/427], [94mLoss[0m : 3.03698
[1mStep[0m  [210/427], [94mLoss[0m : 2.41193
[1mStep[0m  [252/427], [94mLoss[0m : 2.08343
[1mStep[0m  [294/427], [94mLoss[0m : 2.40053
[1mStep[0m  [336/427], [94mLoss[0m : 2.12078
[1mStep[0m  [378/427], [94mLoss[0m : 3.06842
[1mStep[0m  [420/427], [94mLoss[0m : 2.62399

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.400, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.17066
[1mStep[0m  [42/427], [94mLoss[0m : 2.09304
[1mStep[0m  [84/427], [94mLoss[0m : 2.02280
[1mStep[0m  [126/427], [94mLoss[0m : 2.73805
[1mStep[0m  [168/427], [94mLoss[0m : 2.43305
[1mStep[0m  [210/427], [94mLoss[0m : 2.56011
[1mStep[0m  [252/427], [94mLoss[0m : 2.92871
[1mStep[0m  [294/427], [94mLoss[0m : 2.51683
[1mStep[0m  [336/427], [94mLoss[0m : 2.33710
[1mStep[0m  [378/427], [94mLoss[0m : 3.43777
[1mStep[0m  [420/427], [94mLoss[0m : 2.67942

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.94936
[1mStep[0m  [42/427], [94mLoss[0m : 2.90221
[1mStep[0m  [84/427], [94mLoss[0m : 2.59247
[1mStep[0m  [126/427], [94mLoss[0m : 2.10371
[1mStep[0m  [168/427], [94mLoss[0m : 2.39260
[1mStep[0m  [210/427], [94mLoss[0m : 2.40773
[1mStep[0m  [252/427], [94mLoss[0m : 2.24950
[1mStep[0m  [294/427], [94mLoss[0m : 2.15850
[1mStep[0m  [336/427], [94mLoss[0m : 1.98829
[1mStep[0m  [378/427], [94mLoss[0m : 2.63995
[1mStep[0m  [420/427], [94mLoss[0m : 2.64315

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.393, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.99308
[1mStep[0m  [42/427], [94mLoss[0m : 2.88178
[1mStep[0m  [84/427], [94mLoss[0m : 2.73142
[1mStep[0m  [126/427], [94mLoss[0m : 1.69012
[1mStep[0m  [168/427], [94mLoss[0m : 2.49320
[1mStep[0m  [210/427], [94mLoss[0m : 2.69356
[1mStep[0m  [252/427], [94mLoss[0m : 2.33981
[1mStep[0m  [294/427], [94mLoss[0m : 2.33762
[1mStep[0m  [336/427], [94mLoss[0m : 2.08171
[1mStep[0m  [378/427], [94mLoss[0m : 2.80697
[1mStep[0m  [420/427], [94mLoss[0m : 2.44798

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.417, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.80659
[1mStep[0m  [42/427], [94mLoss[0m : 2.15459
[1mStep[0m  [84/427], [94mLoss[0m : 2.27076
[1mStep[0m  [126/427], [94mLoss[0m : 2.83500
[1mStep[0m  [168/427], [94mLoss[0m : 2.03714
[1mStep[0m  [210/427], [94mLoss[0m : 2.55772
[1mStep[0m  [252/427], [94mLoss[0m : 2.73999
[1mStep[0m  [294/427], [94mLoss[0m : 2.23201
[1mStep[0m  [336/427], [94mLoss[0m : 2.84549
[1mStep[0m  [378/427], [94mLoss[0m : 3.04866
[1mStep[0m  [420/427], [94mLoss[0m : 2.10524

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.427, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.21796
[1mStep[0m  [42/427], [94mLoss[0m : 2.59522
[1mStep[0m  [84/427], [94mLoss[0m : 1.94670
[1mStep[0m  [126/427], [94mLoss[0m : 2.03861
[1mStep[0m  [168/427], [94mLoss[0m : 2.20981
[1mStep[0m  [210/427], [94mLoss[0m : 2.63025
[1mStep[0m  [252/427], [94mLoss[0m : 1.92916
[1mStep[0m  [294/427], [94mLoss[0m : 2.48724
[1mStep[0m  [336/427], [94mLoss[0m : 1.97833
[1mStep[0m  [378/427], [94mLoss[0m : 2.93876
[1mStep[0m  [420/427], [94mLoss[0m : 2.35236

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.402, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.425
====================================

Phase 1 - Evaluation MAE:  2.424937233678612
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/427], [94mLoss[0m : 2.40495
[1mStep[0m  [42/427], [94mLoss[0m : 1.96557
[1mStep[0m  [84/427], [94mLoss[0m : 2.84925
[1mStep[0m  [126/427], [94mLoss[0m : 2.41609
[1mStep[0m  [168/427], [94mLoss[0m : 2.66764
[1mStep[0m  [210/427], [94mLoss[0m : 2.67225
[1mStep[0m  [252/427], [94mLoss[0m : 2.67532
[1mStep[0m  [294/427], [94mLoss[0m : 2.13031
[1mStep[0m  [336/427], [94mLoss[0m : 2.17925
[1mStep[0m  [378/427], [94mLoss[0m : 2.31528
[1mStep[0m  [420/427], [94mLoss[0m : 2.34552

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.36420
[1mStep[0m  [42/427], [94mLoss[0m : 2.49737
[1mStep[0m  [84/427], [94mLoss[0m : 2.81343
[1mStep[0m  [126/427], [94mLoss[0m : 1.91518
[1mStep[0m  [168/427], [94mLoss[0m : 2.79121
[1mStep[0m  [210/427], [94mLoss[0m : 2.55281
[1mStep[0m  [252/427], [94mLoss[0m : 2.34691
[1mStep[0m  [294/427], [94mLoss[0m : 2.47576
[1mStep[0m  [336/427], [94mLoss[0m : 2.73819
[1mStep[0m  [378/427], [94mLoss[0m : 2.63539
[1mStep[0m  [420/427], [94mLoss[0m : 2.57816

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.91637
[1mStep[0m  [42/427], [94mLoss[0m : 1.84398
[1mStep[0m  [84/427], [94mLoss[0m : 2.43575
[1mStep[0m  [126/427], [94mLoss[0m : 2.70441
[1mStep[0m  [168/427], [94mLoss[0m : 2.13611
[1mStep[0m  [210/427], [94mLoss[0m : 2.43681
[1mStep[0m  [252/427], [94mLoss[0m : 2.20061
[1mStep[0m  [294/427], [94mLoss[0m : 2.33722
[1mStep[0m  [336/427], [94mLoss[0m : 2.40058
[1mStep[0m  [378/427], [94mLoss[0m : 2.05388
[1mStep[0m  [420/427], [94mLoss[0m : 2.09816

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.89933
[1mStep[0m  [42/427], [94mLoss[0m : 2.30697
[1mStep[0m  [84/427], [94mLoss[0m : 2.15817
[1mStep[0m  [126/427], [94mLoss[0m : 2.11552
[1mStep[0m  [168/427], [94mLoss[0m : 2.16770
[1mStep[0m  [210/427], [94mLoss[0m : 2.31228
[1mStep[0m  [252/427], [94mLoss[0m : 2.12150
[1mStep[0m  [294/427], [94mLoss[0m : 2.38731
[1mStep[0m  [336/427], [94mLoss[0m : 2.63945
[1mStep[0m  [378/427], [94mLoss[0m : 2.10683
[1mStep[0m  [420/427], [94mLoss[0m : 2.06256

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.97111
[1mStep[0m  [42/427], [94mLoss[0m : 2.29942
[1mStep[0m  [84/427], [94mLoss[0m : 2.52872
[1mStep[0m  [126/427], [94mLoss[0m : 2.30282
[1mStep[0m  [168/427], [94mLoss[0m : 1.99333
[1mStep[0m  [210/427], [94mLoss[0m : 1.82123
[1mStep[0m  [252/427], [94mLoss[0m : 2.40249
[1mStep[0m  [294/427], [94mLoss[0m : 2.78913
[1mStep[0m  [336/427], [94mLoss[0m : 1.67504
[1mStep[0m  [378/427], [94mLoss[0m : 2.26899
[1mStep[0m  [420/427], [94mLoss[0m : 2.16625

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.83799
[1mStep[0m  [42/427], [94mLoss[0m : 1.36874
[1mStep[0m  [84/427], [94mLoss[0m : 2.29458
[1mStep[0m  [126/427], [94mLoss[0m : 2.55719
[1mStep[0m  [168/427], [94mLoss[0m : 2.64859
[1mStep[0m  [210/427], [94mLoss[0m : 2.39000
[1mStep[0m  [252/427], [94mLoss[0m : 2.09195
[1mStep[0m  [294/427], [94mLoss[0m : 2.14964
[1mStep[0m  [336/427], [94mLoss[0m : 2.16563
[1mStep[0m  [378/427], [94mLoss[0m : 2.45783
[1mStep[0m  [420/427], [94mLoss[0m : 2.54026

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.72272
[1mStep[0m  [42/427], [94mLoss[0m : 2.05071
[1mStep[0m  [84/427], [94mLoss[0m : 1.52375
[1mStep[0m  [126/427], [94mLoss[0m : 2.18634
[1mStep[0m  [168/427], [94mLoss[0m : 1.82262
[1mStep[0m  [210/427], [94mLoss[0m : 2.51589
[1mStep[0m  [252/427], [94mLoss[0m : 1.90148
[1mStep[0m  [294/427], [94mLoss[0m : 2.27176
[1mStep[0m  [336/427], [94mLoss[0m : 2.17973
[1mStep[0m  [378/427], [94mLoss[0m : 2.45034
[1mStep[0m  [420/427], [94mLoss[0m : 2.15949

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.84092
[1mStep[0m  [42/427], [94mLoss[0m : 2.29765
[1mStep[0m  [84/427], [94mLoss[0m : 1.70182
[1mStep[0m  [126/427], [94mLoss[0m : 1.90182
[1mStep[0m  [168/427], [94mLoss[0m : 1.67657
[1mStep[0m  [210/427], [94mLoss[0m : 2.52254
[1mStep[0m  [252/427], [94mLoss[0m : 1.96197
[1mStep[0m  [294/427], [94mLoss[0m : 2.68621
[1mStep[0m  [336/427], [94mLoss[0m : 1.72154
[1mStep[0m  [378/427], [94mLoss[0m : 1.95131
[1mStep[0m  [420/427], [94mLoss[0m : 2.28036

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.541, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.03064
[1mStep[0m  [42/427], [94mLoss[0m : 1.95573
[1mStep[0m  [84/427], [94mLoss[0m : 2.39240
[1mStep[0m  [126/427], [94mLoss[0m : 2.51597
[1mStep[0m  [168/427], [94mLoss[0m : 2.00711
[1mStep[0m  [210/427], [94mLoss[0m : 2.25243
[1mStep[0m  [252/427], [94mLoss[0m : 2.20418
[1mStep[0m  [294/427], [94mLoss[0m : 1.94592
[1mStep[0m  [336/427], [94mLoss[0m : 2.11564
[1mStep[0m  [378/427], [94mLoss[0m : 2.33612
[1mStep[0m  [420/427], [94mLoss[0m : 2.43388

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.57094
[1mStep[0m  [42/427], [94mLoss[0m : 2.31399
[1mStep[0m  [84/427], [94mLoss[0m : 2.45370
[1mStep[0m  [126/427], [94mLoss[0m : 2.37821
[1mStep[0m  [168/427], [94mLoss[0m : 1.83399
[1mStep[0m  [210/427], [94mLoss[0m : 1.84726
[1mStep[0m  [252/427], [94mLoss[0m : 2.59232
[1mStep[0m  [294/427], [94mLoss[0m : 1.79737
[1mStep[0m  [336/427], [94mLoss[0m : 1.76224
[1mStep[0m  [378/427], [94mLoss[0m : 2.06677
[1mStep[0m  [420/427], [94mLoss[0m : 2.46661

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.99293
[1mStep[0m  [42/427], [94mLoss[0m : 2.05458
[1mStep[0m  [84/427], [94mLoss[0m : 2.49420
[1mStep[0m  [126/427], [94mLoss[0m : 1.40734
[1mStep[0m  [168/427], [94mLoss[0m : 2.55191
[1mStep[0m  [210/427], [94mLoss[0m : 2.38910
[1mStep[0m  [252/427], [94mLoss[0m : 1.89747
[1mStep[0m  [294/427], [94mLoss[0m : 1.68365
[1mStep[0m  [336/427], [94mLoss[0m : 2.09787
[1mStep[0m  [378/427], [94mLoss[0m : 2.58517
[1mStep[0m  [420/427], [94mLoss[0m : 2.94992

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.99699
[1mStep[0m  [42/427], [94mLoss[0m : 2.16788
[1mStep[0m  [84/427], [94mLoss[0m : 2.14971
[1mStep[0m  [126/427], [94mLoss[0m : 1.36308
[1mStep[0m  [168/427], [94mLoss[0m : 2.06488
[1mStep[0m  [210/427], [94mLoss[0m : 1.76680
[1mStep[0m  [252/427], [94mLoss[0m : 2.08287
[1mStep[0m  [294/427], [94mLoss[0m : 2.75626
[1mStep[0m  [336/427], [94mLoss[0m : 1.87277
[1mStep[0m  [378/427], [94mLoss[0m : 1.55496
[1mStep[0m  [420/427], [94mLoss[0m : 1.57286

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.058, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.14103
[1mStep[0m  [42/427], [94mLoss[0m : 1.69097
[1mStep[0m  [84/427], [94mLoss[0m : 2.48310
[1mStep[0m  [126/427], [94mLoss[0m : 2.19784
[1mStep[0m  [168/427], [94mLoss[0m : 1.80042
[1mStep[0m  [210/427], [94mLoss[0m : 1.84895
[1mStep[0m  [252/427], [94mLoss[0m : 1.82314
[1mStep[0m  [294/427], [94mLoss[0m : 1.92543
[1mStep[0m  [336/427], [94mLoss[0m : 2.93574
[1mStep[0m  [378/427], [94mLoss[0m : 1.98353
[1mStep[0m  [420/427], [94mLoss[0m : 2.31973

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.95112
[1mStep[0m  [42/427], [94mLoss[0m : 1.90409
[1mStep[0m  [84/427], [94mLoss[0m : 1.34027
[1mStep[0m  [126/427], [94mLoss[0m : 1.96695
[1mStep[0m  [168/427], [94mLoss[0m : 2.15623
[1mStep[0m  [210/427], [94mLoss[0m : 1.89938
[1mStep[0m  [252/427], [94mLoss[0m : 1.99126
[1mStep[0m  [294/427], [94mLoss[0m : 1.87876
[1mStep[0m  [336/427], [94mLoss[0m : 2.67846
[1mStep[0m  [378/427], [94mLoss[0m : 2.30935
[1mStep[0m  [420/427], [94mLoss[0m : 1.90938

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.97061
[1mStep[0m  [42/427], [94mLoss[0m : 1.89437
[1mStep[0m  [84/427], [94mLoss[0m : 2.42272
[1mStep[0m  [126/427], [94mLoss[0m : 1.94391
[1mStep[0m  [168/427], [94mLoss[0m : 2.54979
[1mStep[0m  [210/427], [94mLoss[0m : 2.08636
[1mStep[0m  [252/427], [94mLoss[0m : 1.75165
[1mStep[0m  [294/427], [94mLoss[0m : 1.50502
[1mStep[0m  [336/427], [94mLoss[0m : 2.85872
[1mStep[0m  [378/427], [94mLoss[0m : 2.65352
[1mStep[0m  [420/427], [94mLoss[0m : 2.32109

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.15826
[1mStep[0m  [42/427], [94mLoss[0m : 2.04878
[1mStep[0m  [84/427], [94mLoss[0m : 2.14759
[1mStep[0m  [126/427], [94mLoss[0m : 1.40889
[1mStep[0m  [168/427], [94mLoss[0m : 1.88104
[1mStep[0m  [210/427], [94mLoss[0m : 1.20366
[1mStep[0m  [252/427], [94mLoss[0m : 2.00401
[1mStep[0m  [294/427], [94mLoss[0m : 1.87175
[1mStep[0m  [336/427], [94mLoss[0m : 2.57898
[1mStep[0m  [378/427], [94mLoss[0m : 1.95712
[1mStep[0m  [420/427], [94mLoss[0m : 1.94298

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.36218
[1mStep[0m  [42/427], [94mLoss[0m : 2.19186
[1mStep[0m  [84/427], [94mLoss[0m : 1.97105
[1mStep[0m  [126/427], [94mLoss[0m : 2.43222
[1mStep[0m  [168/427], [94mLoss[0m : 1.86597
[1mStep[0m  [210/427], [94mLoss[0m : 2.27702
[1mStep[0m  [252/427], [94mLoss[0m : 1.61714
[1mStep[0m  [294/427], [94mLoss[0m : 1.61449
[1mStep[0m  [336/427], [94mLoss[0m : 2.09417
[1mStep[0m  [378/427], [94mLoss[0m : 2.74269
[1mStep[0m  [420/427], [94mLoss[0m : 2.66040

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.64637
[1mStep[0m  [42/427], [94mLoss[0m : 2.11337
[1mStep[0m  [84/427], [94mLoss[0m : 1.58846
[1mStep[0m  [126/427], [94mLoss[0m : 2.12799
[1mStep[0m  [168/427], [94mLoss[0m : 1.69760
[1mStep[0m  [210/427], [94mLoss[0m : 2.03580
[1mStep[0m  [252/427], [94mLoss[0m : 1.44882
[1mStep[0m  [294/427], [94mLoss[0m : 2.12100
[1mStep[0m  [336/427], [94mLoss[0m : 1.84070
[1mStep[0m  [378/427], [94mLoss[0m : 1.91838
[1mStep[0m  [420/427], [94mLoss[0m : 1.68845

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.915, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.05925
[1mStep[0m  [42/427], [94mLoss[0m : 1.61558
[1mStep[0m  [84/427], [94mLoss[0m : 2.31464
[1mStep[0m  [126/427], [94mLoss[0m : 1.48375
[1mStep[0m  [168/427], [94mLoss[0m : 1.64758
[1mStep[0m  [210/427], [94mLoss[0m : 1.31950
[1mStep[0m  [252/427], [94mLoss[0m : 2.70541
[1mStep[0m  [294/427], [94mLoss[0m : 2.14430
[1mStep[0m  [336/427], [94mLoss[0m : 1.48710
[1mStep[0m  [378/427], [94mLoss[0m : 2.12811
[1mStep[0m  [420/427], [94mLoss[0m : 1.54935

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.68448
[1mStep[0m  [42/427], [94mLoss[0m : 1.96515
[1mStep[0m  [84/427], [94mLoss[0m : 1.60011
[1mStep[0m  [126/427], [94mLoss[0m : 1.91343
[1mStep[0m  [168/427], [94mLoss[0m : 2.81554
[1mStep[0m  [210/427], [94mLoss[0m : 1.62676
[1mStep[0m  [252/427], [94mLoss[0m : 1.87755
[1mStep[0m  [294/427], [94mLoss[0m : 1.60694
[1mStep[0m  [336/427], [94mLoss[0m : 1.77652
[1mStep[0m  [378/427], [94mLoss[0m : 1.82393
[1mStep[0m  [420/427], [94mLoss[0m : 1.46219

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.688, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.50300
[1mStep[0m  [42/427], [94mLoss[0m : 2.00249
[1mStep[0m  [84/427], [94mLoss[0m : 1.96759
[1mStep[0m  [126/427], [94mLoss[0m : 1.95363
[1mStep[0m  [168/427], [94mLoss[0m : 1.95989
[1mStep[0m  [210/427], [94mLoss[0m : 1.87362
[1mStep[0m  [252/427], [94mLoss[0m : 2.25173
[1mStep[0m  [294/427], [94mLoss[0m : 2.36040
[1mStep[0m  [336/427], [94mLoss[0m : 2.10207
[1mStep[0m  [378/427], [94mLoss[0m : 1.72854
[1mStep[0m  [420/427], [94mLoss[0m : 1.85580

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.834, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.28431
[1mStep[0m  [42/427], [94mLoss[0m : 1.60796
[1mStep[0m  [84/427], [94mLoss[0m : 2.00869
[1mStep[0m  [126/427], [94mLoss[0m : 1.98830
[1mStep[0m  [168/427], [94mLoss[0m : 1.59339
[1mStep[0m  [210/427], [94mLoss[0m : 1.78706
[1mStep[0m  [252/427], [94mLoss[0m : 2.31010
[1mStep[0m  [294/427], [94mLoss[0m : 1.82873
[1mStep[0m  [336/427], [94mLoss[0m : 2.39942
[1mStep[0m  [378/427], [94mLoss[0m : 2.08829
[1mStep[0m  [420/427], [94mLoss[0m : 1.67080

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.04765
[1mStep[0m  [42/427], [94mLoss[0m : 1.65387
[1mStep[0m  [84/427], [94mLoss[0m : 1.68904
[1mStep[0m  [126/427], [94mLoss[0m : 1.49888
[1mStep[0m  [168/427], [94mLoss[0m : 1.98242
[1mStep[0m  [210/427], [94mLoss[0m : 1.78750
[1mStep[0m  [252/427], [94mLoss[0m : 1.81026
[1mStep[0m  [294/427], [94mLoss[0m : 1.69759
[1mStep[0m  [336/427], [94mLoss[0m : 1.62923
[1mStep[0m  [378/427], [94mLoss[0m : 1.78676
[1mStep[0m  [420/427], [94mLoss[0m : 1.77272

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.547, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.23682
[1mStep[0m  [42/427], [94mLoss[0m : 1.51074
[1mStep[0m  [84/427], [94mLoss[0m : 1.35941
[1mStep[0m  [126/427], [94mLoss[0m : 2.02724
[1mStep[0m  [168/427], [94mLoss[0m : 1.68546
[1mStep[0m  [210/427], [94mLoss[0m : 2.23849
[1mStep[0m  [252/427], [94mLoss[0m : 1.82491
[1mStep[0m  [294/427], [94mLoss[0m : 2.22387
[1mStep[0m  [336/427], [94mLoss[0m : 1.73526
[1mStep[0m  [378/427], [94mLoss[0m : 1.71692
[1mStep[0m  [420/427], [94mLoss[0m : 1.52746

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.61455
[1mStep[0m  [42/427], [94mLoss[0m : 1.51108
[1mStep[0m  [84/427], [94mLoss[0m : 2.14161
[1mStep[0m  [126/427], [94mLoss[0m : 1.79727
[1mStep[0m  [168/427], [94mLoss[0m : 1.92784
[1mStep[0m  [210/427], [94mLoss[0m : 1.56815
[1mStep[0m  [252/427], [94mLoss[0m : 1.89921
[1mStep[0m  [294/427], [94mLoss[0m : 1.50669
[1mStep[0m  [336/427], [94mLoss[0m : 1.86415
[1mStep[0m  [378/427], [94mLoss[0m : 1.94077
[1mStep[0m  [420/427], [94mLoss[0m : 1.96911

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.584, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.47491
[1mStep[0m  [42/427], [94mLoss[0m : 1.82070
[1mStep[0m  [84/427], [94mLoss[0m : 2.44764
[1mStep[0m  [126/427], [94mLoss[0m : 1.87650
[1mStep[0m  [168/427], [94mLoss[0m : 1.70952
[1mStep[0m  [210/427], [94mLoss[0m : 1.84266
[1mStep[0m  [252/427], [94mLoss[0m : 1.81456
[1mStep[0m  [294/427], [94mLoss[0m : 1.49301
[1mStep[0m  [336/427], [94mLoss[0m : 1.88257
[1mStep[0m  [378/427], [94mLoss[0m : 2.26023
[1mStep[0m  [420/427], [94mLoss[0m : 1.89403

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.756, [92mTest[0m: 2.569, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.57409
[1mStep[0m  [42/427], [94mLoss[0m : 1.98142
[1mStep[0m  [84/427], [94mLoss[0m : 1.86556
[1mStep[0m  [126/427], [94mLoss[0m : 2.03692
[1mStep[0m  [168/427], [94mLoss[0m : 1.75663
[1mStep[0m  [210/427], [94mLoss[0m : 1.09167
[1mStep[0m  [252/427], [94mLoss[0m : 1.87308
[1mStep[0m  [294/427], [94mLoss[0m : 1.48296
[1mStep[0m  [336/427], [94mLoss[0m : 1.44366
[1mStep[0m  [378/427], [94mLoss[0m : 1.87015
[1mStep[0m  [420/427], [94mLoss[0m : 1.82171

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.563, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.20587
[1mStep[0m  [42/427], [94mLoss[0m : 1.62484
[1mStep[0m  [84/427], [94mLoss[0m : 1.72316
[1mStep[0m  [126/427], [94mLoss[0m : 1.85738
[1mStep[0m  [168/427], [94mLoss[0m : 1.93949
[1mStep[0m  [210/427], [94mLoss[0m : 1.84972
[1mStep[0m  [252/427], [94mLoss[0m : 2.13808
[1mStep[0m  [294/427], [94mLoss[0m : 1.86190
[1mStep[0m  [336/427], [94mLoss[0m : 1.65057
[1mStep[0m  [378/427], [94mLoss[0m : 1.66819
[1mStep[0m  [420/427], [94mLoss[0m : 2.20654

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.468, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.35519
[1mStep[0m  [42/427], [94mLoss[0m : 1.47355
[1mStep[0m  [84/427], [94mLoss[0m : 2.03098
[1mStep[0m  [126/427], [94mLoss[0m : 1.75287
[1mStep[0m  [168/427], [94mLoss[0m : 1.69572
[1mStep[0m  [210/427], [94mLoss[0m : 1.84188
[1mStep[0m  [252/427], [94mLoss[0m : 1.57075
[1mStep[0m  [294/427], [94mLoss[0m : 2.02953
[1mStep[0m  [336/427], [94mLoss[0m : 1.86799
[1mStep[0m  [378/427], [94mLoss[0m : 1.57611
[1mStep[0m  [420/427], [94mLoss[0m : 1.62125

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.71237
[1mStep[0m  [42/427], [94mLoss[0m : 1.83674
[1mStep[0m  [84/427], [94mLoss[0m : 1.40780
[1mStep[0m  [126/427], [94mLoss[0m : 2.47606
[1mStep[0m  [168/427], [94mLoss[0m : 1.90450
[1mStep[0m  [210/427], [94mLoss[0m : 1.85155
[1mStep[0m  [252/427], [94mLoss[0m : 1.73032
[1mStep[0m  [294/427], [94mLoss[0m : 2.18805
[1mStep[0m  [336/427], [94mLoss[0m : 1.62911
[1mStep[0m  [378/427], [94mLoss[0m : 1.97595
[1mStep[0m  [420/427], [94mLoss[0m : 1.50166

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.592, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.575
====================================

Phase 2 - Evaluation MAE:  2.5745367119569735
MAE score P1      2.424937
MAE score P2      2.574537
loss               1.69922
learning_rate     0.007525
batch_size              32
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/427], [94mLoss[0m : 11.82257
[1mStep[0m  [42/427], [94mLoss[0m : 8.84756
[1mStep[0m  [84/427], [94mLoss[0m : 9.16575
[1mStep[0m  [126/427], [94mLoss[0m : 7.62709
[1mStep[0m  [168/427], [94mLoss[0m : 7.11870
[1mStep[0m  [210/427], [94mLoss[0m : 6.02628
[1mStep[0m  [252/427], [94mLoss[0m : 3.50954
[1mStep[0m  [294/427], [94mLoss[0m : 2.20378
[1mStep[0m  [336/427], [94mLoss[0m : 3.29967
[1mStep[0m  [378/427], [94mLoss[0m : 2.76951
[1mStep[0m  [420/427], [94mLoss[0m : 2.68542

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.698, [92mTest[0m: 11.037, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.06645
[1mStep[0m  [42/427], [94mLoss[0m : 2.88997
[1mStep[0m  [84/427], [94mLoss[0m : 2.74874
[1mStep[0m  [126/427], [94mLoss[0m : 2.52518
[1mStep[0m  [168/427], [94mLoss[0m : 1.80282
[1mStep[0m  [210/427], [94mLoss[0m : 2.43180
[1mStep[0m  [252/427], [94mLoss[0m : 3.66754
[1mStep[0m  [294/427], [94mLoss[0m : 2.72026
[1mStep[0m  [336/427], [94mLoss[0m : 3.01505
[1mStep[0m  [378/427], [94mLoss[0m : 2.35309
[1mStep[0m  [420/427], [94mLoss[0m : 2.44240

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.575, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.19113
[1mStep[0m  [42/427], [94mLoss[0m : 2.26383
[1mStep[0m  [84/427], [94mLoss[0m : 2.11267
[1mStep[0m  [126/427], [94mLoss[0m : 2.25353
[1mStep[0m  [168/427], [94mLoss[0m : 2.07001
[1mStep[0m  [210/427], [94mLoss[0m : 2.90439
[1mStep[0m  [252/427], [94mLoss[0m : 2.83160
[1mStep[0m  [294/427], [94mLoss[0m : 2.87157
[1mStep[0m  [336/427], [94mLoss[0m : 2.33244
[1mStep[0m  [378/427], [94mLoss[0m : 1.68307
[1mStep[0m  [420/427], [94mLoss[0m : 2.15265

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.54312
[1mStep[0m  [42/427], [94mLoss[0m : 2.62639
[1mStep[0m  [84/427], [94mLoss[0m : 2.50721
[1mStep[0m  [126/427], [94mLoss[0m : 2.19497
[1mStep[0m  [168/427], [94mLoss[0m : 2.40677
[1mStep[0m  [210/427], [94mLoss[0m : 3.05303
[1mStep[0m  [252/427], [94mLoss[0m : 2.30831
[1mStep[0m  [294/427], [94mLoss[0m : 2.38663
[1mStep[0m  [336/427], [94mLoss[0m : 2.40268
[1mStep[0m  [378/427], [94mLoss[0m : 3.01585
[1mStep[0m  [420/427], [94mLoss[0m : 2.83652

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.82779
[1mStep[0m  [42/427], [94mLoss[0m : 2.40526
[1mStep[0m  [84/427], [94mLoss[0m : 2.59959
[1mStep[0m  [126/427], [94mLoss[0m : 2.62403
[1mStep[0m  [168/427], [94mLoss[0m : 2.48732
[1mStep[0m  [210/427], [94mLoss[0m : 2.50327
[1mStep[0m  [252/427], [94mLoss[0m : 2.67024
[1mStep[0m  [294/427], [94mLoss[0m : 2.28282
[1mStep[0m  [336/427], [94mLoss[0m : 2.63479
[1mStep[0m  [378/427], [94mLoss[0m : 2.68500
[1mStep[0m  [420/427], [94mLoss[0m : 2.91877

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.88559
[1mStep[0m  [42/427], [94mLoss[0m : 2.33438
[1mStep[0m  [84/427], [94mLoss[0m : 2.42212
[1mStep[0m  [126/427], [94mLoss[0m : 2.16206
[1mStep[0m  [168/427], [94mLoss[0m : 2.00719
[1mStep[0m  [210/427], [94mLoss[0m : 2.57227
[1mStep[0m  [252/427], [94mLoss[0m : 3.15667
[1mStep[0m  [294/427], [94mLoss[0m : 1.97696
[1mStep[0m  [336/427], [94mLoss[0m : 2.83054
[1mStep[0m  [378/427], [94mLoss[0m : 2.28352
[1mStep[0m  [420/427], [94mLoss[0m : 2.90830

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.81617
[1mStep[0m  [42/427], [94mLoss[0m : 2.89521
[1mStep[0m  [84/427], [94mLoss[0m : 2.45135
[1mStep[0m  [126/427], [94mLoss[0m : 1.93123
[1mStep[0m  [168/427], [94mLoss[0m : 2.41909
[1mStep[0m  [210/427], [94mLoss[0m : 3.38531
[1mStep[0m  [252/427], [94mLoss[0m : 3.02739
[1mStep[0m  [294/427], [94mLoss[0m : 2.56537
[1mStep[0m  [336/427], [94mLoss[0m : 2.59949
[1mStep[0m  [378/427], [94mLoss[0m : 2.33897
[1mStep[0m  [420/427], [94mLoss[0m : 1.94866

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.42429
[1mStep[0m  [42/427], [94mLoss[0m : 2.10663
[1mStep[0m  [84/427], [94mLoss[0m : 2.22808
[1mStep[0m  [126/427], [94mLoss[0m : 1.69606
[1mStep[0m  [168/427], [94mLoss[0m : 2.31418
[1mStep[0m  [210/427], [94mLoss[0m : 2.74265
[1mStep[0m  [252/427], [94mLoss[0m : 2.64898
[1mStep[0m  [294/427], [94mLoss[0m : 3.53257
[1mStep[0m  [336/427], [94mLoss[0m : 2.48711
[1mStep[0m  [378/427], [94mLoss[0m : 2.39879
[1mStep[0m  [420/427], [94mLoss[0m : 2.33860

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.94658
[1mStep[0m  [42/427], [94mLoss[0m : 2.75907
[1mStep[0m  [84/427], [94mLoss[0m : 2.69941
[1mStep[0m  [126/427], [94mLoss[0m : 2.37261
[1mStep[0m  [168/427], [94mLoss[0m : 2.41547
[1mStep[0m  [210/427], [94mLoss[0m : 1.80094
[1mStep[0m  [252/427], [94mLoss[0m : 2.08541
[1mStep[0m  [294/427], [94mLoss[0m : 2.52669
[1mStep[0m  [336/427], [94mLoss[0m : 2.26060
[1mStep[0m  [378/427], [94mLoss[0m : 2.82482
[1mStep[0m  [420/427], [94mLoss[0m : 2.45771

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.58681
[1mStep[0m  [42/427], [94mLoss[0m : 2.97214
[1mStep[0m  [84/427], [94mLoss[0m : 2.41790
[1mStep[0m  [126/427], [94mLoss[0m : 2.54556
[1mStep[0m  [168/427], [94mLoss[0m : 2.09775
[1mStep[0m  [210/427], [94mLoss[0m : 2.31790
[1mStep[0m  [252/427], [94mLoss[0m : 2.55344
[1mStep[0m  [294/427], [94mLoss[0m : 2.31380
[1mStep[0m  [336/427], [94mLoss[0m : 2.03392
[1mStep[0m  [378/427], [94mLoss[0m : 2.19034
[1mStep[0m  [420/427], [94mLoss[0m : 1.62578

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.46148
[1mStep[0m  [42/427], [94mLoss[0m : 2.24929
[1mStep[0m  [84/427], [94mLoss[0m : 2.65931
[1mStep[0m  [126/427], [94mLoss[0m : 2.57432
[1mStep[0m  [168/427], [94mLoss[0m : 2.05315
[1mStep[0m  [210/427], [94mLoss[0m : 2.60481
[1mStep[0m  [252/427], [94mLoss[0m : 2.37131
[1mStep[0m  [294/427], [94mLoss[0m : 2.59171
[1mStep[0m  [336/427], [94mLoss[0m : 2.65314
[1mStep[0m  [378/427], [94mLoss[0m : 2.50635
[1mStep[0m  [420/427], [94mLoss[0m : 2.13813

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.46327
[1mStep[0m  [42/427], [94mLoss[0m : 2.01929
[1mStep[0m  [84/427], [94mLoss[0m : 2.68100
[1mStep[0m  [126/427], [94mLoss[0m : 2.00076
[1mStep[0m  [168/427], [94mLoss[0m : 2.77420
[1mStep[0m  [210/427], [94mLoss[0m : 3.30606
[1mStep[0m  [252/427], [94mLoss[0m : 2.25680
[1mStep[0m  [294/427], [94mLoss[0m : 2.15625
[1mStep[0m  [336/427], [94mLoss[0m : 2.58075
[1mStep[0m  [378/427], [94mLoss[0m : 1.92403
[1mStep[0m  [420/427], [94mLoss[0m : 2.45819

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.05965
[1mStep[0m  [42/427], [94mLoss[0m : 3.15086
[1mStep[0m  [84/427], [94mLoss[0m : 2.47303
[1mStep[0m  [126/427], [94mLoss[0m : 2.05649
[1mStep[0m  [168/427], [94mLoss[0m : 2.87455
[1mStep[0m  [210/427], [94mLoss[0m : 2.87358
[1mStep[0m  [252/427], [94mLoss[0m : 2.57636
[1mStep[0m  [294/427], [94mLoss[0m : 2.44262
[1mStep[0m  [336/427], [94mLoss[0m : 2.25964
[1mStep[0m  [378/427], [94mLoss[0m : 2.46914
[1mStep[0m  [420/427], [94mLoss[0m : 2.48353

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.65891
[1mStep[0m  [42/427], [94mLoss[0m : 2.13976
[1mStep[0m  [84/427], [94mLoss[0m : 2.44047
[1mStep[0m  [126/427], [94mLoss[0m : 2.53879
[1mStep[0m  [168/427], [94mLoss[0m : 2.70345
[1mStep[0m  [210/427], [94mLoss[0m : 2.70930
[1mStep[0m  [252/427], [94mLoss[0m : 2.35074
[1mStep[0m  [294/427], [94mLoss[0m : 1.94934
[1mStep[0m  [336/427], [94mLoss[0m : 2.31272
[1mStep[0m  [378/427], [94mLoss[0m : 2.27567
[1mStep[0m  [420/427], [94mLoss[0m : 2.21878

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.49611
[1mStep[0m  [42/427], [94mLoss[0m : 2.69277
[1mStep[0m  [84/427], [94mLoss[0m : 2.55767
[1mStep[0m  [126/427], [94mLoss[0m : 1.59891
[1mStep[0m  [168/427], [94mLoss[0m : 2.79655
[1mStep[0m  [210/427], [94mLoss[0m : 2.11327
[1mStep[0m  [252/427], [94mLoss[0m : 1.98698
[1mStep[0m  [294/427], [94mLoss[0m : 2.77698
[1mStep[0m  [336/427], [94mLoss[0m : 2.56326
[1mStep[0m  [378/427], [94mLoss[0m : 2.40876
[1mStep[0m  [420/427], [94mLoss[0m : 1.89739

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.09900
[1mStep[0m  [42/427], [94mLoss[0m : 2.19090
[1mStep[0m  [84/427], [94mLoss[0m : 2.39649
[1mStep[0m  [126/427], [94mLoss[0m : 2.65961
[1mStep[0m  [168/427], [94mLoss[0m : 1.94575
[1mStep[0m  [210/427], [94mLoss[0m : 2.55959
[1mStep[0m  [252/427], [94mLoss[0m : 1.64707
[1mStep[0m  [294/427], [94mLoss[0m : 2.32816
[1mStep[0m  [336/427], [94mLoss[0m : 2.71564
[1mStep[0m  [378/427], [94mLoss[0m : 2.84747
[1mStep[0m  [420/427], [94mLoss[0m : 1.79656

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.58346
[1mStep[0m  [42/427], [94mLoss[0m : 2.43139
[1mStep[0m  [84/427], [94mLoss[0m : 3.09824
[1mStep[0m  [126/427], [94mLoss[0m : 2.58754
[1mStep[0m  [168/427], [94mLoss[0m : 2.69310
[1mStep[0m  [210/427], [94mLoss[0m : 2.56699
[1mStep[0m  [252/427], [94mLoss[0m : 2.58225
[1mStep[0m  [294/427], [94mLoss[0m : 2.44649
[1mStep[0m  [336/427], [94mLoss[0m : 2.58566
[1mStep[0m  [378/427], [94mLoss[0m : 2.35818
[1mStep[0m  [420/427], [94mLoss[0m : 3.22327

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.58086
[1mStep[0m  [42/427], [94mLoss[0m : 2.66453
[1mStep[0m  [84/427], [94mLoss[0m : 2.28396
[1mStep[0m  [126/427], [94mLoss[0m : 2.45371
[1mStep[0m  [168/427], [94mLoss[0m : 2.28786
[1mStep[0m  [210/427], [94mLoss[0m : 3.38337
[1mStep[0m  [252/427], [94mLoss[0m : 3.18180
[1mStep[0m  [294/427], [94mLoss[0m : 2.42624
[1mStep[0m  [336/427], [94mLoss[0m : 1.54564
[1mStep[0m  [378/427], [94mLoss[0m : 2.17089
[1mStep[0m  [420/427], [94mLoss[0m : 2.64716

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32290
[1mStep[0m  [42/427], [94mLoss[0m : 2.03985
[1mStep[0m  [84/427], [94mLoss[0m : 2.27662
[1mStep[0m  [126/427], [94mLoss[0m : 2.60475
[1mStep[0m  [168/427], [94mLoss[0m : 2.49427
[1mStep[0m  [210/427], [94mLoss[0m : 2.49241
[1mStep[0m  [252/427], [94mLoss[0m : 2.28575
[1mStep[0m  [294/427], [94mLoss[0m : 2.52169
[1mStep[0m  [336/427], [94mLoss[0m : 2.71906
[1mStep[0m  [378/427], [94mLoss[0m : 2.20192
[1mStep[0m  [420/427], [94mLoss[0m : 1.30453

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.37342
[1mStep[0m  [42/427], [94mLoss[0m : 2.41526
[1mStep[0m  [84/427], [94mLoss[0m : 2.42584
[1mStep[0m  [126/427], [94mLoss[0m : 2.43028
[1mStep[0m  [168/427], [94mLoss[0m : 2.61014
[1mStep[0m  [210/427], [94mLoss[0m : 2.44727
[1mStep[0m  [252/427], [94mLoss[0m : 2.39368
[1mStep[0m  [294/427], [94mLoss[0m : 3.18619
[1mStep[0m  [336/427], [94mLoss[0m : 2.50240
[1mStep[0m  [378/427], [94mLoss[0m : 2.61284
[1mStep[0m  [420/427], [94mLoss[0m : 2.64519

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.379, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.38366
[1mStep[0m  [42/427], [94mLoss[0m : 2.41843
[1mStep[0m  [84/427], [94mLoss[0m : 2.76434
[1mStep[0m  [126/427], [94mLoss[0m : 3.35074
[1mStep[0m  [168/427], [94mLoss[0m : 2.56741
[1mStep[0m  [210/427], [94mLoss[0m : 2.21367
[1mStep[0m  [252/427], [94mLoss[0m : 2.23175
[1mStep[0m  [294/427], [94mLoss[0m : 2.51270
[1mStep[0m  [336/427], [94mLoss[0m : 1.56737
[1mStep[0m  [378/427], [94mLoss[0m : 2.62169
[1mStep[0m  [420/427], [94mLoss[0m : 2.08351

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.388, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.31839
[1mStep[0m  [42/427], [94mLoss[0m : 1.89072
[1mStep[0m  [84/427], [94mLoss[0m : 1.71294
[1mStep[0m  [126/427], [94mLoss[0m : 2.73458
[1mStep[0m  [168/427], [94mLoss[0m : 2.03216
[1mStep[0m  [210/427], [94mLoss[0m : 2.83819
[1mStep[0m  [252/427], [94mLoss[0m : 1.76792
[1mStep[0m  [294/427], [94mLoss[0m : 2.96427
[1mStep[0m  [336/427], [94mLoss[0m : 2.10588
[1mStep[0m  [378/427], [94mLoss[0m : 2.14744
[1mStep[0m  [420/427], [94mLoss[0m : 3.08163

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.393, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.48810
[1mStep[0m  [42/427], [94mLoss[0m : 2.79876
[1mStep[0m  [84/427], [94mLoss[0m : 1.51770
[1mStep[0m  [126/427], [94mLoss[0m : 2.36442
[1mStep[0m  [168/427], [94mLoss[0m : 2.82609
[1mStep[0m  [210/427], [94mLoss[0m : 2.53899
[1mStep[0m  [252/427], [94mLoss[0m : 2.02560
[1mStep[0m  [294/427], [94mLoss[0m : 2.70157
[1mStep[0m  [336/427], [94mLoss[0m : 2.18054
[1mStep[0m  [378/427], [94mLoss[0m : 2.60923
[1mStep[0m  [420/427], [94mLoss[0m : 2.94200

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.42493
[1mStep[0m  [42/427], [94mLoss[0m : 2.62380
[1mStep[0m  [84/427], [94mLoss[0m : 2.52578
[1mStep[0m  [126/427], [94mLoss[0m : 2.72664
[1mStep[0m  [168/427], [94mLoss[0m : 3.13844
[1mStep[0m  [210/427], [94mLoss[0m : 2.46891
[1mStep[0m  [252/427], [94mLoss[0m : 2.67151
[1mStep[0m  [294/427], [94mLoss[0m : 2.08330
[1mStep[0m  [336/427], [94mLoss[0m : 2.52802
[1mStep[0m  [378/427], [94mLoss[0m : 2.72510
[1mStep[0m  [420/427], [94mLoss[0m : 2.42637

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.82999
[1mStep[0m  [42/427], [94mLoss[0m : 2.19970
[1mStep[0m  [84/427], [94mLoss[0m : 2.60580
[1mStep[0m  [126/427], [94mLoss[0m : 2.27584
[1mStep[0m  [168/427], [94mLoss[0m : 2.21174
[1mStep[0m  [210/427], [94mLoss[0m : 2.57858
[1mStep[0m  [252/427], [94mLoss[0m : 2.43287
[1mStep[0m  [294/427], [94mLoss[0m : 1.97285
[1mStep[0m  [336/427], [94mLoss[0m : 2.49907
[1mStep[0m  [378/427], [94mLoss[0m : 1.98574
[1mStep[0m  [420/427], [94mLoss[0m : 3.35203

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.373, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.93020
[1mStep[0m  [42/427], [94mLoss[0m : 1.70582
[1mStep[0m  [84/427], [94mLoss[0m : 1.62139
[1mStep[0m  [126/427], [94mLoss[0m : 2.62580
[1mStep[0m  [168/427], [94mLoss[0m : 2.31749
[1mStep[0m  [210/427], [94mLoss[0m : 2.23785
[1mStep[0m  [252/427], [94mLoss[0m : 2.53969
[1mStep[0m  [294/427], [94mLoss[0m : 2.02549
[1mStep[0m  [336/427], [94mLoss[0m : 2.30757
[1mStep[0m  [378/427], [94mLoss[0m : 2.52473
[1mStep[0m  [420/427], [94mLoss[0m : 2.53329

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.365, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.92718
[1mStep[0m  [42/427], [94mLoss[0m : 2.26861
[1mStep[0m  [84/427], [94mLoss[0m : 2.44659
[1mStep[0m  [126/427], [94mLoss[0m : 2.28009
[1mStep[0m  [168/427], [94mLoss[0m : 2.49459
[1mStep[0m  [210/427], [94mLoss[0m : 1.81336
[1mStep[0m  [252/427], [94mLoss[0m : 2.68927
[1mStep[0m  [294/427], [94mLoss[0m : 1.75966
[1mStep[0m  [336/427], [94mLoss[0m : 2.27376
[1mStep[0m  [378/427], [94mLoss[0m : 2.11431
[1mStep[0m  [420/427], [94mLoss[0m : 2.40834

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.402, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.80849
[1mStep[0m  [42/427], [94mLoss[0m : 1.93834
[1mStep[0m  [84/427], [94mLoss[0m : 2.16374
[1mStep[0m  [126/427], [94mLoss[0m : 2.14073
[1mStep[0m  [168/427], [94mLoss[0m : 2.92299
[1mStep[0m  [210/427], [94mLoss[0m : 2.21689
[1mStep[0m  [252/427], [94mLoss[0m : 2.52264
[1mStep[0m  [294/427], [94mLoss[0m : 2.27085
[1mStep[0m  [336/427], [94mLoss[0m : 1.82489
[1mStep[0m  [378/427], [94mLoss[0m : 2.56238
[1mStep[0m  [420/427], [94mLoss[0m : 2.71544

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.369, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.34446
[1mStep[0m  [42/427], [94mLoss[0m : 3.16557
[1mStep[0m  [84/427], [94mLoss[0m : 1.72203
[1mStep[0m  [126/427], [94mLoss[0m : 2.55088
[1mStep[0m  [168/427], [94mLoss[0m : 2.46898
[1mStep[0m  [210/427], [94mLoss[0m : 3.00794
[1mStep[0m  [252/427], [94mLoss[0m : 1.97133
[1mStep[0m  [294/427], [94mLoss[0m : 2.37326
[1mStep[0m  [336/427], [94mLoss[0m : 2.31822
[1mStep[0m  [378/427], [94mLoss[0m : 2.14292
[1mStep[0m  [420/427], [94mLoss[0m : 2.78057

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.352, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32978
[1mStep[0m  [42/427], [94mLoss[0m : 2.24617
[1mStep[0m  [84/427], [94mLoss[0m : 2.47714
[1mStep[0m  [126/427], [94mLoss[0m : 2.35353
[1mStep[0m  [168/427], [94mLoss[0m : 2.37119
[1mStep[0m  [210/427], [94mLoss[0m : 2.45553
[1mStep[0m  [252/427], [94mLoss[0m : 2.62166
[1mStep[0m  [294/427], [94mLoss[0m : 2.11921
[1mStep[0m  [336/427], [94mLoss[0m : 2.69606
[1mStep[0m  [378/427], [94mLoss[0m : 2.44672
[1mStep[0m  [420/427], [94mLoss[0m : 2.48007

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.404, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.425
====================================

Phase 1 - Evaluation MAE:  2.4248658116434663
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/427], [94mLoss[0m : 2.63574
[1mStep[0m  [42/427], [94mLoss[0m : 1.94882
[1mStep[0m  [84/427], [94mLoss[0m : 2.48837
[1mStep[0m  [126/427], [94mLoss[0m : 2.35918
[1mStep[0m  [168/427], [94mLoss[0m : 2.89173
[1mStep[0m  [210/427], [94mLoss[0m : 2.69191
[1mStep[0m  [252/427], [94mLoss[0m : 2.12309
[1mStep[0m  [294/427], [94mLoss[0m : 1.93126
[1mStep[0m  [336/427], [94mLoss[0m : 2.16040
[1mStep[0m  [378/427], [94mLoss[0m : 2.59105
[1mStep[0m  [420/427], [94mLoss[0m : 2.19581

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.80607
[1mStep[0m  [42/427], [94mLoss[0m : 2.69308
[1mStep[0m  [84/427], [94mLoss[0m : 2.26720
[1mStep[0m  [126/427], [94mLoss[0m : 2.42093
[1mStep[0m  [168/427], [94mLoss[0m : 2.40184
[1mStep[0m  [210/427], [94mLoss[0m : 2.50095
[1mStep[0m  [252/427], [94mLoss[0m : 2.26529
[1mStep[0m  [294/427], [94mLoss[0m : 2.29709
[1mStep[0m  [336/427], [94mLoss[0m : 2.71569
[1mStep[0m  [378/427], [94mLoss[0m : 2.38590
[1mStep[0m  [420/427], [94mLoss[0m : 2.23222

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.76737
[1mStep[0m  [42/427], [94mLoss[0m : 2.42479
[1mStep[0m  [84/427], [94mLoss[0m : 2.21811
[1mStep[0m  [126/427], [94mLoss[0m : 2.77863
[1mStep[0m  [168/427], [94mLoss[0m : 2.09385
[1mStep[0m  [210/427], [94mLoss[0m : 2.40035
[1mStep[0m  [252/427], [94mLoss[0m : 2.44516
[1mStep[0m  [294/427], [94mLoss[0m : 2.53930
[1mStep[0m  [336/427], [94mLoss[0m : 2.50710
[1mStep[0m  [378/427], [94mLoss[0m : 2.61100
[1mStep[0m  [420/427], [94mLoss[0m : 2.07039

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.24692
[1mStep[0m  [42/427], [94mLoss[0m : 1.90830
[1mStep[0m  [84/427], [94mLoss[0m : 2.31227
[1mStep[0m  [126/427], [94mLoss[0m : 1.98125
[1mStep[0m  [168/427], [94mLoss[0m : 1.98715
[1mStep[0m  [210/427], [94mLoss[0m : 2.43003
[1mStep[0m  [252/427], [94mLoss[0m : 2.16077
[1mStep[0m  [294/427], [94mLoss[0m : 1.59391
[1mStep[0m  [336/427], [94mLoss[0m : 2.45607
[1mStep[0m  [378/427], [94mLoss[0m : 2.09104
[1mStep[0m  [420/427], [94mLoss[0m : 2.21806

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.239, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.30107
[1mStep[0m  [42/427], [94mLoss[0m : 2.25655
[1mStep[0m  [84/427], [94mLoss[0m : 1.95575
[1mStep[0m  [126/427], [94mLoss[0m : 1.88424
[1mStep[0m  [168/427], [94mLoss[0m : 2.27433
[1mStep[0m  [210/427], [94mLoss[0m : 1.57087
[1mStep[0m  [252/427], [94mLoss[0m : 2.32069
[1mStep[0m  [294/427], [94mLoss[0m : 1.71402
[1mStep[0m  [336/427], [94mLoss[0m : 1.86464
[1mStep[0m  [378/427], [94mLoss[0m : 2.25472
[1mStep[0m  [420/427], [94mLoss[0m : 1.74889

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.85436
[1mStep[0m  [42/427], [94mLoss[0m : 1.40211
[1mStep[0m  [84/427], [94mLoss[0m : 2.20859
[1mStep[0m  [126/427], [94mLoss[0m : 2.40876
[1mStep[0m  [168/427], [94mLoss[0m : 1.46796
[1mStep[0m  [210/427], [94mLoss[0m : 2.22654
[1mStep[0m  [252/427], [94mLoss[0m : 2.21114
[1mStep[0m  [294/427], [94mLoss[0m : 2.13309
[1mStep[0m  [336/427], [94mLoss[0m : 2.21571
[1mStep[0m  [378/427], [94mLoss[0m : 1.90843
[1mStep[0m  [420/427], [94mLoss[0m : 2.14912

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.12825
[1mStep[0m  [42/427], [94mLoss[0m : 1.86782
[1mStep[0m  [84/427], [94mLoss[0m : 1.67737
[1mStep[0m  [126/427], [94mLoss[0m : 2.16313
[1mStep[0m  [168/427], [94mLoss[0m : 1.90240
[1mStep[0m  [210/427], [94mLoss[0m : 1.74944
[1mStep[0m  [252/427], [94mLoss[0m : 2.64528
[1mStep[0m  [294/427], [94mLoss[0m : 1.63211
[1mStep[0m  [336/427], [94mLoss[0m : 1.82612
[1mStep[0m  [378/427], [94mLoss[0m : 1.90315
[1mStep[0m  [420/427], [94mLoss[0m : 2.04030

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.25191
[1mStep[0m  [42/427], [94mLoss[0m : 2.45017
[1mStep[0m  [84/427], [94mLoss[0m : 1.98393
[1mStep[0m  [126/427], [94mLoss[0m : 1.97865
[1mStep[0m  [168/427], [94mLoss[0m : 2.04473
[1mStep[0m  [210/427], [94mLoss[0m : 1.82576
[1mStep[0m  [252/427], [94mLoss[0m : 2.49164
[1mStep[0m  [294/427], [94mLoss[0m : 2.81811
[1mStep[0m  [336/427], [94mLoss[0m : 1.94365
[1mStep[0m  [378/427], [94mLoss[0m : 1.72572
[1mStep[0m  [420/427], [94mLoss[0m : 2.04571

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.33192
[1mStep[0m  [42/427], [94mLoss[0m : 1.46685
[1mStep[0m  [84/427], [94mLoss[0m : 1.70675
[1mStep[0m  [126/427], [94mLoss[0m : 3.36498
[1mStep[0m  [168/427], [94mLoss[0m : 2.02476
[1mStep[0m  [210/427], [94mLoss[0m : 1.76166
[1mStep[0m  [252/427], [94mLoss[0m : 2.14562
[1mStep[0m  [294/427], [94mLoss[0m : 1.73996
[1mStep[0m  [336/427], [94mLoss[0m : 2.40746
[1mStep[0m  [378/427], [94mLoss[0m : 2.02940
[1mStep[0m  [420/427], [94mLoss[0m : 1.51908

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.77782
[1mStep[0m  [42/427], [94mLoss[0m : 1.41425
[1mStep[0m  [84/427], [94mLoss[0m : 2.11965
[1mStep[0m  [126/427], [94mLoss[0m : 1.66866
[1mStep[0m  [168/427], [94mLoss[0m : 1.80842
[1mStep[0m  [210/427], [94mLoss[0m : 2.41736
[1mStep[0m  [252/427], [94mLoss[0m : 2.29146
[1mStep[0m  [294/427], [94mLoss[0m : 1.95926
[1mStep[0m  [336/427], [94mLoss[0m : 1.93702
[1mStep[0m  [378/427], [94mLoss[0m : 2.15764
[1mStep[0m  [420/427], [94mLoss[0m : 1.82606

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.73898
[1mStep[0m  [42/427], [94mLoss[0m : 1.63535
[1mStep[0m  [84/427], [94mLoss[0m : 2.13389
[1mStep[0m  [126/427], [94mLoss[0m : 1.92877
[1mStep[0m  [168/427], [94mLoss[0m : 2.03553
[1mStep[0m  [210/427], [94mLoss[0m : 2.38485
[1mStep[0m  [252/427], [94mLoss[0m : 1.68313
[1mStep[0m  [294/427], [94mLoss[0m : 2.18813
[1mStep[0m  [336/427], [94mLoss[0m : 2.02123
[1mStep[0m  [378/427], [94mLoss[0m : 1.84129
[1mStep[0m  [420/427], [94mLoss[0m : 2.04149

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.934, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.07489
[1mStep[0m  [42/427], [94mLoss[0m : 1.43353
[1mStep[0m  [84/427], [94mLoss[0m : 1.62635
[1mStep[0m  [126/427], [94mLoss[0m : 2.00507
[1mStep[0m  [168/427], [94mLoss[0m : 1.87846
[1mStep[0m  [210/427], [94mLoss[0m : 2.15812
[1mStep[0m  [252/427], [94mLoss[0m : 1.57824
[1mStep[0m  [294/427], [94mLoss[0m : 1.37259
[1mStep[0m  [336/427], [94mLoss[0m : 2.02497
[1mStep[0m  [378/427], [94mLoss[0m : 1.83418
[1mStep[0m  [420/427], [94mLoss[0m : 2.15401

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.908, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.48590
[1mStep[0m  [42/427], [94mLoss[0m : 1.72008
[1mStep[0m  [84/427], [94mLoss[0m : 1.56375
[1mStep[0m  [126/427], [94mLoss[0m : 2.28004
[1mStep[0m  [168/427], [94mLoss[0m : 2.10386
[1mStep[0m  [210/427], [94mLoss[0m : 2.50308
[1mStep[0m  [252/427], [94mLoss[0m : 2.31104
[1mStep[0m  [294/427], [94mLoss[0m : 1.33142
[1mStep[0m  [336/427], [94mLoss[0m : 1.83707
[1mStep[0m  [378/427], [94mLoss[0m : 2.21021
[1mStep[0m  [420/427], [94mLoss[0m : 1.84027

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.866, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.34736
[1mStep[0m  [42/427], [94mLoss[0m : 1.67239
[1mStep[0m  [84/427], [94mLoss[0m : 1.55644
[1mStep[0m  [126/427], [94mLoss[0m : 1.42040
[1mStep[0m  [168/427], [94mLoss[0m : 1.74921
[1mStep[0m  [210/427], [94mLoss[0m : 1.90338
[1mStep[0m  [252/427], [94mLoss[0m : 2.30115
[1mStep[0m  [294/427], [94mLoss[0m : 1.76444
[1mStep[0m  [336/427], [94mLoss[0m : 1.64971
[1mStep[0m  [378/427], [94mLoss[0m : 1.82843
[1mStep[0m  [420/427], [94mLoss[0m : 2.59019

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.525, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.74552
[1mStep[0m  [42/427], [94mLoss[0m : 1.58819
[1mStep[0m  [84/427], [94mLoss[0m : 1.64733
[1mStep[0m  [126/427], [94mLoss[0m : 1.71989
[1mStep[0m  [168/427], [94mLoss[0m : 2.69488
[1mStep[0m  [210/427], [94mLoss[0m : 1.83908
[1mStep[0m  [252/427], [94mLoss[0m : 2.09785
[1mStep[0m  [294/427], [94mLoss[0m : 2.45060
[1mStep[0m  [336/427], [94mLoss[0m : 2.27354
[1mStep[0m  [378/427], [94mLoss[0m : 1.91447
[1mStep[0m  [420/427], [94mLoss[0m : 2.21158

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.50596
[1mStep[0m  [42/427], [94mLoss[0m : 1.81877
[1mStep[0m  [84/427], [94mLoss[0m : 1.79732
[1mStep[0m  [126/427], [94mLoss[0m : 1.98567
[1mStep[0m  [168/427], [94mLoss[0m : 1.56873
[1mStep[0m  [210/427], [94mLoss[0m : 1.75311
[1mStep[0m  [252/427], [94mLoss[0m : 1.84491
[1mStep[0m  [294/427], [94mLoss[0m : 1.66888
[1mStep[0m  [336/427], [94mLoss[0m : 1.83393
[1mStep[0m  [378/427], [94mLoss[0m : 2.37953
[1mStep[0m  [420/427], [94mLoss[0m : 2.06179

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.514, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.42516
[1mStep[0m  [42/427], [94mLoss[0m : 2.45816
[1mStep[0m  [84/427], [94mLoss[0m : 1.74232
[1mStep[0m  [126/427], [94mLoss[0m : 1.60241
[1mStep[0m  [168/427], [94mLoss[0m : 1.60722
[1mStep[0m  [210/427], [94mLoss[0m : 1.72538
[1mStep[0m  [252/427], [94mLoss[0m : 1.34269
[1mStep[0m  [294/427], [94mLoss[0m : 1.61027
[1mStep[0m  [336/427], [94mLoss[0m : 1.58005
[1mStep[0m  [378/427], [94mLoss[0m : 1.71514
[1mStep[0m  [420/427], [94mLoss[0m : 1.64101

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.89383
[1mStep[0m  [42/427], [94mLoss[0m : 2.12833
[1mStep[0m  [84/427], [94mLoss[0m : 1.90169
[1mStep[0m  [126/427], [94mLoss[0m : 1.71963
[1mStep[0m  [168/427], [94mLoss[0m : 1.72328
[1mStep[0m  [210/427], [94mLoss[0m : 1.45879
[1mStep[0m  [252/427], [94mLoss[0m : 1.42783
[1mStep[0m  [294/427], [94mLoss[0m : 1.69071
[1mStep[0m  [336/427], [94mLoss[0m : 1.54982
[1mStep[0m  [378/427], [94mLoss[0m : 1.84586
[1mStep[0m  [420/427], [94mLoss[0m : 1.50669

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.537, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.36385
[1mStep[0m  [42/427], [94mLoss[0m : 1.90525
[1mStep[0m  [84/427], [94mLoss[0m : 1.31033
[1mStep[0m  [126/427], [94mLoss[0m : 1.13648
[1mStep[0m  [168/427], [94mLoss[0m : 1.60123
[1mStep[0m  [210/427], [94mLoss[0m : 1.64588
[1mStep[0m  [252/427], [94mLoss[0m : 1.86625
[1mStep[0m  [294/427], [94mLoss[0m : 1.71530
[1mStep[0m  [336/427], [94mLoss[0m : 1.68118
[1mStep[0m  [378/427], [94mLoss[0m : 2.12508
[1mStep[0m  [420/427], [94mLoss[0m : 1.69580

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.87195
[1mStep[0m  [42/427], [94mLoss[0m : 1.80826
[1mStep[0m  [84/427], [94mLoss[0m : 1.33773
[1mStep[0m  [126/427], [94mLoss[0m : 1.50710
[1mStep[0m  [168/427], [94mLoss[0m : 1.32380
[1mStep[0m  [210/427], [94mLoss[0m : 1.16528
[1mStep[0m  [252/427], [94mLoss[0m : 1.35668
[1mStep[0m  [294/427], [94mLoss[0m : 2.16666
[1mStep[0m  [336/427], [94mLoss[0m : 2.21569
[1mStep[0m  [378/427], [94mLoss[0m : 1.80262
[1mStep[0m  [420/427], [94mLoss[0m : 1.39733

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.502, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.65796
[1mStep[0m  [42/427], [94mLoss[0m : 1.81538
[1mStep[0m  [84/427], [94mLoss[0m : 1.61556
[1mStep[0m  [126/427], [94mLoss[0m : 1.71082
[1mStep[0m  [168/427], [94mLoss[0m : 1.31231
[1mStep[0m  [210/427], [94mLoss[0m : 1.58896
[1mStep[0m  [252/427], [94mLoss[0m : 2.40419
[1mStep[0m  [294/427], [94mLoss[0m : 1.28182
[1mStep[0m  [336/427], [94mLoss[0m : 2.28311
[1mStep[0m  [378/427], [94mLoss[0m : 1.60419
[1mStep[0m  [420/427], [94mLoss[0m : 1.91469

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.443, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.47629
[1mStep[0m  [42/427], [94mLoss[0m : 1.40789
[1mStep[0m  [84/427], [94mLoss[0m : 1.69381
[1mStep[0m  [126/427], [94mLoss[0m : 1.53503
[1mStep[0m  [168/427], [94mLoss[0m : 1.58909
[1mStep[0m  [210/427], [94mLoss[0m : 2.06217
[1mStep[0m  [252/427], [94mLoss[0m : 1.52951
[1mStep[0m  [294/427], [94mLoss[0m : 1.29007
[1mStep[0m  [336/427], [94mLoss[0m : 1.77352
[1mStep[0m  [378/427], [94mLoss[0m : 1.21306
[1mStep[0m  [420/427], [94mLoss[0m : 1.69040

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.550, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.99385
[1mStep[0m  [42/427], [94mLoss[0m : 1.61340
[1mStep[0m  [84/427], [94mLoss[0m : 2.10761
[1mStep[0m  [126/427], [94mLoss[0m : 1.63941
[1mStep[0m  [168/427], [94mLoss[0m : 1.46609
[1mStep[0m  [210/427], [94mLoss[0m : 1.41121
[1mStep[0m  [252/427], [94mLoss[0m : 1.94455
[1mStep[0m  [294/427], [94mLoss[0m : 1.52068
[1mStep[0m  [336/427], [94mLoss[0m : 1.60583
[1mStep[0m  [378/427], [94mLoss[0m : 1.55680
[1mStep[0m  [420/427], [94mLoss[0m : 1.75011

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.449, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.28449
[1mStep[0m  [42/427], [94mLoss[0m : 1.82679
[1mStep[0m  [84/427], [94mLoss[0m : 1.82174
[1mStep[0m  [126/427], [94mLoss[0m : 1.72055
[1mStep[0m  [168/427], [94mLoss[0m : 2.42501
[1mStep[0m  [210/427], [94mLoss[0m : 1.51967
[1mStep[0m  [252/427], [94mLoss[0m : 1.23998
[1mStep[0m  [294/427], [94mLoss[0m : 1.14973
[1mStep[0m  [336/427], [94mLoss[0m : 1.70036
[1mStep[0m  [378/427], [94mLoss[0m : 1.59403
[1mStep[0m  [420/427], [94mLoss[0m : 1.72073

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.559, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.62496
[1mStep[0m  [42/427], [94mLoss[0m : 1.54996
[1mStep[0m  [84/427], [94mLoss[0m : 1.30895
[1mStep[0m  [126/427], [94mLoss[0m : 1.42543
[1mStep[0m  [168/427], [94mLoss[0m : 1.37903
[1mStep[0m  [210/427], [94mLoss[0m : 1.81360
[1mStep[0m  [252/427], [94mLoss[0m : 1.30957
[1mStep[0m  [294/427], [94mLoss[0m : 1.78376
[1mStep[0m  [336/427], [94mLoss[0m : 1.90483
[1mStep[0m  [378/427], [94mLoss[0m : 1.38127
[1mStep[0m  [420/427], [94mLoss[0m : 1.75074

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.558, [92mTest[0m: 2.492, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.37164
[1mStep[0m  [42/427], [94mLoss[0m : 1.56457
[1mStep[0m  [84/427], [94mLoss[0m : 1.65988
[1mStep[0m  [126/427], [94mLoss[0m : 1.57526
[1mStep[0m  [168/427], [94mLoss[0m : 1.81955
[1mStep[0m  [210/427], [94mLoss[0m : 1.37454
[1mStep[0m  [252/427], [94mLoss[0m : 1.19749
[1mStep[0m  [294/427], [94mLoss[0m : 1.40415
[1mStep[0m  [336/427], [94mLoss[0m : 1.42196
[1mStep[0m  [378/427], [94mLoss[0m : 1.66593
[1mStep[0m  [420/427], [94mLoss[0m : 2.06542

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.68528
[1mStep[0m  [42/427], [94mLoss[0m : 1.33261
[1mStep[0m  [84/427], [94mLoss[0m : 1.40705
[1mStep[0m  [126/427], [94mLoss[0m : 1.79361
[1mStep[0m  [168/427], [94mLoss[0m : 2.03321
[1mStep[0m  [210/427], [94mLoss[0m : 1.72246
[1mStep[0m  [252/427], [94mLoss[0m : 1.22013
[1mStep[0m  [294/427], [94mLoss[0m : 1.47146
[1mStep[0m  [336/427], [94mLoss[0m : 1.54751
[1mStep[0m  [378/427], [94mLoss[0m : 1.38501
[1mStep[0m  [420/427], [94mLoss[0m : 1.61594

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.523, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.48646
[1mStep[0m  [42/427], [94mLoss[0m : 1.65644
[1mStep[0m  [84/427], [94mLoss[0m : 1.26225
[1mStep[0m  [126/427], [94mLoss[0m : 1.66911
[1mStep[0m  [168/427], [94mLoss[0m : 1.68799
[1mStep[0m  [210/427], [94mLoss[0m : 0.99863
[1mStep[0m  [252/427], [94mLoss[0m : 1.01790
[1mStep[0m  [294/427], [94mLoss[0m : 1.54092
[1mStep[0m  [336/427], [94mLoss[0m : 1.72345
[1mStep[0m  [378/427], [94mLoss[0m : 1.70861
[1mStep[0m  [420/427], [94mLoss[0m : 2.08213

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.484, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.76830
[1mStep[0m  [42/427], [94mLoss[0m : 1.87314
[1mStep[0m  [84/427], [94mLoss[0m : 1.56051
[1mStep[0m  [126/427], [94mLoss[0m : 1.36333
[1mStep[0m  [168/427], [94mLoss[0m : 1.15986
[1mStep[0m  [210/427], [94mLoss[0m : 1.08254
[1mStep[0m  [252/427], [94mLoss[0m : 1.79336
[1mStep[0m  [294/427], [94mLoss[0m : 1.57825
[1mStep[0m  [336/427], [94mLoss[0m : 1.20276
[1mStep[0m  [378/427], [94mLoss[0m : 1.85227
[1mStep[0m  [420/427], [94mLoss[0m : 1.44646

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.497, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.82869
[1mStep[0m  [42/427], [94mLoss[0m : 1.29019
[1mStep[0m  [84/427], [94mLoss[0m : 1.62231
[1mStep[0m  [126/427], [94mLoss[0m : 1.53703
[1mStep[0m  [168/427], [94mLoss[0m : 1.83206
[1mStep[0m  [210/427], [94mLoss[0m : 1.68829
[1mStep[0m  [252/427], [94mLoss[0m : 1.72678
[1mStep[0m  [294/427], [94mLoss[0m : 1.61498
[1mStep[0m  [336/427], [94mLoss[0m : 1.85667
[1mStep[0m  [378/427], [94mLoss[0m : 1.31180
[1mStep[0m  [420/427], [94mLoss[0m : 1.56146

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.509, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.49408413490779
MAE score P1        2.424866
MAE score P2        2.494084
loss                1.486683
learning_rate       0.007525
batch_size                32
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.5
weight_decay          0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 11.56958
[1mStep[0m  [2/26], [94mLoss[0m : 11.49361
[1mStep[0m  [4/26], [94mLoss[0m : 11.15212
[1mStep[0m  [6/26], [94mLoss[0m : 10.53651
[1mStep[0m  [8/26], [94mLoss[0m : 10.92262
[1mStep[0m  [10/26], [94mLoss[0m : 10.35448
[1mStep[0m  [12/26], [94mLoss[0m : 10.25427
[1mStep[0m  [14/26], [94mLoss[0m : 10.37465
[1mStep[0m  [16/26], [94mLoss[0m : 9.79322
[1mStep[0m  [18/26], [94mLoss[0m : 9.40684
[1mStep[0m  [20/26], [94mLoss[0m : 9.51598
[1mStep[0m  [22/26], [94mLoss[0m : 9.87164
[1mStep[0m  [24/26], [94mLoss[0m : 9.38399

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.292, [92mTest[0m: 11.041, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.07585
[1mStep[0m  [2/26], [94mLoss[0m : 8.65527
[1mStep[0m  [4/26], [94mLoss[0m : 8.77418
[1mStep[0m  [6/26], [94mLoss[0m : 8.54704
[1mStep[0m  [8/26], [94mLoss[0m : 8.44666
[1mStep[0m  [10/26], [94mLoss[0m : 8.35292
[1mStep[0m  [12/26], [94mLoss[0m : 7.93953
[1mStep[0m  [14/26], [94mLoss[0m : 7.89980
[1mStep[0m  [16/26], [94mLoss[0m : 7.62086
[1mStep[0m  [18/26], [94mLoss[0m : 7.87352
[1mStep[0m  [20/26], [94mLoss[0m : 7.43366
[1mStep[0m  [22/26], [94mLoss[0m : 7.06069
[1mStep[0m  [24/26], [94mLoss[0m : 6.86443

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.995, [92mTest[0m: 10.110, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.79003
[1mStep[0m  [2/26], [94mLoss[0m : 6.58875
[1mStep[0m  [4/26], [94mLoss[0m : 6.54317
[1mStep[0m  [6/26], [94mLoss[0m : 6.56635
[1mStep[0m  [8/26], [94mLoss[0m : 5.94819
[1mStep[0m  [10/26], [94mLoss[0m : 5.72280
[1mStep[0m  [12/26], [94mLoss[0m : 5.60642
[1mStep[0m  [14/26], [94mLoss[0m : 5.49976
[1mStep[0m  [16/26], [94mLoss[0m : 5.25544
[1mStep[0m  [18/26], [94mLoss[0m : 5.13948
[1mStep[0m  [20/26], [94mLoss[0m : 4.91729
[1mStep[0m  [22/26], [94mLoss[0m : 5.18887
[1mStep[0m  [24/26], [94mLoss[0m : 4.59719

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.659, [92mTest[0m: 8.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.70608
[1mStep[0m  [2/26], [94mLoss[0m : 4.41228
[1mStep[0m  [4/26], [94mLoss[0m : 4.49427
[1mStep[0m  [6/26], [94mLoss[0m : 4.27707
[1mStep[0m  [8/26], [94mLoss[0m : 4.08328
[1mStep[0m  [10/26], [94mLoss[0m : 3.93167
[1mStep[0m  [12/26], [94mLoss[0m : 4.07970
[1mStep[0m  [14/26], [94mLoss[0m : 4.05804
[1mStep[0m  [16/26], [94mLoss[0m : 3.75314
[1mStep[0m  [18/26], [94mLoss[0m : 3.78970
[1mStep[0m  [20/26], [94mLoss[0m : 3.50709
[1mStep[0m  [22/26], [94mLoss[0m : 3.43428
[1mStep[0m  [24/26], [94mLoss[0m : 3.19918

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.910, [92mTest[0m: 6.292, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.23648
[1mStep[0m  [2/26], [94mLoss[0m : 3.47243
[1mStep[0m  [4/26], [94mLoss[0m : 3.14851
[1mStep[0m  [6/26], [94mLoss[0m : 3.46951
[1mStep[0m  [8/26], [94mLoss[0m : 3.03082
[1mStep[0m  [10/26], [94mLoss[0m : 3.13888
[1mStep[0m  [12/26], [94mLoss[0m : 3.23868
[1mStep[0m  [14/26], [94mLoss[0m : 3.24149
[1mStep[0m  [16/26], [94mLoss[0m : 3.12218
[1mStep[0m  [18/26], [94mLoss[0m : 3.22447
[1mStep[0m  [20/26], [94mLoss[0m : 3.14041
[1mStep[0m  [22/26], [94mLoss[0m : 3.07644
[1mStep[0m  [24/26], [94mLoss[0m : 3.17252

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.202, [92mTest[0m: 4.756, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.04389
[1mStep[0m  [2/26], [94mLoss[0m : 2.88363
[1mStep[0m  [4/26], [94mLoss[0m : 2.93563
[1mStep[0m  [6/26], [94mLoss[0m : 3.03514
[1mStep[0m  [8/26], [94mLoss[0m : 3.08535
[1mStep[0m  [10/26], [94mLoss[0m : 2.99753
[1mStep[0m  [12/26], [94mLoss[0m : 3.07458
[1mStep[0m  [14/26], [94mLoss[0m : 3.00327
[1mStep[0m  [16/26], [94mLoss[0m : 3.02716
[1mStep[0m  [18/26], [94mLoss[0m : 2.86391
[1mStep[0m  [20/26], [94mLoss[0m : 2.94100
[1mStep[0m  [22/26], [94mLoss[0m : 2.94701
[1mStep[0m  [24/26], [94mLoss[0m : 3.05804

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.016, [92mTest[0m: 3.797, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.03287
[1mStep[0m  [2/26], [94mLoss[0m : 3.09047
[1mStep[0m  [4/26], [94mLoss[0m : 2.89834
[1mStep[0m  [6/26], [94mLoss[0m : 2.97522
[1mStep[0m  [8/26], [94mLoss[0m : 2.95207
[1mStep[0m  [10/26], [94mLoss[0m : 2.94036
[1mStep[0m  [12/26], [94mLoss[0m : 2.94276
[1mStep[0m  [14/26], [94mLoss[0m : 3.02287
[1mStep[0m  [16/26], [94mLoss[0m : 2.95124
[1mStep[0m  [18/26], [94mLoss[0m : 2.87875
[1mStep[0m  [20/26], [94mLoss[0m : 2.93920
[1mStep[0m  [22/26], [94mLoss[0m : 3.08528
[1mStep[0m  [24/26], [94mLoss[0m : 2.97932

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.974, [92mTest[0m: 3.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.99888
[1mStep[0m  [2/26], [94mLoss[0m : 2.93259
[1mStep[0m  [4/26], [94mLoss[0m : 2.90751
[1mStep[0m  [6/26], [94mLoss[0m : 2.83820
[1mStep[0m  [8/26], [94mLoss[0m : 2.89551
[1mStep[0m  [10/26], [94mLoss[0m : 3.03596
[1mStep[0m  [12/26], [94mLoss[0m : 3.05714
[1mStep[0m  [14/26], [94mLoss[0m : 2.81953
[1mStep[0m  [16/26], [94mLoss[0m : 2.97806
[1mStep[0m  [18/26], [94mLoss[0m : 2.93824
[1mStep[0m  [20/26], [94mLoss[0m : 2.91141
[1mStep[0m  [22/26], [94mLoss[0m : 3.14931
[1mStep[0m  [24/26], [94mLoss[0m : 3.03700

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.953, [92mTest[0m: 3.221, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.06395
[1mStep[0m  [2/26], [94mLoss[0m : 3.07015
[1mStep[0m  [4/26], [94mLoss[0m : 2.92159
[1mStep[0m  [6/26], [94mLoss[0m : 2.82934
[1mStep[0m  [8/26], [94mLoss[0m : 2.84877
[1mStep[0m  [10/26], [94mLoss[0m : 3.00302
[1mStep[0m  [12/26], [94mLoss[0m : 3.06696
[1mStep[0m  [14/26], [94mLoss[0m : 2.93210
[1mStep[0m  [16/26], [94mLoss[0m : 3.04252
[1mStep[0m  [18/26], [94mLoss[0m : 2.96888
[1mStep[0m  [20/26], [94mLoss[0m : 3.05448
[1mStep[0m  [22/26], [94mLoss[0m : 2.76474
[1mStep[0m  [24/26], [94mLoss[0m : 2.99556

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.945, [92mTest[0m: 3.047, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.89146
[1mStep[0m  [2/26], [94mLoss[0m : 2.91921
[1mStep[0m  [4/26], [94mLoss[0m : 2.91647
[1mStep[0m  [6/26], [94mLoss[0m : 3.09379
[1mStep[0m  [8/26], [94mLoss[0m : 2.80374
[1mStep[0m  [10/26], [94mLoss[0m : 2.82093
[1mStep[0m  [12/26], [94mLoss[0m : 2.77560
[1mStep[0m  [14/26], [94mLoss[0m : 2.89092
[1mStep[0m  [16/26], [94mLoss[0m : 2.86283
[1mStep[0m  [18/26], [94mLoss[0m : 2.90634
[1mStep[0m  [20/26], [94mLoss[0m : 3.04482
[1mStep[0m  [22/26], [94mLoss[0m : 3.00902
[1mStep[0m  [24/26], [94mLoss[0m : 2.89494

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.908, [92mTest[0m: 2.998, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.92001
[1mStep[0m  [2/26], [94mLoss[0m : 3.11942
[1mStep[0m  [4/26], [94mLoss[0m : 2.81262
[1mStep[0m  [6/26], [94mLoss[0m : 2.65957
[1mStep[0m  [8/26], [94mLoss[0m : 2.98304
[1mStep[0m  [10/26], [94mLoss[0m : 2.98243
[1mStep[0m  [12/26], [94mLoss[0m : 2.86923
[1mStep[0m  [14/26], [94mLoss[0m : 2.85183
[1mStep[0m  [16/26], [94mLoss[0m : 2.91977
[1mStep[0m  [18/26], [94mLoss[0m : 2.86629
[1mStep[0m  [20/26], [94mLoss[0m : 2.97680
[1mStep[0m  [22/26], [94mLoss[0m : 2.70223
[1mStep[0m  [24/26], [94mLoss[0m : 2.86260

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.896, [92mTest[0m: 2.973, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.83408
[1mStep[0m  [2/26], [94mLoss[0m : 2.82848
[1mStep[0m  [4/26], [94mLoss[0m : 3.04831
[1mStep[0m  [6/26], [94mLoss[0m : 2.88206
[1mStep[0m  [8/26], [94mLoss[0m : 2.97199
[1mStep[0m  [10/26], [94mLoss[0m : 2.92229
[1mStep[0m  [12/26], [94mLoss[0m : 2.99721
[1mStep[0m  [14/26], [94mLoss[0m : 2.98957
[1mStep[0m  [16/26], [94mLoss[0m : 2.80225
[1mStep[0m  [18/26], [94mLoss[0m : 2.82814
[1mStep[0m  [20/26], [94mLoss[0m : 2.97130
[1mStep[0m  [22/26], [94mLoss[0m : 2.74286
[1mStep[0m  [24/26], [94mLoss[0m : 3.09291

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.905, [92mTest[0m: 2.866, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.81001
[1mStep[0m  [2/26], [94mLoss[0m : 2.86278
[1mStep[0m  [4/26], [94mLoss[0m : 2.99045
[1mStep[0m  [6/26], [94mLoss[0m : 2.72387
[1mStep[0m  [8/26], [94mLoss[0m : 2.92082
[1mStep[0m  [10/26], [94mLoss[0m : 2.75619
[1mStep[0m  [12/26], [94mLoss[0m : 2.98225
[1mStep[0m  [14/26], [94mLoss[0m : 2.73977
[1mStep[0m  [16/26], [94mLoss[0m : 2.89224
[1mStep[0m  [18/26], [94mLoss[0m : 2.92390
[1mStep[0m  [20/26], [94mLoss[0m : 2.84301
[1mStep[0m  [22/26], [94mLoss[0m : 2.94820
[1mStep[0m  [24/26], [94mLoss[0m : 2.88008

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.860, [92mTest[0m: 2.819, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63757
[1mStep[0m  [2/26], [94mLoss[0m : 2.82302
[1mStep[0m  [4/26], [94mLoss[0m : 2.97071
[1mStep[0m  [6/26], [94mLoss[0m : 2.84798
[1mStep[0m  [8/26], [94mLoss[0m : 2.84791
[1mStep[0m  [10/26], [94mLoss[0m : 2.78663
[1mStep[0m  [12/26], [94mLoss[0m : 2.63505
[1mStep[0m  [14/26], [94mLoss[0m : 2.86162
[1mStep[0m  [16/26], [94mLoss[0m : 2.88862
[1mStep[0m  [18/26], [94mLoss[0m : 2.69998
[1mStep[0m  [20/26], [94mLoss[0m : 3.02407
[1mStep[0m  [22/26], [94mLoss[0m : 2.85690
[1mStep[0m  [24/26], [94mLoss[0m : 2.91742

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.846, [92mTest[0m: 2.812, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.83183
[1mStep[0m  [2/26], [94mLoss[0m : 2.78735
[1mStep[0m  [4/26], [94mLoss[0m : 2.78683
[1mStep[0m  [6/26], [94mLoss[0m : 3.05282
[1mStep[0m  [8/26], [94mLoss[0m : 2.69309
[1mStep[0m  [10/26], [94mLoss[0m : 2.65310
[1mStep[0m  [12/26], [94mLoss[0m : 3.05180
[1mStep[0m  [14/26], [94mLoss[0m : 2.82686
[1mStep[0m  [16/26], [94mLoss[0m : 2.97892
[1mStep[0m  [18/26], [94mLoss[0m : 2.98174
[1mStep[0m  [20/26], [94mLoss[0m : 2.89848
[1mStep[0m  [22/26], [94mLoss[0m : 2.77181
[1mStep[0m  [24/26], [94mLoss[0m : 2.64151

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.860, [92mTest[0m: 2.818, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.94190
[1mStep[0m  [2/26], [94mLoss[0m : 2.80527
[1mStep[0m  [4/26], [94mLoss[0m : 2.75504
[1mStep[0m  [6/26], [94mLoss[0m : 2.69472
[1mStep[0m  [8/26], [94mLoss[0m : 2.97635
[1mStep[0m  [10/26], [94mLoss[0m : 2.64038
[1mStep[0m  [12/26], [94mLoss[0m : 2.81648
[1mStep[0m  [14/26], [94mLoss[0m : 2.93735
[1mStep[0m  [16/26], [94mLoss[0m : 2.80480
[1mStep[0m  [18/26], [94mLoss[0m : 2.91711
[1mStep[0m  [20/26], [94mLoss[0m : 2.75516
[1mStep[0m  [22/26], [94mLoss[0m : 2.77472
[1mStep[0m  [24/26], [94mLoss[0m : 2.67732

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.832, [92mTest[0m: 2.745, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.95336
[1mStep[0m  [2/26], [94mLoss[0m : 2.70715
[1mStep[0m  [4/26], [94mLoss[0m : 2.80157
[1mStep[0m  [6/26], [94mLoss[0m : 3.07053
[1mStep[0m  [8/26], [94mLoss[0m : 2.91499
[1mStep[0m  [10/26], [94mLoss[0m : 2.90056
[1mStep[0m  [12/26], [94mLoss[0m : 2.97539
[1mStep[0m  [14/26], [94mLoss[0m : 2.74516
[1mStep[0m  [16/26], [94mLoss[0m : 2.79138
[1mStep[0m  [18/26], [94mLoss[0m : 2.80987
[1mStep[0m  [20/26], [94mLoss[0m : 2.90101
[1mStep[0m  [22/26], [94mLoss[0m : 2.79343
[1mStep[0m  [24/26], [94mLoss[0m : 2.85985

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.843, [92mTest[0m: 2.719, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.88171
[1mStep[0m  [2/26], [94mLoss[0m : 2.73352
[1mStep[0m  [4/26], [94mLoss[0m : 2.82965
[1mStep[0m  [6/26], [94mLoss[0m : 2.70350
[1mStep[0m  [8/26], [94mLoss[0m : 2.82233
[1mStep[0m  [10/26], [94mLoss[0m : 2.84727
[1mStep[0m  [12/26], [94mLoss[0m : 2.84926
[1mStep[0m  [14/26], [94mLoss[0m : 2.99056
[1mStep[0m  [16/26], [94mLoss[0m : 2.85167
[1mStep[0m  [18/26], [94mLoss[0m : 2.81809
[1mStep[0m  [20/26], [94mLoss[0m : 2.80301
[1mStep[0m  [22/26], [94mLoss[0m : 2.80524
[1mStep[0m  [24/26], [94mLoss[0m : 2.66151

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.808, [92mTest[0m: 2.643, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.86250
[1mStep[0m  [2/26], [94mLoss[0m : 2.77906
[1mStep[0m  [4/26], [94mLoss[0m : 2.61213
[1mStep[0m  [6/26], [94mLoss[0m : 2.82929
[1mStep[0m  [8/26], [94mLoss[0m : 2.84278
[1mStep[0m  [10/26], [94mLoss[0m : 2.77176
[1mStep[0m  [12/26], [94mLoss[0m : 2.67287
[1mStep[0m  [14/26], [94mLoss[0m : 2.79744
[1mStep[0m  [16/26], [94mLoss[0m : 3.05311
[1mStep[0m  [18/26], [94mLoss[0m : 2.78889
[1mStep[0m  [20/26], [94mLoss[0m : 2.95262
[1mStep[0m  [22/26], [94mLoss[0m : 2.72272
[1mStep[0m  [24/26], [94mLoss[0m : 2.95734

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.816, [92mTest[0m: 2.696, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79577
[1mStep[0m  [2/26], [94mLoss[0m : 2.77820
[1mStep[0m  [4/26], [94mLoss[0m : 3.00483
[1mStep[0m  [6/26], [94mLoss[0m : 2.83032
[1mStep[0m  [8/26], [94mLoss[0m : 2.70023
[1mStep[0m  [10/26], [94mLoss[0m : 2.77918
[1mStep[0m  [12/26], [94mLoss[0m : 2.74472
[1mStep[0m  [14/26], [94mLoss[0m : 2.83265
[1mStep[0m  [16/26], [94mLoss[0m : 2.81612
[1mStep[0m  [18/26], [94mLoss[0m : 2.96190
[1mStep[0m  [20/26], [94mLoss[0m : 2.94448
[1mStep[0m  [22/26], [94mLoss[0m : 2.82947
[1mStep[0m  [24/26], [94mLoss[0m : 2.57016

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.810, [92mTest[0m: 2.636, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76581
[1mStep[0m  [2/26], [94mLoss[0m : 2.97368
[1mStep[0m  [4/26], [94mLoss[0m : 2.99391
[1mStep[0m  [6/26], [94mLoss[0m : 2.75602
[1mStep[0m  [8/26], [94mLoss[0m : 2.91041
[1mStep[0m  [10/26], [94mLoss[0m : 2.85542
[1mStep[0m  [12/26], [94mLoss[0m : 2.71371
[1mStep[0m  [14/26], [94mLoss[0m : 2.85244
[1mStep[0m  [16/26], [94mLoss[0m : 2.81967
[1mStep[0m  [18/26], [94mLoss[0m : 2.77981
[1mStep[0m  [20/26], [94mLoss[0m : 2.77215
[1mStep[0m  [22/26], [94mLoss[0m : 3.04483
[1mStep[0m  [24/26], [94mLoss[0m : 2.79059

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.839, [92mTest[0m: 2.630, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60992
[1mStep[0m  [2/26], [94mLoss[0m : 2.62243
[1mStep[0m  [4/26], [94mLoss[0m : 2.69199
[1mStep[0m  [6/26], [94mLoss[0m : 2.66733
[1mStep[0m  [8/26], [94mLoss[0m : 2.83534
[1mStep[0m  [10/26], [94mLoss[0m : 2.79916
[1mStep[0m  [12/26], [94mLoss[0m : 2.77701
[1mStep[0m  [14/26], [94mLoss[0m : 2.82045
[1mStep[0m  [16/26], [94mLoss[0m : 2.96744
[1mStep[0m  [18/26], [94mLoss[0m : 2.92039
[1mStep[0m  [20/26], [94mLoss[0m : 2.87640
[1mStep[0m  [22/26], [94mLoss[0m : 2.68502
[1mStep[0m  [24/26], [94mLoss[0m : 2.83002

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.627, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.87615
[1mStep[0m  [2/26], [94mLoss[0m : 2.67179
[1mStep[0m  [4/26], [94mLoss[0m : 2.71924
[1mStep[0m  [6/26], [94mLoss[0m : 2.95876
[1mStep[0m  [8/26], [94mLoss[0m : 2.91238
[1mStep[0m  [10/26], [94mLoss[0m : 2.68276
[1mStep[0m  [12/26], [94mLoss[0m : 2.72687
[1mStep[0m  [14/26], [94mLoss[0m : 2.74376
[1mStep[0m  [16/26], [94mLoss[0m : 2.83832
[1mStep[0m  [18/26], [94mLoss[0m : 2.67007
[1mStep[0m  [20/26], [94mLoss[0m : 2.84273
[1mStep[0m  [22/26], [94mLoss[0m : 2.65714
[1mStep[0m  [24/26], [94mLoss[0m : 2.79098

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.786, [92mTest[0m: 2.605, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.87609
[1mStep[0m  [2/26], [94mLoss[0m : 2.93246
[1mStep[0m  [4/26], [94mLoss[0m : 2.82978
[1mStep[0m  [6/26], [94mLoss[0m : 2.89646
[1mStep[0m  [8/26], [94mLoss[0m : 2.83773
[1mStep[0m  [10/26], [94mLoss[0m : 2.81768
[1mStep[0m  [12/26], [94mLoss[0m : 2.82916
[1mStep[0m  [14/26], [94mLoss[0m : 2.85472
[1mStep[0m  [16/26], [94mLoss[0m : 3.02908
[1mStep[0m  [18/26], [94mLoss[0m : 2.81663
[1mStep[0m  [20/26], [94mLoss[0m : 2.73757
[1mStep[0m  [22/26], [94mLoss[0m : 3.01343
[1mStep[0m  [24/26], [94mLoss[0m : 2.88798

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.809, [92mTest[0m: 2.613, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71210
[1mStep[0m  [2/26], [94mLoss[0m : 2.84242
[1mStep[0m  [4/26], [94mLoss[0m : 2.81194
[1mStep[0m  [6/26], [94mLoss[0m : 2.81857
[1mStep[0m  [8/26], [94mLoss[0m : 2.84405
[1mStep[0m  [10/26], [94mLoss[0m : 2.83539
[1mStep[0m  [12/26], [94mLoss[0m : 2.72196
[1mStep[0m  [14/26], [94mLoss[0m : 2.78472
[1mStep[0m  [16/26], [94mLoss[0m : 2.75820
[1mStep[0m  [18/26], [94mLoss[0m : 2.76353
[1mStep[0m  [20/26], [94mLoss[0m : 2.84273
[1mStep[0m  [22/26], [94mLoss[0m : 2.66757
[1mStep[0m  [24/26], [94mLoss[0m : 2.86618

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.796, [92mTest[0m: 2.592, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65325
[1mStep[0m  [2/26], [94mLoss[0m : 2.84554
[1mStep[0m  [4/26], [94mLoss[0m : 2.63035
[1mStep[0m  [6/26], [94mLoss[0m : 2.77756
[1mStep[0m  [8/26], [94mLoss[0m : 2.65292
[1mStep[0m  [10/26], [94mLoss[0m : 2.78174
[1mStep[0m  [12/26], [94mLoss[0m : 2.82001
[1mStep[0m  [14/26], [94mLoss[0m : 2.70814
[1mStep[0m  [16/26], [94mLoss[0m : 2.80667
[1mStep[0m  [18/26], [94mLoss[0m : 2.63154
[1mStep[0m  [20/26], [94mLoss[0m : 2.80677
[1mStep[0m  [22/26], [94mLoss[0m : 2.79899
[1mStep[0m  [24/26], [94mLoss[0m : 2.79321

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.762, [92mTest[0m: 2.564, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.92953
[1mStep[0m  [2/26], [94mLoss[0m : 2.58105
[1mStep[0m  [4/26], [94mLoss[0m : 2.86373
[1mStep[0m  [6/26], [94mLoss[0m : 2.62957
[1mStep[0m  [8/26], [94mLoss[0m : 2.97090
[1mStep[0m  [10/26], [94mLoss[0m : 2.83918
[1mStep[0m  [12/26], [94mLoss[0m : 2.73945
[1mStep[0m  [14/26], [94mLoss[0m : 3.02504
[1mStep[0m  [16/26], [94mLoss[0m : 2.78453
[1mStep[0m  [18/26], [94mLoss[0m : 2.84936
[1mStep[0m  [20/26], [94mLoss[0m : 2.76602
[1mStep[0m  [22/26], [94mLoss[0m : 2.78156
[1mStep[0m  [24/26], [94mLoss[0m : 2.79572

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.777, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79264
[1mStep[0m  [2/26], [94mLoss[0m : 2.73339
[1mStep[0m  [4/26], [94mLoss[0m : 2.66545
[1mStep[0m  [6/26], [94mLoss[0m : 2.70780
[1mStep[0m  [8/26], [94mLoss[0m : 2.58946
[1mStep[0m  [10/26], [94mLoss[0m : 2.93554
[1mStep[0m  [12/26], [94mLoss[0m : 2.96623
[1mStep[0m  [14/26], [94mLoss[0m : 2.70744
[1mStep[0m  [16/26], [94mLoss[0m : 2.76075
[1mStep[0m  [18/26], [94mLoss[0m : 2.62955
[1mStep[0m  [20/26], [94mLoss[0m : 2.88008
[1mStep[0m  [22/26], [94mLoss[0m : 2.76602
[1mStep[0m  [24/26], [94mLoss[0m : 2.81665

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.789, [92mTest[0m: 2.537, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.85690
[1mStep[0m  [2/26], [94mLoss[0m : 3.06971
[1mStep[0m  [4/26], [94mLoss[0m : 2.74979
[1mStep[0m  [6/26], [94mLoss[0m : 2.61887
[1mStep[0m  [8/26], [94mLoss[0m : 2.67215
[1mStep[0m  [10/26], [94mLoss[0m : 2.67590
[1mStep[0m  [12/26], [94mLoss[0m : 2.79918
[1mStep[0m  [14/26], [94mLoss[0m : 2.96890
[1mStep[0m  [16/26], [94mLoss[0m : 2.89922
[1mStep[0m  [18/26], [94mLoss[0m : 2.83231
[1mStep[0m  [20/26], [94mLoss[0m : 2.85265
[1mStep[0m  [22/26], [94mLoss[0m : 2.72035
[1mStep[0m  [24/26], [94mLoss[0m : 2.80266

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.780, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74263
[1mStep[0m  [2/26], [94mLoss[0m : 2.78630
[1mStep[0m  [4/26], [94mLoss[0m : 2.87896
[1mStep[0m  [6/26], [94mLoss[0m : 2.60804
[1mStep[0m  [8/26], [94mLoss[0m : 2.81488
[1mStep[0m  [10/26], [94mLoss[0m : 2.78661
[1mStep[0m  [12/26], [94mLoss[0m : 2.80892
[1mStep[0m  [14/26], [94mLoss[0m : 2.75886
[1mStep[0m  [16/26], [94mLoss[0m : 2.74414
[1mStep[0m  [18/26], [94mLoss[0m : 2.78756
[1mStep[0m  [20/26], [94mLoss[0m : 2.82499
[1mStep[0m  [22/26], [94mLoss[0m : 2.69328
[1mStep[0m  [24/26], [94mLoss[0m : 2.75504

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.532, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 1 - Evaluation MAE:  2.516368499168983
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.70269
[1mStep[0m  [2/26], [94mLoss[0m : 2.93854
[1mStep[0m  [4/26], [94mLoss[0m : 2.86316
[1mStep[0m  [6/26], [94mLoss[0m : 2.75286
[1mStep[0m  [8/26], [94mLoss[0m : 2.72505
[1mStep[0m  [10/26], [94mLoss[0m : 2.84168
[1mStep[0m  [12/26], [94mLoss[0m : 2.85188
[1mStep[0m  [14/26], [94mLoss[0m : 2.74679
[1mStep[0m  [16/26], [94mLoss[0m : 2.88381
[1mStep[0m  [18/26], [94mLoss[0m : 3.00599
[1mStep[0m  [20/26], [94mLoss[0m : 2.84906
[1mStep[0m  [22/26], [94mLoss[0m : 2.67172
[1mStep[0m  [24/26], [94mLoss[0m : 2.85105

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.831, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.90062
[1mStep[0m  [2/26], [94mLoss[0m : 2.92187
[1mStep[0m  [4/26], [94mLoss[0m : 2.82472
[1mStep[0m  [6/26], [94mLoss[0m : 2.81659
[1mStep[0m  [8/26], [94mLoss[0m : 2.84361
[1mStep[0m  [10/26], [94mLoss[0m : 2.99792
[1mStep[0m  [12/26], [94mLoss[0m : 2.73517
[1mStep[0m  [14/26], [94mLoss[0m : 2.57129
[1mStep[0m  [16/26], [94mLoss[0m : 2.78218
[1mStep[0m  [18/26], [94mLoss[0m : 2.82115
[1mStep[0m  [20/26], [94mLoss[0m : 2.91636
[1mStep[0m  [22/26], [94mLoss[0m : 2.85228
[1mStep[0m  [24/26], [94mLoss[0m : 2.82479

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.823, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79836
[1mStep[0m  [2/26], [94mLoss[0m : 2.76448
[1mStep[0m  [4/26], [94mLoss[0m : 2.70232
[1mStep[0m  [6/26], [94mLoss[0m : 2.84283
[1mStep[0m  [8/26], [94mLoss[0m : 2.69345
[1mStep[0m  [10/26], [94mLoss[0m : 2.98278
[1mStep[0m  [12/26], [94mLoss[0m : 2.72549
[1mStep[0m  [14/26], [94mLoss[0m : 2.69696
[1mStep[0m  [16/26], [94mLoss[0m : 2.62096
[1mStep[0m  [18/26], [94mLoss[0m : 2.85072
[1mStep[0m  [20/26], [94mLoss[0m : 2.68222
[1mStep[0m  [22/26], [94mLoss[0m : 2.66037
[1mStep[0m  [24/26], [94mLoss[0m : 2.77243

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.771, [92mTest[0m: 2.719, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.03307
[1mStep[0m  [2/26], [94mLoss[0m : 2.67539
[1mStep[0m  [4/26], [94mLoss[0m : 2.86421
[1mStep[0m  [6/26], [94mLoss[0m : 2.54223
[1mStep[0m  [8/26], [94mLoss[0m : 2.74538
[1mStep[0m  [10/26], [94mLoss[0m : 2.59718
[1mStep[0m  [12/26], [94mLoss[0m : 2.84096
[1mStep[0m  [14/26], [94mLoss[0m : 2.87526
[1mStep[0m  [16/26], [94mLoss[0m : 2.83044
[1mStep[0m  [18/26], [94mLoss[0m : 2.71084
[1mStep[0m  [20/26], [94mLoss[0m : 2.81819
[1mStep[0m  [22/26], [94mLoss[0m : 2.69707
[1mStep[0m  [24/26], [94mLoss[0m : 2.80399

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.776, [92mTest[0m: 2.689, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.00625
[1mStep[0m  [2/26], [94mLoss[0m : 2.69036
[1mStep[0m  [4/26], [94mLoss[0m : 2.78293
[1mStep[0m  [6/26], [94mLoss[0m : 2.79165
[1mStep[0m  [8/26], [94mLoss[0m : 2.67993
[1mStep[0m  [10/26], [94mLoss[0m : 2.64961
[1mStep[0m  [12/26], [94mLoss[0m : 2.66470
[1mStep[0m  [14/26], [94mLoss[0m : 2.63534
[1mStep[0m  [16/26], [94mLoss[0m : 2.80315
[1mStep[0m  [18/26], [94mLoss[0m : 2.77276
[1mStep[0m  [20/26], [94mLoss[0m : 2.86912
[1mStep[0m  [22/26], [94mLoss[0m : 2.74401
[1mStep[0m  [24/26], [94mLoss[0m : 2.65990

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.734, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76341
[1mStep[0m  [2/26], [94mLoss[0m : 2.85092
[1mStep[0m  [4/26], [94mLoss[0m : 2.85260
[1mStep[0m  [6/26], [94mLoss[0m : 2.70410
[1mStep[0m  [8/26], [94mLoss[0m : 2.78809
[1mStep[0m  [10/26], [94mLoss[0m : 2.63732
[1mStep[0m  [12/26], [94mLoss[0m : 2.93190
[1mStep[0m  [14/26], [94mLoss[0m : 2.79398
[1mStep[0m  [16/26], [94mLoss[0m : 2.92129
[1mStep[0m  [18/26], [94mLoss[0m : 2.72307
[1mStep[0m  [20/26], [94mLoss[0m : 2.68470
[1mStep[0m  [22/26], [94mLoss[0m : 2.96052
[1mStep[0m  [24/26], [94mLoss[0m : 2.53512

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.755, [92mTest[0m: 2.694, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58147
[1mStep[0m  [2/26], [94mLoss[0m : 2.63785
[1mStep[0m  [4/26], [94mLoss[0m : 2.69413
[1mStep[0m  [6/26], [94mLoss[0m : 2.83158
[1mStep[0m  [8/26], [94mLoss[0m : 2.72909
[1mStep[0m  [10/26], [94mLoss[0m : 2.64659
[1mStep[0m  [12/26], [94mLoss[0m : 2.75497
[1mStep[0m  [14/26], [94mLoss[0m : 2.50023
[1mStep[0m  [16/26], [94mLoss[0m : 2.51536
[1mStep[0m  [18/26], [94mLoss[0m : 2.84098
[1mStep[0m  [20/26], [94mLoss[0m : 2.61188
[1mStep[0m  [22/26], [94mLoss[0m : 2.93061
[1mStep[0m  [24/26], [94mLoss[0m : 2.70777

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.728, [92mTest[0m: 2.691, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65354
[1mStep[0m  [2/26], [94mLoss[0m : 2.58065
[1mStep[0m  [4/26], [94mLoss[0m : 2.70399
[1mStep[0m  [6/26], [94mLoss[0m : 2.72142
[1mStep[0m  [8/26], [94mLoss[0m : 2.69043
[1mStep[0m  [10/26], [94mLoss[0m : 2.84731
[1mStep[0m  [12/26], [94mLoss[0m : 2.85526
[1mStep[0m  [14/26], [94mLoss[0m : 2.69003
[1mStep[0m  [16/26], [94mLoss[0m : 2.59379
[1mStep[0m  [18/26], [94mLoss[0m : 2.82519
[1mStep[0m  [20/26], [94mLoss[0m : 2.80358
[1mStep[0m  [22/26], [94mLoss[0m : 2.67315
[1mStep[0m  [24/26], [94mLoss[0m : 2.71504

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.710, [92mTest[0m: 2.633, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61031
[1mStep[0m  [2/26], [94mLoss[0m : 2.59649
[1mStep[0m  [4/26], [94mLoss[0m : 2.74255
[1mStep[0m  [6/26], [94mLoss[0m : 2.62501
[1mStep[0m  [8/26], [94mLoss[0m : 2.57955
[1mStep[0m  [10/26], [94mLoss[0m : 2.64886
[1mStep[0m  [12/26], [94mLoss[0m : 2.60302
[1mStep[0m  [14/26], [94mLoss[0m : 2.67738
[1mStep[0m  [16/26], [94mLoss[0m : 2.60147
[1mStep[0m  [18/26], [94mLoss[0m : 2.69796
[1mStep[0m  [20/26], [94mLoss[0m : 2.73219
[1mStep[0m  [22/26], [94mLoss[0m : 2.77862
[1mStep[0m  [24/26], [94mLoss[0m : 2.77476

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.738, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79883
[1mStep[0m  [2/26], [94mLoss[0m : 2.81668
[1mStep[0m  [4/26], [94mLoss[0m : 2.58618
[1mStep[0m  [6/26], [94mLoss[0m : 2.56298
[1mStep[0m  [8/26], [94mLoss[0m : 2.66646
[1mStep[0m  [10/26], [94mLoss[0m : 2.80238
[1mStep[0m  [12/26], [94mLoss[0m : 2.53543
[1mStep[0m  [14/26], [94mLoss[0m : 2.72121
[1mStep[0m  [16/26], [94mLoss[0m : 2.59958
[1mStep[0m  [18/26], [94mLoss[0m : 2.64000
[1mStep[0m  [20/26], [94mLoss[0m : 2.67633
[1mStep[0m  [22/26], [94mLoss[0m : 2.75435
[1mStep[0m  [24/26], [94mLoss[0m : 2.74559

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.602, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58133
[1mStep[0m  [2/26], [94mLoss[0m : 2.69577
[1mStep[0m  [4/26], [94mLoss[0m : 2.50433
[1mStep[0m  [6/26], [94mLoss[0m : 2.62236
[1mStep[0m  [8/26], [94mLoss[0m : 2.85873
[1mStep[0m  [10/26], [94mLoss[0m : 2.73267
[1mStep[0m  [12/26], [94mLoss[0m : 2.84227
[1mStep[0m  [14/26], [94mLoss[0m : 2.76114
[1mStep[0m  [16/26], [94mLoss[0m : 2.64058
[1mStep[0m  [18/26], [94mLoss[0m : 2.76933
[1mStep[0m  [20/26], [94mLoss[0m : 2.66501
[1mStep[0m  [22/26], [94mLoss[0m : 2.64220
[1mStep[0m  [24/26], [94mLoss[0m : 2.62868

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.627, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65632
[1mStep[0m  [2/26], [94mLoss[0m : 2.55562
[1mStep[0m  [4/26], [94mLoss[0m : 2.84404
[1mStep[0m  [6/26], [94mLoss[0m : 2.59188
[1mStep[0m  [8/26], [94mLoss[0m : 2.69706
[1mStep[0m  [10/26], [94mLoss[0m : 2.64521
[1mStep[0m  [12/26], [94mLoss[0m : 2.75411
[1mStep[0m  [14/26], [94mLoss[0m : 2.61618
[1mStep[0m  [16/26], [94mLoss[0m : 2.76114
[1mStep[0m  [18/26], [94mLoss[0m : 2.59686
[1mStep[0m  [20/26], [94mLoss[0m : 2.52224
[1mStep[0m  [22/26], [94mLoss[0m : 2.58575
[1mStep[0m  [24/26], [94mLoss[0m : 2.59494

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.621, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47541
[1mStep[0m  [2/26], [94mLoss[0m : 2.75781
[1mStep[0m  [4/26], [94mLoss[0m : 2.64840
[1mStep[0m  [6/26], [94mLoss[0m : 2.79864
[1mStep[0m  [8/26], [94mLoss[0m : 2.51926
[1mStep[0m  [10/26], [94mLoss[0m : 2.74241
[1mStep[0m  [12/26], [94mLoss[0m : 2.72745
[1mStep[0m  [14/26], [94mLoss[0m : 2.64754
[1mStep[0m  [16/26], [94mLoss[0m : 2.73533
[1mStep[0m  [18/26], [94mLoss[0m : 2.53841
[1mStep[0m  [20/26], [94mLoss[0m : 2.59329
[1mStep[0m  [22/26], [94mLoss[0m : 2.59793
[1mStep[0m  [24/26], [94mLoss[0m : 2.60792

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.574, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75510
[1mStep[0m  [2/26], [94mLoss[0m : 2.62494
[1mStep[0m  [4/26], [94mLoss[0m : 2.66581
[1mStep[0m  [6/26], [94mLoss[0m : 2.76268
[1mStep[0m  [8/26], [94mLoss[0m : 2.60656
[1mStep[0m  [10/26], [94mLoss[0m : 2.62193
[1mStep[0m  [12/26], [94mLoss[0m : 2.51693
[1mStep[0m  [14/26], [94mLoss[0m : 2.74288
[1mStep[0m  [16/26], [94mLoss[0m : 2.68442
[1mStep[0m  [18/26], [94mLoss[0m : 2.37142
[1mStep[0m  [20/26], [94mLoss[0m : 2.60391
[1mStep[0m  [22/26], [94mLoss[0m : 2.62642
[1mStep[0m  [24/26], [94mLoss[0m : 2.62001

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.550, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46825
[1mStep[0m  [2/26], [94mLoss[0m : 2.59383
[1mStep[0m  [4/26], [94mLoss[0m : 2.64317
[1mStep[0m  [6/26], [94mLoss[0m : 2.71890
[1mStep[0m  [8/26], [94mLoss[0m : 2.54887
[1mStep[0m  [10/26], [94mLoss[0m : 2.73068
[1mStep[0m  [12/26], [94mLoss[0m : 2.62544
[1mStep[0m  [14/26], [94mLoss[0m : 2.41878
[1mStep[0m  [16/26], [94mLoss[0m : 2.58238
[1mStep[0m  [18/26], [94mLoss[0m : 2.78759
[1mStep[0m  [20/26], [94mLoss[0m : 2.75055
[1mStep[0m  [22/26], [94mLoss[0m : 2.53375
[1mStep[0m  [24/26], [94mLoss[0m : 2.56479

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71904
[1mStep[0m  [2/26], [94mLoss[0m : 2.48801
[1mStep[0m  [4/26], [94mLoss[0m : 2.48042
[1mStep[0m  [6/26], [94mLoss[0m : 2.67184
[1mStep[0m  [8/26], [94mLoss[0m : 2.56747
[1mStep[0m  [10/26], [94mLoss[0m : 2.50485
[1mStep[0m  [12/26], [94mLoss[0m : 2.50062
[1mStep[0m  [14/26], [94mLoss[0m : 2.45346
[1mStep[0m  [16/26], [94mLoss[0m : 2.68159
[1mStep[0m  [18/26], [94mLoss[0m : 2.64244
[1mStep[0m  [20/26], [94mLoss[0m : 2.60045
[1mStep[0m  [22/26], [94mLoss[0m : 2.61964
[1mStep[0m  [24/26], [94mLoss[0m : 2.72385

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56915
[1mStep[0m  [2/26], [94mLoss[0m : 2.48823
[1mStep[0m  [4/26], [94mLoss[0m : 2.61343
[1mStep[0m  [6/26], [94mLoss[0m : 2.42171
[1mStep[0m  [8/26], [94mLoss[0m : 2.35291
[1mStep[0m  [10/26], [94mLoss[0m : 2.43531
[1mStep[0m  [12/26], [94mLoss[0m : 2.60522
[1mStep[0m  [14/26], [94mLoss[0m : 2.54400
[1mStep[0m  [16/26], [94mLoss[0m : 2.52694
[1mStep[0m  [18/26], [94mLoss[0m : 2.48005
[1mStep[0m  [20/26], [94mLoss[0m : 2.74731
[1mStep[0m  [22/26], [94mLoss[0m : 2.62638
[1mStep[0m  [24/26], [94mLoss[0m : 2.72619

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.543, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56234
[1mStep[0m  [2/26], [94mLoss[0m : 2.49709
[1mStep[0m  [4/26], [94mLoss[0m : 2.59588
[1mStep[0m  [6/26], [94mLoss[0m : 2.56642
[1mStep[0m  [8/26], [94mLoss[0m : 2.58015
[1mStep[0m  [10/26], [94mLoss[0m : 2.54771
[1mStep[0m  [12/26], [94mLoss[0m : 2.43079
[1mStep[0m  [14/26], [94mLoss[0m : 2.58398
[1mStep[0m  [16/26], [94mLoss[0m : 2.72961
[1mStep[0m  [18/26], [94mLoss[0m : 2.37888
[1mStep[0m  [20/26], [94mLoss[0m : 2.59301
[1mStep[0m  [22/26], [94mLoss[0m : 2.51822
[1mStep[0m  [24/26], [94mLoss[0m : 2.54961

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51907
[1mStep[0m  [2/26], [94mLoss[0m : 2.53593
[1mStep[0m  [4/26], [94mLoss[0m : 2.54615
[1mStep[0m  [6/26], [94mLoss[0m : 2.65979
[1mStep[0m  [8/26], [94mLoss[0m : 2.53655
[1mStep[0m  [10/26], [94mLoss[0m : 2.45588
[1mStep[0m  [12/26], [94mLoss[0m : 2.46104
[1mStep[0m  [14/26], [94mLoss[0m : 2.61839
[1mStep[0m  [16/26], [94mLoss[0m : 2.54351
[1mStep[0m  [18/26], [94mLoss[0m : 2.49293
[1mStep[0m  [20/26], [94mLoss[0m : 2.53469
[1mStep[0m  [22/26], [94mLoss[0m : 2.54287
[1mStep[0m  [24/26], [94mLoss[0m : 2.66791

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43699
[1mStep[0m  [2/26], [94mLoss[0m : 2.58288
[1mStep[0m  [4/26], [94mLoss[0m : 2.61702
[1mStep[0m  [6/26], [94mLoss[0m : 2.42352
[1mStep[0m  [8/26], [94mLoss[0m : 2.57387
[1mStep[0m  [10/26], [94mLoss[0m : 2.40580
[1mStep[0m  [12/26], [94mLoss[0m : 2.57774
[1mStep[0m  [14/26], [94mLoss[0m : 2.39412
[1mStep[0m  [16/26], [94mLoss[0m : 2.58263
[1mStep[0m  [18/26], [94mLoss[0m : 2.65979
[1mStep[0m  [20/26], [94mLoss[0m : 2.38729
[1mStep[0m  [22/26], [94mLoss[0m : 2.45249
[1mStep[0m  [24/26], [94mLoss[0m : 2.69227

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46364
[1mStep[0m  [2/26], [94mLoss[0m : 2.59753
[1mStep[0m  [4/26], [94mLoss[0m : 2.58682
[1mStep[0m  [6/26], [94mLoss[0m : 2.48957
[1mStep[0m  [8/26], [94mLoss[0m : 2.49076
[1mStep[0m  [10/26], [94mLoss[0m : 2.60439
[1mStep[0m  [12/26], [94mLoss[0m : 2.50791
[1mStep[0m  [14/26], [94mLoss[0m : 2.48561
[1mStep[0m  [16/26], [94mLoss[0m : 2.53457
[1mStep[0m  [18/26], [94mLoss[0m : 2.42793
[1mStep[0m  [20/26], [94mLoss[0m : 2.52048
[1mStep[0m  [22/26], [94mLoss[0m : 2.44433
[1mStep[0m  [24/26], [94mLoss[0m : 2.59046

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51601
[1mStep[0m  [2/26], [94mLoss[0m : 2.42866
[1mStep[0m  [4/26], [94mLoss[0m : 2.33118
[1mStep[0m  [6/26], [94mLoss[0m : 2.41809
[1mStep[0m  [8/26], [94mLoss[0m : 2.48304
[1mStep[0m  [10/26], [94mLoss[0m : 2.39962
[1mStep[0m  [12/26], [94mLoss[0m : 2.58135
[1mStep[0m  [14/26], [94mLoss[0m : 2.57240
[1mStep[0m  [16/26], [94mLoss[0m : 2.44670
[1mStep[0m  [18/26], [94mLoss[0m : 2.53870
[1mStep[0m  [20/26], [94mLoss[0m : 2.40415
[1mStep[0m  [22/26], [94mLoss[0m : 2.54065
[1mStep[0m  [24/26], [94mLoss[0m : 2.55509

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41635
[1mStep[0m  [2/26], [94mLoss[0m : 2.34475
[1mStep[0m  [4/26], [94mLoss[0m : 2.54666
[1mStep[0m  [6/26], [94mLoss[0m : 2.43749
[1mStep[0m  [8/26], [94mLoss[0m : 2.53550
[1mStep[0m  [10/26], [94mLoss[0m : 2.61801
[1mStep[0m  [12/26], [94mLoss[0m : 2.41445
[1mStep[0m  [14/26], [94mLoss[0m : 2.58409
[1mStep[0m  [16/26], [94mLoss[0m : 2.55309
[1mStep[0m  [18/26], [94mLoss[0m : 2.50209
[1mStep[0m  [20/26], [94mLoss[0m : 2.38090
[1mStep[0m  [22/26], [94mLoss[0m : 2.71712
[1mStep[0m  [24/26], [94mLoss[0m : 2.53203

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48989
[1mStep[0m  [2/26], [94mLoss[0m : 2.34645
[1mStep[0m  [4/26], [94mLoss[0m : 2.52627
[1mStep[0m  [6/26], [94mLoss[0m : 2.55319
[1mStep[0m  [8/26], [94mLoss[0m : 2.44380
[1mStep[0m  [10/26], [94mLoss[0m : 2.32433
[1mStep[0m  [12/26], [94mLoss[0m : 2.40666
[1mStep[0m  [14/26], [94mLoss[0m : 2.50562
[1mStep[0m  [16/26], [94mLoss[0m : 2.51162
[1mStep[0m  [18/26], [94mLoss[0m : 2.45766
[1mStep[0m  [20/26], [94mLoss[0m : 2.40296
[1mStep[0m  [22/26], [94mLoss[0m : 2.48256
[1mStep[0m  [24/26], [94mLoss[0m : 2.33543

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35927
[1mStep[0m  [2/26], [94mLoss[0m : 2.37415
[1mStep[0m  [4/26], [94mLoss[0m : 2.46856
[1mStep[0m  [6/26], [94mLoss[0m : 2.70048
[1mStep[0m  [8/26], [94mLoss[0m : 2.40416
[1mStep[0m  [10/26], [94mLoss[0m : 2.42763
[1mStep[0m  [12/26], [94mLoss[0m : 2.73052
[1mStep[0m  [14/26], [94mLoss[0m : 2.44456
[1mStep[0m  [16/26], [94mLoss[0m : 2.41526
[1mStep[0m  [18/26], [94mLoss[0m : 2.37366
[1mStep[0m  [20/26], [94mLoss[0m : 2.44971
[1mStep[0m  [22/26], [94mLoss[0m : 2.57623
[1mStep[0m  [24/26], [94mLoss[0m : 2.43810

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19888
[1mStep[0m  [2/26], [94mLoss[0m : 2.36014
[1mStep[0m  [4/26], [94mLoss[0m : 2.34460
[1mStep[0m  [6/26], [94mLoss[0m : 2.34229
[1mStep[0m  [8/26], [94mLoss[0m : 2.36763
[1mStep[0m  [10/26], [94mLoss[0m : 2.53996
[1mStep[0m  [12/26], [94mLoss[0m : 2.60988
[1mStep[0m  [14/26], [94mLoss[0m : 2.46855
[1mStep[0m  [16/26], [94mLoss[0m : 2.49542
[1mStep[0m  [18/26], [94mLoss[0m : 2.46012
[1mStep[0m  [20/26], [94mLoss[0m : 2.30591
[1mStep[0m  [22/26], [94mLoss[0m : 2.40935
[1mStep[0m  [24/26], [94mLoss[0m : 2.38481

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.483, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27992
[1mStep[0m  [2/26], [94mLoss[0m : 2.44674
[1mStep[0m  [4/26], [94mLoss[0m : 2.36132
[1mStep[0m  [6/26], [94mLoss[0m : 2.48906
[1mStep[0m  [8/26], [94mLoss[0m : 2.40817
[1mStep[0m  [10/26], [94mLoss[0m : 2.49575
[1mStep[0m  [12/26], [94mLoss[0m : 2.44474
[1mStep[0m  [14/26], [94mLoss[0m : 2.41856
[1mStep[0m  [16/26], [94mLoss[0m : 2.29380
[1mStep[0m  [18/26], [94mLoss[0m : 2.38215
[1mStep[0m  [20/26], [94mLoss[0m : 2.47532
[1mStep[0m  [22/26], [94mLoss[0m : 2.29514
[1mStep[0m  [24/26], [94mLoss[0m : 2.48101

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.499, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32047
[1mStep[0m  [2/26], [94mLoss[0m : 2.42531
[1mStep[0m  [4/26], [94mLoss[0m : 2.23999
[1mStep[0m  [6/26], [94mLoss[0m : 2.43616
[1mStep[0m  [8/26], [94mLoss[0m : 2.34691
[1mStep[0m  [10/26], [94mLoss[0m : 2.40436
[1mStep[0m  [12/26], [94mLoss[0m : 2.44134
[1mStep[0m  [14/26], [94mLoss[0m : 2.48872
[1mStep[0m  [16/26], [94mLoss[0m : 2.34995
[1mStep[0m  [18/26], [94mLoss[0m : 2.56889
[1mStep[0m  [20/26], [94mLoss[0m : 2.56660
[1mStep[0m  [22/26], [94mLoss[0m : 2.31548
[1mStep[0m  [24/26], [94mLoss[0m : 2.58146

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34188
[1mStep[0m  [2/26], [94mLoss[0m : 2.40041
[1mStep[0m  [4/26], [94mLoss[0m : 2.44364
[1mStep[0m  [6/26], [94mLoss[0m : 2.35078
[1mStep[0m  [8/26], [94mLoss[0m : 2.26976
[1mStep[0m  [10/26], [94mLoss[0m : 2.20046
[1mStep[0m  [12/26], [94mLoss[0m : 2.34828
[1mStep[0m  [14/26], [94mLoss[0m : 2.38525
[1mStep[0m  [16/26], [94mLoss[0m : 2.36979
[1mStep[0m  [18/26], [94mLoss[0m : 2.39475
[1mStep[0m  [20/26], [94mLoss[0m : 2.41876
[1mStep[0m  [22/26], [94mLoss[0m : 2.43225
[1mStep[0m  [24/26], [94mLoss[0m : 2.34201

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43794
[1mStep[0m  [2/26], [94mLoss[0m : 2.28409
[1mStep[0m  [4/26], [94mLoss[0m : 2.25874
[1mStep[0m  [6/26], [94mLoss[0m : 2.29485
[1mStep[0m  [8/26], [94mLoss[0m : 2.34981
[1mStep[0m  [10/26], [94mLoss[0m : 2.28596
[1mStep[0m  [12/26], [94mLoss[0m : 2.40672
[1mStep[0m  [14/26], [94mLoss[0m : 2.39655
[1mStep[0m  [16/26], [94mLoss[0m : 2.37368
[1mStep[0m  [18/26], [94mLoss[0m : 2.40540
[1mStep[0m  [20/26], [94mLoss[0m : 2.46161
[1mStep[0m  [22/26], [94mLoss[0m : 2.36728
[1mStep[0m  [24/26], [94mLoss[0m : 2.31714

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.509
====================================

Phase 2 - Evaluation MAE:  2.5090858386113095
MAE score P1       2.516368
MAE score P2       2.509086
loss               2.377283
learning_rate       0.00505
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay          0.001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.69143
[1mStep[0m  [2/26], [94mLoss[0m : 10.92234
[1mStep[0m  [4/26], [94mLoss[0m : 10.89695
[1mStep[0m  [6/26], [94mLoss[0m : 10.79386
[1mStep[0m  [8/26], [94mLoss[0m : 10.90938
[1mStep[0m  [10/26], [94mLoss[0m : 10.85879
[1mStep[0m  [12/26], [94mLoss[0m : 10.70984
[1mStep[0m  [14/26], [94mLoss[0m : 10.81788
[1mStep[0m  [16/26], [94mLoss[0m : 10.70583
[1mStep[0m  [18/26], [94mLoss[0m : 11.02940
[1mStep[0m  [20/26], [94mLoss[0m : 10.58523
[1mStep[0m  [22/26], [94mLoss[0m : 10.62317
[1mStep[0m  [24/26], [94mLoss[0m : 10.62013

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.767, [92mTest[0m: 10.818, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.72107
[1mStep[0m  [2/26], [94mLoss[0m : 10.71608
[1mStep[0m  [4/26], [94mLoss[0m : 10.73679
[1mStep[0m  [6/26], [94mLoss[0m : 10.27314
[1mStep[0m  [8/26], [94mLoss[0m : 10.55556
[1mStep[0m  [10/26], [94mLoss[0m : 10.58587
[1mStep[0m  [12/26], [94mLoss[0m : 10.32531
[1mStep[0m  [14/26], [94mLoss[0m : 10.91801
[1mStep[0m  [16/26], [94mLoss[0m : 10.70981
[1mStep[0m  [18/26], [94mLoss[0m : 10.54978
[1mStep[0m  [20/26], [94mLoss[0m : 10.36327
[1mStep[0m  [22/26], [94mLoss[0m : 10.42146
[1mStep[0m  [24/26], [94mLoss[0m : 10.51389

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.526, [92mTest[0m: 10.656, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.55870
[1mStep[0m  [2/26], [94mLoss[0m : 10.54674
[1mStep[0m  [4/26], [94mLoss[0m : 10.42395
[1mStep[0m  [6/26], [94mLoss[0m : 10.43832
[1mStep[0m  [8/26], [94mLoss[0m : 10.26779
[1mStep[0m  [10/26], [94mLoss[0m : 10.06959
[1mStep[0m  [12/26], [94mLoss[0m : 10.23654
[1mStep[0m  [14/26], [94mLoss[0m : 10.27349
[1mStep[0m  [16/26], [94mLoss[0m : 10.42387
[1mStep[0m  [18/26], [94mLoss[0m : 10.04954
[1mStep[0m  [20/26], [94mLoss[0m : 10.16551
[1mStep[0m  [22/26], [94mLoss[0m : 10.33111
[1mStep[0m  [24/26], [94mLoss[0m : 10.20051

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.283, [92mTest[0m: 10.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.23359
[1mStep[0m  [2/26], [94mLoss[0m : 10.05047
[1mStep[0m  [4/26], [94mLoss[0m : 10.16335
[1mStep[0m  [6/26], [94mLoss[0m : 10.21356
[1mStep[0m  [8/26], [94mLoss[0m : 10.08571
[1mStep[0m  [10/26], [94mLoss[0m : 10.20349
[1mStep[0m  [12/26], [94mLoss[0m : 10.26368
[1mStep[0m  [14/26], [94mLoss[0m : 9.97653
[1mStep[0m  [16/26], [94mLoss[0m : 10.01367
[1mStep[0m  [18/26], [94mLoss[0m : 9.81539
[1mStep[0m  [20/26], [94mLoss[0m : 9.97700
[1mStep[0m  [22/26], [94mLoss[0m : 10.05972
[1mStep[0m  [24/26], [94mLoss[0m : 10.00581

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.026, [92mTest[0m: 9.982, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.89555
[1mStep[0m  [2/26], [94mLoss[0m : 9.85620
[1mStep[0m  [4/26], [94mLoss[0m : 9.58108
[1mStep[0m  [6/26], [94mLoss[0m : 9.60384
[1mStep[0m  [8/26], [94mLoss[0m : 10.00341
[1mStep[0m  [10/26], [94mLoss[0m : 9.95308
[1mStep[0m  [12/26], [94mLoss[0m : 9.71757
[1mStep[0m  [14/26], [94mLoss[0m : 9.76356
[1mStep[0m  [16/26], [94mLoss[0m : 9.64885
[1mStep[0m  [18/26], [94mLoss[0m : 9.67744
[1mStep[0m  [20/26], [94mLoss[0m : 9.77935
[1mStep[0m  [22/26], [94mLoss[0m : 9.76919
[1mStep[0m  [24/26], [94mLoss[0m : 9.54704

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.739, [92mTest[0m: 9.602, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.55772
[1mStep[0m  [2/26], [94mLoss[0m : 9.75178
[1mStep[0m  [4/26], [94mLoss[0m : 9.44517
[1mStep[0m  [6/26], [94mLoss[0m : 9.42248
[1mStep[0m  [8/26], [94mLoss[0m : 9.53762
[1mStep[0m  [10/26], [94mLoss[0m : 9.54165
[1mStep[0m  [12/26], [94mLoss[0m : 9.62666
[1mStep[0m  [14/26], [94mLoss[0m : 9.47081
[1mStep[0m  [16/26], [94mLoss[0m : 9.19776
[1mStep[0m  [18/26], [94mLoss[0m : 9.22516
[1mStep[0m  [20/26], [94mLoss[0m : 9.47807
[1mStep[0m  [22/26], [94mLoss[0m : 9.52323
[1mStep[0m  [24/26], [94mLoss[0m : 8.99069

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.426, [92mTest[0m: 9.220, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.07859
[1mStep[0m  [2/26], [94mLoss[0m : 9.42430
[1mStep[0m  [4/26], [94mLoss[0m : 9.35771
[1mStep[0m  [6/26], [94mLoss[0m : 9.25055
[1mStep[0m  [8/26], [94mLoss[0m : 9.19187
[1mStep[0m  [10/26], [94mLoss[0m : 8.95957
[1mStep[0m  [12/26], [94mLoss[0m : 9.58914
[1mStep[0m  [14/26], [94mLoss[0m : 8.87229
[1mStep[0m  [16/26], [94mLoss[0m : 9.23460
[1mStep[0m  [18/26], [94mLoss[0m : 9.44798
[1mStep[0m  [20/26], [94mLoss[0m : 9.02532
[1mStep[0m  [22/26], [94mLoss[0m : 8.83536
[1mStep[0m  [24/26], [94mLoss[0m : 8.86304

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.095, [92mTest[0m: 8.733, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.62496
[1mStep[0m  [2/26], [94mLoss[0m : 8.82659
[1mStep[0m  [4/26], [94mLoss[0m : 9.00755
[1mStep[0m  [6/26], [94mLoss[0m : 8.73251
[1mStep[0m  [8/26], [94mLoss[0m : 9.17984
[1mStep[0m  [10/26], [94mLoss[0m : 8.72680
[1mStep[0m  [12/26], [94mLoss[0m : 8.61360
[1mStep[0m  [14/26], [94mLoss[0m : 8.75023
[1mStep[0m  [16/26], [94mLoss[0m : 8.93117
[1mStep[0m  [18/26], [94mLoss[0m : 8.90582
[1mStep[0m  [20/26], [94mLoss[0m : 8.57487
[1mStep[0m  [22/26], [94mLoss[0m : 8.48295
[1mStep[0m  [24/26], [94mLoss[0m : 8.58181

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.740, [92mTest[0m: 8.292, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.65794
[1mStep[0m  [2/26], [94mLoss[0m : 8.37504
[1mStep[0m  [4/26], [94mLoss[0m : 8.33921
[1mStep[0m  [6/26], [94mLoss[0m : 8.59394
[1mStep[0m  [8/26], [94mLoss[0m : 8.62747
[1mStep[0m  [10/26], [94mLoss[0m : 8.21796
[1mStep[0m  [12/26], [94mLoss[0m : 8.40633
[1mStep[0m  [14/26], [94mLoss[0m : 8.08775
[1mStep[0m  [16/26], [94mLoss[0m : 8.41382
[1mStep[0m  [18/26], [94mLoss[0m : 8.35621
[1mStep[0m  [20/26], [94mLoss[0m : 8.45328
[1mStep[0m  [22/26], [94mLoss[0m : 8.51478
[1mStep[0m  [24/26], [94mLoss[0m : 8.08240

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.373, [92mTest[0m: 7.938, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.19880
[1mStep[0m  [2/26], [94mLoss[0m : 8.20257
[1mStep[0m  [4/26], [94mLoss[0m : 8.31885
[1mStep[0m  [6/26], [94mLoss[0m : 8.32578
[1mStep[0m  [8/26], [94mLoss[0m : 8.16903
[1mStep[0m  [10/26], [94mLoss[0m : 8.03820
[1mStep[0m  [12/26], [94mLoss[0m : 8.01838
[1mStep[0m  [14/26], [94mLoss[0m : 7.95238
[1mStep[0m  [16/26], [94mLoss[0m : 8.01145
[1mStep[0m  [18/26], [94mLoss[0m : 7.80991
[1mStep[0m  [20/26], [94mLoss[0m : 7.93680
[1mStep[0m  [22/26], [94mLoss[0m : 7.62808
[1mStep[0m  [24/26], [94mLoss[0m : 7.86974

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.022, [92mTest[0m: 7.553, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.02714
[1mStep[0m  [2/26], [94mLoss[0m : 7.77602
[1mStep[0m  [4/26], [94mLoss[0m : 7.63006
[1mStep[0m  [6/26], [94mLoss[0m : 7.68385
[1mStep[0m  [8/26], [94mLoss[0m : 7.70501
[1mStep[0m  [10/26], [94mLoss[0m : 7.92465
[1mStep[0m  [12/26], [94mLoss[0m : 7.79558
[1mStep[0m  [14/26], [94mLoss[0m : 7.69213
[1mStep[0m  [16/26], [94mLoss[0m : 7.65977
[1mStep[0m  [18/26], [94mLoss[0m : 7.61456
[1mStep[0m  [20/26], [94mLoss[0m : 7.59078
[1mStep[0m  [22/26], [94mLoss[0m : 7.42131
[1mStep[0m  [24/26], [94mLoss[0m : 7.52120

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.696, [92mTest[0m: 7.102, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.37875
[1mStep[0m  [2/26], [94mLoss[0m : 7.41742
[1mStep[0m  [4/26], [94mLoss[0m : 7.88850
[1mStep[0m  [6/26], [94mLoss[0m : 7.39162
[1mStep[0m  [8/26], [94mLoss[0m : 7.46466
[1mStep[0m  [10/26], [94mLoss[0m : 7.37598
[1mStep[0m  [12/26], [94mLoss[0m : 7.40931
[1mStep[0m  [14/26], [94mLoss[0m : 7.34454
[1mStep[0m  [16/26], [94mLoss[0m : 7.33741
[1mStep[0m  [18/26], [94mLoss[0m : 7.31487
[1mStep[0m  [20/26], [94mLoss[0m : 7.45783
[1mStep[0m  [22/26], [94mLoss[0m : 7.18031
[1mStep[0m  [24/26], [94mLoss[0m : 7.42506

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.369, [92mTest[0m: 6.768, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.28242
[1mStep[0m  [2/26], [94mLoss[0m : 7.21374
[1mStep[0m  [4/26], [94mLoss[0m : 7.25554
[1mStep[0m  [6/26], [94mLoss[0m : 6.90548
[1mStep[0m  [8/26], [94mLoss[0m : 7.04904
[1mStep[0m  [10/26], [94mLoss[0m : 6.85391
[1mStep[0m  [12/26], [94mLoss[0m : 7.24283
[1mStep[0m  [14/26], [94mLoss[0m : 7.05267
[1mStep[0m  [16/26], [94mLoss[0m : 7.05341
[1mStep[0m  [18/26], [94mLoss[0m : 6.82801
[1mStep[0m  [20/26], [94mLoss[0m : 7.10869
[1mStep[0m  [22/26], [94mLoss[0m : 7.06741
[1mStep[0m  [24/26], [94mLoss[0m : 6.99952

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.087, [92mTest[0m: 6.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.96189
[1mStep[0m  [2/26], [94mLoss[0m : 6.93855
[1mStep[0m  [4/26], [94mLoss[0m : 6.92072
[1mStep[0m  [6/26], [94mLoss[0m : 6.90920
[1mStep[0m  [8/26], [94mLoss[0m : 6.79332
[1mStep[0m  [10/26], [94mLoss[0m : 6.94482
[1mStep[0m  [12/26], [94mLoss[0m : 7.16368
[1mStep[0m  [14/26], [94mLoss[0m : 6.73832
[1mStep[0m  [16/26], [94mLoss[0m : 6.91285
[1mStep[0m  [18/26], [94mLoss[0m : 6.64284
[1mStep[0m  [20/26], [94mLoss[0m : 6.62154
[1mStep[0m  [22/26], [94mLoss[0m : 6.81822
[1mStep[0m  [24/26], [94mLoss[0m : 6.93675

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.813, [92mTest[0m: 6.143, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.71302
[1mStep[0m  [2/26], [94mLoss[0m : 6.93042
[1mStep[0m  [4/26], [94mLoss[0m : 6.59026
[1mStep[0m  [6/26], [94mLoss[0m : 6.96255
[1mStep[0m  [8/26], [94mLoss[0m : 6.66373
[1mStep[0m  [10/26], [94mLoss[0m : 6.30752
[1mStep[0m  [12/26], [94mLoss[0m : 6.62398
[1mStep[0m  [14/26], [94mLoss[0m : 6.42089
[1mStep[0m  [16/26], [94mLoss[0m : 6.21828
[1mStep[0m  [18/26], [94mLoss[0m : 6.51883
[1mStep[0m  [20/26], [94mLoss[0m : 6.59908
[1mStep[0m  [22/26], [94mLoss[0m : 6.39601
[1mStep[0m  [24/26], [94mLoss[0m : 6.12869

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.553, [92mTest[0m: 5.930, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.26978
[1mStep[0m  [2/26], [94mLoss[0m : 6.22563
[1mStep[0m  [4/26], [94mLoss[0m : 6.19065
[1mStep[0m  [6/26], [94mLoss[0m : 6.03213
[1mStep[0m  [8/26], [94mLoss[0m : 6.08679
[1mStep[0m  [10/26], [94mLoss[0m : 6.34938
[1mStep[0m  [12/26], [94mLoss[0m : 6.25460
[1mStep[0m  [14/26], [94mLoss[0m : 6.22392
[1mStep[0m  [16/26], [94mLoss[0m : 6.41589
[1mStep[0m  [18/26], [94mLoss[0m : 6.29544
[1mStep[0m  [20/26], [94mLoss[0m : 6.16189
[1mStep[0m  [22/26], [94mLoss[0m : 6.21178
[1mStep[0m  [24/26], [94mLoss[0m : 6.03352

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.250, [92mTest[0m: 5.570, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.30422
[1mStep[0m  [2/26], [94mLoss[0m : 6.04846
[1mStep[0m  [4/26], [94mLoss[0m : 6.05913
[1mStep[0m  [6/26], [94mLoss[0m : 6.13673
[1mStep[0m  [8/26], [94mLoss[0m : 6.05353
[1mStep[0m  [10/26], [94mLoss[0m : 6.05138
[1mStep[0m  [12/26], [94mLoss[0m : 5.84356
[1mStep[0m  [14/26], [94mLoss[0m : 5.80746
[1mStep[0m  [16/26], [94mLoss[0m : 5.74566
[1mStep[0m  [18/26], [94mLoss[0m : 5.91472
[1mStep[0m  [20/26], [94mLoss[0m : 6.06089
[1mStep[0m  [22/26], [94mLoss[0m : 6.13352
[1mStep[0m  [24/26], [94mLoss[0m : 5.68339

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.970, [92mTest[0m: 5.305, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.95752
[1mStep[0m  [2/26], [94mLoss[0m : 5.99005
[1mStep[0m  [4/26], [94mLoss[0m : 5.96890
[1mStep[0m  [6/26], [94mLoss[0m : 5.53044
[1mStep[0m  [8/26], [94mLoss[0m : 5.58892
[1mStep[0m  [10/26], [94mLoss[0m : 5.72297
[1mStep[0m  [12/26], [94mLoss[0m : 5.73365
[1mStep[0m  [14/26], [94mLoss[0m : 5.77538
[1mStep[0m  [16/26], [94mLoss[0m : 5.36737
[1mStep[0m  [18/26], [94mLoss[0m : 5.62163
[1mStep[0m  [20/26], [94mLoss[0m : 5.67669
[1mStep[0m  [22/26], [94mLoss[0m : 5.43910
[1mStep[0m  [24/26], [94mLoss[0m : 5.44801

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.663, [92mTest[0m: 4.978, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.29203
[1mStep[0m  [2/26], [94mLoss[0m : 5.52004
[1mStep[0m  [4/26], [94mLoss[0m : 5.24864
[1mStep[0m  [6/26], [94mLoss[0m : 5.58450
[1mStep[0m  [8/26], [94mLoss[0m : 5.43225
[1mStep[0m  [10/26], [94mLoss[0m : 5.17592
[1mStep[0m  [12/26], [94mLoss[0m : 5.48838
[1mStep[0m  [14/26], [94mLoss[0m : 5.47336
[1mStep[0m  [16/26], [94mLoss[0m : 5.19149
[1mStep[0m  [18/26], [94mLoss[0m : 5.10608
[1mStep[0m  [20/26], [94mLoss[0m : 5.39724
[1mStep[0m  [22/26], [94mLoss[0m : 5.15865
[1mStep[0m  [24/26], [94mLoss[0m : 5.34864

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.347, [92mTest[0m: 4.672, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.05795
[1mStep[0m  [2/26], [94mLoss[0m : 5.05411
[1mStep[0m  [4/26], [94mLoss[0m : 5.10307
[1mStep[0m  [6/26], [94mLoss[0m : 5.23498
[1mStep[0m  [8/26], [94mLoss[0m : 5.08830
[1mStep[0m  [10/26], [94mLoss[0m : 4.96235
[1mStep[0m  [12/26], [94mLoss[0m : 5.20048
[1mStep[0m  [14/26], [94mLoss[0m : 5.00077
[1mStep[0m  [16/26], [94mLoss[0m : 5.04758
[1mStep[0m  [18/26], [94mLoss[0m : 4.93438
[1mStep[0m  [20/26], [94mLoss[0m : 4.72456
[1mStep[0m  [22/26], [94mLoss[0m : 4.75008
[1mStep[0m  [24/26], [94mLoss[0m : 5.05734

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.002, [92mTest[0m: 4.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.70396
[1mStep[0m  [2/26], [94mLoss[0m : 4.50502
[1mStep[0m  [4/26], [94mLoss[0m : 5.02668
[1mStep[0m  [6/26], [94mLoss[0m : 4.66352
[1mStep[0m  [8/26], [94mLoss[0m : 4.89132
[1mStep[0m  [10/26], [94mLoss[0m : 4.88344
[1mStep[0m  [12/26], [94mLoss[0m : 4.86523
[1mStep[0m  [14/26], [94mLoss[0m : 4.55472
[1mStep[0m  [16/26], [94mLoss[0m : 4.57919
[1mStep[0m  [18/26], [94mLoss[0m : 4.61961
[1mStep[0m  [20/26], [94mLoss[0m : 4.53231
[1mStep[0m  [22/26], [94mLoss[0m : 4.74071
[1mStep[0m  [24/26], [94mLoss[0m : 4.34963

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.660, [92mTest[0m: 4.145, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.63978
[1mStep[0m  [2/26], [94mLoss[0m : 4.25293
[1mStep[0m  [4/26], [94mLoss[0m : 4.44656
[1mStep[0m  [6/26], [94mLoss[0m : 4.09295
[1mStep[0m  [8/26], [94mLoss[0m : 4.42907
[1mStep[0m  [10/26], [94mLoss[0m : 4.27654
[1mStep[0m  [12/26], [94mLoss[0m : 4.43100
[1mStep[0m  [14/26], [94mLoss[0m : 4.33109
[1mStep[0m  [16/26], [94mLoss[0m : 4.17538
[1mStep[0m  [18/26], [94mLoss[0m : 4.49188
[1mStep[0m  [20/26], [94mLoss[0m : 4.06516
[1mStep[0m  [22/26], [94mLoss[0m : 4.35023
[1mStep[0m  [24/26], [94mLoss[0m : 4.15551

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.362, [92mTest[0m: 3.820, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.06575
[1mStep[0m  [2/26], [94mLoss[0m : 4.29002
[1mStep[0m  [4/26], [94mLoss[0m : 4.10327
[1mStep[0m  [6/26], [94mLoss[0m : 3.99056
[1mStep[0m  [8/26], [94mLoss[0m : 3.94347
[1mStep[0m  [10/26], [94mLoss[0m : 4.20216
[1mStep[0m  [12/26], [94mLoss[0m : 4.09618
[1mStep[0m  [14/26], [94mLoss[0m : 3.93058
[1mStep[0m  [16/26], [94mLoss[0m : 3.74692
[1mStep[0m  [18/26], [94mLoss[0m : 4.30640
[1mStep[0m  [20/26], [94mLoss[0m : 3.94917
[1mStep[0m  [22/26], [94mLoss[0m : 4.10418
[1mStep[0m  [24/26], [94mLoss[0m : 3.97432

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.036, [92mTest[0m: 3.600, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.73583
[1mStep[0m  [2/26], [94mLoss[0m : 3.83477
[1mStep[0m  [4/26], [94mLoss[0m : 3.52054
[1mStep[0m  [6/26], [94mLoss[0m : 4.16726
[1mStep[0m  [8/26], [94mLoss[0m : 3.88732
[1mStep[0m  [10/26], [94mLoss[0m : 3.64775
[1mStep[0m  [12/26], [94mLoss[0m : 3.85074
[1mStep[0m  [14/26], [94mLoss[0m : 3.67845
[1mStep[0m  [16/26], [94mLoss[0m : 3.75571
[1mStep[0m  [18/26], [94mLoss[0m : 3.70593
[1mStep[0m  [20/26], [94mLoss[0m : 3.93759
[1mStep[0m  [22/26], [94mLoss[0m : 3.67967
[1mStep[0m  [24/26], [94mLoss[0m : 3.55017

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.735, [92mTest[0m: 3.270, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.44885
[1mStep[0m  [2/26], [94mLoss[0m : 3.55155
[1mStep[0m  [4/26], [94mLoss[0m : 3.66095
[1mStep[0m  [6/26], [94mLoss[0m : 3.56667
[1mStep[0m  [8/26], [94mLoss[0m : 3.50610
[1mStep[0m  [10/26], [94mLoss[0m : 3.47227
[1mStep[0m  [12/26], [94mLoss[0m : 3.43511
[1mStep[0m  [14/26], [94mLoss[0m : 3.51897
[1mStep[0m  [16/26], [94mLoss[0m : 3.44255
[1mStep[0m  [18/26], [94mLoss[0m : 3.27689
[1mStep[0m  [20/26], [94mLoss[0m : 3.44601
[1mStep[0m  [22/26], [94mLoss[0m : 3.62574
[1mStep[0m  [24/26], [94mLoss[0m : 3.25870

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.432, [92mTest[0m: 3.082, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.13449
[1mStep[0m  [2/26], [94mLoss[0m : 3.50468
[1mStep[0m  [4/26], [94mLoss[0m : 3.27597
[1mStep[0m  [6/26], [94mLoss[0m : 3.18360
[1mStep[0m  [8/26], [94mLoss[0m : 3.32498
[1mStep[0m  [10/26], [94mLoss[0m : 3.19814
[1mStep[0m  [12/26], [94mLoss[0m : 3.07067
[1mStep[0m  [14/26], [94mLoss[0m : 3.23599
[1mStep[0m  [16/26], [94mLoss[0m : 3.26809
[1mStep[0m  [18/26], [94mLoss[0m : 3.16167
[1mStep[0m  [20/26], [94mLoss[0m : 3.08385
[1mStep[0m  [22/26], [94mLoss[0m : 3.25330
[1mStep[0m  [24/26], [94mLoss[0m : 3.09109

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.183, [92mTest[0m: 2.853, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.06521
[1mStep[0m  [2/26], [94mLoss[0m : 3.18011
[1mStep[0m  [4/26], [94mLoss[0m : 3.04296
[1mStep[0m  [6/26], [94mLoss[0m : 3.08877
[1mStep[0m  [8/26], [94mLoss[0m : 2.97826
[1mStep[0m  [10/26], [94mLoss[0m : 2.94474
[1mStep[0m  [12/26], [94mLoss[0m : 3.23097
[1mStep[0m  [14/26], [94mLoss[0m : 2.84534
[1mStep[0m  [16/26], [94mLoss[0m : 2.97618
[1mStep[0m  [18/26], [94mLoss[0m : 2.89054
[1mStep[0m  [20/26], [94mLoss[0m : 2.80582
[1mStep[0m  [22/26], [94mLoss[0m : 2.83703
[1mStep[0m  [24/26], [94mLoss[0m : 2.82634

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.976, [92mTest[0m: 2.675, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.96735
[1mStep[0m  [2/26], [94mLoss[0m : 2.90779
[1mStep[0m  [4/26], [94mLoss[0m : 2.82408
[1mStep[0m  [6/26], [94mLoss[0m : 2.62054
[1mStep[0m  [8/26], [94mLoss[0m : 3.04659
[1mStep[0m  [10/26], [94mLoss[0m : 3.06750
[1mStep[0m  [12/26], [94mLoss[0m : 2.99763
[1mStep[0m  [14/26], [94mLoss[0m : 2.74392
[1mStep[0m  [16/26], [94mLoss[0m : 2.87062
[1mStep[0m  [18/26], [94mLoss[0m : 2.77246
[1mStep[0m  [20/26], [94mLoss[0m : 2.81053
[1mStep[0m  [22/26], [94mLoss[0m : 2.91025
[1mStep[0m  [24/26], [94mLoss[0m : 2.61111

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.835, [92mTest[0m: 2.570, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.85992
[1mStep[0m  [2/26], [94mLoss[0m : 2.85273
[1mStep[0m  [4/26], [94mLoss[0m : 2.72297
[1mStep[0m  [6/26], [94mLoss[0m : 2.71718
[1mStep[0m  [8/26], [94mLoss[0m : 2.55336
[1mStep[0m  [10/26], [94mLoss[0m : 2.62929
[1mStep[0m  [12/26], [94mLoss[0m : 2.79741
[1mStep[0m  [14/26], [94mLoss[0m : 2.69175
[1mStep[0m  [16/26], [94mLoss[0m : 2.65806
[1mStep[0m  [18/26], [94mLoss[0m : 2.55325
[1mStep[0m  [20/26], [94mLoss[0m : 2.62388
[1mStep[0m  [22/26], [94mLoss[0m : 2.68227
[1mStep[0m  [24/26], [94mLoss[0m : 2.79980

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.737, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64204
[1mStep[0m  [2/26], [94mLoss[0m : 2.80719
[1mStep[0m  [4/26], [94mLoss[0m : 2.70103
[1mStep[0m  [6/26], [94mLoss[0m : 2.62721
[1mStep[0m  [8/26], [94mLoss[0m : 2.82180
[1mStep[0m  [10/26], [94mLoss[0m : 2.58782
[1mStep[0m  [12/26], [94mLoss[0m : 2.52680
[1mStep[0m  [14/26], [94mLoss[0m : 2.60900
[1mStep[0m  [16/26], [94mLoss[0m : 2.69801
[1mStep[0m  [18/26], [94mLoss[0m : 2.72363
[1mStep[0m  [20/26], [94mLoss[0m : 2.56553
[1mStep[0m  [22/26], [94mLoss[0m : 2.46215
[1mStep[0m  [24/26], [94mLoss[0m : 2.63801

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.456
====================================

Phase 1 - Evaluation MAE:  2.4558919209700365
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.70925
[1mStep[0m  [2/26], [94mLoss[0m : 2.63496
[1mStep[0m  [4/26], [94mLoss[0m : 2.72334
[1mStep[0m  [6/26], [94mLoss[0m : 2.66470
[1mStep[0m  [8/26], [94mLoss[0m : 2.61574
[1mStep[0m  [10/26], [94mLoss[0m : 2.74810
[1mStep[0m  [12/26], [94mLoss[0m : 2.85073
[1mStep[0m  [14/26], [94mLoss[0m : 2.50762
[1mStep[0m  [16/26], [94mLoss[0m : 2.69459
[1mStep[0m  [18/26], [94mLoss[0m : 2.93447
[1mStep[0m  [20/26], [94mLoss[0m : 2.55345
[1mStep[0m  [22/26], [94mLoss[0m : 2.55840
[1mStep[0m  [24/26], [94mLoss[0m : 2.77006

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66836
[1mStep[0m  [2/26], [94mLoss[0m : 2.46009
[1mStep[0m  [4/26], [94mLoss[0m : 2.53342
[1mStep[0m  [6/26], [94mLoss[0m : 2.44568
[1mStep[0m  [8/26], [94mLoss[0m : 2.73891
[1mStep[0m  [10/26], [94mLoss[0m : 2.58758
[1mStep[0m  [12/26], [94mLoss[0m : 2.38725
[1mStep[0m  [14/26], [94mLoss[0m : 2.56941
[1mStep[0m  [16/26], [94mLoss[0m : 2.82543
[1mStep[0m  [18/26], [94mLoss[0m : 2.53369
[1mStep[0m  [20/26], [94mLoss[0m : 2.49498
[1mStep[0m  [22/26], [94mLoss[0m : 2.67407
[1mStep[0m  [24/26], [94mLoss[0m : 2.81387

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49776
[1mStep[0m  [2/26], [94mLoss[0m : 2.43849
[1mStep[0m  [4/26], [94mLoss[0m : 2.56979
[1mStep[0m  [6/26], [94mLoss[0m : 2.63567
[1mStep[0m  [8/26], [94mLoss[0m : 2.50781
[1mStep[0m  [10/26], [94mLoss[0m : 2.45806
[1mStep[0m  [12/26], [94mLoss[0m : 2.66327
[1mStep[0m  [14/26], [94mLoss[0m : 2.82167
[1mStep[0m  [16/26], [94mLoss[0m : 2.65133
[1mStep[0m  [18/26], [94mLoss[0m : 2.64092
[1mStep[0m  [20/26], [94mLoss[0m : 2.51774
[1mStep[0m  [22/26], [94mLoss[0m : 2.81271
[1mStep[0m  [24/26], [94mLoss[0m : 2.54237

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52769
[1mStep[0m  [2/26], [94mLoss[0m : 2.51923
[1mStep[0m  [4/26], [94mLoss[0m : 2.45514
[1mStep[0m  [6/26], [94mLoss[0m : 2.52833
[1mStep[0m  [8/26], [94mLoss[0m : 2.69377
[1mStep[0m  [10/26], [94mLoss[0m : 2.59749
[1mStep[0m  [12/26], [94mLoss[0m : 2.43828
[1mStep[0m  [14/26], [94mLoss[0m : 2.55213
[1mStep[0m  [16/26], [94mLoss[0m : 2.40041
[1mStep[0m  [18/26], [94mLoss[0m : 2.46604
[1mStep[0m  [20/26], [94mLoss[0m : 2.64713
[1mStep[0m  [22/26], [94mLoss[0m : 2.53689
[1mStep[0m  [24/26], [94mLoss[0m : 2.55474

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53773
[1mStep[0m  [2/26], [94mLoss[0m : 2.48810
[1mStep[0m  [4/26], [94mLoss[0m : 2.60515
[1mStep[0m  [6/26], [94mLoss[0m : 2.63242
[1mStep[0m  [8/26], [94mLoss[0m : 2.51943
[1mStep[0m  [10/26], [94mLoss[0m : 2.50264
[1mStep[0m  [12/26], [94mLoss[0m : 2.55727
[1mStep[0m  [14/26], [94mLoss[0m : 2.56210
[1mStep[0m  [16/26], [94mLoss[0m : 2.57663
[1mStep[0m  [18/26], [94mLoss[0m : 2.45386
[1mStep[0m  [20/26], [94mLoss[0m : 2.52378
[1mStep[0m  [22/26], [94mLoss[0m : 2.41243
[1mStep[0m  [24/26], [94mLoss[0m : 2.66475

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47930
[1mStep[0m  [2/26], [94mLoss[0m : 2.58090
[1mStep[0m  [4/26], [94mLoss[0m : 2.52570
[1mStep[0m  [6/26], [94mLoss[0m : 2.68278
[1mStep[0m  [8/26], [94mLoss[0m : 2.44750
[1mStep[0m  [10/26], [94mLoss[0m : 2.45219
[1mStep[0m  [12/26], [94mLoss[0m : 2.46476
[1mStep[0m  [14/26], [94mLoss[0m : 2.74448
[1mStep[0m  [16/26], [94mLoss[0m : 2.66148
[1mStep[0m  [18/26], [94mLoss[0m : 2.47926
[1mStep[0m  [20/26], [94mLoss[0m : 2.43629
[1mStep[0m  [22/26], [94mLoss[0m : 2.63563
[1mStep[0m  [24/26], [94mLoss[0m : 2.59504

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44029
[1mStep[0m  [2/26], [94mLoss[0m : 2.54644
[1mStep[0m  [4/26], [94mLoss[0m : 2.46978
[1mStep[0m  [6/26], [94mLoss[0m : 2.41554
[1mStep[0m  [8/26], [94mLoss[0m : 2.35809
[1mStep[0m  [10/26], [94mLoss[0m : 2.64239
[1mStep[0m  [12/26], [94mLoss[0m : 2.52861
[1mStep[0m  [14/26], [94mLoss[0m : 2.53426
[1mStep[0m  [16/26], [94mLoss[0m : 2.57361
[1mStep[0m  [18/26], [94mLoss[0m : 2.55043
[1mStep[0m  [20/26], [94mLoss[0m : 2.63344
[1mStep[0m  [22/26], [94mLoss[0m : 2.44975
[1mStep[0m  [24/26], [94mLoss[0m : 2.42279

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54154
[1mStep[0m  [2/26], [94mLoss[0m : 2.37207
[1mStep[0m  [4/26], [94mLoss[0m : 2.53629
[1mStep[0m  [6/26], [94mLoss[0m : 2.49473
[1mStep[0m  [8/26], [94mLoss[0m : 2.49002
[1mStep[0m  [10/26], [94mLoss[0m : 2.51985
[1mStep[0m  [12/26], [94mLoss[0m : 2.59328
[1mStep[0m  [14/26], [94mLoss[0m : 2.52719
[1mStep[0m  [16/26], [94mLoss[0m : 2.51730
[1mStep[0m  [18/26], [94mLoss[0m : 2.64341
[1mStep[0m  [20/26], [94mLoss[0m : 2.57064
[1mStep[0m  [22/26], [94mLoss[0m : 2.42136
[1mStep[0m  [24/26], [94mLoss[0m : 2.46532

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31375
[1mStep[0m  [2/26], [94mLoss[0m : 2.48533
[1mStep[0m  [4/26], [94mLoss[0m : 2.53034
[1mStep[0m  [6/26], [94mLoss[0m : 2.41987
[1mStep[0m  [8/26], [94mLoss[0m : 2.60791
[1mStep[0m  [10/26], [94mLoss[0m : 2.45291
[1mStep[0m  [12/26], [94mLoss[0m : 2.39136
[1mStep[0m  [14/26], [94mLoss[0m : 2.21659
[1mStep[0m  [16/26], [94mLoss[0m : 2.32888
[1mStep[0m  [18/26], [94mLoss[0m : 2.46026
[1mStep[0m  [20/26], [94mLoss[0m : 2.40526
[1mStep[0m  [22/26], [94mLoss[0m : 2.45158
[1mStep[0m  [24/26], [94mLoss[0m : 2.63850

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.544, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63480
[1mStep[0m  [2/26], [94mLoss[0m : 2.32941
[1mStep[0m  [4/26], [94mLoss[0m : 2.60962
[1mStep[0m  [6/26], [94mLoss[0m : 2.45544
[1mStep[0m  [8/26], [94mLoss[0m : 2.48188
[1mStep[0m  [10/26], [94mLoss[0m : 2.56696
[1mStep[0m  [12/26], [94mLoss[0m : 2.54977
[1mStep[0m  [14/26], [94mLoss[0m : 2.47398
[1mStep[0m  [16/26], [94mLoss[0m : 2.36340
[1mStep[0m  [18/26], [94mLoss[0m : 2.31831
[1mStep[0m  [20/26], [94mLoss[0m : 2.38892
[1mStep[0m  [22/26], [94mLoss[0m : 2.36643
[1mStep[0m  [24/26], [94mLoss[0m : 2.59582

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36973
[1mStep[0m  [2/26], [94mLoss[0m : 2.30053
[1mStep[0m  [4/26], [94mLoss[0m : 2.40032
[1mStep[0m  [6/26], [94mLoss[0m : 2.42662
[1mStep[0m  [8/26], [94mLoss[0m : 2.52335
[1mStep[0m  [10/26], [94mLoss[0m : 2.51610
[1mStep[0m  [12/26], [94mLoss[0m : 2.49826
[1mStep[0m  [14/26], [94mLoss[0m : 2.47116
[1mStep[0m  [16/26], [94mLoss[0m : 2.41038
[1mStep[0m  [18/26], [94mLoss[0m : 2.43857
[1mStep[0m  [20/26], [94mLoss[0m : 2.36880
[1mStep[0m  [22/26], [94mLoss[0m : 2.54034
[1mStep[0m  [24/26], [94mLoss[0m : 2.30073

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27627
[1mStep[0m  [2/26], [94mLoss[0m : 2.29769
[1mStep[0m  [4/26], [94mLoss[0m : 2.45748
[1mStep[0m  [6/26], [94mLoss[0m : 2.38825
[1mStep[0m  [8/26], [94mLoss[0m : 2.38100
[1mStep[0m  [10/26], [94mLoss[0m : 2.46704
[1mStep[0m  [12/26], [94mLoss[0m : 2.35978
[1mStep[0m  [14/26], [94mLoss[0m : 2.30310
[1mStep[0m  [16/26], [94mLoss[0m : 2.31327
[1mStep[0m  [18/26], [94mLoss[0m : 2.47119
[1mStep[0m  [20/26], [94mLoss[0m : 2.30272
[1mStep[0m  [22/26], [94mLoss[0m : 2.53865
[1mStep[0m  [24/26], [94mLoss[0m : 2.51740

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34648
[1mStep[0m  [2/26], [94mLoss[0m : 2.23658
[1mStep[0m  [4/26], [94mLoss[0m : 2.35209
[1mStep[0m  [6/26], [94mLoss[0m : 2.29874
[1mStep[0m  [8/26], [94mLoss[0m : 2.37083
[1mStep[0m  [10/26], [94mLoss[0m : 2.53822
[1mStep[0m  [12/26], [94mLoss[0m : 2.43111
[1mStep[0m  [14/26], [94mLoss[0m : 2.40926
[1mStep[0m  [16/26], [94mLoss[0m : 2.41765
[1mStep[0m  [18/26], [94mLoss[0m : 2.39967
[1mStep[0m  [20/26], [94mLoss[0m : 2.42445
[1mStep[0m  [22/26], [94mLoss[0m : 2.46675
[1mStep[0m  [24/26], [94mLoss[0m : 2.39767

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.655, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37343
[1mStep[0m  [2/26], [94mLoss[0m : 2.47620
[1mStep[0m  [4/26], [94mLoss[0m : 2.49702
[1mStep[0m  [6/26], [94mLoss[0m : 2.47695
[1mStep[0m  [8/26], [94mLoss[0m : 2.34980
[1mStep[0m  [10/26], [94mLoss[0m : 2.45526
[1mStep[0m  [12/26], [94mLoss[0m : 2.38953
[1mStep[0m  [14/26], [94mLoss[0m : 2.41122
[1mStep[0m  [16/26], [94mLoss[0m : 2.42615
[1mStep[0m  [18/26], [94mLoss[0m : 2.31487
[1mStep[0m  [20/26], [94mLoss[0m : 2.30266
[1mStep[0m  [22/26], [94mLoss[0m : 2.28056
[1mStep[0m  [24/26], [94mLoss[0m : 2.21240

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.589, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38376
[1mStep[0m  [2/26], [94mLoss[0m : 2.43749
[1mStep[0m  [4/26], [94mLoss[0m : 2.23608
[1mStep[0m  [6/26], [94mLoss[0m : 2.39826
[1mStep[0m  [8/26], [94mLoss[0m : 2.50644
[1mStep[0m  [10/26], [94mLoss[0m : 2.29891
[1mStep[0m  [12/26], [94mLoss[0m : 2.31158
[1mStep[0m  [14/26], [94mLoss[0m : 2.23529
[1mStep[0m  [16/26], [94mLoss[0m : 2.39422
[1mStep[0m  [18/26], [94mLoss[0m : 2.22155
[1mStep[0m  [20/26], [94mLoss[0m : 2.55812
[1mStep[0m  [22/26], [94mLoss[0m : 2.33878
[1mStep[0m  [24/26], [94mLoss[0m : 2.39275

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.660, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38675
[1mStep[0m  [2/26], [94mLoss[0m : 2.40461
[1mStep[0m  [4/26], [94mLoss[0m : 2.30994
[1mStep[0m  [6/26], [94mLoss[0m : 2.26615
[1mStep[0m  [8/26], [94mLoss[0m : 2.11569
[1mStep[0m  [10/26], [94mLoss[0m : 2.28521
[1mStep[0m  [12/26], [94mLoss[0m : 2.19932
[1mStep[0m  [14/26], [94mLoss[0m : 2.41123
[1mStep[0m  [16/26], [94mLoss[0m : 2.40835
[1mStep[0m  [18/26], [94mLoss[0m : 2.24665
[1mStep[0m  [20/26], [94mLoss[0m : 2.30631
[1mStep[0m  [22/26], [94mLoss[0m : 2.41635
[1mStep[0m  [24/26], [94mLoss[0m : 2.25261

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.702, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39045
[1mStep[0m  [2/26], [94mLoss[0m : 2.30203
[1mStep[0m  [4/26], [94mLoss[0m : 2.21992
[1mStep[0m  [6/26], [94mLoss[0m : 2.44495
[1mStep[0m  [8/26], [94mLoss[0m : 2.23535
[1mStep[0m  [10/26], [94mLoss[0m : 2.32035
[1mStep[0m  [12/26], [94mLoss[0m : 2.28039
[1mStep[0m  [14/26], [94mLoss[0m : 2.38800
[1mStep[0m  [16/26], [94mLoss[0m : 2.35352
[1mStep[0m  [18/26], [94mLoss[0m : 2.25963
[1mStep[0m  [20/26], [94mLoss[0m : 2.41002
[1mStep[0m  [22/26], [94mLoss[0m : 2.34326
[1mStep[0m  [24/26], [94mLoss[0m : 2.36174

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.624, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39693
[1mStep[0m  [2/26], [94mLoss[0m : 2.13305
[1mStep[0m  [4/26], [94mLoss[0m : 2.29868
[1mStep[0m  [6/26], [94mLoss[0m : 2.26902
[1mStep[0m  [8/26], [94mLoss[0m : 2.40855
[1mStep[0m  [10/26], [94mLoss[0m : 2.35457
[1mStep[0m  [12/26], [94mLoss[0m : 2.28320
[1mStep[0m  [14/26], [94mLoss[0m : 2.31605
[1mStep[0m  [16/26], [94mLoss[0m : 2.40771
[1mStep[0m  [18/26], [94mLoss[0m : 2.28124
[1mStep[0m  [20/26], [94mLoss[0m : 2.16591
[1mStep[0m  [22/26], [94mLoss[0m : 2.33863
[1mStep[0m  [24/26], [94mLoss[0m : 2.31497

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.581, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33194
[1mStep[0m  [2/26], [94mLoss[0m : 2.21965
[1mStep[0m  [4/26], [94mLoss[0m : 2.34021
[1mStep[0m  [6/26], [94mLoss[0m : 2.15034
[1mStep[0m  [8/26], [94mLoss[0m : 2.44806
[1mStep[0m  [10/26], [94mLoss[0m : 2.41423
[1mStep[0m  [12/26], [94mLoss[0m : 2.37989
[1mStep[0m  [14/26], [94mLoss[0m : 2.30648
[1mStep[0m  [16/26], [94mLoss[0m : 2.28216
[1mStep[0m  [18/26], [94mLoss[0m : 2.12697
[1mStep[0m  [20/26], [94mLoss[0m : 2.27199
[1mStep[0m  [22/26], [94mLoss[0m : 2.25884
[1mStep[0m  [24/26], [94mLoss[0m : 2.28046

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.650, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20096
[1mStep[0m  [2/26], [94mLoss[0m : 2.18876
[1mStep[0m  [4/26], [94mLoss[0m : 2.12591
[1mStep[0m  [6/26], [94mLoss[0m : 2.24421
[1mStep[0m  [8/26], [94mLoss[0m : 2.19415
[1mStep[0m  [10/26], [94mLoss[0m : 2.40154
[1mStep[0m  [12/26], [94mLoss[0m : 2.37733
[1mStep[0m  [14/26], [94mLoss[0m : 2.30670
[1mStep[0m  [16/26], [94mLoss[0m : 2.36414
[1mStep[0m  [18/26], [94mLoss[0m : 2.29058
[1mStep[0m  [20/26], [94mLoss[0m : 2.10153
[1mStep[0m  [22/26], [94mLoss[0m : 2.15335
[1mStep[0m  [24/26], [94mLoss[0m : 2.36723

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.654, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15512
[1mStep[0m  [2/26], [94mLoss[0m : 2.29941
[1mStep[0m  [4/26], [94mLoss[0m : 2.32408
[1mStep[0m  [6/26], [94mLoss[0m : 2.15257
[1mStep[0m  [8/26], [94mLoss[0m : 2.22394
[1mStep[0m  [10/26], [94mLoss[0m : 2.11985
[1mStep[0m  [12/26], [94mLoss[0m : 2.22258
[1mStep[0m  [14/26], [94mLoss[0m : 2.21427
[1mStep[0m  [16/26], [94mLoss[0m : 2.14400
[1mStep[0m  [18/26], [94mLoss[0m : 2.08217
[1mStep[0m  [20/26], [94mLoss[0m : 2.16154
[1mStep[0m  [22/26], [94mLoss[0m : 2.19906
[1mStep[0m  [24/26], [94mLoss[0m : 2.29994

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.680, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.13855
[1mStep[0m  [2/26], [94mLoss[0m : 2.10094
[1mStep[0m  [4/26], [94mLoss[0m : 2.09770
[1mStep[0m  [6/26], [94mLoss[0m : 2.09943
[1mStep[0m  [8/26], [94mLoss[0m : 2.15180
[1mStep[0m  [10/26], [94mLoss[0m : 2.17853
[1mStep[0m  [12/26], [94mLoss[0m : 2.22895
[1mStep[0m  [14/26], [94mLoss[0m : 2.22941
[1mStep[0m  [16/26], [94mLoss[0m : 2.31051
[1mStep[0m  [18/26], [94mLoss[0m : 1.97123
[1mStep[0m  [20/26], [94mLoss[0m : 2.15858
[1mStep[0m  [22/26], [94mLoss[0m : 2.26954
[1mStep[0m  [24/26], [94mLoss[0m : 2.25743

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.777, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19264
[1mStep[0m  [2/26], [94mLoss[0m : 2.16782
[1mStep[0m  [4/26], [94mLoss[0m : 2.14749
[1mStep[0m  [6/26], [94mLoss[0m : 2.24006
[1mStep[0m  [8/26], [94mLoss[0m : 2.26653
[1mStep[0m  [10/26], [94mLoss[0m : 2.24143
[1mStep[0m  [12/26], [94mLoss[0m : 2.07380
[1mStep[0m  [14/26], [94mLoss[0m : 2.19440
[1mStep[0m  [16/26], [94mLoss[0m : 2.31018
[1mStep[0m  [18/26], [94mLoss[0m : 2.21975
[1mStep[0m  [20/26], [94mLoss[0m : 2.12062
[1mStep[0m  [22/26], [94mLoss[0m : 2.05153
[1mStep[0m  [24/26], [94mLoss[0m : 2.11732

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.706, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.13601
[1mStep[0m  [2/26], [94mLoss[0m : 2.19382
[1mStep[0m  [4/26], [94mLoss[0m : 2.14268
[1mStep[0m  [6/26], [94mLoss[0m : 2.14826
[1mStep[0m  [8/26], [94mLoss[0m : 2.19643
[1mStep[0m  [10/26], [94mLoss[0m : 2.30067
[1mStep[0m  [12/26], [94mLoss[0m : 2.24041
[1mStep[0m  [14/26], [94mLoss[0m : 2.27724
[1mStep[0m  [16/26], [94mLoss[0m : 2.21558
[1mStep[0m  [18/26], [94mLoss[0m : 2.14112
[1mStep[0m  [20/26], [94mLoss[0m : 2.27595
[1mStep[0m  [22/26], [94mLoss[0m : 2.29773
[1mStep[0m  [24/26], [94mLoss[0m : 2.17932

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.742, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17056
[1mStep[0m  [2/26], [94mLoss[0m : 2.18766
[1mStep[0m  [4/26], [94mLoss[0m : 2.12111
[1mStep[0m  [6/26], [94mLoss[0m : 2.06849
[1mStep[0m  [8/26], [94mLoss[0m : 2.06456
[1mStep[0m  [10/26], [94mLoss[0m : 2.24572
[1mStep[0m  [12/26], [94mLoss[0m : 2.19744
[1mStep[0m  [14/26], [94mLoss[0m : 2.15978
[1mStep[0m  [16/26], [94mLoss[0m : 2.07964
[1mStep[0m  [18/26], [94mLoss[0m : 2.29434
[1mStep[0m  [20/26], [94mLoss[0m : 2.21449
[1mStep[0m  [22/26], [94mLoss[0m : 2.28334
[1mStep[0m  [24/26], [94mLoss[0m : 2.12010

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.646, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96333
[1mStep[0m  [2/26], [94mLoss[0m : 2.17033
[1mStep[0m  [4/26], [94mLoss[0m : 2.16267
[1mStep[0m  [6/26], [94mLoss[0m : 2.19564
[1mStep[0m  [8/26], [94mLoss[0m : 2.07866
[1mStep[0m  [10/26], [94mLoss[0m : 2.10399
[1mStep[0m  [12/26], [94mLoss[0m : 2.20963
[1mStep[0m  [14/26], [94mLoss[0m : 2.12749
[1mStep[0m  [16/26], [94mLoss[0m : 1.95247
[1mStep[0m  [18/26], [94mLoss[0m : 2.22036
[1mStep[0m  [20/26], [94mLoss[0m : 2.02821
[1mStep[0m  [22/26], [94mLoss[0m : 2.15242
[1mStep[0m  [24/26], [94mLoss[0m : 2.03181

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.708, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.14804
[1mStep[0m  [2/26], [94mLoss[0m : 2.01558
[1mStep[0m  [4/26], [94mLoss[0m : 2.13686
[1mStep[0m  [6/26], [94mLoss[0m : 2.23167
[1mStep[0m  [8/26], [94mLoss[0m : 2.18195
[1mStep[0m  [10/26], [94mLoss[0m : 2.18336
[1mStep[0m  [12/26], [94mLoss[0m : 2.08649
[1mStep[0m  [14/26], [94mLoss[0m : 2.18991
[1mStep[0m  [16/26], [94mLoss[0m : 2.02616
[1mStep[0m  [18/26], [94mLoss[0m : 2.19465
[1mStep[0m  [20/26], [94mLoss[0m : 2.16780
[1mStep[0m  [22/26], [94mLoss[0m : 2.26075
[1mStep[0m  [24/26], [94mLoss[0m : 2.05436

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.721, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03595
[1mStep[0m  [2/26], [94mLoss[0m : 2.15271
[1mStep[0m  [4/26], [94mLoss[0m : 2.07047
[1mStep[0m  [6/26], [94mLoss[0m : 2.06635
[1mStep[0m  [8/26], [94mLoss[0m : 2.13895
[1mStep[0m  [10/26], [94mLoss[0m : 1.98835
[1mStep[0m  [12/26], [94mLoss[0m : 2.09832
[1mStep[0m  [14/26], [94mLoss[0m : 2.15536
[1mStep[0m  [16/26], [94mLoss[0m : 2.09192
[1mStep[0m  [18/26], [94mLoss[0m : 2.08071
[1mStep[0m  [20/26], [94mLoss[0m : 2.04948
[1mStep[0m  [22/26], [94mLoss[0m : 2.24527
[1mStep[0m  [24/26], [94mLoss[0m : 2.17495

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.640, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.05078
[1mStep[0m  [2/26], [94mLoss[0m : 2.08135
[1mStep[0m  [4/26], [94mLoss[0m : 2.13501
[1mStep[0m  [6/26], [94mLoss[0m : 2.16308
[1mStep[0m  [8/26], [94mLoss[0m : 2.19152
[1mStep[0m  [10/26], [94mLoss[0m : 1.97721
[1mStep[0m  [12/26], [94mLoss[0m : 2.12805
[1mStep[0m  [14/26], [94mLoss[0m : 2.20735
[1mStep[0m  [16/26], [94mLoss[0m : 1.95584
[1mStep[0m  [18/26], [94mLoss[0m : 2.10100
[1mStep[0m  [20/26], [94mLoss[0m : 2.11229
[1mStep[0m  [22/26], [94mLoss[0m : 2.08359
[1mStep[0m  [24/26], [94mLoss[0m : 2.09536

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.705, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.05701
[1mStep[0m  [2/26], [94mLoss[0m : 2.02286
[1mStep[0m  [4/26], [94mLoss[0m : 2.15919
[1mStep[0m  [6/26], [94mLoss[0m : 2.06831
[1mStep[0m  [8/26], [94mLoss[0m : 2.11877
[1mStep[0m  [10/26], [94mLoss[0m : 2.21337
[1mStep[0m  [12/26], [94mLoss[0m : 2.06444
[1mStep[0m  [14/26], [94mLoss[0m : 2.00820
[1mStep[0m  [16/26], [94mLoss[0m : 1.92617
[1mStep[0m  [18/26], [94mLoss[0m : 2.07185
[1mStep[0m  [20/26], [94mLoss[0m : 2.07913
[1mStep[0m  [22/26], [94mLoss[0m : 2.10826
[1mStep[0m  [24/26], [94mLoss[0m : 2.13856

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.766, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.750
====================================

Phase 2 - Evaluation MAE:  2.7496474339411807
MAE score P1      2.455892
MAE score P2      2.749647
loss              2.080763
learning_rate      0.00505
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/427], [94mLoss[0m : 10.58157
[1mStep[0m  [42/427], [94mLoss[0m : 3.01634
[1mStep[0m  [84/427], [94mLoss[0m : 2.39860
[1mStep[0m  [126/427], [94mLoss[0m : 2.54658
[1mStep[0m  [168/427], [94mLoss[0m : 2.79615
[1mStep[0m  [210/427], [94mLoss[0m : 2.14275
[1mStep[0m  [252/427], [94mLoss[0m : 2.80008
[1mStep[0m  [294/427], [94mLoss[0m : 2.30170
[1mStep[0m  [336/427], [94mLoss[0m : 2.68867
[1mStep[0m  [378/427], [94mLoss[0m : 2.66036
[1mStep[0m  [420/427], [94mLoss[0m : 2.64471

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.709, [92mTest[0m: 10.728, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.98850
[1mStep[0m  [42/427], [94mLoss[0m : 1.94450
[1mStep[0m  [84/427], [94mLoss[0m : 2.62617
[1mStep[0m  [126/427], [94mLoss[0m : 2.30516
[1mStep[0m  [168/427], [94mLoss[0m : 2.52498
[1mStep[0m  [210/427], [94mLoss[0m : 2.44043
[1mStep[0m  [252/427], [94mLoss[0m : 2.81964
[1mStep[0m  [294/427], [94mLoss[0m : 1.60626
[1mStep[0m  [336/427], [94mLoss[0m : 2.11332
[1mStep[0m  [378/427], [94mLoss[0m : 2.43629
[1mStep[0m  [420/427], [94mLoss[0m : 2.83679

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.17939
[1mStep[0m  [42/427], [94mLoss[0m : 2.71980
[1mStep[0m  [84/427], [94mLoss[0m : 2.81768
[1mStep[0m  [126/427], [94mLoss[0m : 2.35018
[1mStep[0m  [168/427], [94mLoss[0m : 2.60433
[1mStep[0m  [210/427], [94mLoss[0m : 2.86588
[1mStep[0m  [252/427], [94mLoss[0m : 2.26769
[1mStep[0m  [294/427], [94mLoss[0m : 4.33369
[1mStep[0m  [336/427], [94mLoss[0m : 2.36767
[1mStep[0m  [378/427], [94mLoss[0m : 1.86401
[1mStep[0m  [420/427], [94mLoss[0m : 2.21482

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.02582
[1mStep[0m  [42/427], [94mLoss[0m : 2.70685
[1mStep[0m  [84/427], [94mLoss[0m : 2.59822
[1mStep[0m  [126/427], [94mLoss[0m : 2.78621
[1mStep[0m  [168/427], [94mLoss[0m : 2.57621
[1mStep[0m  [210/427], [94mLoss[0m : 2.66699
[1mStep[0m  [252/427], [94mLoss[0m : 2.12539
[1mStep[0m  [294/427], [94mLoss[0m : 2.59881
[1mStep[0m  [336/427], [94mLoss[0m : 2.69493
[1mStep[0m  [378/427], [94mLoss[0m : 2.45527
[1mStep[0m  [420/427], [94mLoss[0m : 2.97724

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.595, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.87577
[1mStep[0m  [42/427], [94mLoss[0m : 2.15211
[1mStep[0m  [84/427], [94mLoss[0m : 2.92449
[1mStep[0m  [126/427], [94mLoss[0m : 2.65174
[1mStep[0m  [168/427], [94mLoss[0m : 4.42268
[1mStep[0m  [210/427], [94mLoss[0m : 2.73336
[1mStep[0m  [252/427], [94mLoss[0m : 2.65828
[1mStep[0m  [294/427], [94mLoss[0m : 2.54482
[1mStep[0m  [336/427], [94mLoss[0m : 2.60171
[1mStep[0m  [378/427], [94mLoss[0m : 3.05572
[1mStep[0m  [420/427], [94mLoss[0m : 2.99065

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.07538
[1mStep[0m  [42/427], [94mLoss[0m : 3.20897
[1mStep[0m  [84/427], [94mLoss[0m : 2.76446
[1mStep[0m  [126/427], [94mLoss[0m : 3.08560
[1mStep[0m  [168/427], [94mLoss[0m : 2.34025
[1mStep[0m  [210/427], [94mLoss[0m : 2.73880
[1mStep[0m  [252/427], [94mLoss[0m : 2.47378
[1mStep[0m  [294/427], [94mLoss[0m : 2.42704
[1mStep[0m  [336/427], [94mLoss[0m : 2.41616
[1mStep[0m  [378/427], [94mLoss[0m : 2.27914
[1mStep[0m  [420/427], [94mLoss[0m : 2.76540

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.499, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.11757
[1mStep[0m  [42/427], [94mLoss[0m : 2.64070
[1mStep[0m  [84/427], [94mLoss[0m : 3.16307
[1mStep[0m  [126/427], [94mLoss[0m : 2.66647
[1mStep[0m  [168/427], [94mLoss[0m : 2.51147
[1mStep[0m  [210/427], [94mLoss[0m : 2.55458
[1mStep[0m  [252/427], [94mLoss[0m : 2.63826
[1mStep[0m  [294/427], [94mLoss[0m : 2.15805
[1mStep[0m  [336/427], [94mLoss[0m : 2.19103
[1mStep[0m  [378/427], [94mLoss[0m : 2.51111
[1mStep[0m  [420/427], [94mLoss[0m : 2.50816

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.79324
[1mStep[0m  [42/427], [94mLoss[0m : 2.42176
[1mStep[0m  [84/427], [94mLoss[0m : 2.66315
[1mStep[0m  [126/427], [94mLoss[0m : 2.05535
[1mStep[0m  [168/427], [94mLoss[0m : 2.95491
[1mStep[0m  [210/427], [94mLoss[0m : 2.66286
[1mStep[0m  [252/427], [94mLoss[0m : 3.21537
[1mStep[0m  [294/427], [94mLoss[0m : 3.01142
[1mStep[0m  [336/427], [94mLoss[0m : 3.85764
[1mStep[0m  [378/427], [94mLoss[0m : 2.62658
[1mStep[0m  [420/427], [94mLoss[0m : 2.46068

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.71363
[1mStep[0m  [42/427], [94mLoss[0m : 2.35916
[1mStep[0m  [84/427], [94mLoss[0m : 2.94533
[1mStep[0m  [126/427], [94mLoss[0m : 2.11808
[1mStep[0m  [168/427], [94mLoss[0m : 2.47812
[1mStep[0m  [210/427], [94mLoss[0m : 2.51044
[1mStep[0m  [252/427], [94mLoss[0m : 2.79952
[1mStep[0m  [294/427], [94mLoss[0m : 2.14621
[1mStep[0m  [336/427], [94mLoss[0m : 2.22965
[1mStep[0m  [378/427], [94mLoss[0m : 2.84242
[1mStep[0m  [420/427], [94mLoss[0m : 2.15338

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.06611
[1mStep[0m  [42/427], [94mLoss[0m : 1.98221
[1mStep[0m  [84/427], [94mLoss[0m : 2.76473
[1mStep[0m  [126/427], [94mLoss[0m : 1.92465
[1mStep[0m  [168/427], [94mLoss[0m : 2.20972
[1mStep[0m  [210/427], [94mLoss[0m : 2.19952
[1mStep[0m  [252/427], [94mLoss[0m : 2.41664
[1mStep[0m  [294/427], [94mLoss[0m : 2.40799
[1mStep[0m  [336/427], [94mLoss[0m : 2.23881
[1mStep[0m  [378/427], [94mLoss[0m : 2.50683
[1mStep[0m  [420/427], [94mLoss[0m : 2.49200

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.26497
[1mStep[0m  [42/427], [94mLoss[0m : 2.55757
[1mStep[0m  [84/427], [94mLoss[0m : 2.09145
[1mStep[0m  [126/427], [94mLoss[0m : 2.73571
[1mStep[0m  [168/427], [94mLoss[0m : 2.39533
[1mStep[0m  [210/427], [94mLoss[0m : 2.65213
[1mStep[0m  [252/427], [94mLoss[0m : 3.00439
[1mStep[0m  [294/427], [94mLoss[0m : 2.65977
[1mStep[0m  [336/427], [94mLoss[0m : 2.52597
[1mStep[0m  [378/427], [94mLoss[0m : 2.87161
[1mStep[0m  [420/427], [94mLoss[0m : 2.66873

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.29462
[1mStep[0m  [42/427], [94mLoss[0m : 2.47879
[1mStep[0m  [84/427], [94mLoss[0m : 2.59360
[1mStep[0m  [126/427], [94mLoss[0m : 2.43648
[1mStep[0m  [168/427], [94mLoss[0m : 2.28478
[1mStep[0m  [210/427], [94mLoss[0m : 2.10984
[1mStep[0m  [252/427], [94mLoss[0m : 2.72356
[1mStep[0m  [294/427], [94mLoss[0m : 2.98802
[1mStep[0m  [336/427], [94mLoss[0m : 2.13671
[1mStep[0m  [378/427], [94mLoss[0m : 2.36458
[1mStep[0m  [420/427], [94mLoss[0m : 2.52036

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.95914
[1mStep[0m  [42/427], [94mLoss[0m : 2.58275
[1mStep[0m  [84/427], [94mLoss[0m : 2.51170
[1mStep[0m  [126/427], [94mLoss[0m : 2.40780
[1mStep[0m  [168/427], [94mLoss[0m : 2.38988
[1mStep[0m  [210/427], [94mLoss[0m : 2.70722
[1mStep[0m  [252/427], [94mLoss[0m : 2.71871
[1mStep[0m  [294/427], [94mLoss[0m : 2.65872
[1mStep[0m  [336/427], [94mLoss[0m : 2.23484
[1mStep[0m  [378/427], [94mLoss[0m : 2.62046
[1mStep[0m  [420/427], [94mLoss[0m : 2.52238

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.31381
[1mStep[0m  [42/427], [94mLoss[0m : 3.11281
[1mStep[0m  [84/427], [94mLoss[0m : 3.10995
[1mStep[0m  [126/427], [94mLoss[0m : 2.92661
[1mStep[0m  [168/427], [94mLoss[0m : 2.41535
[1mStep[0m  [210/427], [94mLoss[0m : 2.29808
[1mStep[0m  [252/427], [94mLoss[0m : 2.49480
[1mStep[0m  [294/427], [94mLoss[0m : 2.25651
[1mStep[0m  [336/427], [94mLoss[0m : 2.19850
[1mStep[0m  [378/427], [94mLoss[0m : 2.81753
[1mStep[0m  [420/427], [94mLoss[0m : 2.69281

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.33178
[1mStep[0m  [42/427], [94mLoss[0m : 2.99468
[1mStep[0m  [84/427], [94mLoss[0m : 2.08133
[1mStep[0m  [126/427], [94mLoss[0m : 3.43739
[1mStep[0m  [168/427], [94mLoss[0m : 2.58752
[1mStep[0m  [210/427], [94mLoss[0m : 2.09338
[1mStep[0m  [252/427], [94mLoss[0m : 3.10978
[1mStep[0m  [294/427], [94mLoss[0m : 2.74943
[1mStep[0m  [336/427], [94mLoss[0m : 2.23437
[1mStep[0m  [378/427], [94mLoss[0m : 2.55801
[1mStep[0m  [420/427], [94mLoss[0m : 2.72792

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.67854
[1mStep[0m  [42/427], [94mLoss[0m : 2.37246
[1mStep[0m  [84/427], [94mLoss[0m : 2.17110
[1mStep[0m  [126/427], [94mLoss[0m : 3.15062
[1mStep[0m  [168/427], [94mLoss[0m : 2.23165
[1mStep[0m  [210/427], [94mLoss[0m : 2.67819
[1mStep[0m  [252/427], [94mLoss[0m : 2.22856
[1mStep[0m  [294/427], [94mLoss[0m : 2.21784
[1mStep[0m  [336/427], [94mLoss[0m : 2.46272
[1mStep[0m  [378/427], [94mLoss[0m : 2.73619
[1mStep[0m  [420/427], [94mLoss[0m : 2.34278

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.31564
[1mStep[0m  [42/427], [94mLoss[0m : 2.20347
[1mStep[0m  [84/427], [94mLoss[0m : 2.56209
[1mStep[0m  [126/427], [94mLoss[0m : 2.34949
[1mStep[0m  [168/427], [94mLoss[0m : 2.58481
[1mStep[0m  [210/427], [94mLoss[0m : 2.18356
[1mStep[0m  [252/427], [94mLoss[0m : 2.72587
[1mStep[0m  [294/427], [94mLoss[0m : 2.59201
[1mStep[0m  [336/427], [94mLoss[0m : 2.25078
[1mStep[0m  [378/427], [94mLoss[0m : 2.71720
[1mStep[0m  [420/427], [94mLoss[0m : 2.51035

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.76458
[1mStep[0m  [42/427], [94mLoss[0m : 2.46048
[1mStep[0m  [84/427], [94mLoss[0m : 2.06083
[1mStep[0m  [126/427], [94mLoss[0m : 2.49107
[1mStep[0m  [168/427], [94mLoss[0m : 2.09792
[1mStep[0m  [210/427], [94mLoss[0m : 2.64815
[1mStep[0m  [252/427], [94mLoss[0m : 2.36359
[1mStep[0m  [294/427], [94mLoss[0m : 2.42616
[1mStep[0m  [336/427], [94mLoss[0m : 2.53531
[1mStep[0m  [378/427], [94mLoss[0m : 2.94632
[1mStep[0m  [420/427], [94mLoss[0m : 2.69700

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.601, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.47092
[1mStep[0m  [42/427], [94mLoss[0m : 2.22366
[1mStep[0m  [84/427], [94mLoss[0m : 2.26425
[1mStep[0m  [126/427], [94mLoss[0m : 3.19734
[1mStep[0m  [168/427], [94mLoss[0m : 2.45854
[1mStep[0m  [210/427], [94mLoss[0m : 1.83284
[1mStep[0m  [252/427], [94mLoss[0m : 2.10808
[1mStep[0m  [294/427], [94mLoss[0m : 2.78186
[1mStep[0m  [336/427], [94mLoss[0m : 2.07350
[1mStep[0m  [378/427], [94mLoss[0m : 2.68700
[1mStep[0m  [420/427], [94mLoss[0m : 2.76982

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.650, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.06855
[1mStep[0m  [42/427], [94mLoss[0m : 2.28737
[1mStep[0m  [84/427], [94mLoss[0m : 2.62285
[1mStep[0m  [126/427], [94mLoss[0m : 2.70055
[1mStep[0m  [168/427], [94mLoss[0m : 2.07911
[1mStep[0m  [210/427], [94mLoss[0m : 2.80967
[1mStep[0m  [252/427], [94mLoss[0m : 2.83520
[1mStep[0m  [294/427], [94mLoss[0m : 2.24668
[1mStep[0m  [336/427], [94mLoss[0m : 2.38879
[1mStep[0m  [378/427], [94mLoss[0m : 2.48501
[1mStep[0m  [420/427], [94mLoss[0m : 3.16199

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.431, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.75759
[1mStep[0m  [42/427], [94mLoss[0m : 3.10035
[1mStep[0m  [84/427], [94mLoss[0m : 2.81412
[1mStep[0m  [126/427], [94mLoss[0m : 2.55749
[1mStep[0m  [168/427], [94mLoss[0m : 2.49371
[1mStep[0m  [210/427], [94mLoss[0m : 2.15484
[1mStep[0m  [252/427], [94mLoss[0m : 2.49694
[1mStep[0m  [294/427], [94mLoss[0m : 2.18373
[1mStep[0m  [336/427], [94mLoss[0m : 2.41371
[1mStep[0m  [378/427], [94mLoss[0m : 2.60759
[1mStep[0m  [420/427], [94mLoss[0m : 2.14217

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.513, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.13672
[1mStep[0m  [42/427], [94mLoss[0m : 2.68444
[1mStep[0m  [84/427], [94mLoss[0m : 3.03660
[1mStep[0m  [126/427], [94mLoss[0m : 2.54306
[1mStep[0m  [168/427], [94mLoss[0m : 2.38858
[1mStep[0m  [210/427], [94mLoss[0m : 2.65532
[1mStep[0m  [252/427], [94mLoss[0m : 2.63127
[1mStep[0m  [294/427], [94mLoss[0m : 2.42045
[1mStep[0m  [336/427], [94mLoss[0m : 2.49140
[1mStep[0m  [378/427], [94mLoss[0m : 2.47453
[1mStep[0m  [420/427], [94mLoss[0m : 2.58075

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.429, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.93792
[1mStep[0m  [42/427], [94mLoss[0m : 2.30720
[1mStep[0m  [84/427], [94mLoss[0m : 3.42576
[1mStep[0m  [126/427], [94mLoss[0m : 2.94677
[1mStep[0m  [168/427], [94mLoss[0m : 2.53879
[1mStep[0m  [210/427], [94mLoss[0m : 2.20000
[1mStep[0m  [252/427], [94mLoss[0m : 2.47909
[1mStep[0m  [294/427], [94mLoss[0m : 2.35912
[1mStep[0m  [336/427], [94mLoss[0m : 1.99273
[1mStep[0m  [378/427], [94mLoss[0m : 2.89617
[1mStep[0m  [420/427], [94mLoss[0m : 2.50178

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.34567
[1mStep[0m  [42/427], [94mLoss[0m : 2.67325
[1mStep[0m  [84/427], [94mLoss[0m : 2.40465
[1mStep[0m  [126/427], [94mLoss[0m : 3.11034
[1mStep[0m  [168/427], [94mLoss[0m : 2.52359
[1mStep[0m  [210/427], [94mLoss[0m : 2.27474
[1mStep[0m  [252/427], [94mLoss[0m : 2.98112
[1mStep[0m  [294/427], [94mLoss[0m : 1.96471
[1mStep[0m  [336/427], [94mLoss[0m : 2.97010
[1mStep[0m  [378/427], [94mLoss[0m : 2.30988
[1mStep[0m  [420/427], [94mLoss[0m : 2.14902

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.453, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.48197
[1mStep[0m  [42/427], [94mLoss[0m : 2.05669
[1mStep[0m  [84/427], [94mLoss[0m : 2.72115
[1mStep[0m  [126/427], [94mLoss[0m : 3.11946
[1mStep[0m  [168/427], [94mLoss[0m : 2.94354
[1mStep[0m  [210/427], [94mLoss[0m : 2.41865
[1mStep[0m  [252/427], [94mLoss[0m : 2.34837
[1mStep[0m  [294/427], [94mLoss[0m : 2.45390
[1mStep[0m  [336/427], [94mLoss[0m : 3.21407
[1mStep[0m  [378/427], [94mLoss[0m : 2.58303
[1mStep[0m  [420/427], [94mLoss[0m : 2.52525

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.436, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.70990
[1mStep[0m  [42/427], [94mLoss[0m : 2.64085
[1mStep[0m  [84/427], [94mLoss[0m : 2.36986
[1mStep[0m  [126/427], [94mLoss[0m : 2.59509
[1mStep[0m  [168/427], [94mLoss[0m : 2.52890
[1mStep[0m  [210/427], [94mLoss[0m : 2.63779
[1mStep[0m  [252/427], [94mLoss[0m : 2.56623
[1mStep[0m  [294/427], [94mLoss[0m : 3.27826
[1mStep[0m  [336/427], [94mLoss[0m : 2.39227
[1mStep[0m  [378/427], [94mLoss[0m : 2.62760
[1mStep[0m  [420/427], [94mLoss[0m : 2.63255

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.33547
[1mStep[0m  [42/427], [94mLoss[0m : 2.71128
[1mStep[0m  [84/427], [94mLoss[0m : 2.77559
[1mStep[0m  [126/427], [94mLoss[0m : 2.37820
[1mStep[0m  [168/427], [94mLoss[0m : 2.61630
[1mStep[0m  [210/427], [94mLoss[0m : 2.11874
[1mStep[0m  [252/427], [94mLoss[0m : 3.24183
[1mStep[0m  [294/427], [94mLoss[0m : 3.34380
[1mStep[0m  [336/427], [94mLoss[0m : 2.08493
[1mStep[0m  [378/427], [94mLoss[0m : 2.78383
[1mStep[0m  [420/427], [94mLoss[0m : 2.56557

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.64440
[1mStep[0m  [42/427], [94mLoss[0m : 2.06823
[1mStep[0m  [84/427], [94mLoss[0m : 2.67638
[1mStep[0m  [126/427], [94mLoss[0m : 2.51389
[1mStep[0m  [168/427], [94mLoss[0m : 2.34784
[1mStep[0m  [210/427], [94mLoss[0m : 2.84495
[1mStep[0m  [252/427], [94mLoss[0m : 2.54278
[1mStep[0m  [294/427], [94mLoss[0m : 2.91337
[1mStep[0m  [336/427], [94mLoss[0m : 2.39011
[1mStep[0m  [378/427], [94mLoss[0m : 2.55427
[1mStep[0m  [420/427], [94mLoss[0m : 2.96061

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.427, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.17823
[1mStep[0m  [42/427], [94mLoss[0m : 2.82769
[1mStep[0m  [84/427], [94mLoss[0m : 2.82310
[1mStep[0m  [126/427], [94mLoss[0m : 2.19779
[1mStep[0m  [168/427], [94mLoss[0m : 2.29272
[1mStep[0m  [210/427], [94mLoss[0m : 2.54506
[1mStep[0m  [252/427], [94mLoss[0m : 2.54039
[1mStep[0m  [294/427], [94mLoss[0m : 2.06023
[1mStep[0m  [336/427], [94mLoss[0m : 2.48251
[1mStep[0m  [378/427], [94mLoss[0m : 2.35943
[1mStep[0m  [420/427], [94mLoss[0m : 2.45473

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.425, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.79778
[1mStep[0m  [42/427], [94mLoss[0m : 2.24830
[1mStep[0m  [84/427], [94mLoss[0m : 3.17103
[1mStep[0m  [126/427], [94mLoss[0m : 2.56080
[1mStep[0m  [168/427], [94mLoss[0m : 1.69001
[1mStep[0m  [210/427], [94mLoss[0m : 2.34075
[1mStep[0m  [252/427], [94mLoss[0m : 3.31396
[1mStep[0m  [294/427], [94mLoss[0m : 2.71026
[1mStep[0m  [336/427], [94mLoss[0m : 2.83103
[1mStep[0m  [378/427], [94mLoss[0m : 2.84984
[1mStep[0m  [420/427], [94mLoss[0m : 2.37170

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.506
====================================

Phase 1 - Evaluation MAE:  2.5057779035657783
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/427], [94mLoss[0m : 2.70273
[1mStep[0m  [42/427], [94mLoss[0m : 2.14257
[1mStep[0m  [84/427], [94mLoss[0m : 2.68161
[1mStep[0m  [126/427], [94mLoss[0m : 3.34197
[1mStep[0m  [168/427], [94mLoss[0m : 2.80994
[1mStep[0m  [210/427], [94mLoss[0m : 2.64332
[1mStep[0m  [252/427], [94mLoss[0m : 2.33192
[1mStep[0m  [294/427], [94mLoss[0m : 2.35996
[1mStep[0m  [336/427], [94mLoss[0m : 3.00104
[1mStep[0m  [378/427], [94mLoss[0m : 2.50937
[1mStep[0m  [420/427], [94mLoss[0m : 2.48373

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.81451
[1mStep[0m  [42/427], [94mLoss[0m : 2.71288
[1mStep[0m  [84/427], [94mLoss[0m : 2.10364
[1mStep[0m  [126/427], [94mLoss[0m : 2.54543
[1mStep[0m  [168/427], [94mLoss[0m : 2.41186
[1mStep[0m  [210/427], [94mLoss[0m : 3.10853
[1mStep[0m  [252/427], [94mLoss[0m : 3.16710
[1mStep[0m  [294/427], [94mLoss[0m : 1.81250
[1mStep[0m  [336/427], [94mLoss[0m : 2.23253
[1mStep[0m  [378/427], [94mLoss[0m : 2.34860
[1mStep[0m  [420/427], [94mLoss[0m : 2.92310

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.61122
[1mStep[0m  [42/427], [94mLoss[0m : 2.43614
[1mStep[0m  [84/427], [94mLoss[0m : 2.22904
[1mStep[0m  [126/427], [94mLoss[0m : 2.93838
[1mStep[0m  [168/427], [94mLoss[0m : 2.77401
[1mStep[0m  [210/427], [94mLoss[0m : 3.16666
[1mStep[0m  [252/427], [94mLoss[0m : 2.37167
[1mStep[0m  [294/427], [94mLoss[0m : 2.74826
[1mStep[0m  [336/427], [94mLoss[0m : 3.01922
[1mStep[0m  [378/427], [94mLoss[0m : 2.53010
[1mStep[0m  [420/427], [94mLoss[0m : 1.89438

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.76297
[1mStep[0m  [42/427], [94mLoss[0m : 2.80713
[1mStep[0m  [84/427], [94mLoss[0m : 2.11788
[1mStep[0m  [126/427], [94mLoss[0m : 3.31126
[1mStep[0m  [168/427], [94mLoss[0m : 2.91795
[1mStep[0m  [210/427], [94mLoss[0m : 2.76340
[1mStep[0m  [252/427], [94mLoss[0m : 2.90394
[1mStep[0m  [294/427], [94mLoss[0m : 3.45534
[1mStep[0m  [336/427], [94mLoss[0m : 3.09225
[1mStep[0m  [378/427], [94mLoss[0m : 2.60368
[1mStep[0m  [420/427], [94mLoss[0m : 2.49571

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.671, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.37640
[1mStep[0m  [42/427], [94mLoss[0m : 2.37930
[1mStep[0m  [84/427], [94mLoss[0m : 2.79127
[1mStep[0m  [126/427], [94mLoss[0m : 2.55975
[1mStep[0m  [168/427], [94mLoss[0m : 2.71678
[1mStep[0m  [210/427], [94mLoss[0m : 2.52814
[1mStep[0m  [252/427], [94mLoss[0m : 2.33767
[1mStep[0m  [294/427], [94mLoss[0m : 2.42149
[1mStep[0m  [336/427], [94mLoss[0m : 2.13682
[1mStep[0m  [378/427], [94mLoss[0m : 2.69086
[1mStep[0m  [420/427], [94mLoss[0m : 2.69646

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.95492
[1mStep[0m  [42/427], [94mLoss[0m : 2.88064
[1mStep[0m  [84/427], [94mLoss[0m : 2.55305
[1mStep[0m  [126/427], [94mLoss[0m : 3.09792
[1mStep[0m  [168/427], [94mLoss[0m : 1.87940
[1mStep[0m  [210/427], [94mLoss[0m : 2.44134
[1mStep[0m  [252/427], [94mLoss[0m : 2.73647
[1mStep[0m  [294/427], [94mLoss[0m : 2.96707
[1mStep[0m  [336/427], [94mLoss[0m : 2.95252
[1mStep[0m  [378/427], [94mLoss[0m : 2.54031
[1mStep[0m  [420/427], [94mLoss[0m : 2.57453

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.83084
[1mStep[0m  [42/427], [94mLoss[0m : 2.77192
[1mStep[0m  [84/427], [94mLoss[0m : 2.49003
[1mStep[0m  [126/427], [94mLoss[0m : 2.35364
[1mStep[0m  [168/427], [94mLoss[0m : 2.48352
[1mStep[0m  [210/427], [94mLoss[0m : 2.70702
[1mStep[0m  [252/427], [94mLoss[0m : 3.22216
[1mStep[0m  [294/427], [94mLoss[0m : 2.68460
[1mStep[0m  [336/427], [94mLoss[0m : 2.75694
[1mStep[0m  [378/427], [94mLoss[0m : 2.49588
[1mStep[0m  [420/427], [94mLoss[0m : 3.27827

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.51427
[1mStep[0m  [42/427], [94mLoss[0m : 3.01963
[1mStep[0m  [84/427], [94mLoss[0m : 2.89400
[1mStep[0m  [126/427], [94mLoss[0m : 2.06349
[1mStep[0m  [168/427], [94mLoss[0m : 2.39389
[1mStep[0m  [210/427], [94mLoss[0m : 2.56809
[1mStep[0m  [252/427], [94mLoss[0m : 1.90538
[1mStep[0m  [294/427], [94mLoss[0m : 2.54505
[1mStep[0m  [336/427], [94mLoss[0m : 3.05548
[1mStep[0m  [378/427], [94mLoss[0m : 2.32662
[1mStep[0m  [420/427], [94mLoss[0m : 2.44294

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.584, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.13207
[1mStep[0m  [42/427], [94mLoss[0m : 2.82209
[1mStep[0m  [84/427], [94mLoss[0m : 2.32122
[1mStep[0m  [126/427], [94mLoss[0m : 2.83756
[1mStep[0m  [168/427], [94mLoss[0m : 1.93813
[1mStep[0m  [210/427], [94mLoss[0m : 2.64666
[1mStep[0m  [252/427], [94mLoss[0m : 2.29476
[1mStep[0m  [294/427], [94mLoss[0m : 2.38885
[1mStep[0m  [336/427], [94mLoss[0m : 2.25141
[1mStep[0m  [378/427], [94mLoss[0m : 3.29295
[1mStep[0m  [420/427], [94mLoss[0m : 2.06095

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.48252
[1mStep[0m  [42/427], [94mLoss[0m : 2.78608
[1mStep[0m  [84/427], [94mLoss[0m : 2.18027
[1mStep[0m  [126/427], [94mLoss[0m : 3.15341
[1mStep[0m  [168/427], [94mLoss[0m : 2.65561
[1mStep[0m  [210/427], [94mLoss[0m : 2.33153
[1mStep[0m  [252/427], [94mLoss[0m : 2.32777
[1mStep[0m  [294/427], [94mLoss[0m : 2.29265
[1mStep[0m  [336/427], [94mLoss[0m : 2.72998
[1mStep[0m  [378/427], [94mLoss[0m : 2.63820
[1mStep[0m  [420/427], [94mLoss[0m : 2.71244

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.58111
[1mStep[0m  [42/427], [94mLoss[0m : 2.56327
[1mStep[0m  [84/427], [94mLoss[0m : 2.07308
[1mStep[0m  [126/427], [94mLoss[0m : 2.83779
[1mStep[0m  [168/427], [94mLoss[0m : 2.54797
[1mStep[0m  [210/427], [94mLoss[0m : 3.00887
[1mStep[0m  [252/427], [94mLoss[0m : 2.25304
[1mStep[0m  [294/427], [94mLoss[0m : 2.53851
[1mStep[0m  [336/427], [94mLoss[0m : 2.62663
[1mStep[0m  [378/427], [94mLoss[0m : 2.52054
[1mStep[0m  [420/427], [94mLoss[0m : 2.12450

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.11300
[1mStep[0m  [42/427], [94mLoss[0m : 2.35905
[1mStep[0m  [84/427], [94mLoss[0m : 2.21297
[1mStep[0m  [126/427], [94mLoss[0m : 1.88641
[1mStep[0m  [168/427], [94mLoss[0m : 3.03258
[1mStep[0m  [210/427], [94mLoss[0m : 2.25880
[1mStep[0m  [252/427], [94mLoss[0m : 2.34892
[1mStep[0m  [294/427], [94mLoss[0m : 2.28755
[1mStep[0m  [336/427], [94mLoss[0m : 2.14537
[1mStep[0m  [378/427], [94mLoss[0m : 3.02714
[1mStep[0m  [420/427], [94mLoss[0m : 2.83309

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.97869
[1mStep[0m  [42/427], [94mLoss[0m : 3.23813
[1mStep[0m  [84/427], [94mLoss[0m : 2.30216
[1mStep[0m  [126/427], [94mLoss[0m : 3.26392
[1mStep[0m  [168/427], [94mLoss[0m : 2.10696
[1mStep[0m  [210/427], [94mLoss[0m : 2.11229
[1mStep[0m  [252/427], [94mLoss[0m : 2.48067
[1mStep[0m  [294/427], [94mLoss[0m : 2.68050
[1mStep[0m  [336/427], [94mLoss[0m : 2.77756
[1mStep[0m  [378/427], [94mLoss[0m : 2.63194
[1mStep[0m  [420/427], [94mLoss[0m : 2.83776

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.24725
[1mStep[0m  [42/427], [94mLoss[0m : 2.40412
[1mStep[0m  [84/427], [94mLoss[0m : 2.03402
[1mStep[0m  [126/427], [94mLoss[0m : 2.86754
[1mStep[0m  [168/427], [94mLoss[0m : 2.74897
[1mStep[0m  [210/427], [94mLoss[0m : 2.27891
[1mStep[0m  [252/427], [94mLoss[0m : 2.28043
[1mStep[0m  [294/427], [94mLoss[0m : 2.16527
[1mStep[0m  [336/427], [94mLoss[0m : 2.82593
[1mStep[0m  [378/427], [94mLoss[0m : 2.94455
[1mStep[0m  [420/427], [94mLoss[0m : 2.89257

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.94495
[1mStep[0m  [42/427], [94mLoss[0m : 2.76049
[1mStep[0m  [84/427], [94mLoss[0m : 2.88151
[1mStep[0m  [126/427], [94mLoss[0m : 2.19298
[1mStep[0m  [168/427], [94mLoss[0m : 2.08181
[1mStep[0m  [210/427], [94mLoss[0m : 2.20486
[1mStep[0m  [252/427], [94mLoss[0m : 2.31492
[1mStep[0m  [294/427], [94mLoss[0m : 2.55485
[1mStep[0m  [336/427], [94mLoss[0m : 2.28903
[1mStep[0m  [378/427], [94mLoss[0m : 2.44011
[1mStep[0m  [420/427], [94mLoss[0m : 2.47720

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.586, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.53329
[1mStep[0m  [42/427], [94mLoss[0m : 3.05620
[1mStep[0m  [84/427], [94mLoss[0m : 2.30067
[1mStep[0m  [126/427], [94mLoss[0m : 2.75901
[1mStep[0m  [168/427], [94mLoss[0m : 1.94744
[1mStep[0m  [210/427], [94mLoss[0m : 2.31845
[1mStep[0m  [252/427], [94mLoss[0m : 2.36277
[1mStep[0m  [294/427], [94mLoss[0m : 2.28628
[1mStep[0m  [336/427], [94mLoss[0m : 3.06438
[1mStep[0m  [378/427], [94mLoss[0m : 2.52038
[1mStep[0m  [420/427], [94mLoss[0m : 2.65291

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.511, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.68919
[1mStep[0m  [42/427], [94mLoss[0m : 2.77970
[1mStep[0m  [84/427], [94mLoss[0m : 2.28904
[1mStep[0m  [126/427], [94mLoss[0m : 2.38946
[1mStep[0m  [168/427], [94mLoss[0m : 2.35583
[1mStep[0m  [210/427], [94mLoss[0m : 2.46845
[1mStep[0m  [252/427], [94mLoss[0m : 2.42021
[1mStep[0m  [294/427], [94mLoss[0m : 3.39133
[1mStep[0m  [336/427], [94mLoss[0m : 2.66121
[1mStep[0m  [378/427], [94mLoss[0m : 3.13793
[1mStep[0m  [420/427], [94mLoss[0m : 2.27400

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.73757
[1mStep[0m  [42/427], [94mLoss[0m : 2.61129
[1mStep[0m  [84/427], [94mLoss[0m : 2.82181
[1mStep[0m  [126/427], [94mLoss[0m : 2.77116
[1mStep[0m  [168/427], [94mLoss[0m : 2.54421
[1mStep[0m  [210/427], [94mLoss[0m : 2.78774
[1mStep[0m  [252/427], [94mLoss[0m : 2.52976
[1mStep[0m  [294/427], [94mLoss[0m : 2.84176
[1mStep[0m  [336/427], [94mLoss[0m : 2.73847
[1mStep[0m  [378/427], [94mLoss[0m : 2.84647
[1mStep[0m  [420/427], [94mLoss[0m : 2.34766

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.480, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.70738
[1mStep[0m  [42/427], [94mLoss[0m : 1.82894
[1mStep[0m  [84/427], [94mLoss[0m : 2.06905
[1mStep[0m  [126/427], [94mLoss[0m : 2.92279
[1mStep[0m  [168/427], [94mLoss[0m : 2.88661
[1mStep[0m  [210/427], [94mLoss[0m : 2.57074
[1mStep[0m  [252/427], [94mLoss[0m : 2.68804
[1mStep[0m  [294/427], [94mLoss[0m : 3.12348
[1mStep[0m  [336/427], [94mLoss[0m : 2.81399
[1mStep[0m  [378/427], [94mLoss[0m : 2.66600
[1mStep[0m  [420/427], [94mLoss[0m : 2.45701

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.83660
[1mStep[0m  [42/427], [94mLoss[0m : 2.30299
[1mStep[0m  [84/427], [94mLoss[0m : 2.11660
[1mStep[0m  [126/427], [94mLoss[0m : 2.50885
[1mStep[0m  [168/427], [94mLoss[0m : 2.33132
[1mStep[0m  [210/427], [94mLoss[0m : 2.83361
[1mStep[0m  [252/427], [94mLoss[0m : 2.61260
[1mStep[0m  [294/427], [94mLoss[0m : 2.82674
[1mStep[0m  [336/427], [94mLoss[0m : 1.91217
[1mStep[0m  [378/427], [94mLoss[0m : 2.14169
[1mStep[0m  [420/427], [94mLoss[0m : 2.47908

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.13731
[1mStep[0m  [42/427], [94mLoss[0m : 2.27598
[1mStep[0m  [84/427], [94mLoss[0m : 1.96168
[1mStep[0m  [126/427], [94mLoss[0m : 2.37752
[1mStep[0m  [168/427], [94mLoss[0m : 1.97527
[1mStep[0m  [210/427], [94mLoss[0m : 3.09700
[1mStep[0m  [252/427], [94mLoss[0m : 2.51502
[1mStep[0m  [294/427], [94mLoss[0m : 2.50713
[1mStep[0m  [336/427], [94mLoss[0m : 2.88915
[1mStep[0m  [378/427], [94mLoss[0m : 2.09553
[1mStep[0m  [420/427], [94mLoss[0m : 2.66352

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.05195
[1mStep[0m  [42/427], [94mLoss[0m : 2.70251
[1mStep[0m  [84/427], [94mLoss[0m : 2.95635
[1mStep[0m  [126/427], [94mLoss[0m : 3.08058
[1mStep[0m  [168/427], [94mLoss[0m : 2.68748
[1mStep[0m  [210/427], [94mLoss[0m : 2.62893
[1mStep[0m  [252/427], [94mLoss[0m : 2.71743
[1mStep[0m  [294/427], [94mLoss[0m : 2.81209
[1mStep[0m  [336/427], [94mLoss[0m : 2.29083
[1mStep[0m  [378/427], [94mLoss[0m : 2.50418
[1mStep[0m  [420/427], [94mLoss[0m : 3.56552

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.622, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.21413
[1mStep[0m  [42/427], [94mLoss[0m : 2.43158
[1mStep[0m  [84/427], [94mLoss[0m : 2.53756
[1mStep[0m  [126/427], [94mLoss[0m : 2.78282
[1mStep[0m  [168/427], [94mLoss[0m : 2.75357
[1mStep[0m  [210/427], [94mLoss[0m : 2.76041
[1mStep[0m  [252/427], [94mLoss[0m : 2.51089
[1mStep[0m  [294/427], [94mLoss[0m : 2.17072
[1mStep[0m  [336/427], [94mLoss[0m : 2.80129
[1mStep[0m  [378/427], [94mLoss[0m : 2.32405
[1mStep[0m  [420/427], [94mLoss[0m : 2.90832

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.55512
[1mStep[0m  [42/427], [94mLoss[0m : 2.11897
[1mStep[0m  [84/427], [94mLoss[0m : 2.82408
[1mStep[0m  [126/427], [94mLoss[0m : 1.54603
[1mStep[0m  [168/427], [94mLoss[0m : 2.60998
[1mStep[0m  [210/427], [94mLoss[0m : 2.45564
[1mStep[0m  [252/427], [94mLoss[0m : 2.46567
[1mStep[0m  [294/427], [94mLoss[0m : 2.22369
[1mStep[0m  [336/427], [94mLoss[0m : 2.16210
[1mStep[0m  [378/427], [94mLoss[0m : 2.51899
[1mStep[0m  [420/427], [94mLoss[0m : 2.28249

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.56901
[1mStep[0m  [42/427], [94mLoss[0m : 2.29954
[1mStep[0m  [84/427], [94mLoss[0m : 2.69118
[1mStep[0m  [126/427], [94mLoss[0m : 2.44168
[1mStep[0m  [168/427], [94mLoss[0m : 2.70229
[1mStep[0m  [210/427], [94mLoss[0m : 2.45165
[1mStep[0m  [252/427], [94mLoss[0m : 2.24412
[1mStep[0m  [294/427], [94mLoss[0m : 2.88167
[1mStep[0m  [336/427], [94mLoss[0m : 2.71401
[1mStep[0m  [378/427], [94mLoss[0m : 2.84993
[1mStep[0m  [420/427], [94mLoss[0m : 2.55287

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.468, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.25239
[1mStep[0m  [42/427], [94mLoss[0m : 2.27722
[1mStep[0m  [84/427], [94mLoss[0m : 2.30430
[1mStep[0m  [126/427], [94mLoss[0m : 3.41462
[1mStep[0m  [168/427], [94mLoss[0m : 2.59693
[1mStep[0m  [210/427], [94mLoss[0m : 2.39511
[1mStep[0m  [252/427], [94mLoss[0m : 2.41151
[1mStep[0m  [294/427], [94mLoss[0m : 2.28146
[1mStep[0m  [336/427], [94mLoss[0m : 2.17742
[1mStep[0m  [378/427], [94mLoss[0m : 2.37879
[1mStep[0m  [420/427], [94mLoss[0m : 2.87943

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.12008
[1mStep[0m  [42/427], [94mLoss[0m : 2.10335
[1mStep[0m  [84/427], [94mLoss[0m : 2.44408
[1mStep[0m  [126/427], [94mLoss[0m : 2.50730
[1mStep[0m  [168/427], [94mLoss[0m : 2.07105
[1mStep[0m  [210/427], [94mLoss[0m : 2.21305
[1mStep[0m  [252/427], [94mLoss[0m : 2.94600
[1mStep[0m  [294/427], [94mLoss[0m : 3.10162
[1mStep[0m  [336/427], [94mLoss[0m : 2.79155
[1mStep[0m  [378/427], [94mLoss[0m : 2.50114
[1mStep[0m  [420/427], [94mLoss[0m : 2.85892

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.08931
[1mStep[0m  [42/427], [94mLoss[0m : 2.77976
[1mStep[0m  [84/427], [94mLoss[0m : 2.33633
[1mStep[0m  [126/427], [94mLoss[0m : 2.82257
[1mStep[0m  [168/427], [94mLoss[0m : 2.89322
[1mStep[0m  [210/427], [94mLoss[0m : 2.62957
[1mStep[0m  [252/427], [94mLoss[0m : 2.52744
[1mStep[0m  [294/427], [94mLoss[0m : 2.01852
[1mStep[0m  [336/427], [94mLoss[0m : 2.15114
[1mStep[0m  [378/427], [94mLoss[0m : 3.12956
[1mStep[0m  [420/427], [94mLoss[0m : 2.92325

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.531, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.28284
[1mStep[0m  [42/427], [94mLoss[0m : 2.26572
[1mStep[0m  [84/427], [94mLoss[0m : 2.27199
[1mStep[0m  [126/427], [94mLoss[0m : 2.91315
[1mStep[0m  [168/427], [94mLoss[0m : 2.79188
[1mStep[0m  [210/427], [94mLoss[0m : 1.60214
[1mStep[0m  [252/427], [94mLoss[0m : 2.59540
[1mStep[0m  [294/427], [94mLoss[0m : 2.23850
[1mStep[0m  [336/427], [94mLoss[0m : 2.82139
[1mStep[0m  [378/427], [94mLoss[0m : 2.35437
[1mStep[0m  [420/427], [94mLoss[0m : 3.01674

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.467, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.34766
[1mStep[0m  [42/427], [94mLoss[0m : 2.55069
[1mStep[0m  [84/427], [94mLoss[0m : 2.61894
[1mStep[0m  [126/427], [94mLoss[0m : 2.18463
[1mStep[0m  [168/427], [94mLoss[0m : 2.50477
[1mStep[0m  [210/427], [94mLoss[0m : 2.73010
[1mStep[0m  [252/427], [94mLoss[0m : 2.35978
[1mStep[0m  [294/427], [94mLoss[0m : 2.67491
[1mStep[0m  [336/427], [94mLoss[0m : 2.59501
[1mStep[0m  [378/427], [94mLoss[0m : 3.36104
[1mStep[0m  [420/427], [94mLoss[0m : 2.19683

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.646, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.464
====================================

Phase 2 - Evaluation MAE:  2.4642022741792347
MAE score P1      2.505778
MAE score P2      2.464202
loss              2.536032
learning_rate     0.007525
batch_size              32
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 11.23740
[1mStep[0m  [2/26], [94mLoss[0m : 10.97965
[1mStep[0m  [4/26], [94mLoss[0m : 10.94039
[1mStep[0m  [6/26], [94mLoss[0m : 10.06483
[1mStep[0m  [8/26], [94mLoss[0m : 9.91920
[1mStep[0m  [10/26], [94mLoss[0m : 9.53679
[1mStep[0m  [12/26], [94mLoss[0m : 9.10837
[1mStep[0m  [14/26], [94mLoss[0m : 8.12760
[1mStep[0m  [16/26], [94mLoss[0m : 7.40281
[1mStep[0m  [18/26], [94mLoss[0m : 6.68642
[1mStep[0m  [20/26], [94mLoss[0m : 5.84509
[1mStep[0m  [22/26], [94mLoss[0m : 4.96572
[1mStep[0m  [24/26], [94mLoss[0m : 4.60313

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.235, [92mTest[0m: 10.980, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.92713
[1mStep[0m  [2/26], [94mLoss[0m : 3.23338
[1mStep[0m  [4/26], [94mLoss[0m : 3.31077
[1mStep[0m  [6/26], [94mLoss[0m : 2.80703
[1mStep[0m  [8/26], [94mLoss[0m : 2.96207
[1mStep[0m  [10/26], [94mLoss[0m : 2.79129
[1mStep[0m  [12/26], [94mLoss[0m : 2.91946
[1mStep[0m  [14/26], [94mLoss[0m : 2.98506
[1mStep[0m  [16/26], [94mLoss[0m : 3.14279
[1mStep[0m  [18/26], [94mLoss[0m : 3.08870
[1mStep[0m  [20/26], [94mLoss[0m : 3.01459
[1mStep[0m  [22/26], [94mLoss[0m : 2.68046
[1mStep[0m  [24/26], [94mLoss[0m : 2.65166

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.992, [92mTest[0m: 6.640, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68076
[1mStep[0m  [2/26], [94mLoss[0m : 2.54344
[1mStep[0m  [4/26], [94mLoss[0m : 2.77636
[1mStep[0m  [6/26], [94mLoss[0m : 2.50073
[1mStep[0m  [8/26], [94mLoss[0m : 2.41541
[1mStep[0m  [10/26], [94mLoss[0m : 2.66461
[1mStep[0m  [12/26], [94mLoss[0m : 2.73165
[1mStep[0m  [14/26], [94mLoss[0m : 2.63101
[1mStep[0m  [16/26], [94mLoss[0m : 2.88690
[1mStep[0m  [18/26], [94mLoss[0m : 2.77875
[1mStep[0m  [20/26], [94mLoss[0m : 2.74739
[1mStep[0m  [22/26], [94mLoss[0m : 2.66252
[1mStep[0m  [24/26], [94mLoss[0m : 2.55639

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.684, [92mTest[0m: 2.872, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64594
[1mStep[0m  [2/26], [94mLoss[0m : 2.60260
[1mStep[0m  [4/26], [94mLoss[0m : 2.62358
[1mStep[0m  [6/26], [94mLoss[0m : 2.74208
[1mStep[0m  [8/26], [94mLoss[0m : 2.64146
[1mStep[0m  [10/26], [94mLoss[0m : 2.66431
[1mStep[0m  [12/26], [94mLoss[0m : 2.54375
[1mStep[0m  [14/26], [94mLoss[0m : 2.43260
[1mStep[0m  [16/26], [94mLoss[0m : 2.77455
[1mStep[0m  [18/26], [94mLoss[0m : 2.51553
[1mStep[0m  [20/26], [94mLoss[0m : 2.52076
[1mStep[0m  [22/26], [94mLoss[0m : 2.50724
[1mStep[0m  [24/26], [94mLoss[0m : 2.50674

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.73791
[1mStep[0m  [2/26], [94mLoss[0m : 2.57105
[1mStep[0m  [4/26], [94mLoss[0m : 2.54093
[1mStep[0m  [6/26], [94mLoss[0m : 2.58328
[1mStep[0m  [8/26], [94mLoss[0m : 2.56705
[1mStep[0m  [10/26], [94mLoss[0m : 2.78035
[1mStep[0m  [12/26], [94mLoss[0m : 2.67086
[1mStep[0m  [14/26], [94mLoss[0m : 2.62722
[1mStep[0m  [16/26], [94mLoss[0m : 2.55434
[1mStep[0m  [18/26], [94mLoss[0m : 2.47439
[1mStep[0m  [20/26], [94mLoss[0m : 2.57289
[1mStep[0m  [22/26], [94mLoss[0m : 2.69106
[1mStep[0m  [24/26], [94mLoss[0m : 2.43434

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67870
[1mStep[0m  [2/26], [94mLoss[0m : 2.45397
[1mStep[0m  [4/26], [94mLoss[0m : 2.55204
[1mStep[0m  [6/26], [94mLoss[0m : 2.57928
[1mStep[0m  [8/26], [94mLoss[0m : 2.60151
[1mStep[0m  [10/26], [94mLoss[0m : 2.49735
[1mStep[0m  [12/26], [94mLoss[0m : 2.43231
[1mStep[0m  [14/26], [94mLoss[0m : 2.59641
[1mStep[0m  [16/26], [94mLoss[0m : 2.57682
[1mStep[0m  [18/26], [94mLoss[0m : 2.59032
[1mStep[0m  [20/26], [94mLoss[0m : 2.56111
[1mStep[0m  [22/26], [94mLoss[0m : 2.49150
[1mStep[0m  [24/26], [94mLoss[0m : 2.62742

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67380
[1mStep[0m  [2/26], [94mLoss[0m : 2.64179
[1mStep[0m  [4/26], [94mLoss[0m : 2.49580
[1mStep[0m  [6/26], [94mLoss[0m : 2.51403
[1mStep[0m  [8/26], [94mLoss[0m : 2.51515
[1mStep[0m  [10/26], [94mLoss[0m : 2.58651
[1mStep[0m  [12/26], [94mLoss[0m : 2.54875
[1mStep[0m  [14/26], [94mLoss[0m : 2.36816
[1mStep[0m  [16/26], [94mLoss[0m : 2.45151
[1mStep[0m  [18/26], [94mLoss[0m : 2.56793
[1mStep[0m  [20/26], [94mLoss[0m : 2.55959
[1mStep[0m  [22/26], [94mLoss[0m : 2.65492
[1mStep[0m  [24/26], [94mLoss[0m : 2.49202

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56018
[1mStep[0m  [2/26], [94mLoss[0m : 2.36051
[1mStep[0m  [4/26], [94mLoss[0m : 2.46031
[1mStep[0m  [6/26], [94mLoss[0m : 2.51514
[1mStep[0m  [8/26], [94mLoss[0m : 2.41513
[1mStep[0m  [10/26], [94mLoss[0m : 2.60054
[1mStep[0m  [12/26], [94mLoss[0m : 2.60191
[1mStep[0m  [14/26], [94mLoss[0m : 2.55071
[1mStep[0m  [16/26], [94mLoss[0m : 2.52587
[1mStep[0m  [18/26], [94mLoss[0m : 2.55702
[1mStep[0m  [20/26], [94mLoss[0m : 2.67835
[1mStep[0m  [22/26], [94mLoss[0m : 2.50441
[1mStep[0m  [24/26], [94mLoss[0m : 2.51681

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57158
[1mStep[0m  [2/26], [94mLoss[0m : 2.38478
[1mStep[0m  [4/26], [94mLoss[0m : 2.60144
[1mStep[0m  [6/26], [94mLoss[0m : 2.44068
[1mStep[0m  [8/26], [94mLoss[0m : 2.22800
[1mStep[0m  [10/26], [94mLoss[0m : 2.62112
[1mStep[0m  [12/26], [94mLoss[0m : 2.52436
[1mStep[0m  [14/26], [94mLoss[0m : 2.52497
[1mStep[0m  [16/26], [94mLoss[0m : 2.50864
[1mStep[0m  [18/26], [94mLoss[0m : 2.39961
[1mStep[0m  [20/26], [94mLoss[0m : 2.60508
[1mStep[0m  [22/26], [94mLoss[0m : 2.64552
[1mStep[0m  [24/26], [94mLoss[0m : 2.51331

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49713
[1mStep[0m  [2/26], [94mLoss[0m : 2.54652
[1mStep[0m  [4/26], [94mLoss[0m : 2.56631
[1mStep[0m  [6/26], [94mLoss[0m : 2.47635
[1mStep[0m  [8/26], [94mLoss[0m : 2.59370
[1mStep[0m  [10/26], [94mLoss[0m : 2.50501
[1mStep[0m  [12/26], [94mLoss[0m : 2.42662
[1mStep[0m  [14/26], [94mLoss[0m : 2.47471
[1mStep[0m  [16/26], [94mLoss[0m : 2.52437
[1mStep[0m  [18/26], [94mLoss[0m : 2.65479
[1mStep[0m  [20/26], [94mLoss[0m : 2.52717
[1mStep[0m  [22/26], [94mLoss[0m : 2.42086
[1mStep[0m  [24/26], [94mLoss[0m : 2.40420

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51762
[1mStep[0m  [2/26], [94mLoss[0m : 2.55062
[1mStep[0m  [4/26], [94mLoss[0m : 2.33585
[1mStep[0m  [6/26], [94mLoss[0m : 2.50979
[1mStep[0m  [8/26], [94mLoss[0m : 2.59228
[1mStep[0m  [10/26], [94mLoss[0m : 2.47119
[1mStep[0m  [12/26], [94mLoss[0m : 2.49085
[1mStep[0m  [14/26], [94mLoss[0m : 2.36328
[1mStep[0m  [16/26], [94mLoss[0m : 2.39552
[1mStep[0m  [18/26], [94mLoss[0m : 2.50212
[1mStep[0m  [20/26], [94mLoss[0m : 2.47576
[1mStep[0m  [22/26], [94mLoss[0m : 2.63700
[1mStep[0m  [24/26], [94mLoss[0m : 2.57133

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60585
[1mStep[0m  [2/26], [94mLoss[0m : 2.55092
[1mStep[0m  [4/26], [94mLoss[0m : 2.43184
[1mStep[0m  [6/26], [94mLoss[0m : 2.42568
[1mStep[0m  [8/26], [94mLoss[0m : 2.43413
[1mStep[0m  [10/26], [94mLoss[0m : 2.53821
[1mStep[0m  [12/26], [94mLoss[0m : 2.57187
[1mStep[0m  [14/26], [94mLoss[0m : 2.45127
[1mStep[0m  [16/26], [94mLoss[0m : 2.37613
[1mStep[0m  [18/26], [94mLoss[0m : 2.34113
[1mStep[0m  [20/26], [94mLoss[0m : 2.46205
[1mStep[0m  [22/26], [94mLoss[0m : 2.57282
[1mStep[0m  [24/26], [94mLoss[0m : 2.43598

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45410
[1mStep[0m  [2/26], [94mLoss[0m : 2.51044
[1mStep[0m  [4/26], [94mLoss[0m : 2.50982
[1mStep[0m  [6/26], [94mLoss[0m : 2.57475
[1mStep[0m  [8/26], [94mLoss[0m : 2.47247
[1mStep[0m  [10/26], [94mLoss[0m : 2.46469
[1mStep[0m  [12/26], [94mLoss[0m : 2.55832
[1mStep[0m  [14/26], [94mLoss[0m : 2.39662
[1mStep[0m  [16/26], [94mLoss[0m : 2.51826
[1mStep[0m  [18/26], [94mLoss[0m : 2.35276
[1mStep[0m  [20/26], [94mLoss[0m : 2.50348
[1mStep[0m  [22/26], [94mLoss[0m : 2.52298
[1mStep[0m  [24/26], [94mLoss[0m : 2.50447

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45581
[1mStep[0m  [2/26], [94mLoss[0m : 2.49167
[1mStep[0m  [4/26], [94mLoss[0m : 2.43902
[1mStep[0m  [6/26], [94mLoss[0m : 2.53262
[1mStep[0m  [8/26], [94mLoss[0m : 2.40574
[1mStep[0m  [10/26], [94mLoss[0m : 2.42464
[1mStep[0m  [12/26], [94mLoss[0m : 2.51423
[1mStep[0m  [14/26], [94mLoss[0m : 2.47684
[1mStep[0m  [16/26], [94mLoss[0m : 2.34528
[1mStep[0m  [18/26], [94mLoss[0m : 2.52533
[1mStep[0m  [20/26], [94mLoss[0m : 2.35270
[1mStep[0m  [22/26], [94mLoss[0m : 2.50825
[1mStep[0m  [24/26], [94mLoss[0m : 2.46965

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49155
[1mStep[0m  [2/26], [94mLoss[0m : 2.40058
[1mStep[0m  [4/26], [94mLoss[0m : 2.23583
[1mStep[0m  [6/26], [94mLoss[0m : 2.46474
[1mStep[0m  [8/26], [94mLoss[0m : 2.48457
[1mStep[0m  [10/26], [94mLoss[0m : 2.47723
[1mStep[0m  [12/26], [94mLoss[0m : 2.54367
[1mStep[0m  [14/26], [94mLoss[0m : 2.45750
[1mStep[0m  [16/26], [94mLoss[0m : 2.32887
[1mStep[0m  [18/26], [94mLoss[0m : 2.42405
[1mStep[0m  [20/26], [94mLoss[0m : 2.60551
[1mStep[0m  [22/26], [94mLoss[0m : 2.35795
[1mStep[0m  [24/26], [94mLoss[0m : 2.45588

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56483
[1mStep[0m  [2/26], [94mLoss[0m : 2.23685
[1mStep[0m  [4/26], [94mLoss[0m : 2.58288
[1mStep[0m  [6/26], [94mLoss[0m : 2.41673
[1mStep[0m  [8/26], [94mLoss[0m : 2.46817
[1mStep[0m  [10/26], [94mLoss[0m : 2.46302
[1mStep[0m  [12/26], [94mLoss[0m : 2.41560
[1mStep[0m  [14/26], [94mLoss[0m : 2.49395
[1mStep[0m  [16/26], [94mLoss[0m : 2.60130
[1mStep[0m  [18/26], [94mLoss[0m : 2.39948
[1mStep[0m  [20/26], [94mLoss[0m : 2.32522
[1mStep[0m  [22/26], [94mLoss[0m : 2.59490
[1mStep[0m  [24/26], [94mLoss[0m : 2.39716

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37218
[1mStep[0m  [2/26], [94mLoss[0m : 2.45020
[1mStep[0m  [4/26], [94mLoss[0m : 2.37797
[1mStep[0m  [6/26], [94mLoss[0m : 2.54839
[1mStep[0m  [8/26], [94mLoss[0m : 2.46213
[1mStep[0m  [10/26], [94mLoss[0m : 2.48530
[1mStep[0m  [12/26], [94mLoss[0m : 2.46766
[1mStep[0m  [14/26], [94mLoss[0m : 2.38204
[1mStep[0m  [16/26], [94mLoss[0m : 2.59051
[1mStep[0m  [18/26], [94mLoss[0m : 2.50225
[1mStep[0m  [20/26], [94mLoss[0m : 2.36433
[1mStep[0m  [22/26], [94mLoss[0m : 2.54590
[1mStep[0m  [24/26], [94mLoss[0m : 2.47524

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32477
[1mStep[0m  [2/26], [94mLoss[0m : 2.32159
[1mStep[0m  [4/26], [94mLoss[0m : 2.43357
[1mStep[0m  [6/26], [94mLoss[0m : 2.34317
[1mStep[0m  [8/26], [94mLoss[0m : 2.37161
[1mStep[0m  [10/26], [94mLoss[0m : 2.38384
[1mStep[0m  [12/26], [94mLoss[0m : 2.31018
[1mStep[0m  [14/26], [94mLoss[0m : 2.43551
[1mStep[0m  [16/26], [94mLoss[0m : 2.46806
[1mStep[0m  [18/26], [94mLoss[0m : 2.46057
[1mStep[0m  [20/26], [94mLoss[0m : 2.33264
[1mStep[0m  [22/26], [94mLoss[0m : 2.45491
[1mStep[0m  [24/26], [94mLoss[0m : 2.53089

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33551
[1mStep[0m  [2/26], [94mLoss[0m : 2.44389
[1mStep[0m  [4/26], [94mLoss[0m : 2.42756
[1mStep[0m  [6/26], [94mLoss[0m : 2.31983
[1mStep[0m  [8/26], [94mLoss[0m : 2.33682
[1mStep[0m  [10/26], [94mLoss[0m : 2.56904
[1mStep[0m  [12/26], [94mLoss[0m : 2.40700
[1mStep[0m  [14/26], [94mLoss[0m : 2.28090
[1mStep[0m  [16/26], [94mLoss[0m : 2.49351
[1mStep[0m  [18/26], [94mLoss[0m : 2.49142
[1mStep[0m  [20/26], [94mLoss[0m : 2.42433
[1mStep[0m  [22/26], [94mLoss[0m : 2.50961
[1mStep[0m  [24/26], [94mLoss[0m : 2.32815

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53907
[1mStep[0m  [2/26], [94mLoss[0m : 2.42215
[1mStep[0m  [4/26], [94mLoss[0m : 2.41643
[1mStep[0m  [6/26], [94mLoss[0m : 2.56459
[1mStep[0m  [8/26], [94mLoss[0m : 2.47440
[1mStep[0m  [10/26], [94mLoss[0m : 2.38209
[1mStep[0m  [12/26], [94mLoss[0m : 2.57056
[1mStep[0m  [14/26], [94mLoss[0m : 2.53240
[1mStep[0m  [16/26], [94mLoss[0m : 2.44251
[1mStep[0m  [18/26], [94mLoss[0m : 2.49510
[1mStep[0m  [20/26], [94mLoss[0m : 2.28010
[1mStep[0m  [22/26], [94mLoss[0m : 2.56454
[1mStep[0m  [24/26], [94mLoss[0m : 2.50034

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.360, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51909
[1mStep[0m  [2/26], [94mLoss[0m : 2.40667
[1mStep[0m  [4/26], [94mLoss[0m : 2.31499
[1mStep[0m  [6/26], [94mLoss[0m : 2.28164
[1mStep[0m  [8/26], [94mLoss[0m : 2.29315
[1mStep[0m  [10/26], [94mLoss[0m : 2.39862
[1mStep[0m  [12/26], [94mLoss[0m : 2.37008
[1mStep[0m  [14/26], [94mLoss[0m : 2.43640
[1mStep[0m  [16/26], [94mLoss[0m : 2.43404
[1mStep[0m  [18/26], [94mLoss[0m : 2.54362
[1mStep[0m  [20/26], [94mLoss[0m : 2.38930
[1mStep[0m  [22/26], [94mLoss[0m : 2.38894
[1mStep[0m  [24/26], [94mLoss[0m : 2.34039

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35156
[1mStep[0m  [2/26], [94mLoss[0m : 2.42866
[1mStep[0m  [4/26], [94mLoss[0m : 2.57917
[1mStep[0m  [6/26], [94mLoss[0m : 2.36699
[1mStep[0m  [8/26], [94mLoss[0m : 2.21468
[1mStep[0m  [10/26], [94mLoss[0m : 2.37439
[1mStep[0m  [12/26], [94mLoss[0m : 2.31978
[1mStep[0m  [14/26], [94mLoss[0m : 2.37140
[1mStep[0m  [16/26], [94mLoss[0m : 2.45806
[1mStep[0m  [18/26], [94mLoss[0m : 2.56207
[1mStep[0m  [20/26], [94mLoss[0m : 2.31064
[1mStep[0m  [22/26], [94mLoss[0m : 2.31405
[1mStep[0m  [24/26], [94mLoss[0m : 2.47010

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39137
[1mStep[0m  [2/26], [94mLoss[0m : 2.34397
[1mStep[0m  [4/26], [94mLoss[0m : 2.32489
[1mStep[0m  [6/26], [94mLoss[0m : 2.45347
[1mStep[0m  [8/26], [94mLoss[0m : 2.42875
[1mStep[0m  [10/26], [94mLoss[0m : 2.30897
[1mStep[0m  [12/26], [94mLoss[0m : 2.43096
[1mStep[0m  [14/26], [94mLoss[0m : 2.37331
[1mStep[0m  [16/26], [94mLoss[0m : 2.51366
[1mStep[0m  [18/26], [94mLoss[0m : 2.31859
[1mStep[0m  [20/26], [94mLoss[0m : 2.38581
[1mStep[0m  [22/26], [94mLoss[0m : 2.38388
[1mStep[0m  [24/26], [94mLoss[0m : 2.42313

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.352, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17135
[1mStep[0m  [2/26], [94mLoss[0m : 2.34014
[1mStep[0m  [4/26], [94mLoss[0m : 2.26410
[1mStep[0m  [6/26], [94mLoss[0m : 2.50258
[1mStep[0m  [8/26], [94mLoss[0m : 2.37425
[1mStep[0m  [10/26], [94mLoss[0m : 2.21522
[1mStep[0m  [12/26], [94mLoss[0m : 2.26343
[1mStep[0m  [14/26], [94mLoss[0m : 2.50625
[1mStep[0m  [16/26], [94mLoss[0m : 2.26399
[1mStep[0m  [18/26], [94mLoss[0m : 2.40298
[1mStep[0m  [20/26], [94mLoss[0m : 2.26279
[1mStep[0m  [22/26], [94mLoss[0m : 2.46869
[1mStep[0m  [24/26], [94mLoss[0m : 2.39999

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39565
[1mStep[0m  [2/26], [94mLoss[0m : 2.38414
[1mStep[0m  [4/26], [94mLoss[0m : 2.22404
[1mStep[0m  [6/26], [94mLoss[0m : 2.36941
[1mStep[0m  [8/26], [94mLoss[0m : 2.54392
[1mStep[0m  [10/26], [94mLoss[0m : 2.40875
[1mStep[0m  [12/26], [94mLoss[0m : 2.32099
[1mStep[0m  [14/26], [94mLoss[0m : 2.28984
[1mStep[0m  [16/26], [94mLoss[0m : 2.39815
[1mStep[0m  [18/26], [94mLoss[0m : 2.31191
[1mStep[0m  [20/26], [94mLoss[0m : 2.58040
[1mStep[0m  [22/26], [94mLoss[0m : 2.34551
[1mStep[0m  [24/26], [94mLoss[0m : 2.30943

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59721
[1mStep[0m  [2/26], [94mLoss[0m : 2.34765
[1mStep[0m  [4/26], [94mLoss[0m : 2.35358
[1mStep[0m  [6/26], [94mLoss[0m : 2.51538
[1mStep[0m  [8/26], [94mLoss[0m : 2.21175
[1mStep[0m  [10/26], [94mLoss[0m : 2.45238
[1mStep[0m  [12/26], [94mLoss[0m : 2.35743
[1mStep[0m  [14/26], [94mLoss[0m : 2.33636
[1mStep[0m  [16/26], [94mLoss[0m : 2.35411
[1mStep[0m  [18/26], [94mLoss[0m : 2.33106
[1mStep[0m  [20/26], [94mLoss[0m : 2.41201
[1mStep[0m  [22/26], [94mLoss[0m : 2.31105
[1mStep[0m  [24/26], [94mLoss[0m : 2.29617

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30968
[1mStep[0m  [2/26], [94mLoss[0m : 2.22077
[1mStep[0m  [4/26], [94mLoss[0m : 2.37633
[1mStep[0m  [6/26], [94mLoss[0m : 2.35696
[1mStep[0m  [8/26], [94mLoss[0m : 2.36093
[1mStep[0m  [10/26], [94mLoss[0m : 2.35743
[1mStep[0m  [12/26], [94mLoss[0m : 2.46679
[1mStep[0m  [14/26], [94mLoss[0m : 2.30053
[1mStep[0m  [16/26], [94mLoss[0m : 2.39541
[1mStep[0m  [18/26], [94mLoss[0m : 2.29493
[1mStep[0m  [20/26], [94mLoss[0m : 2.45616
[1mStep[0m  [22/26], [94mLoss[0m : 2.32270
[1mStep[0m  [24/26], [94mLoss[0m : 2.34923

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67914
[1mStep[0m  [2/26], [94mLoss[0m : 2.34602
[1mStep[0m  [4/26], [94mLoss[0m : 2.58559
[1mStep[0m  [6/26], [94mLoss[0m : 2.21486
[1mStep[0m  [8/26], [94mLoss[0m : 2.15824
[1mStep[0m  [10/26], [94mLoss[0m : 2.29463
[1mStep[0m  [12/26], [94mLoss[0m : 2.38508
[1mStep[0m  [14/26], [94mLoss[0m : 2.30175
[1mStep[0m  [16/26], [94mLoss[0m : 2.46480
[1mStep[0m  [18/26], [94mLoss[0m : 2.31923
[1mStep[0m  [20/26], [94mLoss[0m : 2.40102
[1mStep[0m  [22/26], [94mLoss[0m : 2.40067
[1mStep[0m  [24/26], [94mLoss[0m : 2.32275

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31468
[1mStep[0m  [2/26], [94mLoss[0m : 2.19816
[1mStep[0m  [4/26], [94mLoss[0m : 2.41995
[1mStep[0m  [6/26], [94mLoss[0m : 2.43921
[1mStep[0m  [8/26], [94mLoss[0m : 2.42970
[1mStep[0m  [10/26], [94mLoss[0m : 2.28324
[1mStep[0m  [12/26], [94mLoss[0m : 2.32306
[1mStep[0m  [14/26], [94mLoss[0m : 2.32107
[1mStep[0m  [16/26], [94mLoss[0m : 2.42308
[1mStep[0m  [18/26], [94mLoss[0m : 2.27286
[1mStep[0m  [20/26], [94mLoss[0m : 2.34504
[1mStep[0m  [22/26], [94mLoss[0m : 2.33346
[1mStep[0m  [24/26], [94mLoss[0m : 2.31667

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17445
[1mStep[0m  [2/26], [94mLoss[0m : 2.42080
[1mStep[0m  [4/26], [94mLoss[0m : 2.34695
[1mStep[0m  [6/26], [94mLoss[0m : 2.41554
[1mStep[0m  [8/26], [94mLoss[0m : 2.34949
[1mStep[0m  [10/26], [94mLoss[0m : 2.38532
[1mStep[0m  [12/26], [94mLoss[0m : 2.23901
[1mStep[0m  [14/26], [94mLoss[0m : 2.33125
[1mStep[0m  [16/26], [94mLoss[0m : 2.29485
[1mStep[0m  [18/26], [94mLoss[0m : 2.43204
[1mStep[0m  [20/26], [94mLoss[0m : 2.43893
[1mStep[0m  [22/26], [94mLoss[0m : 2.43870
[1mStep[0m  [24/26], [94mLoss[0m : 2.33386

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.340
====================================

Phase 1 - Evaluation MAE:  2.339804539313683
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.50108
[1mStep[0m  [2/26], [94mLoss[0m : 2.22770
[1mStep[0m  [4/26], [94mLoss[0m : 2.30651
[1mStep[0m  [6/26], [94mLoss[0m : 2.63532
[1mStep[0m  [8/26], [94mLoss[0m : 2.42482
[1mStep[0m  [10/26], [94mLoss[0m : 2.45833
[1mStep[0m  [12/26], [94mLoss[0m : 2.56722
[1mStep[0m  [14/26], [94mLoss[0m : 2.49419
[1mStep[0m  [16/26], [94mLoss[0m : 2.30940
[1mStep[0m  [18/26], [94mLoss[0m : 2.63717
[1mStep[0m  [20/26], [94mLoss[0m : 2.40284
[1mStep[0m  [22/26], [94mLoss[0m : 2.67389
[1mStep[0m  [24/26], [94mLoss[0m : 2.49234

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47759
[1mStep[0m  [2/26], [94mLoss[0m : 2.46149
[1mStep[0m  [4/26], [94mLoss[0m : 2.39941
[1mStep[0m  [6/26], [94mLoss[0m : 2.36558
[1mStep[0m  [8/26], [94mLoss[0m : 2.51964
[1mStep[0m  [10/26], [94mLoss[0m : 2.45094
[1mStep[0m  [12/26], [94mLoss[0m : 2.49849
[1mStep[0m  [14/26], [94mLoss[0m : 2.30642
[1mStep[0m  [16/26], [94mLoss[0m : 2.31375
[1mStep[0m  [18/26], [94mLoss[0m : 2.52972
[1mStep[0m  [20/26], [94mLoss[0m : 2.52642
[1mStep[0m  [22/26], [94mLoss[0m : 2.43836
[1mStep[0m  [24/26], [94mLoss[0m : 2.34017

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.681, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21080
[1mStep[0m  [2/26], [94mLoss[0m : 2.19047
[1mStep[0m  [4/26], [94mLoss[0m : 2.41935
[1mStep[0m  [6/26], [94mLoss[0m : 2.32431
[1mStep[0m  [8/26], [94mLoss[0m : 2.31756
[1mStep[0m  [10/26], [94mLoss[0m : 2.29763
[1mStep[0m  [12/26], [94mLoss[0m : 2.46084
[1mStep[0m  [14/26], [94mLoss[0m : 2.43053
[1mStep[0m  [16/26], [94mLoss[0m : 2.22303
[1mStep[0m  [18/26], [94mLoss[0m : 2.53408
[1mStep[0m  [20/26], [94mLoss[0m : 2.25122
[1mStep[0m  [22/26], [94mLoss[0m : 2.21577
[1mStep[0m  [24/26], [94mLoss[0m : 2.31208

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.18454
[1mStep[0m  [2/26], [94mLoss[0m : 2.35083
[1mStep[0m  [4/26], [94mLoss[0m : 2.29552
[1mStep[0m  [6/26], [94mLoss[0m : 2.20850
[1mStep[0m  [8/26], [94mLoss[0m : 2.25672
[1mStep[0m  [10/26], [94mLoss[0m : 2.35861
[1mStep[0m  [12/26], [94mLoss[0m : 2.39476
[1mStep[0m  [14/26], [94mLoss[0m : 2.26681
[1mStep[0m  [16/26], [94mLoss[0m : 2.11622
[1mStep[0m  [18/26], [94mLoss[0m : 2.32453
[1mStep[0m  [20/26], [94mLoss[0m : 2.19678
[1mStep[0m  [22/26], [94mLoss[0m : 2.43813
[1mStep[0m  [24/26], [94mLoss[0m : 2.31952

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15825
[1mStep[0m  [2/26], [94mLoss[0m : 2.33280
[1mStep[0m  [4/26], [94mLoss[0m : 2.28377
[1mStep[0m  [6/26], [94mLoss[0m : 2.18576
[1mStep[0m  [8/26], [94mLoss[0m : 2.20242
[1mStep[0m  [10/26], [94mLoss[0m : 2.12534
[1mStep[0m  [12/26], [94mLoss[0m : 2.31931
[1mStep[0m  [14/26], [94mLoss[0m : 2.21929
[1mStep[0m  [16/26], [94mLoss[0m : 2.22466
[1mStep[0m  [18/26], [94mLoss[0m : 2.11601
[1mStep[0m  [20/26], [94mLoss[0m : 2.30409
[1mStep[0m  [22/26], [94mLoss[0m : 2.36818
[1mStep[0m  [24/26], [94mLoss[0m : 2.21833

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25995
[1mStep[0m  [2/26], [94mLoss[0m : 2.09238
[1mStep[0m  [4/26], [94mLoss[0m : 2.16672
[1mStep[0m  [6/26], [94mLoss[0m : 2.15919
[1mStep[0m  [8/26], [94mLoss[0m : 2.15725
[1mStep[0m  [10/26], [94mLoss[0m : 2.18710
[1mStep[0m  [12/26], [94mLoss[0m : 2.16690
[1mStep[0m  [14/26], [94mLoss[0m : 2.25792
[1mStep[0m  [16/26], [94mLoss[0m : 2.14815
[1mStep[0m  [18/26], [94mLoss[0m : 2.29433
[1mStep[0m  [20/26], [94mLoss[0m : 2.07844
[1mStep[0m  [22/26], [94mLoss[0m : 2.23954
[1mStep[0m  [24/26], [94mLoss[0m : 2.26623

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09506
[1mStep[0m  [2/26], [94mLoss[0m : 2.22844
[1mStep[0m  [4/26], [94mLoss[0m : 2.07906
[1mStep[0m  [6/26], [94mLoss[0m : 2.06928
[1mStep[0m  [8/26], [94mLoss[0m : 2.07312
[1mStep[0m  [10/26], [94mLoss[0m : 2.08300
[1mStep[0m  [12/26], [94mLoss[0m : 2.01529
[1mStep[0m  [14/26], [94mLoss[0m : 2.15001
[1mStep[0m  [16/26], [94mLoss[0m : 2.09068
[1mStep[0m  [18/26], [94mLoss[0m : 2.09821
[1mStep[0m  [20/26], [94mLoss[0m : 2.04097
[1mStep[0m  [22/26], [94mLoss[0m : 2.14189
[1mStep[0m  [24/26], [94mLoss[0m : 1.98519

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21987
[1mStep[0m  [2/26], [94mLoss[0m : 2.01960
[1mStep[0m  [4/26], [94mLoss[0m : 2.04408
[1mStep[0m  [6/26], [94mLoss[0m : 2.04895
[1mStep[0m  [8/26], [94mLoss[0m : 2.12071
[1mStep[0m  [10/26], [94mLoss[0m : 1.93330
[1mStep[0m  [12/26], [94mLoss[0m : 2.06444
[1mStep[0m  [14/26], [94mLoss[0m : 1.85686
[1mStep[0m  [16/26], [94mLoss[0m : 2.03125
[1mStep[0m  [18/26], [94mLoss[0m : 2.05051
[1mStep[0m  [20/26], [94mLoss[0m : 2.22302
[1mStep[0m  [22/26], [94mLoss[0m : 2.04341
[1mStep[0m  [24/26], [94mLoss[0m : 1.96397

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08504
[1mStep[0m  [2/26], [94mLoss[0m : 1.92986
[1mStep[0m  [4/26], [94mLoss[0m : 2.04017
[1mStep[0m  [6/26], [94mLoss[0m : 2.10233
[1mStep[0m  [8/26], [94mLoss[0m : 1.91432
[1mStep[0m  [10/26], [94mLoss[0m : 1.85016
[1mStep[0m  [12/26], [94mLoss[0m : 1.87999
[1mStep[0m  [14/26], [94mLoss[0m : 2.05219
[1mStep[0m  [16/26], [94mLoss[0m : 1.90595
[1mStep[0m  [18/26], [94mLoss[0m : 1.87067
[1mStep[0m  [20/26], [94mLoss[0m : 2.01513
[1mStep[0m  [22/26], [94mLoss[0m : 1.96026
[1mStep[0m  [24/26], [94mLoss[0m : 1.99014

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.00036
[1mStep[0m  [2/26], [94mLoss[0m : 1.94908
[1mStep[0m  [4/26], [94mLoss[0m : 1.90863
[1mStep[0m  [6/26], [94mLoss[0m : 1.95664
[1mStep[0m  [8/26], [94mLoss[0m : 2.03773
[1mStep[0m  [10/26], [94mLoss[0m : 1.87923
[1mStep[0m  [12/26], [94mLoss[0m : 1.95306
[1mStep[0m  [14/26], [94mLoss[0m : 1.90783
[1mStep[0m  [16/26], [94mLoss[0m : 2.07281
[1mStep[0m  [18/26], [94mLoss[0m : 2.05174
[1mStep[0m  [20/26], [94mLoss[0m : 1.96475
[1mStep[0m  [22/26], [94mLoss[0m : 1.99325
[1mStep[0m  [24/26], [94mLoss[0m : 1.99034

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98981
[1mStep[0m  [2/26], [94mLoss[0m : 1.91611
[1mStep[0m  [4/26], [94mLoss[0m : 1.85815
[1mStep[0m  [6/26], [94mLoss[0m : 1.85038
[1mStep[0m  [8/26], [94mLoss[0m : 1.81594
[1mStep[0m  [10/26], [94mLoss[0m : 1.84845
[1mStep[0m  [12/26], [94mLoss[0m : 1.88675
[1mStep[0m  [14/26], [94mLoss[0m : 2.01138
[1mStep[0m  [16/26], [94mLoss[0m : 1.88387
[1mStep[0m  [18/26], [94mLoss[0m : 2.00097
[1mStep[0m  [20/26], [94mLoss[0m : 2.07712
[1mStep[0m  [22/26], [94mLoss[0m : 1.89182
[1mStep[0m  [24/26], [94mLoss[0m : 1.90306

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.923, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.86754
[1mStep[0m  [2/26], [94mLoss[0m : 1.76667
[1mStep[0m  [4/26], [94mLoss[0m : 1.93121
[1mStep[0m  [6/26], [94mLoss[0m : 1.90952
[1mStep[0m  [8/26], [94mLoss[0m : 1.77840
[1mStep[0m  [10/26], [94mLoss[0m : 1.79444
[1mStep[0m  [12/26], [94mLoss[0m : 1.74811
[1mStep[0m  [14/26], [94mLoss[0m : 1.91324
[1mStep[0m  [16/26], [94mLoss[0m : 1.89526
[1mStep[0m  [18/26], [94mLoss[0m : 2.00927
[1mStep[0m  [20/26], [94mLoss[0m : 1.90450
[1mStep[0m  [22/26], [94mLoss[0m : 1.82850
[1mStep[0m  [24/26], [94mLoss[0m : 1.92240

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88006
[1mStep[0m  [2/26], [94mLoss[0m : 1.79085
[1mStep[0m  [4/26], [94mLoss[0m : 1.78233
[1mStep[0m  [6/26], [94mLoss[0m : 1.77984
[1mStep[0m  [8/26], [94mLoss[0m : 1.90273
[1mStep[0m  [10/26], [94mLoss[0m : 1.84461
[1mStep[0m  [12/26], [94mLoss[0m : 1.83238
[1mStep[0m  [14/26], [94mLoss[0m : 1.75782
[1mStep[0m  [16/26], [94mLoss[0m : 1.81655
[1mStep[0m  [18/26], [94mLoss[0m : 1.85081
[1mStep[0m  [20/26], [94mLoss[0m : 1.79271
[1mStep[0m  [22/26], [94mLoss[0m : 1.84689
[1mStep[0m  [24/26], [94mLoss[0m : 1.90029

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73980
[1mStep[0m  [2/26], [94mLoss[0m : 1.72453
[1mStep[0m  [4/26], [94mLoss[0m : 1.74916
[1mStep[0m  [6/26], [94mLoss[0m : 1.75577
[1mStep[0m  [8/26], [94mLoss[0m : 1.76557
[1mStep[0m  [10/26], [94mLoss[0m : 1.78873
[1mStep[0m  [12/26], [94mLoss[0m : 1.74830
[1mStep[0m  [14/26], [94mLoss[0m : 1.79419
[1mStep[0m  [16/26], [94mLoss[0m : 1.81637
[1mStep[0m  [18/26], [94mLoss[0m : 1.84604
[1mStep[0m  [20/26], [94mLoss[0m : 1.81234
[1mStep[0m  [22/26], [94mLoss[0m : 1.85206
[1mStep[0m  [24/26], [94mLoss[0m : 1.78928

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.68963
[1mStep[0m  [2/26], [94mLoss[0m : 1.76666
[1mStep[0m  [4/26], [94mLoss[0m : 1.79446
[1mStep[0m  [6/26], [94mLoss[0m : 1.70570
[1mStep[0m  [8/26], [94mLoss[0m : 1.70148
[1mStep[0m  [10/26], [94mLoss[0m : 1.73389
[1mStep[0m  [12/26], [94mLoss[0m : 1.69386
[1mStep[0m  [14/26], [94mLoss[0m : 1.78158
[1mStep[0m  [16/26], [94mLoss[0m : 1.62119
[1mStep[0m  [18/26], [94mLoss[0m : 1.70465
[1mStep[0m  [20/26], [94mLoss[0m : 1.68215
[1mStep[0m  [22/26], [94mLoss[0m : 1.86059
[1mStep[0m  [24/26], [94mLoss[0m : 1.94983

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.66389
[1mStep[0m  [2/26], [94mLoss[0m : 1.70631
[1mStep[0m  [4/26], [94mLoss[0m : 1.79368
[1mStep[0m  [6/26], [94mLoss[0m : 1.66288
[1mStep[0m  [8/26], [94mLoss[0m : 1.68238
[1mStep[0m  [10/26], [94mLoss[0m : 1.75556
[1mStep[0m  [12/26], [94mLoss[0m : 1.85240
[1mStep[0m  [14/26], [94mLoss[0m : 1.67234
[1mStep[0m  [16/26], [94mLoss[0m : 1.82501
[1mStep[0m  [18/26], [94mLoss[0m : 1.69440
[1mStep[0m  [20/26], [94mLoss[0m : 1.74614
[1mStep[0m  [22/26], [94mLoss[0m : 1.72289
[1mStep[0m  [24/26], [94mLoss[0m : 1.65281

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76815
[1mStep[0m  [2/26], [94mLoss[0m : 1.61165
[1mStep[0m  [4/26], [94mLoss[0m : 1.54016
[1mStep[0m  [6/26], [94mLoss[0m : 1.59486
[1mStep[0m  [8/26], [94mLoss[0m : 1.69573
[1mStep[0m  [10/26], [94mLoss[0m : 1.81259
[1mStep[0m  [12/26], [94mLoss[0m : 1.61479
[1mStep[0m  [14/26], [94mLoss[0m : 1.67194
[1mStep[0m  [16/26], [94mLoss[0m : 1.67804
[1mStep[0m  [18/26], [94mLoss[0m : 1.59076
[1mStep[0m  [20/26], [94mLoss[0m : 1.67214
[1mStep[0m  [22/26], [94mLoss[0m : 1.61039
[1mStep[0m  [24/26], [94mLoss[0m : 1.77281

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61648
[1mStep[0m  [2/26], [94mLoss[0m : 1.59880
[1mStep[0m  [4/26], [94mLoss[0m : 1.64216
[1mStep[0m  [6/26], [94mLoss[0m : 1.63109
[1mStep[0m  [8/26], [94mLoss[0m : 1.66090
[1mStep[0m  [10/26], [94mLoss[0m : 1.65784
[1mStep[0m  [12/26], [94mLoss[0m : 1.71464
[1mStep[0m  [14/26], [94mLoss[0m : 1.68593
[1mStep[0m  [16/26], [94mLoss[0m : 1.61815
[1mStep[0m  [18/26], [94mLoss[0m : 1.71735
[1mStep[0m  [20/26], [94mLoss[0m : 1.77630
[1mStep[0m  [22/26], [94mLoss[0m : 1.82578
[1mStep[0m  [24/26], [94mLoss[0m : 1.62549

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.670, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.67857
[1mStep[0m  [2/26], [94mLoss[0m : 1.55805
[1mStep[0m  [4/26], [94mLoss[0m : 1.59181
[1mStep[0m  [6/26], [94mLoss[0m : 1.63104
[1mStep[0m  [8/26], [94mLoss[0m : 1.62605
[1mStep[0m  [10/26], [94mLoss[0m : 1.65726
[1mStep[0m  [12/26], [94mLoss[0m : 1.64516
[1mStep[0m  [14/26], [94mLoss[0m : 1.65054
[1mStep[0m  [16/26], [94mLoss[0m : 1.80471
[1mStep[0m  [18/26], [94mLoss[0m : 1.55844
[1mStep[0m  [20/26], [94mLoss[0m : 1.77479
[1mStep[0m  [22/26], [94mLoss[0m : 1.66434
[1mStep[0m  [24/26], [94mLoss[0m : 1.79832

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.60307
[1mStep[0m  [2/26], [94mLoss[0m : 1.57036
[1mStep[0m  [4/26], [94mLoss[0m : 1.60820
[1mStep[0m  [6/26], [94mLoss[0m : 1.58501
[1mStep[0m  [8/26], [94mLoss[0m : 1.50819
[1mStep[0m  [10/26], [94mLoss[0m : 1.61989
[1mStep[0m  [12/26], [94mLoss[0m : 1.63039
[1mStep[0m  [14/26], [94mLoss[0m : 1.61853
[1mStep[0m  [16/26], [94mLoss[0m : 1.57278
[1mStep[0m  [18/26], [94mLoss[0m : 1.72894
[1mStep[0m  [20/26], [94mLoss[0m : 1.72300
[1mStep[0m  [22/26], [94mLoss[0m : 1.71948
[1mStep[0m  [24/26], [94mLoss[0m : 1.60779

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61387
[1mStep[0m  [2/26], [94mLoss[0m : 1.48800
[1mStep[0m  [4/26], [94mLoss[0m : 1.66320
[1mStep[0m  [6/26], [94mLoss[0m : 1.65808
[1mStep[0m  [8/26], [94mLoss[0m : 1.57093
[1mStep[0m  [10/26], [94mLoss[0m : 1.55666
[1mStep[0m  [12/26], [94mLoss[0m : 1.59064
[1mStep[0m  [14/26], [94mLoss[0m : 1.47761
[1mStep[0m  [16/26], [94mLoss[0m : 1.47258
[1mStep[0m  [18/26], [94mLoss[0m : 1.65361
[1mStep[0m  [20/26], [94mLoss[0m : 1.61763
[1mStep[0m  [22/26], [94mLoss[0m : 1.60741
[1mStep[0m  [24/26], [94mLoss[0m : 1.48685

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52777
[1mStep[0m  [2/26], [94mLoss[0m : 1.47773
[1mStep[0m  [4/26], [94mLoss[0m : 1.45617
[1mStep[0m  [6/26], [94mLoss[0m : 1.62777
[1mStep[0m  [8/26], [94mLoss[0m : 1.58343
[1mStep[0m  [10/26], [94mLoss[0m : 1.61557
[1mStep[0m  [12/26], [94mLoss[0m : 1.59131
[1mStep[0m  [14/26], [94mLoss[0m : 1.59831
[1mStep[0m  [16/26], [94mLoss[0m : 1.64535
[1mStep[0m  [18/26], [94mLoss[0m : 1.65596
[1mStep[0m  [20/26], [94mLoss[0m : 1.71747
[1mStep[0m  [22/26], [94mLoss[0m : 1.53418
[1mStep[0m  [24/26], [94mLoss[0m : 1.65399

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.464, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49994
[1mStep[0m  [2/26], [94mLoss[0m : 1.58949
[1mStep[0m  [4/26], [94mLoss[0m : 1.60224
[1mStep[0m  [6/26], [94mLoss[0m : 1.62443
[1mStep[0m  [8/26], [94mLoss[0m : 1.48750
[1mStep[0m  [10/26], [94mLoss[0m : 1.54671
[1mStep[0m  [12/26], [94mLoss[0m : 1.57786
[1mStep[0m  [14/26], [94mLoss[0m : 1.50436
[1mStep[0m  [16/26], [94mLoss[0m : 1.53360
[1mStep[0m  [18/26], [94mLoss[0m : 1.49457
[1mStep[0m  [20/26], [94mLoss[0m : 1.51827
[1mStep[0m  [22/26], [94mLoss[0m : 1.54033
[1mStep[0m  [24/26], [94mLoss[0m : 1.63225

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.41869
[1mStep[0m  [2/26], [94mLoss[0m : 1.53249
[1mStep[0m  [4/26], [94mLoss[0m : 1.53008
[1mStep[0m  [6/26], [94mLoss[0m : 1.49553
[1mStep[0m  [8/26], [94mLoss[0m : 1.52512
[1mStep[0m  [10/26], [94mLoss[0m : 1.53928
[1mStep[0m  [12/26], [94mLoss[0m : 1.46510
[1mStep[0m  [14/26], [94mLoss[0m : 1.54826
[1mStep[0m  [16/26], [94mLoss[0m : 1.50437
[1mStep[0m  [18/26], [94mLoss[0m : 1.54293
[1mStep[0m  [20/26], [94mLoss[0m : 1.46346
[1mStep[0m  [22/26], [94mLoss[0m : 1.58569
[1mStep[0m  [24/26], [94mLoss[0m : 1.45442

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.507, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.48292
[1mStep[0m  [2/26], [94mLoss[0m : 1.41183
[1mStep[0m  [4/26], [94mLoss[0m : 1.43549
[1mStep[0m  [6/26], [94mLoss[0m : 1.52225
[1mStep[0m  [8/26], [94mLoss[0m : 1.42117
[1mStep[0m  [10/26], [94mLoss[0m : 1.58476
[1mStep[0m  [12/26], [94mLoss[0m : 1.49243
[1mStep[0m  [14/26], [94mLoss[0m : 1.50246
[1mStep[0m  [16/26], [94mLoss[0m : 1.62510
[1mStep[0m  [18/26], [94mLoss[0m : 1.49032
[1mStep[0m  [20/26], [94mLoss[0m : 1.39540
[1mStep[0m  [22/26], [94mLoss[0m : 1.51559
[1mStep[0m  [24/26], [94mLoss[0m : 1.48180

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.39114
[1mStep[0m  [2/26], [94mLoss[0m : 1.49209
[1mStep[0m  [4/26], [94mLoss[0m : 1.38903
[1mStep[0m  [6/26], [94mLoss[0m : 1.43475
[1mStep[0m  [8/26], [94mLoss[0m : 1.51288
[1mStep[0m  [10/26], [94mLoss[0m : 1.46945
[1mStep[0m  [12/26], [94mLoss[0m : 1.37493
[1mStep[0m  [14/26], [94mLoss[0m : 1.45840
[1mStep[0m  [16/26], [94mLoss[0m : 1.56040
[1mStep[0m  [18/26], [94mLoss[0m : 1.45066
[1mStep[0m  [20/26], [94mLoss[0m : 1.45962
[1mStep[0m  [22/26], [94mLoss[0m : 1.47987
[1mStep[0m  [24/26], [94mLoss[0m : 1.54492

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.467, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.45204
[1mStep[0m  [2/26], [94mLoss[0m : 1.45050
[1mStep[0m  [4/26], [94mLoss[0m : 1.45942
[1mStep[0m  [6/26], [94mLoss[0m : 1.38797
[1mStep[0m  [8/26], [94mLoss[0m : 1.57306
[1mStep[0m  [10/26], [94mLoss[0m : 1.52084
[1mStep[0m  [12/26], [94mLoss[0m : 1.46078
[1mStep[0m  [14/26], [94mLoss[0m : 1.44401
[1mStep[0m  [16/26], [94mLoss[0m : 1.49361
[1mStep[0m  [18/26], [94mLoss[0m : 1.45264
[1mStep[0m  [20/26], [94mLoss[0m : 1.35989
[1mStep[0m  [22/26], [94mLoss[0m : 1.52967
[1mStep[0m  [24/26], [94mLoss[0m : 1.60443

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.37134
[1mStep[0m  [2/26], [94mLoss[0m : 1.35634
[1mStep[0m  [4/26], [94mLoss[0m : 1.45615
[1mStep[0m  [6/26], [94mLoss[0m : 1.38853
[1mStep[0m  [8/26], [94mLoss[0m : 1.43532
[1mStep[0m  [10/26], [94mLoss[0m : 1.50125
[1mStep[0m  [12/26], [94mLoss[0m : 1.50661
[1mStep[0m  [14/26], [94mLoss[0m : 1.46007
[1mStep[0m  [16/26], [94mLoss[0m : 1.46271
[1mStep[0m  [18/26], [94mLoss[0m : 1.49937
[1mStep[0m  [20/26], [94mLoss[0m : 1.51754
[1mStep[0m  [22/26], [94mLoss[0m : 1.45710
[1mStep[0m  [24/26], [94mLoss[0m : 1.38695

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.38652
[1mStep[0m  [2/26], [94mLoss[0m : 1.31566
[1mStep[0m  [4/26], [94mLoss[0m : 1.40656
[1mStep[0m  [6/26], [94mLoss[0m : 1.38829
[1mStep[0m  [8/26], [94mLoss[0m : 1.38023
[1mStep[0m  [10/26], [94mLoss[0m : 1.33171
[1mStep[0m  [12/26], [94mLoss[0m : 1.41442
[1mStep[0m  [14/26], [94mLoss[0m : 1.40754
[1mStep[0m  [16/26], [94mLoss[0m : 1.43193
[1mStep[0m  [18/26], [94mLoss[0m : 1.47973
[1mStep[0m  [20/26], [94mLoss[0m : 1.44137
[1mStep[0m  [22/26], [94mLoss[0m : 1.44132
[1mStep[0m  [24/26], [94mLoss[0m : 1.43052

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.504, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.33355
[1mStep[0m  [2/26], [94mLoss[0m : 1.39377
[1mStep[0m  [4/26], [94mLoss[0m : 1.40949
[1mStep[0m  [6/26], [94mLoss[0m : 1.37746
[1mStep[0m  [8/26], [94mLoss[0m : 1.36519
[1mStep[0m  [10/26], [94mLoss[0m : 1.45976
[1mStep[0m  [12/26], [94mLoss[0m : 1.39298
[1mStep[0m  [14/26], [94mLoss[0m : 1.42186
[1mStep[0m  [16/26], [94mLoss[0m : 1.40523
[1mStep[0m  [18/26], [94mLoss[0m : 1.45519
[1mStep[0m  [20/26], [94mLoss[0m : 1.37387
[1mStep[0m  [22/26], [94mLoss[0m : 1.46513
[1mStep[0m  [24/26], [94mLoss[0m : 1.44082

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.525, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.486
====================================

Phase 2 - Evaluation MAE:  2.4856041027949405
MAE score P1       2.339805
MAE score P2       2.485604
loss               1.411178
learning_rate       0.00505
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/427], [94mLoss[0m : 9.64540
[1mStep[0m  [42/427], [94mLoss[0m : 3.52961
[1mStep[0m  [84/427], [94mLoss[0m : 3.47340
[1mStep[0m  [126/427], [94mLoss[0m : 3.46245
[1mStep[0m  [168/427], [94mLoss[0m : 2.99041
[1mStep[0m  [210/427], [94mLoss[0m : 3.19930
[1mStep[0m  [252/427], [94mLoss[0m : 2.42803
[1mStep[0m  [294/427], [94mLoss[0m : 2.88543
[1mStep[0m  [336/427], [94mLoss[0m : 2.27505
[1mStep[0m  [378/427], [94mLoss[0m : 3.47233
[1mStep[0m  [420/427], [94mLoss[0m : 2.97960

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.210, [92mTest[0m: 10.868, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.00226
[1mStep[0m  [42/427], [94mLoss[0m : 2.99513
[1mStep[0m  [84/427], [94mLoss[0m : 2.60506
[1mStep[0m  [126/427], [94mLoss[0m : 3.17169
[1mStep[0m  [168/427], [94mLoss[0m : 2.91082
[1mStep[0m  [210/427], [94mLoss[0m : 3.30102
[1mStep[0m  [252/427], [94mLoss[0m : 3.45509
[1mStep[0m  [294/427], [94mLoss[0m : 2.27635
[1mStep[0m  [336/427], [94mLoss[0m : 2.73768
[1mStep[0m  [378/427], [94mLoss[0m : 2.69401
[1mStep[0m  [420/427], [94mLoss[0m : 2.98782

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.624, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.08026
[1mStep[0m  [42/427], [94mLoss[0m : 2.03078
[1mStep[0m  [84/427], [94mLoss[0m : 2.53244
[1mStep[0m  [126/427], [94mLoss[0m : 2.48846
[1mStep[0m  [168/427], [94mLoss[0m : 2.81742
[1mStep[0m  [210/427], [94mLoss[0m : 2.43734
[1mStep[0m  [252/427], [94mLoss[0m : 2.49677
[1mStep[0m  [294/427], [94mLoss[0m : 2.88352
[1mStep[0m  [336/427], [94mLoss[0m : 3.22331
[1mStep[0m  [378/427], [94mLoss[0m : 3.11388
[1mStep[0m  [420/427], [94mLoss[0m : 2.73730

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.35938
[1mStep[0m  [42/427], [94mLoss[0m : 3.00104
[1mStep[0m  [84/427], [94mLoss[0m : 2.49348
[1mStep[0m  [126/427], [94mLoss[0m : 2.54435
[1mStep[0m  [168/427], [94mLoss[0m : 2.81924
[1mStep[0m  [210/427], [94mLoss[0m : 2.86903
[1mStep[0m  [252/427], [94mLoss[0m : 2.52035
[1mStep[0m  [294/427], [94mLoss[0m : 2.45220
[1mStep[0m  [336/427], [94mLoss[0m : 2.16092
[1mStep[0m  [378/427], [94mLoss[0m : 2.04499
[1mStep[0m  [420/427], [94mLoss[0m : 2.37252

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.01915
[1mStep[0m  [42/427], [94mLoss[0m : 1.86851
[1mStep[0m  [84/427], [94mLoss[0m : 2.48187
[1mStep[0m  [126/427], [94mLoss[0m : 2.38759
[1mStep[0m  [168/427], [94mLoss[0m : 2.38764
[1mStep[0m  [210/427], [94mLoss[0m : 3.34093
[1mStep[0m  [252/427], [94mLoss[0m : 2.45650
[1mStep[0m  [294/427], [94mLoss[0m : 2.63551
[1mStep[0m  [336/427], [94mLoss[0m : 2.46377
[1mStep[0m  [378/427], [94mLoss[0m : 2.62347
[1mStep[0m  [420/427], [94mLoss[0m : 2.51075

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.66592
[1mStep[0m  [42/427], [94mLoss[0m : 2.21103
[1mStep[0m  [84/427], [94mLoss[0m : 2.84394
[1mStep[0m  [126/427], [94mLoss[0m : 2.61012
[1mStep[0m  [168/427], [94mLoss[0m : 2.14047
[1mStep[0m  [210/427], [94mLoss[0m : 2.67441
[1mStep[0m  [252/427], [94mLoss[0m : 2.86677
[1mStep[0m  [294/427], [94mLoss[0m : 2.02802
[1mStep[0m  [336/427], [94mLoss[0m : 2.15098
[1mStep[0m  [378/427], [94mLoss[0m : 2.45243
[1mStep[0m  [420/427], [94mLoss[0m : 3.03265

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.89582
[1mStep[0m  [42/427], [94mLoss[0m : 2.54686
[1mStep[0m  [84/427], [94mLoss[0m : 3.11249
[1mStep[0m  [126/427], [94mLoss[0m : 2.51729
[1mStep[0m  [168/427], [94mLoss[0m : 2.37370
[1mStep[0m  [210/427], [94mLoss[0m : 2.71804
[1mStep[0m  [252/427], [94mLoss[0m : 2.36495
[1mStep[0m  [294/427], [94mLoss[0m : 2.02445
[1mStep[0m  [336/427], [94mLoss[0m : 2.19293
[1mStep[0m  [378/427], [94mLoss[0m : 2.91792
[1mStep[0m  [420/427], [94mLoss[0m : 2.63576

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.41086
[1mStep[0m  [42/427], [94mLoss[0m : 2.41341
[1mStep[0m  [84/427], [94mLoss[0m : 2.62878
[1mStep[0m  [126/427], [94mLoss[0m : 2.83746
[1mStep[0m  [168/427], [94mLoss[0m : 2.83107
[1mStep[0m  [210/427], [94mLoss[0m : 2.21836
[1mStep[0m  [252/427], [94mLoss[0m : 2.41343
[1mStep[0m  [294/427], [94mLoss[0m : 2.32214
[1mStep[0m  [336/427], [94mLoss[0m : 2.77285
[1mStep[0m  [378/427], [94mLoss[0m : 2.52821
[1mStep[0m  [420/427], [94mLoss[0m : 1.66414

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.49855
[1mStep[0m  [42/427], [94mLoss[0m : 2.24904
[1mStep[0m  [84/427], [94mLoss[0m : 2.65668
[1mStep[0m  [126/427], [94mLoss[0m : 2.82774
[1mStep[0m  [168/427], [94mLoss[0m : 2.22713
[1mStep[0m  [210/427], [94mLoss[0m : 2.55580
[1mStep[0m  [252/427], [94mLoss[0m : 2.68522
[1mStep[0m  [294/427], [94mLoss[0m : 3.21966
[1mStep[0m  [336/427], [94mLoss[0m : 2.66170
[1mStep[0m  [378/427], [94mLoss[0m : 2.70459
[1mStep[0m  [420/427], [94mLoss[0m : 2.82307

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.84272
[1mStep[0m  [42/427], [94mLoss[0m : 2.55553
[1mStep[0m  [84/427], [94mLoss[0m : 2.81357
[1mStep[0m  [126/427], [94mLoss[0m : 2.29253
[1mStep[0m  [168/427], [94mLoss[0m : 2.63163
[1mStep[0m  [210/427], [94mLoss[0m : 2.41144
[1mStep[0m  [252/427], [94mLoss[0m : 2.91968
[1mStep[0m  [294/427], [94mLoss[0m : 2.96506
[1mStep[0m  [336/427], [94mLoss[0m : 3.01832
[1mStep[0m  [378/427], [94mLoss[0m : 2.54434
[1mStep[0m  [420/427], [94mLoss[0m : 2.36604

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.11667
[1mStep[0m  [42/427], [94mLoss[0m : 2.53146
[1mStep[0m  [84/427], [94mLoss[0m : 2.52217
[1mStep[0m  [126/427], [94mLoss[0m : 2.43010
[1mStep[0m  [168/427], [94mLoss[0m : 2.57768
[1mStep[0m  [210/427], [94mLoss[0m : 3.17243
[1mStep[0m  [252/427], [94mLoss[0m : 3.13873
[1mStep[0m  [294/427], [94mLoss[0m : 2.48238
[1mStep[0m  [336/427], [94mLoss[0m : 2.44795
[1mStep[0m  [378/427], [94mLoss[0m : 2.70608
[1mStep[0m  [420/427], [94mLoss[0m : 2.42473

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32878
[1mStep[0m  [42/427], [94mLoss[0m : 2.73252
[1mStep[0m  [84/427], [94mLoss[0m : 2.85045
[1mStep[0m  [126/427], [94mLoss[0m : 2.55374
[1mStep[0m  [168/427], [94mLoss[0m : 2.25024
[1mStep[0m  [210/427], [94mLoss[0m : 2.47206
[1mStep[0m  [252/427], [94mLoss[0m : 2.76316
[1mStep[0m  [294/427], [94mLoss[0m : 2.15060
[1mStep[0m  [336/427], [94mLoss[0m : 1.87951
[1mStep[0m  [378/427], [94mLoss[0m : 2.42529
[1mStep[0m  [420/427], [94mLoss[0m : 3.24328

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.39483
[1mStep[0m  [42/427], [94mLoss[0m : 2.49723
[1mStep[0m  [84/427], [94mLoss[0m : 2.29611
[1mStep[0m  [126/427], [94mLoss[0m : 2.61056
[1mStep[0m  [168/427], [94mLoss[0m : 2.08660
[1mStep[0m  [210/427], [94mLoss[0m : 2.96741
[1mStep[0m  [252/427], [94mLoss[0m : 2.60047
[1mStep[0m  [294/427], [94mLoss[0m : 2.92795
[1mStep[0m  [336/427], [94mLoss[0m : 2.44200
[1mStep[0m  [378/427], [94mLoss[0m : 3.11950
[1mStep[0m  [420/427], [94mLoss[0m : 2.35947

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.59376
[1mStep[0m  [42/427], [94mLoss[0m : 2.41676
[1mStep[0m  [84/427], [94mLoss[0m : 1.79017
[1mStep[0m  [126/427], [94mLoss[0m : 1.75507
[1mStep[0m  [168/427], [94mLoss[0m : 2.69390
[1mStep[0m  [210/427], [94mLoss[0m : 2.12667
[1mStep[0m  [252/427], [94mLoss[0m : 2.66735
[1mStep[0m  [294/427], [94mLoss[0m : 2.84587
[1mStep[0m  [336/427], [94mLoss[0m : 2.05834
[1mStep[0m  [378/427], [94mLoss[0m : 2.08917
[1mStep[0m  [420/427], [94mLoss[0m : 2.26325

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.07680
[1mStep[0m  [42/427], [94mLoss[0m : 2.72244
[1mStep[0m  [84/427], [94mLoss[0m : 2.01013
[1mStep[0m  [126/427], [94mLoss[0m : 2.19553
[1mStep[0m  [168/427], [94mLoss[0m : 2.53833
[1mStep[0m  [210/427], [94mLoss[0m : 3.21594
[1mStep[0m  [252/427], [94mLoss[0m : 2.53904
[1mStep[0m  [294/427], [94mLoss[0m : 1.81978
[1mStep[0m  [336/427], [94mLoss[0m : 2.64398
[1mStep[0m  [378/427], [94mLoss[0m : 2.73268
[1mStep[0m  [420/427], [94mLoss[0m : 2.89096

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.59442
[1mStep[0m  [42/427], [94mLoss[0m : 2.18178
[1mStep[0m  [84/427], [94mLoss[0m : 2.17542
[1mStep[0m  [126/427], [94mLoss[0m : 2.86471
[1mStep[0m  [168/427], [94mLoss[0m : 2.31745
[1mStep[0m  [210/427], [94mLoss[0m : 2.48166
[1mStep[0m  [252/427], [94mLoss[0m : 2.47183
[1mStep[0m  [294/427], [94mLoss[0m : 2.41670
[1mStep[0m  [336/427], [94mLoss[0m : 2.05788
[1mStep[0m  [378/427], [94mLoss[0m : 2.04035
[1mStep[0m  [420/427], [94mLoss[0m : 2.37498

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.28104
[1mStep[0m  [42/427], [94mLoss[0m : 1.88667
[1mStep[0m  [84/427], [94mLoss[0m : 2.37767
[1mStep[0m  [126/427], [94mLoss[0m : 2.17096
[1mStep[0m  [168/427], [94mLoss[0m : 2.52369
[1mStep[0m  [210/427], [94mLoss[0m : 2.93140
[1mStep[0m  [252/427], [94mLoss[0m : 2.52828
[1mStep[0m  [294/427], [94mLoss[0m : 2.46307
[1mStep[0m  [336/427], [94mLoss[0m : 2.75366
[1mStep[0m  [378/427], [94mLoss[0m : 2.50537
[1mStep[0m  [420/427], [94mLoss[0m : 2.30990

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.66408
[1mStep[0m  [42/427], [94mLoss[0m : 2.01943
[1mStep[0m  [84/427], [94mLoss[0m : 2.67458
[1mStep[0m  [126/427], [94mLoss[0m : 2.40380
[1mStep[0m  [168/427], [94mLoss[0m : 2.38333
[1mStep[0m  [210/427], [94mLoss[0m : 2.20028
[1mStep[0m  [252/427], [94mLoss[0m : 1.84046
[1mStep[0m  [294/427], [94mLoss[0m : 2.43206
[1mStep[0m  [336/427], [94mLoss[0m : 2.47488
[1mStep[0m  [378/427], [94mLoss[0m : 2.10929
[1mStep[0m  [420/427], [94mLoss[0m : 2.71237

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.38911
[1mStep[0m  [42/427], [94mLoss[0m : 1.83345
[1mStep[0m  [84/427], [94mLoss[0m : 2.18177
[1mStep[0m  [126/427], [94mLoss[0m : 3.18199
[1mStep[0m  [168/427], [94mLoss[0m : 2.03000
[1mStep[0m  [210/427], [94mLoss[0m : 2.31691
[1mStep[0m  [252/427], [94mLoss[0m : 2.31722
[1mStep[0m  [294/427], [94mLoss[0m : 2.38892
[1mStep[0m  [336/427], [94mLoss[0m : 2.32449
[1mStep[0m  [378/427], [94mLoss[0m : 2.97736
[1mStep[0m  [420/427], [94mLoss[0m : 1.83881

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.58172
[1mStep[0m  [42/427], [94mLoss[0m : 1.61017
[1mStep[0m  [84/427], [94mLoss[0m : 2.95768
[1mStep[0m  [126/427], [94mLoss[0m : 1.79575
[1mStep[0m  [168/427], [94mLoss[0m : 3.12428
[1mStep[0m  [210/427], [94mLoss[0m : 3.95411
[1mStep[0m  [252/427], [94mLoss[0m : 2.32545
[1mStep[0m  [294/427], [94mLoss[0m : 2.85945
[1mStep[0m  [336/427], [94mLoss[0m : 2.24986
[1mStep[0m  [378/427], [94mLoss[0m : 2.28840
[1mStep[0m  [420/427], [94mLoss[0m : 3.28005

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.16700
[1mStep[0m  [42/427], [94mLoss[0m : 2.63017
[1mStep[0m  [84/427], [94mLoss[0m : 2.42015
[1mStep[0m  [126/427], [94mLoss[0m : 2.17687
[1mStep[0m  [168/427], [94mLoss[0m : 2.29910
[1mStep[0m  [210/427], [94mLoss[0m : 2.25443
[1mStep[0m  [252/427], [94mLoss[0m : 2.12151
[1mStep[0m  [294/427], [94mLoss[0m : 2.70732
[1mStep[0m  [336/427], [94mLoss[0m : 2.82878
[1mStep[0m  [378/427], [94mLoss[0m : 2.47536
[1mStep[0m  [420/427], [94mLoss[0m : 2.99472

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.37626
[1mStep[0m  [42/427], [94mLoss[0m : 2.37506
[1mStep[0m  [84/427], [94mLoss[0m : 1.89033
[1mStep[0m  [126/427], [94mLoss[0m : 2.19365
[1mStep[0m  [168/427], [94mLoss[0m : 2.52401
[1mStep[0m  [210/427], [94mLoss[0m : 2.51619
[1mStep[0m  [252/427], [94mLoss[0m : 2.50103
[1mStep[0m  [294/427], [94mLoss[0m : 2.82098
[1mStep[0m  [336/427], [94mLoss[0m : 3.21883
[1mStep[0m  [378/427], [94mLoss[0m : 2.15548
[1mStep[0m  [420/427], [94mLoss[0m : 2.27505

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.384, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.45787
[1mStep[0m  [42/427], [94mLoss[0m : 1.94603
[1mStep[0m  [84/427], [94mLoss[0m : 2.16180
[1mStep[0m  [126/427], [94mLoss[0m : 1.68408
[1mStep[0m  [168/427], [94mLoss[0m : 2.44869
[1mStep[0m  [210/427], [94mLoss[0m : 1.80928
[1mStep[0m  [252/427], [94mLoss[0m : 2.45887
[1mStep[0m  [294/427], [94mLoss[0m : 2.27970
[1mStep[0m  [336/427], [94mLoss[0m : 3.00394
[1mStep[0m  [378/427], [94mLoss[0m : 2.99092
[1mStep[0m  [420/427], [94mLoss[0m : 2.50513

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.38255
[1mStep[0m  [42/427], [94mLoss[0m : 2.86527
[1mStep[0m  [84/427], [94mLoss[0m : 1.98601
[1mStep[0m  [126/427], [94mLoss[0m : 2.62790
[1mStep[0m  [168/427], [94mLoss[0m : 2.48416
[1mStep[0m  [210/427], [94mLoss[0m : 2.29453
[1mStep[0m  [252/427], [94mLoss[0m : 3.07200
[1mStep[0m  [294/427], [94mLoss[0m : 2.82335
[1mStep[0m  [336/427], [94mLoss[0m : 2.30725
[1mStep[0m  [378/427], [94mLoss[0m : 2.09123
[1mStep[0m  [420/427], [94mLoss[0m : 2.41414

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.391, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.51083
[1mStep[0m  [42/427], [94mLoss[0m : 1.83036
[1mStep[0m  [84/427], [94mLoss[0m : 2.34508
[1mStep[0m  [126/427], [94mLoss[0m : 2.35315
[1mStep[0m  [168/427], [94mLoss[0m : 2.42015
[1mStep[0m  [210/427], [94mLoss[0m : 2.94862
[1mStep[0m  [252/427], [94mLoss[0m : 2.56522
[1mStep[0m  [294/427], [94mLoss[0m : 2.60766
[1mStep[0m  [336/427], [94mLoss[0m : 2.89634
[1mStep[0m  [378/427], [94mLoss[0m : 3.19350
[1mStep[0m  [420/427], [94mLoss[0m : 2.35943

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.34986
[1mStep[0m  [42/427], [94mLoss[0m : 2.54152
[1mStep[0m  [84/427], [94mLoss[0m : 2.20600
[1mStep[0m  [126/427], [94mLoss[0m : 2.51076
[1mStep[0m  [168/427], [94mLoss[0m : 2.14876
[1mStep[0m  [210/427], [94mLoss[0m : 2.82566
[1mStep[0m  [252/427], [94mLoss[0m : 2.13446
[1mStep[0m  [294/427], [94mLoss[0m : 2.65717
[1mStep[0m  [336/427], [94mLoss[0m : 1.96485
[1mStep[0m  [378/427], [94mLoss[0m : 2.31687
[1mStep[0m  [420/427], [94mLoss[0m : 2.15106

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.370, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.93105
[1mStep[0m  [42/427], [94mLoss[0m : 2.54052
[1mStep[0m  [84/427], [94mLoss[0m : 3.05356
[1mStep[0m  [126/427], [94mLoss[0m : 2.39279
[1mStep[0m  [168/427], [94mLoss[0m : 2.51081
[1mStep[0m  [210/427], [94mLoss[0m : 2.74669
[1mStep[0m  [252/427], [94mLoss[0m : 2.45837
[1mStep[0m  [294/427], [94mLoss[0m : 2.37144
[1mStep[0m  [336/427], [94mLoss[0m : 2.43903
[1mStep[0m  [378/427], [94mLoss[0m : 2.03415
[1mStep[0m  [420/427], [94mLoss[0m : 2.37380

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.59728
[1mStep[0m  [42/427], [94mLoss[0m : 2.29194
[1mStep[0m  [84/427], [94mLoss[0m : 3.22871
[1mStep[0m  [126/427], [94mLoss[0m : 2.66619
[1mStep[0m  [168/427], [94mLoss[0m : 2.52280
[1mStep[0m  [210/427], [94mLoss[0m : 2.86911
[1mStep[0m  [252/427], [94mLoss[0m : 2.48499
[1mStep[0m  [294/427], [94mLoss[0m : 2.66321
[1mStep[0m  [336/427], [94mLoss[0m : 1.94202
[1mStep[0m  [378/427], [94mLoss[0m : 2.56821
[1mStep[0m  [420/427], [94mLoss[0m : 2.66050

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.349, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.75635
[1mStep[0m  [42/427], [94mLoss[0m : 2.29208
[1mStep[0m  [84/427], [94mLoss[0m : 1.49660
[1mStep[0m  [126/427], [94mLoss[0m : 2.50994
[1mStep[0m  [168/427], [94mLoss[0m : 2.37516
[1mStep[0m  [210/427], [94mLoss[0m : 2.35326
[1mStep[0m  [252/427], [94mLoss[0m : 2.31202
[1mStep[0m  [294/427], [94mLoss[0m : 3.48715
[1mStep[0m  [336/427], [94mLoss[0m : 2.80605
[1mStep[0m  [378/427], [94mLoss[0m : 2.51520
[1mStep[0m  [420/427], [94mLoss[0m : 3.15814

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.05601
[1mStep[0m  [42/427], [94mLoss[0m : 2.21341
[1mStep[0m  [84/427], [94mLoss[0m : 2.60458
[1mStep[0m  [126/427], [94mLoss[0m : 2.21098
[1mStep[0m  [168/427], [94mLoss[0m : 2.83335
[1mStep[0m  [210/427], [94mLoss[0m : 2.06494
[1mStep[0m  [252/427], [94mLoss[0m : 2.37054
[1mStep[0m  [294/427], [94mLoss[0m : 2.79382
[1mStep[0m  [336/427], [94mLoss[0m : 2.19785
[1mStep[0m  [378/427], [94mLoss[0m : 2.02294
[1mStep[0m  [420/427], [94mLoss[0m : 1.66704

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.374
====================================

Phase 1 - Evaluation MAE:  2.374313688613999
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/427], [94mLoss[0m : 2.59702
[1mStep[0m  [42/427], [94mLoss[0m : 2.45878
[1mStep[0m  [84/427], [94mLoss[0m : 2.15047
[1mStep[0m  [126/427], [94mLoss[0m : 2.94274
[1mStep[0m  [168/427], [94mLoss[0m : 2.32216
[1mStep[0m  [210/427], [94mLoss[0m : 2.58584
[1mStep[0m  [252/427], [94mLoss[0m : 2.15920
[1mStep[0m  [294/427], [94mLoss[0m : 2.44080
[1mStep[0m  [336/427], [94mLoss[0m : 2.91550
[1mStep[0m  [378/427], [94mLoss[0m : 2.69519
[1mStep[0m  [420/427], [94mLoss[0m : 2.26684

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.04111
[1mStep[0m  [42/427], [94mLoss[0m : 2.21207
[1mStep[0m  [84/427], [94mLoss[0m : 2.33148
[1mStep[0m  [126/427], [94mLoss[0m : 2.80274
[1mStep[0m  [168/427], [94mLoss[0m : 2.09361
[1mStep[0m  [210/427], [94mLoss[0m : 3.03757
[1mStep[0m  [252/427], [94mLoss[0m : 2.49781
[1mStep[0m  [294/427], [94mLoss[0m : 1.92637
[1mStep[0m  [336/427], [94mLoss[0m : 2.39224
[1mStep[0m  [378/427], [94mLoss[0m : 2.12847
[1mStep[0m  [420/427], [94mLoss[0m : 2.29007

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.46237
[1mStep[0m  [42/427], [94mLoss[0m : 1.75509
[1mStep[0m  [84/427], [94mLoss[0m : 2.09796
[1mStep[0m  [126/427], [94mLoss[0m : 2.57486
[1mStep[0m  [168/427], [94mLoss[0m : 1.87294
[1mStep[0m  [210/427], [94mLoss[0m : 2.36357
[1mStep[0m  [252/427], [94mLoss[0m : 2.81155
[1mStep[0m  [294/427], [94mLoss[0m : 2.36218
[1mStep[0m  [336/427], [94mLoss[0m : 2.28245
[1mStep[0m  [378/427], [94mLoss[0m : 2.53218
[1mStep[0m  [420/427], [94mLoss[0m : 1.95124

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.82589
[1mStep[0m  [42/427], [94mLoss[0m : 2.59276
[1mStep[0m  [84/427], [94mLoss[0m : 1.83344
[1mStep[0m  [126/427], [94mLoss[0m : 2.25089
[1mStep[0m  [168/427], [94mLoss[0m : 2.38280
[1mStep[0m  [210/427], [94mLoss[0m : 2.17847
[1mStep[0m  [252/427], [94mLoss[0m : 2.66719
[1mStep[0m  [294/427], [94mLoss[0m : 2.32827
[1mStep[0m  [336/427], [94mLoss[0m : 3.70056
[1mStep[0m  [378/427], [94mLoss[0m : 2.32392
[1mStep[0m  [420/427], [94mLoss[0m : 2.44160

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.25714
[1mStep[0m  [42/427], [94mLoss[0m : 1.59564
[1mStep[0m  [84/427], [94mLoss[0m : 2.21800
[1mStep[0m  [126/427], [94mLoss[0m : 2.17494
[1mStep[0m  [168/427], [94mLoss[0m : 1.80972
[1mStep[0m  [210/427], [94mLoss[0m : 2.09641
[1mStep[0m  [252/427], [94mLoss[0m : 2.46282
[1mStep[0m  [294/427], [94mLoss[0m : 2.79420
[1mStep[0m  [336/427], [94mLoss[0m : 3.23936
[1mStep[0m  [378/427], [94mLoss[0m : 2.58274
[1mStep[0m  [420/427], [94mLoss[0m : 2.16188

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.38951
[1mStep[0m  [42/427], [94mLoss[0m : 2.30081
[1mStep[0m  [84/427], [94mLoss[0m : 2.44217
[1mStep[0m  [126/427], [94mLoss[0m : 1.70787
[1mStep[0m  [168/427], [94mLoss[0m : 2.68288
[1mStep[0m  [210/427], [94mLoss[0m : 2.53537
[1mStep[0m  [252/427], [94mLoss[0m : 2.17119
[1mStep[0m  [294/427], [94mLoss[0m : 1.96286
[1mStep[0m  [336/427], [94mLoss[0m : 2.19133
[1mStep[0m  [378/427], [94mLoss[0m : 1.89645
[1mStep[0m  [420/427], [94mLoss[0m : 2.53764

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.09787
[1mStep[0m  [42/427], [94mLoss[0m : 1.97798
[1mStep[0m  [84/427], [94mLoss[0m : 2.40979
[1mStep[0m  [126/427], [94mLoss[0m : 2.55956
[1mStep[0m  [168/427], [94mLoss[0m : 2.11697
[1mStep[0m  [210/427], [94mLoss[0m : 2.06867
[1mStep[0m  [252/427], [94mLoss[0m : 2.32560
[1mStep[0m  [294/427], [94mLoss[0m : 2.28095
[1mStep[0m  [336/427], [94mLoss[0m : 2.76877
[1mStep[0m  [378/427], [94mLoss[0m : 2.74215
[1mStep[0m  [420/427], [94mLoss[0m : 2.06438

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.94647
[1mStep[0m  [42/427], [94mLoss[0m : 2.51515
[1mStep[0m  [84/427], [94mLoss[0m : 2.28367
[1mStep[0m  [126/427], [94mLoss[0m : 2.02596
[1mStep[0m  [168/427], [94mLoss[0m : 1.78100
[1mStep[0m  [210/427], [94mLoss[0m : 2.08823
[1mStep[0m  [252/427], [94mLoss[0m : 2.84029
[1mStep[0m  [294/427], [94mLoss[0m : 1.80694
[1mStep[0m  [336/427], [94mLoss[0m : 2.46706
[1mStep[0m  [378/427], [94mLoss[0m : 1.72484
[1mStep[0m  [420/427], [94mLoss[0m : 1.81176

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.06038
[1mStep[0m  [42/427], [94mLoss[0m : 2.44690
[1mStep[0m  [84/427], [94mLoss[0m : 2.72031
[1mStep[0m  [126/427], [94mLoss[0m : 2.02297
[1mStep[0m  [168/427], [94mLoss[0m : 2.16890
[1mStep[0m  [210/427], [94mLoss[0m : 2.13324
[1mStep[0m  [252/427], [94mLoss[0m : 2.41055
[1mStep[0m  [294/427], [94mLoss[0m : 2.42602
[1mStep[0m  [336/427], [94mLoss[0m : 1.98677
[1mStep[0m  [378/427], [94mLoss[0m : 2.56436
[1mStep[0m  [420/427], [94mLoss[0m : 1.97800

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.81167
[1mStep[0m  [42/427], [94mLoss[0m : 2.37841
[1mStep[0m  [84/427], [94mLoss[0m : 2.27130
[1mStep[0m  [126/427], [94mLoss[0m : 2.62041
[1mStep[0m  [168/427], [94mLoss[0m : 1.68159
[1mStep[0m  [210/427], [94mLoss[0m : 2.25965
[1mStep[0m  [252/427], [94mLoss[0m : 2.30222
[1mStep[0m  [294/427], [94mLoss[0m : 2.40898
[1mStep[0m  [336/427], [94mLoss[0m : 1.68879
[1mStep[0m  [378/427], [94mLoss[0m : 2.25690
[1mStep[0m  [420/427], [94mLoss[0m : 2.22543

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.86934
[1mStep[0m  [42/427], [94mLoss[0m : 1.39004
[1mStep[0m  [84/427], [94mLoss[0m : 1.60932
[1mStep[0m  [126/427], [94mLoss[0m : 2.43561
[1mStep[0m  [168/427], [94mLoss[0m : 1.84544
[1mStep[0m  [210/427], [94mLoss[0m : 2.19010
[1mStep[0m  [252/427], [94mLoss[0m : 2.56243
[1mStep[0m  [294/427], [94mLoss[0m : 2.45580
[1mStep[0m  [336/427], [94mLoss[0m : 1.40095
[1mStep[0m  [378/427], [94mLoss[0m : 1.86865
[1mStep[0m  [420/427], [94mLoss[0m : 1.79536

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.50940
[1mStep[0m  [42/427], [94mLoss[0m : 1.91689
[1mStep[0m  [84/427], [94mLoss[0m : 1.90671
[1mStep[0m  [126/427], [94mLoss[0m : 1.57739
[1mStep[0m  [168/427], [94mLoss[0m : 1.88421
[1mStep[0m  [210/427], [94mLoss[0m : 2.08027
[1mStep[0m  [252/427], [94mLoss[0m : 1.85895
[1mStep[0m  [294/427], [94mLoss[0m : 2.32909
[1mStep[0m  [336/427], [94mLoss[0m : 2.21398
[1mStep[0m  [378/427], [94mLoss[0m : 2.30428
[1mStep[0m  [420/427], [94mLoss[0m : 1.86023

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.97745
[1mStep[0m  [42/427], [94mLoss[0m : 1.87044
[1mStep[0m  [84/427], [94mLoss[0m : 2.19915
[1mStep[0m  [126/427], [94mLoss[0m : 1.72157
[1mStep[0m  [168/427], [94mLoss[0m : 1.67987
[1mStep[0m  [210/427], [94mLoss[0m : 2.24451
[1mStep[0m  [252/427], [94mLoss[0m : 2.56960
[1mStep[0m  [294/427], [94mLoss[0m : 2.09260
[1mStep[0m  [336/427], [94mLoss[0m : 2.44691
[1mStep[0m  [378/427], [94mLoss[0m : 2.21825
[1mStep[0m  [420/427], [94mLoss[0m : 2.22268

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.77242
[1mStep[0m  [42/427], [94mLoss[0m : 1.94657
[1mStep[0m  [84/427], [94mLoss[0m : 1.71547
[1mStep[0m  [126/427], [94mLoss[0m : 2.20884
[1mStep[0m  [168/427], [94mLoss[0m : 1.73998
[1mStep[0m  [210/427], [94mLoss[0m : 1.95836
[1mStep[0m  [252/427], [94mLoss[0m : 2.20453
[1mStep[0m  [294/427], [94mLoss[0m : 2.24563
[1mStep[0m  [336/427], [94mLoss[0m : 1.97235
[1mStep[0m  [378/427], [94mLoss[0m : 2.07698
[1mStep[0m  [420/427], [94mLoss[0m : 2.71318

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.09198
[1mStep[0m  [42/427], [94mLoss[0m : 1.74046
[1mStep[0m  [84/427], [94mLoss[0m : 2.36845
[1mStep[0m  [126/427], [94mLoss[0m : 2.24363
[1mStep[0m  [168/427], [94mLoss[0m : 2.51810
[1mStep[0m  [210/427], [94mLoss[0m : 2.20332
[1mStep[0m  [252/427], [94mLoss[0m : 2.61446
[1mStep[0m  [294/427], [94mLoss[0m : 2.74154
[1mStep[0m  [336/427], [94mLoss[0m : 3.08246
[1mStep[0m  [378/427], [94mLoss[0m : 2.14181
[1mStep[0m  [420/427], [94mLoss[0m : 2.64226

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.129, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.06347
[1mStep[0m  [42/427], [94mLoss[0m : 2.14174
[1mStep[0m  [84/427], [94mLoss[0m : 1.68027
[1mStep[0m  [126/427], [94mLoss[0m : 2.54219
[1mStep[0m  [168/427], [94mLoss[0m : 2.45662
[1mStep[0m  [210/427], [94mLoss[0m : 2.66249
[1mStep[0m  [252/427], [94mLoss[0m : 2.26859
[1mStep[0m  [294/427], [94mLoss[0m : 2.47766
[1mStep[0m  [336/427], [94mLoss[0m : 1.97958
[1mStep[0m  [378/427], [94mLoss[0m : 2.86106
[1mStep[0m  [420/427], [94mLoss[0m : 2.19806

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.86775
[1mStep[0m  [42/427], [94mLoss[0m : 2.06565
[1mStep[0m  [84/427], [94mLoss[0m : 2.26321
[1mStep[0m  [126/427], [94mLoss[0m : 2.13969
[1mStep[0m  [168/427], [94mLoss[0m : 2.80867
[1mStep[0m  [210/427], [94mLoss[0m : 2.53684
[1mStep[0m  [252/427], [94mLoss[0m : 1.91138
[1mStep[0m  [294/427], [94mLoss[0m : 2.14263
[1mStep[0m  [336/427], [94mLoss[0m : 1.72136
[1mStep[0m  [378/427], [94mLoss[0m : 2.41431
[1mStep[0m  [420/427], [94mLoss[0m : 1.97552

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.89040
[1mStep[0m  [42/427], [94mLoss[0m : 1.98335
[1mStep[0m  [84/427], [94mLoss[0m : 2.59785
[1mStep[0m  [126/427], [94mLoss[0m : 1.63356
[1mStep[0m  [168/427], [94mLoss[0m : 2.13791
[1mStep[0m  [210/427], [94mLoss[0m : 2.34452
[1mStep[0m  [252/427], [94mLoss[0m : 2.49949
[1mStep[0m  [294/427], [94mLoss[0m : 2.25506
[1mStep[0m  [336/427], [94mLoss[0m : 1.93914
[1mStep[0m  [378/427], [94mLoss[0m : 1.62175
[1mStep[0m  [420/427], [94mLoss[0m : 1.34814

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.11711
[1mStep[0m  [42/427], [94mLoss[0m : 2.01619
[1mStep[0m  [84/427], [94mLoss[0m : 2.79140
[1mStep[0m  [126/427], [94mLoss[0m : 1.79507
[1mStep[0m  [168/427], [94mLoss[0m : 1.52697
[1mStep[0m  [210/427], [94mLoss[0m : 2.12847
[1mStep[0m  [252/427], [94mLoss[0m : 1.92546
[1mStep[0m  [294/427], [94mLoss[0m : 2.14205
[1mStep[0m  [336/427], [94mLoss[0m : 2.08408
[1mStep[0m  [378/427], [94mLoss[0m : 2.56808
[1mStep[0m  [420/427], [94mLoss[0m : 2.99308

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.80408
[1mStep[0m  [42/427], [94mLoss[0m : 2.19731
[1mStep[0m  [84/427], [94mLoss[0m : 2.26757
[1mStep[0m  [126/427], [94mLoss[0m : 2.05552
[1mStep[0m  [168/427], [94mLoss[0m : 2.04313
[1mStep[0m  [210/427], [94mLoss[0m : 2.08942
[1mStep[0m  [252/427], [94mLoss[0m : 1.90855
[1mStep[0m  [294/427], [94mLoss[0m : 2.99307
[1mStep[0m  [336/427], [94mLoss[0m : 1.53932
[1mStep[0m  [378/427], [94mLoss[0m : 2.54273
[1mStep[0m  [420/427], [94mLoss[0m : 2.07368

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.074, [92mTest[0m: 2.439, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.08368
[1mStep[0m  [42/427], [94mLoss[0m : 2.01121
[1mStep[0m  [84/427], [94mLoss[0m : 2.06567
[1mStep[0m  [126/427], [94mLoss[0m : 2.33047
[1mStep[0m  [168/427], [94mLoss[0m : 2.09216
[1mStep[0m  [210/427], [94mLoss[0m : 2.36117
[1mStep[0m  [252/427], [94mLoss[0m : 2.55745
[1mStep[0m  [294/427], [94mLoss[0m : 2.40189
[1mStep[0m  [336/427], [94mLoss[0m : 2.60561
[1mStep[0m  [378/427], [94mLoss[0m : 2.30248
[1mStep[0m  [420/427], [94mLoss[0m : 1.51694

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.435, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.83974
[1mStep[0m  [42/427], [94mLoss[0m : 1.53749
[1mStep[0m  [84/427], [94mLoss[0m : 2.01949
[1mStep[0m  [126/427], [94mLoss[0m : 2.04408
[1mStep[0m  [168/427], [94mLoss[0m : 2.09024
[1mStep[0m  [210/427], [94mLoss[0m : 1.67847
[1mStep[0m  [252/427], [94mLoss[0m : 1.71206
[1mStep[0m  [294/427], [94mLoss[0m : 2.27257
[1mStep[0m  [336/427], [94mLoss[0m : 2.06691
[1mStep[0m  [378/427], [94mLoss[0m : 1.81207
[1mStep[0m  [420/427], [94mLoss[0m : 2.63084

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.63148
[1mStep[0m  [42/427], [94mLoss[0m : 1.77087
[1mStep[0m  [84/427], [94mLoss[0m : 1.98130
[1mStep[0m  [126/427], [94mLoss[0m : 2.20974
[1mStep[0m  [168/427], [94mLoss[0m : 2.10440
[1mStep[0m  [210/427], [94mLoss[0m : 2.03729
[1mStep[0m  [252/427], [94mLoss[0m : 2.31685
[1mStep[0m  [294/427], [94mLoss[0m : 2.16244
[1mStep[0m  [336/427], [94mLoss[0m : 1.76061
[1mStep[0m  [378/427], [94mLoss[0m : 2.07459
[1mStep[0m  [420/427], [94mLoss[0m : 1.75169

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.404, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.48719
[1mStep[0m  [42/427], [94mLoss[0m : 1.47042
[1mStep[0m  [84/427], [94mLoss[0m : 1.59266
[1mStep[0m  [126/427], [94mLoss[0m : 1.95967
[1mStep[0m  [168/427], [94mLoss[0m : 2.48540
[1mStep[0m  [210/427], [94mLoss[0m : 1.73267
[1mStep[0m  [252/427], [94mLoss[0m : 3.02351
[1mStep[0m  [294/427], [94mLoss[0m : 1.70620
[1mStep[0m  [336/427], [94mLoss[0m : 1.66116
[1mStep[0m  [378/427], [94mLoss[0m : 2.40678
[1mStep[0m  [420/427], [94mLoss[0m : 2.45308

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.437, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.41761
[1mStep[0m  [42/427], [94mLoss[0m : 1.68676
[1mStep[0m  [84/427], [94mLoss[0m : 1.78473
[1mStep[0m  [126/427], [94mLoss[0m : 2.55361
[1mStep[0m  [168/427], [94mLoss[0m : 2.45717
[1mStep[0m  [210/427], [94mLoss[0m : 1.92590
[1mStep[0m  [252/427], [94mLoss[0m : 1.82257
[1mStep[0m  [294/427], [94mLoss[0m : 1.89108
[1mStep[0m  [336/427], [94mLoss[0m : 1.88697
[1mStep[0m  [378/427], [94mLoss[0m : 2.24972
[1mStep[0m  [420/427], [94mLoss[0m : 2.55946

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.55072
[1mStep[0m  [42/427], [94mLoss[0m : 1.66749
[1mStep[0m  [84/427], [94mLoss[0m : 2.12289
[1mStep[0m  [126/427], [94mLoss[0m : 2.08733
[1mStep[0m  [168/427], [94mLoss[0m : 2.13758
[1mStep[0m  [210/427], [94mLoss[0m : 2.03635
[1mStep[0m  [252/427], [94mLoss[0m : 1.81371
[1mStep[0m  [294/427], [94mLoss[0m : 1.83173
[1mStep[0m  [336/427], [94mLoss[0m : 1.77711
[1mStep[0m  [378/427], [94mLoss[0m : 2.03795
[1mStep[0m  [420/427], [94mLoss[0m : 1.94414

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.07343
[1mStep[0m  [42/427], [94mLoss[0m : 1.75070
[1mStep[0m  [84/427], [94mLoss[0m : 2.16451
[1mStep[0m  [126/427], [94mLoss[0m : 2.32470
[1mStep[0m  [168/427], [94mLoss[0m : 1.98318
[1mStep[0m  [210/427], [94mLoss[0m : 1.88085
[1mStep[0m  [252/427], [94mLoss[0m : 2.80690
[1mStep[0m  [294/427], [94mLoss[0m : 1.87450
[1mStep[0m  [336/427], [94mLoss[0m : 1.98472
[1mStep[0m  [378/427], [94mLoss[0m : 1.63250
[1mStep[0m  [420/427], [94mLoss[0m : 2.68943

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.472, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.03345
[1mStep[0m  [42/427], [94mLoss[0m : 1.65273
[1mStep[0m  [84/427], [94mLoss[0m : 1.88350
[1mStep[0m  [126/427], [94mLoss[0m : 1.86333
[1mStep[0m  [168/427], [94mLoss[0m : 2.42379
[1mStep[0m  [210/427], [94mLoss[0m : 1.93134
[1mStep[0m  [252/427], [94mLoss[0m : 2.01872
[1mStep[0m  [294/427], [94mLoss[0m : 1.80837
[1mStep[0m  [336/427], [94mLoss[0m : 2.70866
[1mStep[0m  [378/427], [94mLoss[0m : 2.67057
[1mStep[0m  [420/427], [94mLoss[0m : 2.35761

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.439, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.74989
[1mStep[0m  [42/427], [94mLoss[0m : 2.07512
[1mStep[0m  [84/427], [94mLoss[0m : 1.87249
[1mStep[0m  [126/427], [94mLoss[0m : 1.85068
[1mStep[0m  [168/427], [94mLoss[0m : 1.99950
[1mStep[0m  [210/427], [94mLoss[0m : 1.99282
[1mStep[0m  [252/427], [94mLoss[0m : 2.21171
[1mStep[0m  [294/427], [94mLoss[0m : 2.04682
[1mStep[0m  [336/427], [94mLoss[0m : 1.51380
[1mStep[0m  [378/427], [94mLoss[0m : 2.05490
[1mStep[0m  [420/427], [94mLoss[0m : 1.77125

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.439, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.18408
[1mStep[0m  [42/427], [94mLoss[0m : 1.74981
[1mStep[0m  [84/427], [94mLoss[0m : 2.15663
[1mStep[0m  [126/427], [94mLoss[0m : 1.67818
[1mStep[0m  [168/427], [94mLoss[0m : 1.95296
[1mStep[0m  [210/427], [94mLoss[0m : 1.94863
[1mStep[0m  [252/427], [94mLoss[0m : 2.02014
[1mStep[0m  [294/427], [94mLoss[0m : 2.17472
[1mStep[0m  [336/427], [94mLoss[0m : 2.03822
[1mStep[0m  [378/427], [94mLoss[0m : 2.16157
[1mStep[0m  [420/427], [94mLoss[0m : 2.74495

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.975, [92mTest[0m: 2.481, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.466
====================================

Phase 2 - Evaluation MAE:  2.466338382640355
MAE score P1        2.374314
MAE score P2        2.466338
loss                1.974992
learning_rate       0.007525
batch_size                32
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.5
weight_decay            0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.97499
[1mStep[0m  [2/26], [94mLoss[0m : 10.75600
[1mStep[0m  [4/26], [94mLoss[0m : 10.80477
[1mStep[0m  [6/26], [94mLoss[0m : 10.75068
[1mStep[0m  [8/26], [94mLoss[0m : 10.49349
[1mStep[0m  [10/26], [94mLoss[0m : 10.75387
[1mStep[0m  [12/26], [94mLoss[0m : 10.77757
[1mStep[0m  [14/26], [94mLoss[0m : 10.83886
[1mStep[0m  [16/26], [94mLoss[0m : 10.45734
[1mStep[0m  [18/26], [94mLoss[0m : 10.62968
[1mStep[0m  [20/26], [94mLoss[0m : 10.45376
[1mStep[0m  [22/26], [94mLoss[0m : 10.73878
[1mStep[0m  [24/26], [94mLoss[0m : 10.68899

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.729, [92mTest[0m: 10.845, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.48275
[1mStep[0m  [2/26], [94mLoss[0m : 10.63445
[1mStep[0m  [4/26], [94mLoss[0m : 10.57472
[1mStep[0m  [6/26], [94mLoss[0m : 10.54470
[1mStep[0m  [8/26], [94mLoss[0m : 10.37340
[1mStep[0m  [10/26], [94mLoss[0m : 10.20600
[1mStep[0m  [12/26], [94mLoss[0m : 10.30337
[1mStep[0m  [14/26], [94mLoss[0m : 10.14938
[1mStep[0m  [16/26], [94mLoss[0m : 10.21439
[1mStep[0m  [18/26], [94mLoss[0m : 10.37984
[1mStep[0m  [20/26], [94mLoss[0m : 10.14469
[1mStep[0m  [22/26], [94mLoss[0m : 10.23180
[1mStep[0m  [24/26], [94mLoss[0m : 10.20105

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.322, [92mTest[0m: 10.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.95551
[1mStep[0m  [2/26], [94mLoss[0m : 10.15502
[1mStep[0m  [4/26], [94mLoss[0m : 9.66851
[1mStep[0m  [6/26], [94mLoss[0m : 10.22457
[1mStep[0m  [8/26], [94mLoss[0m : 9.92334
[1mStep[0m  [10/26], [94mLoss[0m : 10.16823
[1mStep[0m  [12/26], [94mLoss[0m : 9.86019
[1mStep[0m  [14/26], [94mLoss[0m : 9.71906
[1mStep[0m  [16/26], [94mLoss[0m : 9.67208
[1mStep[0m  [18/26], [94mLoss[0m : 9.56305
[1mStep[0m  [20/26], [94mLoss[0m : 9.84097
[1mStep[0m  [22/26], [94mLoss[0m : 9.81033
[1mStep[0m  [24/26], [94mLoss[0m : 9.89581

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.891, [92mTest[0m: 9.962, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.70910
[1mStep[0m  [2/26], [94mLoss[0m : 9.50034
[1mStep[0m  [4/26], [94mLoss[0m : 9.49972
[1mStep[0m  [6/26], [94mLoss[0m : 9.59006
[1mStep[0m  [8/26], [94mLoss[0m : 9.44966
[1mStep[0m  [10/26], [94mLoss[0m : 9.33920
[1mStep[0m  [12/26], [94mLoss[0m : 9.55773
[1mStep[0m  [14/26], [94mLoss[0m : 9.41834
[1mStep[0m  [16/26], [94mLoss[0m : 9.08111
[1mStep[0m  [18/26], [94mLoss[0m : 9.52011
[1mStep[0m  [20/26], [94mLoss[0m : 9.04967
[1mStep[0m  [22/26], [94mLoss[0m : 9.26912
[1mStep[0m  [24/26], [94mLoss[0m : 9.24528

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.392, [92mTest[0m: 9.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.10282
[1mStep[0m  [2/26], [94mLoss[0m : 9.11379
[1mStep[0m  [4/26], [94mLoss[0m : 9.01693
[1mStep[0m  [6/26], [94mLoss[0m : 9.11687
[1mStep[0m  [8/26], [94mLoss[0m : 8.97984
[1mStep[0m  [10/26], [94mLoss[0m : 8.87470
[1mStep[0m  [12/26], [94mLoss[0m : 8.81709
[1mStep[0m  [14/26], [94mLoss[0m : 8.51233
[1mStep[0m  [16/26], [94mLoss[0m : 8.78787
[1mStep[0m  [18/26], [94mLoss[0m : 8.89768
[1mStep[0m  [20/26], [94mLoss[0m : 8.65975
[1mStep[0m  [22/26], [94mLoss[0m : 8.58369
[1mStep[0m  [24/26], [94mLoss[0m : 8.13915

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.795, [92mTest[0m: 8.769, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.39572
[1mStep[0m  [2/26], [94mLoss[0m : 8.08918
[1mStep[0m  [4/26], [94mLoss[0m : 8.26108
[1mStep[0m  [6/26], [94mLoss[0m : 8.28962
[1mStep[0m  [8/26], [94mLoss[0m : 8.34022
[1mStep[0m  [10/26], [94mLoss[0m : 8.22234
[1mStep[0m  [12/26], [94mLoss[0m : 8.13973
[1mStep[0m  [14/26], [94mLoss[0m : 8.39926
[1mStep[0m  [16/26], [94mLoss[0m : 7.89708
[1mStep[0m  [18/26], [94mLoss[0m : 7.82816
[1mStep[0m  [20/26], [94mLoss[0m : 7.71116
[1mStep[0m  [22/26], [94mLoss[0m : 7.82942
[1mStep[0m  [24/26], [94mLoss[0m : 7.85313

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.066, [92mTest[0m: 8.019, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.49849
[1mStep[0m  [2/26], [94mLoss[0m : 7.54293
[1mStep[0m  [4/26], [94mLoss[0m : 7.59560
[1mStep[0m  [6/26], [94mLoss[0m : 7.68583
[1mStep[0m  [8/26], [94mLoss[0m : 7.48354
[1mStep[0m  [10/26], [94mLoss[0m : 7.49578
[1mStep[0m  [12/26], [94mLoss[0m : 7.15244
[1mStep[0m  [14/26], [94mLoss[0m : 7.24635
[1mStep[0m  [16/26], [94mLoss[0m : 7.18710
[1mStep[0m  [18/26], [94mLoss[0m : 7.10993
[1mStep[0m  [20/26], [94mLoss[0m : 7.15121
[1mStep[0m  [22/26], [94mLoss[0m : 7.22895
[1mStep[0m  [24/26], [94mLoss[0m : 7.12445

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.336, [92mTest[0m: 7.207, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.04043
[1mStep[0m  [2/26], [94mLoss[0m : 6.78755
[1mStep[0m  [4/26], [94mLoss[0m : 7.22634
[1mStep[0m  [6/26], [94mLoss[0m : 7.00166
[1mStep[0m  [8/26], [94mLoss[0m : 7.07832
[1mStep[0m  [10/26], [94mLoss[0m : 6.81106
[1mStep[0m  [12/26], [94mLoss[0m : 6.67866
[1mStep[0m  [14/26], [94mLoss[0m : 6.28138
[1mStep[0m  [16/26], [94mLoss[0m : 6.59198
[1mStep[0m  [18/26], [94mLoss[0m : 6.69472
[1mStep[0m  [20/26], [94mLoss[0m : 6.35885
[1mStep[0m  [22/26], [94mLoss[0m : 6.13276
[1mStep[0m  [24/26], [94mLoss[0m : 6.46109

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.697, [92mTest[0m: 6.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.35983
[1mStep[0m  [2/26], [94mLoss[0m : 6.63877
[1mStep[0m  [4/26], [94mLoss[0m : 6.09222
[1mStep[0m  [6/26], [94mLoss[0m : 6.42091
[1mStep[0m  [8/26], [94mLoss[0m : 6.36175
[1mStep[0m  [10/26], [94mLoss[0m : 6.41634
[1mStep[0m  [12/26], [94mLoss[0m : 6.21634
[1mStep[0m  [14/26], [94mLoss[0m : 6.10455
[1mStep[0m  [16/26], [94mLoss[0m : 6.30422
[1mStep[0m  [18/26], [94mLoss[0m : 5.87547
[1mStep[0m  [20/26], [94mLoss[0m : 5.73702
[1mStep[0m  [22/26], [94mLoss[0m : 5.81774
[1mStep[0m  [24/26], [94mLoss[0m : 5.76172

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.164, [92mTest[0m: 5.744, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.89694
[1mStep[0m  [2/26], [94mLoss[0m : 5.83736
[1mStep[0m  [4/26], [94mLoss[0m : 5.61598
[1mStep[0m  [6/26], [94mLoss[0m : 5.55897
[1mStep[0m  [8/26], [94mLoss[0m : 5.76205
[1mStep[0m  [10/26], [94mLoss[0m : 5.34916
[1mStep[0m  [12/26], [94mLoss[0m : 5.79706
[1mStep[0m  [14/26], [94mLoss[0m : 5.82715
[1mStep[0m  [16/26], [94mLoss[0m : 5.40181
[1mStep[0m  [18/26], [94mLoss[0m : 5.86300
[1mStep[0m  [20/26], [94mLoss[0m : 5.65946
[1mStep[0m  [22/26], [94mLoss[0m : 5.76501
[1mStep[0m  [24/26], [94mLoss[0m : 5.46585

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.718, [92mTest[0m: 5.128, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.59854
[1mStep[0m  [2/26], [94mLoss[0m : 5.74033
[1mStep[0m  [4/26], [94mLoss[0m : 5.34514
[1mStep[0m  [6/26], [94mLoss[0m : 5.00927
[1mStep[0m  [8/26], [94mLoss[0m : 5.47023
[1mStep[0m  [10/26], [94mLoss[0m : 5.27881
[1mStep[0m  [12/26], [94mLoss[0m : 5.30212
[1mStep[0m  [14/26], [94mLoss[0m : 5.58595
[1mStep[0m  [16/26], [94mLoss[0m : 5.39700
[1mStep[0m  [18/26], [94mLoss[0m : 4.94819
[1mStep[0m  [20/26], [94mLoss[0m : 5.03946
[1mStep[0m  [22/26], [94mLoss[0m : 4.80363
[1mStep[0m  [24/26], [94mLoss[0m : 4.99385

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.250, [92mTest[0m: 4.673, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.73182
[1mStep[0m  [2/26], [94mLoss[0m : 5.15708
[1mStep[0m  [4/26], [94mLoss[0m : 4.84183
[1mStep[0m  [6/26], [94mLoss[0m : 5.29974
[1mStep[0m  [8/26], [94mLoss[0m : 4.74831
[1mStep[0m  [10/26], [94mLoss[0m : 5.03369
[1mStep[0m  [12/26], [94mLoss[0m : 4.82359
[1mStep[0m  [14/26], [94mLoss[0m : 4.67466
[1mStep[0m  [16/26], [94mLoss[0m : 4.64747
[1mStep[0m  [18/26], [94mLoss[0m : 4.73701
[1mStep[0m  [20/26], [94mLoss[0m : 4.54934
[1mStep[0m  [22/26], [94mLoss[0m : 4.87735
[1mStep[0m  [24/26], [94mLoss[0m : 4.39413

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.788, [92mTest[0m: 4.201, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.42372
[1mStep[0m  [2/26], [94mLoss[0m : 4.27009
[1mStep[0m  [4/26], [94mLoss[0m : 4.36170
[1mStep[0m  [6/26], [94mLoss[0m : 4.79142
[1mStep[0m  [8/26], [94mLoss[0m : 4.34921
[1mStep[0m  [10/26], [94mLoss[0m : 4.32453
[1mStep[0m  [12/26], [94mLoss[0m : 4.29252
[1mStep[0m  [14/26], [94mLoss[0m : 4.27814
[1mStep[0m  [16/26], [94mLoss[0m : 4.26422
[1mStep[0m  [18/26], [94mLoss[0m : 4.00206
[1mStep[0m  [20/26], [94mLoss[0m : 4.34324
[1mStep[0m  [22/26], [94mLoss[0m : 4.05010
[1mStep[0m  [24/26], [94mLoss[0m : 4.32924

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.326, [92mTest[0m: 3.764, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.23987
[1mStep[0m  [2/26], [94mLoss[0m : 4.33911
[1mStep[0m  [4/26], [94mLoss[0m : 4.20501
[1mStep[0m  [6/26], [94mLoss[0m : 3.78690
[1mStep[0m  [8/26], [94mLoss[0m : 3.93103
[1mStep[0m  [10/26], [94mLoss[0m : 4.10180
[1mStep[0m  [12/26], [94mLoss[0m : 3.72236
[1mStep[0m  [14/26], [94mLoss[0m : 3.83932
[1mStep[0m  [16/26], [94mLoss[0m : 3.65018
[1mStep[0m  [18/26], [94mLoss[0m : 3.98282
[1mStep[0m  [20/26], [94mLoss[0m : 3.74397
[1mStep[0m  [22/26], [94mLoss[0m : 3.68662
[1mStep[0m  [24/26], [94mLoss[0m : 3.64024

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.885, [92mTest[0m: 3.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.80923
[1mStep[0m  [2/26], [94mLoss[0m : 3.70736
[1mStep[0m  [4/26], [94mLoss[0m : 3.47820
[1mStep[0m  [6/26], [94mLoss[0m : 3.19788
[1mStep[0m  [8/26], [94mLoss[0m : 3.65692
[1mStep[0m  [10/26], [94mLoss[0m : 3.54726
[1mStep[0m  [12/26], [94mLoss[0m : 3.48962
[1mStep[0m  [14/26], [94mLoss[0m : 3.25794
[1mStep[0m  [16/26], [94mLoss[0m : 3.26821
[1mStep[0m  [18/26], [94mLoss[0m : 3.42072
[1mStep[0m  [20/26], [94mLoss[0m : 3.81717
[1mStep[0m  [22/26], [94mLoss[0m : 3.35773
[1mStep[0m  [24/26], [94mLoss[0m : 3.41181

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.494, [92mTest[0m: 3.064, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.35383
[1mStep[0m  [2/26], [94mLoss[0m : 3.39237
[1mStep[0m  [4/26], [94mLoss[0m : 3.57761
[1mStep[0m  [6/26], [94mLoss[0m : 3.21334
[1mStep[0m  [8/26], [94mLoss[0m : 3.30969
[1mStep[0m  [10/26], [94mLoss[0m : 3.27402
[1mStep[0m  [12/26], [94mLoss[0m : 3.15381
[1mStep[0m  [14/26], [94mLoss[0m : 3.06970
[1mStep[0m  [16/26], [94mLoss[0m : 3.14896
[1mStep[0m  [18/26], [94mLoss[0m : 2.84752
[1mStep[0m  [20/26], [94mLoss[0m : 3.03208
[1mStep[0m  [22/26], [94mLoss[0m : 3.11046
[1mStep[0m  [24/26], [94mLoss[0m : 3.00275

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.179, [92mTest[0m: 2.813, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.16367
[1mStep[0m  [2/26], [94mLoss[0m : 2.99722
[1mStep[0m  [4/26], [94mLoss[0m : 2.78286
[1mStep[0m  [6/26], [94mLoss[0m : 2.92702
[1mStep[0m  [8/26], [94mLoss[0m : 3.08453
[1mStep[0m  [10/26], [94mLoss[0m : 2.97338
[1mStep[0m  [12/26], [94mLoss[0m : 2.83760
[1mStep[0m  [14/26], [94mLoss[0m : 2.74611
[1mStep[0m  [16/26], [94mLoss[0m : 3.10232
[1mStep[0m  [18/26], [94mLoss[0m : 2.88219
[1mStep[0m  [20/26], [94mLoss[0m : 3.01881
[1mStep[0m  [22/26], [94mLoss[0m : 3.05717
[1mStep[0m  [24/26], [94mLoss[0m : 2.72020

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.962, [92mTest[0m: 2.619, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.89406
[1mStep[0m  [2/26], [94mLoss[0m : 2.95602
[1mStep[0m  [4/26], [94mLoss[0m : 2.91254
[1mStep[0m  [6/26], [94mLoss[0m : 2.64701
[1mStep[0m  [8/26], [94mLoss[0m : 2.91890
[1mStep[0m  [10/26], [94mLoss[0m : 2.66899
[1mStep[0m  [12/26], [94mLoss[0m : 2.75838
[1mStep[0m  [14/26], [94mLoss[0m : 2.68632
[1mStep[0m  [16/26], [94mLoss[0m : 2.94845
[1mStep[0m  [18/26], [94mLoss[0m : 2.82732
[1mStep[0m  [20/26], [94mLoss[0m : 2.99606
[1mStep[0m  [22/26], [94mLoss[0m : 2.78417
[1mStep[0m  [24/26], [94mLoss[0m : 2.86904

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.862, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60184
[1mStep[0m  [2/26], [94mLoss[0m : 2.82796
[1mStep[0m  [4/26], [94mLoss[0m : 2.92534
[1mStep[0m  [6/26], [94mLoss[0m : 2.90118
[1mStep[0m  [8/26], [94mLoss[0m : 2.79829
[1mStep[0m  [10/26], [94mLoss[0m : 2.84571
[1mStep[0m  [12/26], [94mLoss[0m : 2.69628
[1mStep[0m  [14/26], [94mLoss[0m : 2.70109
[1mStep[0m  [16/26], [94mLoss[0m : 2.76270
[1mStep[0m  [18/26], [94mLoss[0m : 2.79946
[1mStep[0m  [20/26], [94mLoss[0m : 2.86932
[1mStep[0m  [22/26], [94mLoss[0m : 2.78510
[1mStep[0m  [24/26], [94mLoss[0m : 2.69147

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.799, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67776
[1mStep[0m  [2/26], [94mLoss[0m : 2.74157
[1mStep[0m  [4/26], [94mLoss[0m : 2.79870
[1mStep[0m  [6/26], [94mLoss[0m : 2.83709
[1mStep[0m  [8/26], [94mLoss[0m : 2.89884
[1mStep[0m  [10/26], [94mLoss[0m : 2.84910
[1mStep[0m  [12/26], [94mLoss[0m : 2.81061
[1mStep[0m  [14/26], [94mLoss[0m : 2.80151
[1mStep[0m  [16/26], [94mLoss[0m : 2.88887
[1mStep[0m  [18/26], [94mLoss[0m : 2.73750
[1mStep[0m  [20/26], [94mLoss[0m : 2.71278
[1mStep[0m  [22/26], [94mLoss[0m : 2.73600
[1mStep[0m  [24/26], [94mLoss[0m : 2.67621

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.784, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.86494
[1mStep[0m  [2/26], [94mLoss[0m : 2.64710
[1mStep[0m  [4/26], [94mLoss[0m : 2.68908
[1mStep[0m  [6/26], [94mLoss[0m : 2.58562
[1mStep[0m  [8/26], [94mLoss[0m : 2.73521
[1mStep[0m  [10/26], [94mLoss[0m : 2.60909
[1mStep[0m  [12/26], [94mLoss[0m : 2.74440
[1mStep[0m  [14/26], [94mLoss[0m : 2.66295
[1mStep[0m  [16/26], [94mLoss[0m : 3.04239
[1mStep[0m  [18/26], [94mLoss[0m : 2.93183
[1mStep[0m  [20/26], [94mLoss[0m : 2.79922
[1mStep[0m  [22/26], [94mLoss[0m : 2.99331
[1mStep[0m  [24/26], [94mLoss[0m : 2.80285

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.760, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71693
[1mStep[0m  [2/26], [94mLoss[0m : 2.89878
[1mStep[0m  [4/26], [94mLoss[0m : 2.65150
[1mStep[0m  [6/26], [94mLoss[0m : 2.73767
[1mStep[0m  [8/26], [94mLoss[0m : 2.60214
[1mStep[0m  [10/26], [94mLoss[0m : 2.85055
[1mStep[0m  [12/26], [94mLoss[0m : 2.60301
[1mStep[0m  [14/26], [94mLoss[0m : 2.62950
[1mStep[0m  [16/26], [94mLoss[0m : 2.69484
[1mStep[0m  [18/26], [94mLoss[0m : 2.83979
[1mStep[0m  [20/26], [94mLoss[0m : 2.70152
[1mStep[0m  [22/26], [94mLoss[0m : 2.70271
[1mStep[0m  [24/26], [94mLoss[0m : 2.62952

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.04705
[1mStep[0m  [2/26], [94mLoss[0m : 2.85375
[1mStep[0m  [4/26], [94mLoss[0m : 2.60683
[1mStep[0m  [6/26], [94mLoss[0m : 2.81937
[1mStep[0m  [8/26], [94mLoss[0m : 2.65363
[1mStep[0m  [10/26], [94mLoss[0m : 2.61517
[1mStep[0m  [12/26], [94mLoss[0m : 2.66488
[1mStep[0m  [14/26], [94mLoss[0m : 2.74059
[1mStep[0m  [16/26], [94mLoss[0m : 2.72737
[1mStep[0m  [18/26], [94mLoss[0m : 2.75055
[1mStep[0m  [20/26], [94mLoss[0m : 2.72913
[1mStep[0m  [22/26], [94mLoss[0m : 2.68045
[1mStep[0m  [24/26], [94mLoss[0m : 2.74702

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.748, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68428
[1mStep[0m  [2/26], [94mLoss[0m : 2.53372
[1mStep[0m  [4/26], [94mLoss[0m : 2.82751
[1mStep[0m  [6/26], [94mLoss[0m : 2.61577
[1mStep[0m  [8/26], [94mLoss[0m : 2.61155
[1mStep[0m  [10/26], [94mLoss[0m : 2.71212
[1mStep[0m  [12/26], [94mLoss[0m : 2.85024
[1mStep[0m  [14/26], [94mLoss[0m : 2.57316
[1mStep[0m  [16/26], [94mLoss[0m : 2.65677
[1mStep[0m  [18/26], [94mLoss[0m : 2.78523
[1mStep[0m  [20/26], [94mLoss[0m : 2.82342
[1mStep[0m  [22/26], [94mLoss[0m : 2.79419
[1mStep[0m  [24/26], [94mLoss[0m : 2.90220

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76188
[1mStep[0m  [2/26], [94mLoss[0m : 2.86780
[1mStep[0m  [4/26], [94mLoss[0m : 2.74353
[1mStep[0m  [6/26], [94mLoss[0m : 2.93154
[1mStep[0m  [8/26], [94mLoss[0m : 2.77076
[1mStep[0m  [10/26], [94mLoss[0m : 2.62584
[1mStep[0m  [12/26], [94mLoss[0m : 2.72002
[1mStep[0m  [14/26], [94mLoss[0m : 2.72686
[1mStep[0m  [16/26], [94mLoss[0m : 2.81947
[1mStep[0m  [18/26], [94mLoss[0m : 2.58639
[1mStep[0m  [20/26], [94mLoss[0m : 2.71188
[1mStep[0m  [22/26], [94mLoss[0m : 2.62951
[1mStep[0m  [24/26], [94mLoss[0m : 2.78345

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.737, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76772
[1mStep[0m  [2/26], [94mLoss[0m : 2.75258
[1mStep[0m  [4/26], [94mLoss[0m : 2.65321
[1mStep[0m  [6/26], [94mLoss[0m : 2.82558
[1mStep[0m  [8/26], [94mLoss[0m : 2.81666
[1mStep[0m  [10/26], [94mLoss[0m : 2.64407
[1mStep[0m  [12/26], [94mLoss[0m : 2.85760
[1mStep[0m  [14/26], [94mLoss[0m : 2.59674
[1mStep[0m  [16/26], [94mLoss[0m : 2.66277
[1mStep[0m  [18/26], [94mLoss[0m : 2.76339
[1mStep[0m  [20/26], [94mLoss[0m : 2.76723
[1mStep[0m  [22/26], [94mLoss[0m : 2.77458
[1mStep[0m  [24/26], [94mLoss[0m : 2.66654

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74339
[1mStep[0m  [2/26], [94mLoss[0m : 2.78609
[1mStep[0m  [4/26], [94mLoss[0m : 2.77569
[1mStep[0m  [6/26], [94mLoss[0m : 2.81354
[1mStep[0m  [8/26], [94mLoss[0m : 2.78095
[1mStep[0m  [10/26], [94mLoss[0m : 2.76616
[1mStep[0m  [12/26], [94mLoss[0m : 2.75207
[1mStep[0m  [14/26], [94mLoss[0m : 2.67803
[1mStep[0m  [16/26], [94mLoss[0m : 2.73416
[1mStep[0m  [18/26], [94mLoss[0m : 2.47165
[1mStep[0m  [20/26], [94mLoss[0m : 2.72660
[1mStep[0m  [22/26], [94mLoss[0m : 2.80770
[1mStep[0m  [24/26], [94mLoss[0m : 2.85185

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.82297
[1mStep[0m  [2/26], [94mLoss[0m : 2.70620
[1mStep[0m  [4/26], [94mLoss[0m : 2.66740
[1mStep[0m  [6/26], [94mLoss[0m : 2.85462
[1mStep[0m  [8/26], [94mLoss[0m : 2.72508
[1mStep[0m  [10/26], [94mLoss[0m : 2.67095
[1mStep[0m  [12/26], [94mLoss[0m : 2.61782
[1mStep[0m  [14/26], [94mLoss[0m : 2.72877
[1mStep[0m  [16/26], [94mLoss[0m : 2.60499
[1mStep[0m  [18/26], [94mLoss[0m : 2.63405
[1mStep[0m  [20/26], [94mLoss[0m : 2.78441
[1mStep[0m  [22/26], [94mLoss[0m : 2.77730
[1mStep[0m  [24/26], [94mLoss[0m : 2.65244

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65503
[1mStep[0m  [2/26], [94mLoss[0m : 2.73246
[1mStep[0m  [4/26], [94mLoss[0m : 2.71071
[1mStep[0m  [6/26], [94mLoss[0m : 2.85983
[1mStep[0m  [8/26], [94mLoss[0m : 2.62758
[1mStep[0m  [10/26], [94mLoss[0m : 2.72518
[1mStep[0m  [12/26], [94mLoss[0m : 2.77018
[1mStep[0m  [14/26], [94mLoss[0m : 2.69927
[1mStep[0m  [16/26], [94mLoss[0m : 2.78802
[1mStep[0m  [18/26], [94mLoss[0m : 2.69582
[1mStep[0m  [20/26], [94mLoss[0m : 2.79809
[1mStep[0m  [22/26], [94mLoss[0m : 2.51907
[1mStep[0m  [24/26], [94mLoss[0m : 2.70319

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66494
[1mStep[0m  [2/26], [94mLoss[0m : 2.69841
[1mStep[0m  [4/26], [94mLoss[0m : 2.64665
[1mStep[0m  [6/26], [94mLoss[0m : 2.79695
[1mStep[0m  [8/26], [94mLoss[0m : 2.56649
[1mStep[0m  [10/26], [94mLoss[0m : 2.86779
[1mStep[0m  [12/26], [94mLoss[0m : 2.78764
[1mStep[0m  [14/26], [94mLoss[0m : 2.50085
[1mStep[0m  [16/26], [94mLoss[0m : 2.62292
[1mStep[0m  [18/26], [94mLoss[0m : 2.77846
[1mStep[0m  [20/26], [94mLoss[0m : 2.77801
[1mStep[0m  [22/26], [94mLoss[0m : 2.62430
[1mStep[0m  [24/26], [94mLoss[0m : 2.65912

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.412
====================================

Phase 1 - Evaluation MAE:  2.4115794621981106
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 2.69832
[1mStep[0m  [2/26], [94mLoss[0m : 2.64651
[1mStep[0m  [4/26], [94mLoss[0m : 2.76624
[1mStep[0m  [6/26], [94mLoss[0m : 2.76537
[1mStep[0m  [8/26], [94mLoss[0m : 2.94871
[1mStep[0m  [10/26], [94mLoss[0m : 2.83373
[1mStep[0m  [12/26], [94mLoss[0m : 2.65887
[1mStep[0m  [14/26], [94mLoss[0m : 2.68865
[1mStep[0m  [16/26], [94mLoss[0m : 2.77581
[1mStep[0m  [18/26], [94mLoss[0m : 2.71015
[1mStep[0m  [20/26], [94mLoss[0m : 2.55648
[1mStep[0m  [22/26], [94mLoss[0m : 2.61927
[1mStep[0m  [24/26], [94mLoss[0m : 2.62500

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62337
[1mStep[0m  [2/26], [94mLoss[0m : 2.79506
[1mStep[0m  [4/26], [94mLoss[0m : 2.72116
[1mStep[0m  [6/26], [94mLoss[0m : 2.64838
[1mStep[0m  [8/26], [94mLoss[0m : 2.78429
[1mStep[0m  [10/26], [94mLoss[0m : 2.62858
[1mStep[0m  [12/26], [94mLoss[0m : 2.72912
[1mStep[0m  [14/26], [94mLoss[0m : 2.61427
[1mStep[0m  [16/26], [94mLoss[0m : 2.72792
[1mStep[0m  [18/26], [94mLoss[0m : 2.72006
[1mStep[0m  [20/26], [94mLoss[0m : 2.82078
[1mStep[0m  [22/26], [94mLoss[0m : 2.58446
[1mStep[0m  [24/26], [94mLoss[0m : 2.84733

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61512
[1mStep[0m  [2/26], [94mLoss[0m : 2.54678
[1mStep[0m  [4/26], [94mLoss[0m : 2.68896
[1mStep[0m  [6/26], [94mLoss[0m : 2.67821
[1mStep[0m  [8/26], [94mLoss[0m : 2.69349
[1mStep[0m  [10/26], [94mLoss[0m : 2.71995
[1mStep[0m  [12/26], [94mLoss[0m : 2.67297
[1mStep[0m  [14/26], [94mLoss[0m : 2.61968
[1mStep[0m  [16/26], [94mLoss[0m : 2.56917
[1mStep[0m  [18/26], [94mLoss[0m : 2.65855
[1mStep[0m  [20/26], [94mLoss[0m : 2.51557
[1mStep[0m  [22/26], [94mLoss[0m : 2.68612
[1mStep[0m  [24/26], [94mLoss[0m : 2.70039

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67087
[1mStep[0m  [2/26], [94mLoss[0m : 2.61912
[1mStep[0m  [4/26], [94mLoss[0m : 2.50700
[1mStep[0m  [6/26], [94mLoss[0m : 2.66346
[1mStep[0m  [8/26], [94mLoss[0m : 2.74513
[1mStep[0m  [10/26], [94mLoss[0m : 2.60301
[1mStep[0m  [12/26], [94mLoss[0m : 2.62017
[1mStep[0m  [14/26], [94mLoss[0m : 2.64197
[1mStep[0m  [16/26], [94mLoss[0m : 2.65332
[1mStep[0m  [18/26], [94mLoss[0m : 2.72399
[1mStep[0m  [20/26], [94mLoss[0m : 2.52809
[1mStep[0m  [22/26], [94mLoss[0m : 2.63281
[1mStep[0m  [24/26], [94mLoss[0m : 2.59007

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53349
[1mStep[0m  [2/26], [94mLoss[0m : 2.60096
[1mStep[0m  [4/26], [94mLoss[0m : 2.68845
[1mStep[0m  [6/26], [94mLoss[0m : 2.61017
[1mStep[0m  [8/26], [94mLoss[0m : 2.54342
[1mStep[0m  [10/26], [94mLoss[0m : 2.45714
[1mStep[0m  [12/26], [94mLoss[0m : 2.49109
[1mStep[0m  [14/26], [94mLoss[0m : 2.59381
[1mStep[0m  [16/26], [94mLoss[0m : 2.60425
[1mStep[0m  [18/26], [94mLoss[0m : 2.82785
[1mStep[0m  [20/26], [94mLoss[0m : 2.55324
[1mStep[0m  [22/26], [94mLoss[0m : 2.62403
[1mStep[0m  [24/26], [94mLoss[0m : 2.61690

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71267
[1mStep[0m  [2/26], [94mLoss[0m : 2.64938
[1mStep[0m  [4/26], [94mLoss[0m : 2.50172
[1mStep[0m  [6/26], [94mLoss[0m : 2.66580
[1mStep[0m  [8/26], [94mLoss[0m : 2.66162
[1mStep[0m  [10/26], [94mLoss[0m : 2.74466
[1mStep[0m  [12/26], [94mLoss[0m : 2.43229
[1mStep[0m  [14/26], [94mLoss[0m : 2.62909
[1mStep[0m  [16/26], [94mLoss[0m : 2.68897
[1mStep[0m  [18/26], [94mLoss[0m : 2.69814
[1mStep[0m  [20/26], [94mLoss[0m : 2.53477
[1mStep[0m  [22/26], [94mLoss[0m : 2.47246
[1mStep[0m  [24/26], [94mLoss[0m : 2.58593

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.579, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78151
[1mStep[0m  [2/26], [94mLoss[0m : 2.39265
[1mStep[0m  [4/26], [94mLoss[0m : 2.53138
[1mStep[0m  [6/26], [94mLoss[0m : 2.62325
[1mStep[0m  [8/26], [94mLoss[0m : 2.58710
[1mStep[0m  [10/26], [94mLoss[0m : 2.52434
[1mStep[0m  [12/26], [94mLoss[0m : 2.60047
[1mStep[0m  [14/26], [94mLoss[0m : 2.64484
[1mStep[0m  [16/26], [94mLoss[0m : 2.56660
[1mStep[0m  [18/26], [94mLoss[0m : 2.42754
[1mStep[0m  [20/26], [94mLoss[0m : 2.54143
[1mStep[0m  [22/26], [94mLoss[0m : 2.66268
[1mStep[0m  [24/26], [94mLoss[0m : 2.60088

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61491
[1mStep[0m  [2/26], [94mLoss[0m : 2.47828
[1mStep[0m  [4/26], [94mLoss[0m : 2.70674
[1mStep[0m  [6/26], [94mLoss[0m : 2.77802
[1mStep[0m  [8/26], [94mLoss[0m : 2.65763
[1mStep[0m  [10/26], [94mLoss[0m : 2.53891
[1mStep[0m  [12/26], [94mLoss[0m : 2.60278
[1mStep[0m  [14/26], [94mLoss[0m : 2.66606
[1mStep[0m  [16/26], [94mLoss[0m : 2.51860
[1mStep[0m  [18/26], [94mLoss[0m : 2.49782
[1mStep[0m  [20/26], [94mLoss[0m : 2.61539
[1mStep[0m  [22/26], [94mLoss[0m : 2.46179
[1mStep[0m  [24/26], [94mLoss[0m : 2.56185

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50464
[1mStep[0m  [2/26], [94mLoss[0m : 2.38168
[1mStep[0m  [4/26], [94mLoss[0m : 2.73035
[1mStep[0m  [6/26], [94mLoss[0m : 2.54356
[1mStep[0m  [8/26], [94mLoss[0m : 2.69834
[1mStep[0m  [10/26], [94mLoss[0m : 2.62672
[1mStep[0m  [12/26], [94mLoss[0m : 2.49757
[1mStep[0m  [14/26], [94mLoss[0m : 2.52892
[1mStep[0m  [16/26], [94mLoss[0m : 2.58232
[1mStep[0m  [18/26], [94mLoss[0m : 2.43651
[1mStep[0m  [20/26], [94mLoss[0m : 2.39183
[1mStep[0m  [22/26], [94mLoss[0m : 2.54629
[1mStep[0m  [24/26], [94mLoss[0m : 2.60884

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57391
[1mStep[0m  [2/26], [94mLoss[0m : 2.54320
[1mStep[0m  [4/26], [94mLoss[0m : 2.55543
[1mStep[0m  [6/26], [94mLoss[0m : 2.44539
[1mStep[0m  [8/26], [94mLoss[0m : 2.60074
[1mStep[0m  [10/26], [94mLoss[0m : 2.67226
[1mStep[0m  [12/26], [94mLoss[0m : 2.35475
[1mStep[0m  [14/26], [94mLoss[0m : 2.58756
[1mStep[0m  [16/26], [94mLoss[0m : 2.64363
[1mStep[0m  [18/26], [94mLoss[0m : 2.59111
[1mStep[0m  [20/26], [94mLoss[0m : 2.39851
[1mStep[0m  [22/26], [94mLoss[0m : 2.50028
[1mStep[0m  [24/26], [94mLoss[0m : 2.59991

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44872
[1mStep[0m  [2/26], [94mLoss[0m : 2.48644
[1mStep[0m  [4/26], [94mLoss[0m : 2.56367
[1mStep[0m  [6/26], [94mLoss[0m : 2.38493
[1mStep[0m  [8/26], [94mLoss[0m : 2.56490
[1mStep[0m  [10/26], [94mLoss[0m : 2.35204
[1mStep[0m  [12/26], [94mLoss[0m : 2.35780
[1mStep[0m  [14/26], [94mLoss[0m : 2.37393
[1mStep[0m  [16/26], [94mLoss[0m : 2.49028
[1mStep[0m  [18/26], [94mLoss[0m : 2.35447
[1mStep[0m  [20/26], [94mLoss[0m : 2.60521
[1mStep[0m  [22/26], [94mLoss[0m : 2.45820
[1mStep[0m  [24/26], [94mLoss[0m : 2.45941

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35826
[1mStep[0m  [2/26], [94mLoss[0m : 2.49190
[1mStep[0m  [4/26], [94mLoss[0m : 2.40316
[1mStep[0m  [6/26], [94mLoss[0m : 2.41096
[1mStep[0m  [8/26], [94mLoss[0m : 2.39126
[1mStep[0m  [10/26], [94mLoss[0m : 2.63821
[1mStep[0m  [12/26], [94mLoss[0m : 2.40263
[1mStep[0m  [14/26], [94mLoss[0m : 2.30500
[1mStep[0m  [16/26], [94mLoss[0m : 2.49384
[1mStep[0m  [18/26], [94mLoss[0m : 2.42242
[1mStep[0m  [20/26], [94mLoss[0m : 2.41793
[1mStep[0m  [22/26], [94mLoss[0m : 2.44612
[1mStep[0m  [24/26], [94mLoss[0m : 2.47599

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33832
[1mStep[0m  [2/26], [94mLoss[0m : 2.57530
[1mStep[0m  [4/26], [94mLoss[0m : 2.40846
[1mStep[0m  [6/26], [94mLoss[0m : 2.45569
[1mStep[0m  [8/26], [94mLoss[0m : 2.43219
[1mStep[0m  [10/26], [94mLoss[0m : 2.56558
[1mStep[0m  [12/26], [94mLoss[0m : 2.58477
[1mStep[0m  [14/26], [94mLoss[0m : 2.39314
[1mStep[0m  [16/26], [94mLoss[0m : 2.29089
[1mStep[0m  [18/26], [94mLoss[0m : 2.40063
[1mStep[0m  [20/26], [94mLoss[0m : 2.33802
[1mStep[0m  [22/26], [94mLoss[0m : 2.41795
[1mStep[0m  [24/26], [94mLoss[0m : 2.44991

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26712
[1mStep[0m  [2/26], [94mLoss[0m : 2.33756
[1mStep[0m  [4/26], [94mLoss[0m : 2.42065
[1mStep[0m  [6/26], [94mLoss[0m : 2.38258
[1mStep[0m  [8/26], [94mLoss[0m : 2.40920
[1mStep[0m  [10/26], [94mLoss[0m : 2.40722
[1mStep[0m  [12/26], [94mLoss[0m : 2.43017
[1mStep[0m  [14/26], [94mLoss[0m : 2.39795
[1mStep[0m  [16/26], [94mLoss[0m : 2.42336
[1mStep[0m  [18/26], [94mLoss[0m : 2.44407
[1mStep[0m  [20/26], [94mLoss[0m : 2.42803
[1mStep[0m  [22/26], [94mLoss[0m : 2.44949
[1mStep[0m  [24/26], [94mLoss[0m : 2.21852

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28156
[1mStep[0m  [2/26], [94mLoss[0m : 2.33177
[1mStep[0m  [4/26], [94mLoss[0m : 2.34270
[1mStep[0m  [6/26], [94mLoss[0m : 2.30741
[1mStep[0m  [8/26], [94mLoss[0m : 2.36207
[1mStep[0m  [10/26], [94mLoss[0m : 2.44950
[1mStep[0m  [12/26], [94mLoss[0m : 2.19169
[1mStep[0m  [14/26], [94mLoss[0m : 2.52559
[1mStep[0m  [16/26], [94mLoss[0m : 2.33001
[1mStep[0m  [18/26], [94mLoss[0m : 2.40971
[1mStep[0m  [20/26], [94mLoss[0m : 2.47190
[1mStep[0m  [22/26], [94mLoss[0m : 2.43736
[1mStep[0m  [24/26], [94mLoss[0m : 2.54137

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44381
[1mStep[0m  [2/26], [94mLoss[0m : 2.30539
[1mStep[0m  [4/26], [94mLoss[0m : 2.45548
[1mStep[0m  [6/26], [94mLoss[0m : 2.34050
[1mStep[0m  [8/26], [94mLoss[0m : 2.25995
[1mStep[0m  [10/26], [94mLoss[0m : 2.38715
[1mStep[0m  [12/26], [94mLoss[0m : 2.49682
[1mStep[0m  [14/26], [94mLoss[0m : 2.31580
[1mStep[0m  [16/26], [94mLoss[0m : 2.18269
[1mStep[0m  [18/26], [94mLoss[0m : 2.30250
[1mStep[0m  [20/26], [94mLoss[0m : 2.45575
[1mStep[0m  [22/26], [94mLoss[0m : 2.30132
[1mStep[0m  [24/26], [94mLoss[0m : 2.30115

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26259
[1mStep[0m  [2/26], [94mLoss[0m : 2.34015
[1mStep[0m  [4/26], [94mLoss[0m : 2.28281
[1mStep[0m  [6/26], [94mLoss[0m : 2.42555
[1mStep[0m  [8/26], [94mLoss[0m : 2.23773
[1mStep[0m  [10/26], [94mLoss[0m : 2.36860
[1mStep[0m  [12/26], [94mLoss[0m : 2.38612
[1mStep[0m  [14/26], [94mLoss[0m : 2.47183
[1mStep[0m  [16/26], [94mLoss[0m : 2.25118
[1mStep[0m  [18/26], [94mLoss[0m : 2.29609
[1mStep[0m  [20/26], [94mLoss[0m : 2.27926
[1mStep[0m  [22/26], [94mLoss[0m : 2.27789
[1mStep[0m  [24/26], [94mLoss[0m : 2.38177

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20917
[1mStep[0m  [2/26], [94mLoss[0m : 2.26366
[1mStep[0m  [4/26], [94mLoss[0m : 2.30063
[1mStep[0m  [6/26], [94mLoss[0m : 2.29237
[1mStep[0m  [8/26], [94mLoss[0m : 2.46339
[1mStep[0m  [10/26], [94mLoss[0m : 2.21857
[1mStep[0m  [12/26], [94mLoss[0m : 2.24066
[1mStep[0m  [14/26], [94mLoss[0m : 2.27257
[1mStep[0m  [16/26], [94mLoss[0m : 2.13365
[1mStep[0m  [18/26], [94mLoss[0m : 2.28232
[1mStep[0m  [20/26], [94mLoss[0m : 2.47249
[1mStep[0m  [22/26], [94mLoss[0m : 2.36738
[1mStep[0m  [24/26], [94mLoss[0m : 2.32482

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.543, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.13377
[1mStep[0m  [2/26], [94mLoss[0m : 2.26064
[1mStep[0m  [4/26], [94mLoss[0m : 2.27422
[1mStep[0m  [6/26], [94mLoss[0m : 2.22366
[1mStep[0m  [8/26], [94mLoss[0m : 2.24222
[1mStep[0m  [10/26], [94mLoss[0m : 2.31053
[1mStep[0m  [12/26], [94mLoss[0m : 2.31211
[1mStep[0m  [14/26], [94mLoss[0m : 2.39872
[1mStep[0m  [16/26], [94mLoss[0m : 2.30064
[1mStep[0m  [18/26], [94mLoss[0m : 2.31989
[1mStep[0m  [20/26], [94mLoss[0m : 2.41435
[1mStep[0m  [22/26], [94mLoss[0m : 2.16463
[1mStep[0m  [24/26], [94mLoss[0m : 2.34142

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22346
[1mStep[0m  [2/26], [94mLoss[0m : 2.25097
[1mStep[0m  [4/26], [94mLoss[0m : 2.26999
[1mStep[0m  [6/26], [94mLoss[0m : 2.41607
[1mStep[0m  [8/26], [94mLoss[0m : 2.16061
[1mStep[0m  [10/26], [94mLoss[0m : 2.24032
[1mStep[0m  [12/26], [94mLoss[0m : 2.23857
[1mStep[0m  [14/26], [94mLoss[0m : 2.25795
[1mStep[0m  [16/26], [94mLoss[0m : 2.29269
[1mStep[0m  [18/26], [94mLoss[0m : 2.32012
[1mStep[0m  [20/26], [94mLoss[0m : 2.22265
[1mStep[0m  [22/26], [94mLoss[0m : 2.09171
[1mStep[0m  [24/26], [94mLoss[0m : 2.35891

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.539, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23379
[1mStep[0m  [2/26], [94mLoss[0m : 2.24964
[1mStep[0m  [4/26], [94mLoss[0m : 2.22812
[1mStep[0m  [6/26], [94mLoss[0m : 2.09342
[1mStep[0m  [8/26], [94mLoss[0m : 2.09797
[1mStep[0m  [10/26], [94mLoss[0m : 2.21645
[1mStep[0m  [12/26], [94mLoss[0m : 2.18118
[1mStep[0m  [14/26], [94mLoss[0m : 2.16560
[1mStep[0m  [16/26], [94mLoss[0m : 2.17478
[1mStep[0m  [18/26], [94mLoss[0m : 2.16500
[1mStep[0m  [20/26], [94mLoss[0m : 2.33370
[1mStep[0m  [22/26], [94mLoss[0m : 2.30088
[1mStep[0m  [24/26], [94mLoss[0m : 2.23022

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.14523
[1mStep[0m  [2/26], [94mLoss[0m : 2.10857
[1mStep[0m  [4/26], [94mLoss[0m : 2.13463
[1mStep[0m  [6/26], [94mLoss[0m : 2.25230
[1mStep[0m  [8/26], [94mLoss[0m : 2.04126
[1mStep[0m  [10/26], [94mLoss[0m : 2.12768
[1mStep[0m  [12/26], [94mLoss[0m : 2.29042
[1mStep[0m  [14/26], [94mLoss[0m : 2.03468
[1mStep[0m  [16/26], [94mLoss[0m : 2.14904
[1mStep[0m  [18/26], [94mLoss[0m : 2.17618
[1mStep[0m  [20/26], [94mLoss[0m : 2.40968
[1mStep[0m  [22/26], [94mLoss[0m : 2.30058
[1mStep[0m  [24/26], [94mLoss[0m : 2.18413

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23344
[1mStep[0m  [2/26], [94mLoss[0m : 2.18996
[1mStep[0m  [4/26], [94mLoss[0m : 2.13488
[1mStep[0m  [6/26], [94mLoss[0m : 2.15102
[1mStep[0m  [8/26], [94mLoss[0m : 2.00674
[1mStep[0m  [10/26], [94mLoss[0m : 2.16960
[1mStep[0m  [12/26], [94mLoss[0m : 2.19281
[1mStep[0m  [14/26], [94mLoss[0m : 2.19207
[1mStep[0m  [16/26], [94mLoss[0m : 2.16644
[1mStep[0m  [18/26], [94mLoss[0m : 2.19430
[1mStep[0m  [20/26], [94mLoss[0m : 2.10904
[1mStep[0m  [22/26], [94mLoss[0m : 2.10364
[1mStep[0m  [24/26], [94mLoss[0m : 2.25788

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.429, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.11899
[1mStep[0m  [2/26], [94mLoss[0m : 2.15398
[1mStep[0m  [4/26], [94mLoss[0m : 2.17536
[1mStep[0m  [6/26], [94mLoss[0m : 2.08761
[1mStep[0m  [8/26], [94mLoss[0m : 2.07264
[1mStep[0m  [10/26], [94mLoss[0m : 2.29239
[1mStep[0m  [12/26], [94mLoss[0m : 2.07286
[1mStep[0m  [14/26], [94mLoss[0m : 2.21248
[1mStep[0m  [16/26], [94mLoss[0m : 2.12102
[1mStep[0m  [18/26], [94mLoss[0m : 2.32378
[1mStep[0m  [20/26], [94mLoss[0m : 2.22828
[1mStep[0m  [22/26], [94mLoss[0m : 1.91400
[1mStep[0m  [24/26], [94mLoss[0m : 2.14134

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98570
[1mStep[0m  [2/26], [94mLoss[0m : 2.03150
[1mStep[0m  [4/26], [94mLoss[0m : 2.13046
[1mStep[0m  [6/26], [94mLoss[0m : 2.11601
[1mStep[0m  [8/26], [94mLoss[0m : 2.16681
[1mStep[0m  [10/26], [94mLoss[0m : 2.15386
[1mStep[0m  [12/26], [94mLoss[0m : 2.13794
[1mStep[0m  [14/26], [94mLoss[0m : 2.15021
[1mStep[0m  [16/26], [94mLoss[0m : 2.22347
[1mStep[0m  [18/26], [94mLoss[0m : 2.20073
[1mStep[0m  [20/26], [94mLoss[0m : 2.12237
[1mStep[0m  [22/26], [94mLoss[0m : 2.03642
[1mStep[0m  [24/26], [94mLoss[0m : 2.10021

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.128, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.92971
[1mStep[0m  [2/26], [94mLoss[0m : 2.07003
[1mStep[0m  [4/26], [94mLoss[0m : 2.23368
[1mStep[0m  [6/26], [94mLoss[0m : 1.86772
[1mStep[0m  [8/26], [94mLoss[0m : 2.06355
[1mStep[0m  [10/26], [94mLoss[0m : 2.12937
[1mStep[0m  [12/26], [94mLoss[0m : 2.05314
[1mStep[0m  [14/26], [94mLoss[0m : 2.25308
[1mStep[0m  [16/26], [94mLoss[0m : 1.93164
[1mStep[0m  [18/26], [94mLoss[0m : 2.16892
[1mStep[0m  [20/26], [94mLoss[0m : 2.10613
[1mStep[0m  [22/26], [94mLoss[0m : 1.96748
[1mStep[0m  [24/26], [94mLoss[0m : 1.92680

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.078, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.01361
[1mStep[0m  [2/26], [94mLoss[0m : 2.10353
[1mStep[0m  [4/26], [94mLoss[0m : 2.11945
[1mStep[0m  [6/26], [94mLoss[0m : 2.17545
[1mStep[0m  [8/26], [94mLoss[0m : 2.09456
[1mStep[0m  [10/26], [94mLoss[0m : 2.16267
[1mStep[0m  [12/26], [94mLoss[0m : 2.14185
[1mStep[0m  [14/26], [94mLoss[0m : 2.14830
[1mStep[0m  [16/26], [94mLoss[0m : 2.01265
[1mStep[0m  [18/26], [94mLoss[0m : 2.07764
[1mStep[0m  [20/26], [94mLoss[0m : 1.97899
[1mStep[0m  [22/26], [94mLoss[0m : 2.18514
[1mStep[0m  [24/26], [94mLoss[0m : 2.09942

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.445, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08612
[1mStep[0m  [2/26], [94mLoss[0m : 2.09038
[1mStep[0m  [4/26], [94mLoss[0m : 2.07836
[1mStep[0m  [6/26], [94mLoss[0m : 2.07793
[1mStep[0m  [8/26], [94mLoss[0m : 2.08350
[1mStep[0m  [10/26], [94mLoss[0m : 1.92535
[1mStep[0m  [12/26], [94mLoss[0m : 2.04540
[1mStep[0m  [14/26], [94mLoss[0m : 2.11754
[1mStep[0m  [16/26], [94mLoss[0m : 2.06382
[1mStep[0m  [18/26], [94mLoss[0m : 2.05081
[1mStep[0m  [20/26], [94mLoss[0m : 1.94449
[1mStep[0m  [22/26], [94mLoss[0m : 1.98069
[1mStep[0m  [24/26], [94mLoss[0m : 2.13955

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.14073
[1mStep[0m  [2/26], [94mLoss[0m : 1.91808
[1mStep[0m  [4/26], [94mLoss[0m : 2.11463
[1mStep[0m  [6/26], [94mLoss[0m : 1.81923
[1mStep[0m  [8/26], [94mLoss[0m : 1.99408
[1mStep[0m  [10/26], [94mLoss[0m : 2.16116
[1mStep[0m  [12/26], [94mLoss[0m : 2.00803
[1mStep[0m  [14/26], [94mLoss[0m : 1.99108
[1mStep[0m  [16/26], [94mLoss[0m : 2.03109
[1mStep[0m  [18/26], [94mLoss[0m : 2.09846
[1mStep[0m  [20/26], [94mLoss[0m : 1.98001
[1mStep[0m  [22/26], [94mLoss[0m : 2.02869
[1mStep[0m  [24/26], [94mLoss[0m : 1.94506

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.554, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09938
[1mStep[0m  [2/26], [94mLoss[0m : 1.94139
[1mStep[0m  [4/26], [94mLoss[0m : 2.09483
[1mStep[0m  [6/26], [94mLoss[0m : 1.99820
[1mStep[0m  [8/26], [94mLoss[0m : 1.93022
[1mStep[0m  [10/26], [94mLoss[0m : 2.06383
[1mStep[0m  [12/26], [94mLoss[0m : 1.93683
[1mStep[0m  [14/26], [94mLoss[0m : 2.04264
[1mStep[0m  [16/26], [94mLoss[0m : 1.93110
[1mStep[0m  [18/26], [94mLoss[0m : 2.11757
[1mStep[0m  [20/26], [94mLoss[0m : 2.13948
[1mStep[0m  [22/26], [94mLoss[0m : 2.01919
[1mStep[0m  [24/26], [94mLoss[0m : 2.13775

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.587, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.434
====================================

Phase 2 - Evaluation MAE:  2.434041243333083
MAE score P1      2.411579
MAE score P2      2.434041
loss              2.010633
learning_rate      0.00505
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 11.41514
[1mStep[0m  [2/26], [94mLoss[0m : 11.35314
[1mStep[0m  [4/26], [94mLoss[0m : 10.75119
[1mStep[0m  [6/26], [94mLoss[0m : 9.25376
[1mStep[0m  [8/26], [94mLoss[0m : 7.91982
[1mStep[0m  [10/26], [94mLoss[0m : 6.30958
[1mStep[0m  [12/26], [94mLoss[0m : 4.67203
[1mStep[0m  [14/26], [94mLoss[0m : 3.16343
[1mStep[0m  [16/26], [94mLoss[0m : 2.68561
[1mStep[0m  [18/26], [94mLoss[0m : 2.89751
[1mStep[0m  [20/26], [94mLoss[0m : 3.08890
[1mStep[0m  [22/26], [94mLoss[0m : 3.46401
[1mStep[0m  [24/26], [94mLoss[0m : 3.58479

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.050, [92mTest[0m: 11.742, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.36982
[1mStep[0m  [2/26], [94mLoss[0m : 3.30806
[1mStep[0m  [4/26], [94mLoss[0m : 3.06171
[1mStep[0m  [6/26], [94mLoss[0m : 2.80602
[1mStep[0m  [8/26], [94mLoss[0m : 2.72010
[1mStep[0m  [10/26], [94mLoss[0m : 2.49597
[1mStep[0m  [12/26], [94mLoss[0m : 2.48052
[1mStep[0m  [14/26], [94mLoss[0m : 2.72103
[1mStep[0m  [16/26], [94mLoss[0m : 2.61059
[1mStep[0m  [18/26], [94mLoss[0m : 2.64400
[1mStep[0m  [20/26], [94mLoss[0m : 2.57627
[1mStep[0m  [22/26], [94mLoss[0m : 2.67477
[1mStep[0m  [24/26], [94mLoss[0m : 2.51619

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.757, [92mTest[0m: 3.776, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56315
[1mStep[0m  [2/26], [94mLoss[0m : 2.48732
[1mStep[0m  [4/26], [94mLoss[0m : 2.55742
[1mStep[0m  [6/26], [94mLoss[0m : 2.53635
[1mStep[0m  [8/26], [94mLoss[0m : 2.53767
[1mStep[0m  [10/26], [94mLoss[0m : 2.44160
[1mStep[0m  [12/26], [94mLoss[0m : 2.36400
[1mStep[0m  [14/26], [94mLoss[0m : 2.51215
[1mStep[0m  [16/26], [94mLoss[0m : 2.50769
[1mStep[0m  [18/26], [94mLoss[0m : 2.54084
[1mStep[0m  [20/26], [94mLoss[0m : 2.34624
[1mStep[0m  [22/26], [94mLoss[0m : 2.53840
[1mStep[0m  [24/26], [94mLoss[0m : 2.53340

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47859
[1mStep[0m  [2/26], [94mLoss[0m : 2.52475
[1mStep[0m  [4/26], [94mLoss[0m : 2.58098
[1mStep[0m  [6/26], [94mLoss[0m : 2.42418
[1mStep[0m  [8/26], [94mLoss[0m : 2.46085
[1mStep[0m  [10/26], [94mLoss[0m : 2.40553
[1mStep[0m  [12/26], [94mLoss[0m : 2.48063
[1mStep[0m  [14/26], [94mLoss[0m : 2.43048
[1mStep[0m  [16/26], [94mLoss[0m : 2.45308
[1mStep[0m  [18/26], [94mLoss[0m : 2.65209
[1mStep[0m  [20/26], [94mLoss[0m : 2.41301
[1mStep[0m  [22/26], [94mLoss[0m : 2.50610
[1mStep[0m  [24/26], [94mLoss[0m : 2.45259

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29800
[1mStep[0m  [2/26], [94mLoss[0m : 2.44926
[1mStep[0m  [4/26], [94mLoss[0m : 2.44025
[1mStep[0m  [6/26], [94mLoss[0m : 2.41316
[1mStep[0m  [8/26], [94mLoss[0m : 2.41803
[1mStep[0m  [10/26], [94mLoss[0m : 2.51569
[1mStep[0m  [12/26], [94mLoss[0m : 2.46475
[1mStep[0m  [14/26], [94mLoss[0m : 2.52324
[1mStep[0m  [16/26], [94mLoss[0m : 2.60226
[1mStep[0m  [18/26], [94mLoss[0m : 2.36861
[1mStep[0m  [20/26], [94mLoss[0m : 2.47369
[1mStep[0m  [22/26], [94mLoss[0m : 2.41353
[1mStep[0m  [24/26], [94mLoss[0m : 2.53330

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51038
[1mStep[0m  [2/26], [94mLoss[0m : 2.64037
[1mStep[0m  [4/26], [94mLoss[0m : 2.50086
[1mStep[0m  [6/26], [94mLoss[0m : 2.47261
[1mStep[0m  [8/26], [94mLoss[0m : 2.48382
[1mStep[0m  [10/26], [94mLoss[0m : 2.48181
[1mStep[0m  [12/26], [94mLoss[0m : 2.38829
[1mStep[0m  [14/26], [94mLoss[0m : 2.53132
[1mStep[0m  [16/26], [94mLoss[0m : 2.35829
[1mStep[0m  [18/26], [94mLoss[0m : 2.45691
[1mStep[0m  [20/26], [94mLoss[0m : 2.58605
[1mStep[0m  [22/26], [94mLoss[0m : 2.56082
[1mStep[0m  [24/26], [94mLoss[0m : 2.35227

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52453
[1mStep[0m  [2/26], [94mLoss[0m : 2.37113
[1mStep[0m  [4/26], [94mLoss[0m : 2.43303
[1mStep[0m  [6/26], [94mLoss[0m : 2.60027
[1mStep[0m  [8/26], [94mLoss[0m : 2.56683
[1mStep[0m  [10/26], [94mLoss[0m : 2.36189
[1mStep[0m  [12/26], [94mLoss[0m : 2.42271
[1mStep[0m  [14/26], [94mLoss[0m : 2.48988
[1mStep[0m  [16/26], [94mLoss[0m : 2.36847
[1mStep[0m  [18/26], [94mLoss[0m : 2.44630
[1mStep[0m  [20/26], [94mLoss[0m : 2.51297
[1mStep[0m  [22/26], [94mLoss[0m : 2.45477
[1mStep[0m  [24/26], [94mLoss[0m : 2.48590

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44729
[1mStep[0m  [2/26], [94mLoss[0m : 2.68180
[1mStep[0m  [4/26], [94mLoss[0m : 2.53875
[1mStep[0m  [6/26], [94mLoss[0m : 2.51321
[1mStep[0m  [8/26], [94mLoss[0m : 2.36318
[1mStep[0m  [10/26], [94mLoss[0m : 2.42009
[1mStep[0m  [12/26], [94mLoss[0m : 2.37745
[1mStep[0m  [14/26], [94mLoss[0m : 2.47444
[1mStep[0m  [16/26], [94mLoss[0m : 2.62032
[1mStep[0m  [18/26], [94mLoss[0m : 2.40992
[1mStep[0m  [20/26], [94mLoss[0m : 2.51017
[1mStep[0m  [22/26], [94mLoss[0m : 2.25687
[1mStep[0m  [24/26], [94mLoss[0m : 2.47508

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55862
[1mStep[0m  [2/26], [94mLoss[0m : 2.60544
[1mStep[0m  [4/26], [94mLoss[0m : 2.33332
[1mStep[0m  [6/26], [94mLoss[0m : 2.43539
[1mStep[0m  [8/26], [94mLoss[0m : 2.48502
[1mStep[0m  [10/26], [94mLoss[0m : 2.30108
[1mStep[0m  [12/26], [94mLoss[0m : 2.39598
[1mStep[0m  [14/26], [94mLoss[0m : 2.34270
[1mStep[0m  [16/26], [94mLoss[0m : 2.43003
[1mStep[0m  [18/26], [94mLoss[0m : 2.34990
[1mStep[0m  [20/26], [94mLoss[0m : 2.52991
[1mStep[0m  [22/26], [94mLoss[0m : 2.26264
[1mStep[0m  [24/26], [94mLoss[0m : 2.37565

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45274
[1mStep[0m  [2/26], [94mLoss[0m : 2.40317
[1mStep[0m  [4/26], [94mLoss[0m : 2.30705
[1mStep[0m  [6/26], [94mLoss[0m : 2.45595
[1mStep[0m  [8/26], [94mLoss[0m : 2.40163
[1mStep[0m  [10/26], [94mLoss[0m : 2.53016
[1mStep[0m  [12/26], [94mLoss[0m : 2.50802
[1mStep[0m  [14/26], [94mLoss[0m : 2.51201
[1mStep[0m  [16/26], [94mLoss[0m : 2.38767
[1mStep[0m  [18/26], [94mLoss[0m : 2.53797
[1mStep[0m  [20/26], [94mLoss[0m : 2.64613
[1mStep[0m  [22/26], [94mLoss[0m : 2.49924
[1mStep[0m  [24/26], [94mLoss[0m : 2.34263

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43165
[1mStep[0m  [2/26], [94mLoss[0m : 2.40996
[1mStep[0m  [4/26], [94mLoss[0m : 2.52516
[1mStep[0m  [6/26], [94mLoss[0m : 2.45515
[1mStep[0m  [8/26], [94mLoss[0m : 2.36563
[1mStep[0m  [10/26], [94mLoss[0m : 2.30771
[1mStep[0m  [12/26], [94mLoss[0m : 2.45783
[1mStep[0m  [14/26], [94mLoss[0m : 2.58283
[1mStep[0m  [16/26], [94mLoss[0m : 2.47193
[1mStep[0m  [18/26], [94mLoss[0m : 2.28134
[1mStep[0m  [20/26], [94mLoss[0m : 2.43826
[1mStep[0m  [22/26], [94mLoss[0m : 2.49063
[1mStep[0m  [24/26], [94mLoss[0m : 2.56085

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46190
[1mStep[0m  [2/26], [94mLoss[0m : 2.46642
[1mStep[0m  [4/26], [94mLoss[0m : 2.35667
[1mStep[0m  [6/26], [94mLoss[0m : 2.46441
[1mStep[0m  [8/26], [94mLoss[0m : 2.44921
[1mStep[0m  [10/26], [94mLoss[0m : 2.38283
[1mStep[0m  [12/26], [94mLoss[0m : 2.57424
[1mStep[0m  [14/26], [94mLoss[0m : 2.20870
[1mStep[0m  [16/26], [94mLoss[0m : 2.56086
[1mStep[0m  [18/26], [94mLoss[0m : 2.45109
[1mStep[0m  [20/26], [94mLoss[0m : 2.40594
[1mStep[0m  [22/26], [94mLoss[0m : 2.34668
[1mStep[0m  [24/26], [94mLoss[0m : 2.53499

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38722
[1mStep[0m  [2/26], [94mLoss[0m : 2.40626
[1mStep[0m  [4/26], [94mLoss[0m : 2.47443
[1mStep[0m  [6/26], [94mLoss[0m : 2.46491
[1mStep[0m  [8/26], [94mLoss[0m : 2.41362
[1mStep[0m  [10/26], [94mLoss[0m : 2.51057
[1mStep[0m  [12/26], [94mLoss[0m : 2.45421
[1mStep[0m  [14/26], [94mLoss[0m : 2.53657
[1mStep[0m  [16/26], [94mLoss[0m : 2.50089
[1mStep[0m  [18/26], [94mLoss[0m : 2.34532
[1mStep[0m  [20/26], [94mLoss[0m : 2.36248
[1mStep[0m  [22/26], [94mLoss[0m : 2.35298
[1mStep[0m  [24/26], [94mLoss[0m : 2.38502

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47215
[1mStep[0m  [2/26], [94mLoss[0m : 2.50916
[1mStep[0m  [4/26], [94mLoss[0m : 2.29276
[1mStep[0m  [6/26], [94mLoss[0m : 2.53140
[1mStep[0m  [8/26], [94mLoss[0m : 2.46119
[1mStep[0m  [10/26], [94mLoss[0m : 2.46859
[1mStep[0m  [12/26], [94mLoss[0m : 2.44199
[1mStep[0m  [14/26], [94mLoss[0m : 2.65594
[1mStep[0m  [16/26], [94mLoss[0m : 2.45574
[1mStep[0m  [18/26], [94mLoss[0m : 2.56390
[1mStep[0m  [20/26], [94mLoss[0m : 2.41464
[1mStep[0m  [22/26], [94mLoss[0m : 2.37953
[1mStep[0m  [24/26], [94mLoss[0m : 2.59967

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47163
[1mStep[0m  [2/26], [94mLoss[0m : 2.50422
[1mStep[0m  [4/26], [94mLoss[0m : 2.39398
[1mStep[0m  [6/26], [94mLoss[0m : 2.41610
[1mStep[0m  [8/26], [94mLoss[0m : 2.43664
[1mStep[0m  [10/26], [94mLoss[0m : 2.61996
[1mStep[0m  [12/26], [94mLoss[0m : 2.36460
[1mStep[0m  [14/26], [94mLoss[0m : 2.50257
[1mStep[0m  [16/26], [94mLoss[0m : 2.61121
[1mStep[0m  [18/26], [94mLoss[0m : 2.46692
[1mStep[0m  [20/26], [94mLoss[0m : 2.35455
[1mStep[0m  [22/26], [94mLoss[0m : 2.34844
[1mStep[0m  [24/26], [94mLoss[0m : 2.38745

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45237
[1mStep[0m  [2/26], [94mLoss[0m : 2.46397
[1mStep[0m  [4/26], [94mLoss[0m : 2.42627
[1mStep[0m  [6/26], [94mLoss[0m : 2.44012
[1mStep[0m  [8/26], [94mLoss[0m : 2.40277
[1mStep[0m  [10/26], [94mLoss[0m : 2.58400
[1mStep[0m  [12/26], [94mLoss[0m : 2.29185
[1mStep[0m  [14/26], [94mLoss[0m : 2.51852
[1mStep[0m  [16/26], [94mLoss[0m : 2.39727
[1mStep[0m  [18/26], [94mLoss[0m : 2.50007
[1mStep[0m  [20/26], [94mLoss[0m : 2.43385
[1mStep[0m  [22/26], [94mLoss[0m : 2.41811
[1mStep[0m  [24/26], [94mLoss[0m : 2.44775

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48943
[1mStep[0m  [2/26], [94mLoss[0m : 2.44990
[1mStep[0m  [4/26], [94mLoss[0m : 2.48137
[1mStep[0m  [6/26], [94mLoss[0m : 2.54448
[1mStep[0m  [8/26], [94mLoss[0m : 2.32324
[1mStep[0m  [10/26], [94mLoss[0m : 2.41478
[1mStep[0m  [12/26], [94mLoss[0m : 2.51361
[1mStep[0m  [14/26], [94mLoss[0m : 2.50877
[1mStep[0m  [16/26], [94mLoss[0m : 2.41973
[1mStep[0m  [18/26], [94mLoss[0m : 2.40253
[1mStep[0m  [20/26], [94mLoss[0m : 2.37183
[1mStep[0m  [22/26], [94mLoss[0m : 2.50611
[1mStep[0m  [24/26], [94mLoss[0m : 2.51871

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47151
[1mStep[0m  [2/26], [94mLoss[0m : 2.46941
[1mStep[0m  [4/26], [94mLoss[0m : 2.48535
[1mStep[0m  [6/26], [94mLoss[0m : 2.59316
[1mStep[0m  [8/26], [94mLoss[0m : 2.50777
[1mStep[0m  [10/26], [94mLoss[0m : 2.46318
[1mStep[0m  [12/26], [94mLoss[0m : 2.42707
[1mStep[0m  [14/26], [94mLoss[0m : 2.39691
[1mStep[0m  [16/26], [94mLoss[0m : 2.39420
[1mStep[0m  [18/26], [94mLoss[0m : 2.48603
[1mStep[0m  [20/26], [94mLoss[0m : 2.49689
[1mStep[0m  [22/26], [94mLoss[0m : 2.36146
[1mStep[0m  [24/26], [94mLoss[0m : 2.52193

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46896
[1mStep[0m  [2/26], [94mLoss[0m : 2.43592
[1mStep[0m  [4/26], [94mLoss[0m : 2.45421
[1mStep[0m  [6/26], [94mLoss[0m : 2.40859
[1mStep[0m  [8/26], [94mLoss[0m : 2.39960
[1mStep[0m  [10/26], [94mLoss[0m : 2.36742
[1mStep[0m  [12/26], [94mLoss[0m : 2.43786
[1mStep[0m  [14/26], [94mLoss[0m : 2.41540
[1mStep[0m  [16/26], [94mLoss[0m : 2.44482
[1mStep[0m  [18/26], [94mLoss[0m : 2.38207
[1mStep[0m  [20/26], [94mLoss[0m : 2.45072
[1mStep[0m  [22/26], [94mLoss[0m : 2.36293
[1mStep[0m  [24/26], [94mLoss[0m : 2.48732

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43332
[1mStep[0m  [2/26], [94mLoss[0m : 2.43906
[1mStep[0m  [4/26], [94mLoss[0m : 2.54703
[1mStep[0m  [6/26], [94mLoss[0m : 2.32797
[1mStep[0m  [8/26], [94mLoss[0m : 2.54426
[1mStep[0m  [10/26], [94mLoss[0m : 2.43889
[1mStep[0m  [12/26], [94mLoss[0m : 2.25271
[1mStep[0m  [14/26], [94mLoss[0m : 2.43748
[1mStep[0m  [16/26], [94mLoss[0m : 2.56815
[1mStep[0m  [18/26], [94mLoss[0m : 2.40449
[1mStep[0m  [20/26], [94mLoss[0m : 2.55522
[1mStep[0m  [22/26], [94mLoss[0m : 2.39624
[1mStep[0m  [24/26], [94mLoss[0m : 2.57726

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69214
[1mStep[0m  [2/26], [94mLoss[0m : 2.37719
[1mStep[0m  [4/26], [94mLoss[0m : 2.47524
[1mStep[0m  [6/26], [94mLoss[0m : 2.56562
[1mStep[0m  [8/26], [94mLoss[0m : 2.28694
[1mStep[0m  [10/26], [94mLoss[0m : 2.37210
[1mStep[0m  [12/26], [94mLoss[0m : 2.41520
[1mStep[0m  [14/26], [94mLoss[0m : 2.49439
[1mStep[0m  [16/26], [94mLoss[0m : 2.34469
[1mStep[0m  [18/26], [94mLoss[0m : 2.45468
[1mStep[0m  [20/26], [94mLoss[0m : 2.34967
[1mStep[0m  [22/26], [94mLoss[0m : 2.40304
[1mStep[0m  [24/26], [94mLoss[0m : 2.47664

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46778
[1mStep[0m  [2/26], [94mLoss[0m : 2.39818
[1mStep[0m  [4/26], [94mLoss[0m : 2.54869
[1mStep[0m  [6/26], [94mLoss[0m : 2.40015
[1mStep[0m  [8/26], [94mLoss[0m : 2.58517
[1mStep[0m  [10/26], [94mLoss[0m : 2.35745
[1mStep[0m  [12/26], [94mLoss[0m : 2.41764
[1mStep[0m  [14/26], [94mLoss[0m : 2.51353
[1mStep[0m  [16/26], [94mLoss[0m : 2.31095
[1mStep[0m  [18/26], [94mLoss[0m : 2.42270
[1mStep[0m  [20/26], [94mLoss[0m : 2.49219
[1mStep[0m  [22/26], [94mLoss[0m : 2.35443
[1mStep[0m  [24/26], [94mLoss[0m : 2.52990

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.390, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46494
[1mStep[0m  [2/26], [94mLoss[0m : 2.54895
[1mStep[0m  [4/26], [94mLoss[0m : 2.40648
[1mStep[0m  [6/26], [94mLoss[0m : 2.38559
[1mStep[0m  [8/26], [94mLoss[0m : 2.47184
[1mStep[0m  [10/26], [94mLoss[0m : 2.36338
[1mStep[0m  [12/26], [94mLoss[0m : 2.45303
[1mStep[0m  [14/26], [94mLoss[0m : 2.58412
[1mStep[0m  [16/26], [94mLoss[0m : 2.36068
[1mStep[0m  [18/26], [94mLoss[0m : 2.30555
[1mStep[0m  [20/26], [94mLoss[0m : 2.43597
[1mStep[0m  [22/26], [94mLoss[0m : 2.45003
[1mStep[0m  [24/26], [94mLoss[0m : 2.67466

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38665
[1mStep[0m  [2/26], [94mLoss[0m : 2.48086
[1mStep[0m  [4/26], [94mLoss[0m : 2.49186
[1mStep[0m  [6/26], [94mLoss[0m : 2.47375
[1mStep[0m  [8/26], [94mLoss[0m : 2.43514
[1mStep[0m  [10/26], [94mLoss[0m : 2.57028
[1mStep[0m  [12/26], [94mLoss[0m : 2.38392
[1mStep[0m  [14/26], [94mLoss[0m : 2.35868
[1mStep[0m  [16/26], [94mLoss[0m : 2.37706
[1mStep[0m  [18/26], [94mLoss[0m : 2.56960
[1mStep[0m  [20/26], [94mLoss[0m : 2.47583
[1mStep[0m  [22/26], [94mLoss[0m : 2.46519
[1mStep[0m  [24/26], [94mLoss[0m : 2.22797

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51851
[1mStep[0m  [2/26], [94mLoss[0m : 2.40690
[1mStep[0m  [4/26], [94mLoss[0m : 2.32094
[1mStep[0m  [6/26], [94mLoss[0m : 2.52054
[1mStep[0m  [8/26], [94mLoss[0m : 2.39878
[1mStep[0m  [10/26], [94mLoss[0m : 2.49008
[1mStep[0m  [12/26], [94mLoss[0m : 2.38490
[1mStep[0m  [14/26], [94mLoss[0m : 2.43527
[1mStep[0m  [16/26], [94mLoss[0m : 2.50495
[1mStep[0m  [18/26], [94mLoss[0m : 2.47188
[1mStep[0m  [20/26], [94mLoss[0m : 2.25761
[1mStep[0m  [22/26], [94mLoss[0m : 2.45619
[1mStep[0m  [24/26], [94mLoss[0m : 2.27197

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.401, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49681
[1mStep[0m  [2/26], [94mLoss[0m : 2.44343
[1mStep[0m  [4/26], [94mLoss[0m : 2.41918
[1mStep[0m  [6/26], [94mLoss[0m : 2.44530
[1mStep[0m  [8/26], [94mLoss[0m : 2.44118
[1mStep[0m  [10/26], [94mLoss[0m : 2.53391
[1mStep[0m  [12/26], [94mLoss[0m : 2.36615
[1mStep[0m  [14/26], [94mLoss[0m : 2.39717
[1mStep[0m  [16/26], [94mLoss[0m : 2.51659
[1mStep[0m  [18/26], [94mLoss[0m : 2.40319
[1mStep[0m  [20/26], [94mLoss[0m : 2.46288
[1mStep[0m  [22/26], [94mLoss[0m : 2.57392
[1mStep[0m  [24/26], [94mLoss[0m : 2.46272

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35937
[1mStep[0m  [2/26], [94mLoss[0m : 2.51809
[1mStep[0m  [4/26], [94mLoss[0m : 2.34261
[1mStep[0m  [6/26], [94mLoss[0m : 2.40477
[1mStep[0m  [8/26], [94mLoss[0m : 2.24560
[1mStep[0m  [10/26], [94mLoss[0m : 2.36019
[1mStep[0m  [12/26], [94mLoss[0m : 2.50693
[1mStep[0m  [14/26], [94mLoss[0m : 2.29575
[1mStep[0m  [16/26], [94mLoss[0m : 2.58757
[1mStep[0m  [18/26], [94mLoss[0m : 2.63956
[1mStep[0m  [20/26], [94mLoss[0m : 2.45237
[1mStep[0m  [22/26], [94mLoss[0m : 2.50150
[1mStep[0m  [24/26], [94mLoss[0m : 2.30110

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57297
[1mStep[0m  [2/26], [94mLoss[0m : 2.43781
[1mStep[0m  [4/26], [94mLoss[0m : 2.39995
[1mStep[0m  [6/26], [94mLoss[0m : 2.51788
[1mStep[0m  [8/26], [94mLoss[0m : 2.40855
[1mStep[0m  [10/26], [94mLoss[0m : 2.40818
[1mStep[0m  [12/26], [94mLoss[0m : 2.31229
[1mStep[0m  [14/26], [94mLoss[0m : 2.59208
[1mStep[0m  [16/26], [94mLoss[0m : 2.31556
[1mStep[0m  [18/26], [94mLoss[0m : 2.42785
[1mStep[0m  [20/26], [94mLoss[0m : 2.59149
[1mStep[0m  [22/26], [94mLoss[0m : 2.36186
[1mStep[0m  [24/26], [94mLoss[0m : 2.45288

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.393, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45440
[1mStep[0m  [2/26], [94mLoss[0m : 2.51777
[1mStep[0m  [4/26], [94mLoss[0m : 2.35475
[1mStep[0m  [6/26], [94mLoss[0m : 2.51578
[1mStep[0m  [8/26], [94mLoss[0m : 2.55338
[1mStep[0m  [10/26], [94mLoss[0m : 2.46180
[1mStep[0m  [12/26], [94mLoss[0m : 2.33412
[1mStep[0m  [14/26], [94mLoss[0m : 2.32351
[1mStep[0m  [16/26], [94mLoss[0m : 2.39500
[1mStep[0m  [18/26], [94mLoss[0m : 2.30921
[1mStep[0m  [20/26], [94mLoss[0m : 2.48043
[1mStep[0m  [22/26], [94mLoss[0m : 2.50187
[1mStep[0m  [24/26], [94mLoss[0m : 2.57205

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.391, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31663
[1mStep[0m  [2/26], [94mLoss[0m : 2.47453
[1mStep[0m  [4/26], [94mLoss[0m : 2.46424
[1mStep[0m  [6/26], [94mLoss[0m : 2.39646
[1mStep[0m  [8/26], [94mLoss[0m : 2.29107
[1mStep[0m  [10/26], [94mLoss[0m : 2.33587
[1mStep[0m  [12/26], [94mLoss[0m : 2.48322
[1mStep[0m  [14/26], [94mLoss[0m : 2.48016
[1mStep[0m  [16/26], [94mLoss[0m : 2.46667
[1mStep[0m  [18/26], [94mLoss[0m : 2.47181
[1mStep[0m  [20/26], [94mLoss[0m : 2.43459
[1mStep[0m  [22/26], [94mLoss[0m : 2.30851
[1mStep[0m  [24/26], [94mLoss[0m : 2.48758

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 1 - Evaluation MAE:  2.3972399418170633
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.44330
[1mStep[0m  [2/26], [94mLoss[0m : 2.52811
[1mStep[0m  [4/26], [94mLoss[0m : 2.42955
[1mStep[0m  [6/26], [94mLoss[0m : 2.44440
[1mStep[0m  [8/26], [94mLoss[0m : 2.44829
[1mStep[0m  [10/26], [94mLoss[0m : 2.46964
[1mStep[0m  [12/26], [94mLoss[0m : 2.32821
[1mStep[0m  [14/26], [94mLoss[0m : 2.39656
[1mStep[0m  [16/26], [94mLoss[0m : 2.41640
[1mStep[0m  [18/26], [94mLoss[0m : 2.35320
[1mStep[0m  [20/26], [94mLoss[0m : 2.33582
[1mStep[0m  [22/26], [94mLoss[0m : 2.44421
[1mStep[0m  [24/26], [94mLoss[0m : 2.50383

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60526
[1mStep[0m  [2/26], [94mLoss[0m : 2.37837
[1mStep[0m  [4/26], [94mLoss[0m : 2.31512
[1mStep[0m  [6/26], [94mLoss[0m : 2.30422
[1mStep[0m  [8/26], [94mLoss[0m : 2.38695
[1mStep[0m  [10/26], [94mLoss[0m : 2.31628
[1mStep[0m  [12/26], [94mLoss[0m : 2.45756
[1mStep[0m  [14/26], [94mLoss[0m : 2.54668
[1mStep[0m  [16/26], [94mLoss[0m : 2.49198
[1mStep[0m  [18/26], [94mLoss[0m : 2.27792
[1mStep[0m  [20/26], [94mLoss[0m : 2.33057
[1mStep[0m  [22/26], [94mLoss[0m : 2.37650
[1mStep[0m  [24/26], [94mLoss[0m : 2.41853

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.544, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40512
[1mStep[0m  [2/26], [94mLoss[0m : 2.46306
[1mStep[0m  [4/26], [94mLoss[0m : 2.28941
[1mStep[0m  [6/26], [94mLoss[0m : 2.32065
[1mStep[0m  [8/26], [94mLoss[0m : 2.27966
[1mStep[0m  [10/26], [94mLoss[0m : 2.40534
[1mStep[0m  [12/26], [94mLoss[0m : 2.40325
[1mStep[0m  [14/26], [94mLoss[0m : 2.29454
[1mStep[0m  [16/26], [94mLoss[0m : 2.30215
[1mStep[0m  [18/26], [94mLoss[0m : 2.34383
[1mStep[0m  [20/26], [94mLoss[0m : 2.42461
[1mStep[0m  [22/26], [94mLoss[0m : 2.30740
[1mStep[0m  [24/26], [94mLoss[0m : 2.32226

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35398
[1mStep[0m  [2/26], [94mLoss[0m : 2.30351
[1mStep[0m  [4/26], [94mLoss[0m : 2.33061
[1mStep[0m  [6/26], [94mLoss[0m : 2.14108
[1mStep[0m  [8/26], [94mLoss[0m : 2.28849
[1mStep[0m  [10/26], [94mLoss[0m : 2.15410
[1mStep[0m  [12/26], [94mLoss[0m : 2.27230
[1mStep[0m  [14/26], [94mLoss[0m : 2.35286
[1mStep[0m  [16/26], [94mLoss[0m : 2.27261
[1mStep[0m  [18/26], [94mLoss[0m : 2.23010
[1mStep[0m  [20/26], [94mLoss[0m : 2.27380
[1mStep[0m  [22/26], [94mLoss[0m : 2.19131
[1mStep[0m  [24/26], [94mLoss[0m : 2.30812

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24485
[1mStep[0m  [2/26], [94mLoss[0m : 2.27916
[1mStep[0m  [4/26], [94mLoss[0m : 2.17695
[1mStep[0m  [6/26], [94mLoss[0m : 2.27713
[1mStep[0m  [8/26], [94mLoss[0m : 2.10107
[1mStep[0m  [10/26], [94mLoss[0m : 2.11969
[1mStep[0m  [12/26], [94mLoss[0m : 2.18692
[1mStep[0m  [14/26], [94mLoss[0m : 2.31452
[1mStep[0m  [16/26], [94mLoss[0m : 2.22763
[1mStep[0m  [18/26], [94mLoss[0m : 2.31979
[1mStep[0m  [20/26], [94mLoss[0m : 2.24984
[1mStep[0m  [22/26], [94mLoss[0m : 2.24238
[1mStep[0m  [24/26], [94mLoss[0m : 2.08378

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17058
[1mStep[0m  [2/26], [94mLoss[0m : 2.10337
[1mStep[0m  [4/26], [94mLoss[0m : 2.06807
[1mStep[0m  [6/26], [94mLoss[0m : 2.28147
[1mStep[0m  [8/26], [94mLoss[0m : 2.15399
[1mStep[0m  [10/26], [94mLoss[0m : 2.05916
[1mStep[0m  [12/26], [94mLoss[0m : 2.21375
[1mStep[0m  [14/26], [94mLoss[0m : 2.22436
[1mStep[0m  [16/26], [94mLoss[0m : 2.08919
[1mStep[0m  [18/26], [94mLoss[0m : 2.17449
[1mStep[0m  [20/26], [94mLoss[0m : 2.18780
[1mStep[0m  [22/26], [94mLoss[0m : 2.06412
[1mStep[0m  [24/26], [94mLoss[0m : 2.02030

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19064
[1mStep[0m  [2/26], [94mLoss[0m : 2.05512
[1mStep[0m  [4/26], [94mLoss[0m : 2.09552
[1mStep[0m  [6/26], [94mLoss[0m : 2.02491
[1mStep[0m  [8/26], [94mLoss[0m : 1.97191
[1mStep[0m  [10/26], [94mLoss[0m : 2.05590
[1mStep[0m  [12/26], [94mLoss[0m : 2.07566
[1mStep[0m  [14/26], [94mLoss[0m : 2.05200
[1mStep[0m  [16/26], [94mLoss[0m : 2.09672
[1mStep[0m  [18/26], [94mLoss[0m : 2.09021
[1mStep[0m  [20/26], [94mLoss[0m : 2.06062
[1mStep[0m  [22/26], [94mLoss[0m : 2.11153
[1mStep[0m  [24/26], [94mLoss[0m : 2.10615

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.596, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08633
[1mStep[0m  [2/26], [94mLoss[0m : 2.09101
[1mStep[0m  [4/26], [94mLoss[0m : 2.05072
[1mStep[0m  [6/26], [94mLoss[0m : 2.00809
[1mStep[0m  [8/26], [94mLoss[0m : 2.07329
[1mStep[0m  [10/26], [94mLoss[0m : 1.97555
[1mStep[0m  [12/26], [94mLoss[0m : 2.04529
[1mStep[0m  [14/26], [94mLoss[0m : 2.07894
[1mStep[0m  [16/26], [94mLoss[0m : 1.98237
[1mStep[0m  [18/26], [94mLoss[0m : 2.06123
[1mStep[0m  [20/26], [94mLoss[0m : 2.02993
[1mStep[0m  [22/26], [94mLoss[0m : 2.04745
[1mStep[0m  [24/26], [94mLoss[0m : 2.03412

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03012
[1mStep[0m  [2/26], [94mLoss[0m : 1.95313
[1mStep[0m  [4/26], [94mLoss[0m : 1.88870
[1mStep[0m  [6/26], [94mLoss[0m : 2.01040
[1mStep[0m  [8/26], [94mLoss[0m : 1.96897
[1mStep[0m  [10/26], [94mLoss[0m : 2.07589
[1mStep[0m  [12/26], [94mLoss[0m : 2.10293
[1mStep[0m  [14/26], [94mLoss[0m : 1.95153
[1mStep[0m  [16/26], [94mLoss[0m : 1.95398
[1mStep[0m  [18/26], [94mLoss[0m : 1.92485
[1mStep[0m  [20/26], [94mLoss[0m : 1.88929
[1mStep[0m  [22/26], [94mLoss[0m : 1.93041
[1mStep[0m  [24/26], [94mLoss[0m : 1.93700

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93973
[1mStep[0m  [2/26], [94mLoss[0m : 1.95340
[1mStep[0m  [4/26], [94mLoss[0m : 2.00409
[1mStep[0m  [6/26], [94mLoss[0m : 1.88846
[1mStep[0m  [8/26], [94mLoss[0m : 1.85823
[1mStep[0m  [10/26], [94mLoss[0m : 1.90338
[1mStep[0m  [12/26], [94mLoss[0m : 1.92815
[1mStep[0m  [14/26], [94mLoss[0m : 1.86694
[1mStep[0m  [16/26], [94mLoss[0m : 2.06722
[1mStep[0m  [18/26], [94mLoss[0m : 2.02749
[1mStep[0m  [20/26], [94mLoss[0m : 1.91618
[1mStep[0m  [22/26], [94mLoss[0m : 1.88920
[1mStep[0m  [24/26], [94mLoss[0m : 2.04272

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.85002
[1mStep[0m  [2/26], [94mLoss[0m : 1.89532
[1mStep[0m  [4/26], [94mLoss[0m : 2.02465
[1mStep[0m  [6/26], [94mLoss[0m : 1.91836
[1mStep[0m  [8/26], [94mLoss[0m : 1.88989
[1mStep[0m  [10/26], [94mLoss[0m : 1.87669
[1mStep[0m  [12/26], [94mLoss[0m : 1.92446
[1mStep[0m  [14/26], [94mLoss[0m : 1.93242
[1mStep[0m  [16/26], [94mLoss[0m : 1.88154
[1mStep[0m  [18/26], [94mLoss[0m : 1.85930
[1mStep[0m  [20/26], [94mLoss[0m : 1.93186
[1mStep[0m  [22/26], [94mLoss[0m : 1.88923
[1mStep[0m  [24/26], [94mLoss[0m : 1.78064

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.904, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73266
[1mStep[0m  [2/26], [94mLoss[0m : 1.83681
[1mStep[0m  [4/26], [94mLoss[0m : 1.84465
[1mStep[0m  [6/26], [94mLoss[0m : 1.70816
[1mStep[0m  [8/26], [94mLoss[0m : 1.88017
[1mStep[0m  [10/26], [94mLoss[0m : 1.87697
[1mStep[0m  [12/26], [94mLoss[0m : 1.81115
[1mStep[0m  [14/26], [94mLoss[0m : 1.99355
[1mStep[0m  [16/26], [94mLoss[0m : 1.78095
[1mStep[0m  [18/26], [94mLoss[0m : 1.91777
[1mStep[0m  [20/26], [94mLoss[0m : 1.90544
[1mStep[0m  [22/26], [94mLoss[0m : 1.89510
[1mStep[0m  [24/26], [94mLoss[0m : 1.99434

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61569
[1mStep[0m  [2/26], [94mLoss[0m : 1.75301
[1mStep[0m  [4/26], [94mLoss[0m : 1.71377
[1mStep[0m  [6/26], [94mLoss[0m : 1.75764
[1mStep[0m  [8/26], [94mLoss[0m : 1.72040
[1mStep[0m  [10/26], [94mLoss[0m : 1.83329
[1mStep[0m  [12/26], [94mLoss[0m : 1.76033
[1mStep[0m  [14/26], [94mLoss[0m : 1.83552
[1mStep[0m  [16/26], [94mLoss[0m : 1.72114
[1mStep[0m  [18/26], [94mLoss[0m : 1.79116
[1mStep[0m  [20/26], [94mLoss[0m : 1.95171
[1mStep[0m  [22/26], [94mLoss[0m : 1.69920
[1mStep[0m  [24/26], [94mLoss[0m : 1.70350

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.780, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.70039
[1mStep[0m  [2/26], [94mLoss[0m : 1.78253
[1mStep[0m  [4/26], [94mLoss[0m : 1.77116
[1mStep[0m  [6/26], [94mLoss[0m : 1.85945
[1mStep[0m  [8/26], [94mLoss[0m : 1.76678
[1mStep[0m  [10/26], [94mLoss[0m : 1.68204
[1mStep[0m  [12/26], [94mLoss[0m : 1.70568
[1mStep[0m  [14/26], [94mLoss[0m : 1.72687
[1mStep[0m  [16/26], [94mLoss[0m : 1.86823
[1mStep[0m  [18/26], [94mLoss[0m : 1.70874
[1mStep[0m  [20/26], [94mLoss[0m : 1.75860
[1mStep[0m  [22/26], [94mLoss[0m : 1.82995
[1mStep[0m  [24/26], [94mLoss[0m : 1.74713

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.751, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.75513
[1mStep[0m  [2/26], [94mLoss[0m : 1.70702
[1mStep[0m  [4/26], [94mLoss[0m : 1.62100
[1mStep[0m  [6/26], [94mLoss[0m : 1.67863
[1mStep[0m  [8/26], [94mLoss[0m : 1.79160
[1mStep[0m  [10/26], [94mLoss[0m : 1.73111
[1mStep[0m  [12/26], [94mLoss[0m : 1.63171
[1mStep[0m  [14/26], [94mLoss[0m : 1.71617
[1mStep[0m  [16/26], [94mLoss[0m : 1.58474
[1mStep[0m  [18/26], [94mLoss[0m : 1.72914
[1mStep[0m  [20/26], [94mLoss[0m : 1.80602
[1mStep[0m  [22/26], [94mLoss[0m : 1.81244
[1mStep[0m  [24/26], [94mLoss[0m : 1.78242

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.59381
[1mStep[0m  [2/26], [94mLoss[0m : 1.70479
[1mStep[0m  [4/26], [94mLoss[0m : 1.70244
[1mStep[0m  [6/26], [94mLoss[0m : 1.66840
[1mStep[0m  [8/26], [94mLoss[0m : 1.74441
[1mStep[0m  [10/26], [94mLoss[0m : 1.70696
[1mStep[0m  [12/26], [94mLoss[0m : 1.73537
[1mStep[0m  [14/26], [94mLoss[0m : 1.62378
[1mStep[0m  [16/26], [94mLoss[0m : 1.65806
[1mStep[0m  [18/26], [94mLoss[0m : 1.62534
[1mStep[0m  [20/26], [94mLoss[0m : 1.72487
[1mStep[0m  [22/26], [94mLoss[0m : 1.75206
[1mStep[0m  [24/26], [94mLoss[0m : 1.81748

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.57760
[1mStep[0m  [2/26], [94mLoss[0m : 1.58601
[1mStep[0m  [4/26], [94mLoss[0m : 1.70883
[1mStep[0m  [6/26], [94mLoss[0m : 1.69676
[1mStep[0m  [8/26], [94mLoss[0m : 1.54633
[1mStep[0m  [10/26], [94mLoss[0m : 1.56077
[1mStep[0m  [12/26], [94mLoss[0m : 1.72955
[1mStep[0m  [14/26], [94mLoss[0m : 1.73609
[1mStep[0m  [16/26], [94mLoss[0m : 1.71192
[1mStep[0m  [18/26], [94mLoss[0m : 1.77953
[1mStep[0m  [20/26], [94mLoss[0m : 1.67836
[1mStep[0m  [22/26], [94mLoss[0m : 1.65098
[1mStep[0m  [24/26], [94mLoss[0m : 1.68202

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.62469
[1mStep[0m  [2/26], [94mLoss[0m : 1.74622
[1mStep[0m  [4/26], [94mLoss[0m : 1.63450
[1mStep[0m  [6/26], [94mLoss[0m : 1.58343
[1mStep[0m  [8/26], [94mLoss[0m : 1.59901
[1mStep[0m  [10/26], [94mLoss[0m : 1.57246
[1mStep[0m  [12/26], [94mLoss[0m : 1.52653
[1mStep[0m  [14/26], [94mLoss[0m : 1.75093
[1mStep[0m  [16/26], [94mLoss[0m : 1.62620
[1mStep[0m  [18/26], [94mLoss[0m : 1.66115
[1mStep[0m  [20/26], [94mLoss[0m : 1.54775
[1mStep[0m  [22/26], [94mLoss[0m : 1.80766
[1mStep[0m  [24/26], [94mLoss[0m : 1.65980

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.55847
[1mStep[0m  [2/26], [94mLoss[0m : 1.59488
[1mStep[0m  [4/26], [94mLoss[0m : 1.54437
[1mStep[0m  [6/26], [94mLoss[0m : 1.54588
[1mStep[0m  [8/26], [94mLoss[0m : 1.68470
[1mStep[0m  [10/26], [94mLoss[0m : 1.62860
[1mStep[0m  [12/26], [94mLoss[0m : 1.61930
[1mStep[0m  [14/26], [94mLoss[0m : 1.70054
[1mStep[0m  [16/26], [94mLoss[0m : 1.46189
[1mStep[0m  [18/26], [94mLoss[0m : 1.67435
[1mStep[0m  [20/26], [94mLoss[0m : 1.65313
[1mStep[0m  [22/26], [94mLoss[0m : 1.76307
[1mStep[0m  [24/26], [94mLoss[0m : 1.64039

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.69982
[1mStep[0m  [2/26], [94mLoss[0m : 1.60957
[1mStep[0m  [4/26], [94mLoss[0m : 1.52554
[1mStep[0m  [6/26], [94mLoss[0m : 1.64897
[1mStep[0m  [8/26], [94mLoss[0m : 1.62928
[1mStep[0m  [10/26], [94mLoss[0m : 1.53893
[1mStep[0m  [12/26], [94mLoss[0m : 1.66438
[1mStep[0m  [14/26], [94mLoss[0m : 1.63079
[1mStep[0m  [16/26], [94mLoss[0m : 1.59787
[1mStep[0m  [18/26], [94mLoss[0m : 1.63336
[1mStep[0m  [20/26], [94mLoss[0m : 1.62855
[1mStep[0m  [22/26], [94mLoss[0m : 1.61011
[1mStep[0m  [24/26], [94mLoss[0m : 1.63311

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.59733
[1mStep[0m  [2/26], [94mLoss[0m : 1.47700
[1mStep[0m  [4/26], [94mLoss[0m : 1.53846
[1mStep[0m  [6/26], [94mLoss[0m : 1.60863
[1mStep[0m  [8/26], [94mLoss[0m : 1.50489
[1mStep[0m  [10/26], [94mLoss[0m : 1.49539
[1mStep[0m  [12/26], [94mLoss[0m : 1.58550
[1mStep[0m  [14/26], [94mLoss[0m : 1.57731
[1mStep[0m  [16/26], [94mLoss[0m : 1.49350
[1mStep[0m  [18/26], [94mLoss[0m : 1.70140
[1mStep[0m  [20/26], [94mLoss[0m : 1.71422
[1mStep[0m  [22/26], [94mLoss[0m : 1.54564
[1mStep[0m  [24/26], [94mLoss[0m : 1.46058

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.50466
[1mStep[0m  [2/26], [94mLoss[0m : 1.51246
[1mStep[0m  [4/26], [94mLoss[0m : 1.43853
[1mStep[0m  [6/26], [94mLoss[0m : 1.38512
[1mStep[0m  [8/26], [94mLoss[0m : 1.44315
[1mStep[0m  [10/26], [94mLoss[0m : 1.54111
[1mStep[0m  [12/26], [94mLoss[0m : 1.55517
[1mStep[0m  [14/26], [94mLoss[0m : 1.52592
[1mStep[0m  [16/26], [94mLoss[0m : 1.50693
[1mStep[0m  [18/26], [94mLoss[0m : 1.44823
[1mStep[0m  [20/26], [94mLoss[0m : 1.55938
[1mStep[0m  [22/26], [94mLoss[0m : 1.57414
[1mStep[0m  [24/26], [94mLoss[0m : 1.52954

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.500, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49799
[1mStep[0m  [2/26], [94mLoss[0m : 1.56472
[1mStep[0m  [4/26], [94mLoss[0m : 1.50764
[1mStep[0m  [6/26], [94mLoss[0m : 1.54952
[1mStep[0m  [8/26], [94mLoss[0m : 1.31088
[1mStep[0m  [10/26], [94mLoss[0m : 1.49441
[1mStep[0m  [12/26], [94mLoss[0m : 1.43921
[1mStep[0m  [14/26], [94mLoss[0m : 1.51761
[1mStep[0m  [16/26], [94mLoss[0m : 1.44336
[1mStep[0m  [18/26], [94mLoss[0m : 1.57691
[1mStep[0m  [20/26], [94mLoss[0m : 1.59156
[1mStep[0m  [22/26], [94mLoss[0m : 1.57285
[1mStep[0m  [24/26], [94mLoss[0m : 1.62637

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.509, [92mTest[0m: 2.532, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.51156
[1mStep[0m  [2/26], [94mLoss[0m : 1.47843
[1mStep[0m  [4/26], [94mLoss[0m : 1.49442
[1mStep[0m  [6/26], [94mLoss[0m : 1.48402
[1mStep[0m  [8/26], [94mLoss[0m : 1.45905
[1mStep[0m  [10/26], [94mLoss[0m : 1.35963
[1mStep[0m  [12/26], [94mLoss[0m : 1.64186
[1mStep[0m  [14/26], [94mLoss[0m : 1.41815
[1mStep[0m  [16/26], [94mLoss[0m : 1.64379
[1mStep[0m  [18/26], [94mLoss[0m : 1.43480
[1mStep[0m  [20/26], [94mLoss[0m : 1.54724
[1mStep[0m  [22/26], [94mLoss[0m : 1.36570
[1mStep[0m  [24/26], [94mLoss[0m : 1.45893

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.509, [92mTest[0m: 2.524, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.55325
[1mStep[0m  [2/26], [94mLoss[0m : 1.26955
[1mStep[0m  [4/26], [94mLoss[0m : 1.40772
[1mStep[0m  [6/26], [94mLoss[0m : 1.47318
[1mStep[0m  [8/26], [94mLoss[0m : 1.39102
[1mStep[0m  [10/26], [94mLoss[0m : 1.61598
[1mStep[0m  [12/26], [94mLoss[0m : 1.55505
[1mStep[0m  [14/26], [94mLoss[0m : 1.55663
[1mStep[0m  [16/26], [94mLoss[0m : 1.53871
[1mStep[0m  [18/26], [94mLoss[0m : 1.47713
[1mStep[0m  [20/26], [94mLoss[0m : 1.58034
[1mStep[0m  [22/26], [94mLoss[0m : 1.49913
[1mStep[0m  [24/26], [94mLoss[0m : 1.51294

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.39968
[1mStep[0m  [2/26], [94mLoss[0m : 1.45997
[1mStep[0m  [4/26], [94mLoss[0m : 1.44400
[1mStep[0m  [6/26], [94mLoss[0m : 1.51237
[1mStep[0m  [8/26], [94mLoss[0m : 1.27358
[1mStep[0m  [10/26], [94mLoss[0m : 1.44732
[1mStep[0m  [12/26], [94mLoss[0m : 1.43251
[1mStep[0m  [14/26], [94mLoss[0m : 1.58711
[1mStep[0m  [16/26], [94mLoss[0m : 1.44489
[1mStep[0m  [18/26], [94mLoss[0m : 1.50546
[1mStep[0m  [20/26], [94mLoss[0m : 1.50303
[1mStep[0m  [22/26], [94mLoss[0m : 1.42293
[1mStep[0m  [24/26], [94mLoss[0m : 1.50900

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.454, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.491
====================================

Phase 2 - Evaluation MAE:  2.491334218245286
MAE score P1       2.39724
MAE score P2      2.491334
loss              1.453871
learning_rate      0.00505
batch_size             512
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/427], [94mLoss[0m : 11.31784
[1mStep[0m  [42/427], [94mLoss[0m : 9.78072
[1mStep[0m  [84/427], [94mLoss[0m : 10.06935
[1mStep[0m  [126/427], [94mLoss[0m : 10.32267
[1mStep[0m  [168/427], [94mLoss[0m : 8.76281
[1mStep[0m  [210/427], [94mLoss[0m : 8.82139
[1mStep[0m  [252/427], [94mLoss[0m : 8.37985
[1mStep[0m  [294/427], [94mLoss[0m : 6.36341
[1mStep[0m  [336/427], [94mLoss[0m : 5.96477
[1mStep[0m  [378/427], [94mLoss[0m : 4.93628
[1mStep[0m  [420/427], [94mLoss[0m : 4.66939

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.030, [92mTest[0m: 10.857, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.46154
[1mStep[0m  [42/427], [94mLoss[0m : 3.58149
[1mStep[0m  [84/427], [94mLoss[0m : 2.70009
[1mStep[0m  [126/427], [94mLoss[0m : 2.71607
[1mStep[0m  [168/427], [94mLoss[0m : 3.10902
[1mStep[0m  [210/427], [94mLoss[0m : 2.75806
[1mStep[0m  [252/427], [94mLoss[0m : 2.42220
[1mStep[0m  [294/427], [94mLoss[0m : 2.31319
[1mStep[0m  [336/427], [94mLoss[0m : 3.55218
[1mStep[0m  [378/427], [94mLoss[0m : 3.52028
[1mStep[0m  [420/427], [94mLoss[0m : 2.45847

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.022, [92mTest[0m: 3.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.95355
[1mStep[0m  [42/427], [94mLoss[0m : 2.80660
[1mStep[0m  [84/427], [94mLoss[0m : 2.68166
[1mStep[0m  [126/427], [94mLoss[0m : 3.07251
[1mStep[0m  [168/427], [94mLoss[0m : 3.11353
[1mStep[0m  [210/427], [94mLoss[0m : 3.07036
[1mStep[0m  [252/427], [94mLoss[0m : 3.71514
[1mStep[0m  [294/427], [94mLoss[0m : 2.54386
[1mStep[0m  [336/427], [94mLoss[0m : 2.37012
[1mStep[0m  [378/427], [94mLoss[0m : 3.08638
[1mStep[0m  [420/427], [94mLoss[0m : 2.89850

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.810, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32322
[1mStep[0m  [42/427], [94mLoss[0m : 1.85478
[1mStep[0m  [84/427], [94mLoss[0m : 2.88847
[1mStep[0m  [126/427], [94mLoss[0m : 2.75256
[1mStep[0m  [168/427], [94mLoss[0m : 2.52882
[1mStep[0m  [210/427], [94mLoss[0m : 2.34399
[1mStep[0m  [252/427], [94mLoss[0m : 3.01456
[1mStep[0m  [294/427], [94mLoss[0m : 2.64801
[1mStep[0m  [336/427], [94mLoss[0m : 2.86799
[1mStep[0m  [378/427], [94mLoss[0m : 2.69851
[1mStep[0m  [420/427], [94mLoss[0m : 2.78157

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.45133
[1mStep[0m  [42/427], [94mLoss[0m : 2.94165
[1mStep[0m  [84/427], [94mLoss[0m : 2.76406
[1mStep[0m  [126/427], [94mLoss[0m : 2.65438
[1mStep[0m  [168/427], [94mLoss[0m : 2.04642
[1mStep[0m  [210/427], [94mLoss[0m : 2.59651
[1mStep[0m  [252/427], [94mLoss[0m : 2.18308
[1mStep[0m  [294/427], [94mLoss[0m : 2.63554
[1mStep[0m  [336/427], [94mLoss[0m : 2.87429
[1mStep[0m  [378/427], [94mLoss[0m : 2.23131
[1mStep[0m  [420/427], [94mLoss[0m : 3.01152

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.36697
[1mStep[0m  [42/427], [94mLoss[0m : 2.15162
[1mStep[0m  [84/427], [94mLoss[0m : 2.35129
[1mStep[0m  [126/427], [94mLoss[0m : 2.55288
[1mStep[0m  [168/427], [94mLoss[0m : 2.96405
[1mStep[0m  [210/427], [94mLoss[0m : 2.89610
[1mStep[0m  [252/427], [94mLoss[0m : 3.17196
[1mStep[0m  [294/427], [94mLoss[0m : 2.54064
[1mStep[0m  [336/427], [94mLoss[0m : 3.33828
[1mStep[0m  [378/427], [94mLoss[0m : 2.56370
[1mStep[0m  [420/427], [94mLoss[0m : 2.63652

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.16460
[1mStep[0m  [42/427], [94mLoss[0m : 3.18495
[1mStep[0m  [84/427], [94mLoss[0m : 2.29391
[1mStep[0m  [126/427], [94mLoss[0m : 2.49059
[1mStep[0m  [168/427], [94mLoss[0m : 3.14807
[1mStep[0m  [210/427], [94mLoss[0m : 2.36030
[1mStep[0m  [252/427], [94mLoss[0m : 2.71070
[1mStep[0m  [294/427], [94mLoss[0m : 3.37644
[1mStep[0m  [336/427], [94mLoss[0m : 2.72971
[1mStep[0m  [378/427], [94mLoss[0m : 2.20389
[1mStep[0m  [420/427], [94mLoss[0m : 2.47243

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.85364
[1mStep[0m  [42/427], [94mLoss[0m : 2.55831
[1mStep[0m  [84/427], [94mLoss[0m : 2.27642
[1mStep[0m  [126/427], [94mLoss[0m : 2.77565
[1mStep[0m  [168/427], [94mLoss[0m : 2.28169
[1mStep[0m  [210/427], [94mLoss[0m : 3.17213
[1mStep[0m  [252/427], [94mLoss[0m : 2.50494
[1mStep[0m  [294/427], [94mLoss[0m : 2.83626
[1mStep[0m  [336/427], [94mLoss[0m : 3.01146
[1mStep[0m  [378/427], [94mLoss[0m : 2.40060
[1mStep[0m  [420/427], [94mLoss[0m : 2.44511

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.33223
[1mStep[0m  [42/427], [94mLoss[0m : 2.55654
[1mStep[0m  [84/427], [94mLoss[0m : 2.34303
[1mStep[0m  [126/427], [94mLoss[0m : 2.73812
[1mStep[0m  [168/427], [94mLoss[0m : 2.00787
[1mStep[0m  [210/427], [94mLoss[0m : 1.77591
[1mStep[0m  [252/427], [94mLoss[0m : 2.27519
[1mStep[0m  [294/427], [94mLoss[0m : 2.57841
[1mStep[0m  [336/427], [94mLoss[0m : 2.93270
[1mStep[0m  [378/427], [94mLoss[0m : 2.18474
[1mStep[0m  [420/427], [94mLoss[0m : 2.62217

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.92885
[1mStep[0m  [42/427], [94mLoss[0m : 2.56164
[1mStep[0m  [84/427], [94mLoss[0m : 2.89119
[1mStep[0m  [126/427], [94mLoss[0m : 2.37289
[1mStep[0m  [168/427], [94mLoss[0m : 2.32477
[1mStep[0m  [210/427], [94mLoss[0m : 2.63818
[1mStep[0m  [252/427], [94mLoss[0m : 2.64035
[1mStep[0m  [294/427], [94mLoss[0m : 2.28308
[1mStep[0m  [336/427], [94mLoss[0m : 2.21180
[1mStep[0m  [378/427], [94mLoss[0m : 2.73803
[1mStep[0m  [420/427], [94mLoss[0m : 2.32347

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.05655
[1mStep[0m  [42/427], [94mLoss[0m : 2.66901
[1mStep[0m  [84/427], [94mLoss[0m : 2.39665
[1mStep[0m  [126/427], [94mLoss[0m : 2.34767
[1mStep[0m  [168/427], [94mLoss[0m : 2.18547
[1mStep[0m  [210/427], [94mLoss[0m : 2.33235
[1mStep[0m  [252/427], [94mLoss[0m : 2.41495
[1mStep[0m  [294/427], [94mLoss[0m : 1.92521
[1mStep[0m  [336/427], [94mLoss[0m : 2.01975
[1mStep[0m  [378/427], [94mLoss[0m : 2.92416
[1mStep[0m  [420/427], [94mLoss[0m : 3.21101

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.55448
[1mStep[0m  [42/427], [94mLoss[0m : 2.29045
[1mStep[0m  [84/427], [94mLoss[0m : 2.55138
[1mStep[0m  [126/427], [94mLoss[0m : 2.48697
[1mStep[0m  [168/427], [94mLoss[0m : 2.24893
[1mStep[0m  [210/427], [94mLoss[0m : 2.54712
[1mStep[0m  [252/427], [94mLoss[0m : 2.71423
[1mStep[0m  [294/427], [94mLoss[0m : 2.15193
[1mStep[0m  [336/427], [94mLoss[0m : 3.08602
[1mStep[0m  [378/427], [94mLoss[0m : 2.13824
[1mStep[0m  [420/427], [94mLoss[0m : 3.27418

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.87546
[1mStep[0m  [42/427], [94mLoss[0m : 2.84270
[1mStep[0m  [84/427], [94mLoss[0m : 2.69080
[1mStep[0m  [126/427], [94mLoss[0m : 2.54222
[1mStep[0m  [168/427], [94mLoss[0m : 1.89538
[1mStep[0m  [210/427], [94mLoss[0m : 2.43372
[1mStep[0m  [252/427], [94mLoss[0m : 2.62363
[1mStep[0m  [294/427], [94mLoss[0m : 2.00874
[1mStep[0m  [336/427], [94mLoss[0m : 2.04181
[1mStep[0m  [378/427], [94mLoss[0m : 2.36492
[1mStep[0m  [420/427], [94mLoss[0m : 2.51031

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.31840
[1mStep[0m  [42/427], [94mLoss[0m : 2.43763
[1mStep[0m  [84/427], [94mLoss[0m : 2.23855
[1mStep[0m  [126/427], [94mLoss[0m : 3.04857
[1mStep[0m  [168/427], [94mLoss[0m : 2.33237
[1mStep[0m  [210/427], [94mLoss[0m : 2.15766
[1mStep[0m  [252/427], [94mLoss[0m : 1.88783
[1mStep[0m  [294/427], [94mLoss[0m : 3.14169
[1mStep[0m  [336/427], [94mLoss[0m : 2.46972
[1mStep[0m  [378/427], [94mLoss[0m : 2.50784
[1mStep[0m  [420/427], [94mLoss[0m : 3.19050

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.67584
[1mStep[0m  [42/427], [94mLoss[0m : 3.09873
[1mStep[0m  [84/427], [94mLoss[0m : 2.07499
[1mStep[0m  [126/427], [94mLoss[0m : 3.01710
[1mStep[0m  [168/427], [94mLoss[0m : 3.07466
[1mStep[0m  [210/427], [94mLoss[0m : 2.56442
[1mStep[0m  [252/427], [94mLoss[0m : 3.14143
[1mStep[0m  [294/427], [94mLoss[0m : 2.74997
[1mStep[0m  [336/427], [94mLoss[0m : 2.44778
[1mStep[0m  [378/427], [94mLoss[0m : 2.65642
[1mStep[0m  [420/427], [94mLoss[0m : 2.95936

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.84657
[1mStep[0m  [42/427], [94mLoss[0m : 2.26651
[1mStep[0m  [84/427], [94mLoss[0m : 1.90052
[1mStep[0m  [126/427], [94mLoss[0m : 2.79064
[1mStep[0m  [168/427], [94mLoss[0m : 2.51406
[1mStep[0m  [210/427], [94mLoss[0m : 2.97942
[1mStep[0m  [252/427], [94mLoss[0m : 2.14222
[1mStep[0m  [294/427], [94mLoss[0m : 3.03536
[1mStep[0m  [336/427], [94mLoss[0m : 2.64838
[1mStep[0m  [378/427], [94mLoss[0m : 2.39208
[1mStep[0m  [420/427], [94mLoss[0m : 2.32152

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.41717
[1mStep[0m  [42/427], [94mLoss[0m : 2.24490
[1mStep[0m  [84/427], [94mLoss[0m : 2.82902
[1mStep[0m  [126/427], [94mLoss[0m : 2.19620
[1mStep[0m  [168/427], [94mLoss[0m : 2.13346
[1mStep[0m  [210/427], [94mLoss[0m : 2.19818
[1mStep[0m  [252/427], [94mLoss[0m : 2.47561
[1mStep[0m  [294/427], [94mLoss[0m : 2.14675
[1mStep[0m  [336/427], [94mLoss[0m : 2.29945
[1mStep[0m  [378/427], [94mLoss[0m : 3.03992
[1mStep[0m  [420/427], [94mLoss[0m : 2.36597

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.31793
[1mStep[0m  [42/427], [94mLoss[0m : 1.79732
[1mStep[0m  [84/427], [94mLoss[0m : 2.29340
[1mStep[0m  [126/427], [94mLoss[0m : 2.53904
[1mStep[0m  [168/427], [94mLoss[0m : 2.61814
[1mStep[0m  [210/427], [94mLoss[0m : 1.83368
[1mStep[0m  [252/427], [94mLoss[0m : 2.40606
[1mStep[0m  [294/427], [94mLoss[0m : 2.23490
[1mStep[0m  [336/427], [94mLoss[0m : 2.85671
[1mStep[0m  [378/427], [94mLoss[0m : 2.42138
[1mStep[0m  [420/427], [94mLoss[0m : 2.20813

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.19145
[1mStep[0m  [42/427], [94mLoss[0m : 2.69628
[1mStep[0m  [84/427], [94mLoss[0m : 1.88773
[1mStep[0m  [126/427], [94mLoss[0m : 2.21278
[1mStep[0m  [168/427], [94mLoss[0m : 2.71325
[1mStep[0m  [210/427], [94mLoss[0m : 2.50953
[1mStep[0m  [252/427], [94mLoss[0m : 3.19564
[1mStep[0m  [294/427], [94mLoss[0m : 2.03289
[1mStep[0m  [336/427], [94mLoss[0m : 2.19145
[1mStep[0m  [378/427], [94mLoss[0m : 2.31231
[1mStep[0m  [420/427], [94mLoss[0m : 2.35392

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.17550
[1mStep[0m  [42/427], [94mLoss[0m : 2.74819
[1mStep[0m  [84/427], [94mLoss[0m : 2.49449
[1mStep[0m  [126/427], [94mLoss[0m : 2.10262
[1mStep[0m  [168/427], [94mLoss[0m : 2.25305
[1mStep[0m  [210/427], [94mLoss[0m : 2.37227
[1mStep[0m  [252/427], [94mLoss[0m : 2.64605
[1mStep[0m  [294/427], [94mLoss[0m : 2.72746
[1mStep[0m  [336/427], [94mLoss[0m : 2.40303
[1mStep[0m  [378/427], [94mLoss[0m : 2.26805
[1mStep[0m  [420/427], [94mLoss[0m : 2.33426

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.390, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.35163
[1mStep[0m  [42/427], [94mLoss[0m : 2.47108
[1mStep[0m  [84/427], [94mLoss[0m : 2.97230
[1mStep[0m  [126/427], [94mLoss[0m : 2.25372
[1mStep[0m  [168/427], [94mLoss[0m : 2.22482
[1mStep[0m  [210/427], [94mLoss[0m : 2.36234
[1mStep[0m  [252/427], [94mLoss[0m : 2.90690
[1mStep[0m  [294/427], [94mLoss[0m : 1.97864
[1mStep[0m  [336/427], [94mLoss[0m : 2.54203
[1mStep[0m  [378/427], [94mLoss[0m : 2.31514
[1mStep[0m  [420/427], [94mLoss[0m : 2.30335

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.394, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.43916
[1mStep[0m  [42/427], [94mLoss[0m : 2.28833
[1mStep[0m  [84/427], [94mLoss[0m : 2.28754
[1mStep[0m  [126/427], [94mLoss[0m : 2.96284
[1mStep[0m  [168/427], [94mLoss[0m : 2.92581
[1mStep[0m  [210/427], [94mLoss[0m : 2.46973
[1mStep[0m  [252/427], [94mLoss[0m : 2.26363
[1mStep[0m  [294/427], [94mLoss[0m : 2.69597
[1mStep[0m  [336/427], [94mLoss[0m : 2.59838
[1mStep[0m  [378/427], [94mLoss[0m : 2.27512
[1mStep[0m  [420/427], [94mLoss[0m : 2.55208

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.45555
[1mStep[0m  [42/427], [94mLoss[0m : 2.39998
[1mStep[0m  [84/427], [94mLoss[0m : 2.57528
[1mStep[0m  [126/427], [94mLoss[0m : 1.87268
[1mStep[0m  [168/427], [94mLoss[0m : 1.93307
[1mStep[0m  [210/427], [94mLoss[0m : 2.09643
[1mStep[0m  [252/427], [94mLoss[0m : 2.07881
[1mStep[0m  [294/427], [94mLoss[0m : 2.62782
[1mStep[0m  [336/427], [94mLoss[0m : 2.32809
[1mStep[0m  [378/427], [94mLoss[0m : 2.13627
[1mStep[0m  [420/427], [94mLoss[0m : 2.28961

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.413, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.47286
[1mStep[0m  [42/427], [94mLoss[0m : 2.85783
[1mStep[0m  [84/427], [94mLoss[0m : 2.45780
[1mStep[0m  [126/427], [94mLoss[0m : 2.16694
[1mStep[0m  [168/427], [94mLoss[0m : 2.20964
[1mStep[0m  [210/427], [94mLoss[0m : 2.20116
[1mStep[0m  [252/427], [94mLoss[0m : 2.25359
[1mStep[0m  [294/427], [94mLoss[0m : 2.29605
[1mStep[0m  [336/427], [94mLoss[0m : 2.64817
[1mStep[0m  [378/427], [94mLoss[0m : 2.18545
[1mStep[0m  [420/427], [94mLoss[0m : 2.76996

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.387, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.88889
[1mStep[0m  [42/427], [94mLoss[0m : 2.41279
[1mStep[0m  [84/427], [94mLoss[0m : 2.89965
[1mStep[0m  [126/427], [94mLoss[0m : 3.24042
[1mStep[0m  [168/427], [94mLoss[0m : 2.07423
[1mStep[0m  [210/427], [94mLoss[0m : 3.01222
[1mStep[0m  [252/427], [94mLoss[0m : 2.50802
[1mStep[0m  [294/427], [94mLoss[0m : 2.15053
[1mStep[0m  [336/427], [94mLoss[0m : 3.21563
[1mStep[0m  [378/427], [94mLoss[0m : 2.99372
[1mStep[0m  [420/427], [94mLoss[0m : 2.34337

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.388, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.69647
[1mStep[0m  [42/427], [94mLoss[0m : 2.36452
[1mStep[0m  [84/427], [94mLoss[0m : 2.23635
[1mStep[0m  [126/427], [94mLoss[0m : 2.32027
[1mStep[0m  [168/427], [94mLoss[0m : 1.71918
[1mStep[0m  [210/427], [94mLoss[0m : 1.73692
[1mStep[0m  [252/427], [94mLoss[0m : 2.50305
[1mStep[0m  [294/427], [94mLoss[0m : 1.72580
[1mStep[0m  [336/427], [94mLoss[0m : 1.84564
[1mStep[0m  [378/427], [94mLoss[0m : 2.01383
[1mStep[0m  [420/427], [94mLoss[0m : 2.43817

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.39252
[1mStep[0m  [42/427], [94mLoss[0m : 2.79605
[1mStep[0m  [84/427], [94mLoss[0m : 1.88512
[1mStep[0m  [126/427], [94mLoss[0m : 2.82447
[1mStep[0m  [168/427], [94mLoss[0m : 3.25979
[1mStep[0m  [210/427], [94mLoss[0m : 2.34006
[1mStep[0m  [252/427], [94mLoss[0m : 2.45813
[1mStep[0m  [294/427], [94mLoss[0m : 2.45468
[1mStep[0m  [336/427], [94mLoss[0m : 2.34944
[1mStep[0m  [378/427], [94mLoss[0m : 2.36532
[1mStep[0m  [420/427], [94mLoss[0m : 2.19853

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.392, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.91783
[1mStep[0m  [42/427], [94mLoss[0m : 2.48158
[1mStep[0m  [84/427], [94mLoss[0m : 2.85127
[1mStep[0m  [126/427], [94mLoss[0m : 2.46120
[1mStep[0m  [168/427], [94mLoss[0m : 2.25675
[1mStep[0m  [210/427], [94mLoss[0m : 2.28208
[1mStep[0m  [252/427], [94mLoss[0m : 2.15017
[1mStep[0m  [294/427], [94mLoss[0m : 2.03801
[1mStep[0m  [336/427], [94mLoss[0m : 2.30494
[1mStep[0m  [378/427], [94mLoss[0m : 2.39713
[1mStep[0m  [420/427], [94mLoss[0m : 2.54942

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.380, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.02740
[1mStep[0m  [42/427], [94mLoss[0m : 2.33500
[1mStep[0m  [84/427], [94mLoss[0m : 2.62744
[1mStep[0m  [126/427], [94mLoss[0m : 2.24991
[1mStep[0m  [168/427], [94mLoss[0m : 2.06608
[1mStep[0m  [210/427], [94mLoss[0m : 2.07137
[1mStep[0m  [252/427], [94mLoss[0m : 2.42965
[1mStep[0m  [294/427], [94mLoss[0m : 2.14517
[1mStep[0m  [336/427], [94mLoss[0m : 1.93551
[1mStep[0m  [378/427], [94mLoss[0m : 2.97458
[1mStep[0m  [420/427], [94mLoss[0m : 2.06797

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.393, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.08659
[1mStep[0m  [42/427], [94mLoss[0m : 2.08915
[1mStep[0m  [84/427], [94mLoss[0m : 2.33180
[1mStep[0m  [126/427], [94mLoss[0m : 2.34445
[1mStep[0m  [168/427], [94mLoss[0m : 2.67660
[1mStep[0m  [210/427], [94mLoss[0m : 3.38021
[1mStep[0m  [252/427], [94mLoss[0m : 1.96595
[1mStep[0m  [294/427], [94mLoss[0m : 2.80958
[1mStep[0m  [336/427], [94mLoss[0m : 2.15885
[1mStep[0m  [378/427], [94mLoss[0m : 1.98565
[1mStep[0m  [420/427], [94mLoss[0m : 2.74702

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.397, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.418
====================================

Phase 1 - Evaluation MAE:  2.417891676996795
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
