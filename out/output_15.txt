no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  15
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.05858
[1mStep[0m  [2/21], [94mLoss[0m : 10.45591
[1mStep[0m  [4/21], [94mLoss[0m : 9.79954
[1mStep[0m  [6/21], [94mLoss[0m : 9.31468
[1mStep[0m  [8/21], [94mLoss[0m : 8.43734
[1mStep[0m  [10/21], [94mLoss[0m : 7.57074
[1mStep[0m  [12/21], [94mLoss[0m : 6.12826
[1mStep[0m  [14/21], [94mLoss[0m : 4.82605
[1mStep[0m  [16/21], [94mLoss[0m : 3.91540
[1mStep[0m  [18/21], [94mLoss[0m : 3.42984
[1mStep[0m  [20/21], [94mLoss[0m : 3.22895

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.126, [92mTest[0m: 10.874, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.17610
[1mStep[0m  [2/21], [94mLoss[0m : 3.31604
[1mStep[0m  [4/21], [94mLoss[0m : 3.51455
[1mStep[0m  [6/21], [94mLoss[0m : 3.75260
[1mStep[0m  [8/21], [94mLoss[0m : 3.55063
[1mStep[0m  [10/21], [94mLoss[0m : 3.32739
[1mStep[0m  [12/21], [94mLoss[0m : 3.06523
[1mStep[0m  [14/21], [94mLoss[0m : 2.95720
[1mStep[0m  [16/21], [94mLoss[0m : 2.95022
[1mStep[0m  [18/21], [94mLoss[0m : 3.04124
[1mStep[0m  [20/21], [94mLoss[0m : 3.06806

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.228, [92mTest[0m: 5.304, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.04170
[1mStep[0m  [2/21], [94mLoss[0m : 3.02496
[1mStep[0m  [4/21], [94mLoss[0m : 3.10659
[1mStep[0m  [6/21], [94mLoss[0m : 2.84459
[1mStep[0m  [8/21], [94mLoss[0m : 3.04340
[1mStep[0m  [10/21], [94mLoss[0m : 2.79102
[1mStep[0m  [12/21], [94mLoss[0m : 2.78357
[1mStep[0m  [14/21], [94mLoss[0m : 2.85654
[1mStep[0m  [16/21], [94mLoss[0m : 2.79963
[1mStep[0m  [18/21], [94mLoss[0m : 2.97131
[1mStep[0m  [20/21], [94mLoss[0m : 2.81266

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.911, [92mTest[0m: 3.648, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.87904
[1mStep[0m  [2/21], [94mLoss[0m : 2.72404
[1mStep[0m  [4/21], [94mLoss[0m : 2.69903
[1mStep[0m  [6/21], [94mLoss[0m : 2.99292
[1mStep[0m  [8/21], [94mLoss[0m : 2.68926
[1mStep[0m  [10/21], [94mLoss[0m : 2.78102
[1mStep[0m  [12/21], [94mLoss[0m : 2.72496
[1mStep[0m  [14/21], [94mLoss[0m : 2.70300
[1mStep[0m  [16/21], [94mLoss[0m : 2.84121
[1mStep[0m  [18/21], [94mLoss[0m : 2.80208
[1mStep[0m  [20/21], [94mLoss[0m : 2.96857

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.785, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62827
[1mStep[0m  [2/21], [94mLoss[0m : 2.82711
[1mStep[0m  [4/21], [94mLoss[0m : 2.87188
[1mStep[0m  [6/21], [94mLoss[0m : 2.76065
[1mStep[0m  [8/21], [94mLoss[0m : 2.74598
[1mStep[0m  [10/21], [94mLoss[0m : 2.62591
[1mStep[0m  [12/21], [94mLoss[0m : 2.64087
[1mStep[0m  [14/21], [94mLoss[0m : 2.85959
[1mStep[0m  [16/21], [94mLoss[0m : 2.68125
[1mStep[0m  [18/21], [94mLoss[0m : 2.76498
[1mStep[0m  [20/21], [94mLoss[0m : 2.75084

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.731, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65292
[1mStep[0m  [2/21], [94mLoss[0m : 2.75051
[1mStep[0m  [4/21], [94mLoss[0m : 2.65325
[1mStep[0m  [6/21], [94mLoss[0m : 2.80154
[1mStep[0m  [8/21], [94mLoss[0m : 2.71744
[1mStep[0m  [10/21], [94mLoss[0m : 2.79659
[1mStep[0m  [12/21], [94mLoss[0m : 2.65996
[1mStep[0m  [14/21], [94mLoss[0m : 2.69372
[1mStep[0m  [16/21], [94mLoss[0m : 2.75410
[1mStep[0m  [18/21], [94mLoss[0m : 2.72044
[1mStep[0m  [20/21], [94mLoss[0m : 2.80085

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72061
[1mStep[0m  [2/21], [94mLoss[0m : 2.54134
[1mStep[0m  [4/21], [94mLoss[0m : 2.71416
[1mStep[0m  [6/21], [94mLoss[0m : 2.75669
[1mStep[0m  [8/21], [94mLoss[0m : 2.67432
[1mStep[0m  [10/21], [94mLoss[0m : 2.76280
[1mStep[0m  [12/21], [94mLoss[0m : 2.66725
[1mStep[0m  [14/21], [94mLoss[0m : 2.52101
[1mStep[0m  [16/21], [94mLoss[0m : 2.70902
[1mStep[0m  [18/21], [94mLoss[0m : 2.79339
[1mStep[0m  [20/21], [94mLoss[0m : 2.58900

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67535
[1mStep[0m  [2/21], [94mLoss[0m : 2.55495
[1mStep[0m  [4/21], [94mLoss[0m : 2.69597
[1mStep[0m  [6/21], [94mLoss[0m : 2.67896
[1mStep[0m  [8/21], [94mLoss[0m : 2.47308
[1mStep[0m  [10/21], [94mLoss[0m : 2.60736
[1mStep[0m  [12/21], [94mLoss[0m : 2.60899
[1mStep[0m  [14/21], [94mLoss[0m : 2.69443
[1mStep[0m  [16/21], [94mLoss[0m : 2.65330
[1mStep[0m  [18/21], [94mLoss[0m : 2.62331
[1mStep[0m  [20/21], [94mLoss[0m : 2.60949

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69954
[1mStep[0m  [2/21], [94mLoss[0m : 2.53255
[1mStep[0m  [4/21], [94mLoss[0m : 2.64333
[1mStep[0m  [6/21], [94mLoss[0m : 2.62165
[1mStep[0m  [8/21], [94mLoss[0m : 2.65506
[1mStep[0m  [10/21], [94mLoss[0m : 2.59298
[1mStep[0m  [12/21], [94mLoss[0m : 2.41703
[1mStep[0m  [14/21], [94mLoss[0m : 2.62060
[1mStep[0m  [16/21], [94mLoss[0m : 2.77432
[1mStep[0m  [18/21], [94mLoss[0m : 2.70437
[1mStep[0m  [20/21], [94mLoss[0m : 2.61107

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67351
[1mStep[0m  [2/21], [94mLoss[0m : 2.66852
[1mStep[0m  [4/21], [94mLoss[0m : 2.61009
[1mStep[0m  [6/21], [94mLoss[0m : 2.65818
[1mStep[0m  [8/21], [94mLoss[0m : 2.80317
[1mStep[0m  [10/21], [94mLoss[0m : 2.79507
[1mStep[0m  [12/21], [94mLoss[0m : 2.46535
[1mStep[0m  [14/21], [94mLoss[0m : 2.59503
[1mStep[0m  [16/21], [94mLoss[0m : 2.55416
[1mStep[0m  [18/21], [94mLoss[0m : 2.66982
[1mStep[0m  [20/21], [94mLoss[0m : 2.57892

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52404
[1mStep[0m  [2/21], [94mLoss[0m : 2.67436
[1mStep[0m  [4/21], [94mLoss[0m : 2.74378
[1mStep[0m  [6/21], [94mLoss[0m : 2.62751
[1mStep[0m  [8/21], [94mLoss[0m : 2.59353
[1mStep[0m  [10/21], [94mLoss[0m : 2.63421
[1mStep[0m  [12/21], [94mLoss[0m : 2.70144
[1mStep[0m  [14/21], [94mLoss[0m : 2.80384
[1mStep[0m  [16/21], [94mLoss[0m : 2.66697
[1mStep[0m  [18/21], [94mLoss[0m : 2.64150
[1mStep[0m  [20/21], [94mLoss[0m : 2.44859

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45281
[1mStep[0m  [2/21], [94mLoss[0m : 2.54244
[1mStep[0m  [4/21], [94mLoss[0m : 2.73124
[1mStep[0m  [6/21], [94mLoss[0m : 2.60343
[1mStep[0m  [8/21], [94mLoss[0m : 2.60540
[1mStep[0m  [10/21], [94mLoss[0m : 2.73268
[1mStep[0m  [12/21], [94mLoss[0m : 2.59967
[1mStep[0m  [14/21], [94mLoss[0m : 2.63750
[1mStep[0m  [16/21], [94mLoss[0m : 2.60331
[1mStep[0m  [18/21], [94mLoss[0m : 2.65213
[1mStep[0m  [20/21], [94mLoss[0m : 2.56619

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65571
[1mStep[0m  [2/21], [94mLoss[0m : 2.44618
[1mStep[0m  [4/21], [94mLoss[0m : 2.53798
[1mStep[0m  [6/21], [94mLoss[0m : 2.59624
[1mStep[0m  [8/21], [94mLoss[0m : 2.60900
[1mStep[0m  [10/21], [94mLoss[0m : 2.47424
[1mStep[0m  [12/21], [94mLoss[0m : 2.54976
[1mStep[0m  [14/21], [94mLoss[0m : 2.54932
[1mStep[0m  [16/21], [94mLoss[0m : 2.49301
[1mStep[0m  [18/21], [94mLoss[0m : 2.47214
[1mStep[0m  [20/21], [94mLoss[0m : 2.61077

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72375
[1mStep[0m  [2/21], [94mLoss[0m : 2.53657
[1mStep[0m  [4/21], [94mLoss[0m : 2.29964
[1mStep[0m  [6/21], [94mLoss[0m : 2.58659
[1mStep[0m  [8/21], [94mLoss[0m : 2.60988
[1mStep[0m  [10/21], [94mLoss[0m : 2.63970
[1mStep[0m  [12/21], [94mLoss[0m : 2.44547
[1mStep[0m  [14/21], [94mLoss[0m : 2.68160
[1mStep[0m  [16/21], [94mLoss[0m : 2.51573
[1mStep[0m  [18/21], [94mLoss[0m : 2.54351
[1mStep[0m  [20/21], [94mLoss[0m : 2.60583

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68978
[1mStep[0m  [2/21], [94mLoss[0m : 2.62585
[1mStep[0m  [4/21], [94mLoss[0m : 2.60108
[1mStep[0m  [6/21], [94mLoss[0m : 2.58842
[1mStep[0m  [8/21], [94mLoss[0m : 2.48186
[1mStep[0m  [10/21], [94mLoss[0m : 2.50389
[1mStep[0m  [12/21], [94mLoss[0m : 2.63821
[1mStep[0m  [14/21], [94mLoss[0m : 2.66314
[1mStep[0m  [16/21], [94mLoss[0m : 2.57958
[1mStep[0m  [18/21], [94mLoss[0m : 2.55218
[1mStep[0m  [20/21], [94mLoss[0m : 2.63080

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42660
[1mStep[0m  [2/21], [94mLoss[0m : 2.40931
[1mStep[0m  [4/21], [94mLoss[0m : 2.56905
[1mStep[0m  [6/21], [94mLoss[0m : 2.46293
[1mStep[0m  [8/21], [94mLoss[0m : 2.45604
[1mStep[0m  [10/21], [94mLoss[0m : 2.60459
[1mStep[0m  [12/21], [94mLoss[0m : 2.62258
[1mStep[0m  [14/21], [94mLoss[0m : 2.51044
[1mStep[0m  [16/21], [94mLoss[0m : 2.55312
[1mStep[0m  [18/21], [94mLoss[0m : 2.52889
[1mStep[0m  [20/21], [94mLoss[0m : 2.53176

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42150
[1mStep[0m  [2/21], [94mLoss[0m : 2.42877
[1mStep[0m  [4/21], [94mLoss[0m : 2.49571
[1mStep[0m  [6/21], [94mLoss[0m : 2.69521
[1mStep[0m  [8/21], [94mLoss[0m : 2.59316
[1mStep[0m  [10/21], [94mLoss[0m : 2.73590
[1mStep[0m  [12/21], [94mLoss[0m : 2.47330
[1mStep[0m  [14/21], [94mLoss[0m : 2.46136
[1mStep[0m  [16/21], [94mLoss[0m : 2.65116
[1mStep[0m  [18/21], [94mLoss[0m : 2.43249
[1mStep[0m  [20/21], [94mLoss[0m : 2.51555

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47542
[1mStep[0m  [2/21], [94mLoss[0m : 2.52824
[1mStep[0m  [4/21], [94mLoss[0m : 2.48033
[1mStep[0m  [6/21], [94mLoss[0m : 2.52713
[1mStep[0m  [8/21], [94mLoss[0m : 2.67147
[1mStep[0m  [10/21], [94mLoss[0m : 2.38849
[1mStep[0m  [12/21], [94mLoss[0m : 2.65021
[1mStep[0m  [14/21], [94mLoss[0m : 2.51717
[1mStep[0m  [16/21], [94mLoss[0m : 2.47298
[1mStep[0m  [18/21], [94mLoss[0m : 2.60294
[1mStep[0m  [20/21], [94mLoss[0m : 2.57339

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45291
[1mStep[0m  [2/21], [94mLoss[0m : 2.46319
[1mStep[0m  [4/21], [94mLoss[0m : 2.57407
[1mStep[0m  [6/21], [94mLoss[0m : 2.53532
[1mStep[0m  [8/21], [94mLoss[0m : 2.41440
[1mStep[0m  [10/21], [94mLoss[0m : 2.49852
[1mStep[0m  [12/21], [94mLoss[0m : 2.48181
[1mStep[0m  [14/21], [94mLoss[0m : 2.48546
[1mStep[0m  [16/21], [94mLoss[0m : 2.51057
[1mStep[0m  [18/21], [94mLoss[0m : 2.37264
[1mStep[0m  [20/21], [94mLoss[0m : 2.45947

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44306
[1mStep[0m  [2/21], [94mLoss[0m : 2.43857
[1mStep[0m  [4/21], [94mLoss[0m : 2.48878
[1mStep[0m  [6/21], [94mLoss[0m : 2.64429
[1mStep[0m  [8/21], [94mLoss[0m : 2.53944
[1mStep[0m  [10/21], [94mLoss[0m : 2.43432
[1mStep[0m  [12/21], [94mLoss[0m : 2.48367
[1mStep[0m  [14/21], [94mLoss[0m : 2.57538
[1mStep[0m  [16/21], [94mLoss[0m : 2.56261
[1mStep[0m  [18/21], [94mLoss[0m : 2.34528
[1mStep[0m  [20/21], [94mLoss[0m : 2.55051

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47836
[1mStep[0m  [2/21], [94mLoss[0m : 2.45949
[1mStep[0m  [4/21], [94mLoss[0m : 2.55179
[1mStep[0m  [6/21], [94mLoss[0m : 2.60527
[1mStep[0m  [8/21], [94mLoss[0m : 2.52336
[1mStep[0m  [10/21], [94mLoss[0m : 2.50053
[1mStep[0m  [12/21], [94mLoss[0m : 2.37072
[1mStep[0m  [14/21], [94mLoss[0m : 2.48712
[1mStep[0m  [16/21], [94mLoss[0m : 2.54922
[1mStep[0m  [18/21], [94mLoss[0m : 2.51248
[1mStep[0m  [20/21], [94mLoss[0m : 2.53291

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61621
[1mStep[0m  [2/21], [94mLoss[0m : 2.44047
[1mStep[0m  [4/21], [94mLoss[0m : 2.49135
[1mStep[0m  [6/21], [94mLoss[0m : 2.37178
[1mStep[0m  [8/21], [94mLoss[0m : 2.47672
[1mStep[0m  [10/21], [94mLoss[0m : 2.44814
[1mStep[0m  [12/21], [94mLoss[0m : 2.35318
[1mStep[0m  [14/21], [94mLoss[0m : 2.47851
[1mStep[0m  [16/21], [94mLoss[0m : 2.42889
[1mStep[0m  [18/21], [94mLoss[0m : 2.54640
[1mStep[0m  [20/21], [94mLoss[0m : 2.58986

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57576
[1mStep[0m  [2/21], [94mLoss[0m : 2.62025
[1mStep[0m  [4/21], [94mLoss[0m : 2.37292
[1mStep[0m  [6/21], [94mLoss[0m : 2.58985
[1mStep[0m  [8/21], [94mLoss[0m : 2.44196
[1mStep[0m  [10/21], [94mLoss[0m : 2.47192
[1mStep[0m  [12/21], [94mLoss[0m : 2.57656
[1mStep[0m  [14/21], [94mLoss[0m : 2.32950
[1mStep[0m  [16/21], [94mLoss[0m : 2.44501
[1mStep[0m  [18/21], [94mLoss[0m : 2.51093
[1mStep[0m  [20/21], [94mLoss[0m : 2.36577

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50573
[1mStep[0m  [2/21], [94mLoss[0m : 2.50649
[1mStep[0m  [4/21], [94mLoss[0m : 2.49470
[1mStep[0m  [6/21], [94mLoss[0m : 2.26826
[1mStep[0m  [8/21], [94mLoss[0m : 2.57404
[1mStep[0m  [10/21], [94mLoss[0m : 2.36488
[1mStep[0m  [12/21], [94mLoss[0m : 2.38132
[1mStep[0m  [14/21], [94mLoss[0m : 2.54601
[1mStep[0m  [16/21], [94mLoss[0m : 2.54201
[1mStep[0m  [18/21], [94mLoss[0m : 2.52567
[1mStep[0m  [20/21], [94mLoss[0m : 2.45499

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55739
[1mStep[0m  [2/21], [94mLoss[0m : 2.49273
[1mStep[0m  [4/21], [94mLoss[0m : 2.50804
[1mStep[0m  [6/21], [94mLoss[0m : 2.53101
[1mStep[0m  [8/21], [94mLoss[0m : 2.50038
[1mStep[0m  [10/21], [94mLoss[0m : 2.37250
[1mStep[0m  [12/21], [94mLoss[0m : 2.54206
[1mStep[0m  [14/21], [94mLoss[0m : 2.48892
[1mStep[0m  [16/21], [94mLoss[0m : 2.50310
[1mStep[0m  [18/21], [94mLoss[0m : 2.37873
[1mStep[0m  [20/21], [94mLoss[0m : 2.45635

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.311, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51545
[1mStep[0m  [2/21], [94mLoss[0m : 2.44012
[1mStep[0m  [4/21], [94mLoss[0m : 2.42478
[1mStep[0m  [6/21], [94mLoss[0m : 2.49350
[1mStep[0m  [8/21], [94mLoss[0m : 2.54994
[1mStep[0m  [10/21], [94mLoss[0m : 2.41111
[1mStep[0m  [12/21], [94mLoss[0m : 2.51616
[1mStep[0m  [14/21], [94mLoss[0m : 2.39740
[1mStep[0m  [16/21], [94mLoss[0m : 2.52999
[1mStep[0m  [18/21], [94mLoss[0m : 2.44220
[1mStep[0m  [20/21], [94mLoss[0m : 2.38896

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41903
[1mStep[0m  [2/21], [94mLoss[0m : 2.41988
[1mStep[0m  [4/21], [94mLoss[0m : 2.42424
[1mStep[0m  [6/21], [94mLoss[0m : 2.31826
[1mStep[0m  [8/21], [94mLoss[0m : 2.50010
[1mStep[0m  [10/21], [94mLoss[0m : 2.47576
[1mStep[0m  [12/21], [94mLoss[0m : 2.43829
[1mStep[0m  [14/21], [94mLoss[0m : 2.48242
[1mStep[0m  [16/21], [94mLoss[0m : 2.53584
[1mStep[0m  [18/21], [94mLoss[0m : 2.44609
[1mStep[0m  [20/21], [94mLoss[0m : 2.40726

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36337
[1mStep[0m  [2/21], [94mLoss[0m : 2.54634
[1mStep[0m  [4/21], [94mLoss[0m : 2.48530
[1mStep[0m  [6/21], [94mLoss[0m : 2.31004
[1mStep[0m  [8/21], [94mLoss[0m : 2.62282
[1mStep[0m  [10/21], [94mLoss[0m : 2.54114
[1mStep[0m  [12/21], [94mLoss[0m : 2.58561
[1mStep[0m  [14/21], [94mLoss[0m : 2.29180
[1mStep[0m  [16/21], [94mLoss[0m : 2.34024
[1mStep[0m  [18/21], [94mLoss[0m : 2.61145
[1mStep[0m  [20/21], [94mLoss[0m : 2.54376

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34940
[1mStep[0m  [2/21], [94mLoss[0m : 2.41328
[1mStep[0m  [4/21], [94mLoss[0m : 2.46158
[1mStep[0m  [6/21], [94mLoss[0m : 2.36852
[1mStep[0m  [8/21], [94mLoss[0m : 2.41888
[1mStep[0m  [10/21], [94mLoss[0m : 2.36847
[1mStep[0m  [12/21], [94mLoss[0m : 2.29500
[1mStep[0m  [14/21], [94mLoss[0m : 2.37876
[1mStep[0m  [16/21], [94mLoss[0m : 2.46304
[1mStep[0m  [18/21], [94mLoss[0m : 2.50345
[1mStep[0m  [20/21], [94mLoss[0m : 2.43922

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48084
[1mStep[0m  [2/21], [94mLoss[0m : 2.27711
[1mStep[0m  [4/21], [94mLoss[0m : 2.51190
[1mStep[0m  [6/21], [94mLoss[0m : 2.39266
[1mStep[0m  [8/21], [94mLoss[0m : 2.38392
[1mStep[0m  [10/21], [94mLoss[0m : 2.44977
[1mStep[0m  [12/21], [94mLoss[0m : 2.31211
[1mStep[0m  [14/21], [94mLoss[0m : 2.52184
[1mStep[0m  [16/21], [94mLoss[0m : 2.53354
[1mStep[0m  [18/21], [94mLoss[0m : 2.47974
[1mStep[0m  [20/21], [94mLoss[0m : 2.35945

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.310
====================================

Phase 1 - Evaluation MAE:  2.309816871370588
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.24522
[1mStep[0m  [2/21], [94mLoss[0m : 2.52277
[1mStep[0m  [4/21], [94mLoss[0m : 2.50621
[1mStep[0m  [6/21], [94mLoss[0m : 2.52326
[1mStep[0m  [8/21], [94mLoss[0m : 2.55611
[1mStep[0m  [10/21], [94mLoss[0m : 2.59185
[1mStep[0m  [12/21], [94mLoss[0m : 2.61196
[1mStep[0m  [14/21], [94mLoss[0m : 2.59819
[1mStep[0m  [16/21], [94mLoss[0m : 2.34954
[1mStep[0m  [18/21], [94mLoss[0m : 2.42646
[1mStep[0m  [20/21], [94mLoss[0m : 2.74459

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.312, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37989
[1mStep[0m  [2/21], [94mLoss[0m : 2.60646
[1mStep[0m  [4/21], [94mLoss[0m : 2.52799
[1mStep[0m  [6/21], [94mLoss[0m : 2.56137
[1mStep[0m  [8/21], [94mLoss[0m : 2.44876
[1mStep[0m  [10/21], [94mLoss[0m : 2.42813
[1mStep[0m  [12/21], [94mLoss[0m : 2.59526
[1mStep[0m  [14/21], [94mLoss[0m : 2.44904
[1mStep[0m  [16/21], [94mLoss[0m : 2.45927
[1mStep[0m  [18/21], [94mLoss[0m : 2.44848
[1mStep[0m  [20/21], [94mLoss[0m : 2.47635

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.441, [92mTest[0m: 3.094, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36556
[1mStep[0m  [2/21], [94mLoss[0m : 2.25323
[1mStep[0m  [4/21], [94mLoss[0m : 2.39693
[1mStep[0m  [6/21], [94mLoss[0m : 2.48959
[1mStep[0m  [8/21], [94mLoss[0m : 2.27976
[1mStep[0m  [10/21], [94mLoss[0m : 2.35642
[1mStep[0m  [12/21], [94mLoss[0m : 2.47978
[1mStep[0m  [14/21], [94mLoss[0m : 2.42728
[1mStep[0m  [16/21], [94mLoss[0m : 2.43298
[1mStep[0m  [18/21], [94mLoss[0m : 2.32947
[1mStep[0m  [20/21], [94mLoss[0m : 2.47469

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.375, [92mTest[0m: 3.097, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30107
[1mStep[0m  [2/21], [94mLoss[0m : 2.44813
[1mStep[0m  [4/21], [94mLoss[0m : 2.21769
[1mStep[0m  [6/21], [94mLoss[0m : 2.20102
[1mStep[0m  [8/21], [94mLoss[0m : 2.19517
[1mStep[0m  [10/21], [94mLoss[0m : 2.47302
[1mStep[0m  [12/21], [94mLoss[0m : 2.15260
[1mStep[0m  [14/21], [94mLoss[0m : 2.19119
[1mStep[0m  [16/21], [94mLoss[0m : 2.46490
[1mStep[0m  [18/21], [94mLoss[0m : 2.26304
[1mStep[0m  [20/21], [94mLoss[0m : 2.24747

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.857, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20685
[1mStep[0m  [2/21], [94mLoss[0m : 2.17834
[1mStep[0m  [4/21], [94mLoss[0m : 2.10062
[1mStep[0m  [6/21], [94mLoss[0m : 2.25186
[1mStep[0m  [8/21], [94mLoss[0m : 2.19154
[1mStep[0m  [10/21], [94mLoss[0m : 2.22388
[1mStep[0m  [12/21], [94mLoss[0m : 2.28907
[1mStep[0m  [14/21], [94mLoss[0m : 2.13032
[1mStep[0m  [16/21], [94mLoss[0m : 2.36968
[1mStep[0m  [18/21], [94mLoss[0m : 2.20739
[1mStep[0m  [20/21], [94mLoss[0m : 2.25716

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.228, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08375
[1mStep[0m  [2/21], [94mLoss[0m : 2.23913
[1mStep[0m  [4/21], [94mLoss[0m : 2.19502
[1mStep[0m  [6/21], [94mLoss[0m : 2.11057
[1mStep[0m  [8/21], [94mLoss[0m : 2.25365
[1mStep[0m  [10/21], [94mLoss[0m : 2.05866
[1mStep[0m  [12/21], [94mLoss[0m : 2.11804
[1mStep[0m  [14/21], [94mLoss[0m : 2.35935
[1mStep[0m  [16/21], [94mLoss[0m : 2.30217
[1mStep[0m  [18/21], [94mLoss[0m : 2.30653
[1mStep[0m  [20/21], [94mLoss[0m : 2.23378

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95255
[1mStep[0m  [2/21], [94mLoss[0m : 2.05642
[1mStep[0m  [4/21], [94mLoss[0m : 2.05255
[1mStep[0m  [6/21], [94mLoss[0m : 2.02106
[1mStep[0m  [8/21], [94mLoss[0m : 2.05720
[1mStep[0m  [10/21], [94mLoss[0m : 2.21751
[1mStep[0m  [12/21], [94mLoss[0m : 2.13863
[1mStep[0m  [14/21], [94mLoss[0m : 2.23372
[1mStep[0m  [16/21], [94mLoss[0m : 2.17479
[1mStep[0m  [18/21], [94mLoss[0m : 2.11703
[1mStep[0m  [20/21], [94mLoss[0m : 2.08423

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93995
[1mStep[0m  [2/21], [94mLoss[0m : 2.03314
[1mStep[0m  [4/21], [94mLoss[0m : 1.99415
[1mStep[0m  [6/21], [94mLoss[0m : 2.02302
[1mStep[0m  [8/21], [94mLoss[0m : 1.95033
[1mStep[0m  [10/21], [94mLoss[0m : 1.97824
[1mStep[0m  [12/21], [94mLoss[0m : 2.11821
[1mStep[0m  [14/21], [94mLoss[0m : 2.06143
[1mStep[0m  [16/21], [94mLoss[0m : 1.98789
[1mStep[0m  [18/21], [94mLoss[0m : 2.10413
[1mStep[0m  [20/21], [94mLoss[0m : 2.21287

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93649
[1mStep[0m  [2/21], [94mLoss[0m : 1.98964
[1mStep[0m  [4/21], [94mLoss[0m : 1.95685
[1mStep[0m  [6/21], [94mLoss[0m : 1.96783
[1mStep[0m  [8/21], [94mLoss[0m : 2.05034
[1mStep[0m  [10/21], [94mLoss[0m : 2.00198
[1mStep[0m  [12/21], [94mLoss[0m : 2.09808
[1mStep[0m  [14/21], [94mLoss[0m : 2.03105
[1mStep[0m  [16/21], [94mLoss[0m : 2.00613
[1mStep[0m  [18/21], [94mLoss[0m : 1.99685
[1mStep[0m  [20/21], [94mLoss[0m : 1.87989

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08274
[1mStep[0m  [2/21], [94mLoss[0m : 1.97088
[1mStep[0m  [4/21], [94mLoss[0m : 1.88623
[1mStep[0m  [6/21], [94mLoss[0m : 1.92826
[1mStep[0m  [8/21], [94mLoss[0m : 2.10698
[1mStep[0m  [10/21], [94mLoss[0m : 2.01155
[1mStep[0m  [12/21], [94mLoss[0m : 2.02833
[1mStep[0m  [14/21], [94mLoss[0m : 2.08316
[1mStep[0m  [16/21], [94mLoss[0m : 1.97943
[1mStep[0m  [18/21], [94mLoss[0m : 2.05996
[1mStep[0m  [20/21], [94mLoss[0m : 1.85965

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.975, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91512
[1mStep[0m  [2/21], [94mLoss[0m : 1.73482
[1mStep[0m  [4/21], [94mLoss[0m : 1.89016
[1mStep[0m  [6/21], [94mLoss[0m : 1.84385
[1mStep[0m  [8/21], [94mLoss[0m : 1.80257
[1mStep[0m  [10/21], [94mLoss[0m : 2.02536
[1mStep[0m  [12/21], [94mLoss[0m : 1.88333
[1mStep[0m  [14/21], [94mLoss[0m : 1.92202
[1mStep[0m  [16/21], [94mLoss[0m : 1.86770
[1mStep[0m  [18/21], [94mLoss[0m : 1.89739
[1mStep[0m  [20/21], [94mLoss[0m : 1.99459

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.83342
[1mStep[0m  [2/21], [94mLoss[0m : 1.88122
[1mStep[0m  [4/21], [94mLoss[0m : 1.91303
[1mStep[0m  [6/21], [94mLoss[0m : 1.85505
[1mStep[0m  [8/21], [94mLoss[0m : 1.90460
[1mStep[0m  [10/21], [94mLoss[0m : 1.84812
[1mStep[0m  [12/21], [94mLoss[0m : 1.83393
[1mStep[0m  [14/21], [94mLoss[0m : 1.91505
[1mStep[0m  [16/21], [94mLoss[0m : 1.97387
[1mStep[0m  [18/21], [94mLoss[0m : 1.99120
[1mStep[0m  [20/21], [94mLoss[0m : 1.94842

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86329
[1mStep[0m  [2/21], [94mLoss[0m : 1.87859
[1mStep[0m  [4/21], [94mLoss[0m : 1.94544
[1mStep[0m  [6/21], [94mLoss[0m : 1.85079
[1mStep[0m  [8/21], [94mLoss[0m : 1.94320
[1mStep[0m  [10/21], [94mLoss[0m : 1.77027
[1mStep[0m  [12/21], [94mLoss[0m : 1.87865
[1mStep[0m  [14/21], [94mLoss[0m : 1.70110
[1mStep[0m  [16/21], [94mLoss[0m : 1.76372
[1mStep[0m  [18/21], [94mLoss[0m : 1.83060
[1mStep[0m  [20/21], [94mLoss[0m : 1.90217

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71433
[1mStep[0m  [2/21], [94mLoss[0m : 1.74183
[1mStep[0m  [4/21], [94mLoss[0m : 1.82260
[1mStep[0m  [6/21], [94mLoss[0m : 1.77777
[1mStep[0m  [8/21], [94mLoss[0m : 1.77260
[1mStep[0m  [10/21], [94mLoss[0m : 1.74035
[1mStep[0m  [12/21], [94mLoss[0m : 1.71967
[1mStep[0m  [14/21], [94mLoss[0m : 1.84255
[1mStep[0m  [16/21], [94mLoss[0m : 1.78992
[1mStep[0m  [18/21], [94mLoss[0m : 1.90531
[1mStep[0m  [20/21], [94mLoss[0m : 1.73494

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.79795
[1mStep[0m  [2/21], [94mLoss[0m : 1.72065
[1mStep[0m  [4/21], [94mLoss[0m : 1.75470
[1mStep[0m  [6/21], [94mLoss[0m : 1.74904
[1mStep[0m  [8/21], [94mLoss[0m : 1.72725
[1mStep[0m  [10/21], [94mLoss[0m : 1.75076
[1mStep[0m  [12/21], [94mLoss[0m : 1.69744
[1mStep[0m  [14/21], [94mLoss[0m : 1.86640
[1mStep[0m  [16/21], [94mLoss[0m : 1.81284
[1mStep[0m  [18/21], [94mLoss[0m : 1.74397
[1mStep[0m  [20/21], [94mLoss[0m : 2.03146

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77807
[1mStep[0m  [2/21], [94mLoss[0m : 1.69137
[1mStep[0m  [4/21], [94mLoss[0m : 1.73241
[1mStep[0m  [6/21], [94mLoss[0m : 1.61526
[1mStep[0m  [8/21], [94mLoss[0m : 1.89037
[1mStep[0m  [10/21], [94mLoss[0m : 1.75031
[1mStep[0m  [12/21], [94mLoss[0m : 1.69099
[1mStep[0m  [14/21], [94mLoss[0m : 1.79744
[1mStep[0m  [16/21], [94mLoss[0m : 1.84720
[1mStep[0m  [18/21], [94mLoss[0m : 1.82640
[1mStep[0m  [20/21], [94mLoss[0m : 1.73344

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.58918
[1mStep[0m  [2/21], [94mLoss[0m : 1.83429
[1mStep[0m  [4/21], [94mLoss[0m : 1.64212
[1mStep[0m  [6/21], [94mLoss[0m : 1.81190
[1mStep[0m  [8/21], [94mLoss[0m : 1.67647
[1mStep[0m  [10/21], [94mLoss[0m : 1.68395
[1mStep[0m  [12/21], [94mLoss[0m : 1.70451
[1mStep[0m  [14/21], [94mLoss[0m : 1.79562
[1mStep[0m  [16/21], [94mLoss[0m : 1.76284
[1mStep[0m  [18/21], [94mLoss[0m : 1.89988
[1mStep[0m  [20/21], [94mLoss[0m : 1.78331

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.74611
[1mStep[0m  [2/21], [94mLoss[0m : 1.69725
[1mStep[0m  [4/21], [94mLoss[0m : 1.60596
[1mStep[0m  [6/21], [94mLoss[0m : 1.72417
[1mStep[0m  [8/21], [94mLoss[0m : 1.77910
[1mStep[0m  [10/21], [94mLoss[0m : 1.82774
[1mStep[0m  [12/21], [94mLoss[0m : 1.70454
[1mStep[0m  [14/21], [94mLoss[0m : 1.67339
[1mStep[0m  [16/21], [94mLoss[0m : 1.78838
[1mStep[0m  [18/21], [94mLoss[0m : 1.78850
[1mStep[0m  [20/21], [94mLoss[0m : 1.70816

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.68012
[1mStep[0m  [2/21], [94mLoss[0m : 1.51005
[1mStep[0m  [4/21], [94mLoss[0m : 1.72846
[1mStep[0m  [6/21], [94mLoss[0m : 1.80824
[1mStep[0m  [8/21], [94mLoss[0m : 1.67732
[1mStep[0m  [10/21], [94mLoss[0m : 1.68352
[1mStep[0m  [12/21], [94mLoss[0m : 1.66627
[1mStep[0m  [14/21], [94mLoss[0m : 1.65264
[1mStep[0m  [16/21], [94mLoss[0m : 1.61870
[1mStep[0m  [18/21], [94mLoss[0m : 1.77231
[1mStep[0m  [20/21], [94mLoss[0m : 1.78715

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.58988
[1mStep[0m  [2/21], [94mLoss[0m : 1.58025
[1mStep[0m  [4/21], [94mLoss[0m : 1.69668
[1mStep[0m  [6/21], [94mLoss[0m : 1.66373
[1mStep[0m  [8/21], [94mLoss[0m : 1.70457
[1mStep[0m  [10/21], [94mLoss[0m : 1.63007
[1mStep[0m  [12/21], [94mLoss[0m : 1.61181
[1mStep[0m  [14/21], [94mLoss[0m : 1.68140
[1mStep[0m  [16/21], [94mLoss[0m : 1.65217
[1mStep[0m  [18/21], [94mLoss[0m : 1.65061
[1mStep[0m  [20/21], [94mLoss[0m : 1.62924

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.658, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.65405
[1mStep[0m  [2/21], [94mLoss[0m : 1.54252
[1mStep[0m  [4/21], [94mLoss[0m : 1.56130
[1mStep[0m  [6/21], [94mLoss[0m : 1.55304
[1mStep[0m  [8/21], [94mLoss[0m : 1.54065
[1mStep[0m  [10/21], [94mLoss[0m : 1.63116
[1mStep[0m  [12/21], [94mLoss[0m : 1.53951
[1mStep[0m  [14/21], [94mLoss[0m : 1.65439
[1mStep[0m  [16/21], [94mLoss[0m : 1.70343
[1mStep[0m  [18/21], [94mLoss[0m : 1.64423
[1mStep[0m  [20/21], [94mLoss[0m : 1.59338

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.56547
[1mStep[0m  [2/21], [94mLoss[0m : 1.48645
[1mStep[0m  [4/21], [94mLoss[0m : 1.63316
[1mStep[0m  [6/21], [94mLoss[0m : 1.57365
[1mStep[0m  [8/21], [94mLoss[0m : 1.49476
[1mStep[0m  [10/21], [94mLoss[0m : 1.56123
[1mStep[0m  [12/21], [94mLoss[0m : 1.50216
[1mStep[0m  [14/21], [94mLoss[0m : 1.68485
[1mStep[0m  [16/21], [94mLoss[0m : 1.57627
[1mStep[0m  [18/21], [94mLoss[0m : 1.54741
[1mStep[0m  [20/21], [94mLoss[0m : 1.71409

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.53570
[1mStep[0m  [2/21], [94mLoss[0m : 1.66981
[1mStep[0m  [4/21], [94mLoss[0m : 1.64457
[1mStep[0m  [6/21], [94mLoss[0m : 1.36256
[1mStep[0m  [8/21], [94mLoss[0m : 1.58400
[1mStep[0m  [10/21], [94mLoss[0m : 1.54104
[1mStep[0m  [12/21], [94mLoss[0m : 1.62220
[1mStep[0m  [14/21], [94mLoss[0m : 1.64266
[1mStep[0m  [16/21], [94mLoss[0m : 1.54992
[1mStep[0m  [18/21], [94mLoss[0m : 1.46414
[1mStep[0m  [20/21], [94mLoss[0m : 1.54663

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.53278
[1mStep[0m  [2/21], [94mLoss[0m : 1.53899
[1mStep[0m  [4/21], [94mLoss[0m : 1.64895
[1mStep[0m  [6/21], [94mLoss[0m : 1.57107
[1mStep[0m  [8/21], [94mLoss[0m : 1.59800
[1mStep[0m  [10/21], [94mLoss[0m : 1.54943
[1mStep[0m  [12/21], [94mLoss[0m : 1.44882
[1mStep[0m  [14/21], [94mLoss[0m : 1.51546
[1mStep[0m  [16/21], [94mLoss[0m : 1.55248
[1mStep[0m  [18/21], [94mLoss[0m : 1.60282
[1mStep[0m  [20/21], [94mLoss[0m : 1.51735

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.533, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.49433
[1mStep[0m  [2/21], [94mLoss[0m : 1.45678
[1mStep[0m  [4/21], [94mLoss[0m : 1.61301
[1mStep[0m  [6/21], [94mLoss[0m : 1.50017
[1mStep[0m  [8/21], [94mLoss[0m : 1.51834
[1mStep[0m  [10/21], [94mLoss[0m : 1.49648
[1mStep[0m  [12/21], [94mLoss[0m : 1.54250
[1mStep[0m  [14/21], [94mLoss[0m : 1.56173
[1mStep[0m  [16/21], [94mLoss[0m : 1.62395
[1mStep[0m  [18/21], [94mLoss[0m : 1.53014
[1mStep[0m  [20/21], [94mLoss[0m : 1.49263

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.542, [92mTest[0m: 2.545, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51165
[1mStep[0m  [2/21], [94mLoss[0m : 1.42643
[1mStep[0m  [4/21], [94mLoss[0m : 1.47336
[1mStep[0m  [6/21], [94mLoss[0m : 1.43871
[1mStep[0m  [8/21], [94mLoss[0m : 1.65638
[1mStep[0m  [10/21], [94mLoss[0m : 1.45735
[1mStep[0m  [12/21], [94mLoss[0m : 1.50336
[1mStep[0m  [14/21], [94mLoss[0m : 1.65471
[1mStep[0m  [16/21], [94mLoss[0m : 1.56040
[1mStep[0m  [18/21], [94mLoss[0m : 1.51658
[1mStep[0m  [20/21], [94mLoss[0m : 1.46512

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.511, [92mTest[0m: 2.493, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43740
[1mStep[0m  [2/21], [94mLoss[0m : 1.52766
[1mStep[0m  [4/21], [94mLoss[0m : 1.50654
[1mStep[0m  [6/21], [94mLoss[0m : 1.39442
[1mStep[0m  [8/21], [94mLoss[0m : 1.47554
[1mStep[0m  [10/21], [94mLoss[0m : 1.54157
[1mStep[0m  [12/21], [94mLoss[0m : 1.50308
[1mStep[0m  [14/21], [94mLoss[0m : 1.50502
[1mStep[0m  [16/21], [94mLoss[0m : 1.55384
[1mStep[0m  [18/21], [94mLoss[0m : 1.47032
[1mStep[0m  [20/21], [94mLoss[0m : 1.56205

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.42304
[1mStep[0m  [2/21], [94mLoss[0m : 1.46926
[1mStep[0m  [4/21], [94mLoss[0m : 1.50978
[1mStep[0m  [6/21], [94mLoss[0m : 1.48229
[1mStep[0m  [8/21], [94mLoss[0m : 1.38792
[1mStep[0m  [10/21], [94mLoss[0m : 1.49382
[1mStep[0m  [12/21], [94mLoss[0m : 1.44637
[1mStep[0m  [14/21], [94mLoss[0m : 1.50514
[1mStep[0m  [16/21], [94mLoss[0m : 1.49074
[1mStep[0m  [18/21], [94mLoss[0m : 1.51058
[1mStep[0m  [20/21], [94mLoss[0m : 1.49074

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.485, [92mTest[0m: 2.524, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.47455
[1mStep[0m  [2/21], [94mLoss[0m : 1.41054
[1mStep[0m  [4/21], [94mLoss[0m : 1.49093
[1mStep[0m  [6/21], [94mLoss[0m : 1.51131
[1mStep[0m  [8/21], [94mLoss[0m : 1.42319
[1mStep[0m  [10/21], [94mLoss[0m : 1.51812
[1mStep[0m  [12/21], [94mLoss[0m : 1.42049
[1mStep[0m  [14/21], [94mLoss[0m : 1.46279
[1mStep[0m  [16/21], [94mLoss[0m : 1.45366
[1mStep[0m  [18/21], [94mLoss[0m : 1.49492
[1mStep[0m  [20/21], [94mLoss[0m : 1.47485

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.499, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.516054425920759
MAE score P1       2.309817
MAE score P2       2.516054
loss               1.484589
learning_rate          0.01
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay           0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.00392
[1mStep[0m  [4/42], [94mLoss[0m : 9.95548
[1mStep[0m  [8/42], [94mLoss[0m : 9.30528
[1mStep[0m  [12/42], [94mLoss[0m : 8.17275
[1mStep[0m  [16/42], [94mLoss[0m : 7.10679
[1mStep[0m  [20/42], [94mLoss[0m : 6.09580
[1mStep[0m  [24/42], [94mLoss[0m : 4.85713
[1mStep[0m  [28/42], [94mLoss[0m : 4.01995
[1mStep[0m  [32/42], [94mLoss[0m : 3.40310
[1mStep[0m  [36/42], [94mLoss[0m : 3.35163
[1mStep[0m  [40/42], [94mLoss[0m : 2.88039

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.292, [92mTest[0m: 11.054, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86432
[1mStep[0m  [4/42], [94mLoss[0m : 2.52426
[1mStep[0m  [8/42], [94mLoss[0m : 2.88639
[1mStep[0m  [12/42], [94mLoss[0m : 2.77577
[1mStep[0m  [16/42], [94mLoss[0m : 2.74458
[1mStep[0m  [20/42], [94mLoss[0m : 2.42031
[1mStep[0m  [24/42], [94mLoss[0m : 2.56009
[1mStep[0m  [28/42], [94mLoss[0m : 2.54418
[1mStep[0m  [32/42], [94mLoss[0m : 2.77068
[1mStep[0m  [36/42], [94mLoss[0m : 2.56311
[1mStep[0m  [40/42], [94mLoss[0m : 2.80343

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.822, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74382
[1mStep[0m  [4/42], [94mLoss[0m : 2.67079
[1mStep[0m  [8/42], [94mLoss[0m : 2.47166
[1mStep[0m  [12/42], [94mLoss[0m : 2.40373
[1mStep[0m  [16/42], [94mLoss[0m : 2.59639
[1mStep[0m  [20/42], [94mLoss[0m : 2.57418
[1mStep[0m  [24/42], [94mLoss[0m : 2.82371
[1mStep[0m  [28/42], [94mLoss[0m : 2.72470
[1mStep[0m  [32/42], [94mLoss[0m : 2.57411
[1mStep[0m  [36/42], [94mLoss[0m : 2.61286
[1mStep[0m  [40/42], [94mLoss[0m : 2.73710

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59863
[1mStep[0m  [4/42], [94mLoss[0m : 2.66325
[1mStep[0m  [8/42], [94mLoss[0m : 2.43074
[1mStep[0m  [12/42], [94mLoss[0m : 2.55232
[1mStep[0m  [16/42], [94mLoss[0m : 2.64113
[1mStep[0m  [20/42], [94mLoss[0m : 2.48839
[1mStep[0m  [24/42], [94mLoss[0m : 2.40458
[1mStep[0m  [28/42], [94mLoss[0m : 2.56618
[1mStep[0m  [32/42], [94mLoss[0m : 2.58880
[1mStep[0m  [36/42], [94mLoss[0m : 2.80541
[1mStep[0m  [40/42], [94mLoss[0m : 2.44432

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69931
[1mStep[0m  [4/42], [94mLoss[0m : 2.67864
[1mStep[0m  [8/42], [94mLoss[0m : 2.65912
[1mStep[0m  [12/42], [94mLoss[0m : 2.76125
[1mStep[0m  [16/42], [94mLoss[0m : 2.73709
[1mStep[0m  [20/42], [94mLoss[0m : 2.46717
[1mStep[0m  [24/42], [94mLoss[0m : 2.23419
[1mStep[0m  [28/42], [94mLoss[0m : 2.68773
[1mStep[0m  [32/42], [94mLoss[0m : 2.69616
[1mStep[0m  [36/42], [94mLoss[0m : 2.61980
[1mStep[0m  [40/42], [94mLoss[0m : 2.62526

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79837
[1mStep[0m  [4/42], [94mLoss[0m : 2.61022
[1mStep[0m  [8/42], [94mLoss[0m : 2.68555
[1mStep[0m  [12/42], [94mLoss[0m : 2.70493
[1mStep[0m  [16/42], [94mLoss[0m : 2.54533
[1mStep[0m  [20/42], [94mLoss[0m : 2.56631
[1mStep[0m  [24/42], [94mLoss[0m : 2.57208
[1mStep[0m  [28/42], [94mLoss[0m : 2.53901
[1mStep[0m  [32/42], [94mLoss[0m : 2.58551
[1mStep[0m  [36/42], [94mLoss[0m : 2.44586
[1mStep[0m  [40/42], [94mLoss[0m : 2.67867

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61384
[1mStep[0m  [4/42], [94mLoss[0m : 2.38357
[1mStep[0m  [8/42], [94mLoss[0m : 2.36594
[1mStep[0m  [12/42], [94mLoss[0m : 2.76417
[1mStep[0m  [16/42], [94mLoss[0m : 2.53352
[1mStep[0m  [20/42], [94mLoss[0m : 2.67456
[1mStep[0m  [24/42], [94mLoss[0m : 2.50271
[1mStep[0m  [28/42], [94mLoss[0m : 2.57489
[1mStep[0m  [32/42], [94mLoss[0m : 2.58141
[1mStep[0m  [36/42], [94mLoss[0m : 2.69269
[1mStep[0m  [40/42], [94mLoss[0m : 2.63613

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47037
[1mStep[0m  [4/42], [94mLoss[0m : 2.59989
[1mStep[0m  [8/42], [94mLoss[0m : 2.47828
[1mStep[0m  [12/42], [94mLoss[0m : 2.69033
[1mStep[0m  [16/42], [94mLoss[0m : 2.57468
[1mStep[0m  [20/42], [94mLoss[0m : 2.60441
[1mStep[0m  [24/42], [94mLoss[0m : 2.49994
[1mStep[0m  [28/42], [94mLoss[0m : 2.48444
[1mStep[0m  [32/42], [94mLoss[0m : 2.41589
[1mStep[0m  [36/42], [94mLoss[0m : 2.57763
[1mStep[0m  [40/42], [94mLoss[0m : 2.48624

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54970
[1mStep[0m  [4/42], [94mLoss[0m : 2.65808
[1mStep[0m  [8/42], [94mLoss[0m : 2.84560
[1mStep[0m  [12/42], [94mLoss[0m : 2.59390
[1mStep[0m  [16/42], [94mLoss[0m : 2.38977
[1mStep[0m  [20/42], [94mLoss[0m : 2.72643
[1mStep[0m  [24/42], [94mLoss[0m : 2.79329
[1mStep[0m  [28/42], [94mLoss[0m : 2.60921
[1mStep[0m  [32/42], [94mLoss[0m : 2.43776
[1mStep[0m  [36/42], [94mLoss[0m : 2.45803
[1mStep[0m  [40/42], [94mLoss[0m : 2.52426

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39006
[1mStep[0m  [4/42], [94mLoss[0m : 2.32325
[1mStep[0m  [8/42], [94mLoss[0m : 2.74677
[1mStep[0m  [12/42], [94mLoss[0m : 2.72563
[1mStep[0m  [16/42], [94mLoss[0m : 2.67797
[1mStep[0m  [20/42], [94mLoss[0m : 2.56782
[1mStep[0m  [24/42], [94mLoss[0m : 2.47742
[1mStep[0m  [28/42], [94mLoss[0m : 2.71444
[1mStep[0m  [32/42], [94mLoss[0m : 2.41414
[1mStep[0m  [36/42], [94mLoss[0m : 2.72593
[1mStep[0m  [40/42], [94mLoss[0m : 2.65276

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56569
[1mStep[0m  [4/42], [94mLoss[0m : 2.51284
[1mStep[0m  [8/42], [94mLoss[0m : 2.52753
[1mStep[0m  [12/42], [94mLoss[0m : 2.63341
[1mStep[0m  [16/42], [94mLoss[0m : 2.31875
[1mStep[0m  [20/42], [94mLoss[0m : 2.61114
[1mStep[0m  [24/42], [94mLoss[0m : 2.86369
[1mStep[0m  [28/42], [94mLoss[0m : 2.55032
[1mStep[0m  [32/42], [94mLoss[0m : 2.59423
[1mStep[0m  [36/42], [94mLoss[0m : 2.59633
[1mStep[0m  [40/42], [94mLoss[0m : 2.32595

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45461
[1mStep[0m  [4/42], [94mLoss[0m : 2.70921
[1mStep[0m  [8/42], [94mLoss[0m : 2.53974
[1mStep[0m  [12/42], [94mLoss[0m : 2.54292
[1mStep[0m  [16/42], [94mLoss[0m : 2.23180
[1mStep[0m  [20/42], [94mLoss[0m : 2.49200
[1mStep[0m  [24/42], [94mLoss[0m : 2.37407
[1mStep[0m  [28/42], [94mLoss[0m : 2.56563
[1mStep[0m  [32/42], [94mLoss[0m : 2.68893
[1mStep[0m  [36/42], [94mLoss[0m : 2.64489
[1mStep[0m  [40/42], [94mLoss[0m : 2.48897

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50613
[1mStep[0m  [4/42], [94mLoss[0m : 2.74053
[1mStep[0m  [8/42], [94mLoss[0m : 2.44496
[1mStep[0m  [12/42], [94mLoss[0m : 2.33090
[1mStep[0m  [16/42], [94mLoss[0m : 2.45616
[1mStep[0m  [20/42], [94mLoss[0m : 2.55268
[1mStep[0m  [24/42], [94mLoss[0m : 2.59473
[1mStep[0m  [28/42], [94mLoss[0m : 2.50406
[1mStep[0m  [32/42], [94mLoss[0m : 2.50122
[1mStep[0m  [36/42], [94mLoss[0m : 2.49562
[1mStep[0m  [40/42], [94mLoss[0m : 2.61771

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57952
[1mStep[0m  [4/42], [94mLoss[0m : 2.60533
[1mStep[0m  [8/42], [94mLoss[0m : 2.59364
[1mStep[0m  [12/42], [94mLoss[0m : 2.55891
[1mStep[0m  [16/42], [94mLoss[0m : 2.52200
[1mStep[0m  [20/42], [94mLoss[0m : 2.39973
[1mStep[0m  [24/42], [94mLoss[0m : 2.56459
[1mStep[0m  [28/42], [94mLoss[0m : 2.71885
[1mStep[0m  [32/42], [94mLoss[0m : 2.39027
[1mStep[0m  [36/42], [94mLoss[0m : 2.64050
[1mStep[0m  [40/42], [94mLoss[0m : 2.43888

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49394
[1mStep[0m  [4/42], [94mLoss[0m : 2.52751
[1mStep[0m  [8/42], [94mLoss[0m : 2.58777
[1mStep[0m  [12/42], [94mLoss[0m : 2.52507
[1mStep[0m  [16/42], [94mLoss[0m : 2.41550
[1mStep[0m  [20/42], [94mLoss[0m : 2.58933
[1mStep[0m  [24/42], [94mLoss[0m : 2.61739
[1mStep[0m  [28/42], [94mLoss[0m : 2.55492
[1mStep[0m  [32/42], [94mLoss[0m : 2.47154
[1mStep[0m  [36/42], [94mLoss[0m : 2.53822
[1mStep[0m  [40/42], [94mLoss[0m : 2.53417

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51358
[1mStep[0m  [4/42], [94mLoss[0m : 2.63978
[1mStep[0m  [8/42], [94mLoss[0m : 2.57103
[1mStep[0m  [12/42], [94mLoss[0m : 2.68656
[1mStep[0m  [16/42], [94mLoss[0m : 2.44320
[1mStep[0m  [20/42], [94mLoss[0m : 2.57647
[1mStep[0m  [24/42], [94mLoss[0m : 2.39348
[1mStep[0m  [28/42], [94mLoss[0m : 2.38222
[1mStep[0m  [32/42], [94mLoss[0m : 2.72869
[1mStep[0m  [36/42], [94mLoss[0m : 2.50070
[1mStep[0m  [40/42], [94mLoss[0m : 2.44412

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56134
[1mStep[0m  [4/42], [94mLoss[0m : 2.73831
[1mStep[0m  [8/42], [94mLoss[0m : 2.61786
[1mStep[0m  [12/42], [94mLoss[0m : 2.60953
[1mStep[0m  [16/42], [94mLoss[0m : 2.42124
[1mStep[0m  [20/42], [94mLoss[0m : 2.41140
[1mStep[0m  [24/42], [94mLoss[0m : 2.33270
[1mStep[0m  [28/42], [94mLoss[0m : 2.59477
[1mStep[0m  [32/42], [94mLoss[0m : 2.52756
[1mStep[0m  [36/42], [94mLoss[0m : 2.57658
[1mStep[0m  [40/42], [94mLoss[0m : 2.58041

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39869
[1mStep[0m  [4/42], [94mLoss[0m : 2.50598
[1mStep[0m  [8/42], [94mLoss[0m : 2.56695
[1mStep[0m  [12/42], [94mLoss[0m : 2.39574
[1mStep[0m  [16/42], [94mLoss[0m : 2.40040
[1mStep[0m  [20/42], [94mLoss[0m : 2.55457
[1mStep[0m  [24/42], [94mLoss[0m : 2.46891
[1mStep[0m  [28/42], [94mLoss[0m : 2.55562
[1mStep[0m  [32/42], [94mLoss[0m : 2.57004
[1mStep[0m  [36/42], [94mLoss[0m : 2.70517
[1mStep[0m  [40/42], [94mLoss[0m : 2.51304

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47290
[1mStep[0m  [4/42], [94mLoss[0m : 2.53063
[1mStep[0m  [8/42], [94mLoss[0m : 2.42954
[1mStep[0m  [12/42], [94mLoss[0m : 2.46231
[1mStep[0m  [16/42], [94mLoss[0m : 2.67523
[1mStep[0m  [20/42], [94mLoss[0m : 2.59017
[1mStep[0m  [24/42], [94mLoss[0m : 2.62068
[1mStep[0m  [28/42], [94mLoss[0m : 2.42272
[1mStep[0m  [32/42], [94mLoss[0m : 2.41268
[1mStep[0m  [36/42], [94mLoss[0m : 2.58959
[1mStep[0m  [40/42], [94mLoss[0m : 2.42438

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36201
[1mStep[0m  [4/42], [94mLoss[0m : 2.37070
[1mStep[0m  [8/42], [94mLoss[0m : 2.45230
[1mStep[0m  [12/42], [94mLoss[0m : 2.83446
[1mStep[0m  [16/42], [94mLoss[0m : 2.58877
[1mStep[0m  [20/42], [94mLoss[0m : 2.76826
[1mStep[0m  [24/42], [94mLoss[0m : 2.59409
[1mStep[0m  [28/42], [94mLoss[0m : 2.62638
[1mStep[0m  [32/42], [94mLoss[0m : 2.19940
[1mStep[0m  [36/42], [94mLoss[0m : 2.41260
[1mStep[0m  [40/42], [94mLoss[0m : 2.55608

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56451
[1mStep[0m  [4/42], [94mLoss[0m : 2.49749
[1mStep[0m  [8/42], [94mLoss[0m : 2.51599
[1mStep[0m  [12/42], [94mLoss[0m : 2.28915
[1mStep[0m  [16/42], [94mLoss[0m : 2.75049
[1mStep[0m  [20/42], [94mLoss[0m : 2.44387
[1mStep[0m  [24/42], [94mLoss[0m : 2.69280
[1mStep[0m  [28/42], [94mLoss[0m : 2.60643
[1mStep[0m  [32/42], [94mLoss[0m : 2.48719
[1mStep[0m  [36/42], [94mLoss[0m : 2.67681
[1mStep[0m  [40/42], [94mLoss[0m : 2.59163

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42040
[1mStep[0m  [4/42], [94mLoss[0m : 2.41699
[1mStep[0m  [8/42], [94mLoss[0m : 2.59070
[1mStep[0m  [12/42], [94mLoss[0m : 2.58579
[1mStep[0m  [16/42], [94mLoss[0m : 2.50643
[1mStep[0m  [20/42], [94mLoss[0m : 2.60708
[1mStep[0m  [24/42], [94mLoss[0m : 2.70791
[1mStep[0m  [28/42], [94mLoss[0m : 2.62085
[1mStep[0m  [32/42], [94mLoss[0m : 2.57033
[1mStep[0m  [36/42], [94mLoss[0m : 2.61089
[1mStep[0m  [40/42], [94mLoss[0m : 2.73184

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36557
[1mStep[0m  [4/42], [94mLoss[0m : 2.37101
[1mStep[0m  [8/42], [94mLoss[0m : 2.60902
[1mStep[0m  [12/42], [94mLoss[0m : 2.57780
[1mStep[0m  [16/42], [94mLoss[0m : 2.59755
[1mStep[0m  [20/42], [94mLoss[0m : 2.74503
[1mStep[0m  [24/42], [94mLoss[0m : 2.70508
[1mStep[0m  [28/42], [94mLoss[0m : 2.33810
[1mStep[0m  [32/42], [94mLoss[0m : 2.60073
[1mStep[0m  [36/42], [94mLoss[0m : 2.70992
[1mStep[0m  [40/42], [94mLoss[0m : 2.72400

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53560
[1mStep[0m  [4/42], [94mLoss[0m : 2.47846
[1mStep[0m  [8/42], [94mLoss[0m : 2.55830
[1mStep[0m  [12/42], [94mLoss[0m : 2.44455
[1mStep[0m  [16/42], [94mLoss[0m : 2.58677
[1mStep[0m  [20/42], [94mLoss[0m : 2.22335
[1mStep[0m  [24/42], [94mLoss[0m : 2.64600
[1mStep[0m  [28/42], [94mLoss[0m : 2.46213
[1mStep[0m  [32/42], [94mLoss[0m : 2.70222
[1mStep[0m  [36/42], [94mLoss[0m : 2.60268
[1mStep[0m  [40/42], [94mLoss[0m : 2.52212

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79131
[1mStep[0m  [4/42], [94mLoss[0m : 2.49893
[1mStep[0m  [8/42], [94mLoss[0m : 2.50688
[1mStep[0m  [12/42], [94mLoss[0m : 2.51759
[1mStep[0m  [16/42], [94mLoss[0m : 2.44156
[1mStep[0m  [20/42], [94mLoss[0m : 2.56079
[1mStep[0m  [24/42], [94mLoss[0m : 2.81375
[1mStep[0m  [28/42], [94mLoss[0m : 2.54095
[1mStep[0m  [32/42], [94mLoss[0m : 2.51547
[1mStep[0m  [36/42], [94mLoss[0m : 2.37341
[1mStep[0m  [40/42], [94mLoss[0m : 2.31190

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38339
[1mStep[0m  [4/42], [94mLoss[0m : 2.56388
[1mStep[0m  [8/42], [94mLoss[0m : 2.50882
[1mStep[0m  [12/42], [94mLoss[0m : 2.90821
[1mStep[0m  [16/42], [94mLoss[0m : 2.46700
[1mStep[0m  [20/42], [94mLoss[0m : 2.51407
[1mStep[0m  [24/42], [94mLoss[0m : 2.46330
[1mStep[0m  [28/42], [94mLoss[0m : 2.51967
[1mStep[0m  [32/42], [94mLoss[0m : 2.57492
[1mStep[0m  [36/42], [94mLoss[0m : 2.78130
[1mStep[0m  [40/42], [94mLoss[0m : 2.48834

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55818
[1mStep[0m  [4/42], [94mLoss[0m : 2.81368
[1mStep[0m  [8/42], [94mLoss[0m : 2.47014
[1mStep[0m  [12/42], [94mLoss[0m : 2.64716
[1mStep[0m  [16/42], [94mLoss[0m : 2.52076
[1mStep[0m  [20/42], [94mLoss[0m : 2.54217
[1mStep[0m  [24/42], [94mLoss[0m : 2.31625
[1mStep[0m  [28/42], [94mLoss[0m : 2.50826
[1mStep[0m  [32/42], [94mLoss[0m : 2.24375
[1mStep[0m  [36/42], [94mLoss[0m : 2.56596
[1mStep[0m  [40/42], [94mLoss[0m : 2.58641

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60049
[1mStep[0m  [4/42], [94mLoss[0m : 2.75374
[1mStep[0m  [8/42], [94mLoss[0m : 2.62959
[1mStep[0m  [12/42], [94mLoss[0m : 2.46546
[1mStep[0m  [16/42], [94mLoss[0m : 2.73322
[1mStep[0m  [20/42], [94mLoss[0m : 2.39330
[1mStep[0m  [24/42], [94mLoss[0m : 2.47546
[1mStep[0m  [28/42], [94mLoss[0m : 2.55726
[1mStep[0m  [32/42], [94mLoss[0m : 2.48691
[1mStep[0m  [36/42], [94mLoss[0m : 2.59413
[1mStep[0m  [40/42], [94mLoss[0m : 2.55142

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37424
[1mStep[0m  [4/42], [94mLoss[0m : 2.60272
[1mStep[0m  [8/42], [94mLoss[0m : 2.61238
[1mStep[0m  [12/42], [94mLoss[0m : 2.41862
[1mStep[0m  [16/42], [94mLoss[0m : 2.58168
[1mStep[0m  [20/42], [94mLoss[0m : 2.40702
[1mStep[0m  [24/42], [94mLoss[0m : 2.41757
[1mStep[0m  [28/42], [94mLoss[0m : 2.53710
[1mStep[0m  [32/42], [94mLoss[0m : 2.39880
[1mStep[0m  [36/42], [94mLoss[0m : 2.46789
[1mStep[0m  [40/42], [94mLoss[0m : 2.49912

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65301
[1mStep[0m  [4/42], [94mLoss[0m : 2.76342
[1mStep[0m  [8/42], [94mLoss[0m : 2.45459
[1mStep[0m  [12/42], [94mLoss[0m : 2.60061
[1mStep[0m  [16/42], [94mLoss[0m : 2.35333
[1mStep[0m  [20/42], [94mLoss[0m : 2.40047
[1mStep[0m  [24/42], [94mLoss[0m : 2.56328
[1mStep[0m  [28/42], [94mLoss[0m : 2.55063
[1mStep[0m  [32/42], [94mLoss[0m : 2.39792
[1mStep[0m  [36/42], [94mLoss[0m : 2.65749
[1mStep[0m  [40/42], [94mLoss[0m : 2.37285

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.3373228141239712
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.62922
[1mStep[0m  [4/42], [94mLoss[0m : 2.54027
[1mStep[0m  [8/42], [94mLoss[0m : 2.71124
[1mStep[0m  [12/42], [94mLoss[0m : 2.48460
[1mStep[0m  [16/42], [94mLoss[0m : 2.29414
[1mStep[0m  [20/42], [94mLoss[0m : 2.54723
[1mStep[0m  [24/42], [94mLoss[0m : 2.61336
[1mStep[0m  [28/42], [94mLoss[0m : 2.59271
[1mStep[0m  [32/42], [94mLoss[0m : 2.54042
[1mStep[0m  [36/42], [94mLoss[0m : 2.64634
[1mStep[0m  [40/42], [94mLoss[0m : 2.32341

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53575
[1mStep[0m  [4/42], [94mLoss[0m : 2.54782
[1mStep[0m  [8/42], [94mLoss[0m : 2.45729
[1mStep[0m  [12/42], [94mLoss[0m : 2.62187
[1mStep[0m  [16/42], [94mLoss[0m : 2.68435
[1mStep[0m  [20/42], [94mLoss[0m : 2.41175
[1mStep[0m  [24/42], [94mLoss[0m : 2.62019
[1mStep[0m  [28/42], [94mLoss[0m : 2.34978
[1mStep[0m  [32/42], [94mLoss[0m : 2.16355
[1mStep[0m  [36/42], [94mLoss[0m : 2.39759
[1mStep[0m  [40/42], [94mLoss[0m : 2.68427

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31269
[1mStep[0m  [4/42], [94mLoss[0m : 2.35256
[1mStep[0m  [8/42], [94mLoss[0m : 2.36587
[1mStep[0m  [12/42], [94mLoss[0m : 2.77914
[1mStep[0m  [16/42], [94mLoss[0m : 2.48577
[1mStep[0m  [20/42], [94mLoss[0m : 2.53513
[1mStep[0m  [24/42], [94mLoss[0m : 2.56816
[1mStep[0m  [28/42], [94mLoss[0m : 2.74375
[1mStep[0m  [32/42], [94mLoss[0m : 2.27140
[1mStep[0m  [36/42], [94mLoss[0m : 2.50987
[1mStep[0m  [40/42], [94mLoss[0m : 2.55840

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41294
[1mStep[0m  [4/42], [94mLoss[0m : 2.40656
[1mStep[0m  [8/42], [94mLoss[0m : 2.37541
[1mStep[0m  [12/42], [94mLoss[0m : 2.45268
[1mStep[0m  [16/42], [94mLoss[0m : 2.32690
[1mStep[0m  [20/42], [94mLoss[0m : 2.36077
[1mStep[0m  [24/42], [94mLoss[0m : 2.27355
[1mStep[0m  [28/42], [94mLoss[0m : 2.36164
[1mStep[0m  [32/42], [94mLoss[0m : 2.38472
[1mStep[0m  [36/42], [94mLoss[0m : 2.26508
[1mStep[0m  [40/42], [94mLoss[0m : 2.31771

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32126
[1mStep[0m  [4/42], [94mLoss[0m : 2.69113
[1mStep[0m  [8/42], [94mLoss[0m : 2.61589
[1mStep[0m  [12/42], [94mLoss[0m : 2.27244
[1mStep[0m  [16/42], [94mLoss[0m : 2.09696
[1mStep[0m  [20/42], [94mLoss[0m : 2.28360
[1mStep[0m  [24/42], [94mLoss[0m : 2.28588
[1mStep[0m  [28/42], [94mLoss[0m : 2.18913
[1mStep[0m  [32/42], [94mLoss[0m : 2.32061
[1mStep[0m  [36/42], [94mLoss[0m : 2.26757
[1mStep[0m  [40/42], [94mLoss[0m : 2.40972

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.316, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53564
[1mStep[0m  [4/42], [94mLoss[0m : 2.38517
[1mStep[0m  [8/42], [94mLoss[0m : 2.45126
[1mStep[0m  [12/42], [94mLoss[0m : 2.41790
[1mStep[0m  [16/42], [94mLoss[0m : 2.04981
[1mStep[0m  [20/42], [94mLoss[0m : 2.28230
[1mStep[0m  [24/42], [94mLoss[0m : 2.29462
[1mStep[0m  [28/42], [94mLoss[0m : 2.47910
[1mStep[0m  [32/42], [94mLoss[0m : 2.21037
[1mStep[0m  [36/42], [94mLoss[0m : 2.35858
[1mStep[0m  [40/42], [94mLoss[0m : 2.58253

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26356
[1mStep[0m  [4/42], [94mLoss[0m : 2.16047
[1mStep[0m  [8/42], [94mLoss[0m : 2.50034
[1mStep[0m  [12/42], [94mLoss[0m : 2.29014
[1mStep[0m  [16/42], [94mLoss[0m : 2.36698
[1mStep[0m  [20/42], [94mLoss[0m : 2.31157
[1mStep[0m  [24/42], [94mLoss[0m : 2.35028
[1mStep[0m  [28/42], [94mLoss[0m : 2.40141
[1mStep[0m  [32/42], [94mLoss[0m : 2.09342
[1mStep[0m  [36/42], [94mLoss[0m : 2.27792
[1mStep[0m  [40/42], [94mLoss[0m : 2.38407

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06510
[1mStep[0m  [4/42], [94mLoss[0m : 2.18742
[1mStep[0m  [8/42], [94mLoss[0m : 2.30749
[1mStep[0m  [12/42], [94mLoss[0m : 2.07371
[1mStep[0m  [16/42], [94mLoss[0m : 2.19079
[1mStep[0m  [20/42], [94mLoss[0m : 2.22956
[1mStep[0m  [24/42], [94mLoss[0m : 2.21242
[1mStep[0m  [28/42], [94mLoss[0m : 2.35972
[1mStep[0m  [32/42], [94mLoss[0m : 2.19194
[1mStep[0m  [36/42], [94mLoss[0m : 2.39900
[1mStep[0m  [40/42], [94mLoss[0m : 2.28046

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30999
[1mStep[0m  [4/42], [94mLoss[0m : 2.12312
[1mStep[0m  [8/42], [94mLoss[0m : 2.27541
[1mStep[0m  [12/42], [94mLoss[0m : 2.18540
[1mStep[0m  [16/42], [94mLoss[0m : 2.10706
[1mStep[0m  [20/42], [94mLoss[0m : 2.17380
[1mStep[0m  [24/42], [94mLoss[0m : 2.20125
[1mStep[0m  [28/42], [94mLoss[0m : 2.22845
[1mStep[0m  [32/42], [94mLoss[0m : 2.42066
[1mStep[0m  [36/42], [94mLoss[0m : 2.45806
[1mStep[0m  [40/42], [94mLoss[0m : 2.17391

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11932
[1mStep[0m  [4/42], [94mLoss[0m : 2.28786
[1mStep[0m  [8/42], [94mLoss[0m : 2.22623
[1mStep[0m  [12/42], [94mLoss[0m : 2.11349
[1mStep[0m  [16/42], [94mLoss[0m : 2.37784
[1mStep[0m  [20/42], [94mLoss[0m : 2.17988
[1mStep[0m  [24/42], [94mLoss[0m : 2.28029
[1mStep[0m  [28/42], [94mLoss[0m : 1.98237
[1mStep[0m  [32/42], [94mLoss[0m : 2.17968
[1mStep[0m  [36/42], [94mLoss[0m : 2.13740
[1mStep[0m  [40/42], [94mLoss[0m : 2.14002

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06583
[1mStep[0m  [4/42], [94mLoss[0m : 2.27358
[1mStep[0m  [8/42], [94mLoss[0m : 2.14421
[1mStep[0m  [12/42], [94mLoss[0m : 2.10512
[1mStep[0m  [16/42], [94mLoss[0m : 2.02897
[1mStep[0m  [20/42], [94mLoss[0m : 2.07026
[1mStep[0m  [24/42], [94mLoss[0m : 1.96790
[1mStep[0m  [28/42], [94mLoss[0m : 2.11844
[1mStep[0m  [32/42], [94mLoss[0m : 2.33357
[1mStep[0m  [36/42], [94mLoss[0m : 2.00861
[1mStep[0m  [40/42], [94mLoss[0m : 2.22094

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05164
[1mStep[0m  [4/42], [94mLoss[0m : 1.90960
[1mStep[0m  [8/42], [94mLoss[0m : 1.95716
[1mStep[0m  [12/42], [94mLoss[0m : 1.98236
[1mStep[0m  [16/42], [94mLoss[0m : 1.93792
[1mStep[0m  [20/42], [94mLoss[0m : 1.99489
[1mStep[0m  [24/42], [94mLoss[0m : 2.11148
[1mStep[0m  [28/42], [94mLoss[0m : 2.01075
[1mStep[0m  [32/42], [94mLoss[0m : 2.13228
[1mStep[0m  [36/42], [94mLoss[0m : 2.06961
[1mStep[0m  [40/42], [94mLoss[0m : 2.26133

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18011
[1mStep[0m  [4/42], [94mLoss[0m : 2.07235
[1mStep[0m  [8/42], [94mLoss[0m : 2.00731
[1mStep[0m  [12/42], [94mLoss[0m : 2.18095
[1mStep[0m  [16/42], [94mLoss[0m : 1.92137
[1mStep[0m  [20/42], [94mLoss[0m : 2.24370
[1mStep[0m  [24/42], [94mLoss[0m : 2.03231
[1mStep[0m  [28/42], [94mLoss[0m : 2.15762
[1mStep[0m  [32/42], [94mLoss[0m : 2.06301
[1mStep[0m  [36/42], [94mLoss[0m : 2.18473
[1mStep[0m  [40/42], [94mLoss[0m : 2.04244

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01043
[1mStep[0m  [4/42], [94mLoss[0m : 1.93029
[1mStep[0m  [8/42], [94mLoss[0m : 1.83912
[1mStep[0m  [12/42], [94mLoss[0m : 1.94665
[1mStep[0m  [16/42], [94mLoss[0m : 2.02687
[1mStep[0m  [20/42], [94mLoss[0m : 2.07667
[1mStep[0m  [24/42], [94mLoss[0m : 1.99527
[1mStep[0m  [28/42], [94mLoss[0m : 2.05904
[1mStep[0m  [32/42], [94mLoss[0m : 2.01797
[1mStep[0m  [36/42], [94mLoss[0m : 1.93436
[1mStep[0m  [40/42], [94mLoss[0m : 2.08145

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03791
[1mStep[0m  [4/42], [94mLoss[0m : 1.94071
[1mStep[0m  [8/42], [94mLoss[0m : 2.01658
[1mStep[0m  [12/42], [94mLoss[0m : 1.85219
[1mStep[0m  [16/42], [94mLoss[0m : 1.96597
[1mStep[0m  [20/42], [94mLoss[0m : 1.93553
[1mStep[0m  [24/42], [94mLoss[0m : 1.88424
[1mStep[0m  [28/42], [94mLoss[0m : 1.79106
[1mStep[0m  [32/42], [94mLoss[0m : 1.86237
[1mStep[0m  [36/42], [94mLoss[0m : 1.99963
[1mStep[0m  [40/42], [94mLoss[0m : 2.01758

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97691
[1mStep[0m  [4/42], [94mLoss[0m : 1.89096
[1mStep[0m  [8/42], [94mLoss[0m : 1.82554
[1mStep[0m  [12/42], [94mLoss[0m : 2.07828
[1mStep[0m  [16/42], [94mLoss[0m : 1.90198
[1mStep[0m  [20/42], [94mLoss[0m : 1.91834
[1mStep[0m  [24/42], [94mLoss[0m : 1.88786
[1mStep[0m  [28/42], [94mLoss[0m : 1.93608
[1mStep[0m  [32/42], [94mLoss[0m : 1.76448
[1mStep[0m  [36/42], [94mLoss[0m : 1.75097
[1mStep[0m  [40/42], [94mLoss[0m : 2.02908

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88774
[1mStep[0m  [4/42], [94mLoss[0m : 1.78611
[1mStep[0m  [8/42], [94mLoss[0m : 1.95586
[1mStep[0m  [12/42], [94mLoss[0m : 1.76060
[1mStep[0m  [16/42], [94mLoss[0m : 1.84618
[1mStep[0m  [20/42], [94mLoss[0m : 1.84497
[1mStep[0m  [24/42], [94mLoss[0m : 1.81491
[1mStep[0m  [28/42], [94mLoss[0m : 1.88384
[1mStep[0m  [32/42], [94mLoss[0m : 1.93032
[1mStep[0m  [36/42], [94mLoss[0m : 1.86641
[1mStep[0m  [40/42], [94mLoss[0m : 1.98160

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.902, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82946
[1mStep[0m  [4/42], [94mLoss[0m : 1.99455
[1mStep[0m  [8/42], [94mLoss[0m : 1.99148
[1mStep[0m  [12/42], [94mLoss[0m : 1.71652
[1mStep[0m  [16/42], [94mLoss[0m : 2.04245
[1mStep[0m  [20/42], [94mLoss[0m : 1.74800
[1mStep[0m  [24/42], [94mLoss[0m : 1.87299
[1mStep[0m  [28/42], [94mLoss[0m : 1.90067
[1mStep[0m  [32/42], [94mLoss[0m : 1.86836
[1mStep[0m  [36/42], [94mLoss[0m : 1.88342
[1mStep[0m  [40/42], [94mLoss[0m : 1.94720

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.866, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78601
[1mStep[0m  [4/42], [94mLoss[0m : 1.75567
[1mStep[0m  [8/42], [94mLoss[0m : 1.74642
[1mStep[0m  [12/42], [94mLoss[0m : 1.84274
[1mStep[0m  [16/42], [94mLoss[0m : 1.75033
[1mStep[0m  [20/42], [94mLoss[0m : 1.61990
[1mStep[0m  [24/42], [94mLoss[0m : 1.83169
[1mStep[0m  [28/42], [94mLoss[0m : 1.84750
[1mStep[0m  [32/42], [94mLoss[0m : 1.95101
[1mStep[0m  [36/42], [94mLoss[0m : 1.79100
[1mStep[0m  [40/42], [94mLoss[0m : 1.84349

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.520, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89190
[1mStep[0m  [4/42], [94mLoss[0m : 1.94903
[1mStep[0m  [8/42], [94mLoss[0m : 1.88505
[1mStep[0m  [12/42], [94mLoss[0m : 1.74599
[1mStep[0m  [16/42], [94mLoss[0m : 1.71003
[1mStep[0m  [20/42], [94mLoss[0m : 1.87529
[1mStep[0m  [24/42], [94mLoss[0m : 1.94285
[1mStep[0m  [28/42], [94mLoss[0m : 1.78252
[1mStep[0m  [32/42], [94mLoss[0m : 2.02026
[1mStep[0m  [36/42], [94mLoss[0m : 1.69880
[1mStep[0m  [40/42], [94mLoss[0m : 1.79327

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92312
[1mStep[0m  [4/42], [94mLoss[0m : 1.71546
[1mStep[0m  [8/42], [94mLoss[0m : 1.71028
[1mStep[0m  [12/42], [94mLoss[0m : 1.90397
[1mStep[0m  [16/42], [94mLoss[0m : 1.70312
[1mStep[0m  [20/42], [94mLoss[0m : 1.79855
[1mStep[0m  [24/42], [94mLoss[0m : 1.79243
[1mStep[0m  [28/42], [94mLoss[0m : 1.74240
[1mStep[0m  [32/42], [94mLoss[0m : 1.71709
[1mStep[0m  [36/42], [94mLoss[0m : 1.83385
[1mStep[0m  [40/42], [94mLoss[0m : 1.70541

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62688
[1mStep[0m  [4/42], [94mLoss[0m : 1.85507
[1mStep[0m  [8/42], [94mLoss[0m : 1.57473
[1mStep[0m  [12/42], [94mLoss[0m : 1.63685
[1mStep[0m  [16/42], [94mLoss[0m : 1.74752
[1mStep[0m  [20/42], [94mLoss[0m : 1.71679
[1mStep[0m  [24/42], [94mLoss[0m : 1.86459
[1mStep[0m  [28/42], [94mLoss[0m : 1.71366
[1mStep[0m  [32/42], [94mLoss[0m : 1.80658
[1mStep[0m  [36/42], [94mLoss[0m : 1.65698
[1mStep[0m  [40/42], [94mLoss[0m : 1.72278

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.746, [92mTest[0m: 2.448, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72721
[1mStep[0m  [4/42], [94mLoss[0m : 1.60146
[1mStep[0m  [8/42], [94mLoss[0m : 1.58130
[1mStep[0m  [12/42], [94mLoss[0m : 1.65180
[1mStep[0m  [16/42], [94mLoss[0m : 1.72976
[1mStep[0m  [20/42], [94mLoss[0m : 1.73985
[1mStep[0m  [24/42], [94mLoss[0m : 1.68450
[1mStep[0m  [28/42], [94mLoss[0m : 1.72335
[1mStep[0m  [32/42], [94mLoss[0m : 1.58242
[1mStep[0m  [36/42], [94mLoss[0m : 1.73384
[1mStep[0m  [40/42], [94mLoss[0m : 1.84533

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.482, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76159
[1mStep[0m  [4/42], [94mLoss[0m : 1.83267
[1mStep[0m  [8/42], [94mLoss[0m : 1.64655
[1mStep[0m  [12/42], [94mLoss[0m : 1.67927
[1mStep[0m  [16/42], [94mLoss[0m : 1.51782
[1mStep[0m  [20/42], [94mLoss[0m : 1.77719
[1mStep[0m  [24/42], [94mLoss[0m : 1.68754
[1mStep[0m  [28/42], [94mLoss[0m : 1.79493
[1mStep[0m  [32/42], [94mLoss[0m : 1.65488
[1mStep[0m  [36/42], [94mLoss[0m : 1.73181
[1mStep[0m  [40/42], [94mLoss[0m : 1.78745

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63790
[1mStep[0m  [4/42], [94mLoss[0m : 1.65450
[1mStep[0m  [8/42], [94mLoss[0m : 1.66358
[1mStep[0m  [12/42], [94mLoss[0m : 1.68020
[1mStep[0m  [16/42], [94mLoss[0m : 1.68390
[1mStep[0m  [20/42], [94mLoss[0m : 1.58200
[1mStep[0m  [24/42], [94mLoss[0m : 1.84345
[1mStep[0m  [28/42], [94mLoss[0m : 1.72770
[1mStep[0m  [32/42], [94mLoss[0m : 1.80944
[1mStep[0m  [36/42], [94mLoss[0m : 1.67586
[1mStep[0m  [40/42], [94mLoss[0m : 1.74200

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.53814
[1mStep[0m  [4/42], [94mLoss[0m : 1.65940
[1mStep[0m  [8/42], [94mLoss[0m : 1.63035
[1mStep[0m  [12/42], [94mLoss[0m : 1.65546
[1mStep[0m  [16/42], [94mLoss[0m : 1.58728
[1mStep[0m  [20/42], [94mLoss[0m : 1.75549
[1mStep[0m  [24/42], [94mLoss[0m : 1.81835
[1mStep[0m  [28/42], [94mLoss[0m : 1.66506
[1mStep[0m  [32/42], [94mLoss[0m : 1.69717
[1mStep[0m  [36/42], [94mLoss[0m : 1.66814
[1mStep[0m  [40/42], [94mLoss[0m : 1.74931

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.670, [92mTest[0m: 2.441, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58253
[1mStep[0m  [4/42], [94mLoss[0m : 1.67059
[1mStep[0m  [8/42], [94mLoss[0m : 1.61762
[1mStep[0m  [12/42], [94mLoss[0m : 1.51245
[1mStep[0m  [16/42], [94mLoss[0m : 1.56853
[1mStep[0m  [20/42], [94mLoss[0m : 1.75420
[1mStep[0m  [24/42], [94mLoss[0m : 1.56792
[1mStep[0m  [28/42], [94mLoss[0m : 1.68955
[1mStep[0m  [32/42], [94mLoss[0m : 1.72548
[1mStep[0m  [36/42], [94mLoss[0m : 1.80784
[1mStep[0m  [40/42], [94mLoss[0m : 1.74880

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.634, [92mTest[0m: 2.529, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58044
[1mStep[0m  [4/42], [94mLoss[0m : 1.64065
[1mStep[0m  [8/42], [94mLoss[0m : 1.60436
[1mStep[0m  [12/42], [94mLoss[0m : 1.64231
[1mStep[0m  [16/42], [94mLoss[0m : 1.55038
[1mStep[0m  [20/42], [94mLoss[0m : 1.68790
[1mStep[0m  [24/42], [94mLoss[0m : 1.48333
[1mStep[0m  [28/42], [94mLoss[0m : 1.71303
[1mStep[0m  [32/42], [94mLoss[0m : 1.64261
[1mStep[0m  [36/42], [94mLoss[0m : 1.79836
[1mStep[0m  [40/42], [94mLoss[0m : 1.62331

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.621, [92mTest[0m: 2.548, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52604
[1mStep[0m  [4/42], [94mLoss[0m : 1.65667
[1mStep[0m  [8/42], [94mLoss[0m : 1.58553
[1mStep[0m  [12/42], [94mLoss[0m : 1.43296
[1mStep[0m  [16/42], [94mLoss[0m : 1.61003
[1mStep[0m  [20/42], [94mLoss[0m : 1.72501
[1mStep[0m  [24/42], [94mLoss[0m : 1.58734
[1mStep[0m  [28/42], [94mLoss[0m : 1.54032
[1mStep[0m  [32/42], [94mLoss[0m : 1.67797
[1mStep[0m  [36/42], [94mLoss[0m : 1.62691
[1mStep[0m  [40/42], [94mLoss[0m : 1.62890

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58053
[1mStep[0m  [4/42], [94mLoss[0m : 1.67484
[1mStep[0m  [8/42], [94mLoss[0m : 1.58856
[1mStep[0m  [12/42], [94mLoss[0m : 1.74109
[1mStep[0m  [16/42], [94mLoss[0m : 1.62795
[1mStep[0m  [20/42], [94mLoss[0m : 1.47091
[1mStep[0m  [24/42], [94mLoss[0m : 1.55197
[1mStep[0m  [28/42], [94mLoss[0m : 1.60334
[1mStep[0m  [32/42], [94mLoss[0m : 1.54249
[1mStep[0m  [36/42], [94mLoss[0m : 1.57197
[1mStep[0m  [40/42], [94mLoss[0m : 1.65495

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.481
====================================

Phase 2 - Evaluation MAE:  2.4807630436761037
MAE score P1       2.337323
MAE score P2       2.480763
loss               1.619735
learning_rate          0.01
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.16050
[1mStep[0m  [2/21], [94mLoss[0m : 10.82324
[1mStep[0m  [4/21], [94mLoss[0m : 10.87191
[1mStep[0m  [6/21], [94mLoss[0m : 10.58140
[1mStep[0m  [8/21], [94mLoss[0m : 10.53120
[1mStep[0m  [10/21], [94mLoss[0m : 10.50461
[1mStep[0m  [12/21], [94mLoss[0m : 9.96626
[1mStep[0m  [14/21], [94mLoss[0m : 9.75224
[1mStep[0m  [16/21], [94mLoss[0m : 9.23021
[1mStep[0m  [18/21], [94mLoss[0m : 9.18925
[1mStep[0m  [20/21], [94mLoss[0m : 8.64899

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.142, [92mTest[0m: 10.854, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.59130
[1mStep[0m  [2/21], [94mLoss[0m : 8.43657
[1mStep[0m  [4/21], [94mLoss[0m : 7.78219
[1mStep[0m  [6/21], [94mLoss[0m : 7.49239
[1mStep[0m  [8/21], [94mLoss[0m : 7.33870
[1mStep[0m  [10/21], [94mLoss[0m : 7.07528
[1mStep[0m  [12/21], [94mLoss[0m : 6.88020
[1mStep[0m  [14/21], [94mLoss[0m : 6.47076
[1mStep[0m  [16/21], [94mLoss[0m : 6.21444
[1mStep[0m  [18/21], [94mLoss[0m : 5.84880
[1mStep[0m  [20/21], [94mLoss[0m : 6.07969

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.104, [92mTest[0m: 8.664, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.58492
[1mStep[0m  [2/21], [94mLoss[0m : 5.56321
[1mStep[0m  [4/21], [94mLoss[0m : 4.98295
[1mStep[0m  [6/21], [94mLoss[0m : 4.82528
[1mStep[0m  [8/21], [94mLoss[0m : 4.51402
[1mStep[0m  [10/21], [94mLoss[0m : 4.22476
[1mStep[0m  [12/21], [94mLoss[0m : 3.88803
[1mStep[0m  [14/21], [94mLoss[0m : 3.63418
[1mStep[0m  [16/21], [94mLoss[0m : 3.52417
[1mStep[0m  [18/21], [94mLoss[0m : 3.46813
[1mStep[0m  [20/21], [94mLoss[0m : 3.05914

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.276, [92mTest[0m: 5.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84673
[1mStep[0m  [2/21], [94mLoss[0m : 2.71301
[1mStep[0m  [4/21], [94mLoss[0m : 2.74211
[1mStep[0m  [6/21], [94mLoss[0m : 2.65068
[1mStep[0m  [8/21], [94mLoss[0m : 2.55802
[1mStep[0m  [10/21], [94mLoss[0m : 2.60287
[1mStep[0m  [12/21], [94mLoss[0m : 2.72561
[1mStep[0m  [14/21], [94mLoss[0m : 2.66950
[1mStep[0m  [16/21], [94mLoss[0m : 2.69417
[1mStep[0m  [18/21], [94mLoss[0m : 2.53238
[1mStep[0m  [20/21], [94mLoss[0m : 2.40848

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.677, [92mTest[0m: 4.130, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52237
[1mStep[0m  [2/21], [94mLoss[0m : 2.67233
[1mStep[0m  [4/21], [94mLoss[0m : 2.58054
[1mStep[0m  [6/21], [94mLoss[0m : 2.58870
[1mStep[0m  [8/21], [94mLoss[0m : 2.60490
[1mStep[0m  [10/21], [94mLoss[0m : 2.48233
[1mStep[0m  [12/21], [94mLoss[0m : 2.47176
[1mStep[0m  [14/21], [94mLoss[0m : 2.50181
[1mStep[0m  [16/21], [94mLoss[0m : 2.45245
[1mStep[0m  [18/21], [94mLoss[0m : 2.50373
[1mStep[0m  [20/21], [94mLoss[0m : 2.63659

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.540, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46681
[1mStep[0m  [2/21], [94mLoss[0m : 2.56855
[1mStep[0m  [4/21], [94mLoss[0m : 2.48723
[1mStep[0m  [6/21], [94mLoss[0m : 2.57643
[1mStep[0m  [8/21], [94mLoss[0m : 2.49222
[1mStep[0m  [10/21], [94mLoss[0m : 2.48904
[1mStep[0m  [12/21], [94mLoss[0m : 2.49393
[1mStep[0m  [14/21], [94mLoss[0m : 2.45088
[1mStep[0m  [16/21], [94mLoss[0m : 2.57173
[1mStep[0m  [18/21], [94mLoss[0m : 2.43571
[1mStep[0m  [20/21], [94mLoss[0m : 2.39815

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50790
[1mStep[0m  [2/21], [94mLoss[0m : 2.49987
[1mStep[0m  [4/21], [94mLoss[0m : 2.58213
[1mStep[0m  [6/21], [94mLoss[0m : 2.41831
[1mStep[0m  [8/21], [94mLoss[0m : 2.46712
[1mStep[0m  [10/21], [94mLoss[0m : 2.50180
[1mStep[0m  [12/21], [94mLoss[0m : 2.45586
[1mStep[0m  [14/21], [94mLoss[0m : 2.45250
[1mStep[0m  [16/21], [94mLoss[0m : 2.53921
[1mStep[0m  [18/21], [94mLoss[0m : 2.55434
[1mStep[0m  [20/21], [94mLoss[0m : 2.35354

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59754
[1mStep[0m  [2/21], [94mLoss[0m : 2.56882
[1mStep[0m  [4/21], [94mLoss[0m : 2.33267
[1mStep[0m  [6/21], [94mLoss[0m : 2.43344
[1mStep[0m  [8/21], [94mLoss[0m : 2.31508
[1mStep[0m  [10/21], [94mLoss[0m : 2.60751
[1mStep[0m  [12/21], [94mLoss[0m : 2.54158
[1mStep[0m  [14/21], [94mLoss[0m : 2.44091
[1mStep[0m  [16/21], [94mLoss[0m : 2.40247
[1mStep[0m  [18/21], [94mLoss[0m : 2.46256
[1mStep[0m  [20/21], [94mLoss[0m : 2.29893

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42504
[1mStep[0m  [2/21], [94mLoss[0m : 2.65805
[1mStep[0m  [4/21], [94mLoss[0m : 2.23761
[1mStep[0m  [6/21], [94mLoss[0m : 2.37195
[1mStep[0m  [8/21], [94mLoss[0m : 2.44424
[1mStep[0m  [10/21], [94mLoss[0m : 2.49030
[1mStep[0m  [12/21], [94mLoss[0m : 2.39418
[1mStep[0m  [14/21], [94mLoss[0m : 2.44217
[1mStep[0m  [16/21], [94mLoss[0m : 2.56066
[1mStep[0m  [18/21], [94mLoss[0m : 2.43298
[1mStep[0m  [20/21], [94mLoss[0m : 2.45621

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29667
[1mStep[0m  [2/21], [94mLoss[0m : 2.46889
[1mStep[0m  [4/21], [94mLoss[0m : 2.27101
[1mStep[0m  [6/21], [94mLoss[0m : 2.54220
[1mStep[0m  [8/21], [94mLoss[0m : 2.48041
[1mStep[0m  [10/21], [94mLoss[0m : 2.51063
[1mStep[0m  [12/21], [94mLoss[0m : 2.32117
[1mStep[0m  [14/21], [94mLoss[0m : 2.62315
[1mStep[0m  [16/21], [94mLoss[0m : 2.46070
[1mStep[0m  [18/21], [94mLoss[0m : 2.46822
[1mStep[0m  [20/21], [94mLoss[0m : 2.36947

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26335
[1mStep[0m  [2/21], [94mLoss[0m : 2.44610
[1mStep[0m  [4/21], [94mLoss[0m : 2.47376
[1mStep[0m  [6/21], [94mLoss[0m : 2.56406
[1mStep[0m  [8/21], [94mLoss[0m : 2.49892
[1mStep[0m  [10/21], [94mLoss[0m : 2.40355
[1mStep[0m  [12/21], [94mLoss[0m : 2.31715
[1mStep[0m  [14/21], [94mLoss[0m : 2.44402
[1mStep[0m  [16/21], [94mLoss[0m : 2.27702
[1mStep[0m  [18/21], [94mLoss[0m : 2.31167
[1mStep[0m  [20/21], [94mLoss[0m : 2.46267

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41754
[1mStep[0m  [2/21], [94mLoss[0m : 2.43274
[1mStep[0m  [4/21], [94mLoss[0m : 2.40385
[1mStep[0m  [6/21], [94mLoss[0m : 2.38960
[1mStep[0m  [8/21], [94mLoss[0m : 2.44902
[1mStep[0m  [10/21], [94mLoss[0m : 2.41569
[1mStep[0m  [12/21], [94mLoss[0m : 2.34555
[1mStep[0m  [14/21], [94mLoss[0m : 2.37276
[1mStep[0m  [16/21], [94mLoss[0m : 2.60891
[1mStep[0m  [18/21], [94mLoss[0m : 2.33059
[1mStep[0m  [20/21], [94mLoss[0m : 2.45055

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36946
[1mStep[0m  [2/21], [94mLoss[0m : 2.43157
[1mStep[0m  [4/21], [94mLoss[0m : 2.33891
[1mStep[0m  [6/21], [94mLoss[0m : 2.44754
[1mStep[0m  [8/21], [94mLoss[0m : 2.39660
[1mStep[0m  [10/21], [94mLoss[0m : 2.44015
[1mStep[0m  [12/21], [94mLoss[0m : 2.43489
[1mStep[0m  [14/21], [94mLoss[0m : 2.38914
[1mStep[0m  [16/21], [94mLoss[0m : 2.39006
[1mStep[0m  [18/21], [94mLoss[0m : 2.42097
[1mStep[0m  [20/21], [94mLoss[0m : 2.53580

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58118
[1mStep[0m  [2/21], [94mLoss[0m : 2.34565
[1mStep[0m  [4/21], [94mLoss[0m : 2.33664
[1mStep[0m  [6/21], [94mLoss[0m : 2.34253
[1mStep[0m  [8/21], [94mLoss[0m : 2.27662
[1mStep[0m  [10/21], [94mLoss[0m : 2.49626
[1mStep[0m  [12/21], [94mLoss[0m : 2.34373
[1mStep[0m  [14/21], [94mLoss[0m : 2.48336
[1mStep[0m  [16/21], [94mLoss[0m : 2.33976
[1mStep[0m  [18/21], [94mLoss[0m : 2.53968
[1mStep[0m  [20/21], [94mLoss[0m : 2.37741

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50739
[1mStep[0m  [2/21], [94mLoss[0m : 2.44407
[1mStep[0m  [4/21], [94mLoss[0m : 2.53802
[1mStep[0m  [6/21], [94mLoss[0m : 2.48458
[1mStep[0m  [8/21], [94mLoss[0m : 2.29145
[1mStep[0m  [10/21], [94mLoss[0m : 2.36666
[1mStep[0m  [12/21], [94mLoss[0m : 2.29754
[1mStep[0m  [14/21], [94mLoss[0m : 2.30358
[1mStep[0m  [16/21], [94mLoss[0m : 2.38789
[1mStep[0m  [18/21], [94mLoss[0m : 2.39042
[1mStep[0m  [20/21], [94mLoss[0m : 2.33377

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46485
[1mStep[0m  [2/21], [94mLoss[0m : 2.46396
[1mStep[0m  [4/21], [94mLoss[0m : 2.39184
[1mStep[0m  [6/21], [94mLoss[0m : 2.46468
[1mStep[0m  [8/21], [94mLoss[0m : 2.41499
[1mStep[0m  [10/21], [94mLoss[0m : 2.36522
[1mStep[0m  [12/21], [94mLoss[0m : 2.51161
[1mStep[0m  [14/21], [94mLoss[0m : 2.34910
[1mStep[0m  [16/21], [94mLoss[0m : 2.35926
[1mStep[0m  [18/21], [94mLoss[0m : 2.41921
[1mStep[0m  [20/21], [94mLoss[0m : 2.33982

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28453
[1mStep[0m  [2/21], [94mLoss[0m : 2.35651
[1mStep[0m  [4/21], [94mLoss[0m : 2.36491
[1mStep[0m  [6/21], [94mLoss[0m : 2.37649
[1mStep[0m  [8/21], [94mLoss[0m : 2.41947
[1mStep[0m  [10/21], [94mLoss[0m : 2.40012
[1mStep[0m  [12/21], [94mLoss[0m : 2.43439
[1mStep[0m  [14/21], [94mLoss[0m : 2.37893
[1mStep[0m  [16/21], [94mLoss[0m : 2.49356
[1mStep[0m  [18/21], [94mLoss[0m : 2.28487
[1mStep[0m  [20/21], [94mLoss[0m : 2.48273

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45808
[1mStep[0m  [2/21], [94mLoss[0m : 2.31497
[1mStep[0m  [4/21], [94mLoss[0m : 2.50937
[1mStep[0m  [6/21], [94mLoss[0m : 2.38157
[1mStep[0m  [8/21], [94mLoss[0m : 2.43386
[1mStep[0m  [10/21], [94mLoss[0m : 2.30612
[1mStep[0m  [12/21], [94mLoss[0m : 2.50434
[1mStep[0m  [14/21], [94mLoss[0m : 2.34407
[1mStep[0m  [16/21], [94mLoss[0m : 2.38337
[1mStep[0m  [18/21], [94mLoss[0m : 2.33395
[1mStep[0m  [20/21], [94mLoss[0m : 2.36904

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37304
[1mStep[0m  [2/21], [94mLoss[0m : 2.28658
[1mStep[0m  [4/21], [94mLoss[0m : 2.19305
[1mStep[0m  [6/21], [94mLoss[0m : 2.47822
[1mStep[0m  [8/21], [94mLoss[0m : 2.53108
[1mStep[0m  [10/21], [94mLoss[0m : 2.33230
[1mStep[0m  [12/21], [94mLoss[0m : 2.50230
[1mStep[0m  [14/21], [94mLoss[0m : 2.38029
[1mStep[0m  [16/21], [94mLoss[0m : 2.37524
[1mStep[0m  [18/21], [94mLoss[0m : 2.33210
[1mStep[0m  [20/21], [94mLoss[0m : 2.36124

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32328
[1mStep[0m  [2/21], [94mLoss[0m : 2.30841
[1mStep[0m  [4/21], [94mLoss[0m : 2.42845
[1mStep[0m  [6/21], [94mLoss[0m : 2.41921
[1mStep[0m  [8/21], [94mLoss[0m : 2.30466
[1mStep[0m  [10/21], [94mLoss[0m : 2.38766
[1mStep[0m  [12/21], [94mLoss[0m : 2.36963
[1mStep[0m  [14/21], [94mLoss[0m : 2.43418
[1mStep[0m  [16/21], [94mLoss[0m : 2.14858
[1mStep[0m  [18/21], [94mLoss[0m : 2.45946
[1mStep[0m  [20/21], [94mLoss[0m : 2.43105

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50559
[1mStep[0m  [2/21], [94mLoss[0m : 2.23615
[1mStep[0m  [4/21], [94mLoss[0m : 2.31571
[1mStep[0m  [6/21], [94mLoss[0m : 2.37567
[1mStep[0m  [8/21], [94mLoss[0m : 2.40668
[1mStep[0m  [10/21], [94mLoss[0m : 2.22830
[1mStep[0m  [12/21], [94mLoss[0m : 2.43307
[1mStep[0m  [14/21], [94mLoss[0m : 2.47512
[1mStep[0m  [16/21], [94mLoss[0m : 2.30178
[1mStep[0m  [18/21], [94mLoss[0m : 2.29632
[1mStep[0m  [20/21], [94mLoss[0m : 2.40930

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35828
[1mStep[0m  [2/21], [94mLoss[0m : 2.29864
[1mStep[0m  [4/21], [94mLoss[0m : 2.33394
[1mStep[0m  [6/21], [94mLoss[0m : 2.41824
[1mStep[0m  [8/21], [94mLoss[0m : 2.28834
[1mStep[0m  [10/21], [94mLoss[0m : 2.28823
[1mStep[0m  [12/21], [94mLoss[0m : 2.48544
[1mStep[0m  [14/21], [94mLoss[0m : 2.50905
[1mStep[0m  [16/21], [94mLoss[0m : 2.33983
[1mStep[0m  [18/21], [94mLoss[0m : 2.46649
[1mStep[0m  [20/21], [94mLoss[0m : 2.46014

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42613
[1mStep[0m  [2/21], [94mLoss[0m : 2.32183
[1mStep[0m  [4/21], [94mLoss[0m : 2.34347
[1mStep[0m  [6/21], [94mLoss[0m : 2.28404
[1mStep[0m  [8/21], [94mLoss[0m : 2.31261
[1mStep[0m  [10/21], [94mLoss[0m : 2.40389
[1mStep[0m  [12/21], [94mLoss[0m : 2.27947
[1mStep[0m  [14/21], [94mLoss[0m : 2.45303
[1mStep[0m  [16/21], [94mLoss[0m : 2.35901
[1mStep[0m  [18/21], [94mLoss[0m : 2.34712
[1mStep[0m  [20/21], [94mLoss[0m : 2.30734

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26541
[1mStep[0m  [2/21], [94mLoss[0m : 2.45142
[1mStep[0m  [4/21], [94mLoss[0m : 2.37928
[1mStep[0m  [6/21], [94mLoss[0m : 2.27783
[1mStep[0m  [8/21], [94mLoss[0m : 2.25075
[1mStep[0m  [10/21], [94mLoss[0m : 2.30985
[1mStep[0m  [12/21], [94mLoss[0m : 2.36436
[1mStep[0m  [14/21], [94mLoss[0m : 2.26398
[1mStep[0m  [16/21], [94mLoss[0m : 2.40307
[1mStep[0m  [18/21], [94mLoss[0m : 2.39094
[1mStep[0m  [20/21], [94mLoss[0m : 2.39037

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34270
[1mStep[0m  [2/21], [94mLoss[0m : 2.55949
[1mStep[0m  [4/21], [94mLoss[0m : 2.31352
[1mStep[0m  [6/21], [94mLoss[0m : 2.34825
[1mStep[0m  [8/21], [94mLoss[0m : 2.27050
[1mStep[0m  [10/21], [94mLoss[0m : 2.46501
[1mStep[0m  [12/21], [94mLoss[0m : 2.43283
[1mStep[0m  [14/21], [94mLoss[0m : 2.24241
[1mStep[0m  [16/21], [94mLoss[0m : 2.27444
[1mStep[0m  [18/21], [94mLoss[0m : 2.36797
[1mStep[0m  [20/21], [94mLoss[0m : 2.38797

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38691
[1mStep[0m  [2/21], [94mLoss[0m : 2.36858
[1mStep[0m  [4/21], [94mLoss[0m : 2.16968
[1mStep[0m  [6/21], [94mLoss[0m : 2.37824
[1mStep[0m  [8/21], [94mLoss[0m : 2.40605
[1mStep[0m  [10/21], [94mLoss[0m : 2.28765
[1mStep[0m  [12/21], [94mLoss[0m : 2.41913
[1mStep[0m  [14/21], [94mLoss[0m : 2.28473
[1mStep[0m  [16/21], [94mLoss[0m : 2.41890
[1mStep[0m  [18/21], [94mLoss[0m : 2.53728
[1mStep[0m  [20/21], [94mLoss[0m : 2.51731

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26106
[1mStep[0m  [2/21], [94mLoss[0m : 2.44051
[1mStep[0m  [4/21], [94mLoss[0m : 2.27174
[1mStep[0m  [6/21], [94mLoss[0m : 2.33502
[1mStep[0m  [8/21], [94mLoss[0m : 2.48876
[1mStep[0m  [10/21], [94mLoss[0m : 2.33793
[1mStep[0m  [12/21], [94mLoss[0m : 2.34575
[1mStep[0m  [14/21], [94mLoss[0m : 2.37350
[1mStep[0m  [16/21], [94mLoss[0m : 2.27653
[1mStep[0m  [18/21], [94mLoss[0m : 2.38092
[1mStep[0m  [20/21], [94mLoss[0m : 2.38383

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33419
[1mStep[0m  [2/21], [94mLoss[0m : 2.29905
[1mStep[0m  [4/21], [94mLoss[0m : 2.32409
[1mStep[0m  [6/21], [94mLoss[0m : 2.39502
[1mStep[0m  [8/21], [94mLoss[0m : 2.27924
[1mStep[0m  [10/21], [94mLoss[0m : 2.31737
[1mStep[0m  [12/21], [94mLoss[0m : 2.37831
[1mStep[0m  [14/21], [94mLoss[0m : 2.38546
[1mStep[0m  [16/21], [94mLoss[0m : 2.36150
[1mStep[0m  [18/21], [94mLoss[0m : 2.41894
[1mStep[0m  [20/21], [94mLoss[0m : 2.24691

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28675
[1mStep[0m  [2/21], [94mLoss[0m : 2.27653
[1mStep[0m  [4/21], [94mLoss[0m : 2.44297
[1mStep[0m  [6/21], [94mLoss[0m : 2.45216
[1mStep[0m  [8/21], [94mLoss[0m : 2.34812
[1mStep[0m  [10/21], [94mLoss[0m : 2.37477
[1mStep[0m  [12/21], [94mLoss[0m : 2.20335
[1mStep[0m  [14/21], [94mLoss[0m : 2.28053
[1mStep[0m  [16/21], [94mLoss[0m : 2.36864
[1mStep[0m  [18/21], [94mLoss[0m : 2.37986
[1mStep[0m  [20/21], [94mLoss[0m : 2.32662

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.342, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29954
[1mStep[0m  [2/21], [94mLoss[0m : 2.23745
[1mStep[0m  [4/21], [94mLoss[0m : 2.46469
[1mStep[0m  [6/21], [94mLoss[0m : 2.32376
[1mStep[0m  [8/21], [94mLoss[0m : 2.18730
[1mStep[0m  [10/21], [94mLoss[0m : 2.49263
[1mStep[0m  [12/21], [94mLoss[0m : 2.34026
[1mStep[0m  [14/21], [94mLoss[0m : 2.35192
[1mStep[0m  [16/21], [94mLoss[0m : 2.22059
[1mStep[0m  [18/21], [94mLoss[0m : 2.18932
[1mStep[0m  [20/21], [94mLoss[0m : 2.32787

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.33890928540911
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.23192
[1mStep[0m  [2/21], [94mLoss[0m : 2.40673
[1mStep[0m  [4/21], [94mLoss[0m : 2.37969
[1mStep[0m  [6/21], [94mLoss[0m : 2.38003
[1mStep[0m  [8/21], [94mLoss[0m : 2.54551
[1mStep[0m  [10/21], [94mLoss[0m : 2.50226
[1mStep[0m  [12/21], [94mLoss[0m : 2.61979
[1mStep[0m  [14/21], [94mLoss[0m : 2.51052
[1mStep[0m  [16/21], [94mLoss[0m : 2.35358
[1mStep[0m  [18/21], [94mLoss[0m : 2.37872
[1mStep[0m  [20/21], [94mLoss[0m : 2.27192

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34840
[1mStep[0m  [2/21], [94mLoss[0m : 2.25822
[1mStep[0m  [4/21], [94mLoss[0m : 2.22073
[1mStep[0m  [6/21], [94mLoss[0m : 2.24519
[1mStep[0m  [8/21], [94mLoss[0m : 2.33762
[1mStep[0m  [10/21], [94mLoss[0m : 2.37415
[1mStep[0m  [12/21], [94mLoss[0m : 2.28643
[1mStep[0m  [14/21], [94mLoss[0m : 2.31252
[1mStep[0m  [16/21], [94mLoss[0m : 2.20420
[1mStep[0m  [18/21], [94mLoss[0m : 2.34516
[1mStep[0m  [20/21], [94mLoss[0m : 2.16464

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.500, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31279
[1mStep[0m  [2/21], [94mLoss[0m : 2.29470
[1mStep[0m  [4/21], [94mLoss[0m : 2.20896
[1mStep[0m  [6/21], [94mLoss[0m : 2.14084
[1mStep[0m  [8/21], [94mLoss[0m : 2.21109
[1mStep[0m  [10/21], [94mLoss[0m : 2.23242
[1mStep[0m  [12/21], [94mLoss[0m : 2.11248
[1mStep[0m  [14/21], [94mLoss[0m : 2.32863
[1mStep[0m  [16/21], [94mLoss[0m : 2.17733
[1mStep[0m  [18/21], [94mLoss[0m : 2.12150
[1mStep[0m  [20/21], [94mLoss[0m : 2.16955

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22773
[1mStep[0m  [2/21], [94mLoss[0m : 2.11081
[1mStep[0m  [4/21], [94mLoss[0m : 2.14213
[1mStep[0m  [6/21], [94mLoss[0m : 2.03450
[1mStep[0m  [8/21], [94mLoss[0m : 2.04788
[1mStep[0m  [10/21], [94mLoss[0m : 2.10696
[1mStep[0m  [12/21], [94mLoss[0m : 2.12510
[1mStep[0m  [14/21], [94mLoss[0m : 2.04309
[1mStep[0m  [16/21], [94mLoss[0m : 2.23643
[1mStep[0m  [18/21], [94mLoss[0m : 2.24354
[1mStep[0m  [20/21], [94mLoss[0m : 2.06699

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06284
[1mStep[0m  [2/21], [94mLoss[0m : 1.91957
[1mStep[0m  [4/21], [94mLoss[0m : 2.04820
[1mStep[0m  [6/21], [94mLoss[0m : 2.04334
[1mStep[0m  [8/21], [94mLoss[0m : 1.86781
[1mStep[0m  [10/21], [94mLoss[0m : 2.01270
[1mStep[0m  [12/21], [94mLoss[0m : 2.15947
[1mStep[0m  [14/21], [94mLoss[0m : 2.01872
[1mStep[0m  [16/21], [94mLoss[0m : 2.14405
[1mStep[0m  [18/21], [94mLoss[0m : 2.15964
[1mStep[0m  [20/21], [94mLoss[0m : 2.10595

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86976
[1mStep[0m  [2/21], [94mLoss[0m : 1.84945
[1mStep[0m  [4/21], [94mLoss[0m : 1.87642
[1mStep[0m  [6/21], [94mLoss[0m : 1.86010
[1mStep[0m  [8/21], [94mLoss[0m : 1.97081
[1mStep[0m  [10/21], [94mLoss[0m : 2.02012
[1mStep[0m  [12/21], [94mLoss[0m : 1.93135
[1mStep[0m  [14/21], [94mLoss[0m : 1.97017
[1mStep[0m  [16/21], [94mLoss[0m : 2.00808
[1mStep[0m  [18/21], [94mLoss[0m : 1.94263
[1mStep[0m  [20/21], [94mLoss[0m : 1.97866

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86085
[1mStep[0m  [2/21], [94mLoss[0m : 1.95317
[1mStep[0m  [4/21], [94mLoss[0m : 1.95012
[1mStep[0m  [6/21], [94mLoss[0m : 1.81665
[1mStep[0m  [8/21], [94mLoss[0m : 1.80355
[1mStep[0m  [10/21], [94mLoss[0m : 1.96995
[1mStep[0m  [12/21], [94mLoss[0m : 1.91447
[1mStep[0m  [14/21], [94mLoss[0m : 1.99224
[1mStep[0m  [16/21], [94mLoss[0m : 1.87363
[1mStep[0m  [18/21], [94mLoss[0m : 1.94525
[1mStep[0m  [20/21], [94mLoss[0m : 2.02614

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71605
[1mStep[0m  [2/21], [94mLoss[0m : 1.94975
[1mStep[0m  [4/21], [94mLoss[0m : 1.75205
[1mStep[0m  [6/21], [94mLoss[0m : 1.81815
[1mStep[0m  [8/21], [94mLoss[0m : 1.76767
[1mStep[0m  [10/21], [94mLoss[0m : 1.90091
[1mStep[0m  [12/21], [94mLoss[0m : 1.87835
[1mStep[0m  [14/21], [94mLoss[0m : 1.88744
[1mStep[0m  [16/21], [94mLoss[0m : 1.80757
[1mStep[0m  [18/21], [94mLoss[0m : 1.79837
[1mStep[0m  [20/21], [94mLoss[0m : 1.97455

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.60976
[1mStep[0m  [2/21], [94mLoss[0m : 1.64809
[1mStep[0m  [4/21], [94mLoss[0m : 1.73706
[1mStep[0m  [6/21], [94mLoss[0m : 1.60862
[1mStep[0m  [8/21], [94mLoss[0m : 1.71911
[1mStep[0m  [10/21], [94mLoss[0m : 1.63869
[1mStep[0m  [12/21], [94mLoss[0m : 1.83189
[1mStep[0m  [14/21], [94mLoss[0m : 1.66059
[1mStep[0m  [16/21], [94mLoss[0m : 1.89499
[1mStep[0m  [18/21], [94mLoss[0m : 1.72473
[1mStep[0m  [20/21], [94mLoss[0m : 2.06021

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77931
[1mStep[0m  [2/21], [94mLoss[0m : 1.63986
[1mStep[0m  [4/21], [94mLoss[0m : 1.73005
[1mStep[0m  [6/21], [94mLoss[0m : 1.73079
[1mStep[0m  [8/21], [94mLoss[0m : 1.59418
[1mStep[0m  [10/21], [94mLoss[0m : 1.74570
[1mStep[0m  [12/21], [94mLoss[0m : 1.83226
[1mStep[0m  [14/21], [94mLoss[0m : 1.75169
[1mStep[0m  [16/21], [94mLoss[0m : 1.68164
[1mStep[0m  [18/21], [94mLoss[0m : 1.71785
[1mStep[0m  [20/21], [94mLoss[0m : 1.76250

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.56273
[1mStep[0m  [2/21], [94mLoss[0m : 1.60854
[1mStep[0m  [4/21], [94mLoss[0m : 1.67343
[1mStep[0m  [6/21], [94mLoss[0m : 1.68088
[1mStep[0m  [8/21], [94mLoss[0m : 1.56775
[1mStep[0m  [10/21], [94mLoss[0m : 1.72574
[1mStep[0m  [12/21], [94mLoss[0m : 1.66721
[1mStep[0m  [14/21], [94mLoss[0m : 1.63174
[1mStep[0m  [16/21], [94mLoss[0m : 1.75453
[1mStep[0m  [18/21], [94mLoss[0m : 1.67372
[1mStep[0m  [20/21], [94mLoss[0m : 1.66873

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.540, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.49019
[1mStep[0m  [2/21], [94mLoss[0m : 1.56478
[1mStep[0m  [4/21], [94mLoss[0m : 1.56702
[1mStep[0m  [6/21], [94mLoss[0m : 1.63442
[1mStep[0m  [8/21], [94mLoss[0m : 1.60636
[1mStep[0m  [10/21], [94mLoss[0m : 1.59057
[1mStep[0m  [12/21], [94mLoss[0m : 1.59121
[1mStep[0m  [14/21], [94mLoss[0m : 1.60235
[1mStep[0m  [16/21], [94mLoss[0m : 1.58080
[1mStep[0m  [18/21], [94mLoss[0m : 1.69171
[1mStep[0m  [20/21], [94mLoss[0m : 1.67922

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.622, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.47526
[1mStep[0m  [2/21], [94mLoss[0m : 1.50626
[1mStep[0m  [4/21], [94mLoss[0m : 1.52668
[1mStep[0m  [6/21], [94mLoss[0m : 1.48204
[1mStep[0m  [8/21], [94mLoss[0m : 1.54537
[1mStep[0m  [10/21], [94mLoss[0m : 1.48469
[1mStep[0m  [12/21], [94mLoss[0m : 1.62528
[1mStep[0m  [14/21], [94mLoss[0m : 1.56675
[1mStep[0m  [16/21], [94mLoss[0m : 1.64700
[1mStep[0m  [18/21], [94mLoss[0m : 1.60889
[1mStep[0m  [20/21], [94mLoss[0m : 1.56097

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.48197
[1mStep[0m  [2/21], [94mLoss[0m : 1.42877
[1mStep[0m  [4/21], [94mLoss[0m : 1.39511
[1mStep[0m  [6/21], [94mLoss[0m : 1.46978
[1mStep[0m  [8/21], [94mLoss[0m : 1.47555
[1mStep[0m  [10/21], [94mLoss[0m : 1.54222
[1mStep[0m  [12/21], [94mLoss[0m : 1.50960
[1mStep[0m  [14/21], [94mLoss[0m : 1.60166
[1mStep[0m  [16/21], [94mLoss[0m : 1.58230
[1mStep[0m  [18/21], [94mLoss[0m : 1.47616
[1mStep[0m  [20/21], [94mLoss[0m : 1.52097

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.503, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55249
[1mStep[0m  [2/21], [94mLoss[0m : 1.47665
[1mStep[0m  [4/21], [94mLoss[0m : 1.41726
[1mStep[0m  [6/21], [94mLoss[0m : 1.47999
[1mStep[0m  [8/21], [94mLoss[0m : 1.42759
[1mStep[0m  [10/21], [94mLoss[0m : 1.43053
[1mStep[0m  [12/21], [94mLoss[0m : 1.46198
[1mStep[0m  [14/21], [94mLoss[0m : 1.48596
[1mStep[0m  [16/21], [94mLoss[0m : 1.44634
[1mStep[0m  [18/21], [94mLoss[0m : 1.48847
[1mStep[0m  [20/21], [94mLoss[0m : 1.52741

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.527, [96mlr[0m: 0.01
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 14 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.456
====================================

Phase 2 - Evaluation MAE:  2.4559991700308665
MAE score P1      2.338909
MAE score P2      2.455999
loss              1.469973
learning_rate         0.01
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.00369
[1mStep[0m  [4/42], [94mLoss[0m : 8.51693
[1mStep[0m  [8/42], [94mLoss[0m : 4.09565
[1mStep[0m  [12/42], [94mLoss[0m : 3.46809
[1mStep[0m  [16/42], [94mLoss[0m : 4.55342
[1mStep[0m  [20/42], [94mLoss[0m : 3.66742
[1mStep[0m  [24/42], [94mLoss[0m : 2.71550
[1mStep[0m  [28/42], [94mLoss[0m : 2.81209
[1mStep[0m  [32/42], [94mLoss[0m : 2.90558
[1mStep[0m  [36/42], [94mLoss[0m : 2.54008
[1mStep[0m  [40/42], [94mLoss[0m : 2.73447

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.256, [92mTest[0m: 11.134, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86626
[1mStep[0m  [4/42], [94mLoss[0m : 2.63541
[1mStep[0m  [8/42], [94mLoss[0m : 2.43255
[1mStep[0m  [12/42], [94mLoss[0m : 2.95738
[1mStep[0m  [16/42], [94mLoss[0m : 2.66524
[1mStep[0m  [20/42], [94mLoss[0m : 2.52299
[1mStep[0m  [24/42], [94mLoss[0m : 2.69478
[1mStep[0m  [28/42], [94mLoss[0m : 2.44710
[1mStep[0m  [32/42], [94mLoss[0m : 2.54173
[1mStep[0m  [36/42], [94mLoss[0m : 2.64485
[1mStep[0m  [40/42], [94mLoss[0m : 2.54030

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.708, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68977
[1mStep[0m  [4/42], [94mLoss[0m : 2.55980
[1mStep[0m  [8/42], [94mLoss[0m : 2.62088
[1mStep[0m  [12/42], [94mLoss[0m : 2.71397
[1mStep[0m  [16/42], [94mLoss[0m : 2.40052
[1mStep[0m  [20/42], [94mLoss[0m : 2.59368
[1mStep[0m  [24/42], [94mLoss[0m : 2.66249
[1mStep[0m  [28/42], [94mLoss[0m : 2.55190
[1mStep[0m  [32/42], [94mLoss[0m : 2.63214
[1mStep[0m  [36/42], [94mLoss[0m : 2.62528
[1mStep[0m  [40/42], [94mLoss[0m : 2.44095

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55254
[1mStep[0m  [4/42], [94mLoss[0m : 2.46256
[1mStep[0m  [8/42], [94mLoss[0m : 2.56463
[1mStep[0m  [12/42], [94mLoss[0m : 2.37617
[1mStep[0m  [16/42], [94mLoss[0m : 2.62505
[1mStep[0m  [20/42], [94mLoss[0m : 2.57163
[1mStep[0m  [24/42], [94mLoss[0m : 2.39794
[1mStep[0m  [28/42], [94mLoss[0m : 2.68559
[1mStep[0m  [32/42], [94mLoss[0m : 2.66394
[1mStep[0m  [36/42], [94mLoss[0m : 2.70016
[1mStep[0m  [40/42], [94mLoss[0m : 2.51923

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46268
[1mStep[0m  [4/42], [94mLoss[0m : 2.52076
[1mStep[0m  [8/42], [94mLoss[0m : 2.43976
[1mStep[0m  [12/42], [94mLoss[0m : 2.68628
[1mStep[0m  [16/42], [94mLoss[0m : 2.51357
[1mStep[0m  [20/42], [94mLoss[0m : 2.48828
[1mStep[0m  [24/42], [94mLoss[0m : 2.43971
[1mStep[0m  [28/42], [94mLoss[0m : 2.63822
[1mStep[0m  [32/42], [94mLoss[0m : 2.60110
[1mStep[0m  [36/42], [94mLoss[0m : 2.56719
[1mStep[0m  [40/42], [94mLoss[0m : 2.63580

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71359
[1mStep[0m  [4/42], [94mLoss[0m : 2.61802
[1mStep[0m  [8/42], [94mLoss[0m : 2.54879
[1mStep[0m  [12/42], [94mLoss[0m : 2.33378
[1mStep[0m  [16/42], [94mLoss[0m : 2.58442
[1mStep[0m  [20/42], [94mLoss[0m : 2.56525
[1mStep[0m  [24/42], [94mLoss[0m : 2.63476
[1mStep[0m  [28/42], [94mLoss[0m : 2.61713
[1mStep[0m  [32/42], [94mLoss[0m : 2.56008
[1mStep[0m  [36/42], [94mLoss[0m : 2.52233
[1mStep[0m  [40/42], [94mLoss[0m : 2.69511

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43226
[1mStep[0m  [4/42], [94mLoss[0m : 2.38531
[1mStep[0m  [8/42], [94mLoss[0m : 2.37616
[1mStep[0m  [12/42], [94mLoss[0m : 2.54109
[1mStep[0m  [16/42], [94mLoss[0m : 2.50500
[1mStep[0m  [20/42], [94mLoss[0m : 2.46564
[1mStep[0m  [24/42], [94mLoss[0m : 2.67281
[1mStep[0m  [28/42], [94mLoss[0m : 2.60766
[1mStep[0m  [32/42], [94mLoss[0m : 2.47008
[1mStep[0m  [36/42], [94mLoss[0m : 2.58352
[1mStep[0m  [40/42], [94mLoss[0m : 2.52568

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61099
[1mStep[0m  [4/42], [94mLoss[0m : 2.69034
[1mStep[0m  [8/42], [94mLoss[0m : 2.53744
[1mStep[0m  [12/42], [94mLoss[0m : 2.53749
[1mStep[0m  [16/42], [94mLoss[0m : 2.47242
[1mStep[0m  [20/42], [94mLoss[0m : 2.63568
[1mStep[0m  [24/42], [94mLoss[0m : 2.40669
[1mStep[0m  [28/42], [94mLoss[0m : 2.55040
[1mStep[0m  [32/42], [94mLoss[0m : 2.66777
[1mStep[0m  [36/42], [94mLoss[0m : 2.51785
[1mStep[0m  [40/42], [94mLoss[0m : 2.28360

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37313
[1mStep[0m  [4/42], [94mLoss[0m : 2.51672
[1mStep[0m  [8/42], [94mLoss[0m : 2.43110
[1mStep[0m  [12/42], [94mLoss[0m : 2.48573
[1mStep[0m  [16/42], [94mLoss[0m : 2.74666
[1mStep[0m  [20/42], [94mLoss[0m : 2.50227
[1mStep[0m  [24/42], [94mLoss[0m : 2.58968
[1mStep[0m  [28/42], [94mLoss[0m : 2.66199
[1mStep[0m  [32/42], [94mLoss[0m : 2.53986
[1mStep[0m  [36/42], [94mLoss[0m : 2.60349
[1mStep[0m  [40/42], [94mLoss[0m : 2.19633

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58584
[1mStep[0m  [4/42], [94mLoss[0m : 2.58416
[1mStep[0m  [8/42], [94mLoss[0m : 2.47455
[1mStep[0m  [12/42], [94mLoss[0m : 2.48405
[1mStep[0m  [16/42], [94mLoss[0m : 2.45952
[1mStep[0m  [20/42], [94mLoss[0m : 2.44812
[1mStep[0m  [24/42], [94mLoss[0m : 2.39993
[1mStep[0m  [28/42], [94mLoss[0m : 2.38717
[1mStep[0m  [32/42], [94mLoss[0m : 2.56894
[1mStep[0m  [36/42], [94mLoss[0m : 2.76657
[1mStep[0m  [40/42], [94mLoss[0m : 2.44866

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38337
[1mStep[0m  [4/42], [94mLoss[0m : 2.32793
[1mStep[0m  [8/42], [94mLoss[0m : 2.53411
[1mStep[0m  [12/42], [94mLoss[0m : 2.52031
[1mStep[0m  [16/42], [94mLoss[0m : 2.31320
[1mStep[0m  [20/42], [94mLoss[0m : 2.56288
[1mStep[0m  [24/42], [94mLoss[0m : 2.40773
[1mStep[0m  [28/42], [94mLoss[0m : 2.58852
[1mStep[0m  [32/42], [94mLoss[0m : 2.60126
[1mStep[0m  [36/42], [94mLoss[0m : 2.46573
[1mStep[0m  [40/42], [94mLoss[0m : 2.39810

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54716
[1mStep[0m  [4/42], [94mLoss[0m : 2.75221
[1mStep[0m  [8/42], [94mLoss[0m : 2.47295
[1mStep[0m  [12/42], [94mLoss[0m : 2.48496
[1mStep[0m  [16/42], [94mLoss[0m : 2.40905
[1mStep[0m  [20/42], [94mLoss[0m : 2.67579
[1mStep[0m  [24/42], [94mLoss[0m : 2.52026
[1mStep[0m  [28/42], [94mLoss[0m : 2.56592
[1mStep[0m  [32/42], [94mLoss[0m : 2.51667
[1mStep[0m  [36/42], [94mLoss[0m : 2.61357
[1mStep[0m  [40/42], [94mLoss[0m : 2.66106

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63279
[1mStep[0m  [4/42], [94mLoss[0m : 2.65697
[1mStep[0m  [8/42], [94mLoss[0m : 2.66843
[1mStep[0m  [12/42], [94mLoss[0m : 2.62554
[1mStep[0m  [16/42], [94mLoss[0m : 2.51376
[1mStep[0m  [20/42], [94mLoss[0m : 2.52456
[1mStep[0m  [24/42], [94mLoss[0m : 2.64800
[1mStep[0m  [28/42], [94mLoss[0m : 2.41143
[1mStep[0m  [32/42], [94mLoss[0m : 2.83413
[1mStep[0m  [36/42], [94mLoss[0m : 2.46686
[1mStep[0m  [40/42], [94mLoss[0m : 2.33638

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37208
[1mStep[0m  [4/42], [94mLoss[0m : 2.53821
[1mStep[0m  [8/42], [94mLoss[0m : 2.26838
[1mStep[0m  [12/42], [94mLoss[0m : 2.45267
[1mStep[0m  [16/42], [94mLoss[0m : 2.55079
[1mStep[0m  [20/42], [94mLoss[0m : 2.63551
[1mStep[0m  [24/42], [94mLoss[0m : 2.61996
[1mStep[0m  [28/42], [94mLoss[0m : 2.60293
[1mStep[0m  [32/42], [94mLoss[0m : 2.21689
[1mStep[0m  [36/42], [94mLoss[0m : 2.55474
[1mStep[0m  [40/42], [94mLoss[0m : 2.27823

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40714
[1mStep[0m  [4/42], [94mLoss[0m : 2.25259
[1mStep[0m  [8/42], [94mLoss[0m : 2.46435
[1mStep[0m  [12/42], [94mLoss[0m : 2.34111
[1mStep[0m  [16/42], [94mLoss[0m : 2.35196
[1mStep[0m  [20/42], [94mLoss[0m : 2.64395
[1mStep[0m  [24/42], [94mLoss[0m : 2.77788
[1mStep[0m  [28/42], [94mLoss[0m : 2.54842
[1mStep[0m  [32/42], [94mLoss[0m : 2.53922
[1mStep[0m  [36/42], [94mLoss[0m : 2.50546
[1mStep[0m  [40/42], [94mLoss[0m : 2.36613

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55925
[1mStep[0m  [4/42], [94mLoss[0m : 2.33991
[1mStep[0m  [8/42], [94mLoss[0m : 2.37783
[1mStep[0m  [12/42], [94mLoss[0m : 2.33084
[1mStep[0m  [16/42], [94mLoss[0m : 2.51716
[1mStep[0m  [20/42], [94mLoss[0m : 2.50143
[1mStep[0m  [24/42], [94mLoss[0m : 2.50027
[1mStep[0m  [28/42], [94mLoss[0m : 2.49443
[1mStep[0m  [32/42], [94mLoss[0m : 2.46055
[1mStep[0m  [36/42], [94mLoss[0m : 2.65114
[1mStep[0m  [40/42], [94mLoss[0m : 2.41265

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41511
[1mStep[0m  [4/42], [94mLoss[0m : 2.49312
[1mStep[0m  [8/42], [94mLoss[0m : 2.58267
[1mStep[0m  [12/42], [94mLoss[0m : 2.47622
[1mStep[0m  [16/42], [94mLoss[0m : 2.48771
[1mStep[0m  [20/42], [94mLoss[0m : 2.53508
[1mStep[0m  [24/42], [94mLoss[0m : 2.36567
[1mStep[0m  [28/42], [94mLoss[0m : 2.34463
[1mStep[0m  [32/42], [94mLoss[0m : 2.35207
[1mStep[0m  [36/42], [94mLoss[0m : 2.44624
[1mStep[0m  [40/42], [94mLoss[0m : 2.38981

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48643
[1mStep[0m  [4/42], [94mLoss[0m : 2.69464
[1mStep[0m  [8/42], [94mLoss[0m : 2.30768
[1mStep[0m  [12/42], [94mLoss[0m : 2.56594
[1mStep[0m  [16/42], [94mLoss[0m : 2.53203
[1mStep[0m  [20/42], [94mLoss[0m : 2.66598
[1mStep[0m  [24/42], [94mLoss[0m : 2.60049
[1mStep[0m  [28/42], [94mLoss[0m : 2.41927
[1mStep[0m  [32/42], [94mLoss[0m : 2.55081
[1mStep[0m  [36/42], [94mLoss[0m : 2.47362
[1mStep[0m  [40/42], [94mLoss[0m : 2.37703

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40599
[1mStep[0m  [4/42], [94mLoss[0m : 2.46683
[1mStep[0m  [8/42], [94mLoss[0m : 2.41774
[1mStep[0m  [12/42], [94mLoss[0m : 2.54021
[1mStep[0m  [16/42], [94mLoss[0m : 2.47597
[1mStep[0m  [20/42], [94mLoss[0m : 2.62220
[1mStep[0m  [24/42], [94mLoss[0m : 2.48194
[1mStep[0m  [28/42], [94mLoss[0m : 2.46888
[1mStep[0m  [32/42], [94mLoss[0m : 2.66578
[1mStep[0m  [36/42], [94mLoss[0m : 2.62574
[1mStep[0m  [40/42], [94mLoss[0m : 2.54413

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50385
[1mStep[0m  [4/42], [94mLoss[0m : 2.47855
[1mStep[0m  [8/42], [94mLoss[0m : 2.57054
[1mStep[0m  [12/42], [94mLoss[0m : 2.50876
[1mStep[0m  [16/42], [94mLoss[0m : 2.62078
[1mStep[0m  [20/42], [94mLoss[0m : 2.38948
[1mStep[0m  [24/42], [94mLoss[0m : 2.53844
[1mStep[0m  [28/42], [94mLoss[0m : 2.55205
[1mStep[0m  [32/42], [94mLoss[0m : 2.46138
[1mStep[0m  [36/42], [94mLoss[0m : 2.41631
[1mStep[0m  [40/42], [94mLoss[0m : 2.48167

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64385
[1mStep[0m  [4/42], [94mLoss[0m : 2.55414
[1mStep[0m  [8/42], [94mLoss[0m : 2.50509
[1mStep[0m  [12/42], [94mLoss[0m : 2.46870
[1mStep[0m  [16/42], [94mLoss[0m : 2.41413
[1mStep[0m  [20/42], [94mLoss[0m : 2.23082
[1mStep[0m  [24/42], [94mLoss[0m : 2.40276
[1mStep[0m  [28/42], [94mLoss[0m : 2.50597
[1mStep[0m  [32/42], [94mLoss[0m : 2.30008
[1mStep[0m  [36/42], [94mLoss[0m : 2.40854
[1mStep[0m  [40/42], [94mLoss[0m : 2.53687

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36586
[1mStep[0m  [4/42], [94mLoss[0m : 2.33929
[1mStep[0m  [8/42], [94mLoss[0m : 2.59595
[1mStep[0m  [12/42], [94mLoss[0m : 2.58469
[1mStep[0m  [16/42], [94mLoss[0m : 2.39865
[1mStep[0m  [20/42], [94mLoss[0m : 2.51737
[1mStep[0m  [24/42], [94mLoss[0m : 2.23919
[1mStep[0m  [28/42], [94mLoss[0m : 2.53895
[1mStep[0m  [32/42], [94mLoss[0m : 2.45532
[1mStep[0m  [36/42], [94mLoss[0m : 2.59902
[1mStep[0m  [40/42], [94mLoss[0m : 2.50240

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54093
[1mStep[0m  [4/42], [94mLoss[0m : 2.47427
[1mStep[0m  [8/42], [94mLoss[0m : 2.50707
[1mStep[0m  [12/42], [94mLoss[0m : 2.42812
[1mStep[0m  [16/42], [94mLoss[0m : 2.31609
[1mStep[0m  [20/42], [94mLoss[0m : 2.16025
[1mStep[0m  [24/42], [94mLoss[0m : 2.42109
[1mStep[0m  [28/42], [94mLoss[0m : 2.68219
[1mStep[0m  [32/42], [94mLoss[0m : 2.45008
[1mStep[0m  [36/42], [94mLoss[0m : 2.45387
[1mStep[0m  [40/42], [94mLoss[0m : 2.55418

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29230
[1mStep[0m  [4/42], [94mLoss[0m : 2.47498
[1mStep[0m  [8/42], [94mLoss[0m : 2.51616
[1mStep[0m  [12/42], [94mLoss[0m : 2.52791
[1mStep[0m  [16/42], [94mLoss[0m : 2.52691
[1mStep[0m  [20/42], [94mLoss[0m : 2.51501
[1mStep[0m  [24/42], [94mLoss[0m : 2.33271
[1mStep[0m  [28/42], [94mLoss[0m : 2.64509
[1mStep[0m  [32/42], [94mLoss[0m : 2.42222
[1mStep[0m  [36/42], [94mLoss[0m : 2.60053
[1mStep[0m  [40/42], [94mLoss[0m : 2.55222

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54100
[1mStep[0m  [4/42], [94mLoss[0m : 2.59807
[1mStep[0m  [8/42], [94mLoss[0m : 2.31580
[1mStep[0m  [12/42], [94mLoss[0m : 2.52576
[1mStep[0m  [16/42], [94mLoss[0m : 2.27807
[1mStep[0m  [20/42], [94mLoss[0m : 2.47280
[1mStep[0m  [24/42], [94mLoss[0m : 2.34639
[1mStep[0m  [28/42], [94mLoss[0m : 2.61632
[1mStep[0m  [32/42], [94mLoss[0m : 2.52693
[1mStep[0m  [36/42], [94mLoss[0m : 2.42146
[1mStep[0m  [40/42], [94mLoss[0m : 2.44818

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46192
[1mStep[0m  [4/42], [94mLoss[0m : 2.46244
[1mStep[0m  [8/42], [94mLoss[0m : 2.50293
[1mStep[0m  [12/42], [94mLoss[0m : 2.35890
[1mStep[0m  [16/42], [94mLoss[0m : 2.71725
[1mStep[0m  [20/42], [94mLoss[0m : 2.50725
[1mStep[0m  [24/42], [94mLoss[0m : 2.37701
[1mStep[0m  [28/42], [94mLoss[0m : 2.57458
[1mStep[0m  [32/42], [94mLoss[0m : 2.61115
[1mStep[0m  [36/42], [94mLoss[0m : 2.20006
[1mStep[0m  [40/42], [94mLoss[0m : 2.39666

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51616
[1mStep[0m  [4/42], [94mLoss[0m : 2.45440
[1mStep[0m  [8/42], [94mLoss[0m : 2.53721
[1mStep[0m  [12/42], [94mLoss[0m : 2.48129
[1mStep[0m  [16/42], [94mLoss[0m : 2.39249
[1mStep[0m  [20/42], [94mLoss[0m : 2.48506
[1mStep[0m  [24/42], [94mLoss[0m : 2.41373
[1mStep[0m  [28/42], [94mLoss[0m : 2.47174
[1mStep[0m  [32/42], [94mLoss[0m : 2.70728
[1mStep[0m  [36/42], [94mLoss[0m : 2.48342
[1mStep[0m  [40/42], [94mLoss[0m : 2.38134

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50309
[1mStep[0m  [4/42], [94mLoss[0m : 2.43768
[1mStep[0m  [8/42], [94mLoss[0m : 2.67245
[1mStep[0m  [12/42], [94mLoss[0m : 2.55428
[1mStep[0m  [16/42], [94mLoss[0m : 2.60195
[1mStep[0m  [20/42], [94mLoss[0m : 2.55305
[1mStep[0m  [24/42], [94mLoss[0m : 2.33011
[1mStep[0m  [28/42], [94mLoss[0m : 2.33476
[1mStep[0m  [32/42], [94mLoss[0m : 2.67285
[1mStep[0m  [36/42], [94mLoss[0m : 2.71454
[1mStep[0m  [40/42], [94mLoss[0m : 2.36279

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57258
[1mStep[0m  [4/42], [94mLoss[0m : 2.24971
[1mStep[0m  [8/42], [94mLoss[0m : 2.46831
[1mStep[0m  [12/42], [94mLoss[0m : 2.52954
[1mStep[0m  [16/42], [94mLoss[0m : 2.34456
[1mStep[0m  [20/42], [94mLoss[0m : 2.44797
[1mStep[0m  [24/42], [94mLoss[0m : 2.45546
[1mStep[0m  [28/42], [94mLoss[0m : 2.78226
[1mStep[0m  [32/42], [94mLoss[0m : 2.48641
[1mStep[0m  [36/42], [94mLoss[0m : 2.35952
[1mStep[0m  [40/42], [94mLoss[0m : 2.30708

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35327
[1mStep[0m  [4/42], [94mLoss[0m : 2.40636
[1mStep[0m  [8/42], [94mLoss[0m : 2.50042
[1mStep[0m  [12/42], [94mLoss[0m : 2.31080
[1mStep[0m  [16/42], [94mLoss[0m : 2.75372
[1mStep[0m  [20/42], [94mLoss[0m : 2.39104
[1mStep[0m  [24/42], [94mLoss[0m : 2.34842
[1mStep[0m  [28/42], [94mLoss[0m : 2.54410
[1mStep[0m  [32/42], [94mLoss[0m : 2.59428
[1mStep[0m  [36/42], [94mLoss[0m : 2.59018
[1mStep[0m  [40/42], [94mLoss[0m : 2.57404

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.3319626535688127
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.43997
[1mStep[0m  [4/42], [94mLoss[0m : 2.38277
[1mStep[0m  [8/42], [94mLoss[0m : 2.41162
[1mStep[0m  [12/42], [94mLoss[0m : 2.60492
[1mStep[0m  [16/42], [94mLoss[0m : 2.66551
[1mStep[0m  [20/42], [94mLoss[0m : 2.61642
[1mStep[0m  [24/42], [94mLoss[0m : 2.43099
[1mStep[0m  [28/42], [94mLoss[0m : 2.59304
[1mStep[0m  [32/42], [94mLoss[0m : 2.43755
[1mStep[0m  [36/42], [94mLoss[0m : 2.53788
[1mStep[0m  [40/42], [94mLoss[0m : 2.16003

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13951
[1mStep[0m  [4/42], [94mLoss[0m : 2.48903
[1mStep[0m  [8/42], [94mLoss[0m : 2.61694
[1mStep[0m  [12/42], [94mLoss[0m : 2.61036
[1mStep[0m  [16/42], [94mLoss[0m : 2.43666
[1mStep[0m  [20/42], [94mLoss[0m : 2.32093
[1mStep[0m  [24/42], [94mLoss[0m : 2.39568
[1mStep[0m  [28/42], [94mLoss[0m : 2.38083
[1mStep[0m  [32/42], [94mLoss[0m : 2.40655
[1mStep[0m  [36/42], [94mLoss[0m : 2.39946
[1mStep[0m  [40/42], [94mLoss[0m : 2.19702

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18073
[1mStep[0m  [4/42], [94mLoss[0m : 2.05363
[1mStep[0m  [8/42], [94mLoss[0m : 2.18805
[1mStep[0m  [12/42], [94mLoss[0m : 2.23547
[1mStep[0m  [16/42], [94mLoss[0m : 2.36388
[1mStep[0m  [20/42], [94mLoss[0m : 2.20533
[1mStep[0m  [24/42], [94mLoss[0m : 2.23453
[1mStep[0m  [28/42], [94mLoss[0m : 2.39637
[1mStep[0m  [32/42], [94mLoss[0m : 2.45269
[1mStep[0m  [36/42], [94mLoss[0m : 2.14144
[1mStep[0m  [40/42], [94mLoss[0m : 2.25581

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18628
[1mStep[0m  [4/42], [94mLoss[0m : 2.10936
[1mStep[0m  [8/42], [94mLoss[0m : 2.13851
[1mStep[0m  [12/42], [94mLoss[0m : 2.28054
[1mStep[0m  [16/42], [94mLoss[0m : 2.19615
[1mStep[0m  [20/42], [94mLoss[0m : 2.22165
[1mStep[0m  [24/42], [94mLoss[0m : 2.29358
[1mStep[0m  [28/42], [94mLoss[0m : 2.25710
[1mStep[0m  [32/42], [94mLoss[0m : 2.06091
[1mStep[0m  [36/42], [94mLoss[0m : 2.12559
[1mStep[0m  [40/42], [94mLoss[0m : 2.22628

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18772
[1mStep[0m  [4/42], [94mLoss[0m : 2.02284
[1mStep[0m  [8/42], [94mLoss[0m : 2.32406
[1mStep[0m  [12/42], [94mLoss[0m : 2.21034
[1mStep[0m  [16/42], [94mLoss[0m : 2.25086
[1mStep[0m  [20/42], [94mLoss[0m : 1.86615
[1mStep[0m  [24/42], [94mLoss[0m : 2.11812
[1mStep[0m  [28/42], [94mLoss[0m : 2.30414
[1mStep[0m  [32/42], [94mLoss[0m : 2.38517
[1mStep[0m  [36/42], [94mLoss[0m : 2.11513
[1mStep[0m  [40/42], [94mLoss[0m : 1.99462

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01318
[1mStep[0m  [4/42], [94mLoss[0m : 2.20947
[1mStep[0m  [8/42], [94mLoss[0m : 2.21455
[1mStep[0m  [12/42], [94mLoss[0m : 2.08195
[1mStep[0m  [16/42], [94mLoss[0m : 2.05144
[1mStep[0m  [20/42], [94mLoss[0m : 2.10189
[1mStep[0m  [24/42], [94mLoss[0m : 2.21664
[1mStep[0m  [28/42], [94mLoss[0m : 2.15251
[1mStep[0m  [32/42], [94mLoss[0m : 2.09424
[1mStep[0m  [36/42], [94mLoss[0m : 2.03116
[1mStep[0m  [40/42], [94mLoss[0m : 2.09363

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08952
[1mStep[0m  [4/42], [94mLoss[0m : 1.95119
[1mStep[0m  [8/42], [94mLoss[0m : 2.13378
[1mStep[0m  [12/42], [94mLoss[0m : 2.24220
[1mStep[0m  [16/42], [94mLoss[0m : 1.99691
[1mStep[0m  [20/42], [94mLoss[0m : 2.20680
[1mStep[0m  [24/42], [94mLoss[0m : 1.93883
[1mStep[0m  [28/42], [94mLoss[0m : 1.92590
[1mStep[0m  [32/42], [94mLoss[0m : 1.97761
[1mStep[0m  [36/42], [94mLoss[0m : 1.92499
[1mStep[0m  [40/42], [94mLoss[0m : 2.05445

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03105
[1mStep[0m  [4/42], [94mLoss[0m : 1.98508
[1mStep[0m  [8/42], [94mLoss[0m : 2.02348
[1mStep[0m  [12/42], [94mLoss[0m : 1.82249
[1mStep[0m  [16/42], [94mLoss[0m : 1.83327
[1mStep[0m  [20/42], [94mLoss[0m : 1.78471
[1mStep[0m  [24/42], [94mLoss[0m : 1.86613
[1mStep[0m  [28/42], [94mLoss[0m : 1.99994
[1mStep[0m  [32/42], [94mLoss[0m : 1.99579
[1mStep[0m  [36/42], [94mLoss[0m : 1.95750
[1mStep[0m  [40/42], [94mLoss[0m : 2.10919

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84912
[1mStep[0m  [4/42], [94mLoss[0m : 1.86664
[1mStep[0m  [8/42], [94mLoss[0m : 1.89596
[1mStep[0m  [12/42], [94mLoss[0m : 2.03446
[1mStep[0m  [16/42], [94mLoss[0m : 1.86975
[1mStep[0m  [20/42], [94mLoss[0m : 1.73048
[1mStep[0m  [24/42], [94mLoss[0m : 1.85845
[1mStep[0m  [28/42], [94mLoss[0m : 1.91581
[1mStep[0m  [32/42], [94mLoss[0m : 2.05090
[1mStep[0m  [36/42], [94mLoss[0m : 2.08756
[1mStep[0m  [40/42], [94mLoss[0m : 1.86540

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78057
[1mStep[0m  [4/42], [94mLoss[0m : 1.76687
[1mStep[0m  [8/42], [94mLoss[0m : 1.70393
[1mStep[0m  [12/42], [94mLoss[0m : 1.72645
[1mStep[0m  [16/42], [94mLoss[0m : 1.84065
[1mStep[0m  [20/42], [94mLoss[0m : 1.81832
[1mStep[0m  [24/42], [94mLoss[0m : 1.91931
[1mStep[0m  [28/42], [94mLoss[0m : 1.81769
[1mStep[0m  [32/42], [94mLoss[0m : 1.67000
[1mStep[0m  [36/42], [94mLoss[0m : 1.93343
[1mStep[0m  [40/42], [94mLoss[0m : 1.94553

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64796
[1mStep[0m  [4/42], [94mLoss[0m : 1.91828
[1mStep[0m  [8/42], [94mLoss[0m : 1.77186
[1mStep[0m  [12/42], [94mLoss[0m : 1.67662
[1mStep[0m  [16/42], [94mLoss[0m : 1.87284
[1mStep[0m  [20/42], [94mLoss[0m : 1.70934
[1mStep[0m  [24/42], [94mLoss[0m : 1.66762
[1mStep[0m  [28/42], [94mLoss[0m : 1.95467
[1mStep[0m  [32/42], [94mLoss[0m : 1.81341
[1mStep[0m  [36/42], [94mLoss[0m : 2.03727
[1mStep[0m  [40/42], [94mLoss[0m : 1.85471

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.807, [92mTest[0m: 2.489, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68480
[1mStep[0m  [4/42], [94mLoss[0m : 1.93032
[1mStep[0m  [8/42], [94mLoss[0m : 1.58685
[1mStep[0m  [12/42], [94mLoss[0m : 1.63287
[1mStep[0m  [16/42], [94mLoss[0m : 1.67019
[1mStep[0m  [20/42], [94mLoss[0m : 1.89733
[1mStep[0m  [24/42], [94mLoss[0m : 1.53768
[1mStep[0m  [28/42], [94mLoss[0m : 1.66694
[1mStep[0m  [32/42], [94mLoss[0m : 1.73938
[1mStep[0m  [36/42], [94mLoss[0m : 1.67399
[1mStep[0m  [40/42], [94mLoss[0m : 1.91330

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69487
[1mStep[0m  [4/42], [94mLoss[0m : 1.76690
[1mStep[0m  [8/42], [94mLoss[0m : 1.57866
[1mStep[0m  [12/42], [94mLoss[0m : 1.52229
[1mStep[0m  [16/42], [94mLoss[0m : 1.73456
[1mStep[0m  [20/42], [94mLoss[0m : 1.76942
[1mStep[0m  [24/42], [94mLoss[0m : 1.75644
[1mStep[0m  [28/42], [94mLoss[0m : 1.81966
[1mStep[0m  [32/42], [94mLoss[0m : 1.79885
[1mStep[0m  [36/42], [94mLoss[0m : 1.75483
[1mStep[0m  [40/42], [94mLoss[0m : 1.82989

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56446
[1mStep[0m  [4/42], [94mLoss[0m : 1.72264
[1mStep[0m  [8/42], [94mLoss[0m : 1.71035
[1mStep[0m  [12/42], [94mLoss[0m : 1.57003
[1mStep[0m  [16/42], [94mLoss[0m : 1.77114
[1mStep[0m  [20/42], [94mLoss[0m : 1.76992
[1mStep[0m  [24/42], [94mLoss[0m : 1.82811
[1mStep[0m  [28/42], [94mLoss[0m : 1.61068
[1mStep[0m  [32/42], [94mLoss[0m : 1.57132
[1mStep[0m  [36/42], [94mLoss[0m : 1.58824
[1mStep[0m  [40/42], [94mLoss[0m : 1.66237

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63062
[1mStep[0m  [4/42], [94mLoss[0m : 1.67636
[1mStep[0m  [8/42], [94mLoss[0m : 1.60985
[1mStep[0m  [12/42], [94mLoss[0m : 1.64136
[1mStep[0m  [16/42], [94mLoss[0m : 1.77501
[1mStep[0m  [20/42], [94mLoss[0m : 1.63745
[1mStep[0m  [24/42], [94mLoss[0m : 1.58126
[1mStep[0m  [28/42], [94mLoss[0m : 1.61832
[1mStep[0m  [32/42], [94mLoss[0m : 1.59878
[1mStep[0m  [36/42], [94mLoss[0m : 1.68328
[1mStep[0m  [40/42], [94mLoss[0m : 1.68007

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62539
[1mStep[0m  [4/42], [94mLoss[0m : 1.57687
[1mStep[0m  [8/42], [94mLoss[0m : 1.48429
[1mStep[0m  [12/42], [94mLoss[0m : 1.63648
[1mStep[0m  [16/42], [94mLoss[0m : 1.47419
[1mStep[0m  [20/42], [94mLoss[0m : 1.41730
[1mStep[0m  [24/42], [94mLoss[0m : 1.77531
[1mStep[0m  [28/42], [94mLoss[0m : 1.67432
[1mStep[0m  [32/42], [94mLoss[0m : 1.51993
[1mStep[0m  [36/42], [94mLoss[0m : 1.70111
[1mStep[0m  [40/42], [94mLoss[0m : 1.47116

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67346
[1mStep[0m  [4/42], [94mLoss[0m : 1.34691
[1mStep[0m  [8/42], [94mLoss[0m : 1.68186
[1mStep[0m  [12/42], [94mLoss[0m : 1.67913
[1mStep[0m  [16/42], [94mLoss[0m : 1.54666
[1mStep[0m  [20/42], [94mLoss[0m : 1.57104
[1mStep[0m  [24/42], [94mLoss[0m : 1.38729
[1mStep[0m  [28/42], [94mLoss[0m : 1.45589
[1mStep[0m  [32/42], [94mLoss[0m : 1.63664
[1mStep[0m  [36/42], [94mLoss[0m : 1.60772
[1mStep[0m  [40/42], [94mLoss[0m : 1.62067

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.540, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57783
[1mStep[0m  [4/42], [94mLoss[0m : 1.50543
[1mStep[0m  [8/42], [94mLoss[0m : 1.48488
[1mStep[0m  [12/42], [94mLoss[0m : 1.54272
[1mStep[0m  [16/42], [94mLoss[0m : 1.54528
[1mStep[0m  [20/42], [94mLoss[0m : 1.58948
[1mStep[0m  [24/42], [94mLoss[0m : 1.43453
[1mStep[0m  [28/42], [94mLoss[0m : 1.51287
[1mStep[0m  [32/42], [94mLoss[0m : 1.63050
[1mStep[0m  [36/42], [94mLoss[0m : 1.47070
[1mStep[0m  [40/42], [94mLoss[0m : 1.57467

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.609, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36506
[1mStep[0m  [4/42], [94mLoss[0m : 1.44651
[1mStep[0m  [8/42], [94mLoss[0m : 1.39149
[1mStep[0m  [12/42], [94mLoss[0m : 1.50846
[1mStep[0m  [16/42], [94mLoss[0m : 1.60028
[1mStep[0m  [20/42], [94mLoss[0m : 1.71187
[1mStep[0m  [24/42], [94mLoss[0m : 1.49772
[1mStep[0m  [28/42], [94mLoss[0m : 1.53625
[1mStep[0m  [32/42], [94mLoss[0m : 1.53269
[1mStep[0m  [36/42], [94mLoss[0m : 1.48195
[1mStep[0m  [40/42], [94mLoss[0m : 1.54839

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44385
[1mStep[0m  [4/42], [94mLoss[0m : 1.43697
[1mStep[0m  [8/42], [94mLoss[0m : 1.47855
[1mStep[0m  [12/42], [94mLoss[0m : 1.36275
[1mStep[0m  [16/42], [94mLoss[0m : 1.41695
[1mStep[0m  [20/42], [94mLoss[0m : 1.52113
[1mStep[0m  [24/42], [94mLoss[0m : 1.36744
[1mStep[0m  [28/42], [94mLoss[0m : 1.45706
[1mStep[0m  [32/42], [94mLoss[0m : 1.49283
[1mStep[0m  [36/42], [94mLoss[0m : 1.42374
[1mStep[0m  [40/42], [94mLoss[0m : 1.57836

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40470
[1mStep[0m  [4/42], [94mLoss[0m : 1.55257
[1mStep[0m  [8/42], [94mLoss[0m : 1.38215
[1mStep[0m  [12/42], [94mLoss[0m : 1.36504
[1mStep[0m  [16/42], [94mLoss[0m : 1.50607
[1mStep[0m  [20/42], [94mLoss[0m : 1.49681
[1mStep[0m  [24/42], [94mLoss[0m : 1.39558
[1mStep[0m  [28/42], [94mLoss[0m : 1.42614
[1mStep[0m  [32/42], [94mLoss[0m : 1.49570
[1mStep[0m  [36/42], [94mLoss[0m : 1.57889
[1mStep[0m  [40/42], [94mLoss[0m : 1.34833

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.600, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44024
[1mStep[0m  [4/42], [94mLoss[0m : 1.42634
[1mStep[0m  [8/42], [94mLoss[0m : 1.40231
[1mStep[0m  [12/42], [94mLoss[0m : 1.40121
[1mStep[0m  [16/42], [94mLoss[0m : 1.41541
[1mStep[0m  [20/42], [94mLoss[0m : 1.45038
[1mStep[0m  [24/42], [94mLoss[0m : 1.49685
[1mStep[0m  [28/42], [94mLoss[0m : 1.38961
[1mStep[0m  [32/42], [94mLoss[0m : 1.54349
[1mStep[0m  [36/42], [94mLoss[0m : 1.46725
[1mStep[0m  [40/42], [94mLoss[0m : 1.47141

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.424, [92mTest[0m: 2.552, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55128
[1mStep[0m  [4/42], [94mLoss[0m : 1.28867
[1mStep[0m  [8/42], [94mLoss[0m : 1.48917
[1mStep[0m  [12/42], [94mLoss[0m : 1.51588
[1mStep[0m  [16/42], [94mLoss[0m : 1.36479
[1mStep[0m  [20/42], [94mLoss[0m : 1.26988
[1mStep[0m  [24/42], [94mLoss[0m : 1.66837
[1mStep[0m  [28/42], [94mLoss[0m : 1.31143
[1mStep[0m  [32/42], [94mLoss[0m : 1.55919
[1mStep[0m  [36/42], [94mLoss[0m : 1.26643
[1mStep[0m  [40/42], [94mLoss[0m : 1.38833

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.551, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.531
====================================

Phase 2 - Evaluation MAE:  2.530655230794634
MAE score P1        2.331963
MAE score P2        2.530655
loss                1.410609
learning_rate           0.01
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.9
weight_decay          0.0001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.35889
[1mStep[0m  [4/42], [94mLoss[0m : 10.64155
[1mStep[0m  [8/42], [94mLoss[0m : 9.68524
[1mStep[0m  [12/42], [94mLoss[0m : 9.29908
[1mStep[0m  [16/42], [94mLoss[0m : 8.61364
[1mStep[0m  [20/42], [94mLoss[0m : 7.34637
[1mStep[0m  [24/42], [94mLoss[0m : 6.83321
[1mStep[0m  [28/42], [94mLoss[0m : 6.35580
[1mStep[0m  [32/42], [94mLoss[0m : 5.31487
[1mStep[0m  [36/42], [94mLoss[0m : 4.71594
[1mStep[0m  [40/42], [94mLoss[0m : 4.39639

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.623, [92mTest[0m: 11.030, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.72249
[1mStep[0m  [4/42], [94mLoss[0m : 3.55409
[1mStep[0m  [8/42], [94mLoss[0m : 2.85227
[1mStep[0m  [12/42], [94mLoss[0m : 2.99258
[1mStep[0m  [16/42], [94mLoss[0m : 2.80654
[1mStep[0m  [20/42], [94mLoss[0m : 2.69369
[1mStep[0m  [24/42], [94mLoss[0m : 3.01288
[1mStep[0m  [28/42], [94mLoss[0m : 2.75305
[1mStep[0m  [32/42], [94mLoss[0m : 2.78308
[1mStep[0m  [36/42], [94mLoss[0m : 2.70521
[1mStep[0m  [40/42], [94mLoss[0m : 2.82033

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.965, [92mTest[0m: 5.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55864
[1mStep[0m  [4/42], [94mLoss[0m : 2.72766
[1mStep[0m  [8/42], [94mLoss[0m : 2.73923
[1mStep[0m  [12/42], [94mLoss[0m : 2.79326
[1mStep[0m  [16/42], [94mLoss[0m : 2.74817
[1mStep[0m  [20/42], [94mLoss[0m : 2.60741
[1mStep[0m  [24/42], [94mLoss[0m : 2.83458
[1mStep[0m  [28/42], [94mLoss[0m : 2.72870
[1mStep[0m  [32/42], [94mLoss[0m : 2.54405
[1mStep[0m  [36/42], [94mLoss[0m : 2.38948
[1mStep[0m  [40/42], [94mLoss[0m : 2.64605

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.709, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61171
[1mStep[0m  [4/42], [94mLoss[0m : 2.58258
[1mStep[0m  [8/42], [94mLoss[0m : 2.68178
[1mStep[0m  [12/42], [94mLoss[0m : 2.63766
[1mStep[0m  [16/42], [94mLoss[0m : 2.66071
[1mStep[0m  [20/42], [94mLoss[0m : 2.40022
[1mStep[0m  [24/42], [94mLoss[0m : 2.61139
[1mStep[0m  [28/42], [94mLoss[0m : 2.54501
[1mStep[0m  [32/42], [94mLoss[0m : 2.72357
[1mStep[0m  [36/42], [94mLoss[0m : 2.69340
[1mStep[0m  [40/42], [94mLoss[0m : 2.57558

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61082
[1mStep[0m  [4/42], [94mLoss[0m : 2.48855
[1mStep[0m  [8/42], [94mLoss[0m : 2.68983
[1mStep[0m  [12/42], [94mLoss[0m : 2.57951
[1mStep[0m  [16/42], [94mLoss[0m : 2.71594
[1mStep[0m  [20/42], [94mLoss[0m : 2.64029
[1mStep[0m  [24/42], [94mLoss[0m : 2.64455
[1mStep[0m  [28/42], [94mLoss[0m : 2.68115
[1mStep[0m  [32/42], [94mLoss[0m : 2.71686
[1mStep[0m  [36/42], [94mLoss[0m : 2.46888
[1mStep[0m  [40/42], [94mLoss[0m : 2.60433

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75626
[1mStep[0m  [4/42], [94mLoss[0m : 2.51040
[1mStep[0m  [8/42], [94mLoss[0m : 2.41668
[1mStep[0m  [12/42], [94mLoss[0m : 2.42356
[1mStep[0m  [16/42], [94mLoss[0m : 2.50686
[1mStep[0m  [20/42], [94mLoss[0m : 2.52568
[1mStep[0m  [24/42], [94mLoss[0m : 2.86695
[1mStep[0m  [28/42], [94mLoss[0m : 2.75184
[1mStep[0m  [32/42], [94mLoss[0m : 2.39271
[1mStep[0m  [36/42], [94mLoss[0m : 2.71881
[1mStep[0m  [40/42], [94mLoss[0m : 2.56776

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73429
[1mStep[0m  [4/42], [94mLoss[0m : 2.48962
[1mStep[0m  [8/42], [94mLoss[0m : 2.61608
[1mStep[0m  [12/42], [94mLoss[0m : 2.65244
[1mStep[0m  [16/42], [94mLoss[0m : 2.64869
[1mStep[0m  [20/42], [94mLoss[0m : 2.41688
[1mStep[0m  [24/42], [94mLoss[0m : 2.50805
[1mStep[0m  [28/42], [94mLoss[0m : 2.57979
[1mStep[0m  [32/42], [94mLoss[0m : 2.76474
[1mStep[0m  [36/42], [94mLoss[0m : 2.52503
[1mStep[0m  [40/42], [94mLoss[0m : 2.54497

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38945
[1mStep[0m  [4/42], [94mLoss[0m : 2.41727
[1mStep[0m  [8/42], [94mLoss[0m : 2.92782
[1mStep[0m  [12/42], [94mLoss[0m : 2.51737
[1mStep[0m  [16/42], [94mLoss[0m : 2.51895
[1mStep[0m  [20/42], [94mLoss[0m : 2.66103
[1mStep[0m  [24/42], [94mLoss[0m : 2.33305
[1mStep[0m  [28/42], [94mLoss[0m : 2.74389
[1mStep[0m  [32/42], [94mLoss[0m : 2.66777
[1mStep[0m  [36/42], [94mLoss[0m : 2.49448
[1mStep[0m  [40/42], [94mLoss[0m : 2.63467

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48599
[1mStep[0m  [4/42], [94mLoss[0m : 2.65515
[1mStep[0m  [8/42], [94mLoss[0m : 2.47782
[1mStep[0m  [12/42], [94mLoss[0m : 2.66161
[1mStep[0m  [16/42], [94mLoss[0m : 2.72750
[1mStep[0m  [20/42], [94mLoss[0m : 2.59201
[1mStep[0m  [24/42], [94mLoss[0m : 2.57636
[1mStep[0m  [28/42], [94mLoss[0m : 2.43847
[1mStep[0m  [32/42], [94mLoss[0m : 2.49892
[1mStep[0m  [36/42], [94mLoss[0m : 2.52697
[1mStep[0m  [40/42], [94mLoss[0m : 2.30946

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45996
[1mStep[0m  [4/42], [94mLoss[0m : 2.57262
[1mStep[0m  [8/42], [94mLoss[0m : 2.46241
[1mStep[0m  [12/42], [94mLoss[0m : 2.46747
[1mStep[0m  [16/42], [94mLoss[0m : 2.71051
[1mStep[0m  [20/42], [94mLoss[0m : 2.70700
[1mStep[0m  [24/42], [94mLoss[0m : 2.47808
[1mStep[0m  [28/42], [94mLoss[0m : 2.27864
[1mStep[0m  [32/42], [94mLoss[0m : 2.43853
[1mStep[0m  [36/42], [94mLoss[0m : 2.73657
[1mStep[0m  [40/42], [94mLoss[0m : 2.48734

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55034
[1mStep[0m  [4/42], [94mLoss[0m : 2.78650
[1mStep[0m  [8/42], [94mLoss[0m : 2.40130
[1mStep[0m  [12/42], [94mLoss[0m : 2.61117
[1mStep[0m  [16/42], [94mLoss[0m : 2.25188
[1mStep[0m  [20/42], [94mLoss[0m : 2.38221
[1mStep[0m  [24/42], [94mLoss[0m : 2.64360
[1mStep[0m  [28/42], [94mLoss[0m : 2.43901
[1mStep[0m  [32/42], [94mLoss[0m : 2.69757
[1mStep[0m  [36/42], [94mLoss[0m : 2.32191
[1mStep[0m  [40/42], [94mLoss[0m : 2.49491

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60956
[1mStep[0m  [4/42], [94mLoss[0m : 2.45003
[1mStep[0m  [8/42], [94mLoss[0m : 2.45674
[1mStep[0m  [12/42], [94mLoss[0m : 2.53750
[1mStep[0m  [16/42], [94mLoss[0m : 2.29900
[1mStep[0m  [20/42], [94mLoss[0m : 2.77572
[1mStep[0m  [24/42], [94mLoss[0m : 2.49455
[1mStep[0m  [28/42], [94mLoss[0m : 2.35227
[1mStep[0m  [32/42], [94mLoss[0m : 2.30926
[1mStep[0m  [36/42], [94mLoss[0m : 2.49379
[1mStep[0m  [40/42], [94mLoss[0m : 2.55027

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41263
[1mStep[0m  [4/42], [94mLoss[0m : 2.61483
[1mStep[0m  [8/42], [94mLoss[0m : 2.60857
[1mStep[0m  [12/42], [94mLoss[0m : 2.44889
[1mStep[0m  [16/42], [94mLoss[0m : 2.53284
[1mStep[0m  [20/42], [94mLoss[0m : 2.44949
[1mStep[0m  [24/42], [94mLoss[0m : 2.37017
[1mStep[0m  [28/42], [94mLoss[0m : 2.52015
[1mStep[0m  [32/42], [94mLoss[0m : 2.48228
[1mStep[0m  [36/42], [94mLoss[0m : 2.41093
[1mStep[0m  [40/42], [94mLoss[0m : 2.39909

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41867
[1mStep[0m  [4/42], [94mLoss[0m : 2.28087
[1mStep[0m  [8/42], [94mLoss[0m : 2.38152
[1mStep[0m  [12/42], [94mLoss[0m : 2.44288
[1mStep[0m  [16/42], [94mLoss[0m : 2.63424
[1mStep[0m  [20/42], [94mLoss[0m : 2.51691
[1mStep[0m  [24/42], [94mLoss[0m : 2.53526
[1mStep[0m  [28/42], [94mLoss[0m : 2.45439
[1mStep[0m  [32/42], [94mLoss[0m : 2.47569
[1mStep[0m  [36/42], [94mLoss[0m : 2.54623
[1mStep[0m  [40/42], [94mLoss[0m : 2.43567

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51802
[1mStep[0m  [4/42], [94mLoss[0m : 2.41247
[1mStep[0m  [8/42], [94mLoss[0m : 2.50875
[1mStep[0m  [12/42], [94mLoss[0m : 2.32722
[1mStep[0m  [16/42], [94mLoss[0m : 2.32146
[1mStep[0m  [20/42], [94mLoss[0m : 2.58361
[1mStep[0m  [24/42], [94mLoss[0m : 2.37290
[1mStep[0m  [28/42], [94mLoss[0m : 2.69711
[1mStep[0m  [32/42], [94mLoss[0m : 2.52274
[1mStep[0m  [36/42], [94mLoss[0m : 2.46601
[1mStep[0m  [40/42], [94mLoss[0m : 2.41288

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33822
[1mStep[0m  [4/42], [94mLoss[0m : 2.43721
[1mStep[0m  [8/42], [94mLoss[0m : 2.74815
[1mStep[0m  [12/42], [94mLoss[0m : 2.45571
[1mStep[0m  [16/42], [94mLoss[0m : 2.56154
[1mStep[0m  [20/42], [94mLoss[0m : 2.47017
[1mStep[0m  [24/42], [94mLoss[0m : 2.30706
[1mStep[0m  [28/42], [94mLoss[0m : 2.42111
[1mStep[0m  [32/42], [94mLoss[0m : 2.45808
[1mStep[0m  [36/42], [94mLoss[0m : 2.27174
[1mStep[0m  [40/42], [94mLoss[0m : 2.40825

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32293
[1mStep[0m  [4/42], [94mLoss[0m : 2.45422
[1mStep[0m  [8/42], [94mLoss[0m : 2.48686
[1mStep[0m  [12/42], [94mLoss[0m : 2.41015
[1mStep[0m  [16/42], [94mLoss[0m : 2.58805
[1mStep[0m  [20/42], [94mLoss[0m : 2.59333
[1mStep[0m  [24/42], [94mLoss[0m : 2.42976
[1mStep[0m  [28/42], [94mLoss[0m : 2.45314
[1mStep[0m  [32/42], [94mLoss[0m : 2.59331
[1mStep[0m  [36/42], [94mLoss[0m : 2.40771
[1mStep[0m  [40/42], [94mLoss[0m : 2.51226

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48837
[1mStep[0m  [4/42], [94mLoss[0m : 2.35134
[1mStep[0m  [8/42], [94mLoss[0m : 2.29315
[1mStep[0m  [12/42], [94mLoss[0m : 2.37280
[1mStep[0m  [16/42], [94mLoss[0m : 2.32540
[1mStep[0m  [20/42], [94mLoss[0m : 2.22914
[1mStep[0m  [24/42], [94mLoss[0m : 2.19279
[1mStep[0m  [28/42], [94mLoss[0m : 2.20192
[1mStep[0m  [32/42], [94mLoss[0m : 2.38555
[1mStep[0m  [36/42], [94mLoss[0m : 2.40129
[1mStep[0m  [40/42], [94mLoss[0m : 2.48969

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55795
[1mStep[0m  [4/42], [94mLoss[0m : 2.55617
[1mStep[0m  [8/42], [94mLoss[0m : 2.45082
[1mStep[0m  [12/42], [94mLoss[0m : 2.47396
[1mStep[0m  [16/42], [94mLoss[0m : 2.53616
[1mStep[0m  [20/42], [94mLoss[0m : 2.42603
[1mStep[0m  [24/42], [94mLoss[0m : 2.45927
[1mStep[0m  [28/42], [94mLoss[0m : 2.42550
[1mStep[0m  [32/42], [94mLoss[0m : 2.57201
[1mStep[0m  [36/42], [94mLoss[0m : 2.65551
[1mStep[0m  [40/42], [94mLoss[0m : 2.15676

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28609
[1mStep[0m  [4/42], [94mLoss[0m : 2.19343
[1mStep[0m  [8/42], [94mLoss[0m : 2.32481
[1mStep[0m  [12/42], [94mLoss[0m : 2.52109
[1mStep[0m  [16/42], [94mLoss[0m : 2.41562
[1mStep[0m  [20/42], [94mLoss[0m : 2.24261
[1mStep[0m  [24/42], [94mLoss[0m : 2.36896
[1mStep[0m  [28/42], [94mLoss[0m : 2.56760
[1mStep[0m  [32/42], [94mLoss[0m : 2.58219
[1mStep[0m  [36/42], [94mLoss[0m : 2.57872
[1mStep[0m  [40/42], [94mLoss[0m : 2.63430

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50980
[1mStep[0m  [4/42], [94mLoss[0m : 2.34555
[1mStep[0m  [8/42], [94mLoss[0m : 2.40374
[1mStep[0m  [12/42], [94mLoss[0m : 2.46195
[1mStep[0m  [16/42], [94mLoss[0m : 2.49740
[1mStep[0m  [20/42], [94mLoss[0m : 2.44660
[1mStep[0m  [24/42], [94mLoss[0m : 2.35421
[1mStep[0m  [28/42], [94mLoss[0m : 2.39022
[1mStep[0m  [32/42], [94mLoss[0m : 2.35457
[1mStep[0m  [36/42], [94mLoss[0m : 2.28242
[1mStep[0m  [40/42], [94mLoss[0m : 2.50284

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38005
[1mStep[0m  [4/42], [94mLoss[0m : 2.42096
[1mStep[0m  [8/42], [94mLoss[0m : 2.42045
[1mStep[0m  [12/42], [94mLoss[0m : 2.43367
[1mStep[0m  [16/42], [94mLoss[0m : 2.53811
[1mStep[0m  [20/42], [94mLoss[0m : 2.56797
[1mStep[0m  [24/42], [94mLoss[0m : 2.20836
[1mStep[0m  [28/42], [94mLoss[0m : 2.42221
[1mStep[0m  [32/42], [94mLoss[0m : 2.29817
[1mStep[0m  [36/42], [94mLoss[0m : 2.47558
[1mStep[0m  [40/42], [94mLoss[0m : 2.47183

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37890
[1mStep[0m  [4/42], [94mLoss[0m : 2.45452
[1mStep[0m  [8/42], [94mLoss[0m : 2.21177
[1mStep[0m  [12/42], [94mLoss[0m : 2.29802
[1mStep[0m  [16/42], [94mLoss[0m : 2.52754
[1mStep[0m  [20/42], [94mLoss[0m : 2.38857
[1mStep[0m  [24/42], [94mLoss[0m : 2.46632
[1mStep[0m  [28/42], [94mLoss[0m : 2.31265
[1mStep[0m  [32/42], [94mLoss[0m : 2.42043
[1mStep[0m  [36/42], [94mLoss[0m : 2.36514
[1mStep[0m  [40/42], [94mLoss[0m : 2.40744

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35488
[1mStep[0m  [4/42], [94mLoss[0m : 2.53437
[1mStep[0m  [8/42], [94mLoss[0m : 2.35351
[1mStep[0m  [12/42], [94mLoss[0m : 2.59288
[1mStep[0m  [16/42], [94mLoss[0m : 2.17012
[1mStep[0m  [20/42], [94mLoss[0m : 2.47289
[1mStep[0m  [24/42], [94mLoss[0m : 2.58642
[1mStep[0m  [28/42], [94mLoss[0m : 2.63689
[1mStep[0m  [32/42], [94mLoss[0m : 2.34749
[1mStep[0m  [36/42], [94mLoss[0m : 2.36761
[1mStep[0m  [40/42], [94mLoss[0m : 2.40582

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55100
[1mStep[0m  [4/42], [94mLoss[0m : 2.12783
[1mStep[0m  [8/42], [94mLoss[0m : 2.38401
[1mStep[0m  [12/42], [94mLoss[0m : 2.45934
[1mStep[0m  [16/42], [94mLoss[0m : 2.58940
[1mStep[0m  [20/42], [94mLoss[0m : 2.29439
[1mStep[0m  [24/42], [94mLoss[0m : 2.55233
[1mStep[0m  [28/42], [94mLoss[0m : 2.33029
[1mStep[0m  [32/42], [94mLoss[0m : 2.30655
[1mStep[0m  [36/42], [94mLoss[0m : 2.46402
[1mStep[0m  [40/42], [94mLoss[0m : 2.48044

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19133
[1mStep[0m  [4/42], [94mLoss[0m : 2.39634
[1mStep[0m  [8/42], [94mLoss[0m : 2.40213
[1mStep[0m  [12/42], [94mLoss[0m : 2.55005
[1mStep[0m  [16/42], [94mLoss[0m : 2.61817
[1mStep[0m  [20/42], [94mLoss[0m : 2.29443
[1mStep[0m  [24/42], [94mLoss[0m : 2.32584
[1mStep[0m  [28/42], [94mLoss[0m : 2.33617
[1mStep[0m  [32/42], [94mLoss[0m : 2.44460
[1mStep[0m  [36/42], [94mLoss[0m : 2.41939
[1mStep[0m  [40/42], [94mLoss[0m : 2.37955

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47027
[1mStep[0m  [4/42], [94mLoss[0m : 2.06924
[1mStep[0m  [8/42], [94mLoss[0m : 2.83114
[1mStep[0m  [12/42], [94mLoss[0m : 2.37326
[1mStep[0m  [16/42], [94mLoss[0m : 2.32187
[1mStep[0m  [20/42], [94mLoss[0m : 2.28126
[1mStep[0m  [24/42], [94mLoss[0m : 2.40033
[1mStep[0m  [28/42], [94mLoss[0m : 2.30881
[1mStep[0m  [32/42], [94mLoss[0m : 2.36988
[1mStep[0m  [36/42], [94mLoss[0m : 2.28515
[1mStep[0m  [40/42], [94mLoss[0m : 2.21354

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48413
[1mStep[0m  [4/42], [94mLoss[0m : 2.43284
[1mStep[0m  [8/42], [94mLoss[0m : 2.47784
[1mStep[0m  [12/42], [94mLoss[0m : 2.67596
[1mStep[0m  [16/42], [94mLoss[0m : 2.46735
[1mStep[0m  [20/42], [94mLoss[0m : 2.22427
[1mStep[0m  [24/42], [94mLoss[0m : 2.23252
[1mStep[0m  [28/42], [94mLoss[0m : 2.49521
[1mStep[0m  [32/42], [94mLoss[0m : 2.48950
[1mStep[0m  [36/42], [94mLoss[0m : 2.59291
[1mStep[0m  [40/42], [94mLoss[0m : 2.40153

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36301
[1mStep[0m  [4/42], [94mLoss[0m : 2.26395
[1mStep[0m  [8/42], [94mLoss[0m : 2.45168
[1mStep[0m  [12/42], [94mLoss[0m : 2.31998
[1mStep[0m  [16/42], [94mLoss[0m : 2.40136
[1mStep[0m  [20/42], [94mLoss[0m : 2.60337
[1mStep[0m  [24/42], [94mLoss[0m : 2.31346
[1mStep[0m  [28/42], [94mLoss[0m : 2.39114
[1mStep[0m  [32/42], [94mLoss[0m : 2.48748
[1mStep[0m  [36/42], [94mLoss[0m : 2.38895
[1mStep[0m  [40/42], [94mLoss[0m : 2.38783

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49468
[1mStep[0m  [4/42], [94mLoss[0m : 2.42479
[1mStep[0m  [8/42], [94mLoss[0m : 2.45762
[1mStep[0m  [12/42], [94mLoss[0m : 2.43129
[1mStep[0m  [16/42], [94mLoss[0m : 2.27595
[1mStep[0m  [20/42], [94mLoss[0m : 2.34621
[1mStep[0m  [24/42], [94mLoss[0m : 2.24104
[1mStep[0m  [28/42], [94mLoss[0m : 2.49927
[1mStep[0m  [32/42], [94mLoss[0m : 2.37035
[1mStep[0m  [36/42], [94mLoss[0m : 2.50962
[1mStep[0m  [40/42], [94mLoss[0m : 2.41543

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.320
====================================

Phase 1 - Evaluation MAE:  2.3204824924468994
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.45645
[1mStep[0m  [4/42], [94mLoss[0m : 2.52006
[1mStep[0m  [8/42], [94mLoss[0m : 2.21500
[1mStep[0m  [12/42], [94mLoss[0m : 2.55079
[1mStep[0m  [16/42], [94mLoss[0m : 2.53288
[1mStep[0m  [20/42], [94mLoss[0m : 2.53863
[1mStep[0m  [24/42], [94mLoss[0m : 2.62016
[1mStep[0m  [28/42], [94mLoss[0m : 2.56165
[1mStep[0m  [32/42], [94mLoss[0m : 2.39195
[1mStep[0m  [36/42], [94mLoss[0m : 2.24838
[1mStep[0m  [40/42], [94mLoss[0m : 2.10349

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35508
[1mStep[0m  [4/42], [94mLoss[0m : 2.49689
[1mStep[0m  [8/42], [94mLoss[0m : 2.39861
[1mStep[0m  [12/42], [94mLoss[0m : 2.50722
[1mStep[0m  [16/42], [94mLoss[0m : 2.61673
[1mStep[0m  [20/42], [94mLoss[0m : 2.30121
[1mStep[0m  [24/42], [94mLoss[0m : 2.50998
[1mStep[0m  [28/42], [94mLoss[0m : 2.41732
[1mStep[0m  [32/42], [94mLoss[0m : 2.43559
[1mStep[0m  [36/42], [94mLoss[0m : 2.42504
[1mStep[0m  [40/42], [94mLoss[0m : 2.45279

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29948
[1mStep[0m  [4/42], [94mLoss[0m : 2.49693
[1mStep[0m  [8/42], [94mLoss[0m : 2.39061
[1mStep[0m  [12/42], [94mLoss[0m : 2.40363
[1mStep[0m  [16/42], [94mLoss[0m : 2.36685
[1mStep[0m  [20/42], [94mLoss[0m : 2.42120
[1mStep[0m  [24/42], [94mLoss[0m : 2.58726
[1mStep[0m  [28/42], [94mLoss[0m : 2.42062
[1mStep[0m  [32/42], [94mLoss[0m : 2.59113
[1mStep[0m  [36/42], [94mLoss[0m : 2.02917
[1mStep[0m  [40/42], [94mLoss[0m : 2.34511

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22268
[1mStep[0m  [4/42], [94mLoss[0m : 2.37089
[1mStep[0m  [8/42], [94mLoss[0m : 2.14421
[1mStep[0m  [12/42], [94mLoss[0m : 2.24162
[1mStep[0m  [16/42], [94mLoss[0m : 2.42722
[1mStep[0m  [20/42], [94mLoss[0m : 2.06843
[1mStep[0m  [24/42], [94mLoss[0m : 2.32451
[1mStep[0m  [28/42], [94mLoss[0m : 2.51285
[1mStep[0m  [32/42], [94mLoss[0m : 2.38191
[1mStep[0m  [36/42], [94mLoss[0m : 2.14051
[1mStep[0m  [40/42], [94mLoss[0m : 2.43953

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42645
[1mStep[0m  [4/42], [94mLoss[0m : 2.21287
[1mStep[0m  [8/42], [94mLoss[0m : 2.19318
[1mStep[0m  [12/42], [94mLoss[0m : 2.19063
[1mStep[0m  [16/42], [94mLoss[0m : 2.37385
[1mStep[0m  [20/42], [94mLoss[0m : 2.38245
[1mStep[0m  [24/42], [94mLoss[0m : 2.51160
[1mStep[0m  [28/42], [94mLoss[0m : 2.28262
[1mStep[0m  [32/42], [94mLoss[0m : 2.36788
[1mStep[0m  [36/42], [94mLoss[0m : 2.19840
[1mStep[0m  [40/42], [94mLoss[0m : 2.32387

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23473
[1mStep[0m  [4/42], [94mLoss[0m : 2.30915
[1mStep[0m  [8/42], [94mLoss[0m : 2.39366
[1mStep[0m  [12/42], [94mLoss[0m : 1.98303
[1mStep[0m  [16/42], [94mLoss[0m : 2.18963
[1mStep[0m  [20/42], [94mLoss[0m : 2.40195
[1mStep[0m  [24/42], [94mLoss[0m : 2.25076
[1mStep[0m  [28/42], [94mLoss[0m : 2.24483
[1mStep[0m  [32/42], [94mLoss[0m : 2.13969
[1mStep[0m  [36/42], [94mLoss[0m : 2.24283
[1mStep[0m  [40/42], [94mLoss[0m : 2.39329

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26739
[1mStep[0m  [4/42], [94mLoss[0m : 1.97657
[1mStep[0m  [8/42], [94mLoss[0m : 2.12986
[1mStep[0m  [12/42], [94mLoss[0m : 2.16255
[1mStep[0m  [16/42], [94mLoss[0m : 2.20408
[1mStep[0m  [20/42], [94mLoss[0m : 2.35309
[1mStep[0m  [24/42], [94mLoss[0m : 2.13924
[1mStep[0m  [28/42], [94mLoss[0m : 2.16703
[1mStep[0m  [32/42], [94mLoss[0m : 2.11936
[1mStep[0m  [36/42], [94mLoss[0m : 2.17641
[1mStep[0m  [40/42], [94mLoss[0m : 2.09268

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05690
[1mStep[0m  [4/42], [94mLoss[0m : 2.34484
[1mStep[0m  [8/42], [94mLoss[0m : 2.08423
[1mStep[0m  [12/42], [94mLoss[0m : 2.23319
[1mStep[0m  [16/42], [94mLoss[0m : 2.22835
[1mStep[0m  [20/42], [94mLoss[0m : 2.14015
[1mStep[0m  [24/42], [94mLoss[0m : 2.30552
[1mStep[0m  [28/42], [94mLoss[0m : 2.22892
[1mStep[0m  [32/42], [94mLoss[0m : 1.98171
[1mStep[0m  [36/42], [94mLoss[0m : 2.16742
[1mStep[0m  [40/42], [94mLoss[0m : 2.27215

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00846
[1mStep[0m  [4/42], [94mLoss[0m : 2.00819
[1mStep[0m  [8/42], [94mLoss[0m : 2.13238
[1mStep[0m  [12/42], [94mLoss[0m : 1.96379
[1mStep[0m  [16/42], [94mLoss[0m : 2.07854
[1mStep[0m  [20/42], [94mLoss[0m : 2.03627
[1mStep[0m  [24/42], [94mLoss[0m : 2.19487
[1mStep[0m  [28/42], [94mLoss[0m : 2.01950
[1mStep[0m  [32/42], [94mLoss[0m : 1.95365
[1mStep[0m  [36/42], [94mLoss[0m : 2.18047
[1mStep[0m  [40/42], [94mLoss[0m : 2.13289

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98428
[1mStep[0m  [4/42], [94mLoss[0m : 1.97355
[1mStep[0m  [8/42], [94mLoss[0m : 1.99943
[1mStep[0m  [12/42], [94mLoss[0m : 2.02750
[1mStep[0m  [16/42], [94mLoss[0m : 1.82045
[1mStep[0m  [20/42], [94mLoss[0m : 1.93016
[1mStep[0m  [24/42], [94mLoss[0m : 2.04698
[1mStep[0m  [28/42], [94mLoss[0m : 2.08508
[1mStep[0m  [32/42], [94mLoss[0m : 2.03106
[1mStep[0m  [36/42], [94mLoss[0m : 2.01290
[1mStep[0m  [40/42], [94mLoss[0m : 2.08955

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03253
[1mStep[0m  [4/42], [94mLoss[0m : 2.15945
[1mStep[0m  [8/42], [94mLoss[0m : 1.99158
[1mStep[0m  [12/42], [94mLoss[0m : 2.00807
[1mStep[0m  [16/42], [94mLoss[0m : 1.87960
[1mStep[0m  [20/42], [94mLoss[0m : 1.92671
[1mStep[0m  [24/42], [94mLoss[0m : 2.05648
[1mStep[0m  [28/42], [94mLoss[0m : 1.98815
[1mStep[0m  [32/42], [94mLoss[0m : 1.96983
[1mStep[0m  [36/42], [94mLoss[0m : 2.00464
[1mStep[0m  [40/42], [94mLoss[0m : 2.18133

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76743
[1mStep[0m  [4/42], [94mLoss[0m : 2.00880
[1mStep[0m  [8/42], [94mLoss[0m : 2.08414
[1mStep[0m  [12/42], [94mLoss[0m : 1.87365
[1mStep[0m  [16/42], [94mLoss[0m : 2.00264
[1mStep[0m  [20/42], [94mLoss[0m : 1.92838
[1mStep[0m  [24/42], [94mLoss[0m : 2.11213
[1mStep[0m  [28/42], [94mLoss[0m : 1.85485
[1mStep[0m  [32/42], [94mLoss[0m : 2.18210
[1mStep[0m  [36/42], [94mLoss[0m : 1.92590
[1mStep[0m  [40/42], [94mLoss[0m : 2.06153

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11915
[1mStep[0m  [4/42], [94mLoss[0m : 1.92051
[1mStep[0m  [8/42], [94mLoss[0m : 1.92328
[1mStep[0m  [12/42], [94mLoss[0m : 1.85695
[1mStep[0m  [16/42], [94mLoss[0m : 2.09689
[1mStep[0m  [20/42], [94mLoss[0m : 1.71174
[1mStep[0m  [24/42], [94mLoss[0m : 2.19720
[1mStep[0m  [28/42], [94mLoss[0m : 1.92242
[1mStep[0m  [32/42], [94mLoss[0m : 1.87202
[1mStep[0m  [36/42], [94mLoss[0m : 2.05509
[1mStep[0m  [40/42], [94mLoss[0m : 1.81390

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83400
[1mStep[0m  [4/42], [94mLoss[0m : 1.79258
[1mStep[0m  [8/42], [94mLoss[0m : 1.80138
[1mStep[0m  [12/42], [94mLoss[0m : 1.79086
[1mStep[0m  [16/42], [94mLoss[0m : 1.93590
[1mStep[0m  [20/42], [94mLoss[0m : 2.00941
[1mStep[0m  [24/42], [94mLoss[0m : 1.89381
[1mStep[0m  [28/42], [94mLoss[0m : 1.98858
[1mStep[0m  [32/42], [94mLoss[0m : 1.83349
[1mStep[0m  [36/42], [94mLoss[0m : 1.98489
[1mStep[0m  [40/42], [94mLoss[0m : 2.04281

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02517
[1mStep[0m  [4/42], [94mLoss[0m : 1.85811
[1mStep[0m  [8/42], [94mLoss[0m : 2.04246
[1mStep[0m  [12/42], [94mLoss[0m : 1.92878
[1mStep[0m  [16/42], [94mLoss[0m : 2.02448
[1mStep[0m  [20/42], [94mLoss[0m : 1.72334
[1mStep[0m  [24/42], [94mLoss[0m : 1.80930
[1mStep[0m  [28/42], [94mLoss[0m : 1.82854
[1mStep[0m  [32/42], [94mLoss[0m : 1.85654
[1mStep[0m  [36/42], [94mLoss[0m : 1.83626
[1mStep[0m  [40/42], [94mLoss[0m : 1.88385

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74026
[1mStep[0m  [4/42], [94mLoss[0m : 1.60527
[1mStep[0m  [8/42], [94mLoss[0m : 2.04098
[1mStep[0m  [12/42], [94mLoss[0m : 1.90423
[1mStep[0m  [16/42], [94mLoss[0m : 1.71552
[1mStep[0m  [20/42], [94mLoss[0m : 1.81495
[1mStep[0m  [24/42], [94mLoss[0m : 1.85535
[1mStep[0m  [28/42], [94mLoss[0m : 1.86834
[1mStep[0m  [32/42], [94mLoss[0m : 1.91844
[1mStep[0m  [36/42], [94mLoss[0m : 1.90679
[1mStep[0m  [40/42], [94mLoss[0m : 1.95476

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83397
[1mStep[0m  [4/42], [94mLoss[0m : 1.86186
[1mStep[0m  [8/42], [94mLoss[0m : 1.69169
[1mStep[0m  [12/42], [94mLoss[0m : 1.83370
[1mStep[0m  [16/42], [94mLoss[0m : 1.73957
[1mStep[0m  [20/42], [94mLoss[0m : 1.70817
[1mStep[0m  [24/42], [94mLoss[0m : 1.83533
[1mStep[0m  [28/42], [94mLoss[0m : 1.82968
[1mStep[0m  [32/42], [94mLoss[0m : 1.82151
[1mStep[0m  [36/42], [94mLoss[0m : 1.83216
[1mStep[0m  [40/42], [94mLoss[0m : 1.67530

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56995
[1mStep[0m  [4/42], [94mLoss[0m : 1.71203
[1mStep[0m  [8/42], [94mLoss[0m : 1.73071
[1mStep[0m  [12/42], [94mLoss[0m : 1.67379
[1mStep[0m  [16/42], [94mLoss[0m : 1.83812
[1mStep[0m  [20/42], [94mLoss[0m : 1.74654
[1mStep[0m  [24/42], [94mLoss[0m : 1.73438
[1mStep[0m  [28/42], [94mLoss[0m : 1.72698
[1mStep[0m  [32/42], [94mLoss[0m : 1.62129
[1mStep[0m  [36/42], [94mLoss[0m : 1.80386
[1mStep[0m  [40/42], [94mLoss[0m : 1.91383

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61958
[1mStep[0m  [4/42], [94mLoss[0m : 1.54746
[1mStep[0m  [8/42], [94mLoss[0m : 1.52204
[1mStep[0m  [12/42], [94mLoss[0m : 1.78256
[1mStep[0m  [16/42], [94mLoss[0m : 1.77328
[1mStep[0m  [20/42], [94mLoss[0m : 1.83401
[1mStep[0m  [24/42], [94mLoss[0m : 1.68346
[1mStep[0m  [28/42], [94mLoss[0m : 1.89066
[1mStep[0m  [32/42], [94mLoss[0m : 1.76099
[1mStep[0m  [36/42], [94mLoss[0m : 1.71294
[1mStep[0m  [40/42], [94mLoss[0m : 1.72210

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.545, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57771
[1mStep[0m  [4/42], [94mLoss[0m : 1.73310
[1mStep[0m  [8/42], [94mLoss[0m : 1.61751
[1mStep[0m  [12/42], [94mLoss[0m : 1.59718
[1mStep[0m  [16/42], [94mLoss[0m : 1.51128
[1mStep[0m  [20/42], [94mLoss[0m : 1.60278
[1mStep[0m  [24/42], [94mLoss[0m : 1.67716
[1mStep[0m  [28/42], [94mLoss[0m : 1.78472
[1mStep[0m  [32/42], [94mLoss[0m : 1.72568
[1mStep[0m  [36/42], [94mLoss[0m : 1.69160
[1mStep[0m  [40/42], [94mLoss[0m : 1.53558

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51281
[1mStep[0m  [4/42], [94mLoss[0m : 1.64477
[1mStep[0m  [8/42], [94mLoss[0m : 1.67954
[1mStep[0m  [12/42], [94mLoss[0m : 1.65621
[1mStep[0m  [16/42], [94mLoss[0m : 1.65353
[1mStep[0m  [20/42], [94mLoss[0m : 1.70176
[1mStep[0m  [24/42], [94mLoss[0m : 1.71922
[1mStep[0m  [28/42], [94mLoss[0m : 1.70141
[1mStep[0m  [32/42], [94mLoss[0m : 1.60148
[1mStep[0m  [36/42], [94mLoss[0m : 1.70027
[1mStep[0m  [40/42], [94mLoss[0m : 1.67013

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64950
[1mStep[0m  [4/42], [94mLoss[0m : 1.66229
[1mStep[0m  [8/42], [94mLoss[0m : 1.56412
[1mStep[0m  [12/42], [94mLoss[0m : 1.55970
[1mStep[0m  [16/42], [94mLoss[0m : 1.71157
[1mStep[0m  [20/42], [94mLoss[0m : 1.59846
[1mStep[0m  [24/42], [94mLoss[0m : 1.56762
[1mStep[0m  [28/42], [94mLoss[0m : 1.73601
[1mStep[0m  [32/42], [94mLoss[0m : 1.58696
[1mStep[0m  [36/42], [94mLoss[0m : 1.59335
[1mStep[0m  [40/42], [94mLoss[0m : 1.56126

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.462, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62209
[1mStep[0m  [4/42], [94mLoss[0m : 1.66476
[1mStep[0m  [8/42], [94mLoss[0m : 1.68324
[1mStep[0m  [12/42], [94mLoss[0m : 1.57730
[1mStep[0m  [16/42], [94mLoss[0m : 1.61378
[1mStep[0m  [20/42], [94mLoss[0m : 1.53244
[1mStep[0m  [24/42], [94mLoss[0m : 1.68407
[1mStep[0m  [28/42], [94mLoss[0m : 1.60271
[1mStep[0m  [32/42], [94mLoss[0m : 1.73257
[1mStep[0m  [36/42], [94mLoss[0m : 1.50994
[1mStep[0m  [40/42], [94mLoss[0m : 1.59647

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49429
[1mStep[0m  [4/42], [94mLoss[0m : 1.65707
[1mStep[0m  [8/42], [94mLoss[0m : 1.42713
[1mStep[0m  [12/42], [94mLoss[0m : 1.51839
[1mStep[0m  [16/42], [94mLoss[0m : 1.69026
[1mStep[0m  [20/42], [94mLoss[0m : 1.56840
[1mStep[0m  [24/42], [94mLoss[0m : 1.66088
[1mStep[0m  [28/42], [94mLoss[0m : 1.57500
[1mStep[0m  [32/42], [94mLoss[0m : 1.58092
[1mStep[0m  [36/42], [94mLoss[0m : 1.54894
[1mStep[0m  [40/42], [94mLoss[0m : 1.51231

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.576, [92mTest[0m: 2.498, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55326
[1mStep[0m  [4/42], [94mLoss[0m : 1.46925
[1mStep[0m  [8/42], [94mLoss[0m : 1.56350
[1mStep[0m  [12/42], [94mLoss[0m : 1.49307
[1mStep[0m  [16/42], [94mLoss[0m : 1.42942
[1mStep[0m  [20/42], [94mLoss[0m : 1.67361
[1mStep[0m  [24/42], [94mLoss[0m : 1.79100
[1mStep[0m  [28/42], [94mLoss[0m : 1.44513
[1mStep[0m  [32/42], [94mLoss[0m : 1.61010
[1mStep[0m  [36/42], [94mLoss[0m : 1.43105
[1mStep[0m  [40/42], [94mLoss[0m : 1.62617

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63054
[1mStep[0m  [4/42], [94mLoss[0m : 1.42189
[1mStep[0m  [8/42], [94mLoss[0m : 1.60116
[1mStep[0m  [12/42], [94mLoss[0m : 1.55506
[1mStep[0m  [16/42], [94mLoss[0m : 1.64470
[1mStep[0m  [20/42], [94mLoss[0m : 1.46519
[1mStep[0m  [24/42], [94mLoss[0m : 1.53060
[1mStep[0m  [28/42], [94mLoss[0m : 1.66310
[1mStep[0m  [32/42], [94mLoss[0m : 1.60311
[1mStep[0m  [36/42], [94mLoss[0m : 1.57173
[1mStep[0m  [40/42], [94mLoss[0m : 1.58187

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60002
[1mStep[0m  [4/42], [94mLoss[0m : 1.48269
[1mStep[0m  [8/42], [94mLoss[0m : 1.42547
[1mStep[0m  [12/42], [94mLoss[0m : 1.45745
[1mStep[0m  [16/42], [94mLoss[0m : 1.55196
[1mStep[0m  [20/42], [94mLoss[0m : 1.50331
[1mStep[0m  [24/42], [94mLoss[0m : 1.50432
[1mStep[0m  [28/42], [94mLoss[0m : 1.46668
[1mStep[0m  [32/42], [94mLoss[0m : 1.53415
[1mStep[0m  [36/42], [94mLoss[0m : 1.62548
[1mStep[0m  [40/42], [94mLoss[0m : 1.51958

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.558, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51477
[1mStep[0m  [4/42], [94mLoss[0m : 1.41897
[1mStep[0m  [8/42], [94mLoss[0m : 1.65032
[1mStep[0m  [12/42], [94mLoss[0m : 1.46265
[1mStep[0m  [16/42], [94mLoss[0m : 1.56893
[1mStep[0m  [20/42], [94mLoss[0m : 1.55903
[1mStep[0m  [24/42], [94mLoss[0m : 1.55993
[1mStep[0m  [28/42], [94mLoss[0m : 1.45983
[1mStep[0m  [32/42], [94mLoss[0m : 1.46742
[1mStep[0m  [36/42], [94mLoss[0m : 1.47130
[1mStep[0m  [40/42], [94mLoss[0m : 1.57447

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.552, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48364
[1mStep[0m  [4/42], [94mLoss[0m : 1.30369
[1mStep[0m  [8/42], [94mLoss[0m : 1.48393
[1mStep[0m  [12/42], [94mLoss[0m : 1.48156
[1mStep[0m  [16/42], [94mLoss[0m : 1.39418
[1mStep[0m  [20/42], [94mLoss[0m : 1.46754
[1mStep[0m  [24/42], [94mLoss[0m : 1.58588
[1mStep[0m  [28/42], [94mLoss[0m : 1.39815
[1mStep[0m  [32/42], [94mLoss[0m : 1.46735
[1mStep[0m  [36/42], [94mLoss[0m : 1.42087
[1mStep[0m  [40/42], [94mLoss[0m : 1.54274

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.480, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47887
[1mStep[0m  [4/42], [94mLoss[0m : 1.34974
[1mStep[0m  [8/42], [94mLoss[0m : 1.46226
[1mStep[0m  [12/42], [94mLoss[0m : 1.53040
[1mStep[0m  [16/42], [94mLoss[0m : 1.32621
[1mStep[0m  [20/42], [94mLoss[0m : 1.36433
[1mStep[0m  [24/42], [94mLoss[0m : 1.33474
[1mStep[0m  [28/42], [94mLoss[0m : 1.48691
[1mStep[0m  [32/42], [94mLoss[0m : 1.54235
[1mStep[0m  [36/42], [94mLoss[0m : 1.46262
[1mStep[0m  [40/42], [94mLoss[0m : 1.34187

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.546, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.539
====================================

Phase 2 - Evaluation MAE:  2.5385491847991943
MAE score P1       2.320482
MAE score P2       2.538549
loss               1.451509
learning_rate          0.01
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.04942
[1mStep[0m  [2/21], [94mLoss[0m : 10.75725
[1mStep[0m  [4/21], [94mLoss[0m : 10.69745
[1mStep[0m  [6/21], [94mLoss[0m : 10.82956
[1mStep[0m  [8/21], [94mLoss[0m : 10.42150
[1mStep[0m  [10/21], [94mLoss[0m : 10.59608
[1mStep[0m  [12/21], [94mLoss[0m : 10.61868
[1mStep[0m  [14/21], [94mLoss[0m : 10.81368
[1mStep[0m  [16/21], [94mLoss[0m : 10.51910
[1mStep[0m  [18/21], [94mLoss[0m : 10.36005
[1mStep[0m  [20/21], [94mLoss[0m : 10.13640

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.652, [92mTest[0m: 10.916, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.24259
[1mStep[0m  [2/21], [94mLoss[0m : 10.35383
[1mStep[0m  [4/21], [94mLoss[0m : 10.35987
[1mStep[0m  [6/21], [94mLoss[0m : 10.62035
[1mStep[0m  [8/21], [94mLoss[0m : 10.16929
[1mStep[0m  [10/21], [94mLoss[0m : 10.25894
[1mStep[0m  [12/21], [94mLoss[0m : 9.92946
[1mStep[0m  [14/21], [94mLoss[0m : 10.18216
[1mStep[0m  [16/21], [94mLoss[0m : 9.76075
[1mStep[0m  [18/21], [94mLoss[0m : 9.71510
[1mStep[0m  [20/21], [94mLoss[0m : 9.97326

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.149, [92mTest[0m: 10.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.82137
[1mStep[0m  [2/21], [94mLoss[0m : 9.87991
[1mStep[0m  [4/21], [94mLoss[0m : 9.84750
[1mStep[0m  [6/21], [94mLoss[0m : 9.85608
[1mStep[0m  [8/21], [94mLoss[0m : 9.83557
[1mStep[0m  [10/21], [94mLoss[0m : 9.65038
[1mStep[0m  [12/21], [94mLoss[0m : 9.69278
[1mStep[0m  [14/21], [94mLoss[0m : 9.46737
[1mStep[0m  [16/21], [94mLoss[0m : 9.40656
[1mStep[0m  [18/21], [94mLoss[0m : 9.46677
[1mStep[0m  [20/21], [94mLoss[0m : 9.24109

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.609, [92mTest[0m: 9.751, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.40828
[1mStep[0m  [2/21], [94mLoss[0m : 9.27172
[1mStep[0m  [4/21], [94mLoss[0m : 9.29964
[1mStep[0m  [6/21], [94mLoss[0m : 9.20217
[1mStep[0m  [8/21], [94mLoss[0m : 9.00544
[1mStep[0m  [10/21], [94mLoss[0m : 9.13620
[1mStep[0m  [12/21], [94mLoss[0m : 9.02534
[1mStep[0m  [14/21], [94mLoss[0m : 8.78793
[1mStep[0m  [16/21], [94mLoss[0m : 8.98028
[1mStep[0m  [18/21], [94mLoss[0m : 8.61146
[1mStep[0m  [20/21], [94mLoss[0m : 8.65546

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.995, [92mTest[0m: 9.112, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.43087
[1mStep[0m  [2/21], [94mLoss[0m : 8.48770
[1mStep[0m  [4/21], [94mLoss[0m : 8.33428
[1mStep[0m  [6/21], [94mLoss[0m : 8.31053
[1mStep[0m  [8/21], [94mLoss[0m : 8.14883
[1mStep[0m  [10/21], [94mLoss[0m : 8.41968
[1mStep[0m  [12/21], [94mLoss[0m : 8.08681
[1mStep[0m  [14/21], [94mLoss[0m : 8.11068
[1mStep[0m  [16/21], [94mLoss[0m : 7.97855
[1mStep[0m  [18/21], [94mLoss[0m : 7.85346
[1mStep[0m  [20/21], [94mLoss[0m : 7.75430

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.224, [92mTest[0m: 8.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.93114
[1mStep[0m  [2/21], [94mLoss[0m : 7.58041
[1mStep[0m  [4/21], [94mLoss[0m : 7.60685
[1mStep[0m  [6/21], [94mLoss[0m : 7.54425
[1mStep[0m  [8/21], [94mLoss[0m : 7.56433
[1mStep[0m  [10/21], [94mLoss[0m : 7.24656
[1mStep[0m  [12/21], [94mLoss[0m : 7.15730
[1mStep[0m  [14/21], [94mLoss[0m : 7.38767
[1mStep[0m  [16/21], [94mLoss[0m : 7.00538
[1mStep[0m  [18/21], [94mLoss[0m : 6.84300
[1mStep[0m  [20/21], [94mLoss[0m : 6.76121

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.258, [92mTest[0m: 7.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.75744
[1mStep[0m  [2/21], [94mLoss[0m : 6.93735
[1mStep[0m  [4/21], [94mLoss[0m : 6.45172
[1mStep[0m  [6/21], [94mLoss[0m : 6.45460
[1mStep[0m  [8/21], [94mLoss[0m : 6.34064
[1mStep[0m  [10/21], [94mLoss[0m : 5.96322
[1mStep[0m  [12/21], [94mLoss[0m : 5.73640
[1mStep[0m  [14/21], [94mLoss[0m : 5.98632
[1mStep[0m  [16/21], [94mLoss[0m : 6.12720
[1mStep[0m  [18/21], [94mLoss[0m : 5.85387
[1mStep[0m  [20/21], [94mLoss[0m : 5.82981

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.228, [92mTest[0m: 6.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.82340
[1mStep[0m  [2/21], [94mLoss[0m : 5.62349
[1mStep[0m  [4/21], [94mLoss[0m : 5.41995
[1mStep[0m  [6/21], [94mLoss[0m : 5.62607
[1mStep[0m  [8/21], [94mLoss[0m : 5.32448
[1mStep[0m  [10/21], [94mLoss[0m : 5.17432
[1mStep[0m  [12/21], [94mLoss[0m : 5.06707
[1mStep[0m  [14/21], [94mLoss[0m : 4.98395
[1mStep[0m  [16/21], [94mLoss[0m : 5.06598
[1mStep[0m  [18/21], [94mLoss[0m : 4.82851
[1mStep[0m  [20/21], [94mLoss[0m : 4.95678

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.283, [92mTest[0m: 5.034, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.86144
[1mStep[0m  [2/21], [94mLoss[0m : 4.66525
[1mStep[0m  [4/21], [94mLoss[0m : 4.64367
[1mStep[0m  [6/21], [94mLoss[0m : 4.59943
[1mStep[0m  [8/21], [94mLoss[0m : 4.46444
[1mStep[0m  [10/21], [94mLoss[0m : 4.09154
[1mStep[0m  [12/21], [94mLoss[0m : 4.14612
[1mStep[0m  [14/21], [94mLoss[0m : 4.08804
[1mStep[0m  [16/21], [94mLoss[0m : 4.16788
[1mStep[0m  [18/21], [94mLoss[0m : 4.01829
[1mStep[0m  [20/21], [94mLoss[0m : 3.55714

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.267, [92mTest[0m: 3.899, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.73907
[1mStep[0m  [2/21], [94mLoss[0m : 3.78088
[1mStep[0m  [4/21], [94mLoss[0m : 3.73971
[1mStep[0m  [6/21], [94mLoss[0m : 3.57322
[1mStep[0m  [8/21], [94mLoss[0m : 3.57168
[1mStep[0m  [10/21], [94mLoss[0m : 3.41128
[1mStep[0m  [12/21], [94mLoss[0m : 3.21459
[1mStep[0m  [14/21], [94mLoss[0m : 3.10036
[1mStep[0m  [16/21], [94mLoss[0m : 3.00407
[1mStep[0m  [18/21], [94mLoss[0m : 3.24701
[1mStep[0m  [20/21], [94mLoss[0m : 3.13250

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.451, [92mTest[0m: 3.044, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.08885
[1mStep[0m  [2/21], [94mLoss[0m : 3.22216
[1mStep[0m  [4/21], [94mLoss[0m : 3.35676
[1mStep[0m  [6/21], [94mLoss[0m : 3.00765
[1mStep[0m  [8/21], [94mLoss[0m : 3.17667
[1mStep[0m  [10/21], [94mLoss[0m : 3.06314
[1mStep[0m  [12/21], [94mLoss[0m : 3.03296
[1mStep[0m  [14/21], [94mLoss[0m : 3.01862
[1mStep[0m  [16/21], [94mLoss[0m : 3.00784
[1mStep[0m  [18/21], [94mLoss[0m : 2.88563
[1mStep[0m  [20/21], [94mLoss[0m : 3.11448

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.076, [92mTest[0m: 2.592, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.96958
[1mStep[0m  [2/21], [94mLoss[0m : 3.03075
[1mStep[0m  [4/21], [94mLoss[0m : 2.94493
[1mStep[0m  [6/21], [94mLoss[0m : 3.02921
[1mStep[0m  [8/21], [94mLoss[0m : 2.80091
[1mStep[0m  [10/21], [94mLoss[0m : 2.77639
[1mStep[0m  [12/21], [94mLoss[0m : 2.70902
[1mStep[0m  [14/21], [94mLoss[0m : 2.90354
[1mStep[0m  [16/21], [94mLoss[0m : 2.86653
[1mStep[0m  [18/21], [94mLoss[0m : 2.79839
[1mStep[0m  [20/21], [94mLoss[0m : 3.04480

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.879, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.89360
[1mStep[0m  [2/21], [94mLoss[0m : 2.88230
[1mStep[0m  [4/21], [94mLoss[0m : 2.88291
[1mStep[0m  [6/21], [94mLoss[0m : 2.86349
[1mStep[0m  [8/21], [94mLoss[0m : 2.89070
[1mStep[0m  [10/21], [94mLoss[0m : 2.72447
[1mStep[0m  [12/21], [94mLoss[0m : 2.71502
[1mStep[0m  [14/21], [94mLoss[0m : 2.70746
[1mStep[0m  [16/21], [94mLoss[0m : 2.75323
[1mStep[0m  [18/21], [94mLoss[0m : 2.69282
[1mStep[0m  [20/21], [94mLoss[0m : 2.66895

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.835, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82771
[1mStep[0m  [2/21], [94mLoss[0m : 2.96360
[1mStep[0m  [4/21], [94mLoss[0m : 2.86517
[1mStep[0m  [6/21], [94mLoss[0m : 2.85054
[1mStep[0m  [8/21], [94mLoss[0m : 2.60886
[1mStep[0m  [10/21], [94mLoss[0m : 2.82738
[1mStep[0m  [12/21], [94mLoss[0m : 2.95665
[1mStep[0m  [14/21], [94mLoss[0m : 2.91317
[1mStep[0m  [16/21], [94mLoss[0m : 2.96580
[1mStep[0m  [18/21], [94mLoss[0m : 2.99589
[1mStep[0m  [20/21], [94mLoss[0m : 2.68982

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.841, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.86740
[1mStep[0m  [2/21], [94mLoss[0m : 2.76465
[1mStep[0m  [4/21], [94mLoss[0m : 2.78300
[1mStep[0m  [6/21], [94mLoss[0m : 3.15397
[1mStep[0m  [8/21], [94mLoss[0m : 2.88685
[1mStep[0m  [10/21], [94mLoss[0m : 2.72327
[1mStep[0m  [12/21], [94mLoss[0m : 2.69786
[1mStep[0m  [14/21], [94mLoss[0m : 2.87583
[1mStep[0m  [16/21], [94mLoss[0m : 2.71115
[1mStep[0m  [18/21], [94mLoss[0m : 2.76036
[1mStep[0m  [20/21], [94mLoss[0m : 2.85799

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.807, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64317
[1mStep[0m  [2/21], [94mLoss[0m : 2.88098
[1mStep[0m  [4/21], [94mLoss[0m : 2.87249
[1mStep[0m  [6/21], [94mLoss[0m : 2.91963
[1mStep[0m  [8/21], [94mLoss[0m : 2.79154
[1mStep[0m  [10/21], [94mLoss[0m : 2.71009
[1mStep[0m  [12/21], [94mLoss[0m : 2.83128
[1mStep[0m  [14/21], [94mLoss[0m : 2.73593
[1mStep[0m  [16/21], [94mLoss[0m : 2.78649
[1mStep[0m  [18/21], [94mLoss[0m : 2.74072
[1mStep[0m  [20/21], [94mLoss[0m : 2.77130

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.781, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.85429
[1mStep[0m  [2/21], [94mLoss[0m : 2.52358
[1mStep[0m  [4/21], [94mLoss[0m : 2.88391
[1mStep[0m  [6/21], [94mLoss[0m : 2.74191
[1mStep[0m  [8/21], [94mLoss[0m : 2.68263
[1mStep[0m  [10/21], [94mLoss[0m : 2.81606
[1mStep[0m  [12/21], [94mLoss[0m : 2.63936
[1mStep[0m  [14/21], [94mLoss[0m : 2.64606
[1mStep[0m  [16/21], [94mLoss[0m : 2.93651
[1mStep[0m  [18/21], [94mLoss[0m : 2.94990
[1mStep[0m  [20/21], [94mLoss[0m : 2.64543

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.748, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77885
[1mStep[0m  [2/21], [94mLoss[0m : 2.69710
[1mStep[0m  [4/21], [94mLoss[0m : 2.75340
[1mStep[0m  [6/21], [94mLoss[0m : 2.92928
[1mStep[0m  [8/21], [94mLoss[0m : 2.77413
[1mStep[0m  [10/21], [94mLoss[0m : 2.49545
[1mStep[0m  [12/21], [94mLoss[0m : 2.77900
[1mStep[0m  [14/21], [94mLoss[0m : 2.69705
[1mStep[0m  [16/21], [94mLoss[0m : 2.84854
[1mStep[0m  [18/21], [94mLoss[0m : 2.74477
[1mStep[0m  [20/21], [94mLoss[0m : 2.73210

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.770, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.85793
[1mStep[0m  [2/21], [94mLoss[0m : 2.70408
[1mStep[0m  [4/21], [94mLoss[0m : 2.77106
[1mStep[0m  [6/21], [94mLoss[0m : 2.48151
[1mStep[0m  [8/21], [94mLoss[0m : 2.74111
[1mStep[0m  [10/21], [94mLoss[0m : 2.75560
[1mStep[0m  [12/21], [94mLoss[0m : 2.77790
[1mStep[0m  [14/21], [94mLoss[0m : 2.75184
[1mStep[0m  [16/21], [94mLoss[0m : 2.61506
[1mStep[0m  [18/21], [94mLoss[0m : 2.54685
[1mStep[0m  [20/21], [94mLoss[0m : 2.65082

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66642
[1mStep[0m  [2/21], [94mLoss[0m : 2.83167
[1mStep[0m  [4/21], [94mLoss[0m : 2.88347
[1mStep[0m  [6/21], [94mLoss[0m : 2.64887
[1mStep[0m  [8/21], [94mLoss[0m : 2.68439
[1mStep[0m  [10/21], [94mLoss[0m : 2.60075
[1mStep[0m  [12/21], [94mLoss[0m : 2.84501
[1mStep[0m  [14/21], [94mLoss[0m : 2.66621
[1mStep[0m  [16/21], [94mLoss[0m : 2.60484
[1mStep[0m  [18/21], [94mLoss[0m : 2.55790
[1mStep[0m  [20/21], [94mLoss[0m : 2.79904

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70361
[1mStep[0m  [2/21], [94mLoss[0m : 2.68059
[1mStep[0m  [4/21], [94mLoss[0m : 2.62809
[1mStep[0m  [6/21], [94mLoss[0m : 2.60783
[1mStep[0m  [8/21], [94mLoss[0m : 2.89082
[1mStep[0m  [10/21], [94mLoss[0m : 2.75909
[1mStep[0m  [12/21], [94mLoss[0m : 2.67207
[1mStep[0m  [14/21], [94mLoss[0m : 2.55253
[1mStep[0m  [16/21], [94mLoss[0m : 2.73209
[1mStep[0m  [18/21], [94mLoss[0m : 2.66700
[1mStep[0m  [20/21], [94mLoss[0m : 2.77510

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60426
[1mStep[0m  [2/21], [94mLoss[0m : 2.61771
[1mStep[0m  [4/21], [94mLoss[0m : 2.58624
[1mStep[0m  [6/21], [94mLoss[0m : 2.82337
[1mStep[0m  [8/21], [94mLoss[0m : 2.71763
[1mStep[0m  [10/21], [94mLoss[0m : 2.71002
[1mStep[0m  [12/21], [94mLoss[0m : 2.73219
[1mStep[0m  [14/21], [94mLoss[0m : 2.74327
[1mStep[0m  [16/21], [94mLoss[0m : 2.64319
[1mStep[0m  [18/21], [94mLoss[0m : 2.71753
[1mStep[0m  [20/21], [94mLoss[0m : 2.67739

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76835
[1mStep[0m  [2/21], [94mLoss[0m : 2.86495
[1mStep[0m  [4/21], [94mLoss[0m : 2.73453
[1mStep[0m  [6/21], [94mLoss[0m : 2.62633
[1mStep[0m  [8/21], [94mLoss[0m : 2.50370
[1mStep[0m  [10/21], [94mLoss[0m : 2.75758
[1mStep[0m  [12/21], [94mLoss[0m : 2.66687
[1mStep[0m  [14/21], [94mLoss[0m : 2.71429
[1mStep[0m  [16/21], [94mLoss[0m : 2.72108
[1mStep[0m  [18/21], [94mLoss[0m : 2.67707
[1mStep[0m  [20/21], [94mLoss[0m : 2.54351

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59929
[1mStep[0m  [2/21], [94mLoss[0m : 2.95114
[1mStep[0m  [4/21], [94mLoss[0m : 2.52246
[1mStep[0m  [6/21], [94mLoss[0m : 2.77468
[1mStep[0m  [8/21], [94mLoss[0m : 2.55921
[1mStep[0m  [10/21], [94mLoss[0m : 2.77984
[1mStep[0m  [12/21], [94mLoss[0m : 2.65450
[1mStep[0m  [14/21], [94mLoss[0m : 2.58825
[1mStep[0m  [16/21], [94mLoss[0m : 2.62584
[1mStep[0m  [18/21], [94mLoss[0m : 2.58980
[1mStep[0m  [20/21], [94mLoss[0m : 2.57102

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80487
[1mStep[0m  [2/21], [94mLoss[0m : 2.83277
[1mStep[0m  [4/21], [94mLoss[0m : 2.58759
[1mStep[0m  [6/21], [94mLoss[0m : 2.59338
[1mStep[0m  [8/21], [94mLoss[0m : 2.64567
[1mStep[0m  [10/21], [94mLoss[0m : 2.76642
[1mStep[0m  [12/21], [94mLoss[0m : 2.67512
[1mStep[0m  [14/21], [94mLoss[0m : 2.58261
[1mStep[0m  [16/21], [94mLoss[0m : 2.75703
[1mStep[0m  [18/21], [94mLoss[0m : 2.86066
[1mStep[0m  [20/21], [94mLoss[0m : 2.60658

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54095
[1mStep[0m  [2/21], [94mLoss[0m : 2.71567
[1mStep[0m  [4/21], [94mLoss[0m : 2.79350
[1mStep[0m  [6/21], [94mLoss[0m : 2.79317
[1mStep[0m  [8/21], [94mLoss[0m : 2.67194
[1mStep[0m  [10/21], [94mLoss[0m : 2.71060
[1mStep[0m  [12/21], [94mLoss[0m : 2.45445
[1mStep[0m  [14/21], [94mLoss[0m : 2.62888
[1mStep[0m  [16/21], [94mLoss[0m : 2.68382
[1mStep[0m  [18/21], [94mLoss[0m : 2.58755
[1mStep[0m  [20/21], [94mLoss[0m : 2.59490

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61279
[1mStep[0m  [2/21], [94mLoss[0m : 2.67165
[1mStep[0m  [4/21], [94mLoss[0m : 2.61448
[1mStep[0m  [6/21], [94mLoss[0m : 2.58668
[1mStep[0m  [8/21], [94mLoss[0m : 2.66978
[1mStep[0m  [10/21], [94mLoss[0m : 2.65273
[1mStep[0m  [12/21], [94mLoss[0m : 2.56534
[1mStep[0m  [14/21], [94mLoss[0m : 2.78746
[1mStep[0m  [16/21], [94mLoss[0m : 2.65713
[1mStep[0m  [18/21], [94mLoss[0m : 2.79326
[1mStep[0m  [20/21], [94mLoss[0m : 2.58668

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68083
[1mStep[0m  [2/21], [94mLoss[0m : 2.77359
[1mStep[0m  [4/21], [94mLoss[0m : 2.61942
[1mStep[0m  [6/21], [94mLoss[0m : 2.70090
[1mStep[0m  [8/21], [94mLoss[0m : 2.58610
[1mStep[0m  [10/21], [94mLoss[0m : 2.80866
[1mStep[0m  [12/21], [94mLoss[0m : 2.59200
[1mStep[0m  [14/21], [94mLoss[0m : 2.63379
[1mStep[0m  [16/21], [94mLoss[0m : 2.84201
[1mStep[0m  [18/21], [94mLoss[0m : 2.72719
[1mStep[0m  [20/21], [94mLoss[0m : 2.64884

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72418
[1mStep[0m  [2/21], [94mLoss[0m : 2.73509
[1mStep[0m  [4/21], [94mLoss[0m : 2.50390
[1mStep[0m  [6/21], [94mLoss[0m : 2.69025
[1mStep[0m  [8/21], [94mLoss[0m : 2.47160
[1mStep[0m  [10/21], [94mLoss[0m : 2.57482
[1mStep[0m  [12/21], [94mLoss[0m : 2.63635
[1mStep[0m  [14/21], [94mLoss[0m : 2.50200
[1mStep[0m  [16/21], [94mLoss[0m : 2.53246
[1mStep[0m  [18/21], [94mLoss[0m : 2.65066
[1mStep[0m  [20/21], [94mLoss[0m : 2.77843

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60133
[1mStep[0m  [2/21], [94mLoss[0m : 2.53358
[1mStep[0m  [4/21], [94mLoss[0m : 2.78212
[1mStep[0m  [6/21], [94mLoss[0m : 2.59231
[1mStep[0m  [8/21], [94mLoss[0m : 2.69014
[1mStep[0m  [10/21], [94mLoss[0m : 2.69402
[1mStep[0m  [12/21], [94mLoss[0m : 2.68241
[1mStep[0m  [14/21], [94mLoss[0m : 2.64666
[1mStep[0m  [16/21], [94mLoss[0m : 2.75993
[1mStep[0m  [18/21], [94mLoss[0m : 2.56491
[1mStep[0m  [20/21], [94mLoss[0m : 2.48708

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.3298706327165877
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.53164
[1mStep[0m  [2/21], [94mLoss[0m : 2.76129
[1mStep[0m  [4/21], [94mLoss[0m : 2.69473
[1mStep[0m  [6/21], [94mLoss[0m : 2.59898
[1mStep[0m  [8/21], [94mLoss[0m : 2.73955
[1mStep[0m  [10/21], [94mLoss[0m : 2.72780
[1mStep[0m  [12/21], [94mLoss[0m : 2.77233
[1mStep[0m  [14/21], [94mLoss[0m : 2.77194
[1mStep[0m  [16/21], [94mLoss[0m : 2.51237
[1mStep[0m  [18/21], [94mLoss[0m : 2.60498
[1mStep[0m  [20/21], [94mLoss[0m : 2.52617

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75020
[1mStep[0m  [2/21], [94mLoss[0m : 2.62992
[1mStep[0m  [4/21], [94mLoss[0m : 2.65012
[1mStep[0m  [6/21], [94mLoss[0m : 2.61184
[1mStep[0m  [8/21], [94mLoss[0m : 2.71491
[1mStep[0m  [10/21], [94mLoss[0m : 2.74157
[1mStep[0m  [12/21], [94mLoss[0m : 2.54898
[1mStep[0m  [14/21], [94mLoss[0m : 2.62297
[1mStep[0m  [16/21], [94mLoss[0m : 2.49665
[1mStep[0m  [18/21], [94mLoss[0m : 2.75618
[1mStep[0m  [20/21], [94mLoss[0m : 2.60990

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66332
[1mStep[0m  [2/21], [94mLoss[0m : 2.63751
[1mStep[0m  [4/21], [94mLoss[0m : 2.72536
[1mStep[0m  [6/21], [94mLoss[0m : 2.50556
[1mStep[0m  [8/21], [94mLoss[0m : 2.59379
[1mStep[0m  [10/21], [94mLoss[0m : 2.65103
[1mStep[0m  [12/21], [94mLoss[0m : 2.61174
[1mStep[0m  [14/21], [94mLoss[0m : 2.52237
[1mStep[0m  [16/21], [94mLoss[0m : 2.53943
[1mStep[0m  [18/21], [94mLoss[0m : 2.38949
[1mStep[0m  [20/21], [94mLoss[0m : 2.76811

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56438
[1mStep[0m  [2/21], [94mLoss[0m : 2.64374
[1mStep[0m  [4/21], [94mLoss[0m : 2.59547
[1mStep[0m  [6/21], [94mLoss[0m : 2.64270
[1mStep[0m  [8/21], [94mLoss[0m : 2.58425
[1mStep[0m  [10/21], [94mLoss[0m : 2.47440
[1mStep[0m  [12/21], [94mLoss[0m : 2.61044
[1mStep[0m  [14/21], [94mLoss[0m : 2.63640
[1mStep[0m  [16/21], [94mLoss[0m : 2.53477
[1mStep[0m  [18/21], [94mLoss[0m : 2.55185
[1mStep[0m  [20/21], [94mLoss[0m : 2.63273

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56492
[1mStep[0m  [2/21], [94mLoss[0m : 2.45475
[1mStep[0m  [4/21], [94mLoss[0m : 2.54040
[1mStep[0m  [6/21], [94mLoss[0m : 2.64514
[1mStep[0m  [8/21], [94mLoss[0m : 2.43288
[1mStep[0m  [10/21], [94mLoss[0m : 2.68888
[1mStep[0m  [12/21], [94mLoss[0m : 2.55521
[1mStep[0m  [14/21], [94mLoss[0m : 2.57924
[1mStep[0m  [16/21], [94mLoss[0m : 2.68541
[1mStep[0m  [18/21], [94mLoss[0m : 2.52671
[1mStep[0m  [20/21], [94mLoss[0m : 2.77268

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51317
[1mStep[0m  [2/21], [94mLoss[0m : 2.54265
[1mStep[0m  [4/21], [94mLoss[0m : 2.60932
[1mStep[0m  [6/21], [94mLoss[0m : 2.61703
[1mStep[0m  [8/21], [94mLoss[0m : 2.39991
[1mStep[0m  [10/21], [94mLoss[0m : 2.43168
[1mStep[0m  [12/21], [94mLoss[0m : 2.54767
[1mStep[0m  [14/21], [94mLoss[0m : 2.49154
[1mStep[0m  [16/21], [94mLoss[0m : 2.54983
[1mStep[0m  [18/21], [94mLoss[0m : 2.67123
[1mStep[0m  [20/21], [94mLoss[0m : 2.34410

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54277
[1mStep[0m  [2/21], [94mLoss[0m : 2.47159
[1mStep[0m  [4/21], [94mLoss[0m : 2.47771
[1mStep[0m  [6/21], [94mLoss[0m : 2.56826
[1mStep[0m  [8/21], [94mLoss[0m : 2.33288
[1mStep[0m  [10/21], [94mLoss[0m : 2.57097
[1mStep[0m  [12/21], [94mLoss[0m : 2.56510
[1mStep[0m  [14/21], [94mLoss[0m : 2.37266
[1mStep[0m  [16/21], [94mLoss[0m : 2.51373
[1mStep[0m  [18/21], [94mLoss[0m : 2.29429
[1mStep[0m  [20/21], [94mLoss[0m : 2.41583

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56167
[1mStep[0m  [2/21], [94mLoss[0m : 2.42097
[1mStep[0m  [4/21], [94mLoss[0m : 2.47291
[1mStep[0m  [6/21], [94mLoss[0m : 2.57180
[1mStep[0m  [8/21], [94mLoss[0m : 2.51914
[1mStep[0m  [10/21], [94mLoss[0m : 2.62504
[1mStep[0m  [12/21], [94mLoss[0m : 2.62280
[1mStep[0m  [14/21], [94mLoss[0m : 2.63644
[1mStep[0m  [16/21], [94mLoss[0m : 2.46314
[1mStep[0m  [18/21], [94mLoss[0m : 2.50686
[1mStep[0m  [20/21], [94mLoss[0m : 2.44316

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37289
[1mStep[0m  [2/21], [94mLoss[0m : 2.48902
[1mStep[0m  [4/21], [94mLoss[0m : 2.45282
[1mStep[0m  [6/21], [94mLoss[0m : 2.38391
[1mStep[0m  [8/21], [94mLoss[0m : 2.37326
[1mStep[0m  [10/21], [94mLoss[0m : 2.53337
[1mStep[0m  [12/21], [94mLoss[0m : 2.53509
[1mStep[0m  [14/21], [94mLoss[0m : 2.58373
[1mStep[0m  [16/21], [94mLoss[0m : 2.42918
[1mStep[0m  [18/21], [94mLoss[0m : 2.45054
[1mStep[0m  [20/21], [94mLoss[0m : 2.43300

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36278
[1mStep[0m  [2/21], [94mLoss[0m : 2.40282
[1mStep[0m  [4/21], [94mLoss[0m : 2.47213
[1mStep[0m  [6/21], [94mLoss[0m : 2.51690
[1mStep[0m  [8/21], [94mLoss[0m : 2.33014
[1mStep[0m  [10/21], [94mLoss[0m : 2.36875
[1mStep[0m  [12/21], [94mLoss[0m : 2.34876
[1mStep[0m  [14/21], [94mLoss[0m : 2.48076
[1mStep[0m  [16/21], [94mLoss[0m : 2.41200
[1mStep[0m  [18/21], [94mLoss[0m : 2.21306
[1mStep[0m  [20/21], [94mLoss[0m : 2.30913

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41729
[1mStep[0m  [2/21], [94mLoss[0m : 2.42143
[1mStep[0m  [4/21], [94mLoss[0m : 2.19184
[1mStep[0m  [6/21], [94mLoss[0m : 2.42038
[1mStep[0m  [8/21], [94mLoss[0m : 2.50923
[1mStep[0m  [10/21], [94mLoss[0m : 2.57660
[1mStep[0m  [12/21], [94mLoss[0m : 2.39417
[1mStep[0m  [14/21], [94mLoss[0m : 2.51883
[1mStep[0m  [16/21], [94mLoss[0m : 2.35599
[1mStep[0m  [18/21], [94mLoss[0m : 2.42867
[1mStep[0m  [20/21], [94mLoss[0m : 2.33210

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46099
[1mStep[0m  [2/21], [94mLoss[0m : 2.29815
[1mStep[0m  [4/21], [94mLoss[0m : 2.31926
[1mStep[0m  [6/21], [94mLoss[0m : 2.27778
[1mStep[0m  [8/21], [94mLoss[0m : 2.27492
[1mStep[0m  [10/21], [94mLoss[0m : 2.35587
[1mStep[0m  [12/21], [94mLoss[0m : 2.45331
[1mStep[0m  [14/21], [94mLoss[0m : 2.20298
[1mStep[0m  [16/21], [94mLoss[0m : 2.29871
[1mStep[0m  [18/21], [94mLoss[0m : 2.33173
[1mStep[0m  [20/21], [94mLoss[0m : 2.45810

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32916
[1mStep[0m  [2/21], [94mLoss[0m : 2.20412
[1mStep[0m  [4/21], [94mLoss[0m : 2.38539
[1mStep[0m  [6/21], [94mLoss[0m : 2.13368
[1mStep[0m  [8/21], [94mLoss[0m : 2.36181
[1mStep[0m  [10/21], [94mLoss[0m : 2.24729
[1mStep[0m  [12/21], [94mLoss[0m : 2.39233
[1mStep[0m  [14/21], [94mLoss[0m : 2.33724
[1mStep[0m  [16/21], [94mLoss[0m : 2.49424
[1mStep[0m  [18/21], [94mLoss[0m : 2.36539
[1mStep[0m  [20/21], [94mLoss[0m : 2.27265

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17277
[1mStep[0m  [2/21], [94mLoss[0m : 2.27084
[1mStep[0m  [4/21], [94mLoss[0m : 2.31188
[1mStep[0m  [6/21], [94mLoss[0m : 2.21631
[1mStep[0m  [8/21], [94mLoss[0m : 2.33790
[1mStep[0m  [10/21], [94mLoss[0m : 2.29686
[1mStep[0m  [12/21], [94mLoss[0m : 2.32198
[1mStep[0m  [14/21], [94mLoss[0m : 2.18717
[1mStep[0m  [16/21], [94mLoss[0m : 2.25693
[1mStep[0m  [18/21], [94mLoss[0m : 2.29887
[1mStep[0m  [20/21], [94mLoss[0m : 2.21231

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31667
[1mStep[0m  [2/21], [94mLoss[0m : 2.21262
[1mStep[0m  [4/21], [94mLoss[0m : 2.32455
[1mStep[0m  [6/21], [94mLoss[0m : 2.24038
[1mStep[0m  [8/21], [94mLoss[0m : 2.23251
[1mStep[0m  [10/21], [94mLoss[0m : 2.26337
[1mStep[0m  [12/21], [94mLoss[0m : 2.24483
[1mStep[0m  [14/21], [94mLoss[0m : 2.17400
[1mStep[0m  [16/21], [94mLoss[0m : 2.22760
[1mStep[0m  [18/21], [94mLoss[0m : 2.33252
[1mStep[0m  [20/21], [94mLoss[0m : 2.38672

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28457
[1mStep[0m  [2/21], [94mLoss[0m : 2.20057
[1mStep[0m  [4/21], [94mLoss[0m : 2.07636
[1mStep[0m  [6/21], [94mLoss[0m : 2.12491
[1mStep[0m  [8/21], [94mLoss[0m : 2.09586
[1mStep[0m  [10/21], [94mLoss[0m : 2.29317
[1mStep[0m  [12/21], [94mLoss[0m : 2.24420
[1mStep[0m  [14/21], [94mLoss[0m : 2.24279
[1mStep[0m  [16/21], [94mLoss[0m : 2.28309
[1mStep[0m  [18/21], [94mLoss[0m : 2.30010
[1mStep[0m  [20/21], [94mLoss[0m : 2.24350

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15673
[1mStep[0m  [2/21], [94mLoss[0m : 2.25523
[1mStep[0m  [4/21], [94mLoss[0m : 2.22590
[1mStep[0m  [6/21], [94mLoss[0m : 2.08937
[1mStep[0m  [8/21], [94mLoss[0m : 2.23207
[1mStep[0m  [10/21], [94mLoss[0m : 2.20551
[1mStep[0m  [12/21], [94mLoss[0m : 2.21080
[1mStep[0m  [14/21], [94mLoss[0m : 1.99697
[1mStep[0m  [16/21], [94mLoss[0m : 2.06610
[1mStep[0m  [18/21], [94mLoss[0m : 2.16019
[1mStep[0m  [20/21], [94mLoss[0m : 2.13896

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16555
[1mStep[0m  [2/21], [94mLoss[0m : 2.04807
[1mStep[0m  [4/21], [94mLoss[0m : 2.10052
[1mStep[0m  [6/21], [94mLoss[0m : 2.11534
[1mStep[0m  [8/21], [94mLoss[0m : 2.03876
[1mStep[0m  [10/21], [94mLoss[0m : 2.25875
[1mStep[0m  [12/21], [94mLoss[0m : 2.25598
[1mStep[0m  [14/21], [94mLoss[0m : 2.30080
[1mStep[0m  [16/21], [94mLoss[0m : 2.02155
[1mStep[0m  [18/21], [94mLoss[0m : 2.28057
[1mStep[0m  [20/21], [94mLoss[0m : 2.06028

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13201
[1mStep[0m  [2/21], [94mLoss[0m : 2.07883
[1mStep[0m  [4/21], [94mLoss[0m : 2.04533
[1mStep[0m  [6/21], [94mLoss[0m : 2.05328
[1mStep[0m  [8/21], [94mLoss[0m : 2.18351
[1mStep[0m  [10/21], [94mLoss[0m : 2.20245
[1mStep[0m  [12/21], [94mLoss[0m : 2.17613
[1mStep[0m  [14/21], [94mLoss[0m : 2.09209
[1mStep[0m  [16/21], [94mLoss[0m : 2.13048
[1mStep[0m  [18/21], [94mLoss[0m : 2.08665
[1mStep[0m  [20/21], [94mLoss[0m : 2.18583

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08789
[1mStep[0m  [2/21], [94mLoss[0m : 1.95589
[1mStep[0m  [4/21], [94mLoss[0m : 2.00971
[1mStep[0m  [6/21], [94mLoss[0m : 2.17285
[1mStep[0m  [8/21], [94mLoss[0m : 2.08721
[1mStep[0m  [10/21], [94mLoss[0m : 2.26051
[1mStep[0m  [12/21], [94mLoss[0m : 1.91908
[1mStep[0m  [14/21], [94mLoss[0m : 2.18237
[1mStep[0m  [16/21], [94mLoss[0m : 2.12499
[1mStep[0m  [18/21], [94mLoss[0m : 2.18410
[1mStep[0m  [20/21], [94mLoss[0m : 2.21166

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.05711
[1mStep[0m  [2/21], [94mLoss[0m : 2.06487
[1mStep[0m  [4/21], [94mLoss[0m : 1.98321
[1mStep[0m  [6/21], [94mLoss[0m : 2.10510
[1mStep[0m  [8/21], [94mLoss[0m : 2.14498
[1mStep[0m  [10/21], [94mLoss[0m : 1.88656
[1mStep[0m  [12/21], [94mLoss[0m : 2.09585
[1mStep[0m  [14/21], [94mLoss[0m : 2.06978
[1mStep[0m  [16/21], [94mLoss[0m : 2.15702
[1mStep[0m  [18/21], [94mLoss[0m : 2.07608
[1mStep[0m  [20/21], [94mLoss[0m : 2.12581

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07736
[1mStep[0m  [2/21], [94mLoss[0m : 2.03042
[1mStep[0m  [4/21], [94mLoss[0m : 1.96611
[1mStep[0m  [6/21], [94mLoss[0m : 2.12456
[1mStep[0m  [8/21], [94mLoss[0m : 2.01362
[1mStep[0m  [10/21], [94mLoss[0m : 2.17087
[1mStep[0m  [12/21], [94mLoss[0m : 1.90659
[1mStep[0m  [14/21], [94mLoss[0m : 2.12678
[1mStep[0m  [16/21], [94mLoss[0m : 2.09680
[1mStep[0m  [18/21], [94mLoss[0m : 1.92747
[1mStep[0m  [20/21], [94mLoss[0m : 2.11235

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.453, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90227
[1mStep[0m  [2/21], [94mLoss[0m : 1.99205
[1mStep[0m  [4/21], [94mLoss[0m : 1.98181
[1mStep[0m  [6/21], [94mLoss[0m : 1.91422
[1mStep[0m  [8/21], [94mLoss[0m : 2.04175
[1mStep[0m  [10/21], [94mLoss[0m : 2.06737
[1mStep[0m  [12/21], [94mLoss[0m : 2.06207
[1mStep[0m  [14/21], [94mLoss[0m : 1.93179
[1mStep[0m  [16/21], [94mLoss[0m : 2.09044
[1mStep[0m  [18/21], [94mLoss[0m : 2.01482
[1mStep[0m  [20/21], [94mLoss[0m : 1.97103

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.440, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91967
[1mStep[0m  [2/21], [94mLoss[0m : 1.92502
[1mStep[0m  [4/21], [94mLoss[0m : 1.88512
[1mStep[0m  [6/21], [94mLoss[0m : 1.80039
[1mStep[0m  [8/21], [94mLoss[0m : 2.12905
[1mStep[0m  [10/21], [94mLoss[0m : 2.08068
[1mStep[0m  [12/21], [94mLoss[0m : 1.88250
[1mStep[0m  [14/21], [94mLoss[0m : 2.12142
[1mStep[0m  [16/21], [94mLoss[0m : 1.90893
[1mStep[0m  [18/21], [94mLoss[0m : 2.00411
[1mStep[0m  [20/21], [94mLoss[0m : 1.96064

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.497, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.00960
[1mStep[0m  [2/21], [94mLoss[0m : 1.93683
[1mStep[0m  [4/21], [94mLoss[0m : 1.94869
[1mStep[0m  [6/21], [94mLoss[0m : 1.96007
[1mStep[0m  [8/21], [94mLoss[0m : 1.86142
[1mStep[0m  [10/21], [94mLoss[0m : 1.92679
[1mStep[0m  [12/21], [94mLoss[0m : 1.97690
[1mStep[0m  [14/21], [94mLoss[0m : 2.01855
[1mStep[0m  [16/21], [94mLoss[0m : 1.86594
[1mStep[0m  [18/21], [94mLoss[0m : 1.98975
[1mStep[0m  [20/21], [94mLoss[0m : 1.91036

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.457, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98902
[1mStep[0m  [2/21], [94mLoss[0m : 1.79068
[1mStep[0m  [4/21], [94mLoss[0m : 1.97781
[1mStep[0m  [6/21], [94mLoss[0m : 1.88197
[1mStep[0m  [8/21], [94mLoss[0m : 1.94257
[1mStep[0m  [10/21], [94mLoss[0m : 1.79438
[1mStep[0m  [12/21], [94mLoss[0m : 1.93052
[1mStep[0m  [14/21], [94mLoss[0m : 1.94535
[1mStep[0m  [16/21], [94mLoss[0m : 1.76894
[1mStep[0m  [18/21], [94mLoss[0m : 1.88769
[1mStep[0m  [20/21], [94mLoss[0m : 2.02764

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.924, [92mTest[0m: 2.518, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95332
[1mStep[0m  [2/21], [94mLoss[0m : 1.81877
[1mStep[0m  [4/21], [94mLoss[0m : 1.99162
[1mStep[0m  [6/21], [94mLoss[0m : 1.93613
[1mStep[0m  [8/21], [94mLoss[0m : 1.82575
[1mStep[0m  [10/21], [94mLoss[0m : 1.95708
[1mStep[0m  [12/21], [94mLoss[0m : 1.90663
[1mStep[0m  [14/21], [94mLoss[0m : 1.92382
[1mStep[0m  [16/21], [94mLoss[0m : 1.87792
[1mStep[0m  [18/21], [94mLoss[0m : 1.88838
[1mStep[0m  [20/21], [94mLoss[0m : 2.04785

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.518, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91753
[1mStep[0m  [2/21], [94mLoss[0m : 1.84075
[1mStep[0m  [4/21], [94mLoss[0m : 1.96540
[1mStep[0m  [6/21], [94mLoss[0m : 1.91361
[1mStep[0m  [8/21], [94mLoss[0m : 1.86903
[1mStep[0m  [10/21], [94mLoss[0m : 1.84723
[1mStep[0m  [12/21], [94mLoss[0m : 1.97907
[1mStep[0m  [14/21], [94mLoss[0m : 1.94947
[1mStep[0m  [16/21], [94mLoss[0m : 1.92790
[1mStep[0m  [18/21], [94mLoss[0m : 1.85062
[1mStep[0m  [20/21], [94mLoss[0m : 1.83029

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.524, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.79726
[1mStep[0m  [2/21], [94mLoss[0m : 1.91625
[1mStep[0m  [4/21], [94mLoss[0m : 1.79892
[1mStep[0m  [6/21], [94mLoss[0m : 1.88552
[1mStep[0m  [8/21], [94mLoss[0m : 1.74041
[1mStep[0m  [10/21], [94mLoss[0m : 1.86432
[1mStep[0m  [12/21], [94mLoss[0m : 1.92757
[1mStep[0m  [14/21], [94mLoss[0m : 1.92512
[1mStep[0m  [16/21], [94mLoss[0m : 1.88997
[1mStep[0m  [18/21], [94mLoss[0m : 1.95472
[1mStep[0m  [20/21], [94mLoss[0m : 1.79668

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.866, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77761
[1mStep[0m  [2/21], [94mLoss[0m : 1.67649
[1mStep[0m  [4/21], [94mLoss[0m : 1.73108
[1mStep[0m  [6/21], [94mLoss[0m : 1.79075
[1mStep[0m  [8/21], [94mLoss[0m : 1.91809
[1mStep[0m  [10/21], [94mLoss[0m : 1.80795
[1mStep[0m  [12/21], [94mLoss[0m : 1.65421
[1mStep[0m  [14/21], [94mLoss[0m : 1.77214
[1mStep[0m  [16/21], [94mLoss[0m : 1.97802
[1mStep[0m  [18/21], [94mLoss[0m : 1.79586
[1mStep[0m  [20/21], [94mLoss[0m : 1.92504

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.653
====================================

Phase 2 - Evaluation MAE:  2.6531051908220564
MAE score P1       2.329871
MAE score P2       2.653105
loss               1.819462
learning_rate          0.01
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.97473
[1mStep[0m  [2/21], [94mLoss[0m : 10.67219
[1mStep[0m  [4/21], [94mLoss[0m : 10.08715
[1mStep[0m  [6/21], [94mLoss[0m : 9.02083
[1mStep[0m  [8/21], [94mLoss[0m : 8.64560
[1mStep[0m  [10/21], [94mLoss[0m : 8.21308
[1mStep[0m  [12/21], [94mLoss[0m : 7.43148
[1mStep[0m  [14/21], [94mLoss[0m : 6.72677
[1mStep[0m  [16/21], [94mLoss[0m : 6.38584
[1mStep[0m  [18/21], [94mLoss[0m : 5.98477
[1mStep[0m  [20/21], [94mLoss[0m : 4.80333

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.137, [92mTest[0m: 10.915, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.82982
[1mStep[0m  [2/21], [94mLoss[0m : 4.44423
[1mStep[0m  [4/21], [94mLoss[0m : 4.26662
[1mStep[0m  [6/21], [94mLoss[0m : 3.93847
[1mStep[0m  [8/21], [94mLoss[0m : 3.61854
[1mStep[0m  [10/21], [94mLoss[0m : 3.60086
[1mStep[0m  [12/21], [94mLoss[0m : 3.02418
[1mStep[0m  [14/21], [94mLoss[0m : 3.02304
[1mStep[0m  [16/21], [94mLoss[0m : 3.10664
[1mStep[0m  [18/21], [94mLoss[0m : 2.89280
[1mStep[0m  [20/21], [94mLoss[0m : 3.17202

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.578, [92mTest[0m: 8.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.02717
[1mStep[0m  [2/21], [94mLoss[0m : 2.99167
[1mStep[0m  [4/21], [94mLoss[0m : 2.75246
[1mStep[0m  [6/21], [94mLoss[0m : 2.83243
[1mStep[0m  [8/21], [94mLoss[0m : 2.79203
[1mStep[0m  [10/21], [94mLoss[0m : 2.96100
[1mStep[0m  [12/21], [94mLoss[0m : 2.87809
[1mStep[0m  [14/21], [94mLoss[0m : 2.94946
[1mStep[0m  [16/21], [94mLoss[0m : 2.88229
[1mStep[0m  [18/21], [94mLoss[0m : 2.82602
[1mStep[0m  [20/21], [94mLoss[0m : 2.87156

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.847, [92mTest[0m: 4.858, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82689
[1mStep[0m  [2/21], [94mLoss[0m : 2.80151
[1mStep[0m  [4/21], [94mLoss[0m : 2.75909
[1mStep[0m  [6/21], [94mLoss[0m : 2.71380
[1mStep[0m  [8/21], [94mLoss[0m : 2.92560
[1mStep[0m  [10/21], [94mLoss[0m : 2.74319
[1mStep[0m  [12/21], [94mLoss[0m : 2.45540
[1mStep[0m  [14/21], [94mLoss[0m : 2.85681
[1mStep[0m  [16/21], [94mLoss[0m : 2.62846
[1mStep[0m  [18/21], [94mLoss[0m : 2.74066
[1mStep[0m  [20/21], [94mLoss[0m : 2.87691

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.793, [92mTest[0m: 3.872, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81123
[1mStep[0m  [2/21], [94mLoss[0m : 2.69204
[1mStep[0m  [4/21], [94mLoss[0m : 2.88315
[1mStep[0m  [6/21], [94mLoss[0m : 2.73079
[1mStep[0m  [8/21], [94mLoss[0m : 2.92674
[1mStep[0m  [10/21], [94mLoss[0m : 2.73890
[1mStep[0m  [12/21], [94mLoss[0m : 2.69268
[1mStep[0m  [14/21], [94mLoss[0m : 2.61620
[1mStep[0m  [16/21], [94mLoss[0m : 2.52933
[1mStep[0m  [18/21], [94mLoss[0m : 2.87705
[1mStep[0m  [20/21], [94mLoss[0m : 2.96345

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.773, [92mTest[0m: 3.299, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71078
[1mStep[0m  [2/21], [94mLoss[0m : 2.57523
[1mStep[0m  [4/21], [94mLoss[0m : 2.64775
[1mStep[0m  [6/21], [94mLoss[0m : 2.81355
[1mStep[0m  [8/21], [94mLoss[0m : 2.76786
[1mStep[0m  [10/21], [94mLoss[0m : 2.72599
[1mStep[0m  [12/21], [94mLoss[0m : 2.66807
[1mStep[0m  [14/21], [94mLoss[0m : 2.85082
[1mStep[0m  [16/21], [94mLoss[0m : 2.63352
[1mStep[0m  [18/21], [94mLoss[0m : 2.69714
[1mStep[0m  [20/21], [94mLoss[0m : 2.59823

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.721, [92mTest[0m: 3.163, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72176
[1mStep[0m  [2/21], [94mLoss[0m : 2.66999
[1mStep[0m  [4/21], [94mLoss[0m : 2.75802
[1mStep[0m  [6/21], [94mLoss[0m : 2.87534
[1mStep[0m  [8/21], [94mLoss[0m : 2.75094
[1mStep[0m  [10/21], [94mLoss[0m : 2.67651
[1mStep[0m  [12/21], [94mLoss[0m : 2.77487
[1mStep[0m  [14/21], [94mLoss[0m : 2.70939
[1mStep[0m  [16/21], [94mLoss[0m : 2.66497
[1mStep[0m  [18/21], [94mLoss[0m : 2.56743
[1mStep[0m  [20/21], [94mLoss[0m : 2.67497

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.944, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64154
[1mStep[0m  [2/21], [94mLoss[0m : 2.72878
[1mStep[0m  [4/21], [94mLoss[0m : 2.73329
[1mStep[0m  [6/21], [94mLoss[0m : 2.64607
[1mStep[0m  [8/21], [94mLoss[0m : 2.74029
[1mStep[0m  [10/21], [94mLoss[0m : 2.64445
[1mStep[0m  [12/21], [94mLoss[0m : 2.82565
[1mStep[0m  [14/21], [94mLoss[0m : 2.60538
[1mStep[0m  [16/21], [94mLoss[0m : 2.69569
[1mStep[0m  [18/21], [94mLoss[0m : 2.69523
[1mStep[0m  [20/21], [94mLoss[0m : 2.70592

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.949, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66453
[1mStep[0m  [2/21], [94mLoss[0m : 2.63485
[1mStep[0m  [4/21], [94mLoss[0m : 2.83550
[1mStep[0m  [6/21], [94mLoss[0m : 2.69183
[1mStep[0m  [8/21], [94mLoss[0m : 2.53538
[1mStep[0m  [10/21], [94mLoss[0m : 2.66728
[1mStep[0m  [12/21], [94mLoss[0m : 2.84496
[1mStep[0m  [14/21], [94mLoss[0m : 2.72130
[1mStep[0m  [16/21], [94mLoss[0m : 2.74582
[1mStep[0m  [18/21], [94mLoss[0m : 2.78984
[1mStep[0m  [20/21], [94mLoss[0m : 2.76131

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.881, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.88792
[1mStep[0m  [2/21], [94mLoss[0m : 2.67963
[1mStep[0m  [4/21], [94mLoss[0m : 2.76025
[1mStep[0m  [6/21], [94mLoss[0m : 2.66776
[1mStep[0m  [8/21], [94mLoss[0m : 2.79108
[1mStep[0m  [10/21], [94mLoss[0m : 2.64830
[1mStep[0m  [12/21], [94mLoss[0m : 2.71222
[1mStep[0m  [14/21], [94mLoss[0m : 2.65651
[1mStep[0m  [16/21], [94mLoss[0m : 2.58881
[1mStep[0m  [18/21], [94mLoss[0m : 2.71096
[1mStep[0m  [20/21], [94mLoss[0m : 2.59209

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.807, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69986
[1mStep[0m  [2/21], [94mLoss[0m : 2.57271
[1mStep[0m  [4/21], [94mLoss[0m : 2.77908
[1mStep[0m  [6/21], [94mLoss[0m : 2.60994
[1mStep[0m  [8/21], [94mLoss[0m : 2.55598
[1mStep[0m  [10/21], [94mLoss[0m : 2.64199
[1mStep[0m  [12/21], [94mLoss[0m : 2.70835
[1mStep[0m  [14/21], [94mLoss[0m : 2.60575
[1mStep[0m  [16/21], [94mLoss[0m : 2.68630
[1mStep[0m  [18/21], [94mLoss[0m : 2.58143
[1mStep[0m  [20/21], [94mLoss[0m : 2.81554

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.803, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75690
[1mStep[0m  [2/21], [94mLoss[0m : 2.70274
[1mStep[0m  [4/21], [94mLoss[0m : 2.65374
[1mStep[0m  [6/21], [94mLoss[0m : 2.68269
[1mStep[0m  [8/21], [94mLoss[0m : 2.69210
[1mStep[0m  [10/21], [94mLoss[0m : 2.72552
[1mStep[0m  [12/21], [94mLoss[0m : 2.57572
[1mStep[0m  [14/21], [94mLoss[0m : 2.69564
[1mStep[0m  [16/21], [94mLoss[0m : 2.54587
[1mStep[0m  [18/21], [94mLoss[0m : 2.60318
[1mStep[0m  [20/21], [94mLoss[0m : 2.60572

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.733, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53923
[1mStep[0m  [2/21], [94mLoss[0m : 2.56017
[1mStep[0m  [4/21], [94mLoss[0m : 2.53386
[1mStep[0m  [6/21], [94mLoss[0m : 2.68221
[1mStep[0m  [8/21], [94mLoss[0m : 2.51616
[1mStep[0m  [10/21], [94mLoss[0m : 2.64220
[1mStep[0m  [12/21], [94mLoss[0m : 2.80414
[1mStep[0m  [14/21], [94mLoss[0m : 2.60625
[1mStep[0m  [16/21], [94mLoss[0m : 2.59709
[1mStep[0m  [18/21], [94mLoss[0m : 2.58475
[1mStep[0m  [20/21], [94mLoss[0m : 2.69158

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.677, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70940
[1mStep[0m  [2/21], [94mLoss[0m : 2.60088
[1mStep[0m  [4/21], [94mLoss[0m : 2.62274
[1mStep[0m  [6/21], [94mLoss[0m : 2.63695
[1mStep[0m  [8/21], [94mLoss[0m : 2.39527
[1mStep[0m  [10/21], [94mLoss[0m : 2.61947
[1mStep[0m  [12/21], [94mLoss[0m : 2.59907
[1mStep[0m  [14/21], [94mLoss[0m : 2.81605
[1mStep[0m  [16/21], [94mLoss[0m : 2.58716
[1mStep[0m  [18/21], [94mLoss[0m : 2.55774
[1mStep[0m  [20/21], [94mLoss[0m : 2.70381

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.667, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59982
[1mStep[0m  [2/21], [94mLoss[0m : 2.62528
[1mStep[0m  [4/21], [94mLoss[0m : 2.66885
[1mStep[0m  [6/21], [94mLoss[0m : 2.58162
[1mStep[0m  [8/21], [94mLoss[0m : 2.65622
[1mStep[0m  [10/21], [94mLoss[0m : 2.59946
[1mStep[0m  [12/21], [94mLoss[0m : 2.78229
[1mStep[0m  [14/21], [94mLoss[0m : 2.52886
[1mStep[0m  [16/21], [94mLoss[0m : 2.73614
[1mStep[0m  [18/21], [94mLoss[0m : 2.66385
[1mStep[0m  [20/21], [94mLoss[0m : 2.58299

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.690, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67701
[1mStep[0m  [2/21], [94mLoss[0m : 2.72931
[1mStep[0m  [4/21], [94mLoss[0m : 2.63937
[1mStep[0m  [6/21], [94mLoss[0m : 2.78480
[1mStep[0m  [8/21], [94mLoss[0m : 2.53052
[1mStep[0m  [10/21], [94mLoss[0m : 2.62839
[1mStep[0m  [12/21], [94mLoss[0m : 2.73492
[1mStep[0m  [14/21], [94mLoss[0m : 2.62066
[1mStep[0m  [16/21], [94mLoss[0m : 2.46582
[1mStep[0m  [18/21], [94mLoss[0m : 2.62689
[1mStep[0m  [20/21], [94mLoss[0m : 2.66523

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.649, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76003
[1mStep[0m  [2/21], [94mLoss[0m : 2.58911
[1mStep[0m  [4/21], [94mLoss[0m : 2.75089
[1mStep[0m  [6/21], [94mLoss[0m : 2.57171
[1mStep[0m  [8/21], [94mLoss[0m : 2.51114
[1mStep[0m  [10/21], [94mLoss[0m : 2.49285
[1mStep[0m  [12/21], [94mLoss[0m : 2.60773
[1mStep[0m  [14/21], [94mLoss[0m : 2.53558
[1mStep[0m  [16/21], [94mLoss[0m : 2.56638
[1mStep[0m  [18/21], [94mLoss[0m : 2.62504
[1mStep[0m  [20/21], [94mLoss[0m : 2.52666

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.608, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46291
[1mStep[0m  [2/21], [94mLoss[0m : 2.72540
[1mStep[0m  [4/21], [94mLoss[0m : 2.53193
[1mStep[0m  [6/21], [94mLoss[0m : 2.60860
[1mStep[0m  [8/21], [94mLoss[0m : 2.59429
[1mStep[0m  [10/21], [94mLoss[0m : 2.72708
[1mStep[0m  [12/21], [94mLoss[0m : 2.50944
[1mStep[0m  [14/21], [94mLoss[0m : 2.64117
[1mStep[0m  [16/21], [94mLoss[0m : 2.61610
[1mStep[0m  [18/21], [94mLoss[0m : 2.61907
[1mStep[0m  [20/21], [94mLoss[0m : 2.49184

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.578, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60213
[1mStep[0m  [2/21], [94mLoss[0m : 2.64174
[1mStep[0m  [4/21], [94mLoss[0m : 2.57869
[1mStep[0m  [6/21], [94mLoss[0m : 2.54652
[1mStep[0m  [8/21], [94mLoss[0m : 2.56127
[1mStep[0m  [10/21], [94mLoss[0m : 2.56832
[1mStep[0m  [12/21], [94mLoss[0m : 2.72221
[1mStep[0m  [14/21], [94mLoss[0m : 2.54986
[1mStep[0m  [16/21], [94mLoss[0m : 2.55339
[1mStep[0m  [18/21], [94mLoss[0m : 2.65097
[1mStep[0m  [20/21], [94mLoss[0m : 2.64162

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.602, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58814
[1mStep[0m  [2/21], [94mLoss[0m : 2.65410
[1mStep[0m  [4/21], [94mLoss[0m : 2.61698
[1mStep[0m  [6/21], [94mLoss[0m : 2.51309
[1mStep[0m  [8/21], [94mLoss[0m : 2.46758
[1mStep[0m  [10/21], [94mLoss[0m : 2.62123
[1mStep[0m  [12/21], [94mLoss[0m : 2.46448
[1mStep[0m  [14/21], [94mLoss[0m : 2.49932
[1mStep[0m  [16/21], [94mLoss[0m : 2.51257
[1mStep[0m  [18/21], [94mLoss[0m : 2.57242
[1mStep[0m  [20/21], [94mLoss[0m : 2.57203

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.581, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52036
[1mStep[0m  [2/21], [94mLoss[0m : 2.63720
[1mStep[0m  [4/21], [94mLoss[0m : 2.56679
[1mStep[0m  [6/21], [94mLoss[0m : 2.46940
[1mStep[0m  [8/21], [94mLoss[0m : 2.43768
[1mStep[0m  [10/21], [94mLoss[0m : 2.56917
[1mStep[0m  [12/21], [94mLoss[0m : 2.69443
[1mStep[0m  [14/21], [94mLoss[0m : 2.72181
[1mStep[0m  [16/21], [94mLoss[0m : 2.56533
[1mStep[0m  [18/21], [94mLoss[0m : 2.75452
[1mStep[0m  [20/21], [94mLoss[0m : 2.39597

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.575, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53461
[1mStep[0m  [2/21], [94mLoss[0m : 2.62592
[1mStep[0m  [4/21], [94mLoss[0m : 2.65941
[1mStep[0m  [6/21], [94mLoss[0m : 2.72803
[1mStep[0m  [8/21], [94mLoss[0m : 2.52916
[1mStep[0m  [10/21], [94mLoss[0m : 2.59926
[1mStep[0m  [12/21], [94mLoss[0m : 2.57507
[1mStep[0m  [14/21], [94mLoss[0m : 2.68062
[1mStep[0m  [16/21], [94mLoss[0m : 2.54003
[1mStep[0m  [18/21], [94mLoss[0m : 2.64675
[1mStep[0m  [20/21], [94mLoss[0m : 2.56381

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49527
[1mStep[0m  [2/21], [94mLoss[0m : 2.54590
[1mStep[0m  [4/21], [94mLoss[0m : 2.50575
[1mStep[0m  [6/21], [94mLoss[0m : 2.64068
[1mStep[0m  [8/21], [94mLoss[0m : 2.56583
[1mStep[0m  [10/21], [94mLoss[0m : 2.48844
[1mStep[0m  [12/21], [94mLoss[0m : 2.50261
[1mStep[0m  [14/21], [94mLoss[0m : 2.53282
[1mStep[0m  [16/21], [94mLoss[0m : 2.68402
[1mStep[0m  [18/21], [94mLoss[0m : 2.76379
[1mStep[0m  [20/21], [94mLoss[0m : 2.55588

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.548, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49897
[1mStep[0m  [2/21], [94mLoss[0m : 2.62123
[1mStep[0m  [4/21], [94mLoss[0m : 2.58389
[1mStep[0m  [6/21], [94mLoss[0m : 2.55224
[1mStep[0m  [8/21], [94mLoss[0m : 2.53582
[1mStep[0m  [10/21], [94mLoss[0m : 2.55942
[1mStep[0m  [12/21], [94mLoss[0m : 2.62697
[1mStep[0m  [14/21], [94mLoss[0m : 2.73000
[1mStep[0m  [16/21], [94mLoss[0m : 2.42440
[1mStep[0m  [18/21], [94mLoss[0m : 2.39340
[1mStep[0m  [20/21], [94mLoss[0m : 2.48903

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.550, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65595
[1mStep[0m  [2/21], [94mLoss[0m : 2.70662
[1mStep[0m  [4/21], [94mLoss[0m : 2.40336
[1mStep[0m  [6/21], [94mLoss[0m : 2.44248
[1mStep[0m  [8/21], [94mLoss[0m : 2.63726
[1mStep[0m  [10/21], [94mLoss[0m : 2.57827
[1mStep[0m  [12/21], [94mLoss[0m : 2.50458
[1mStep[0m  [14/21], [94mLoss[0m : 2.74098
[1mStep[0m  [16/21], [94mLoss[0m : 2.55437
[1mStep[0m  [18/21], [94mLoss[0m : 2.62366
[1mStep[0m  [20/21], [94mLoss[0m : 2.60129

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.531, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52743
[1mStep[0m  [2/21], [94mLoss[0m : 2.54810
[1mStep[0m  [4/21], [94mLoss[0m : 2.43490
[1mStep[0m  [6/21], [94mLoss[0m : 2.42479
[1mStep[0m  [8/21], [94mLoss[0m : 2.49151
[1mStep[0m  [10/21], [94mLoss[0m : 2.58399
[1mStep[0m  [12/21], [94mLoss[0m : 2.45916
[1mStep[0m  [14/21], [94mLoss[0m : 2.58685
[1mStep[0m  [16/21], [94mLoss[0m : 2.48200
[1mStep[0m  [18/21], [94mLoss[0m : 2.54218
[1mStep[0m  [20/21], [94mLoss[0m : 2.57385

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.514, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55297
[1mStep[0m  [2/21], [94mLoss[0m : 2.41230
[1mStep[0m  [4/21], [94mLoss[0m : 2.47338
[1mStep[0m  [6/21], [94mLoss[0m : 2.63568
[1mStep[0m  [8/21], [94mLoss[0m : 2.59862
[1mStep[0m  [10/21], [94mLoss[0m : 2.55999
[1mStep[0m  [12/21], [94mLoss[0m : 2.45294
[1mStep[0m  [14/21], [94mLoss[0m : 2.51391
[1mStep[0m  [16/21], [94mLoss[0m : 2.48454
[1mStep[0m  [18/21], [94mLoss[0m : 2.42180
[1mStep[0m  [20/21], [94mLoss[0m : 2.49072

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39978
[1mStep[0m  [2/21], [94mLoss[0m : 2.49316
[1mStep[0m  [4/21], [94mLoss[0m : 2.60733
[1mStep[0m  [6/21], [94mLoss[0m : 2.51908
[1mStep[0m  [8/21], [94mLoss[0m : 2.58744
[1mStep[0m  [10/21], [94mLoss[0m : 2.60301
[1mStep[0m  [12/21], [94mLoss[0m : 2.64385
[1mStep[0m  [14/21], [94mLoss[0m : 2.59147
[1mStep[0m  [16/21], [94mLoss[0m : 2.56415
[1mStep[0m  [18/21], [94mLoss[0m : 2.67470
[1mStep[0m  [20/21], [94mLoss[0m : 2.50845

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.497, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50594
[1mStep[0m  [2/21], [94mLoss[0m : 2.59588
[1mStep[0m  [4/21], [94mLoss[0m : 2.68532
[1mStep[0m  [6/21], [94mLoss[0m : 2.58788
[1mStep[0m  [8/21], [94mLoss[0m : 2.54151
[1mStep[0m  [10/21], [94mLoss[0m : 2.57520
[1mStep[0m  [12/21], [94mLoss[0m : 2.59370
[1mStep[0m  [14/21], [94mLoss[0m : 2.50211
[1mStep[0m  [16/21], [94mLoss[0m : 2.38660
[1mStep[0m  [18/21], [94mLoss[0m : 2.61573
[1mStep[0m  [20/21], [94mLoss[0m : 2.54629

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53811
[1mStep[0m  [2/21], [94mLoss[0m : 2.69426
[1mStep[0m  [4/21], [94mLoss[0m : 2.63254
[1mStep[0m  [6/21], [94mLoss[0m : 2.71237
[1mStep[0m  [8/21], [94mLoss[0m : 2.63374
[1mStep[0m  [10/21], [94mLoss[0m : 2.55036
[1mStep[0m  [12/21], [94mLoss[0m : 2.54962
[1mStep[0m  [14/21], [94mLoss[0m : 2.65128
[1mStep[0m  [16/21], [94mLoss[0m : 2.56652
[1mStep[0m  [18/21], [94mLoss[0m : 2.54888
[1mStep[0m  [20/21], [94mLoss[0m : 2.63802

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.468, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.428
====================================

Phase 1 - Evaluation MAE:  2.42840313911438
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.41532
[1mStep[0m  [2/21], [94mLoss[0m : 2.63332
[1mStep[0m  [4/21], [94mLoss[0m : 2.69240
[1mStep[0m  [6/21], [94mLoss[0m : 2.49509
[1mStep[0m  [8/21], [94mLoss[0m : 2.56287
[1mStep[0m  [10/21], [94mLoss[0m : 2.61449
[1mStep[0m  [12/21], [94mLoss[0m : 2.57037
[1mStep[0m  [14/21], [94mLoss[0m : 2.69583
[1mStep[0m  [16/21], [94mLoss[0m : 2.71532
[1mStep[0m  [18/21], [94mLoss[0m : 2.51986
[1mStep[0m  [20/21], [94mLoss[0m : 2.69808

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59530
[1mStep[0m  [2/21], [94mLoss[0m : 2.60868
[1mStep[0m  [4/21], [94mLoss[0m : 2.40939
[1mStep[0m  [6/21], [94mLoss[0m : 2.75188
[1mStep[0m  [8/21], [94mLoss[0m : 2.51409
[1mStep[0m  [10/21], [94mLoss[0m : 2.61010
[1mStep[0m  [12/21], [94mLoss[0m : 2.69389
[1mStep[0m  [14/21], [94mLoss[0m : 2.63341
[1mStep[0m  [16/21], [94mLoss[0m : 2.66912
[1mStep[0m  [18/21], [94mLoss[0m : 2.67970
[1mStep[0m  [20/21], [94mLoss[0m : 2.46828

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.855, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42564
[1mStep[0m  [2/21], [94mLoss[0m : 2.47378
[1mStep[0m  [4/21], [94mLoss[0m : 2.39982
[1mStep[0m  [6/21], [94mLoss[0m : 2.51617
[1mStep[0m  [8/21], [94mLoss[0m : 2.59855
[1mStep[0m  [10/21], [94mLoss[0m : 2.56140
[1mStep[0m  [12/21], [94mLoss[0m : 2.57480
[1mStep[0m  [14/21], [94mLoss[0m : 2.45004
[1mStep[0m  [16/21], [94mLoss[0m : 2.53709
[1mStep[0m  [18/21], [94mLoss[0m : 2.69748
[1mStep[0m  [20/21], [94mLoss[0m : 2.61734

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.531, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72802
[1mStep[0m  [2/21], [94mLoss[0m : 2.57339
[1mStep[0m  [4/21], [94mLoss[0m : 2.53295
[1mStep[0m  [6/21], [94mLoss[0m : 2.35098
[1mStep[0m  [8/21], [94mLoss[0m : 2.36882
[1mStep[0m  [10/21], [94mLoss[0m : 2.60436
[1mStep[0m  [12/21], [94mLoss[0m : 2.58887
[1mStep[0m  [14/21], [94mLoss[0m : 2.51857
[1mStep[0m  [16/21], [94mLoss[0m : 2.52302
[1mStep[0m  [18/21], [94mLoss[0m : 2.47194
[1mStep[0m  [20/21], [94mLoss[0m : 2.56575

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33553
[1mStep[0m  [2/21], [94mLoss[0m : 2.53066
[1mStep[0m  [4/21], [94mLoss[0m : 2.59690
[1mStep[0m  [6/21], [94mLoss[0m : 2.59792
[1mStep[0m  [8/21], [94mLoss[0m : 2.50860
[1mStep[0m  [10/21], [94mLoss[0m : 2.48462
[1mStep[0m  [12/21], [94mLoss[0m : 2.42900
[1mStep[0m  [14/21], [94mLoss[0m : 2.56473
[1mStep[0m  [16/21], [94mLoss[0m : 2.40075
[1mStep[0m  [18/21], [94mLoss[0m : 2.47897
[1mStep[0m  [20/21], [94mLoss[0m : 2.60693

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61579
[1mStep[0m  [2/21], [94mLoss[0m : 2.44753
[1mStep[0m  [4/21], [94mLoss[0m : 2.44003
[1mStep[0m  [6/21], [94mLoss[0m : 2.43226
[1mStep[0m  [8/21], [94mLoss[0m : 2.47389
[1mStep[0m  [10/21], [94mLoss[0m : 2.47518
[1mStep[0m  [12/21], [94mLoss[0m : 2.58606
[1mStep[0m  [14/21], [94mLoss[0m : 2.60566
[1mStep[0m  [16/21], [94mLoss[0m : 2.43611
[1mStep[0m  [18/21], [94mLoss[0m : 2.51682
[1mStep[0m  [20/21], [94mLoss[0m : 2.43365

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36994
[1mStep[0m  [2/21], [94mLoss[0m : 2.36040
[1mStep[0m  [4/21], [94mLoss[0m : 2.46300
[1mStep[0m  [6/21], [94mLoss[0m : 2.61460
[1mStep[0m  [8/21], [94mLoss[0m : 2.37797
[1mStep[0m  [10/21], [94mLoss[0m : 2.43786
[1mStep[0m  [12/21], [94mLoss[0m : 2.43485
[1mStep[0m  [14/21], [94mLoss[0m : 2.44167
[1mStep[0m  [16/21], [94mLoss[0m : 2.37100
[1mStep[0m  [18/21], [94mLoss[0m : 2.45621
[1mStep[0m  [20/21], [94mLoss[0m : 2.55217

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38635
[1mStep[0m  [2/21], [94mLoss[0m : 2.41634
[1mStep[0m  [4/21], [94mLoss[0m : 2.40442
[1mStep[0m  [6/21], [94mLoss[0m : 2.44477
[1mStep[0m  [8/21], [94mLoss[0m : 2.53736
[1mStep[0m  [10/21], [94mLoss[0m : 2.51108
[1mStep[0m  [12/21], [94mLoss[0m : 2.38991
[1mStep[0m  [14/21], [94mLoss[0m : 2.35090
[1mStep[0m  [16/21], [94mLoss[0m : 2.53017
[1mStep[0m  [18/21], [94mLoss[0m : 2.46502
[1mStep[0m  [20/21], [94mLoss[0m : 2.33012

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47768
[1mStep[0m  [2/21], [94mLoss[0m : 2.33314
[1mStep[0m  [4/21], [94mLoss[0m : 2.53905
[1mStep[0m  [6/21], [94mLoss[0m : 2.35538
[1mStep[0m  [8/21], [94mLoss[0m : 2.43182
[1mStep[0m  [10/21], [94mLoss[0m : 2.41620
[1mStep[0m  [12/21], [94mLoss[0m : 2.23329
[1mStep[0m  [14/21], [94mLoss[0m : 2.38267
[1mStep[0m  [16/21], [94mLoss[0m : 2.47753
[1mStep[0m  [18/21], [94mLoss[0m : 2.38949
[1mStep[0m  [20/21], [94mLoss[0m : 2.37654

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.21185
[1mStep[0m  [2/21], [94mLoss[0m : 2.27118
[1mStep[0m  [4/21], [94mLoss[0m : 2.28029
[1mStep[0m  [6/21], [94mLoss[0m : 2.32238
[1mStep[0m  [8/21], [94mLoss[0m : 2.39965
[1mStep[0m  [10/21], [94mLoss[0m : 2.36992
[1mStep[0m  [12/21], [94mLoss[0m : 2.38080
[1mStep[0m  [14/21], [94mLoss[0m : 2.41334
[1mStep[0m  [16/21], [94mLoss[0m : 2.25684
[1mStep[0m  [18/21], [94mLoss[0m : 2.27291
[1mStep[0m  [20/21], [94mLoss[0m : 2.34385

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15453
[1mStep[0m  [2/21], [94mLoss[0m : 2.42424
[1mStep[0m  [4/21], [94mLoss[0m : 2.44696
[1mStep[0m  [6/21], [94mLoss[0m : 2.38666
[1mStep[0m  [8/21], [94mLoss[0m : 2.30327
[1mStep[0m  [10/21], [94mLoss[0m : 2.18962
[1mStep[0m  [12/21], [94mLoss[0m : 2.35261
[1mStep[0m  [14/21], [94mLoss[0m : 2.32668
[1mStep[0m  [16/21], [94mLoss[0m : 2.38449
[1mStep[0m  [18/21], [94mLoss[0m : 2.30788
[1mStep[0m  [20/21], [94mLoss[0m : 2.41384

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39448
[1mStep[0m  [2/21], [94mLoss[0m : 2.45716
[1mStep[0m  [4/21], [94mLoss[0m : 2.50234
[1mStep[0m  [6/21], [94mLoss[0m : 2.27447
[1mStep[0m  [8/21], [94mLoss[0m : 2.30738
[1mStep[0m  [10/21], [94mLoss[0m : 2.27216
[1mStep[0m  [12/21], [94mLoss[0m : 2.15316
[1mStep[0m  [14/21], [94mLoss[0m : 2.28030
[1mStep[0m  [16/21], [94mLoss[0m : 2.22574
[1mStep[0m  [18/21], [94mLoss[0m : 2.41167
[1mStep[0m  [20/21], [94mLoss[0m : 2.46382

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48618
[1mStep[0m  [2/21], [94mLoss[0m : 2.30521
[1mStep[0m  [4/21], [94mLoss[0m : 2.15161
[1mStep[0m  [6/21], [94mLoss[0m : 2.28654
[1mStep[0m  [8/21], [94mLoss[0m : 2.26327
[1mStep[0m  [10/21], [94mLoss[0m : 2.32270
[1mStep[0m  [12/21], [94mLoss[0m : 2.21199
[1mStep[0m  [14/21], [94mLoss[0m : 2.21195
[1mStep[0m  [16/21], [94mLoss[0m : 2.30451
[1mStep[0m  [18/21], [94mLoss[0m : 2.17518
[1mStep[0m  [20/21], [94mLoss[0m : 2.34792

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.21033
[1mStep[0m  [2/21], [94mLoss[0m : 2.30907
[1mStep[0m  [4/21], [94mLoss[0m : 2.23922
[1mStep[0m  [6/21], [94mLoss[0m : 2.25982
[1mStep[0m  [8/21], [94mLoss[0m : 2.25144
[1mStep[0m  [10/21], [94mLoss[0m : 2.20189
[1mStep[0m  [12/21], [94mLoss[0m : 2.23886
[1mStep[0m  [14/21], [94mLoss[0m : 2.25439
[1mStep[0m  [16/21], [94mLoss[0m : 2.17194
[1mStep[0m  [18/21], [94mLoss[0m : 2.32250
[1mStep[0m  [20/21], [94mLoss[0m : 2.17203

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26293
[1mStep[0m  [2/21], [94mLoss[0m : 2.14673
[1mStep[0m  [4/21], [94mLoss[0m : 2.16811
[1mStep[0m  [6/21], [94mLoss[0m : 2.06467
[1mStep[0m  [8/21], [94mLoss[0m : 2.33388
[1mStep[0m  [10/21], [94mLoss[0m : 2.27002
[1mStep[0m  [12/21], [94mLoss[0m : 2.15036
[1mStep[0m  [14/21], [94mLoss[0m : 2.13506
[1mStep[0m  [16/21], [94mLoss[0m : 2.25011
[1mStep[0m  [18/21], [94mLoss[0m : 2.12635
[1mStep[0m  [20/21], [94mLoss[0m : 2.18403

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.228, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12226
[1mStep[0m  [2/21], [94mLoss[0m : 2.16396
[1mStep[0m  [4/21], [94mLoss[0m : 2.08777
[1mStep[0m  [6/21], [94mLoss[0m : 2.04934
[1mStep[0m  [8/21], [94mLoss[0m : 2.12599
[1mStep[0m  [10/21], [94mLoss[0m : 2.15028
[1mStep[0m  [12/21], [94mLoss[0m : 2.32799
[1mStep[0m  [14/21], [94mLoss[0m : 2.15952
[1mStep[0m  [16/21], [94mLoss[0m : 2.28884
[1mStep[0m  [18/21], [94mLoss[0m : 2.15009
[1mStep[0m  [20/21], [94mLoss[0m : 2.10360

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13762
[1mStep[0m  [2/21], [94mLoss[0m : 2.08984
[1mStep[0m  [4/21], [94mLoss[0m : 2.24629
[1mStep[0m  [6/21], [94mLoss[0m : 2.05730
[1mStep[0m  [8/21], [94mLoss[0m : 2.01621
[1mStep[0m  [10/21], [94mLoss[0m : 2.16015
[1mStep[0m  [12/21], [94mLoss[0m : 2.08584
[1mStep[0m  [14/21], [94mLoss[0m : 2.20323
[1mStep[0m  [16/21], [94mLoss[0m : 2.24852
[1mStep[0m  [18/21], [94mLoss[0m : 2.23621
[1mStep[0m  [20/21], [94mLoss[0m : 2.07299

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.04121
[1mStep[0m  [2/21], [94mLoss[0m : 1.97549
[1mStep[0m  [4/21], [94mLoss[0m : 2.16573
[1mStep[0m  [6/21], [94mLoss[0m : 2.11107
[1mStep[0m  [8/21], [94mLoss[0m : 2.23931
[1mStep[0m  [10/21], [94mLoss[0m : 2.00115
[1mStep[0m  [12/21], [94mLoss[0m : 2.20220
[1mStep[0m  [14/21], [94mLoss[0m : 2.14026
[1mStep[0m  [16/21], [94mLoss[0m : 2.24923
[1mStep[0m  [18/21], [94mLoss[0m : 2.27656
[1mStep[0m  [20/21], [94mLoss[0m : 2.11853

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.05003
[1mStep[0m  [2/21], [94mLoss[0m : 2.02380
[1mStep[0m  [4/21], [94mLoss[0m : 2.13268
[1mStep[0m  [6/21], [94mLoss[0m : 2.08325
[1mStep[0m  [8/21], [94mLoss[0m : 2.20914
[1mStep[0m  [10/21], [94mLoss[0m : 2.09608
[1mStep[0m  [12/21], [94mLoss[0m : 2.23436
[1mStep[0m  [14/21], [94mLoss[0m : 2.31440
[1mStep[0m  [16/21], [94mLoss[0m : 2.05586
[1mStep[0m  [18/21], [94mLoss[0m : 2.15851
[1mStep[0m  [20/21], [94mLoss[0m : 2.10587

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.505, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.04327
[1mStep[0m  [2/21], [94mLoss[0m : 2.15428
[1mStep[0m  [4/21], [94mLoss[0m : 2.10879
[1mStep[0m  [6/21], [94mLoss[0m : 2.05814
[1mStep[0m  [8/21], [94mLoss[0m : 2.17192
[1mStep[0m  [10/21], [94mLoss[0m : 2.09868
[1mStep[0m  [12/21], [94mLoss[0m : 2.08339
[1mStep[0m  [14/21], [94mLoss[0m : 2.10197
[1mStep[0m  [16/21], [94mLoss[0m : 2.09165
[1mStep[0m  [18/21], [94mLoss[0m : 2.18638
[1mStep[0m  [20/21], [94mLoss[0m : 2.14532

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.551, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09928
[1mStep[0m  [2/21], [94mLoss[0m : 2.05989
[1mStep[0m  [4/21], [94mLoss[0m : 2.03441
[1mStep[0m  [6/21], [94mLoss[0m : 2.01568
[1mStep[0m  [8/21], [94mLoss[0m : 2.05116
[1mStep[0m  [10/21], [94mLoss[0m : 1.97729
[1mStep[0m  [12/21], [94mLoss[0m : 1.93705
[1mStep[0m  [14/21], [94mLoss[0m : 1.88512
[1mStep[0m  [16/21], [94mLoss[0m : 1.96142
[1mStep[0m  [18/21], [94mLoss[0m : 2.14112
[1mStep[0m  [20/21], [94mLoss[0m : 2.06158

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98986
[1mStep[0m  [2/21], [94mLoss[0m : 1.89311
[1mStep[0m  [4/21], [94mLoss[0m : 1.99126
[1mStep[0m  [6/21], [94mLoss[0m : 1.97878
[1mStep[0m  [8/21], [94mLoss[0m : 1.92998
[1mStep[0m  [10/21], [94mLoss[0m : 1.98091
[1mStep[0m  [12/21], [94mLoss[0m : 1.86616
[1mStep[0m  [14/21], [94mLoss[0m : 2.00601
[1mStep[0m  [16/21], [94mLoss[0m : 1.96673
[1mStep[0m  [18/21], [94mLoss[0m : 1.95604
[1mStep[0m  [20/21], [94mLoss[0m : 1.97493

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.523, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06048
[1mStep[0m  [2/21], [94mLoss[0m : 2.09808
[1mStep[0m  [4/21], [94mLoss[0m : 1.95846
[1mStep[0m  [6/21], [94mLoss[0m : 2.08330
[1mStep[0m  [8/21], [94mLoss[0m : 1.88510
[1mStep[0m  [10/21], [94mLoss[0m : 1.94312
[1mStep[0m  [12/21], [94mLoss[0m : 1.97644
[1mStep[0m  [14/21], [94mLoss[0m : 2.00086
[1mStep[0m  [16/21], [94mLoss[0m : 2.05382
[1mStep[0m  [18/21], [94mLoss[0m : 1.88969
[1mStep[0m  [20/21], [94mLoss[0m : 1.97253

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.555, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.87147
[1mStep[0m  [2/21], [94mLoss[0m : 1.96837
[1mStep[0m  [4/21], [94mLoss[0m : 1.89503
[1mStep[0m  [6/21], [94mLoss[0m : 1.85118
[1mStep[0m  [8/21], [94mLoss[0m : 1.98350
[1mStep[0m  [10/21], [94mLoss[0m : 2.06484
[1mStep[0m  [12/21], [94mLoss[0m : 1.91684
[1mStep[0m  [14/21], [94mLoss[0m : 2.02433
[1mStep[0m  [16/21], [94mLoss[0m : 1.91492
[1mStep[0m  [18/21], [94mLoss[0m : 2.02115
[1mStep[0m  [20/21], [94mLoss[0m : 1.97856

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.568, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82289
[1mStep[0m  [2/21], [94mLoss[0m : 1.84465
[1mStep[0m  [4/21], [94mLoss[0m : 1.94888
[1mStep[0m  [6/21], [94mLoss[0m : 1.86987
[1mStep[0m  [8/21], [94mLoss[0m : 1.96345
[1mStep[0m  [10/21], [94mLoss[0m : 1.91613
[1mStep[0m  [12/21], [94mLoss[0m : 1.83749
[1mStep[0m  [14/21], [94mLoss[0m : 1.82851
[1mStep[0m  [16/21], [94mLoss[0m : 1.82511
[1mStep[0m  [18/21], [94mLoss[0m : 1.95209
[1mStep[0m  [20/21], [94mLoss[0m : 2.03825

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.904, [92mTest[0m: 2.565, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99609
[1mStep[0m  [2/21], [94mLoss[0m : 1.84073
[1mStep[0m  [4/21], [94mLoss[0m : 1.71163
[1mStep[0m  [6/21], [94mLoss[0m : 1.79745
[1mStep[0m  [8/21], [94mLoss[0m : 1.89950
[1mStep[0m  [10/21], [94mLoss[0m : 2.03688
[1mStep[0m  [12/21], [94mLoss[0m : 1.81751
[1mStep[0m  [14/21], [94mLoss[0m : 1.94802
[1mStep[0m  [16/21], [94mLoss[0m : 1.93463
[1mStep[0m  [18/21], [94mLoss[0m : 1.94042
[1mStep[0m  [20/21], [94mLoss[0m : 1.89311

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.891, [92mTest[0m: 2.595, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78765
[1mStep[0m  [2/21], [94mLoss[0m : 1.85275
[1mStep[0m  [4/21], [94mLoss[0m : 1.92826
[1mStep[0m  [6/21], [94mLoss[0m : 1.86798
[1mStep[0m  [8/21], [94mLoss[0m : 1.75146
[1mStep[0m  [10/21], [94mLoss[0m : 1.88699
[1mStep[0m  [12/21], [94mLoss[0m : 1.91012
[1mStep[0m  [14/21], [94mLoss[0m : 1.91978
[1mStep[0m  [16/21], [94mLoss[0m : 1.96239
[1mStep[0m  [18/21], [94mLoss[0m : 2.06937
[1mStep[0m  [20/21], [94mLoss[0m : 1.80795

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.630, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.87914
[1mStep[0m  [2/21], [94mLoss[0m : 1.78211
[1mStep[0m  [4/21], [94mLoss[0m : 1.93206
[1mStep[0m  [6/21], [94mLoss[0m : 1.86513
[1mStep[0m  [8/21], [94mLoss[0m : 1.85669
[1mStep[0m  [10/21], [94mLoss[0m : 1.94776
[1mStep[0m  [12/21], [94mLoss[0m : 1.89762
[1mStep[0m  [14/21], [94mLoss[0m : 1.85429
[1mStep[0m  [16/21], [94mLoss[0m : 1.90441
[1mStep[0m  [18/21], [94mLoss[0m : 1.81499
[1mStep[0m  [20/21], [94mLoss[0m : 1.90394

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.568, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82634
[1mStep[0m  [2/21], [94mLoss[0m : 1.83776
[1mStep[0m  [4/21], [94mLoss[0m : 1.82861
[1mStep[0m  [6/21], [94mLoss[0m : 1.86918
[1mStep[0m  [8/21], [94mLoss[0m : 1.96871
[1mStep[0m  [10/21], [94mLoss[0m : 1.96160
[1mStep[0m  [12/21], [94mLoss[0m : 1.92982
[1mStep[0m  [14/21], [94mLoss[0m : 1.82002
[1mStep[0m  [16/21], [94mLoss[0m : 1.85355
[1mStep[0m  [18/21], [94mLoss[0m : 1.87795
[1mStep[0m  [20/21], [94mLoss[0m : 1.79534

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.543, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77130
[1mStep[0m  [2/21], [94mLoss[0m : 1.81800
[1mStep[0m  [4/21], [94mLoss[0m : 1.67634
[1mStep[0m  [6/21], [94mLoss[0m : 1.80793
[1mStep[0m  [8/21], [94mLoss[0m : 1.80913
[1mStep[0m  [10/21], [94mLoss[0m : 1.76498
[1mStep[0m  [12/21], [94mLoss[0m : 1.88876
[1mStep[0m  [14/21], [94mLoss[0m : 1.90532
[1mStep[0m  [16/21], [94mLoss[0m : 1.87512
[1mStep[0m  [18/21], [94mLoss[0m : 1.84892
[1mStep[0m  [20/21], [94mLoss[0m : 1.82810

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.541, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.587
====================================

Phase 2 - Evaluation MAE:  2.5869756085532054
MAE score P1        2.428403
MAE score P2        2.586976
loss                 1.81737
learning_rate           0.01
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.5
weight_decay            0.01
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.84647
[1mStep[0m  [2/21], [94mLoss[0m : 10.90322
[1mStep[0m  [4/21], [94mLoss[0m : 10.73871
[1mStep[0m  [6/21], [94mLoss[0m : 10.83782
[1mStep[0m  [8/21], [94mLoss[0m : 10.42852
[1mStep[0m  [10/21], [94mLoss[0m : 10.71287
[1mStep[0m  [12/21], [94mLoss[0m : 10.74484
[1mStep[0m  [14/21], [94mLoss[0m : 10.61272
[1mStep[0m  [16/21], [94mLoss[0m : 10.49617
[1mStep[0m  [18/21], [94mLoss[0m : 10.52023
[1mStep[0m  [20/21], [94mLoss[0m : 10.55307

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.667, [92mTest[0m: 10.810, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.36449
[1mStep[0m  [2/21], [94mLoss[0m : 10.43452
[1mStep[0m  [4/21], [94mLoss[0m : 10.59815
[1mStep[0m  [6/21], [94mLoss[0m : 10.21147
[1mStep[0m  [8/21], [94mLoss[0m : 10.21925
[1mStep[0m  [10/21], [94mLoss[0m : 10.22521
[1mStep[0m  [12/21], [94mLoss[0m : 10.09314
[1mStep[0m  [14/21], [94mLoss[0m : 9.95050
[1mStep[0m  [16/21], [94mLoss[0m : 9.90780
[1mStep[0m  [18/21], [94mLoss[0m : 10.31514
[1mStep[0m  [20/21], [94mLoss[0m : 10.02358

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.165, [92mTest[0m: 10.283, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.90891
[1mStep[0m  [2/21], [94mLoss[0m : 9.63066
[1mStep[0m  [4/21], [94mLoss[0m : 9.94603
[1mStep[0m  [6/21], [94mLoss[0m : 9.85783
[1mStep[0m  [8/21], [94mLoss[0m : 9.83228
[1mStep[0m  [10/21], [94mLoss[0m : 9.55675
[1mStep[0m  [12/21], [94mLoss[0m : 9.46321
[1mStep[0m  [14/21], [94mLoss[0m : 9.26973
[1mStep[0m  [16/21], [94mLoss[0m : 9.27960
[1mStep[0m  [18/21], [94mLoss[0m : 9.30318
[1mStep[0m  [20/21], [94mLoss[0m : 9.21881

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.594, [92mTest[0m: 9.662, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.45765
[1mStep[0m  [2/21], [94mLoss[0m : 9.31243
[1mStep[0m  [4/21], [94mLoss[0m : 9.29770
[1mStep[0m  [6/21], [94mLoss[0m : 9.01204
[1mStep[0m  [8/21], [94mLoss[0m : 9.00199
[1mStep[0m  [10/21], [94mLoss[0m : 8.75584
[1mStep[0m  [12/21], [94mLoss[0m : 9.01091
[1mStep[0m  [14/21], [94mLoss[0m : 8.74010
[1mStep[0m  [16/21], [94mLoss[0m : 9.08226
[1mStep[0m  [18/21], [94mLoss[0m : 8.52894
[1mStep[0m  [20/21], [94mLoss[0m : 8.59951

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.952, [92mTest[0m: 8.942, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.71403
[1mStep[0m  [2/21], [94mLoss[0m : 8.64409
[1mStep[0m  [4/21], [94mLoss[0m : 8.57727
[1mStep[0m  [6/21], [94mLoss[0m : 8.19468
[1mStep[0m  [8/21], [94mLoss[0m : 8.24138
[1mStep[0m  [10/21], [94mLoss[0m : 8.04786
[1mStep[0m  [12/21], [94mLoss[0m : 8.05062
[1mStep[0m  [14/21], [94mLoss[0m : 7.80992
[1mStep[0m  [16/21], [94mLoss[0m : 8.17100
[1mStep[0m  [18/21], [94mLoss[0m : 7.95481
[1mStep[0m  [20/21], [94mLoss[0m : 7.88927

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.138, [92mTest[0m: 8.106, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.61972
[1mStep[0m  [2/21], [94mLoss[0m : 7.32699
[1mStep[0m  [4/21], [94mLoss[0m : 7.41149
[1mStep[0m  [6/21], [94mLoss[0m : 7.26168
[1mStep[0m  [8/21], [94mLoss[0m : 7.40985
[1mStep[0m  [10/21], [94mLoss[0m : 7.19631
[1mStep[0m  [12/21], [94mLoss[0m : 7.22093
[1mStep[0m  [14/21], [94mLoss[0m : 7.05847
[1mStep[0m  [16/21], [94mLoss[0m : 6.83302
[1mStep[0m  [18/21], [94mLoss[0m : 7.14704
[1mStep[0m  [20/21], [94mLoss[0m : 6.68463

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.198, [92mTest[0m: 7.177, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.42387
[1mStep[0m  [2/21], [94mLoss[0m : 6.70802
[1mStep[0m  [4/21], [94mLoss[0m : 6.52939
[1mStep[0m  [6/21], [94mLoss[0m : 6.41914
[1mStep[0m  [8/21], [94mLoss[0m : 6.35227
[1mStep[0m  [10/21], [94mLoss[0m : 6.33265
[1mStep[0m  [12/21], [94mLoss[0m : 6.02790
[1mStep[0m  [14/21], [94mLoss[0m : 5.96899
[1mStep[0m  [16/21], [94mLoss[0m : 6.09789
[1mStep[0m  [18/21], [94mLoss[0m : 6.12826
[1mStep[0m  [20/21], [94mLoss[0m : 5.93080

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.265, [92mTest[0m: 6.102, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.86007
[1mStep[0m  [2/21], [94mLoss[0m : 5.90824
[1mStep[0m  [4/21], [94mLoss[0m : 5.55894
[1mStep[0m  [6/21], [94mLoss[0m : 5.44159
[1mStep[0m  [8/21], [94mLoss[0m : 5.52455
[1mStep[0m  [10/21], [94mLoss[0m : 5.14645
[1mStep[0m  [12/21], [94mLoss[0m : 5.25740
[1mStep[0m  [14/21], [94mLoss[0m : 5.05252
[1mStep[0m  [16/21], [94mLoss[0m : 5.01484
[1mStep[0m  [18/21], [94mLoss[0m : 4.81500
[1mStep[0m  [20/21], [94mLoss[0m : 5.13076

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.333, [92mTest[0m: 4.893, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.14916
[1mStep[0m  [2/21], [94mLoss[0m : 4.35482
[1mStep[0m  [4/21], [94mLoss[0m : 4.68175
[1mStep[0m  [6/21], [94mLoss[0m : 4.38489
[1mStep[0m  [8/21], [94mLoss[0m : 4.30352
[1mStep[0m  [10/21], [94mLoss[0m : 4.10595
[1mStep[0m  [12/21], [94mLoss[0m : 4.19594
[1mStep[0m  [14/21], [94mLoss[0m : 4.14267
[1mStep[0m  [16/21], [94mLoss[0m : 3.94815
[1mStep[0m  [18/21], [94mLoss[0m : 4.09894
[1mStep[0m  [20/21], [94mLoss[0m : 3.68222

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.290, [92mTest[0m: 3.782, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.85907
[1mStep[0m  [2/21], [94mLoss[0m : 3.75763
[1mStep[0m  [4/21], [94mLoss[0m : 3.72629
[1mStep[0m  [6/21], [94mLoss[0m : 3.44561
[1mStep[0m  [8/21], [94mLoss[0m : 3.57574
[1mStep[0m  [10/21], [94mLoss[0m : 3.40953
[1mStep[0m  [12/21], [94mLoss[0m : 3.38923
[1mStep[0m  [14/21], [94mLoss[0m : 3.34734
[1mStep[0m  [16/21], [94mLoss[0m : 3.33495
[1mStep[0m  [18/21], [94mLoss[0m : 2.92651
[1mStep[0m  [20/21], [94mLoss[0m : 3.12215

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.435, [92mTest[0m: 2.948, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.11701
[1mStep[0m  [2/21], [94mLoss[0m : 3.20860
[1mStep[0m  [4/21], [94mLoss[0m : 3.01437
[1mStep[0m  [6/21], [94mLoss[0m : 3.33907
[1mStep[0m  [8/21], [94mLoss[0m : 2.98363
[1mStep[0m  [10/21], [94mLoss[0m : 3.13597
[1mStep[0m  [12/21], [94mLoss[0m : 2.91644
[1mStep[0m  [14/21], [94mLoss[0m : 2.87865
[1mStep[0m  [16/21], [94mLoss[0m : 2.77662
[1mStep[0m  [18/21], [94mLoss[0m : 3.09505
[1mStep[0m  [20/21], [94mLoss[0m : 2.88851

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.019, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.90149
[1mStep[0m  [2/21], [94mLoss[0m : 3.10861
[1mStep[0m  [4/21], [94mLoss[0m : 2.92040
[1mStep[0m  [6/21], [94mLoss[0m : 2.88228
[1mStep[0m  [8/21], [94mLoss[0m : 2.82524
[1mStep[0m  [10/21], [94mLoss[0m : 2.73700
[1mStep[0m  [12/21], [94mLoss[0m : 2.82663
[1mStep[0m  [14/21], [94mLoss[0m : 2.78360
[1mStep[0m  [16/21], [94mLoss[0m : 2.81043
[1mStep[0m  [18/21], [94mLoss[0m : 2.82873
[1mStep[0m  [20/21], [94mLoss[0m : 2.87178

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.865, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71071
[1mStep[0m  [2/21], [94mLoss[0m : 2.80597
[1mStep[0m  [4/21], [94mLoss[0m : 2.72673
[1mStep[0m  [6/21], [94mLoss[0m : 2.87270
[1mStep[0m  [8/21], [94mLoss[0m : 2.54833
[1mStep[0m  [10/21], [94mLoss[0m : 2.75496
[1mStep[0m  [12/21], [94mLoss[0m : 2.75934
[1mStep[0m  [14/21], [94mLoss[0m : 2.88120
[1mStep[0m  [16/21], [94mLoss[0m : 2.75453
[1mStep[0m  [18/21], [94mLoss[0m : 2.82144
[1mStep[0m  [20/21], [94mLoss[0m : 2.77244

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.792, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.94321
[1mStep[0m  [2/21], [94mLoss[0m : 2.65377
[1mStep[0m  [4/21], [94mLoss[0m : 2.74764
[1mStep[0m  [6/21], [94mLoss[0m : 2.66987
[1mStep[0m  [8/21], [94mLoss[0m : 2.88011
[1mStep[0m  [10/21], [94mLoss[0m : 2.79140
[1mStep[0m  [12/21], [94mLoss[0m : 2.77520
[1mStep[0m  [14/21], [94mLoss[0m : 2.88026
[1mStep[0m  [16/21], [94mLoss[0m : 2.81094
[1mStep[0m  [18/21], [94mLoss[0m : 2.84933
[1mStep[0m  [20/21], [94mLoss[0m : 2.62083

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.789, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68614
[1mStep[0m  [2/21], [94mLoss[0m : 2.73715
[1mStep[0m  [4/21], [94mLoss[0m : 2.67590
[1mStep[0m  [6/21], [94mLoss[0m : 2.83664
[1mStep[0m  [8/21], [94mLoss[0m : 2.90153
[1mStep[0m  [10/21], [94mLoss[0m : 2.88576
[1mStep[0m  [12/21], [94mLoss[0m : 2.84583
[1mStep[0m  [14/21], [94mLoss[0m : 2.69473
[1mStep[0m  [16/21], [94mLoss[0m : 2.88436
[1mStep[0m  [18/21], [94mLoss[0m : 2.78527
[1mStep[0m  [20/21], [94mLoss[0m : 2.71128

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.771, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.88437
[1mStep[0m  [2/21], [94mLoss[0m : 2.88313
[1mStep[0m  [4/21], [94mLoss[0m : 2.81591
[1mStep[0m  [6/21], [94mLoss[0m : 2.88302
[1mStep[0m  [8/21], [94mLoss[0m : 2.73559
[1mStep[0m  [10/21], [94mLoss[0m : 2.72391
[1mStep[0m  [12/21], [94mLoss[0m : 2.63474
[1mStep[0m  [14/21], [94mLoss[0m : 2.70261
[1mStep[0m  [16/21], [94mLoss[0m : 2.62514
[1mStep[0m  [18/21], [94mLoss[0m : 2.86109
[1mStep[0m  [20/21], [94mLoss[0m : 2.63747

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.738, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61225
[1mStep[0m  [2/21], [94mLoss[0m : 2.82538
[1mStep[0m  [4/21], [94mLoss[0m : 2.92150
[1mStep[0m  [6/21], [94mLoss[0m : 2.59823
[1mStep[0m  [8/21], [94mLoss[0m : 2.66471
[1mStep[0m  [10/21], [94mLoss[0m : 2.69273
[1mStep[0m  [12/21], [94mLoss[0m : 2.70942
[1mStep[0m  [14/21], [94mLoss[0m : 2.77500
[1mStep[0m  [16/21], [94mLoss[0m : 2.61798
[1mStep[0m  [18/21], [94mLoss[0m : 2.76075
[1mStep[0m  [20/21], [94mLoss[0m : 2.86244

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.746, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53097
[1mStep[0m  [2/21], [94mLoss[0m : 2.70739
[1mStep[0m  [4/21], [94mLoss[0m : 2.76804
[1mStep[0m  [6/21], [94mLoss[0m : 3.05786
[1mStep[0m  [8/21], [94mLoss[0m : 2.56081
[1mStep[0m  [10/21], [94mLoss[0m : 2.87497
[1mStep[0m  [12/21], [94mLoss[0m : 2.89704
[1mStep[0m  [14/21], [94mLoss[0m : 2.69288
[1mStep[0m  [16/21], [94mLoss[0m : 2.87030
[1mStep[0m  [18/21], [94mLoss[0m : 2.69511
[1mStep[0m  [20/21], [94mLoss[0m : 2.62343

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.728, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41552
[1mStep[0m  [2/21], [94mLoss[0m : 2.69439
[1mStep[0m  [4/21], [94mLoss[0m : 2.69812
[1mStep[0m  [6/21], [94mLoss[0m : 2.82438
[1mStep[0m  [8/21], [94mLoss[0m : 2.61352
[1mStep[0m  [10/21], [94mLoss[0m : 2.78071
[1mStep[0m  [12/21], [94mLoss[0m : 2.57656
[1mStep[0m  [14/21], [94mLoss[0m : 2.73047
[1mStep[0m  [16/21], [94mLoss[0m : 2.78256
[1mStep[0m  [18/21], [94mLoss[0m : 2.68019
[1mStep[0m  [20/21], [94mLoss[0m : 2.88935

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60855
[1mStep[0m  [2/21], [94mLoss[0m : 2.61539
[1mStep[0m  [4/21], [94mLoss[0m : 2.62633
[1mStep[0m  [6/21], [94mLoss[0m : 2.76727
[1mStep[0m  [8/21], [94mLoss[0m : 2.83398
[1mStep[0m  [10/21], [94mLoss[0m : 2.65008
[1mStep[0m  [12/21], [94mLoss[0m : 2.71038
[1mStep[0m  [14/21], [94mLoss[0m : 2.55152
[1mStep[0m  [16/21], [94mLoss[0m : 2.72709
[1mStep[0m  [18/21], [94mLoss[0m : 2.61760
[1mStep[0m  [20/21], [94mLoss[0m : 2.69908

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.359, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54878
[1mStep[0m  [2/21], [94mLoss[0m : 2.75018
[1mStep[0m  [4/21], [94mLoss[0m : 2.76399
[1mStep[0m  [6/21], [94mLoss[0m : 2.80591
[1mStep[0m  [8/21], [94mLoss[0m : 2.78302
[1mStep[0m  [10/21], [94mLoss[0m : 2.76389
[1mStep[0m  [12/21], [94mLoss[0m : 2.71487
[1mStep[0m  [14/21], [94mLoss[0m : 2.58121
[1mStep[0m  [16/21], [94mLoss[0m : 2.69056
[1mStep[0m  [18/21], [94mLoss[0m : 2.64201
[1mStep[0m  [20/21], [94mLoss[0m : 2.74047

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63875
[1mStep[0m  [2/21], [94mLoss[0m : 2.78253
[1mStep[0m  [4/21], [94mLoss[0m : 2.68070
[1mStep[0m  [6/21], [94mLoss[0m : 2.77712
[1mStep[0m  [8/21], [94mLoss[0m : 2.61793
[1mStep[0m  [10/21], [94mLoss[0m : 2.57948
[1mStep[0m  [12/21], [94mLoss[0m : 2.68657
[1mStep[0m  [14/21], [94mLoss[0m : 2.66812
[1mStep[0m  [16/21], [94mLoss[0m : 2.68431
[1mStep[0m  [18/21], [94mLoss[0m : 2.63994
[1mStep[0m  [20/21], [94mLoss[0m : 2.68304

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69496
[1mStep[0m  [2/21], [94mLoss[0m : 2.77484
[1mStep[0m  [4/21], [94mLoss[0m : 2.54113
[1mStep[0m  [6/21], [94mLoss[0m : 2.69731
[1mStep[0m  [8/21], [94mLoss[0m : 2.66045
[1mStep[0m  [10/21], [94mLoss[0m : 2.75012
[1mStep[0m  [12/21], [94mLoss[0m : 2.74784
[1mStep[0m  [14/21], [94mLoss[0m : 2.59289
[1mStep[0m  [16/21], [94mLoss[0m : 2.59895
[1mStep[0m  [18/21], [94mLoss[0m : 2.60912
[1mStep[0m  [20/21], [94mLoss[0m : 2.67155

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66877
[1mStep[0m  [2/21], [94mLoss[0m : 2.73148
[1mStep[0m  [4/21], [94mLoss[0m : 2.68506
[1mStep[0m  [6/21], [94mLoss[0m : 2.64137
[1mStep[0m  [8/21], [94mLoss[0m : 2.48519
[1mStep[0m  [10/21], [94mLoss[0m : 2.80302
[1mStep[0m  [12/21], [94mLoss[0m : 2.89097
[1mStep[0m  [14/21], [94mLoss[0m : 2.52921
[1mStep[0m  [16/21], [94mLoss[0m : 2.66665
[1mStep[0m  [18/21], [94mLoss[0m : 2.52273
[1mStep[0m  [20/21], [94mLoss[0m : 2.63256

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73065
[1mStep[0m  [2/21], [94mLoss[0m : 2.59475
[1mStep[0m  [4/21], [94mLoss[0m : 2.63344
[1mStep[0m  [6/21], [94mLoss[0m : 2.57770
[1mStep[0m  [8/21], [94mLoss[0m : 2.77569
[1mStep[0m  [10/21], [94mLoss[0m : 2.76278
[1mStep[0m  [12/21], [94mLoss[0m : 2.65583
[1mStep[0m  [14/21], [94mLoss[0m : 2.58484
[1mStep[0m  [16/21], [94mLoss[0m : 2.63399
[1mStep[0m  [18/21], [94mLoss[0m : 2.59009
[1mStep[0m  [20/21], [94mLoss[0m : 2.73158

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61754
[1mStep[0m  [2/21], [94mLoss[0m : 2.60502
[1mStep[0m  [4/21], [94mLoss[0m : 2.61281
[1mStep[0m  [6/21], [94mLoss[0m : 2.84828
[1mStep[0m  [8/21], [94mLoss[0m : 2.78690
[1mStep[0m  [10/21], [94mLoss[0m : 2.70669
[1mStep[0m  [12/21], [94mLoss[0m : 2.67548
[1mStep[0m  [14/21], [94mLoss[0m : 2.73316
[1mStep[0m  [16/21], [94mLoss[0m : 2.73558
[1mStep[0m  [18/21], [94mLoss[0m : 2.53750
[1mStep[0m  [20/21], [94mLoss[0m : 2.72792

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64615
[1mStep[0m  [2/21], [94mLoss[0m : 2.64268
[1mStep[0m  [4/21], [94mLoss[0m : 2.62533
[1mStep[0m  [6/21], [94mLoss[0m : 2.69662
[1mStep[0m  [8/21], [94mLoss[0m : 2.76837
[1mStep[0m  [10/21], [94mLoss[0m : 2.67342
[1mStep[0m  [12/21], [94mLoss[0m : 2.68295
[1mStep[0m  [14/21], [94mLoss[0m : 2.56931
[1mStep[0m  [16/21], [94mLoss[0m : 2.49616
[1mStep[0m  [18/21], [94mLoss[0m : 2.82034
[1mStep[0m  [20/21], [94mLoss[0m : 2.71839

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60368
[1mStep[0m  [2/21], [94mLoss[0m : 2.47171
[1mStep[0m  [4/21], [94mLoss[0m : 2.65654
[1mStep[0m  [6/21], [94mLoss[0m : 2.52128
[1mStep[0m  [8/21], [94mLoss[0m : 2.58580
[1mStep[0m  [10/21], [94mLoss[0m : 2.60208
[1mStep[0m  [12/21], [94mLoss[0m : 2.61161
[1mStep[0m  [14/21], [94mLoss[0m : 2.67584
[1mStep[0m  [16/21], [94mLoss[0m : 2.70132
[1mStep[0m  [18/21], [94mLoss[0m : 2.75826
[1mStep[0m  [20/21], [94mLoss[0m : 2.76163

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54684
[1mStep[0m  [2/21], [94mLoss[0m : 2.72791
[1mStep[0m  [4/21], [94mLoss[0m : 2.71303
[1mStep[0m  [6/21], [94mLoss[0m : 2.65584
[1mStep[0m  [8/21], [94mLoss[0m : 2.65201
[1mStep[0m  [10/21], [94mLoss[0m : 2.68040
[1mStep[0m  [12/21], [94mLoss[0m : 2.57810
[1mStep[0m  [14/21], [94mLoss[0m : 2.77081
[1mStep[0m  [16/21], [94mLoss[0m : 2.63144
[1mStep[0m  [18/21], [94mLoss[0m : 2.43370
[1mStep[0m  [20/21], [94mLoss[0m : 2.66668

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65890
[1mStep[0m  [2/21], [94mLoss[0m : 2.58572
[1mStep[0m  [4/21], [94mLoss[0m : 2.75434
[1mStep[0m  [6/21], [94mLoss[0m : 2.52976
[1mStep[0m  [8/21], [94mLoss[0m : 2.62994
[1mStep[0m  [10/21], [94mLoss[0m : 2.45705
[1mStep[0m  [12/21], [94mLoss[0m : 2.46348
[1mStep[0m  [14/21], [94mLoss[0m : 2.71323
[1mStep[0m  [16/21], [94mLoss[0m : 2.45698
[1mStep[0m  [18/21], [94mLoss[0m : 2.70381
[1mStep[0m  [20/21], [94mLoss[0m : 2.73554

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.3374244485582625
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.66973
[1mStep[0m  [2/21], [94mLoss[0m : 2.78271
[1mStep[0m  [4/21], [94mLoss[0m : 2.66517
[1mStep[0m  [6/21], [94mLoss[0m : 2.64259
[1mStep[0m  [8/21], [94mLoss[0m : 2.74929
[1mStep[0m  [10/21], [94mLoss[0m : 2.63853
[1mStep[0m  [12/21], [94mLoss[0m : 2.55613
[1mStep[0m  [14/21], [94mLoss[0m : 2.61735
[1mStep[0m  [16/21], [94mLoss[0m : 2.65612
[1mStep[0m  [18/21], [94mLoss[0m : 2.63112
[1mStep[0m  [20/21], [94mLoss[0m : 2.60163

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65480
[1mStep[0m  [2/21], [94mLoss[0m : 2.65727
[1mStep[0m  [4/21], [94mLoss[0m : 2.70589
[1mStep[0m  [6/21], [94mLoss[0m : 2.53700
[1mStep[0m  [8/21], [94mLoss[0m : 2.84311
[1mStep[0m  [10/21], [94mLoss[0m : 2.77343
[1mStep[0m  [12/21], [94mLoss[0m : 2.52284
[1mStep[0m  [14/21], [94mLoss[0m : 2.89744
[1mStep[0m  [16/21], [94mLoss[0m : 2.62003
[1mStep[0m  [18/21], [94mLoss[0m : 2.51734
[1mStep[0m  [20/21], [94mLoss[0m : 2.58057

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43106
[1mStep[0m  [2/21], [94mLoss[0m : 2.75055
[1mStep[0m  [4/21], [94mLoss[0m : 2.49585
[1mStep[0m  [6/21], [94mLoss[0m : 2.54487
[1mStep[0m  [8/21], [94mLoss[0m : 2.63740
[1mStep[0m  [10/21], [94mLoss[0m : 2.55321
[1mStep[0m  [12/21], [94mLoss[0m : 2.54789
[1mStep[0m  [14/21], [94mLoss[0m : 2.50264
[1mStep[0m  [16/21], [94mLoss[0m : 2.59037
[1mStep[0m  [18/21], [94mLoss[0m : 2.61028
[1mStep[0m  [20/21], [94mLoss[0m : 2.54815

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48130
[1mStep[0m  [2/21], [94mLoss[0m : 2.46319
[1mStep[0m  [4/21], [94mLoss[0m : 2.70387
[1mStep[0m  [6/21], [94mLoss[0m : 2.45934
[1mStep[0m  [8/21], [94mLoss[0m : 2.68058
[1mStep[0m  [10/21], [94mLoss[0m : 2.57926
[1mStep[0m  [12/21], [94mLoss[0m : 2.59831
[1mStep[0m  [14/21], [94mLoss[0m : 2.55537
[1mStep[0m  [16/21], [94mLoss[0m : 2.54661
[1mStep[0m  [18/21], [94mLoss[0m : 2.69192
[1mStep[0m  [20/21], [94mLoss[0m : 2.46530

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51419
[1mStep[0m  [2/21], [94mLoss[0m : 2.54086
[1mStep[0m  [4/21], [94mLoss[0m : 2.40544
[1mStep[0m  [6/21], [94mLoss[0m : 2.52511
[1mStep[0m  [8/21], [94mLoss[0m : 2.40293
[1mStep[0m  [10/21], [94mLoss[0m : 2.57992
[1mStep[0m  [12/21], [94mLoss[0m : 2.54548
[1mStep[0m  [14/21], [94mLoss[0m : 2.63331
[1mStep[0m  [16/21], [94mLoss[0m : 2.70918
[1mStep[0m  [18/21], [94mLoss[0m : 2.55895
[1mStep[0m  [20/21], [94mLoss[0m : 2.57457

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41584
[1mStep[0m  [2/21], [94mLoss[0m : 2.57865
[1mStep[0m  [4/21], [94mLoss[0m : 2.48677
[1mStep[0m  [6/21], [94mLoss[0m : 2.64592
[1mStep[0m  [8/21], [94mLoss[0m : 2.38270
[1mStep[0m  [10/21], [94mLoss[0m : 2.73202
[1mStep[0m  [12/21], [94mLoss[0m : 2.53049
[1mStep[0m  [14/21], [94mLoss[0m : 2.43869
[1mStep[0m  [16/21], [94mLoss[0m : 2.58624
[1mStep[0m  [18/21], [94mLoss[0m : 2.46588
[1mStep[0m  [20/21], [94mLoss[0m : 2.59556

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51158
[1mStep[0m  [2/21], [94mLoss[0m : 2.51068
[1mStep[0m  [4/21], [94mLoss[0m : 2.35211
[1mStep[0m  [6/21], [94mLoss[0m : 2.57972
[1mStep[0m  [8/21], [94mLoss[0m : 2.45226
[1mStep[0m  [10/21], [94mLoss[0m : 2.46741
[1mStep[0m  [12/21], [94mLoss[0m : 2.58499
[1mStep[0m  [14/21], [94mLoss[0m : 2.55893
[1mStep[0m  [16/21], [94mLoss[0m : 2.43543
[1mStep[0m  [18/21], [94mLoss[0m : 2.54197
[1mStep[0m  [20/21], [94mLoss[0m : 2.42327

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38157
[1mStep[0m  [2/21], [94mLoss[0m : 2.43175
[1mStep[0m  [4/21], [94mLoss[0m : 2.30136
[1mStep[0m  [6/21], [94mLoss[0m : 2.52908
[1mStep[0m  [8/21], [94mLoss[0m : 2.41586
[1mStep[0m  [10/21], [94mLoss[0m : 2.35818
[1mStep[0m  [12/21], [94mLoss[0m : 2.46442
[1mStep[0m  [14/21], [94mLoss[0m : 2.34964
[1mStep[0m  [16/21], [94mLoss[0m : 2.50839
[1mStep[0m  [18/21], [94mLoss[0m : 2.53262
[1mStep[0m  [20/21], [94mLoss[0m : 2.53038

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23104
[1mStep[0m  [2/21], [94mLoss[0m : 2.40793
[1mStep[0m  [4/21], [94mLoss[0m : 2.58269
[1mStep[0m  [6/21], [94mLoss[0m : 2.39113
[1mStep[0m  [8/21], [94mLoss[0m : 2.45647
[1mStep[0m  [10/21], [94mLoss[0m : 2.40673
[1mStep[0m  [12/21], [94mLoss[0m : 2.45610
[1mStep[0m  [14/21], [94mLoss[0m : 2.63747
[1mStep[0m  [16/21], [94mLoss[0m : 2.49879
[1mStep[0m  [18/21], [94mLoss[0m : 2.37455
[1mStep[0m  [20/21], [94mLoss[0m : 2.30131

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40530
[1mStep[0m  [2/21], [94mLoss[0m : 2.49033
[1mStep[0m  [4/21], [94mLoss[0m : 2.42710
[1mStep[0m  [6/21], [94mLoss[0m : 2.57203
[1mStep[0m  [8/21], [94mLoss[0m : 2.30306
[1mStep[0m  [10/21], [94mLoss[0m : 2.43017
[1mStep[0m  [12/21], [94mLoss[0m : 2.34702
[1mStep[0m  [14/21], [94mLoss[0m : 2.40521
[1mStep[0m  [16/21], [94mLoss[0m : 2.39078
[1mStep[0m  [18/21], [94mLoss[0m : 2.28445
[1mStep[0m  [20/21], [94mLoss[0m : 2.37099

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37505
[1mStep[0m  [2/21], [94mLoss[0m : 2.24692
[1mStep[0m  [4/21], [94mLoss[0m : 2.45862
[1mStep[0m  [6/21], [94mLoss[0m : 2.28635
[1mStep[0m  [8/21], [94mLoss[0m : 2.35637
[1mStep[0m  [10/21], [94mLoss[0m : 2.37332
[1mStep[0m  [12/21], [94mLoss[0m : 2.54981
[1mStep[0m  [14/21], [94mLoss[0m : 2.47676
[1mStep[0m  [16/21], [94mLoss[0m : 2.47392
[1mStep[0m  [18/21], [94mLoss[0m : 2.43764
[1mStep[0m  [20/21], [94mLoss[0m : 2.35226

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46198
[1mStep[0m  [2/21], [94mLoss[0m : 2.31451
[1mStep[0m  [4/21], [94mLoss[0m : 2.38750
[1mStep[0m  [6/21], [94mLoss[0m : 2.32679
[1mStep[0m  [8/21], [94mLoss[0m : 2.41719
[1mStep[0m  [10/21], [94mLoss[0m : 2.25624
[1mStep[0m  [12/21], [94mLoss[0m : 2.28180
[1mStep[0m  [14/21], [94mLoss[0m : 2.35294
[1mStep[0m  [16/21], [94mLoss[0m : 2.38666
[1mStep[0m  [18/21], [94mLoss[0m : 2.24301
[1mStep[0m  [20/21], [94mLoss[0m : 2.28583

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26924
[1mStep[0m  [2/21], [94mLoss[0m : 2.32294
[1mStep[0m  [4/21], [94mLoss[0m : 2.27317
[1mStep[0m  [6/21], [94mLoss[0m : 2.17959
[1mStep[0m  [8/21], [94mLoss[0m : 2.24639
[1mStep[0m  [10/21], [94mLoss[0m : 2.39304
[1mStep[0m  [12/21], [94mLoss[0m : 2.35685
[1mStep[0m  [14/21], [94mLoss[0m : 2.36864
[1mStep[0m  [16/21], [94mLoss[0m : 2.32358
[1mStep[0m  [18/21], [94mLoss[0m : 2.31604
[1mStep[0m  [20/21], [94mLoss[0m : 2.34630

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29615
[1mStep[0m  [2/21], [94mLoss[0m : 2.25331
[1mStep[0m  [4/21], [94mLoss[0m : 2.19519
[1mStep[0m  [6/21], [94mLoss[0m : 2.18862
[1mStep[0m  [8/21], [94mLoss[0m : 2.30966
[1mStep[0m  [10/21], [94mLoss[0m : 2.24711
[1mStep[0m  [12/21], [94mLoss[0m : 2.24273
[1mStep[0m  [14/21], [94mLoss[0m : 2.16322
[1mStep[0m  [16/21], [94mLoss[0m : 2.25932
[1mStep[0m  [18/21], [94mLoss[0m : 2.27344
[1mStep[0m  [20/21], [94mLoss[0m : 2.36869

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41688
[1mStep[0m  [2/21], [94mLoss[0m : 2.28392
[1mStep[0m  [4/21], [94mLoss[0m : 2.17217
[1mStep[0m  [6/21], [94mLoss[0m : 2.29515
[1mStep[0m  [8/21], [94mLoss[0m : 2.16975
[1mStep[0m  [10/21], [94mLoss[0m : 2.17412
[1mStep[0m  [12/21], [94mLoss[0m : 2.20583
[1mStep[0m  [14/21], [94mLoss[0m : 2.25118
[1mStep[0m  [16/21], [94mLoss[0m : 2.02988
[1mStep[0m  [18/21], [94mLoss[0m : 2.30237
[1mStep[0m  [20/21], [94mLoss[0m : 2.14655

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22321
[1mStep[0m  [2/21], [94mLoss[0m : 2.10580
[1mStep[0m  [4/21], [94mLoss[0m : 2.23863
[1mStep[0m  [6/21], [94mLoss[0m : 2.24281
[1mStep[0m  [8/21], [94mLoss[0m : 2.13007
[1mStep[0m  [10/21], [94mLoss[0m : 2.26727
[1mStep[0m  [12/21], [94mLoss[0m : 2.11004
[1mStep[0m  [14/21], [94mLoss[0m : 2.23352
[1mStep[0m  [16/21], [94mLoss[0m : 2.22502
[1mStep[0m  [18/21], [94mLoss[0m : 2.23739
[1mStep[0m  [20/21], [94mLoss[0m : 2.10942

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.186, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06030
[1mStep[0m  [2/21], [94mLoss[0m : 2.20638
[1mStep[0m  [4/21], [94mLoss[0m : 2.09498
[1mStep[0m  [6/21], [94mLoss[0m : 1.99885
[1mStep[0m  [8/21], [94mLoss[0m : 2.19175
[1mStep[0m  [10/21], [94mLoss[0m : 2.07352
[1mStep[0m  [12/21], [94mLoss[0m : 2.27301
[1mStep[0m  [14/21], [94mLoss[0m : 2.12913
[1mStep[0m  [16/21], [94mLoss[0m : 2.21823
[1mStep[0m  [18/21], [94mLoss[0m : 2.23635
[1mStep[0m  [20/21], [94mLoss[0m : 2.15285

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02740
[1mStep[0m  [2/21], [94mLoss[0m : 2.09068
[1mStep[0m  [4/21], [94mLoss[0m : 2.12619
[1mStep[0m  [6/21], [94mLoss[0m : 2.06943
[1mStep[0m  [8/21], [94mLoss[0m : 2.07632
[1mStep[0m  [10/21], [94mLoss[0m : 2.30350
[1mStep[0m  [12/21], [94mLoss[0m : 2.09839
[1mStep[0m  [14/21], [94mLoss[0m : 2.10858
[1mStep[0m  [16/21], [94mLoss[0m : 2.12883
[1mStep[0m  [18/21], [94mLoss[0m : 2.19591
[1mStep[0m  [20/21], [94mLoss[0m : 2.16121

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07552
[1mStep[0m  [2/21], [94mLoss[0m : 1.86816
[1mStep[0m  [4/21], [94mLoss[0m : 1.97787
[1mStep[0m  [6/21], [94mLoss[0m : 2.00051
[1mStep[0m  [8/21], [94mLoss[0m : 2.12345
[1mStep[0m  [10/21], [94mLoss[0m : 2.09401
[1mStep[0m  [12/21], [94mLoss[0m : 2.04326
[1mStep[0m  [14/21], [94mLoss[0m : 2.05231
[1mStep[0m  [16/21], [94mLoss[0m : 2.13183
[1mStep[0m  [18/21], [94mLoss[0m : 1.98281
[1mStep[0m  [20/21], [94mLoss[0m : 2.10413

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06132
[1mStep[0m  [2/21], [94mLoss[0m : 2.12417
[1mStep[0m  [4/21], [94mLoss[0m : 2.12178
[1mStep[0m  [6/21], [94mLoss[0m : 2.09112
[1mStep[0m  [8/21], [94mLoss[0m : 2.05052
[1mStep[0m  [10/21], [94mLoss[0m : 2.08801
[1mStep[0m  [12/21], [94mLoss[0m : 1.98327
[1mStep[0m  [14/21], [94mLoss[0m : 2.07445
[1mStep[0m  [16/21], [94mLoss[0m : 2.12641
[1mStep[0m  [18/21], [94mLoss[0m : 2.11920
[1mStep[0m  [20/21], [94mLoss[0m : 2.08611

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.515, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92528
[1mStep[0m  [2/21], [94mLoss[0m : 1.94874
[1mStep[0m  [4/21], [94mLoss[0m : 2.15313
[1mStep[0m  [6/21], [94mLoss[0m : 2.11021
[1mStep[0m  [8/21], [94mLoss[0m : 1.99255
[1mStep[0m  [10/21], [94mLoss[0m : 2.00429
[1mStep[0m  [12/21], [94mLoss[0m : 1.93062
[1mStep[0m  [14/21], [94mLoss[0m : 2.08410
[1mStep[0m  [16/21], [94mLoss[0m : 2.01648
[1mStep[0m  [18/21], [94mLoss[0m : 2.01868
[1mStep[0m  [20/21], [94mLoss[0m : 2.05155

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.510, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95050
[1mStep[0m  [2/21], [94mLoss[0m : 1.93520
[1mStep[0m  [4/21], [94mLoss[0m : 1.90363
[1mStep[0m  [6/21], [94mLoss[0m : 2.09444
[1mStep[0m  [8/21], [94mLoss[0m : 1.93997
[1mStep[0m  [10/21], [94mLoss[0m : 1.98107
[1mStep[0m  [12/21], [94mLoss[0m : 2.04875
[1mStep[0m  [14/21], [94mLoss[0m : 2.00779
[1mStep[0m  [16/21], [94mLoss[0m : 2.03572
[1mStep[0m  [18/21], [94mLoss[0m : 2.02336
[1mStep[0m  [20/21], [94mLoss[0m : 2.10964

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96515
[1mStep[0m  [2/21], [94mLoss[0m : 1.90138
[1mStep[0m  [4/21], [94mLoss[0m : 2.04433
[1mStep[0m  [6/21], [94mLoss[0m : 1.92926
[1mStep[0m  [8/21], [94mLoss[0m : 2.03040
[1mStep[0m  [10/21], [94mLoss[0m : 2.04946
[1mStep[0m  [12/21], [94mLoss[0m : 2.07754
[1mStep[0m  [14/21], [94mLoss[0m : 1.99070
[1mStep[0m  [16/21], [94mLoss[0m : 2.00114
[1mStep[0m  [18/21], [94mLoss[0m : 2.05772
[1mStep[0m  [20/21], [94mLoss[0m : 2.02971

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.459, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01091
[1mStep[0m  [2/21], [94mLoss[0m : 1.95412
[1mStep[0m  [4/21], [94mLoss[0m : 1.92589
[1mStep[0m  [6/21], [94mLoss[0m : 1.84088
[1mStep[0m  [8/21], [94mLoss[0m : 2.08350
[1mStep[0m  [10/21], [94mLoss[0m : 1.83216
[1mStep[0m  [12/21], [94mLoss[0m : 1.90406
[1mStep[0m  [14/21], [94mLoss[0m : 1.93783
[1mStep[0m  [16/21], [94mLoss[0m : 1.84826
[1mStep[0m  [18/21], [94mLoss[0m : 2.02202
[1mStep[0m  [20/21], [94mLoss[0m : 1.94607

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.589, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.81709
[1mStep[0m  [2/21], [94mLoss[0m : 1.78398
[1mStep[0m  [4/21], [94mLoss[0m : 1.87976
[1mStep[0m  [6/21], [94mLoss[0m : 1.94375
[1mStep[0m  [8/21], [94mLoss[0m : 1.90445
[1mStep[0m  [10/21], [94mLoss[0m : 2.02997
[1mStep[0m  [12/21], [94mLoss[0m : 1.94730
[1mStep[0m  [14/21], [94mLoss[0m : 2.04852
[1mStep[0m  [16/21], [94mLoss[0m : 1.98470
[1mStep[0m  [18/21], [94mLoss[0m : 1.86965
[1mStep[0m  [20/21], [94mLoss[0m : 1.94419

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82920
[1mStep[0m  [2/21], [94mLoss[0m : 1.86337
[1mStep[0m  [4/21], [94mLoss[0m : 1.88331
[1mStep[0m  [6/21], [94mLoss[0m : 1.84349
[1mStep[0m  [8/21], [94mLoss[0m : 1.90640
[1mStep[0m  [10/21], [94mLoss[0m : 2.06936
[1mStep[0m  [12/21], [94mLoss[0m : 1.89888
[1mStep[0m  [14/21], [94mLoss[0m : 1.88088
[1mStep[0m  [16/21], [94mLoss[0m : 1.99840
[1mStep[0m  [18/21], [94mLoss[0m : 1.81700
[1mStep[0m  [20/21], [94mLoss[0m : 1.87333

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.541, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.89901
[1mStep[0m  [2/21], [94mLoss[0m : 1.74952
[1mStep[0m  [4/21], [94mLoss[0m : 1.86510
[1mStep[0m  [6/21], [94mLoss[0m : 1.83835
[1mStep[0m  [8/21], [94mLoss[0m : 1.75640
[1mStep[0m  [10/21], [94mLoss[0m : 1.86670
[1mStep[0m  [12/21], [94mLoss[0m : 1.83508
[1mStep[0m  [14/21], [94mLoss[0m : 1.97273
[1mStep[0m  [16/21], [94mLoss[0m : 1.75808
[1mStep[0m  [18/21], [94mLoss[0m : 1.98020
[1mStep[0m  [20/21], [94mLoss[0m : 1.87978

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.875, [92mTest[0m: 2.531, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.79688
[1mStep[0m  [2/21], [94mLoss[0m : 1.88737
[1mStep[0m  [4/21], [94mLoss[0m : 1.69279
[1mStep[0m  [6/21], [94mLoss[0m : 1.77749
[1mStep[0m  [8/21], [94mLoss[0m : 1.89328
[1mStep[0m  [10/21], [94mLoss[0m : 1.81694
[1mStep[0m  [12/21], [94mLoss[0m : 1.81020
[1mStep[0m  [14/21], [94mLoss[0m : 1.81401
[1mStep[0m  [16/21], [94mLoss[0m : 1.83514
[1mStep[0m  [18/21], [94mLoss[0m : 1.90828
[1mStep[0m  [20/21], [94mLoss[0m : 1.81330

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.579, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.79440
[1mStep[0m  [2/21], [94mLoss[0m : 1.91793
[1mStep[0m  [4/21], [94mLoss[0m : 1.83005
[1mStep[0m  [6/21], [94mLoss[0m : 1.89928
[1mStep[0m  [8/21], [94mLoss[0m : 1.84207
[1mStep[0m  [10/21], [94mLoss[0m : 1.67230
[1mStep[0m  [12/21], [94mLoss[0m : 1.81288
[1mStep[0m  [14/21], [94mLoss[0m : 1.73334
[1mStep[0m  [16/21], [94mLoss[0m : 1.89239
[1mStep[0m  [18/21], [94mLoss[0m : 1.88553
[1mStep[0m  [20/21], [94mLoss[0m : 1.93510

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.533, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78931
[1mStep[0m  [2/21], [94mLoss[0m : 1.82018
[1mStep[0m  [4/21], [94mLoss[0m : 1.81687
[1mStep[0m  [6/21], [94mLoss[0m : 1.85185
[1mStep[0m  [8/21], [94mLoss[0m : 1.81627
[1mStep[0m  [10/21], [94mLoss[0m : 2.00575
[1mStep[0m  [12/21], [94mLoss[0m : 1.71154
[1mStep[0m  [14/21], [94mLoss[0m : 1.75141
[1mStep[0m  [16/21], [94mLoss[0m : 1.67253
[1mStep[0m  [18/21], [94mLoss[0m : 1.78572
[1mStep[0m  [20/21], [94mLoss[0m : 1.75611

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.609, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.558
====================================

Phase 2 - Evaluation MAE:  2.558320794786726
MAE score P1        2.337424
MAE score P2        2.558321
loss                1.789923
learning_rate           0.01
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.5
weight_decay          0.0001
Name: 7, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.36334
[1mStep[0m  [2/21], [94mLoss[0m : 8.68997
[1mStep[0m  [4/21], [94mLoss[0m : 6.79619
[1mStep[0m  [6/21], [94mLoss[0m : 4.14089
[1mStep[0m  [8/21], [94mLoss[0m : 3.18780
[1mStep[0m  [10/21], [94mLoss[0m : 2.85541
[1mStep[0m  [12/21], [94mLoss[0m : 2.74415
[1mStep[0m  [14/21], [94mLoss[0m : 2.75960
[1mStep[0m  [16/21], [94mLoss[0m : 2.53593
[1mStep[0m  [18/21], [94mLoss[0m : 2.70924
[1mStep[0m  [20/21], [94mLoss[0m : 2.70317

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.435, [92mTest[0m: 10.528, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55258
[1mStep[0m  [2/21], [94mLoss[0m : 2.58668
[1mStep[0m  [4/21], [94mLoss[0m : 2.59248
[1mStep[0m  [6/21], [94mLoss[0m : 2.57536
[1mStep[0m  [8/21], [94mLoss[0m : 2.48392
[1mStep[0m  [10/21], [94mLoss[0m : 2.70192
[1mStep[0m  [12/21], [94mLoss[0m : 2.63632
[1mStep[0m  [14/21], [94mLoss[0m : 2.49255
[1mStep[0m  [16/21], [94mLoss[0m : 2.39916
[1mStep[0m  [18/21], [94mLoss[0m : 2.51813
[1mStep[0m  [20/21], [94mLoss[0m : 2.54559

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.618, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49676
[1mStep[0m  [2/21], [94mLoss[0m : 2.56417
[1mStep[0m  [4/21], [94mLoss[0m : 2.38839
[1mStep[0m  [6/21], [94mLoss[0m : 2.59334
[1mStep[0m  [8/21], [94mLoss[0m : 2.69067
[1mStep[0m  [10/21], [94mLoss[0m : 2.33454
[1mStep[0m  [12/21], [94mLoss[0m : 2.35760
[1mStep[0m  [14/21], [94mLoss[0m : 2.54310
[1mStep[0m  [16/21], [94mLoss[0m : 2.46329
[1mStep[0m  [18/21], [94mLoss[0m : 2.59794
[1mStep[0m  [20/21], [94mLoss[0m : 2.54222

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62434
[1mStep[0m  [2/21], [94mLoss[0m : 2.50415
[1mStep[0m  [4/21], [94mLoss[0m : 2.65236
[1mStep[0m  [6/21], [94mLoss[0m : 2.48673
[1mStep[0m  [8/21], [94mLoss[0m : 2.44644
[1mStep[0m  [10/21], [94mLoss[0m : 2.44389
[1mStep[0m  [12/21], [94mLoss[0m : 2.43800
[1mStep[0m  [14/21], [94mLoss[0m : 2.46566
[1mStep[0m  [16/21], [94mLoss[0m : 2.29396
[1mStep[0m  [18/21], [94mLoss[0m : 2.47249
[1mStep[0m  [20/21], [94mLoss[0m : 2.40373

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47592
[1mStep[0m  [2/21], [94mLoss[0m : 2.45248
[1mStep[0m  [4/21], [94mLoss[0m : 2.46330
[1mStep[0m  [6/21], [94mLoss[0m : 2.43435
[1mStep[0m  [8/21], [94mLoss[0m : 2.56281
[1mStep[0m  [10/21], [94mLoss[0m : 2.49028
[1mStep[0m  [12/21], [94mLoss[0m : 2.52088
[1mStep[0m  [14/21], [94mLoss[0m : 2.47729
[1mStep[0m  [16/21], [94mLoss[0m : 2.43286
[1mStep[0m  [18/21], [94mLoss[0m : 2.49515
[1mStep[0m  [20/21], [94mLoss[0m : 2.48239

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58363
[1mStep[0m  [2/21], [94mLoss[0m : 2.55263
[1mStep[0m  [4/21], [94mLoss[0m : 2.24837
[1mStep[0m  [6/21], [94mLoss[0m : 2.51351
[1mStep[0m  [8/21], [94mLoss[0m : 2.45574
[1mStep[0m  [10/21], [94mLoss[0m : 2.55830
[1mStep[0m  [12/21], [94mLoss[0m : 2.63403
[1mStep[0m  [14/21], [94mLoss[0m : 2.50566
[1mStep[0m  [16/21], [94mLoss[0m : 2.46756
[1mStep[0m  [18/21], [94mLoss[0m : 2.39333
[1mStep[0m  [20/21], [94mLoss[0m : 2.45642

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40729
[1mStep[0m  [2/21], [94mLoss[0m : 2.57722
[1mStep[0m  [4/21], [94mLoss[0m : 2.45547
[1mStep[0m  [6/21], [94mLoss[0m : 2.45530
[1mStep[0m  [8/21], [94mLoss[0m : 2.46375
[1mStep[0m  [10/21], [94mLoss[0m : 2.45040
[1mStep[0m  [12/21], [94mLoss[0m : 2.51257
[1mStep[0m  [14/21], [94mLoss[0m : 2.50448
[1mStep[0m  [16/21], [94mLoss[0m : 2.54761
[1mStep[0m  [18/21], [94mLoss[0m : 2.46721
[1mStep[0m  [20/21], [94mLoss[0m : 2.39541

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35730
[1mStep[0m  [2/21], [94mLoss[0m : 2.51205
[1mStep[0m  [4/21], [94mLoss[0m : 2.25204
[1mStep[0m  [6/21], [94mLoss[0m : 2.44968
[1mStep[0m  [8/21], [94mLoss[0m : 2.47433
[1mStep[0m  [10/21], [94mLoss[0m : 2.35531
[1mStep[0m  [12/21], [94mLoss[0m : 2.68889
[1mStep[0m  [14/21], [94mLoss[0m : 2.40206
[1mStep[0m  [16/21], [94mLoss[0m : 2.54479
[1mStep[0m  [18/21], [94mLoss[0m : 2.35961
[1mStep[0m  [20/21], [94mLoss[0m : 2.49781

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49589
[1mStep[0m  [2/21], [94mLoss[0m : 2.24463
[1mStep[0m  [4/21], [94mLoss[0m : 2.45988
[1mStep[0m  [6/21], [94mLoss[0m : 2.69504
[1mStep[0m  [8/21], [94mLoss[0m : 2.50898
[1mStep[0m  [10/21], [94mLoss[0m : 2.51009
[1mStep[0m  [12/21], [94mLoss[0m : 2.48497
[1mStep[0m  [14/21], [94mLoss[0m : 2.47575
[1mStep[0m  [16/21], [94mLoss[0m : 2.36690
[1mStep[0m  [18/21], [94mLoss[0m : 2.39303
[1mStep[0m  [20/21], [94mLoss[0m : 2.49401

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48800
[1mStep[0m  [2/21], [94mLoss[0m : 2.29373
[1mStep[0m  [4/21], [94mLoss[0m : 2.60329
[1mStep[0m  [6/21], [94mLoss[0m : 2.38322
[1mStep[0m  [8/21], [94mLoss[0m : 2.40208
[1mStep[0m  [10/21], [94mLoss[0m : 2.34641
[1mStep[0m  [12/21], [94mLoss[0m : 2.42702
[1mStep[0m  [14/21], [94mLoss[0m : 2.34312
[1mStep[0m  [16/21], [94mLoss[0m : 2.55973
[1mStep[0m  [18/21], [94mLoss[0m : 2.47923
[1mStep[0m  [20/21], [94mLoss[0m : 2.42141

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49435
[1mStep[0m  [2/21], [94mLoss[0m : 2.48579
[1mStep[0m  [4/21], [94mLoss[0m : 2.39253
[1mStep[0m  [6/21], [94mLoss[0m : 2.45690
[1mStep[0m  [8/21], [94mLoss[0m : 2.33831
[1mStep[0m  [10/21], [94mLoss[0m : 2.37054
[1mStep[0m  [12/21], [94mLoss[0m : 2.55137
[1mStep[0m  [14/21], [94mLoss[0m : 2.39169
[1mStep[0m  [16/21], [94mLoss[0m : 2.47615
[1mStep[0m  [18/21], [94mLoss[0m : 2.45128
[1mStep[0m  [20/21], [94mLoss[0m : 2.32785

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35270
[1mStep[0m  [2/21], [94mLoss[0m : 2.44875
[1mStep[0m  [4/21], [94mLoss[0m : 2.28422
[1mStep[0m  [6/21], [94mLoss[0m : 2.55515
[1mStep[0m  [8/21], [94mLoss[0m : 2.39782
[1mStep[0m  [10/21], [94mLoss[0m : 2.41570
[1mStep[0m  [12/21], [94mLoss[0m : 2.44559
[1mStep[0m  [14/21], [94mLoss[0m : 2.45722
[1mStep[0m  [16/21], [94mLoss[0m : 2.31222
[1mStep[0m  [18/21], [94mLoss[0m : 2.40869
[1mStep[0m  [20/21], [94mLoss[0m : 2.44929

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35300
[1mStep[0m  [2/21], [94mLoss[0m : 2.39819
[1mStep[0m  [4/21], [94mLoss[0m : 2.43335
[1mStep[0m  [6/21], [94mLoss[0m : 2.44942
[1mStep[0m  [8/21], [94mLoss[0m : 2.33658
[1mStep[0m  [10/21], [94mLoss[0m : 2.37621
[1mStep[0m  [12/21], [94mLoss[0m : 2.48395
[1mStep[0m  [14/21], [94mLoss[0m : 2.42853
[1mStep[0m  [16/21], [94mLoss[0m : 2.43544
[1mStep[0m  [18/21], [94mLoss[0m : 2.38941
[1mStep[0m  [20/21], [94mLoss[0m : 2.30002

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54743
[1mStep[0m  [2/21], [94mLoss[0m : 2.47904
[1mStep[0m  [4/21], [94mLoss[0m : 2.51969
[1mStep[0m  [6/21], [94mLoss[0m : 2.34885
[1mStep[0m  [8/21], [94mLoss[0m : 2.48881
[1mStep[0m  [10/21], [94mLoss[0m : 2.43714
[1mStep[0m  [12/21], [94mLoss[0m : 2.50950
[1mStep[0m  [14/21], [94mLoss[0m : 2.43639
[1mStep[0m  [16/21], [94mLoss[0m : 2.31554
[1mStep[0m  [18/21], [94mLoss[0m : 2.43625
[1mStep[0m  [20/21], [94mLoss[0m : 2.40798

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42893
[1mStep[0m  [2/21], [94mLoss[0m : 2.46043
[1mStep[0m  [4/21], [94mLoss[0m : 2.39641
[1mStep[0m  [6/21], [94mLoss[0m : 2.46418
[1mStep[0m  [8/21], [94mLoss[0m : 2.42269
[1mStep[0m  [10/21], [94mLoss[0m : 2.36773
[1mStep[0m  [12/21], [94mLoss[0m : 2.39927
[1mStep[0m  [14/21], [94mLoss[0m : 2.53166
[1mStep[0m  [16/21], [94mLoss[0m : 2.47568
[1mStep[0m  [18/21], [94mLoss[0m : 2.54366
[1mStep[0m  [20/21], [94mLoss[0m : 2.48304

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44132
[1mStep[0m  [2/21], [94mLoss[0m : 2.42573
[1mStep[0m  [4/21], [94mLoss[0m : 2.42570
[1mStep[0m  [6/21], [94mLoss[0m : 2.41180
[1mStep[0m  [8/21], [94mLoss[0m : 2.40827
[1mStep[0m  [10/21], [94mLoss[0m : 2.57323
[1mStep[0m  [12/21], [94mLoss[0m : 2.36785
[1mStep[0m  [14/21], [94mLoss[0m : 2.41793
[1mStep[0m  [16/21], [94mLoss[0m : 2.49584
[1mStep[0m  [18/21], [94mLoss[0m : 2.47361
[1mStep[0m  [20/21], [94mLoss[0m : 2.40359

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54223
[1mStep[0m  [2/21], [94mLoss[0m : 2.45900
[1mStep[0m  [4/21], [94mLoss[0m : 2.38935
[1mStep[0m  [6/21], [94mLoss[0m : 2.48652
[1mStep[0m  [8/21], [94mLoss[0m : 2.42824
[1mStep[0m  [10/21], [94mLoss[0m : 2.40790
[1mStep[0m  [12/21], [94mLoss[0m : 2.47361
[1mStep[0m  [14/21], [94mLoss[0m : 2.54559
[1mStep[0m  [16/21], [94mLoss[0m : 2.39193
[1mStep[0m  [18/21], [94mLoss[0m : 2.22490
[1mStep[0m  [20/21], [94mLoss[0m : 2.32881

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35112
[1mStep[0m  [2/21], [94mLoss[0m : 2.46954
[1mStep[0m  [4/21], [94mLoss[0m : 2.47846
[1mStep[0m  [6/21], [94mLoss[0m : 2.46415
[1mStep[0m  [8/21], [94mLoss[0m : 2.28333
[1mStep[0m  [10/21], [94mLoss[0m : 2.42233
[1mStep[0m  [12/21], [94mLoss[0m : 2.45042
[1mStep[0m  [14/21], [94mLoss[0m : 2.43660
[1mStep[0m  [16/21], [94mLoss[0m : 2.38229
[1mStep[0m  [18/21], [94mLoss[0m : 2.46703
[1mStep[0m  [20/21], [94mLoss[0m : 2.28578

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35812
[1mStep[0m  [2/21], [94mLoss[0m : 2.43082
[1mStep[0m  [4/21], [94mLoss[0m : 2.53035
[1mStep[0m  [6/21], [94mLoss[0m : 2.29345
[1mStep[0m  [8/21], [94mLoss[0m : 2.48484
[1mStep[0m  [10/21], [94mLoss[0m : 2.41437
[1mStep[0m  [12/21], [94mLoss[0m : 2.38704
[1mStep[0m  [14/21], [94mLoss[0m : 2.54602
[1mStep[0m  [16/21], [94mLoss[0m : 2.45384
[1mStep[0m  [18/21], [94mLoss[0m : 2.36543
[1mStep[0m  [20/21], [94mLoss[0m : 2.36308

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48966
[1mStep[0m  [2/21], [94mLoss[0m : 2.28161
[1mStep[0m  [4/21], [94mLoss[0m : 2.38479
[1mStep[0m  [6/21], [94mLoss[0m : 2.44713
[1mStep[0m  [8/21], [94mLoss[0m : 2.49176
[1mStep[0m  [10/21], [94mLoss[0m : 2.32356
[1mStep[0m  [12/21], [94mLoss[0m : 2.47499
[1mStep[0m  [14/21], [94mLoss[0m : 2.35164
[1mStep[0m  [16/21], [94mLoss[0m : 2.54434
[1mStep[0m  [18/21], [94mLoss[0m : 2.35086
[1mStep[0m  [20/21], [94mLoss[0m : 2.54132

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46583
[1mStep[0m  [2/21], [94mLoss[0m : 2.41092
[1mStep[0m  [4/21], [94mLoss[0m : 2.34479
[1mStep[0m  [6/21], [94mLoss[0m : 2.49650
[1mStep[0m  [8/21], [94mLoss[0m : 2.36884
[1mStep[0m  [10/21], [94mLoss[0m : 2.42666
[1mStep[0m  [12/21], [94mLoss[0m : 2.45410
[1mStep[0m  [14/21], [94mLoss[0m : 2.51095
[1mStep[0m  [16/21], [94mLoss[0m : 2.46950
[1mStep[0m  [18/21], [94mLoss[0m : 2.53309
[1mStep[0m  [20/21], [94mLoss[0m : 2.31531

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55246
[1mStep[0m  [2/21], [94mLoss[0m : 2.32885
[1mStep[0m  [4/21], [94mLoss[0m : 2.29817
[1mStep[0m  [6/21], [94mLoss[0m : 2.57214
[1mStep[0m  [8/21], [94mLoss[0m : 2.55529
[1mStep[0m  [10/21], [94mLoss[0m : 2.42331
[1mStep[0m  [12/21], [94mLoss[0m : 2.43548
[1mStep[0m  [14/21], [94mLoss[0m : 2.38013
[1mStep[0m  [16/21], [94mLoss[0m : 2.38400
[1mStep[0m  [18/21], [94mLoss[0m : 2.31995
[1mStep[0m  [20/21], [94mLoss[0m : 2.31154

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45025
[1mStep[0m  [2/21], [94mLoss[0m : 2.29857
[1mStep[0m  [4/21], [94mLoss[0m : 2.48340
[1mStep[0m  [6/21], [94mLoss[0m : 2.42461
[1mStep[0m  [8/21], [94mLoss[0m : 2.32127
[1mStep[0m  [10/21], [94mLoss[0m : 2.58541
[1mStep[0m  [12/21], [94mLoss[0m : 2.40768
[1mStep[0m  [14/21], [94mLoss[0m : 2.46879
[1mStep[0m  [16/21], [94mLoss[0m : 2.56845
[1mStep[0m  [18/21], [94mLoss[0m : 2.43273
[1mStep[0m  [20/21], [94mLoss[0m : 2.31478

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32683
[1mStep[0m  [2/21], [94mLoss[0m : 2.46861
[1mStep[0m  [4/21], [94mLoss[0m : 2.44192
[1mStep[0m  [6/21], [94mLoss[0m : 2.39090
[1mStep[0m  [8/21], [94mLoss[0m : 2.47817
[1mStep[0m  [10/21], [94mLoss[0m : 2.37574
[1mStep[0m  [12/21], [94mLoss[0m : 2.50616
[1mStep[0m  [14/21], [94mLoss[0m : 2.24255
[1mStep[0m  [16/21], [94mLoss[0m : 2.45671
[1mStep[0m  [18/21], [94mLoss[0m : 2.39130
[1mStep[0m  [20/21], [94mLoss[0m : 2.48756

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56676
[1mStep[0m  [2/21], [94mLoss[0m : 2.41448
[1mStep[0m  [4/21], [94mLoss[0m : 2.40130
[1mStep[0m  [6/21], [94mLoss[0m : 2.31310
[1mStep[0m  [8/21], [94mLoss[0m : 2.40370
[1mStep[0m  [10/21], [94mLoss[0m : 2.43058
[1mStep[0m  [12/21], [94mLoss[0m : 2.49717
[1mStep[0m  [14/21], [94mLoss[0m : 2.48261
[1mStep[0m  [16/21], [94mLoss[0m : 2.32276
[1mStep[0m  [18/21], [94mLoss[0m : 2.47392
[1mStep[0m  [20/21], [94mLoss[0m : 2.34160

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35359
[1mStep[0m  [2/21], [94mLoss[0m : 2.30803
[1mStep[0m  [4/21], [94mLoss[0m : 2.58066
[1mStep[0m  [6/21], [94mLoss[0m : 2.49016
[1mStep[0m  [8/21], [94mLoss[0m : 2.47204
[1mStep[0m  [10/21], [94mLoss[0m : 2.42209
[1mStep[0m  [12/21], [94mLoss[0m : 2.39479
[1mStep[0m  [14/21], [94mLoss[0m : 2.35413
[1mStep[0m  [16/21], [94mLoss[0m : 2.35267
[1mStep[0m  [18/21], [94mLoss[0m : 2.36599
[1mStep[0m  [20/21], [94mLoss[0m : 2.52105

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45486
[1mStep[0m  [2/21], [94mLoss[0m : 2.50902
[1mStep[0m  [4/21], [94mLoss[0m : 2.48137
[1mStep[0m  [6/21], [94mLoss[0m : 2.31086
[1mStep[0m  [8/21], [94mLoss[0m : 2.67970
[1mStep[0m  [10/21], [94mLoss[0m : 2.51955
[1mStep[0m  [12/21], [94mLoss[0m : 2.34791
[1mStep[0m  [14/21], [94mLoss[0m : 2.46446
[1mStep[0m  [16/21], [94mLoss[0m : 2.47877
[1mStep[0m  [18/21], [94mLoss[0m : 2.42577
[1mStep[0m  [20/21], [94mLoss[0m : 2.40710

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36366
[1mStep[0m  [2/21], [94mLoss[0m : 2.39211
[1mStep[0m  [4/21], [94mLoss[0m : 2.31476
[1mStep[0m  [6/21], [94mLoss[0m : 2.34501
[1mStep[0m  [8/21], [94mLoss[0m : 2.41723
[1mStep[0m  [10/21], [94mLoss[0m : 2.57382
[1mStep[0m  [12/21], [94mLoss[0m : 2.58176
[1mStep[0m  [14/21], [94mLoss[0m : 2.32834
[1mStep[0m  [16/21], [94mLoss[0m : 2.38833
[1mStep[0m  [18/21], [94mLoss[0m : 2.43150
[1mStep[0m  [20/21], [94mLoss[0m : 2.45379

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.342, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36375
[1mStep[0m  [2/21], [94mLoss[0m : 2.42097
[1mStep[0m  [4/21], [94mLoss[0m : 2.37267
[1mStep[0m  [6/21], [94mLoss[0m : 2.49689
[1mStep[0m  [8/21], [94mLoss[0m : 2.26670
[1mStep[0m  [10/21], [94mLoss[0m : 2.30698
[1mStep[0m  [12/21], [94mLoss[0m : 2.34389
[1mStep[0m  [14/21], [94mLoss[0m : 2.28671
[1mStep[0m  [16/21], [94mLoss[0m : 2.51318
[1mStep[0m  [18/21], [94mLoss[0m : 2.43335
[1mStep[0m  [20/21], [94mLoss[0m : 2.45004

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41689
[1mStep[0m  [2/21], [94mLoss[0m : 2.42701
[1mStep[0m  [4/21], [94mLoss[0m : 2.38825
[1mStep[0m  [6/21], [94mLoss[0m : 2.37328
[1mStep[0m  [8/21], [94mLoss[0m : 2.51892
[1mStep[0m  [10/21], [94mLoss[0m : 2.41470
[1mStep[0m  [12/21], [94mLoss[0m : 2.39859
[1mStep[0m  [14/21], [94mLoss[0m : 2.35177
[1mStep[0m  [16/21], [94mLoss[0m : 2.47658
[1mStep[0m  [18/21], [94mLoss[0m : 2.48480
[1mStep[0m  [20/21], [94mLoss[0m : 2.35098

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.3298610959734236
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.39848
[1mStep[0m  [2/21], [94mLoss[0m : 2.39247
[1mStep[0m  [4/21], [94mLoss[0m : 2.45916
[1mStep[0m  [6/21], [94mLoss[0m : 2.40986
[1mStep[0m  [8/21], [94mLoss[0m : 2.33682
[1mStep[0m  [10/21], [94mLoss[0m : 2.48471
[1mStep[0m  [12/21], [94mLoss[0m : 2.49754
[1mStep[0m  [14/21], [94mLoss[0m : 2.45429
[1mStep[0m  [16/21], [94mLoss[0m : 2.56996
[1mStep[0m  [18/21], [94mLoss[0m : 2.41017
[1mStep[0m  [20/21], [94mLoss[0m : 2.35914

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26420
[1mStep[0m  [2/21], [94mLoss[0m : 2.46571
[1mStep[0m  [4/21], [94mLoss[0m : 2.41584
[1mStep[0m  [6/21], [94mLoss[0m : 2.30096
[1mStep[0m  [8/21], [94mLoss[0m : 2.33684
[1mStep[0m  [10/21], [94mLoss[0m : 2.56255
[1mStep[0m  [12/21], [94mLoss[0m : 2.37493
[1mStep[0m  [14/21], [94mLoss[0m : 2.32507
[1mStep[0m  [16/21], [94mLoss[0m : 2.42659
[1mStep[0m  [18/21], [94mLoss[0m : 2.36685
[1mStep[0m  [20/21], [94mLoss[0m : 2.38057

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44741
[1mStep[0m  [2/21], [94mLoss[0m : 2.38809
[1mStep[0m  [4/21], [94mLoss[0m : 2.24045
[1mStep[0m  [6/21], [94mLoss[0m : 2.26753
[1mStep[0m  [8/21], [94mLoss[0m : 2.32485
[1mStep[0m  [10/21], [94mLoss[0m : 2.48700
[1mStep[0m  [12/21], [94mLoss[0m : 2.31099
[1mStep[0m  [14/21], [94mLoss[0m : 2.38353
[1mStep[0m  [16/21], [94mLoss[0m : 2.55539
[1mStep[0m  [18/21], [94mLoss[0m : 2.42758
[1mStep[0m  [20/21], [94mLoss[0m : 2.40595

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26291
[1mStep[0m  [2/21], [94mLoss[0m : 2.42311
[1mStep[0m  [4/21], [94mLoss[0m : 2.30934
[1mStep[0m  [6/21], [94mLoss[0m : 2.24749
[1mStep[0m  [8/21], [94mLoss[0m : 2.36478
[1mStep[0m  [10/21], [94mLoss[0m : 2.38375
[1mStep[0m  [12/21], [94mLoss[0m : 2.35577
[1mStep[0m  [14/21], [94mLoss[0m : 2.31912
[1mStep[0m  [16/21], [94mLoss[0m : 2.39285
[1mStep[0m  [18/21], [94mLoss[0m : 2.32105
[1mStep[0m  [20/21], [94mLoss[0m : 2.44796

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42636
[1mStep[0m  [2/21], [94mLoss[0m : 2.29942
[1mStep[0m  [4/21], [94mLoss[0m : 2.35331
[1mStep[0m  [6/21], [94mLoss[0m : 2.28271
[1mStep[0m  [8/21], [94mLoss[0m : 2.24252
[1mStep[0m  [10/21], [94mLoss[0m : 2.22630
[1mStep[0m  [12/21], [94mLoss[0m : 2.30035
[1mStep[0m  [14/21], [94mLoss[0m : 2.36897
[1mStep[0m  [16/21], [94mLoss[0m : 2.23247
[1mStep[0m  [18/21], [94mLoss[0m : 2.03307
[1mStep[0m  [20/21], [94mLoss[0m : 2.40621

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36492
[1mStep[0m  [2/21], [94mLoss[0m : 2.24861
[1mStep[0m  [4/21], [94mLoss[0m : 2.22734
[1mStep[0m  [6/21], [94mLoss[0m : 2.43308
[1mStep[0m  [8/21], [94mLoss[0m : 2.28169
[1mStep[0m  [10/21], [94mLoss[0m : 2.41667
[1mStep[0m  [12/21], [94mLoss[0m : 2.32587
[1mStep[0m  [14/21], [94mLoss[0m : 2.24571
[1mStep[0m  [16/21], [94mLoss[0m : 2.16530
[1mStep[0m  [18/21], [94mLoss[0m : 2.27994
[1mStep[0m  [20/21], [94mLoss[0m : 2.31855

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13687
[1mStep[0m  [2/21], [94mLoss[0m : 2.14806
[1mStep[0m  [4/21], [94mLoss[0m : 2.29917
[1mStep[0m  [6/21], [94mLoss[0m : 2.23724
[1mStep[0m  [8/21], [94mLoss[0m : 2.27957
[1mStep[0m  [10/21], [94mLoss[0m : 2.21512
[1mStep[0m  [12/21], [94mLoss[0m : 2.33419
[1mStep[0m  [14/21], [94mLoss[0m : 2.14758
[1mStep[0m  [16/21], [94mLoss[0m : 2.32534
[1mStep[0m  [18/21], [94mLoss[0m : 2.23980
[1mStep[0m  [20/21], [94mLoss[0m : 2.26152

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22448
[1mStep[0m  [2/21], [94mLoss[0m : 2.23080
[1mStep[0m  [4/21], [94mLoss[0m : 2.27553
[1mStep[0m  [6/21], [94mLoss[0m : 2.24519
[1mStep[0m  [8/21], [94mLoss[0m : 2.20816
[1mStep[0m  [10/21], [94mLoss[0m : 2.32819
[1mStep[0m  [12/21], [94mLoss[0m : 2.19461
[1mStep[0m  [14/21], [94mLoss[0m : 2.16312
[1mStep[0m  [16/21], [94mLoss[0m : 2.24119
[1mStep[0m  [18/21], [94mLoss[0m : 2.14476
[1mStep[0m  [20/21], [94mLoss[0m : 2.21884

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.234, [92mTest[0m: 2.562, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07995
[1mStep[0m  [2/21], [94mLoss[0m : 2.12632
[1mStep[0m  [4/21], [94mLoss[0m : 2.24771
[1mStep[0m  [6/21], [94mLoss[0m : 2.21220
[1mStep[0m  [8/21], [94mLoss[0m : 2.21612
[1mStep[0m  [10/21], [94mLoss[0m : 2.24637
[1mStep[0m  [12/21], [94mLoss[0m : 1.99073
[1mStep[0m  [14/21], [94mLoss[0m : 2.24681
[1mStep[0m  [16/21], [94mLoss[0m : 2.36642
[1mStep[0m  [18/21], [94mLoss[0m : 2.28775
[1mStep[0m  [20/21], [94mLoss[0m : 2.30766

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.561, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08820
[1mStep[0m  [2/21], [94mLoss[0m : 2.25272
[1mStep[0m  [4/21], [94mLoss[0m : 2.13765
[1mStep[0m  [6/21], [94mLoss[0m : 2.08156
[1mStep[0m  [8/21], [94mLoss[0m : 2.25970
[1mStep[0m  [10/21], [94mLoss[0m : 2.11720
[1mStep[0m  [12/21], [94mLoss[0m : 2.17110
[1mStep[0m  [14/21], [94mLoss[0m : 2.15727
[1mStep[0m  [16/21], [94mLoss[0m : 2.17997
[1mStep[0m  [18/21], [94mLoss[0m : 2.14541
[1mStep[0m  [20/21], [94mLoss[0m : 2.09316

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12091
[1mStep[0m  [2/21], [94mLoss[0m : 1.96275
[1mStep[0m  [4/21], [94mLoss[0m : 2.19749
[1mStep[0m  [6/21], [94mLoss[0m : 2.16155
[1mStep[0m  [8/21], [94mLoss[0m : 2.12454
[1mStep[0m  [10/21], [94mLoss[0m : 2.10459
[1mStep[0m  [12/21], [94mLoss[0m : 2.20009
[1mStep[0m  [14/21], [94mLoss[0m : 2.21768
[1mStep[0m  [16/21], [94mLoss[0m : 1.97595
[1mStep[0m  [18/21], [94mLoss[0m : 2.11916
[1mStep[0m  [20/21], [94mLoss[0m : 2.09142

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17292
[1mStep[0m  [2/21], [94mLoss[0m : 2.12194
[1mStep[0m  [4/21], [94mLoss[0m : 1.93796
[1mStep[0m  [6/21], [94mLoss[0m : 2.14914
[1mStep[0m  [8/21], [94mLoss[0m : 2.28996
[1mStep[0m  [10/21], [94mLoss[0m : 2.14887
[1mStep[0m  [12/21], [94mLoss[0m : 2.11587
[1mStep[0m  [14/21], [94mLoss[0m : 1.90838
[1mStep[0m  [16/21], [94mLoss[0m : 2.05392
[1mStep[0m  [18/21], [94mLoss[0m : 2.11487
[1mStep[0m  [20/21], [94mLoss[0m : 2.16172

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.578, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.04183
[1mStep[0m  [2/21], [94mLoss[0m : 2.10483
[1mStep[0m  [4/21], [94mLoss[0m : 1.99837
[1mStep[0m  [6/21], [94mLoss[0m : 2.01353
[1mStep[0m  [8/21], [94mLoss[0m : 1.99047
[1mStep[0m  [10/21], [94mLoss[0m : 2.02694
[1mStep[0m  [12/21], [94mLoss[0m : 1.99021
[1mStep[0m  [14/21], [94mLoss[0m : 2.05446
[1mStep[0m  [16/21], [94mLoss[0m : 2.02209
[1mStep[0m  [18/21], [94mLoss[0m : 2.05027
[1mStep[0m  [20/21], [94mLoss[0m : 1.95707

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.058, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.00698
[1mStep[0m  [2/21], [94mLoss[0m : 2.04805
[1mStep[0m  [4/21], [94mLoss[0m : 2.07455
[1mStep[0m  [6/21], [94mLoss[0m : 1.97388
[1mStep[0m  [8/21], [94mLoss[0m : 2.01670
[1mStep[0m  [10/21], [94mLoss[0m : 2.20577
[1mStep[0m  [12/21], [94mLoss[0m : 2.05672
[1mStep[0m  [14/21], [94mLoss[0m : 2.01379
[1mStep[0m  [16/21], [94mLoss[0m : 2.09256
[1mStep[0m  [18/21], [94mLoss[0m : 2.10565
[1mStep[0m  [20/21], [94mLoss[0m : 1.85295

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98210
[1mStep[0m  [2/21], [94mLoss[0m : 1.91669
[1mStep[0m  [4/21], [94mLoss[0m : 2.01505
[1mStep[0m  [6/21], [94mLoss[0m : 1.89567
[1mStep[0m  [8/21], [94mLoss[0m : 1.96632
[1mStep[0m  [10/21], [94mLoss[0m : 2.02510
[1mStep[0m  [12/21], [94mLoss[0m : 2.07050
[1mStep[0m  [14/21], [94mLoss[0m : 1.84053
[1mStep[0m  [16/21], [94mLoss[0m : 2.03939
[1mStep[0m  [18/21], [94mLoss[0m : 1.93372
[1mStep[0m  [20/21], [94mLoss[0m : 2.00687

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95838
[1mStep[0m  [2/21], [94mLoss[0m : 1.92952
[1mStep[0m  [4/21], [94mLoss[0m : 1.91872
[1mStep[0m  [6/21], [94mLoss[0m : 1.93016
[1mStep[0m  [8/21], [94mLoss[0m : 1.97005
[1mStep[0m  [10/21], [94mLoss[0m : 2.04904
[1mStep[0m  [12/21], [94mLoss[0m : 1.78046
[1mStep[0m  [14/21], [94mLoss[0m : 2.00635
[1mStep[0m  [16/21], [94mLoss[0m : 2.01432
[1mStep[0m  [18/21], [94mLoss[0m : 1.94946
[1mStep[0m  [20/21], [94mLoss[0m : 1.89004

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.942, [92mTest[0m: 2.526, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77471
[1mStep[0m  [2/21], [94mLoss[0m : 1.88574
[1mStep[0m  [4/21], [94mLoss[0m : 1.94331
[1mStep[0m  [6/21], [94mLoss[0m : 1.86154
[1mStep[0m  [8/21], [94mLoss[0m : 1.83519
[1mStep[0m  [10/21], [94mLoss[0m : 1.93591
[1mStep[0m  [12/21], [94mLoss[0m : 1.81613
[1mStep[0m  [14/21], [94mLoss[0m : 1.85253
[1mStep[0m  [16/21], [94mLoss[0m : 1.97189
[1mStep[0m  [18/21], [94mLoss[0m : 1.92656
[1mStep[0m  [20/21], [94mLoss[0m : 1.91980

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77701
[1mStep[0m  [2/21], [94mLoss[0m : 1.84057
[1mStep[0m  [4/21], [94mLoss[0m : 1.80747
[1mStep[0m  [6/21], [94mLoss[0m : 1.84707
[1mStep[0m  [8/21], [94mLoss[0m : 1.76136
[1mStep[0m  [10/21], [94mLoss[0m : 1.96252
[1mStep[0m  [12/21], [94mLoss[0m : 1.85604
[1mStep[0m  [14/21], [94mLoss[0m : 1.80574
[1mStep[0m  [16/21], [94mLoss[0m : 1.88351
[1mStep[0m  [18/21], [94mLoss[0m : 1.87649
[1mStep[0m  [20/21], [94mLoss[0m : 1.95432

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.866, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77838
[1mStep[0m  [2/21], [94mLoss[0m : 1.91108
[1mStep[0m  [4/21], [94mLoss[0m : 1.69155
[1mStep[0m  [6/21], [94mLoss[0m : 1.79944
[1mStep[0m  [8/21], [94mLoss[0m : 1.82448
[1mStep[0m  [10/21], [94mLoss[0m : 1.79156
[1mStep[0m  [12/21], [94mLoss[0m : 1.71453
[1mStep[0m  [14/21], [94mLoss[0m : 1.89481
[1mStep[0m  [16/21], [94mLoss[0m : 1.96529
[1mStep[0m  [18/21], [94mLoss[0m : 1.80007
[1mStep[0m  [20/21], [94mLoss[0m : 1.74943

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.513, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82358
[1mStep[0m  [2/21], [94mLoss[0m : 1.87406
[1mStep[0m  [4/21], [94mLoss[0m : 1.86468
[1mStep[0m  [6/21], [94mLoss[0m : 1.82082
[1mStep[0m  [8/21], [94mLoss[0m : 1.68793
[1mStep[0m  [10/21], [94mLoss[0m : 1.78985
[1mStep[0m  [12/21], [94mLoss[0m : 1.86631
[1mStep[0m  [14/21], [94mLoss[0m : 1.71365
[1mStep[0m  [16/21], [94mLoss[0m : 1.83006
[1mStep[0m  [18/21], [94mLoss[0m : 1.78232
[1mStep[0m  [20/21], [94mLoss[0m : 1.68778

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.552, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.66379
[1mStep[0m  [2/21], [94mLoss[0m : 1.69075
[1mStep[0m  [4/21], [94mLoss[0m : 1.86462
[1mStep[0m  [6/21], [94mLoss[0m : 1.74456
[1mStep[0m  [8/21], [94mLoss[0m : 1.73686
[1mStep[0m  [10/21], [94mLoss[0m : 1.83504
[1mStep[0m  [12/21], [94mLoss[0m : 1.64493
[1mStep[0m  [14/21], [94mLoss[0m : 1.73076
[1mStep[0m  [16/21], [94mLoss[0m : 1.82612
[1mStep[0m  [18/21], [94mLoss[0m : 1.73499
[1mStep[0m  [20/21], [94mLoss[0m : 1.72171

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.532, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.52997
[1mStep[0m  [2/21], [94mLoss[0m : 1.70956
[1mStep[0m  [4/21], [94mLoss[0m : 1.67638
[1mStep[0m  [6/21], [94mLoss[0m : 1.67818
[1mStep[0m  [8/21], [94mLoss[0m : 1.66226
[1mStep[0m  [10/21], [94mLoss[0m : 1.77137
[1mStep[0m  [12/21], [94mLoss[0m : 1.73493
[1mStep[0m  [14/21], [94mLoss[0m : 1.72860
[1mStep[0m  [16/21], [94mLoss[0m : 1.72421
[1mStep[0m  [18/21], [94mLoss[0m : 1.63613
[1mStep[0m  [20/21], [94mLoss[0m : 1.72762

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.459, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.68308
[1mStep[0m  [2/21], [94mLoss[0m : 1.73650
[1mStep[0m  [4/21], [94mLoss[0m : 1.55695
[1mStep[0m  [6/21], [94mLoss[0m : 1.61040
[1mStep[0m  [8/21], [94mLoss[0m : 1.69086
[1mStep[0m  [10/21], [94mLoss[0m : 1.63315
[1mStep[0m  [12/21], [94mLoss[0m : 1.75085
[1mStep[0m  [14/21], [94mLoss[0m : 1.82585
[1mStep[0m  [16/21], [94mLoss[0m : 1.52930
[1mStep[0m  [18/21], [94mLoss[0m : 1.71147
[1mStep[0m  [20/21], [94mLoss[0m : 1.77659

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.541, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57103
[1mStep[0m  [2/21], [94mLoss[0m : 1.57448
[1mStep[0m  [4/21], [94mLoss[0m : 1.71167
[1mStep[0m  [6/21], [94mLoss[0m : 1.51350
[1mStep[0m  [8/21], [94mLoss[0m : 1.59048
[1mStep[0m  [10/21], [94mLoss[0m : 1.54926
[1mStep[0m  [12/21], [94mLoss[0m : 1.71778
[1mStep[0m  [14/21], [94mLoss[0m : 1.81949
[1mStep[0m  [16/21], [94mLoss[0m : 1.65779
[1mStep[0m  [18/21], [94mLoss[0m : 1.67091
[1mStep[0m  [20/21], [94mLoss[0m : 1.67968

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.567, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.60384
[1mStep[0m  [2/21], [94mLoss[0m : 1.62391
[1mStep[0m  [4/21], [94mLoss[0m : 1.59086
[1mStep[0m  [6/21], [94mLoss[0m : 1.51920
[1mStep[0m  [8/21], [94mLoss[0m : 1.46992
[1mStep[0m  [10/21], [94mLoss[0m : 1.76963
[1mStep[0m  [12/21], [94mLoss[0m : 1.65843
[1mStep[0m  [14/21], [94mLoss[0m : 1.67904
[1mStep[0m  [16/21], [94mLoss[0m : 1.66020
[1mStep[0m  [18/21], [94mLoss[0m : 1.67420
[1mStep[0m  [20/21], [94mLoss[0m : 1.56227

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51259
[1mStep[0m  [2/21], [94mLoss[0m : 1.65431
[1mStep[0m  [4/21], [94mLoss[0m : 1.55798
[1mStep[0m  [6/21], [94mLoss[0m : 1.65682
[1mStep[0m  [8/21], [94mLoss[0m : 1.72832
[1mStep[0m  [10/21], [94mLoss[0m : 1.52609
[1mStep[0m  [12/21], [94mLoss[0m : 1.59097
[1mStep[0m  [14/21], [94mLoss[0m : 1.64228
[1mStep[0m  [16/21], [94mLoss[0m : 1.58754
[1mStep[0m  [18/21], [94mLoss[0m : 1.55919
[1mStep[0m  [20/21], [94mLoss[0m : 1.62489

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.482, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.47682
[1mStep[0m  [2/21], [94mLoss[0m : 1.60592
[1mStep[0m  [4/21], [94mLoss[0m : 1.63122
[1mStep[0m  [6/21], [94mLoss[0m : 1.47069
[1mStep[0m  [8/21], [94mLoss[0m : 1.60069
[1mStep[0m  [10/21], [94mLoss[0m : 1.61796
[1mStep[0m  [12/21], [94mLoss[0m : 1.60183
[1mStep[0m  [14/21], [94mLoss[0m : 1.63904
[1mStep[0m  [16/21], [94mLoss[0m : 1.63622
[1mStep[0m  [18/21], [94mLoss[0m : 1.60367
[1mStep[0m  [20/21], [94mLoss[0m : 1.58690

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.588, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.44721
[1mStep[0m  [2/21], [94mLoss[0m : 1.47003
[1mStep[0m  [4/21], [94mLoss[0m : 1.58246
[1mStep[0m  [6/21], [94mLoss[0m : 1.50492
[1mStep[0m  [8/21], [94mLoss[0m : 1.46363
[1mStep[0m  [10/21], [94mLoss[0m : 1.58461
[1mStep[0m  [12/21], [94mLoss[0m : 1.48747
[1mStep[0m  [14/21], [94mLoss[0m : 1.62140
[1mStep[0m  [16/21], [94mLoss[0m : 1.50880
[1mStep[0m  [18/21], [94mLoss[0m : 1.59390
[1mStep[0m  [20/21], [94mLoss[0m : 1.61873

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.553, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43955
[1mStep[0m  [2/21], [94mLoss[0m : 1.54924
[1mStep[0m  [4/21], [94mLoss[0m : 1.49547
[1mStep[0m  [6/21], [94mLoss[0m : 1.58501
[1mStep[0m  [8/21], [94mLoss[0m : 1.53533
[1mStep[0m  [10/21], [94mLoss[0m : 1.52235
[1mStep[0m  [12/21], [94mLoss[0m : 1.39620
[1mStep[0m  [14/21], [94mLoss[0m : 1.59961
[1mStep[0m  [16/21], [94mLoss[0m : 1.48802
[1mStep[0m  [18/21], [94mLoss[0m : 1.74581
[1mStep[0m  [20/21], [94mLoss[0m : 1.49713

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.675, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.45641
[1mStep[0m  [2/21], [94mLoss[0m : 1.53862
[1mStep[0m  [4/21], [94mLoss[0m : 1.44714
[1mStep[0m  [6/21], [94mLoss[0m : 1.44264
[1mStep[0m  [8/21], [94mLoss[0m : 1.53903
[1mStep[0m  [10/21], [94mLoss[0m : 1.47052
[1mStep[0m  [12/21], [94mLoss[0m : 1.53006
[1mStep[0m  [14/21], [94mLoss[0m : 1.43067
[1mStep[0m  [16/21], [94mLoss[0m : 1.52470
[1mStep[0m  [18/21], [94mLoss[0m : 1.44567
[1mStep[0m  [20/21], [94mLoss[0m : 1.57757

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.646
====================================

Phase 2 - Evaluation MAE:  2.6461128847939626
MAE score P1      2.329861
MAE score P2      2.646113
loss              1.504732
learning_rate         0.01
batch_size             512
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 8, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.72151
[1mStep[0m  [2/21], [94mLoss[0m : 10.47347
[1mStep[0m  [4/21], [94mLoss[0m : 8.45329
[1mStep[0m  [6/21], [94mLoss[0m : 6.14314
[1mStep[0m  [8/21], [94mLoss[0m : 3.87472
[1mStep[0m  [10/21], [94mLoss[0m : 2.64223
[1mStep[0m  [12/21], [94mLoss[0m : 3.30539
[1mStep[0m  [14/21], [94mLoss[0m : 4.12011
[1mStep[0m  [16/21], [94mLoss[0m : 4.17016
[1mStep[0m  [18/21], [94mLoss[0m : 3.71613
[1mStep[0m  [20/21], [94mLoss[0m : 3.24984

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.450, [92mTest[0m: 10.797, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.96490
[1mStep[0m  [2/21], [94mLoss[0m : 2.52112
[1mStep[0m  [4/21], [94mLoss[0m : 2.53873
[1mStep[0m  [6/21], [94mLoss[0m : 2.65991
[1mStep[0m  [8/21], [94mLoss[0m : 2.90204
[1mStep[0m  [10/21], [94mLoss[0m : 3.01582
[1mStep[0m  [12/21], [94mLoss[0m : 2.63142
[1mStep[0m  [14/21], [94mLoss[0m : 2.43208
[1mStep[0m  [16/21], [94mLoss[0m : 2.57134
[1mStep[0m  [18/21], [94mLoss[0m : 2.56376
[1mStep[0m  [20/21], [94mLoss[0m : 2.65636

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.671, [92mTest[0m: 3.135, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58430
[1mStep[0m  [2/21], [94mLoss[0m : 2.53588
[1mStep[0m  [4/21], [94mLoss[0m : 2.44567
[1mStep[0m  [6/21], [94mLoss[0m : 2.45585
[1mStep[0m  [8/21], [94mLoss[0m : 2.40800
[1mStep[0m  [10/21], [94mLoss[0m : 2.56871
[1mStep[0m  [12/21], [94mLoss[0m : 2.46163
[1mStep[0m  [14/21], [94mLoss[0m : 2.48845
[1mStep[0m  [16/21], [94mLoss[0m : 2.46405
[1mStep[0m  [18/21], [94mLoss[0m : 2.32444
[1mStep[0m  [20/21], [94mLoss[0m : 2.47136

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.627, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43202
[1mStep[0m  [2/21], [94mLoss[0m : 2.37432
[1mStep[0m  [4/21], [94mLoss[0m : 2.39078
[1mStep[0m  [6/21], [94mLoss[0m : 2.43416
[1mStep[0m  [8/21], [94mLoss[0m : 2.60427
[1mStep[0m  [10/21], [94mLoss[0m : 2.54242
[1mStep[0m  [12/21], [94mLoss[0m : 2.43409
[1mStep[0m  [14/21], [94mLoss[0m : 2.49077
[1mStep[0m  [16/21], [94mLoss[0m : 2.44535
[1mStep[0m  [18/21], [94mLoss[0m : 2.37466
[1mStep[0m  [20/21], [94mLoss[0m : 2.50037

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56829
[1mStep[0m  [2/21], [94mLoss[0m : 2.40530
[1mStep[0m  [4/21], [94mLoss[0m : 2.37247
[1mStep[0m  [6/21], [94mLoss[0m : 2.34766
[1mStep[0m  [8/21], [94mLoss[0m : 2.39442
[1mStep[0m  [10/21], [94mLoss[0m : 2.39662
[1mStep[0m  [12/21], [94mLoss[0m : 2.33129
[1mStep[0m  [14/21], [94mLoss[0m : 2.48878
[1mStep[0m  [16/21], [94mLoss[0m : 2.40791
[1mStep[0m  [18/21], [94mLoss[0m : 2.46805
[1mStep[0m  [20/21], [94mLoss[0m : 2.40500

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46475
[1mStep[0m  [2/21], [94mLoss[0m : 2.45761
[1mStep[0m  [4/21], [94mLoss[0m : 2.39452
[1mStep[0m  [6/21], [94mLoss[0m : 2.34880
[1mStep[0m  [8/21], [94mLoss[0m : 2.44277
[1mStep[0m  [10/21], [94mLoss[0m : 2.61908
[1mStep[0m  [12/21], [94mLoss[0m : 2.51906
[1mStep[0m  [14/21], [94mLoss[0m : 2.39603
[1mStep[0m  [16/21], [94mLoss[0m : 2.37519
[1mStep[0m  [18/21], [94mLoss[0m : 2.44365
[1mStep[0m  [20/21], [94mLoss[0m : 2.44306

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37560
[1mStep[0m  [2/21], [94mLoss[0m : 2.54675
[1mStep[0m  [4/21], [94mLoss[0m : 2.38035
[1mStep[0m  [6/21], [94mLoss[0m : 2.46083
[1mStep[0m  [8/21], [94mLoss[0m : 2.42168
[1mStep[0m  [10/21], [94mLoss[0m : 2.47689
[1mStep[0m  [12/21], [94mLoss[0m : 2.50527
[1mStep[0m  [14/21], [94mLoss[0m : 2.50364
[1mStep[0m  [16/21], [94mLoss[0m : 2.37633
[1mStep[0m  [18/21], [94mLoss[0m : 2.35185
[1mStep[0m  [20/21], [94mLoss[0m : 2.25850

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43393
[1mStep[0m  [2/21], [94mLoss[0m : 2.33512
[1mStep[0m  [4/21], [94mLoss[0m : 2.47680
[1mStep[0m  [6/21], [94mLoss[0m : 2.50321
[1mStep[0m  [8/21], [94mLoss[0m : 2.42356
[1mStep[0m  [10/21], [94mLoss[0m : 2.38898
[1mStep[0m  [12/21], [94mLoss[0m : 2.43028
[1mStep[0m  [14/21], [94mLoss[0m : 2.27845
[1mStep[0m  [16/21], [94mLoss[0m : 2.32317
[1mStep[0m  [18/21], [94mLoss[0m : 2.41094
[1mStep[0m  [20/21], [94mLoss[0m : 2.42177

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50405
[1mStep[0m  [2/21], [94mLoss[0m : 2.31062
[1mStep[0m  [4/21], [94mLoss[0m : 2.45072
[1mStep[0m  [6/21], [94mLoss[0m : 2.41711
[1mStep[0m  [8/21], [94mLoss[0m : 2.43999
[1mStep[0m  [10/21], [94mLoss[0m : 2.38990
[1mStep[0m  [12/21], [94mLoss[0m : 2.30606
[1mStep[0m  [14/21], [94mLoss[0m : 2.49908
[1mStep[0m  [16/21], [94mLoss[0m : 2.55849
[1mStep[0m  [18/21], [94mLoss[0m : 2.46592
[1mStep[0m  [20/21], [94mLoss[0m : 2.38882

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44082
[1mStep[0m  [2/21], [94mLoss[0m : 2.44151
[1mStep[0m  [4/21], [94mLoss[0m : 2.51904
[1mStep[0m  [6/21], [94mLoss[0m : 2.37635
[1mStep[0m  [8/21], [94mLoss[0m : 2.41088
[1mStep[0m  [10/21], [94mLoss[0m : 2.49545
[1mStep[0m  [12/21], [94mLoss[0m : 2.47147
[1mStep[0m  [14/21], [94mLoss[0m : 2.38941
[1mStep[0m  [16/21], [94mLoss[0m : 2.52041
[1mStep[0m  [18/21], [94mLoss[0m : 2.33513
[1mStep[0m  [20/21], [94mLoss[0m : 2.32948

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43655
[1mStep[0m  [2/21], [94mLoss[0m : 2.31438
[1mStep[0m  [4/21], [94mLoss[0m : 2.37077
[1mStep[0m  [6/21], [94mLoss[0m : 2.35001
[1mStep[0m  [8/21], [94mLoss[0m : 2.54606
[1mStep[0m  [10/21], [94mLoss[0m : 2.39681
[1mStep[0m  [12/21], [94mLoss[0m : 2.38279
[1mStep[0m  [14/21], [94mLoss[0m : 2.31577
[1mStep[0m  [16/21], [94mLoss[0m : 2.38238
[1mStep[0m  [18/21], [94mLoss[0m : 2.46930
[1mStep[0m  [20/21], [94mLoss[0m : 2.40884

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40852
[1mStep[0m  [2/21], [94mLoss[0m : 2.38655
[1mStep[0m  [4/21], [94mLoss[0m : 2.42059
[1mStep[0m  [6/21], [94mLoss[0m : 2.48933
[1mStep[0m  [8/21], [94mLoss[0m : 2.46425
[1mStep[0m  [10/21], [94mLoss[0m : 2.52205
[1mStep[0m  [12/21], [94mLoss[0m : 2.38880
[1mStep[0m  [14/21], [94mLoss[0m : 2.49293
[1mStep[0m  [16/21], [94mLoss[0m : 2.47145
[1mStep[0m  [18/21], [94mLoss[0m : 2.33223
[1mStep[0m  [20/21], [94mLoss[0m : 2.40699

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42454
[1mStep[0m  [2/21], [94mLoss[0m : 2.48570
[1mStep[0m  [4/21], [94mLoss[0m : 2.44244
[1mStep[0m  [6/21], [94mLoss[0m : 2.47380
[1mStep[0m  [8/21], [94mLoss[0m : 2.55023
[1mStep[0m  [10/21], [94mLoss[0m : 2.43722
[1mStep[0m  [12/21], [94mLoss[0m : 2.32716
[1mStep[0m  [14/21], [94mLoss[0m : 2.36350
[1mStep[0m  [16/21], [94mLoss[0m : 2.43293
[1mStep[0m  [18/21], [94mLoss[0m : 2.44222
[1mStep[0m  [20/21], [94mLoss[0m : 2.33975

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38270
[1mStep[0m  [2/21], [94mLoss[0m : 2.51943
[1mStep[0m  [4/21], [94mLoss[0m : 2.35449
[1mStep[0m  [6/21], [94mLoss[0m : 2.40657
[1mStep[0m  [8/21], [94mLoss[0m : 2.50939
[1mStep[0m  [10/21], [94mLoss[0m : 2.40361
[1mStep[0m  [12/21], [94mLoss[0m : 2.31476
[1mStep[0m  [14/21], [94mLoss[0m : 2.38131
[1mStep[0m  [16/21], [94mLoss[0m : 2.26734
[1mStep[0m  [18/21], [94mLoss[0m : 2.50139
[1mStep[0m  [20/21], [94mLoss[0m : 2.42000

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33345
[1mStep[0m  [2/21], [94mLoss[0m : 2.42491
[1mStep[0m  [4/21], [94mLoss[0m : 2.53238
[1mStep[0m  [6/21], [94mLoss[0m : 2.28322
[1mStep[0m  [8/21], [94mLoss[0m : 2.28364
[1mStep[0m  [10/21], [94mLoss[0m : 2.51058
[1mStep[0m  [12/21], [94mLoss[0m : 2.38120
[1mStep[0m  [14/21], [94mLoss[0m : 2.37328
[1mStep[0m  [16/21], [94mLoss[0m : 2.60633
[1mStep[0m  [18/21], [94mLoss[0m : 2.30992
[1mStep[0m  [20/21], [94mLoss[0m : 2.54645

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40919
[1mStep[0m  [2/21], [94mLoss[0m : 2.32763
[1mStep[0m  [4/21], [94mLoss[0m : 2.29909
[1mStep[0m  [6/21], [94mLoss[0m : 2.47915
[1mStep[0m  [8/21], [94mLoss[0m : 2.26986
[1mStep[0m  [10/21], [94mLoss[0m : 2.24640
[1mStep[0m  [12/21], [94mLoss[0m : 2.43440
[1mStep[0m  [14/21], [94mLoss[0m : 2.44340
[1mStep[0m  [16/21], [94mLoss[0m : 2.37652
[1mStep[0m  [18/21], [94mLoss[0m : 2.51297
[1mStep[0m  [20/21], [94mLoss[0m : 2.45638

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34188
[1mStep[0m  [2/21], [94mLoss[0m : 2.42348
[1mStep[0m  [4/21], [94mLoss[0m : 2.42332
[1mStep[0m  [6/21], [94mLoss[0m : 2.38028
[1mStep[0m  [8/21], [94mLoss[0m : 2.54865
[1mStep[0m  [10/21], [94mLoss[0m : 2.47317
[1mStep[0m  [12/21], [94mLoss[0m : 2.38056
[1mStep[0m  [14/21], [94mLoss[0m : 2.55044
[1mStep[0m  [16/21], [94mLoss[0m : 2.50093
[1mStep[0m  [18/21], [94mLoss[0m : 2.41357
[1mStep[0m  [20/21], [94mLoss[0m : 2.32024

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39485
[1mStep[0m  [2/21], [94mLoss[0m : 2.34606
[1mStep[0m  [4/21], [94mLoss[0m : 2.44198
[1mStep[0m  [6/21], [94mLoss[0m : 2.38472
[1mStep[0m  [8/21], [94mLoss[0m : 2.41700
[1mStep[0m  [10/21], [94mLoss[0m : 2.39974
[1mStep[0m  [12/21], [94mLoss[0m : 2.51926
[1mStep[0m  [14/21], [94mLoss[0m : 2.29595
[1mStep[0m  [16/21], [94mLoss[0m : 2.47511
[1mStep[0m  [18/21], [94mLoss[0m : 2.46640
[1mStep[0m  [20/21], [94mLoss[0m : 2.47679

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40097
[1mStep[0m  [2/21], [94mLoss[0m : 2.32780
[1mStep[0m  [4/21], [94mLoss[0m : 2.40063
[1mStep[0m  [6/21], [94mLoss[0m : 2.45970
[1mStep[0m  [8/21], [94mLoss[0m : 2.41390
[1mStep[0m  [10/21], [94mLoss[0m : 2.56540
[1mStep[0m  [12/21], [94mLoss[0m : 2.31129
[1mStep[0m  [14/21], [94mLoss[0m : 2.37269
[1mStep[0m  [16/21], [94mLoss[0m : 2.55375
[1mStep[0m  [18/21], [94mLoss[0m : 2.53662
[1mStep[0m  [20/21], [94mLoss[0m : 2.45459

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52118
[1mStep[0m  [2/21], [94mLoss[0m : 2.42599
[1mStep[0m  [4/21], [94mLoss[0m : 2.47697
[1mStep[0m  [6/21], [94mLoss[0m : 2.42302
[1mStep[0m  [8/21], [94mLoss[0m : 2.40678
[1mStep[0m  [10/21], [94mLoss[0m : 2.29320
[1mStep[0m  [12/21], [94mLoss[0m : 2.44490
[1mStep[0m  [14/21], [94mLoss[0m : 2.34768
[1mStep[0m  [16/21], [94mLoss[0m : 2.49558
[1mStep[0m  [18/21], [94mLoss[0m : 2.34113
[1mStep[0m  [20/21], [94mLoss[0m : 2.45813

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24765
[1mStep[0m  [2/21], [94mLoss[0m : 2.47509
[1mStep[0m  [4/21], [94mLoss[0m : 2.44606
[1mStep[0m  [6/21], [94mLoss[0m : 2.42332
[1mStep[0m  [8/21], [94mLoss[0m : 2.58891
[1mStep[0m  [10/21], [94mLoss[0m : 2.31438
[1mStep[0m  [12/21], [94mLoss[0m : 2.43950
[1mStep[0m  [14/21], [94mLoss[0m : 2.35437
[1mStep[0m  [16/21], [94mLoss[0m : 2.48991
[1mStep[0m  [18/21], [94mLoss[0m : 2.46837
[1mStep[0m  [20/21], [94mLoss[0m : 2.48407

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33330
[1mStep[0m  [2/21], [94mLoss[0m : 2.38714
[1mStep[0m  [4/21], [94mLoss[0m : 2.45496
[1mStep[0m  [6/21], [94mLoss[0m : 2.30528
[1mStep[0m  [8/21], [94mLoss[0m : 2.47528
[1mStep[0m  [10/21], [94mLoss[0m : 2.51258
[1mStep[0m  [12/21], [94mLoss[0m : 2.48491
[1mStep[0m  [14/21], [94mLoss[0m : 2.25570
[1mStep[0m  [16/21], [94mLoss[0m : 2.33992
[1mStep[0m  [18/21], [94mLoss[0m : 2.43679
[1mStep[0m  [20/21], [94mLoss[0m : 2.37597

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48903
[1mStep[0m  [2/21], [94mLoss[0m : 2.22561
[1mStep[0m  [4/21], [94mLoss[0m : 2.45747
[1mStep[0m  [6/21], [94mLoss[0m : 2.23335
[1mStep[0m  [8/21], [94mLoss[0m : 2.27742
[1mStep[0m  [10/21], [94mLoss[0m : 2.38481
[1mStep[0m  [12/21], [94mLoss[0m : 2.43745
[1mStep[0m  [14/21], [94mLoss[0m : 2.33514
[1mStep[0m  [16/21], [94mLoss[0m : 2.54153
[1mStep[0m  [18/21], [94mLoss[0m : 2.39130
[1mStep[0m  [20/21], [94mLoss[0m : 2.38249

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54941
[1mStep[0m  [2/21], [94mLoss[0m : 2.44771
[1mStep[0m  [4/21], [94mLoss[0m : 2.29708
[1mStep[0m  [6/21], [94mLoss[0m : 2.42902
[1mStep[0m  [8/21], [94mLoss[0m : 2.49970
[1mStep[0m  [10/21], [94mLoss[0m : 2.39865
[1mStep[0m  [12/21], [94mLoss[0m : 2.25835
[1mStep[0m  [14/21], [94mLoss[0m : 2.36288
[1mStep[0m  [16/21], [94mLoss[0m : 2.42993
[1mStep[0m  [18/21], [94mLoss[0m : 2.45116
[1mStep[0m  [20/21], [94mLoss[0m : 2.43514

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40448
[1mStep[0m  [2/21], [94mLoss[0m : 2.35163
[1mStep[0m  [4/21], [94mLoss[0m : 2.40620
[1mStep[0m  [6/21], [94mLoss[0m : 2.54272
[1mStep[0m  [8/21], [94mLoss[0m : 2.48151
[1mStep[0m  [10/21], [94mLoss[0m : 2.38039
[1mStep[0m  [12/21], [94mLoss[0m : 2.32829
[1mStep[0m  [14/21], [94mLoss[0m : 2.56300
[1mStep[0m  [16/21], [94mLoss[0m : 2.43744
[1mStep[0m  [18/21], [94mLoss[0m : 2.31118
[1mStep[0m  [20/21], [94mLoss[0m : 2.49617

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41812
[1mStep[0m  [2/21], [94mLoss[0m : 2.40812
[1mStep[0m  [4/21], [94mLoss[0m : 2.46924
[1mStep[0m  [6/21], [94mLoss[0m : 2.33738
[1mStep[0m  [8/21], [94mLoss[0m : 2.32681
[1mStep[0m  [10/21], [94mLoss[0m : 2.15862
[1mStep[0m  [12/21], [94mLoss[0m : 2.44899
[1mStep[0m  [14/21], [94mLoss[0m : 2.38843
[1mStep[0m  [16/21], [94mLoss[0m : 2.46643
[1mStep[0m  [18/21], [94mLoss[0m : 2.38133
[1mStep[0m  [20/21], [94mLoss[0m : 2.36553

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40663
[1mStep[0m  [2/21], [94mLoss[0m : 2.51046
[1mStep[0m  [4/21], [94mLoss[0m : 2.40965
[1mStep[0m  [6/21], [94mLoss[0m : 2.27113
[1mStep[0m  [8/21], [94mLoss[0m : 2.32843
[1mStep[0m  [10/21], [94mLoss[0m : 2.37305
[1mStep[0m  [12/21], [94mLoss[0m : 2.56056
[1mStep[0m  [14/21], [94mLoss[0m : 2.26825
[1mStep[0m  [16/21], [94mLoss[0m : 2.32289
[1mStep[0m  [18/21], [94mLoss[0m : 2.33201
[1mStep[0m  [20/21], [94mLoss[0m : 2.51175

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34466
[1mStep[0m  [2/21], [94mLoss[0m : 2.32905
[1mStep[0m  [4/21], [94mLoss[0m : 2.30066
[1mStep[0m  [6/21], [94mLoss[0m : 2.37810
[1mStep[0m  [8/21], [94mLoss[0m : 2.41586
[1mStep[0m  [10/21], [94mLoss[0m : 2.44705
[1mStep[0m  [12/21], [94mLoss[0m : 2.39258
[1mStep[0m  [14/21], [94mLoss[0m : 2.37607
[1mStep[0m  [16/21], [94mLoss[0m : 2.50124
[1mStep[0m  [18/21], [94mLoss[0m : 2.40859
[1mStep[0m  [20/21], [94mLoss[0m : 2.49616

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25225
[1mStep[0m  [2/21], [94mLoss[0m : 2.44215
[1mStep[0m  [4/21], [94mLoss[0m : 2.33902
[1mStep[0m  [6/21], [94mLoss[0m : 2.40525
[1mStep[0m  [8/21], [94mLoss[0m : 2.45894
[1mStep[0m  [10/21], [94mLoss[0m : 2.48583
[1mStep[0m  [12/21], [94mLoss[0m : 2.22930
[1mStep[0m  [14/21], [94mLoss[0m : 2.35637
[1mStep[0m  [16/21], [94mLoss[0m : 2.44192
[1mStep[0m  [18/21], [94mLoss[0m : 2.24385
[1mStep[0m  [20/21], [94mLoss[0m : 2.45382

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24831
[1mStep[0m  [2/21], [94mLoss[0m : 2.47297
[1mStep[0m  [4/21], [94mLoss[0m : 2.43437
[1mStep[0m  [6/21], [94mLoss[0m : 2.41353
[1mStep[0m  [8/21], [94mLoss[0m : 2.39222
[1mStep[0m  [10/21], [94mLoss[0m : 2.37903
[1mStep[0m  [12/21], [94mLoss[0m : 2.31401
[1mStep[0m  [14/21], [94mLoss[0m : 2.36100
[1mStep[0m  [16/21], [94mLoss[0m : 2.51173
[1mStep[0m  [18/21], [94mLoss[0m : 2.25746
[1mStep[0m  [20/21], [94mLoss[0m : 2.45622

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.313, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.3248184749058316
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.23408
[1mStep[0m  [2/21], [94mLoss[0m : 2.33334
[1mStep[0m  [4/21], [94mLoss[0m : 2.33741
[1mStep[0m  [6/21], [94mLoss[0m : 2.59397
[1mStep[0m  [8/21], [94mLoss[0m : 2.39965
[1mStep[0m  [10/21], [94mLoss[0m : 2.45525
[1mStep[0m  [12/21], [94mLoss[0m : 2.68213
[1mStep[0m  [14/21], [94mLoss[0m : 2.36859
[1mStep[0m  [16/21], [94mLoss[0m : 2.57590
[1mStep[0m  [18/21], [94mLoss[0m : 2.54756
[1mStep[0m  [20/21], [94mLoss[0m : 2.41552

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32863
[1mStep[0m  [2/21], [94mLoss[0m : 2.29629
[1mStep[0m  [4/21], [94mLoss[0m : 2.42980
[1mStep[0m  [6/21], [94mLoss[0m : 2.37932
[1mStep[0m  [8/21], [94mLoss[0m : 2.34370
[1mStep[0m  [10/21], [94mLoss[0m : 2.26731
[1mStep[0m  [12/21], [94mLoss[0m : 2.36755
[1mStep[0m  [14/21], [94mLoss[0m : 2.34251
[1mStep[0m  [16/21], [94mLoss[0m : 2.40712
[1mStep[0m  [18/21], [94mLoss[0m : 2.33520
[1mStep[0m  [20/21], [94mLoss[0m : 2.30549

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26392
[1mStep[0m  [2/21], [94mLoss[0m : 2.18076
[1mStep[0m  [4/21], [94mLoss[0m : 2.03244
[1mStep[0m  [6/21], [94mLoss[0m : 2.19036
[1mStep[0m  [8/21], [94mLoss[0m : 2.27917
[1mStep[0m  [10/21], [94mLoss[0m : 2.41190
[1mStep[0m  [12/21], [94mLoss[0m : 2.31150
[1mStep[0m  [14/21], [94mLoss[0m : 2.26757
[1mStep[0m  [16/21], [94mLoss[0m : 2.24025
[1mStep[0m  [18/21], [94mLoss[0m : 2.20053
[1mStep[0m  [20/21], [94mLoss[0m : 2.27494

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.03424
[1mStep[0m  [2/21], [94mLoss[0m : 2.12990
[1mStep[0m  [4/21], [94mLoss[0m : 2.01530
[1mStep[0m  [6/21], [94mLoss[0m : 2.21618
[1mStep[0m  [8/21], [94mLoss[0m : 2.00112
[1mStep[0m  [10/21], [94mLoss[0m : 2.04534
[1mStep[0m  [12/21], [94mLoss[0m : 2.12025
[1mStep[0m  [14/21], [94mLoss[0m : 2.15243
[1mStep[0m  [16/21], [94mLoss[0m : 2.05259
[1mStep[0m  [18/21], [94mLoss[0m : 2.20119
[1mStep[0m  [20/21], [94mLoss[0m : 2.28290

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95372
[1mStep[0m  [2/21], [94mLoss[0m : 2.04956
[1mStep[0m  [4/21], [94mLoss[0m : 2.03233
[1mStep[0m  [6/21], [94mLoss[0m : 2.11037
[1mStep[0m  [8/21], [94mLoss[0m : 2.13222
[1mStep[0m  [10/21], [94mLoss[0m : 1.93263
[1mStep[0m  [12/21], [94mLoss[0m : 2.09495
[1mStep[0m  [14/21], [94mLoss[0m : 2.10536
[1mStep[0m  [16/21], [94mLoss[0m : 2.07993
[1mStep[0m  [18/21], [94mLoss[0m : 2.08498
[1mStep[0m  [20/21], [94mLoss[0m : 2.02174

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97349
[1mStep[0m  [2/21], [94mLoss[0m : 1.91718
[1mStep[0m  [4/21], [94mLoss[0m : 1.82886
[1mStep[0m  [6/21], [94mLoss[0m : 1.95345
[1mStep[0m  [8/21], [94mLoss[0m : 1.96438
[1mStep[0m  [10/21], [94mLoss[0m : 2.02284
[1mStep[0m  [12/21], [94mLoss[0m : 2.05467
[1mStep[0m  [14/21], [94mLoss[0m : 1.93726
[1mStep[0m  [16/21], [94mLoss[0m : 1.89369
[1mStep[0m  [18/21], [94mLoss[0m : 1.97399
[1mStep[0m  [20/21], [94mLoss[0m : 1.92456

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01113
[1mStep[0m  [2/21], [94mLoss[0m : 1.93491
[1mStep[0m  [4/21], [94mLoss[0m : 1.87240
[1mStep[0m  [6/21], [94mLoss[0m : 1.86422
[1mStep[0m  [8/21], [94mLoss[0m : 1.87234
[1mStep[0m  [10/21], [94mLoss[0m : 2.01756
[1mStep[0m  [12/21], [94mLoss[0m : 1.88323
[1mStep[0m  [14/21], [94mLoss[0m : 1.84171
[1mStep[0m  [16/21], [94mLoss[0m : 1.94112
[1mStep[0m  [18/21], [94mLoss[0m : 2.05392
[1mStep[0m  [20/21], [94mLoss[0m : 1.83067

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.87716
[1mStep[0m  [2/21], [94mLoss[0m : 1.84174
[1mStep[0m  [4/21], [94mLoss[0m : 1.79184
[1mStep[0m  [6/21], [94mLoss[0m : 1.89093
[1mStep[0m  [8/21], [94mLoss[0m : 1.96162
[1mStep[0m  [10/21], [94mLoss[0m : 1.67722
[1mStep[0m  [12/21], [94mLoss[0m : 1.91104
[1mStep[0m  [14/21], [94mLoss[0m : 1.79110
[1mStep[0m  [16/21], [94mLoss[0m : 1.83381
[1mStep[0m  [18/21], [94mLoss[0m : 1.87838
[1mStep[0m  [20/21], [94mLoss[0m : 1.83089

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77157
[1mStep[0m  [2/21], [94mLoss[0m : 1.76962
[1mStep[0m  [4/21], [94mLoss[0m : 1.72118
[1mStep[0m  [6/21], [94mLoss[0m : 1.60081
[1mStep[0m  [8/21], [94mLoss[0m : 1.73963
[1mStep[0m  [10/21], [94mLoss[0m : 1.86630
[1mStep[0m  [12/21], [94mLoss[0m : 1.88433
[1mStep[0m  [14/21], [94mLoss[0m : 1.83664
[1mStep[0m  [16/21], [94mLoss[0m : 1.78249
[1mStep[0m  [18/21], [94mLoss[0m : 1.78786
[1mStep[0m  [20/21], [94mLoss[0m : 1.82675

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.556, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.64631
[1mStep[0m  [2/21], [94mLoss[0m : 1.61895
[1mStep[0m  [4/21], [94mLoss[0m : 1.67186
[1mStep[0m  [6/21], [94mLoss[0m : 1.74175
[1mStep[0m  [8/21], [94mLoss[0m : 1.75282
[1mStep[0m  [10/21], [94mLoss[0m : 1.75376
[1mStep[0m  [12/21], [94mLoss[0m : 1.75217
[1mStep[0m  [14/21], [94mLoss[0m : 1.68709
[1mStep[0m  [16/21], [94mLoss[0m : 1.74248
[1mStep[0m  [18/21], [94mLoss[0m : 1.73298
[1mStep[0m  [20/21], [94mLoss[0m : 1.66199

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63141
[1mStep[0m  [2/21], [94mLoss[0m : 1.66415
[1mStep[0m  [4/21], [94mLoss[0m : 1.61443
[1mStep[0m  [6/21], [94mLoss[0m : 1.51643
[1mStep[0m  [8/21], [94mLoss[0m : 1.50881
[1mStep[0m  [10/21], [94mLoss[0m : 1.65092
[1mStep[0m  [12/21], [94mLoss[0m : 1.58943
[1mStep[0m  [14/21], [94mLoss[0m : 1.56634
[1mStep[0m  [16/21], [94mLoss[0m : 1.73893
[1mStep[0m  [18/21], [94mLoss[0m : 1.63396
[1mStep[0m  [20/21], [94mLoss[0m : 1.72577

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.60537
[1mStep[0m  [2/21], [94mLoss[0m : 1.61036
[1mStep[0m  [4/21], [94mLoss[0m : 1.65543
[1mStep[0m  [6/21], [94mLoss[0m : 1.54302
[1mStep[0m  [8/21], [94mLoss[0m : 1.68266
[1mStep[0m  [10/21], [94mLoss[0m : 1.61240
[1mStep[0m  [12/21], [94mLoss[0m : 1.65955
[1mStep[0m  [14/21], [94mLoss[0m : 1.56101
[1mStep[0m  [16/21], [94mLoss[0m : 1.74544
[1mStep[0m  [18/21], [94mLoss[0m : 1.55771
[1mStep[0m  [20/21], [94mLoss[0m : 1.58385

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.48825
[1mStep[0m  [2/21], [94mLoss[0m : 1.49805
[1mStep[0m  [4/21], [94mLoss[0m : 1.59718
[1mStep[0m  [6/21], [94mLoss[0m : 1.67140
[1mStep[0m  [8/21], [94mLoss[0m : 1.49771
[1mStep[0m  [10/21], [94mLoss[0m : 1.46682
[1mStep[0m  [12/21], [94mLoss[0m : 1.72114
[1mStep[0m  [14/21], [94mLoss[0m : 1.63681
[1mStep[0m  [16/21], [94mLoss[0m : 1.60086
[1mStep[0m  [18/21], [94mLoss[0m : 1.63408
[1mStep[0m  [20/21], [94mLoss[0m : 1.61367

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.560, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.40801
[1mStep[0m  [2/21], [94mLoss[0m : 1.52883
[1mStep[0m  [4/21], [94mLoss[0m : 1.54400
[1mStep[0m  [6/21], [94mLoss[0m : 1.58142
[1mStep[0m  [8/21], [94mLoss[0m : 1.65146
[1mStep[0m  [10/21], [94mLoss[0m : 1.55895
[1mStep[0m  [12/21], [94mLoss[0m : 1.46104
[1mStep[0m  [14/21], [94mLoss[0m : 1.63217
[1mStep[0m  [16/21], [94mLoss[0m : 1.39395
[1mStep[0m  [18/21], [94mLoss[0m : 1.51399
[1mStep[0m  [20/21], [94mLoss[0m : 1.56268

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.56067
[1mStep[0m  [2/21], [94mLoss[0m : 1.34934
[1mStep[0m  [4/21], [94mLoss[0m : 1.44136
[1mStep[0m  [6/21], [94mLoss[0m : 1.57394
[1mStep[0m  [8/21], [94mLoss[0m : 1.50497
[1mStep[0m  [10/21], [94mLoss[0m : 1.49378
[1mStep[0m  [12/21], [94mLoss[0m : 1.44310
[1mStep[0m  [14/21], [94mLoss[0m : 1.56398
[1mStep[0m  [16/21], [94mLoss[0m : 1.51751
[1mStep[0m  [18/21], [94mLoss[0m : 1.52303
[1mStep[0m  [20/21], [94mLoss[0m : 1.48229

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.589, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.28724
[1mStep[0m  [2/21], [94mLoss[0m : 1.34015
[1mStep[0m  [4/21], [94mLoss[0m : 1.41238
[1mStep[0m  [6/21], [94mLoss[0m : 1.40074
[1mStep[0m  [8/21], [94mLoss[0m : 1.48427
[1mStep[0m  [10/21], [94mLoss[0m : 1.47977
[1mStep[0m  [12/21], [94mLoss[0m : 1.45568
[1mStep[0m  [14/21], [94mLoss[0m : 1.59153
[1mStep[0m  [16/21], [94mLoss[0m : 1.48790
[1mStep[0m  [18/21], [94mLoss[0m : 1.56389
[1mStep[0m  [20/21], [94mLoss[0m : 1.53052

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.41270
[1mStep[0m  [2/21], [94mLoss[0m : 1.38482
[1mStep[0m  [4/21], [94mLoss[0m : 1.33940
[1mStep[0m  [6/21], [94mLoss[0m : 1.43503
[1mStep[0m  [8/21], [94mLoss[0m : 1.50221
[1mStep[0m  [10/21], [94mLoss[0m : 1.54666
[1mStep[0m  [12/21], [94mLoss[0m : 1.34599
[1mStep[0m  [14/21], [94mLoss[0m : 1.48795
[1mStep[0m  [16/21], [94mLoss[0m : 1.43778
[1mStep[0m  [18/21], [94mLoss[0m : 1.52537
[1mStep[0m  [20/21], [94mLoss[0m : 1.42908

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.437, [92mTest[0m: 2.571, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.32962
[1mStep[0m  [2/21], [94mLoss[0m : 1.44884
[1mStep[0m  [4/21], [94mLoss[0m : 1.36409
[1mStep[0m  [6/21], [94mLoss[0m : 1.49181
[1mStep[0m  [8/21], [94mLoss[0m : 1.39874
[1mStep[0m  [10/21], [94mLoss[0m : 1.45580
[1mStep[0m  [12/21], [94mLoss[0m : 1.39063
[1mStep[0m  [14/21], [94mLoss[0m : 1.42251
[1mStep[0m  [16/21], [94mLoss[0m : 1.38123
[1mStep[0m  [18/21], [94mLoss[0m : 1.37890
[1mStep[0m  [20/21], [94mLoss[0m : 1.42439

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.406, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.33580
[1mStep[0m  [2/21], [94mLoss[0m : 1.32227
[1mStep[0m  [4/21], [94mLoss[0m : 1.40137
[1mStep[0m  [6/21], [94mLoss[0m : 1.43706
[1mStep[0m  [8/21], [94mLoss[0m : 1.31611
[1mStep[0m  [10/21], [94mLoss[0m : 1.36974
[1mStep[0m  [12/21], [94mLoss[0m : 1.29674
[1mStep[0m  [14/21], [94mLoss[0m : 1.55025
[1mStep[0m  [16/21], [94mLoss[0m : 1.31831
[1mStep[0m  [18/21], [94mLoss[0m : 1.49430
[1mStep[0m  [20/21], [94mLoss[0m : 1.32553

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.394, [92mTest[0m: 2.539, [96mlr[0m: 0.01
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 18 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.517
====================================

Phase 2 - Evaluation MAE:  2.5174427713666643
MAE score P1      2.324818
MAE score P2      2.517443
loss              1.393968
learning_rate         0.01
batch_size             512
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.75393
[1mStep[0m  [2/21], [94mLoss[0m : 9.02873
[1mStep[0m  [4/21], [94mLoss[0m : 7.28381
[1mStep[0m  [6/21], [94mLoss[0m : 5.64581
[1mStep[0m  [8/21], [94mLoss[0m : 4.03882
[1mStep[0m  [10/21], [94mLoss[0m : 3.46675
[1mStep[0m  [12/21], [94mLoss[0m : 3.19097
[1mStep[0m  [14/21], [94mLoss[0m : 2.87413
[1mStep[0m  [16/21], [94mLoss[0m : 2.96763
[1mStep[0m  [18/21], [94mLoss[0m : 2.59129
[1mStep[0m  [20/21], [94mLoss[0m : 2.69010

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.887, [92mTest[0m: 10.667, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70348
[1mStep[0m  [2/21], [94mLoss[0m : 2.71040
[1mStep[0m  [4/21], [94mLoss[0m : 2.55475
[1mStep[0m  [6/21], [94mLoss[0m : 2.67844
[1mStep[0m  [8/21], [94mLoss[0m : 2.87772
[1mStep[0m  [10/21], [94mLoss[0m : 2.44465
[1mStep[0m  [12/21], [94mLoss[0m : 2.69187
[1mStep[0m  [14/21], [94mLoss[0m : 2.52279
[1mStep[0m  [16/21], [94mLoss[0m : 2.74113
[1mStep[0m  [18/21], [94mLoss[0m : 2.66746
[1mStep[0m  [20/21], [94mLoss[0m : 2.70406

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.771, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69443
[1mStep[0m  [2/21], [94mLoss[0m : 2.43777
[1mStep[0m  [4/21], [94mLoss[0m : 2.59719
[1mStep[0m  [6/21], [94mLoss[0m : 2.56095
[1mStep[0m  [8/21], [94mLoss[0m : 2.65041
[1mStep[0m  [10/21], [94mLoss[0m : 2.53609
[1mStep[0m  [12/21], [94mLoss[0m : 2.52901
[1mStep[0m  [14/21], [94mLoss[0m : 2.41926
[1mStep[0m  [16/21], [94mLoss[0m : 2.74882
[1mStep[0m  [18/21], [94mLoss[0m : 2.48273
[1mStep[0m  [20/21], [94mLoss[0m : 2.39060

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.538, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55037
[1mStep[0m  [2/21], [94mLoss[0m : 2.52951
[1mStep[0m  [4/21], [94mLoss[0m : 2.64713
[1mStep[0m  [6/21], [94mLoss[0m : 2.48137
[1mStep[0m  [8/21], [94mLoss[0m : 2.52966
[1mStep[0m  [10/21], [94mLoss[0m : 2.65291
[1mStep[0m  [12/21], [94mLoss[0m : 2.49264
[1mStep[0m  [14/21], [94mLoss[0m : 2.66186
[1mStep[0m  [16/21], [94mLoss[0m : 2.43888
[1mStep[0m  [18/21], [94mLoss[0m : 2.55579
[1mStep[0m  [20/21], [94mLoss[0m : 2.30733

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44552
[1mStep[0m  [2/21], [94mLoss[0m : 2.37785
[1mStep[0m  [4/21], [94mLoss[0m : 2.53732
[1mStep[0m  [6/21], [94mLoss[0m : 2.40847
[1mStep[0m  [8/21], [94mLoss[0m : 2.54583
[1mStep[0m  [10/21], [94mLoss[0m : 2.60177
[1mStep[0m  [12/21], [94mLoss[0m : 2.37135
[1mStep[0m  [14/21], [94mLoss[0m : 2.54923
[1mStep[0m  [16/21], [94mLoss[0m : 2.44493
[1mStep[0m  [18/21], [94mLoss[0m : 2.45115
[1mStep[0m  [20/21], [94mLoss[0m : 2.55668

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52479
[1mStep[0m  [2/21], [94mLoss[0m : 2.51562
[1mStep[0m  [4/21], [94mLoss[0m : 2.44085
[1mStep[0m  [6/21], [94mLoss[0m : 2.56924
[1mStep[0m  [8/21], [94mLoss[0m : 2.45448
[1mStep[0m  [10/21], [94mLoss[0m : 2.56138
[1mStep[0m  [12/21], [94mLoss[0m : 2.55157
[1mStep[0m  [14/21], [94mLoss[0m : 2.41012
[1mStep[0m  [16/21], [94mLoss[0m : 2.38794
[1mStep[0m  [18/21], [94mLoss[0m : 2.42782
[1mStep[0m  [20/21], [94mLoss[0m : 2.41392

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44193
[1mStep[0m  [2/21], [94mLoss[0m : 2.51976
[1mStep[0m  [4/21], [94mLoss[0m : 2.36407
[1mStep[0m  [6/21], [94mLoss[0m : 2.35423
[1mStep[0m  [8/21], [94mLoss[0m : 2.46303
[1mStep[0m  [10/21], [94mLoss[0m : 2.45712
[1mStep[0m  [12/21], [94mLoss[0m : 2.55771
[1mStep[0m  [14/21], [94mLoss[0m : 2.39042
[1mStep[0m  [16/21], [94mLoss[0m : 2.54410
[1mStep[0m  [18/21], [94mLoss[0m : 2.46650
[1mStep[0m  [20/21], [94mLoss[0m : 2.33593

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48538
[1mStep[0m  [2/21], [94mLoss[0m : 2.45635
[1mStep[0m  [4/21], [94mLoss[0m : 2.54759
[1mStep[0m  [6/21], [94mLoss[0m : 2.48518
[1mStep[0m  [8/21], [94mLoss[0m : 2.41397
[1mStep[0m  [10/21], [94mLoss[0m : 2.42425
[1mStep[0m  [12/21], [94mLoss[0m : 2.39885
[1mStep[0m  [14/21], [94mLoss[0m : 2.46559
[1mStep[0m  [16/21], [94mLoss[0m : 2.47810
[1mStep[0m  [18/21], [94mLoss[0m : 2.42211
[1mStep[0m  [20/21], [94mLoss[0m : 2.37855

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40170
[1mStep[0m  [2/21], [94mLoss[0m : 2.54236
[1mStep[0m  [4/21], [94mLoss[0m : 2.35923
[1mStep[0m  [6/21], [94mLoss[0m : 2.65483
[1mStep[0m  [8/21], [94mLoss[0m : 2.41065
[1mStep[0m  [10/21], [94mLoss[0m : 2.42621
[1mStep[0m  [12/21], [94mLoss[0m : 2.30628
[1mStep[0m  [14/21], [94mLoss[0m : 2.43347
[1mStep[0m  [16/21], [94mLoss[0m : 2.31112
[1mStep[0m  [18/21], [94mLoss[0m : 2.40253
[1mStep[0m  [20/21], [94mLoss[0m : 2.69876

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44721
[1mStep[0m  [2/21], [94mLoss[0m : 2.48999
[1mStep[0m  [4/21], [94mLoss[0m : 2.52818
[1mStep[0m  [6/21], [94mLoss[0m : 2.49995
[1mStep[0m  [8/21], [94mLoss[0m : 2.52699
[1mStep[0m  [10/21], [94mLoss[0m : 2.50397
[1mStep[0m  [12/21], [94mLoss[0m : 2.61385
[1mStep[0m  [14/21], [94mLoss[0m : 2.53281
[1mStep[0m  [16/21], [94mLoss[0m : 2.25318
[1mStep[0m  [18/21], [94mLoss[0m : 2.43832
[1mStep[0m  [20/21], [94mLoss[0m : 2.45295

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43161
[1mStep[0m  [2/21], [94mLoss[0m : 2.72475
[1mStep[0m  [4/21], [94mLoss[0m : 2.35993
[1mStep[0m  [6/21], [94mLoss[0m : 2.32516
[1mStep[0m  [8/21], [94mLoss[0m : 2.32589
[1mStep[0m  [10/21], [94mLoss[0m : 2.39842
[1mStep[0m  [12/21], [94mLoss[0m : 2.48134
[1mStep[0m  [14/21], [94mLoss[0m : 2.48355
[1mStep[0m  [16/21], [94mLoss[0m : 2.46203
[1mStep[0m  [18/21], [94mLoss[0m : 2.59185
[1mStep[0m  [20/21], [94mLoss[0m : 2.47044

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36398
[1mStep[0m  [2/21], [94mLoss[0m : 2.39459
[1mStep[0m  [4/21], [94mLoss[0m : 2.47240
[1mStep[0m  [6/21], [94mLoss[0m : 2.43501
[1mStep[0m  [8/21], [94mLoss[0m : 2.47307
[1mStep[0m  [10/21], [94mLoss[0m : 2.46386
[1mStep[0m  [12/21], [94mLoss[0m : 2.50466
[1mStep[0m  [14/21], [94mLoss[0m : 2.49599
[1mStep[0m  [16/21], [94mLoss[0m : 2.36155
[1mStep[0m  [18/21], [94mLoss[0m : 2.38656
[1mStep[0m  [20/21], [94mLoss[0m : 2.48155

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42185
[1mStep[0m  [2/21], [94mLoss[0m : 2.51808
[1mStep[0m  [4/21], [94mLoss[0m : 2.37189
[1mStep[0m  [6/21], [94mLoss[0m : 2.50342
[1mStep[0m  [8/21], [94mLoss[0m : 2.50542
[1mStep[0m  [10/21], [94mLoss[0m : 2.44986
[1mStep[0m  [12/21], [94mLoss[0m : 2.45053
[1mStep[0m  [14/21], [94mLoss[0m : 2.32372
[1mStep[0m  [16/21], [94mLoss[0m : 2.46074
[1mStep[0m  [18/21], [94mLoss[0m : 2.51494
[1mStep[0m  [20/21], [94mLoss[0m : 2.48392

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43305
[1mStep[0m  [2/21], [94mLoss[0m : 2.44349
[1mStep[0m  [4/21], [94mLoss[0m : 2.50475
[1mStep[0m  [6/21], [94mLoss[0m : 2.51837
[1mStep[0m  [8/21], [94mLoss[0m : 2.39336
[1mStep[0m  [10/21], [94mLoss[0m : 2.38300
[1mStep[0m  [12/21], [94mLoss[0m : 2.39529
[1mStep[0m  [14/21], [94mLoss[0m : 2.31490
[1mStep[0m  [16/21], [94mLoss[0m : 2.52241
[1mStep[0m  [18/21], [94mLoss[0m : 2.34889
[1mStep[0m  [20/21], [94mLoss[0m : 2.45477

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42502
[1mStep[0m  [2/21], [94mLoss[0m : 2.47693
[1mStep[0m  [4/21], [94mLoss[0m : 2.36721
[1mStep[0m  [6/21], [94mLoss[0m : 2.39946
[1mStep[0m  [8/21], [94mLoss[0m : 2.59933
[1mStep[0m  [10/21], [94mLoss[0m : 2.28762
[1mStep[0m  [12/21], [94mLoss[0m : 2.50114
[1mStep[0m  [14/21], [94mLoss[0m : 2.38844
[1mStep[0m  [16/21], [94mLoss[0m : 2.53268
[1mStep[0m  [18/21], [94mLoss[0m : 2.38261
[1mStep[0m  [20/21], [94mLoss[0m : 2.30478

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41630
[1mStep[0m  [2/21], [94mLoss[0m : 2.37424
[1mStep[0m  [4/21], [94mLoss[0m : 2.41091
[1mStep[0m  [6/21], [94mLoss[0m : 2.40212
[1mStep[0m  [8/21], [94mLoss[0m : 2.45679
[1mStep[0m  [10/21], [94mLoss[0m : 2.49198
[1mStep[0m  [12/21], [94mLoss[0m : 2.45643
[1mStep[0m  [14/21], [94mLoss[0m : 2.42261
[1mStep[0m  [16/21], [94mLoss[0m : 2.46427
[1mStep[0m  [18/21], [94mLoss[0m : 2.52798
[1mStep[0m  [20/21], [94mLoss[0m : 2.30864

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28206
[1mStep[0m  [2/21], [94mLoss[0m : 2.36047
[1mStep[0m  [4/21], [94mLoss[0m : 2.57963
[1mStep[0m  [6/21], [94mLoss[0m : 2.32908
[1mStep[0m  [8/21], [94mLoss[0m : 2.28993
[1mStep[0m  [10/21], [94mLoss[0m : 2.50357
[1mStep[0m  [12/21], [94mLoss[0m : 2.52228
[1mStep[0m  [14/21], [94mLoss[0m : 2.47621
[1mStep[0m  [16/21], [94mLoss[0m : 2.39679
[1mStep[0m  [18/21], [94mLoss[0m : 2.32533
[1mStep[0m  [20/21], [94mLoss[0m : 2.46850

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46673
[1mStep[0m  [2/21], [94mLoss[0m : 2.30237
[1mStep[0m  [4/21], [94mLoss[0m : 2.59737
[1mStep[0m  [6/21], [94mLoss[0m : 2.51313
[1mStep[0m  [8/21], [94mLoss[0m : 2.46573
[1mStep[0m  [10/21], [94mLoss[0m : 2.40664
[1mStep[0m  [12/21], [94mLoss[0m : 2.46695
[1mStep[0m  [14/21], [94mLoss[0m : 2.56258
[1mStep[0m  [16/21], [94mLoss[0m : 2.38831
[1mStep[0m  [18/21], [94mLoss[0m : 2.46205
[1mStep[0m  [20/21], [94mLoss[0m : 2.42396

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30063
[1mStep[0m  [2/21], [94mLoss[0m : 2.43560
[1mStep[0m  [4/21], [94mLoss[0m : 2.52623
[1mStep[0m  [6/21], [94mLoss[0m : 2.43160
[1mStep[0m  [8/21], [94mLoss[0m : 2.54086
[1mStep[0m  [10/21], [94mLoss[0m : 2.43273
[1mStep[0m  [12/21], [94mLoss[0m : 2.46591
[1mStep[0m  [14/21], [94mLoss[0m : 2.47264
[1mStep[0m  [16/21], [94mLoss[0m : 2.37136
[1mStep[0m  [18/21], [94mLoss[0m : 2.46202
[1mStep[0m  [20/21], [94mLoss[0m : 2.38460

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24351
[1mStep[0m  [2/21], [94mLoss[0m : 2.29103
[1mStep[0m  [4/21], [94mLoss[0m : 2.38323
[1mStep[0m  [6/21], [94mLoss[0m : 2.52584
[1mStep[0m  [8/21], [94mLoss[0m : 2.47094
[1mStep[0m  [10/21], [94mLoss[0m : 2.48042
[1mStep[0m  [12/21], [94mLoss[0m : 2.45911
[1mStep[0m  [14/21], [94mLoss[0m : 2.46168
[1mStep[0m  [16/21], [94mLoss[0m : 2.37745
[1mStep[0m  [18/21], [94mLoss[0m : 2.49985
[1mStep[0m  [20/21], [94mLoss[0m : 2.38597

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48342
[1mStep[0m  [2/21], [94mLoss[0m : 2.38612
[1mStep[0m  [4/21], [94mLoss[0m : 2.43087
[1mStep[0m  [6/21], [94mLoss[0m : 2.52750
[1mStep[0m  [8/21], [94mLoss[0m : 2.47492
[1mStep[0m  [10/21], [94mLoss[0m : 2.42193
[1mStep[0m  [12/21], [94mLoss[0m : 2.44978
[1mStep[0m  [14/21], [94mLoss[0m : 2.41862
[1mStep[0m  [16/21], [94mLoss[0m : 2.46235
[1mStep[0m  [18/21], [94mLoss[0m : 2.52970
[1mStep[0m  [20/21], [94mLoss[0m : 2.22030

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56091
[1mStep[0m  [2/21], [94mLoss[0m : 2.36096
[1mStep[0m  [4/21], [94mLoss[0m : 2.48852
[1mStep[0m  [6/21], [94mLoss[0m : 2.40151
[1mStep[0m  [8/21], [94mLoss[0m : 2.34988
[1mStep[0m  [10/21], [94mLoss[0m : 2.52169
[1mStep[0m  [12/21], [94mLoss[0m : 2.41199
[1mStep[0m  [14/21], [94mLoss[0m : 2.36995
[1mStep[0m  [16/21], [94mLoss[0m : 2.38853
[1mStep[0m  [18/21], [94mLoss[0m : 2.48005
[1mStep[0m  [20/21], [94mLoss[0m : 2.39315

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43434
[1mStep[0m  [2/21], [94mLoss[0m : 2.51960
[1mStep[0m  [4/21], [94mLoss[0m : 2.50918
[1mStep[0m  [6/21], [94mLoss[0m : 2.50701
[1mStep[0m  [8/21], [94mLoss[0m : 2.38449
[1mStep[0m  [10/21], [94mLoss[0m : 2.55694
[1mStep[0m  [12/21], [94mLoss[0m : 2.28434
[1mStep[0m  [14/21], [94mLoss[0m : 2.51556
[1mStep[0m  [16/21], [94mLoss[0m : 2.33341
[1mStep[0m  [18/21], [94mLoss[0m : 2.42457
[1mStep[0m  [20/21], [94mLoss[0m : 2.22279

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44383
[1mStep[0m  [2/21], [94mLoss[0m : 2.46291
[1mStep[0m  [4/21], [94mLoss[0m : 2.54045
[1mStep[0m  [6/21], [94mLoss[0m : 2.57123
[1mStep[0m  [8/21], [94mLoss[0m : 2.44388
[1mStep[0m  [10/21], [94mLoss[0m : 2.58664
[1mStep[0m  [12/21], [94mLoss[0m : 2.37946
[1mStep[0m  [14/21], [94mLoss[0m : 2.32065
[1mStep[0m  [16/21], [94mLoss[0m : 2.43606
[1mStep[0m  [18/21], [94mLoss[0m : 2.36041
[1mStep[0m  [20/21], [94mLoss[0m : 2.40815

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56087
[1mStep[0m  [2/21], [94mLoss[0m : 2.48110
[1mStep[0m  [4/21], [94mLoss[0m : 2.55134
[1mStep[0m  [6/21], [94mLoss[0m : 2.47172
[1mStep[0m  [8/21], [94mLoss[0m : 2.41947
[1mStep[0m  [10/21], [94mLoss[0m : 2.54968
[1mStep[0m  [12/21], [94mLoss[0m : 2.37247
[1mStep[0m  [14/21], [94mLoss[0m : 2.39484
[1mStep[0m  [16/21], [94mLoss[0m : 2.33572
[1mStep[0m  [18/21], [94mLoss[0m : 2.46035
[1mStep[0m  [20/21], [94mLoss[0m : 2.23814

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39450
[1mStep[0m  [2/21], [94mLoss[0m : 2.40361
[1mStep[0m  [4/21], [94mLoss[0m : 2.55683
[1mStep[0m  [6/21], [94mLoss[0m : 2.39334
[1mStep[0m  [8/21], [94mLoss[0m : 2.27323
[1mStep[0m  [10/21], [94mLoss[0m : 2.38410
[1mStep[0m  [12/21], [94mLoss[0m : 2.53142
[1mStep[0m  [14/21], [94mLoss[0m : 2.51589
[1mStep[0m  [16/21], [94mLoss[0m : 2.47414
[1mStep[0m  [18/21], [94mLoss[0m : 2.48047
[1mStep[0m  [20/21], [94mLoss[0m : 2.31673

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47882
[1mStep[0m  [2/21], [94mLoss[0m : 2.49744
[1mStep[0m  [4/21], [94mLoss[0m : 2.46267
[1mStep[0m  [6/21], [94mLoss[0m : 2.43333
[1mStep[0m  [8/21], [94mLoss[0m : 2.40997
[1mStep[0m  [10/21], [94mLoss[0m : 2.40039
[1mStep[0m  [12/21], [94mLoss[0m : 2.26037
[1mStep[0m  [14/21], [94mLoss[0m : 2.52435
[1mStep[0m  [16/21], [94mLoss[0m : 2.43700
[1mStep[0m  [18/21], [94mLoss[0m : 2.44454
[1mStep[0m  [20/21], [94mLoss[0m : 2.43232

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42561
[1mStep[0m  [2/21], [94mLoss[0m : 2.41589
[1mStep[0m  [4/21], [94mLoss[0m : 2.38807
[1mStep[0m  [6/21], [94mLoss[0m : 2.43674
[1mStep[0m  [8/21], [94mLoss[0m : 2.34342
[1mStep[0m  [10/21], [94mLoss[0m : 2.38490
[1mStep[0m  [12/21], [94mLoss[0m : 2.45591
[1mStep[0m  [14/21], [94mLoss[0m : 2.42617
[1mStep[0m  [16/21], [94mLoss[0m : 2.42164
[1mStep[0m  [18/21], [94mLoss[0m : 2.41741
[1mStep[0m  [20/21], [94mLoss[0m : 2.45652

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40939
[1mStep[0m  [2/21], [94mLoss[0m : 2.39193
[1mStep[0m  [4/21], [94mLoss[0m : 2.30902
[1mStep[0m  [6/21], [94mLoss[0m : 2.43270
[1mStep[0m  [8/21], [94mLoss[0m : 2.47269
[1mStep[0m  [10/21], [94mLoss[0m : 2.39971
[1mStep[0m  [12/21], [94mLoss[0m : 2.37860
[1mStep[0m  [14/21], [94mLoss[0m : 2.30545
[1mStep[0m  [16/21], [94mLoss[0m : 2.59807
[1mStep[0m  [18/21], [94mLoss[0m : 2.40241
[1mStep[0m  [20/21], [94mLoss[0m : 2.52344

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44161
[1mStep[0m  [2/21], [94mLoss[0m : 2.36962
[1mStep[0m  [4/21], [94mLoss[0m : 2.39670
[1mStep[0m  [6/21], [94mLoss[0m : 2.61902
[1mStep[0m  [8/21], [94mLoss[0m : 2.27134
[1mStep[0m  [10/21], [94mLoss[0m : 2.39233
[1mStep[0m  [12/21], [94mLoss[0m : 2.49839
[1mStep[0m  [14/21], [94mLoss[0m : 2.41068
[1mStep[0m  [16/21], [94mLoss[0m : 2.52481
[1mStep[0m  [18/21], [94mLoss[0m : 2.32622
[1mStep[0m  [20/21], [94mLoss[0m : 2.51350

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.337470701762608
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.42785
[1mStep[0m  [2/21], [94mLoss[0m : 2.40832
[1mStep[0m  [4/21], [94mLoss[0m : 2.43875
[1mStep[0m  [6/21], [94mLoss[0m : 2.42719
[1mStep[0m  [8/21], [94mLoss[0m : 2.34173
[1mStep[0m  [10/21], [94mLoss[0m : 2.55102
[1mStep[0m  [12/21], [94mLoss[0m : 2.37158
[1mStep[0m  [14/21], [94mLoss[0m : 2.48052
[1mStep[0m  [16/21], [94mLoss[0m : 2.32530
[1mStep[0m  [18/21], [94mLoss[0m : 2.36761
[1mStep[0m  [20/21], [94mLoss[0m : 2.32754

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49446
[1mStep[0m  [2/21], [94mLoss[0m : 2.36975
[1mStep[0m  [4/21], [94mLoss[0m : 2.55793
[1mStep[0m  [6/21], [94mLoss[0m : 2.30370
[1mStep[0m  [8/21], [94mLoss[0m : 2.40407
[1mStep[0m  [10/21], [94mLoss[0m : 2.50073
[1mStep[0m  [12/21], [94mLoss[0m : 2.24550
[1mStep[0m  [14/21], [94mLoss[0m : 2.34188
[1mStep[0m  [16/21], [94mLoss[0m : 2.50199
[1mStep[0m  [18/21], [94mLoss[0m : 2.38025
[1mStep[0m  [20/21], [94mLoss[0m : 2.19307

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42002
[1mStep[0m  [2/21], [94mLoss[0m : 2.41339
[1mStep[0m  [4/21], [94mLoss[0m : 2.23692
[1mStep[0m  [6/21], [94mLoss[0m : 2.33304
[1mStep[0m  [8/21], [94mLoss[0m : 2.45030
[1mStep[0m  [10/21], [94mLoss[0m : 2.31960
[1mStep[0m  [12/21], [94mLoss[0m : 2.22249
[1mStep[0m  [14/21], [94mLoss[0m : 2.29486
[1mStep[0m  [16/21], [94mLoss[0m : 2.47751
[1mStep[0m  [18/21], [94mLoss[0m : 2.46213
[1mStep[0m  [20/21], [94mLoss[0m : 2.48793

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48544
[1mStep[0m  [2/21], [94mLoss[0m : 2.33387
[1mStep[0m  [4/21], [94mLoss[0m : 2.36497
[1mStep[0m  [6/21], [94mLoss[0m : 2.47283
[1mStep[0m  [8/21], [94mLoss[0m : 2.44276
[1mStep[0m  [10/21], [94mLoss[0m : 2.29820
[1mStep[0m  [12/21], [94mLoss[0m : 2.42993
[1mStep[0m  [14/21], [94mLoss[0m : 2.42047
[1mStep[0m  [16/21], [94mLoss[0m : 2.27036
[1mStep[0m  [18/21], [94mLoss[0m : 2.25437
[1mStep[0m  [20/21], [94mLoss[0m : 2.28140

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28206
[1mStep[0m  [2/21], [94mLoss[0m : 2.41179
[1mStep[0m  [4/21], [94mLoss[0m : 2.37954
[1mStep[0m  [6/21], [94mLoss[0m : 2.26616
[1mStep[0m  [8/21], [94mLoss[0m : 2.42180
[1mStep[0m  [10/21], [94mLoss[0m : 2.42804
[1mStep[0m  [12/21], [94mLoss[0m : 2.37347
[1mStep[0m  [14/21], [94mLoss[0m : 2.51185
[1mStep[0m  [16/21], [94mLoss[0m : 2.35736
[1mStep[0m  [18/21], [94mLoss[0m : 2.30193
[1mStep[0m  [20/21], [94mLoss[0m : 2.32167

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48150
[1mStep[0m  [2/21], [94mLoss[0m : 2.39471
[1mStep[0m  [4/21], [94mLoss[0m : 2.38078
[1mStep[0m  [6/21], [94mLoss[0m : 2.43031
[1mStep[0m  [8/21], [94mLoss[0m : 2.28626
[1mStep[0m  [10/21], [94mLoss[0m : 2.45365
[1mStep[0m  [12/21], [94mLoss[0m : 2.48169
[1mStep[0m  [14/21], [94mLoss[0m : 2.37354
[1mStep[0m  [16/21], [94mLoss[0m : 2.30040
[1mStep[0m  [18/21], [94mLoss[0m : 2.28001
[1mStep[0m  [20/21], [94mLoss[0m : 2.49829

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35055
[1mStep[0m  [2/21], [94mLoss[0m : 2.40697
[1mStep[0m  [4/21], [94mLoss[0m : 2.31302
[1mStep[0m  [6/21], [94mLoss[0m : 2.39501
[1mStep[0m  [8/21], [94mLoss[0m : 2.43239
[1mStep[0m  [10/21], [94mLoss[0m : 2.27419
[1mStep[0m  [12/21], [94mLoss[0m : 2.22870
[1mStep[0m  [14/21], [94mLoss[0m : 2.39980
[1mStep[0m  [16/21], [94mLoss[0m : 2.38134
[1mStep[0m  [18/21], [94mLoss[0m : 2.33787
[1mStep[0m  [20/21], [94mLoss[0m : 2.35608

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20762
[1mStep[0m  [2/21], [94mLoss[0m : 2.40192
[1mStep[0m  [4/21], [94mLoss[0m : 2.35918
[1mStep[0m  [6/21], [94mLoss[0m : 2.32358
[1mStep[0m  [8/21], [94mLoss[0m : 2.46868
[1mStep[0m  [10/21], [94mLoss[0m : 2.30154
[1mStep[0m  [12/21], [94mLoss[0m : 2.28351
[1mStep[0m  [14/21], [94mLoss[0m : 2.40023
[1mStep[0m  [16/21], [94mLoss[0m : 2.29373
[1mStep[0m  [18/21], [94mLoss[0m : 2.47151
[1mStep[0m  [20/21], [94mLoss[0m : 2.21364

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38951
[1mStep[0m  [2/21], [94mLoss[0m : 2.25242
[1mStep[0m  [4/21], [94mLoss[0m : 2.20938
[1mStep[0m  [6/21], [94mLoss[0m : 2.20858
[1mStep[0m  [8/21], [94mLoss[0m : 2.32734
[1mStep[0m  [10/21], [94mLoss[0m : 2.33216
[1mStep[0m  [12/21], [94mLoss[0m : 2.23618
[1mStep[0m  [14/21], [94mLoss[0m : 2.42533
[1mStep[0m  [16/21], [94mLoss[0m : 2.49619
[1mStep[0m  [18/21], [94mLoss[0m : 2.26879
[1mStep[0m  [20/21], [94mLoss[0m : 2.30725

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37870
[1mStep[0m  [2/21], [94mLoss[0m : 2.36497
[1mStep[0m  [4/21], [94mLoss[0m : 2.18615
[1mStep[0m  [6/21], [94mLoss[0m : 2.26603
[1mStep[0m  [8/21], [94mLoss[0m : 2.32578
[1mStep[0m  [10/21], [94mLoss[0m : 2.32176
[1mStep[0m  [12/21], [94mLoss[0m : 2.27918
[1mStep[0m  [14/21], [94mLoss[0m : 2.30337
[1mStep[0m  [16/21], [94mLoss[0m : 2.25200
[1mStep[0m  [18/21], [94mLoss[0m : 2.23666
[1mStep[0m  [20/21], [94mLoss[0m : 2.44026

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26789
[1mStep[0m  [2/21], [94mLoss[0m : 2.28484
[1mStep[0m  [4/21], [94mLoss[0m : 2.41623
[1mStep[0m  [6/21], [94mLoss[0m : 2.27737
[1mStep[0m  [8/21], [94mLoss[0m : 2.35632
[1mStep[0m  [10/21], [94mLoss[0m : 2.30120
[1mStep[0m  [12/21], [94mLoss[0m : 2.25013
[1mStep[0m  [14/21], [94mLoss[0m : 2.31323
[1mStep[0m  [16/21], [94mLoss[0m : 2.08789
[1mStep[0m  [18/21], [94mLoss[0m : 2.17957
[1mStep[0m  [20/21], [94mLoss[0m : 2.20524

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26146
[1mStep[0m  [2/21], [94mLoss[0m : 2.32508
[1mStep[0m  [4/21], [94mLoss[0m : 2.26748
[1mStep[0m  [6/21], [94mLoss[0m : 2.23273
[1mStep[0m  [8/21], [94mLoss[0m : 2.32944
[1mStep[0m  [10/21], [94mLoss[0m : 2.25033
[1mStep[0m  [12/21], [94mLoss[0m : 2.16847
[1mStep[0m  [14/21], [94mLoss[0m : 2.28466
[1mStep[0m  [16/21], [94mLoss[0m : 2.32057
[1mStep[0m  [18/21], [94mLoss[0m : 2.14502
[1mStep[0m  [20/21], [94mLoss[0m : 2.27866

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.21645
[1mStep[0m  [2/21], [94mLoss[0m : 2.26589
[1mStep[0m  [4/21], [94mLoss[0m : 2.19102
[1mStep[0m  [6/21], [94mLoss[0m : 2.20314
[1mStep[0m  [8/21], [94mLoss[0m : 2.12114
[1mStep[0m  [10/21], [94mLoss[0m : 2.38745
[1mStep[0m  [12/21], [94mLoss[0m : 2.37641
[1mStep[0m  [14/21], [94mLoss[0m : 2.19230
[1mStep[0m  [16/21], [94mLoss[0m : 2.14254
[1mStep[0m  [18/21], [94mLoss[0m : 2.20841
[1mStep[0m  [20/21], [94mLoss[0m : 2.26367

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20024
[1mStep[0m  [2/21], [94mLoss[0m : 2.29919
[1mStep[0m  [4/21], [94mLoss[0m : 2.24783
[1mStep[0m  [6/21], [94mLoss[0m : 2.20719
[1mStep[0m  [8/21], [94mLoss[0m : 1.95090
[1mStep[0m  [10/21], [94mLoss[0m : 2.15989
[1mStep[0m  [12/21], [94mLoss[0m : 2.26959
[1mStep[0m  [14/21], [94mLoss[0m : 2.27241
[1mStep[0m  [16/21], [94mLoss[0m : 2.11563
[1mStep[0m  [18/21], [94mLoss[0m : 2.36409
[1mStep[0m  [20/21], [94mLoss[0m : 2.31272

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26704
[1mStep[0m  [2/21], [94mLoss[0m : 2.08194
[1mStep[0m  [4/21], [94mLoss[0m : 2.24822
[1mStep[0m  [6/21], [94mLoss[0m : 2.24060
[1mStep[0m  [8/21], [94mLoss[0m : 2.08976
[1mStep[0m  [10/21], [94mLoss[0m : 2.23924
[1mStep[0m  [12/21], [94mLoss[0m : 2.31148
[1mStep[0m  [14/21], [94mLoss[0m : 2.06924
[1mStep[0m  [16/21], [94mLoss[0m : 2.21738
[1mStep[0m  [18/21], [94mLoss[0m : 2.14679
[1mStep[0m  [20/21], [94mLoss[0m : 2.18355

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18717
[1mStep[0m  [2/21], [94mLoss[0m : 2.12830
[1mStep[0m  [4/21], [94mLoss[0m : 2.10220
[1mStep[0m  [6/21], [94mLoss[0m : 2.13263
[1mStep[0m  [8/21], [94mLoss[0m : 2.30373
[1mStep[0m  [10/21], [94mLoss[0m : 2.13986
[1mStep[0m  [12/21], [94mLoss[0m : 2.22223
[1mStep[0m  [14/21], [94mLoss[0m : 2.17100
[1mStep[0m  [16/21], [94mLoss[0m : 2.12929
[1mStep[0m  [18/21], [94mLoss[0m : 2.11718
[1mStep[0m  [20/21], [94mLoss[0m : 2.22526

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.547, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10245
[1mStep[0m  [2/21], [94mLoss[0m : 2.19433
[1mStep[0m  [4/21], [94mLoss[0m : 2.20026
[1mStep[0m  [6/21], [94mLoss[0m : 2.13419
[1mStep[0m  [8/21], [94mLoss[0m : 2.07323
[1mStep[0m  [10/21], [94mLoss[0m : 2.20514
[1mStep[0m  [12/21], [94mLoss[0m : 2.26000
[1mStep[0m  [14/21], [94mLoss[0m : 2.18581
[1mStep[0m  [16/21], [94mLoss[0m : 2.25749
[1mStep[0m  [18/21], [94mLoss[0m : 2.18604
[1mStep[0m  [20/21], [94mLoss[0m : 2.19932

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16881
[1mStep[0m  [2/21], [94mLoss[0m : 2.06252
[1mStep[0m  [4/21], [94mLoss[0m : 2.07345
[1mStep[0m  [6/21], [94mLoss[0m : 2.19765
[1mStep[0m  [8/21], [94mLoss[0m : 2.18483
[1mStep[0m  [10/21], [94mLoss[0m : 1.92723
[1mStep[0m  [12/21], [94mLoss[0m : 2.25933
[1mStep[0m  [14/21], [94mLoss[0m : 2.12891
[1mStep[0m  [16/21], [94mLoss[0m : 2.07755
[1mStep[0m  [18/21], [94mLoss[0m : 2.07040
[1mStep[0m  [20/21], [94mLoss[0m : 2.08863

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02978
[1mStep[0m  [2/21], [94mLoss[0m : 2.11486
[1mStep[0m  [4/21], [94mLoss[0m : 2.11730
[1mStep[0m  [6/21], [94mLoss[0m : 2.11211
[1mStep[0m  [8/21], [94mLoss[0m : 2.04701
[1mStep[0m  [10/21], [94mLoss[0m : 1.97042
[1mStep[0m  [12/21], [94mLoss[0m : 2.16211
[1mStep[0m  [14/21], [94mLoss[0m : 2.05925
[1mStep[0m  [16/21], [94mLoss[0m : 2.19568
[1mStep[0m  [18/21], [94mLoss[0m : 2.03387
[1mStep[0m  [20/21], [94mLoss[0m : 2.18869

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12353
[1mStep[0m  [2/21], [94mLoss[0m : 2.10864
[1mStep[0m  [4/21], [94mLoss[0m : 2.16280
[1mStep[0m  [6/21], [94mLoss[0m : 2.09894
[1mStep[0m  [8/21], [94mLoss[0m : 2.18546
[1mStep[0m  [10/21], [94mLoss[0m : 2.08988
[1mStep[0m  [12/21], [94mLoss[0m : 2.16645
[1mStep[0m  [14/21], [94mLoss[0m : 2.02024
[1mStep[0m  [16/21], [94mLoss[0m : 1.97818
[1mStep[0m  [18/21], [94mLoss[0m : 2.02930
[1mStep[0m  [20/21], [94mLoss[0m : 2.06235

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.03063
[1mStep[0m  [2/21], [94mLoss[0m : 2.12851
[1mStep[0m  [4/21], [94mLoss[0m : 1.96014
[1mStep[0m  [6/21], [94mLoss[0m : 2.12494
[1mStep[0m  [8/21], [94mLoss[0m : 2.15701
[1mStep[0m  [10/21], [94mLoss[0m : 2.12587
[1mStep[0m  [12/21], [94mLoss[0m : 2.03702
[1mStep[0m  [14/21], [94mLoss[0m : 1.96062
[1mStep[0m  [16/21], [94mLoss[0m : 1.93131
[1mStep[0m  [18/21], [94mLoss[0m : 1.99245
[1mStep[0m  [20/21], [94mLoss[0m : 2.23580

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.420, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92251
[1mStep[0m  [2/21], [94mLoss[0m : 2.05380
[1mStep[0m  [4/21], [94mLoss[0m : 2.04423
[1mStep[0m  [6/21], [94mLoss[0m : 2.01817
[1mStep[0m  [8/21], [94mLoss[0m : 2.05924
[1mStep[0m  [10/21], [94mLoss[0m : 1.98417
[1mStep[0m  [12/21], [94mLoss[0m : 2.06610
[1mStep[0m  [14/21], [94mLoss[0m : 2.05109
[1mStep[0m  [16/21], [94mLoss[0m : 2.01949
[1mStep[0m  [18/21], [94mLoss[0m : 2.09114
[1mStep[0m  [20/21], [94mLoss[0m : 2.09689

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88487
[1mStep[0m  [2/21], [94mLoss[0m : 1.89799
[1mStep[0m  [4/21], [94mLoss[0m : 2.02083
[1mStep[0m  [6/21], [94mLoss[0m : 1.96905
[1mStep[0m  [8/21], [94mLoss[0m : 1.99601
[1mStep[0m  [10/21], [94mLoss[0m : 2.09369
[1mStep[0m  [12/21], [94mLoss[0m : 2.20444
[1mStep[0m  [14/21], [94mLoss[0m : 2.05153
[1mStep[0m  [16/21], [94mLoss[0m : 1.96601
[1mStep[0m  [18/21], [94mLoss[0m : 2.16130
[1mStep[0m  [20/21], [94mLoss[0m : 1.93433

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97453
[1mStep[0m  [2/21], [94mLoss[0m : 2.04795
[1mStep[0m  [4/21], [94mLoss[0m : 1.99128
[1mStep[0m  [6/21], [94mLoss[0m : 2.01821
[1mStep[0m  [8/21], [94mLoss[0m : 1.96070
[1mStep[0m  [10/21], [94mLoss[0m : 2.01159
[1mStep[0m  [12/21], [94mLoss[0m : 1.89347
[1mStep[0m  [14/21], [94mLoss[0m : 2.03061
[1mStep[0m  [16/21], [94mLoss[0m : 1.89971
[1mStep[0m  [18/21], [94mLoss[0m : 1.86673
[1mStep[0m  [20/21], [94mLoss[0m : 2.06075

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.416, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01388
[1mStep[0m  [2/21], [94mLoss[0m : 1.99074
[1mStep[0m  [4/21], [94mLoss[0m : 1.95437
[1mStep[0m  [6/21], [94mLoss[0m : 2.04943
[1mStep[0m  [8/21], [94mLoss[0m : 2.00491
[1mStep[0m  [10/21], [94mLoss[0m : 1.99430
[1mStep[0m  [12/21], [94mLoss[0m : 1.99092
[1mStep[0m  [14/21], [94mLoss[0m : 1.89319
[1mStep[0m  [16/21], [94mLoss[0m : 1.94532
[1mStep[0m  [18/21], [94mLoss[0m : 1.83839
[1mStep[0m  [20/21], [94mLoss[0m : 1.90344

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95514
[1mStep[0m  [2/21], [94mLoss[0m : 1.87740
[1mStep[0m  [4/21], [94mLoss[0m : 1.89764
[1mStep[0m  [6/21], [94mLoss[0m : 2.04061
[1mStep[0m  [8/21], [94mLoss[0m : 2.02117
[1mStep[0m  [10/21], [94mLoss[0m : 1.96834
[1mStep[0m  [12/21], [94mLoss[0m : 1.88349
[1mStep[0m  [14/21], [94mLoss[0m : 1.93162
[1mStep[0m  [16/21], [94mLoss[0m : 1.97524
[1mStep[0m  [18/21], [94mLoss[0m : 1.80894
[1mStep[0m  [20/21], [94mLoss[0m : 1.99130

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.942, [92mTest[0m: 2.457, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.80282
[1mStep[0m  [2/21], [94mLoss[0m : 1.94382
[1mStep[0m  [4/21], [94mLoss[0m : 1.97315
[1mStep[0m  [6/21], [94mLoss[0m : 1.86231
[1mStep[0m  [8/21], [94mLoss[0m : 1.84914
[1mStep[0m  [10/21], [94mLoss[0m : 1.93364
[1mStep[0m  [12/21], [94mLoss[0m : 1.82872
[1mStep[0m  [14/21], [94mLoss[0m : 2.02837
[1mStep[0m  [16/21], [94mLoss[0m : 2.00969
[1mStep[0m  [18/21], [94mLoss[0m : 1.92365
[1mStep[0m  [20/21], [94mLoss[0m : 1.85162

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96120
[1mStep[0m  [2/21], [94mLoss[0m : 1.91997
[1mStep[0m  [4/21], [94mLoss[0m : 1.89668
[1mStep[0m  [6/21], [94mLoss[0m : 1.79796
[1mStep[0m  [8/21], [94mLoss[0m : 1.86817
[1mStep[0m  [10/21], [94mLoss[0m : 1.92912
[1mStep[0m  [12/21], [94mLoss[0m : 1.82877
[1mStep[0m  [14/21], [94mLoss[0m : 1.91304
[1mStep[0m  [16/21], [94mLoss[0m : 1.85269
[1mStep[0m  [18/21], [94mLoss[0m : 1.91549
[1mStep[0m  [20/21], [94mLoss[0m : 1.91084

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77608
[1mStep[0m  [2/21], [94mLoss[0m : 1.88006
[1mStep[0m  [4/21], [94mLoss[0m : 1.77561
[1mStep[0m  [6/21], [94mLoss[0m : 1.89326
[1mStep[0m  [8/21], [94mLoss[0m : 1.72757
[1mStep[0m  [10/21], [94mLoss[0m : 1.86034
[1mStep[0m  [12/21], [94mLoss[0m : 1.88587
[1mStep[0m  [14/21], [94mLoss[0m : 1.83951
[1mStep[0m  [16/21], [94mLoss[0m : 1.78205
[1mStep[0m  [18/21], [94mLoss[0m : 1.88936
[1mStep[0m  [20/21], [94mLoss[0m : 1.83955

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90908
[1mStep[0m  [2/21], [94mLoss[0m : 1.85678
[1mStep[0m  [4/21], [94mLoss[0m : 1.79010
[1mStep[0m  [6/21], [94mLoss[0m : 1.82975
[1mStep[0m  [8/21], [94mLoss[0m : 1.78990
[1mStep[0m  [10/21], [94mLoss[0m : 1.84152
[1mStep[0m  [12/21], [94mLoss[0m : 1.82724
[1mStep[0m  [14/21], [94mLoss[0m : 1.73157
[1mStep[0m  [16/21], [94mLoss[0m : 1.79239
[1mStep[0m  [18/21], [94mLoss[0m : 1.84230
[1mStep[0m  [20/21], [94mLoss[0m : 1.76291

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.435, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.527
====================================

Phase 2 - Evaluation MAE:  2.5273423194885254
MAE score P1      2.337471
MAE score P2      2.527342
loss              1.819196
learning_rate         0.01
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 10, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.92410
[1mStep[0m  [2/21], [94mLoss[0m : 10.42438
[1mStep[0m  [4/21], [94mLoss[0m : 9.85660
[1mStep[0m  [6/21], [94mLoss[0m : 9.67869
[1mStep[0m  [8/21], [94mLoss[0m : 9.56914
[1mStep[0m  [10/21], [94mLoss[0m : 9.23793
[1mStep[0m  [12/21], [94mLoss[0m : 8.95361
[1mStep[0m  [14/21], [94mLoss[0m : 8.24666
[1mStep[0m  [16/21], [94mLoss[0m : 8.23728
[1mStep[0m  [18/21], [94mLoss[0m : 8.06129
[1mStep[0m  [20/21], [94mLoss[0m : 7.35531

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.131, [92mTest[0m: 10.865, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.19788
[1mStep[0m  [2/21], [94mLoss[0m : 6.81736
[1mStep[0m  [4/21], [94mLoss[0m : 6.53510
[1mStep[0m  [6/21], [94mLoss[0m : 6.07057
[1mStep[0m  [8/21], [94mLoss[0m : 5.70740
[1mStep[0m  [10/21], [94mLoss[0m : 5.37506
[1mStep[0m  [12/21], [94mLoss[0m : 5.18831
[1mStep[0m  [14/21], [94mLoss[0m : 4.97734
[1mStep[0m  [16/21], [94mLoss[0m : 4.70218
[1mStep[0m  [18/21], [94mLoss[0m : 4.11030
[1mStep[0m  [20/21], [94mLoss[0m : 3.90122

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.520, [92mTest[0m: 8.674, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.91584
[1mStep[0m  [2/21], [94mLoss[0m : 3.72553
[1mStep[0m  [4/21], [94mLoss[0m : 3.66378
[1mStep[0m  [6/21], [94mLoss[0m : 3.44739
[1mStep[0m  [8/21], [94mLoss[0m : 3.43612
[1mStep[0m  [10/21], [94mLoss[0m : 2.95278
[1mStep[0m  [12/21], [94mLoss[0m : 3.22932
[1mStep[0m  [14/21], [94mLoss[0m : 3.01803
[1mStep[0m  [16/21], [94mLoss[0m : 2.96921
[1mStep[0m  [18/21], [94mLoss[0m : 2.84864
[1mStep[0m  [20/21], [94mLoss[0m : 2.82607

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.249, [92mTest[0m: 5.128, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.94911
[1mStep[0m  [2/21], [94mLoss[0m : 2.93273
[1mStep[0m  [4/21], [94mLoss[0m : 2.82993
[1mStep[0m  [6/21], [94mLoss[0m : 2.69621
[1mStep[0m  [8/21], [94mLoss[0m : 2.65947
[1mStep[0m  [10/21], [94mLoss[0m : 2.71506
[1mStep[0m  [12/21], [94mLoss[0m : 2.71226
[1mStep[0m  [14/21], [94mLoss[0m : 2.76302
[1mStep[0m  [16/21], [94mLoss[0m : 2.62006
[1mStep[0m  [18/21], [94mLoss[0m : 2.42777
[1mStep[0m  [20/21], [94mLoss[0m : 2.53701

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.706, [92mTest[0m: 3.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71096
[1mStep[0m  [2/21], [94mLoss[0m : 2.48991
[1mStep[0m  [4/21], [94mLoss[0m : 2.62687
[1mStep[0m  [6/21], [94mLoss[0m : 2.69480
[1mStep[0m  [8/21], [94mLoss[0m : 2.58507
[1mStep[0m  [10/21], [94mLoss[0m : 2.52095
[1mStep[0m  [12/21], [94mLoss[0m : 2.60341
[1mStep[0m  [14/21], [94mLoss[0m : 2.67897
[1mStep[0m  [16/21], [94mLoss[0m : 2.50923
[1mStep[0m  [18/21], [94mLoss[0m : 2.56396
[1mStep[0m  [20/21], [94mLoss[0m : 2.46368

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.813, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54644
[1mStep[0m  [2/21], [94mLoss[0m : 2.44715
[1mStep[0m  [4/21], [94mLoss[0m : 2.55765
[1mStep[0m  [6/21], [94mLoss[0m : 2.61915
[1mStep[0m  [8/21], [94mLoss[0m : 2.78103
[1mStep[0m  [10/21], [94mLoss[0m : 2.62302
[1mStep[0m  [12/21], [94mLoss[0m : 2.71650
[1mStep[0m  [14/21], [94mLoss[0m : 2.44614
[1mStep[0m  [16/21], [94mLoss[0m : 2.75857
[1mStep[0m  [18/21], [94mLoss[0m : 2.41237
[1mStep[0m  [20/21], [94mLoss[0m : 2.60805

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.663, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51053
[1mStep[0m  [2/21], [94mLoss[0m : 2.50719
[1mStep[0m  [4/21], [94mLoss[0m : 2.49169
[1mStep[0m  [6/21], [94mLoss[0m : 2.52624
[1mStep[0m  [8/21], [94mLoss[0m : 2.44856
[1mStep[0m  [10/21], [94mLoss[0m : 2.52489
[1mStep[0m  [12/21], [94mLoss[0m : 2.51277
[1mStep[0m  [14/21], [94mLoss[0m : 2.56865
[1mStep[0m  [16/21], [94mLoss[0m : 2.46328
[1mStep[0m  [18/21], [94mLoss[0m : 2.48974
[1mStep[0m  [20/21], [94mLoss[0m : 2.58727

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.629, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54067
[1mStep[0m  [2/21], [94mLoss[0m : 2.61317
[1mStep[0m  [4/21], [94mLoss[0m : 2.56026
[1mStep[0m  [6/21], [94mLoss[0m : 2.52704
[1mStep[0m  [8/21], [94mLoss[0m : 2.55081
[1mStep[0m  [10/21], [94mLoss[0m : 2.41396
[1mStep[0m  [12/21], [94mLoss[0m : 2.64141
[1mStep[0m  [14/21], [94mLoss[0m : 2.53179
[1mStep[0m  [16/21], [94mLoss[0m : 2.48637
[1mStep[0m  [18/21], [94mLoss[0m : 2.45367
[1mStep[0m  [20/21], [94mLoss[0m : 2.46309

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.570, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49506
[1mStep[0m  [2/21], [94mLoss[0m : 2.65402
[1mStep[0m  [4/21], [94mLoss[0m : 2.40858
[1mStep[0m  [6/21], [94mLoss[0m : 2.48553
[1mStep[0m  [8/21], [94mLoss[0m : 2.61499
[1mStep[0m  [10/21], [94mLoss[0m : 2.60415
[1mStep[0m  [12/21], [94mLoss[0m : 2.43269
[1mStep[0m  [14/21], [94mLoss[0m : 2.46110
[1mStep[0m  [16/21], [94mLoss[0m : 2.50409
[1mStep[0m  [18/21], [94mLoss[0m : 2.50460
[1mStep[0m  [20/21], [94mLoss[0m : 2.50504

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.562, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41658
[1mStep[0m  [2/21], [94mLoss[0m : 2.55954
[1mStep[0m  [4/21], [94mLoss[0m : 2.56899
[1mStep[0m  [6/21], [94mLoss[0m : 2.48892
[1mStep[0m  [8/21], [94mLoss[0m : 2.44260
[1mStep[0m  [10/21], [94mLoss[0m : 2.56795
[1mStep[0m  [12/21], [94mLoss[0m : 2.63008
[1mStep[0m  [14/21], [94mLoss[0m : 2.59673
[1mStep[0m  [16/21], [94mLoss[0m : 2.50055
[1mStep[0m  [18/21], [94mLoss[0m : 2.63245
[1mStep[0m  [20/21], [94mLoss[0m : 2.43938

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44073
[1mStep[0m  [2/21], [94mLoss[0m : 2.49130
[1mStep[0m  [4/21], [94mLoss[0m : 2.36350
[1mStep[0m  [6/21], [94mLoss[0m : 2.56800
[1mStep[0m  [8/21], [94mLoss[0m : 2.60749
[1mStep[0m  [10/21], [94mLoss[0m : 2.52126
[1mStep[0m  [12/21], [94mLoss[0m : 2.46060
[1mStep[0m  [14/21], [94mLoss[0m : 2.64084
[1mStep[0m  [16/21], [94mLoss[0m : 2.50791
[1mStep[0m  [18/21], [94mLoss[0m : 2.52125
[1mStep[0m  [20/21], [94mLoss[0m : 2.45730

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56987
[1mStep[0m  [2/21], [94mLoss[0m : 2.61816
[1mStep[0m  [4/21], [94mLoss[0m : 2.41405
[1mStep[0m  [6/21], [94mLoss[0m : 2.48937
[1mStep[0m  [8/21], [94mLoss[0m : 2.58861
[1mStep[0m  [10/21], [94mLoss[0m : 2.45600
[1mStep[0m  [12/21], [94mLoss[0m : 2.38957
[1mStep[0m  [14/21], [94mLoss[0m : 2.48983
[1mStep[0m  [16/21], [94mLoss[0m : 2.53330
[1mStep[0m  [18/21], [94mLoss[0m : 2.55254
[1mStep[0m  [20/21], [94mLoss[0m : 2.58527

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55375
[1mStep[0m  [2/21], [94mLoss[0m : 2.53753
[1mStep[0m  [4/21], [94mLoss[0m : 2.54327
[1mStep[0m  [6/21], [94mLoss[0m : 2.48546
[1mStep[0m  [8/21], [94mLoss[0m : 2.48900
[1mStep[0m  [10/21], [94mLoss[0m : 2.44748
[1mStep[0m  [12/21], [94mLoss[0m : 2.63473
[1mStep[0m  [14/21], [94mLoss[0m : 2.40327
[1mStep[0m  [16/21], [94mLoss[0m : 2.42797
[1mStep[0m  [18/21], [94mLoss[0m : 2.41012
[1mStep[0m  [20/21], [94mLoss[0m : 2.46272

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44450
[1mStep[0m  [2/21], [94mLoss[0m : 2.43965
[1mStep[0m  [4/21], [94mLoss[0m : 2.27189
[1mStep[0m  [6/21], [94mLoss[0m : 2.41063
[1mStep[0m  [8/21], [94mLoss[0m : 2.47835
[1mStep[0m  [10/21], [94mLoss[0m : 2.48765
[1mStep[0m  [12/21], [94mLoss[0m : 2.47500
[1mStep[0m  [14/21], [94mLoss[0m : 2.49334
[1mStep[0m  [16/21], [94mLoss[0m : 2.44840
[1mStep[0m  [18/21], [94mLoss[0m : 2.52569
[1mStep[0m  [20/21], [94mLoss[0m : 2.48708

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40762
[1mStep[0m  [2/21], [94mLoss[0m : 2.38486
[1mStep[0m  [4/21], [94mLoss[0m : 2.39898
[1mStep[0m  [6/21], [94mLoss[0m : 2.40579
[1mStep[0m  [8/21], [94mLoss[0m : 2.36996
[1mStep[0m  [10/21], [94mLoss[0m : 2.61229
[1mStep[0m  [12/21], [94mLoss[0m : 2.54247
[1mStep[0m  [14/21], [94mLoss[0m : 2.40026
[1mStep[0m  [16/21], [94mLoss[0m : 2.36012
[1mStep[0m  [18/21], [94mLoss[0m : 2.54374
[1mStep[0m  [20/21], [94mLoss[0m : 2.50271

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65491
[1mStep[0m  [2/21], [94mLoss[0m : 2.48218
[1mStep[0m  [4/21], [94mLoss[0m : 2.38393
[1mStep[0m  [6/21], [94mLoss[0m : 2.37678
[1mStep[0m  [8/21], [94mLoss[0m : 2.61631
[1mStep[0m  [10/21], [94mLoss[0m : 2.48064
[1mStep[0m  [12/21], [94mLoss[0m : 2.53909
[1mStep[0m  [14/21], [94mLoss[0m : 2.50420
[1mStep[0m  [16/21], [94mLoss[0m : 2.47016
[1mStep[0m  [18/21], [94mLoss[0m : 2.43007
[1mStep[0m  [20/21], [94mLoss[0m : 2.43952

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33804
[1mStep[0m  [2/21], [94mLoss[0m : 2.29236
[1mStep[0m  [4/21], [94mLoss[0m : 2.35085
[1mStep[0m  [6/21], [94mLoss[0m : 2.40913
[1mStep[0m  [8/21], [94mLoss[0m : 2.74337
[1mStep[0m  [10/21], [94mLoss[0m : 2.50522
[1mStep[0m  [12/21], [94mLoss[0m : 2.51728
[1mStep[0m  [14/21], [94mLoss[0m : 2.47161
[1mStep[0m  [16/21], [94mLoss[0m : 2.55142
[1mStep[0m  [18/21], [94mLoss[0m : 2.64332
[1mStep[0m  [20/21], [94mLoss[0m : 2.43840

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44382
[1mStep[0m  [2/21], [94mLoss[0m : 2.53478
[1mStep[0m  [4/21], [94mLoss[0m : 2.38077
[1mStep[0m  [6/21], [94mLoss[0m : 2.45890
[1mStep[0m  [8/21], [94mLoss[0m : 2.51270
[1mStep[0m  [10/21], [94mLoss[0m : 2.38594
[1mStep[0m  [12/21], [94mLoss[0m : 2.37700
[1mStep[0m  [14/21], [94mLoss[0m : 2.42789
[1mStep[0m  [16/21], [94mLoss[0m : 2.64063
[1mStep[0m  [18/21], [94mLoss[0m : 2.44770
[1mStep[0m  [20/21], [94mLoss[0m : 2.41044

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34678
[1mStep[0m  [2/21], [94mLoss[0m : 2.38111
[1mStep[0m  [4/21], [94mLoss[0m : 2.41629
[1mStep[0m  [6/21], [94mLoss[0m : 2.29061
[1mStep[0m  [8/21], [94mLoss[0m : 2.42916
[1mStep[0m  [10/21], [94mLoss[0m : 2.36097
[1mStep[0m  [12/21], [94mLoss[0m : 2.43819
[1mStep[0m  [14/21], [94mLoss[0m : 2.51653
[1mStep[0m  [16/21], [94mLoss[0m : 2.33314
[1mStep[0m  [18/21], [94mLoss[0m : 2.52323
[1mStep[0m  [20/21], [94mLoss[0m : 2.55234

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44001
[1mStep[0m  [2/21], [94mLoss[0m : 2.44417
[1mStep[0m  [4/21], [94mLoss[0m : 2.40182
[1mStep[0m  [6/21], [94mLoss[0m : 2.56819
[1mStep[0m  [8/21], [94mLoss[0m : 2.25285
[1mStep[0m  [10/21], [94mLoss[0m : 2.63568
[1mStep[0m  [12/21], [94mLoss[0m : 2.45026
[1mStep[0m  [14/21], [94mLoss[0m : 2.39144
[1mStep[0m  [16/21], [94mLoss[0m : 2.65323
[1mStep[0m  [18/21], [94mLoss[0m : 2.38506
[1mStep[0m  [20/21], [94mLoss[0m : 2.51434

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.449, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53415
[1mStep[0m  [2/21], [94mLoss[0m : 2.44918
[1mStep[0m  [4/21], [94mLoss[0m : 2.48137
[1mStep[0m  [6/21], [94mLoss[0m : 2.40030
[1mStep[0m  [8/21], [94mLoss[0m : 2.53028
[1mStep[0m  [10/21], [94mLoss[0m : 2.49552
[1mStep[0m  [12/21], [94mLoss[0m : 2.44880
[1mStep[0m  [14/21], [94mLoss[0m : 2.38464
[1mStep[0m  [16/21], [94mLoss[0m : 2.38083
[1mStep[0m  [18/21], [94mLoss[0m : 2.49512
[1mStep[0m  [20/21], [94mLoss[0m : 2.25820

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.443, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34884
[1mStep[0m  [2/21], [94mLoss[0m : 2.32896
[1mStep[0m  [4/21], [94mLoss[0m : 2.28961
[1mStep[0m  [6/21], [94mLoss[0m : 2.38462
[1mStep[0m  [8/21], [94mLoss[0m : 2.43997
[1mStep[0m  [10/21], [94mLoss[0m : 2.41823
[1mStep[0m  [12/21], [94mLoss[0m : 2.40456
[1mStep[0m  [14/21], [94mLoss[0m : 2.51852
[1mStep[0m  [16/21], [94mLoss[0m : 2.56521
[1mStep[0m  [18/21], [94mLoss[0m : 2.52525
[1mStep[0m  [20/21], [94mLoss[0m : 2.41091

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.402, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42508
[1mStep[0m  [2/21], [94mLoss[0m : 2.52254
[1mStep[0m  [4/21], [94mLoss[0m : 2.41118
[1mStep[0m  [6/21], [94mLoss[0m : 2.50373
[1mStep[0m  [8/21], [94mLoss[0m : 2.67464
[1mStep[0m  [10/21], [94mLoss[0m : 2.43346
[1mStep[0m  [12/21], [94mLoss[0m : 2.48689
[1mStep[0m  [14/21], [94mLoss[0m : 2.48851
[1mStep[0m  [16/21], [94mLoss[0m : 2.45711
[1mStep[0m  [18/21], [94mLoss[0m : 2.54884
[1mStep[0m  [20/21], [94mLoss[0m : 2.46869

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.414, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40688
[1mStep[0m  [2/21], [94mLoss[0m : 2.30243
[1mStep[0m  [4/21], [94mLoss[0m : 2.45166
[1mStep[0m  [6/21], [94mLoss[0m : 2.59628
[1mStep[0m  [8/21], [94mLoss[0m : 2.21446
[1mStep[0m  [10/21], [94mLoss[0m : 2.52309
[1mStep[0m  [12/21], [94mLoss[0m : 2.58938
[1mStep[0m  [14/21], [94mLoss[0m : 2.42297
[1mStep[0m  [16/21], [94mLoss[0m : 2.52445
[1mStep[0m  [18/21], [94mLoss[0m : 2.35719
[1mStep[0m  [20/21], [94mLoss[0m : 2.38694

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.433, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63218
[1mStep[0m  [2/21], [94mLoss[0m : 2.41454
[1mStep[0m  [4/21], [94mLoss[0m : 2.56836
[1mStep[0m  [6/21], [94mLoss[0m : 2.41744
[1mStep[0m  [8/21], [94mLoss[0m : 2.44272
[1mStep[0m  [10/21], [94mLoss[0m : 2.37607
[1mStep[0m  [12/21], [94mLoss[0m : 2.29922
[1mStep[0m  [14/21], [94mLoss[0m : 2.64061
[1mStep[0m  [16/21], [94mLoss[0m : 2.37765
[1mStep[0m  [18/21], [94mLoss[0m : 2.46283
[1mStep[0m  [20/21], [94mLoss[0m : 2.41818

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23251
[1mStep[0m  [2/21], [94mLoss[0m : 2.42525
[1mStep[0m  [4/21], [94mLoss[0m : 2.29868
[1mStep[0m  [6/21], [94mLoss[0m : 2.46835
[1mStep[0m  [8/21], [94mLoss[0m : 2.44427
[1mStep[0m  [10/21], [94mLoss[0m : 2.49849
[1mStep[0m  [12/21], [94mLoss[0m : 2.51801
[1mStep[0m  [14/21], [94mLoss[0m : 2.37164
[1mStep[0m  [16/21], [94mLoss[0m : 2.63683
[1mStep[0m  [18/21], [94mLoss[0m : 2.38532
[1mStep[0m  [20/21], [94mLoss[0m : 2.45036

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.409, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47632
[1mStep[0m  [2/21], [94mLoss[0m : 2.39191
[1mStep[0m  [4/21], [94mLoss[0m : 2.47318
[1mStep[0m  [6/21], [94mLoss[0m : 2.42488
[1mStep[0m  [8/21], [94mLoss[0m : 2.44041
[1mStep[0m  [10/21], [94mLoss[0m : 2.53177
[1mStep[0m  [12/21], [94mLoss[0m : 2.30966
[1mStep[0m  [14/21], [94mLoss[0m : 2.41814
[1mStep[0m  [16/21], [94mLoss[0m : 2.48785
[1mStep[0m  [18/21], [94mLoss[0m : 2.46669
[1mStep[0m  [20/21], [94mLoss[0m : 2.38591

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.460, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29540
[1mStep[0m  [2/21], [94mLoss[0m : 2.37358
[1mStep[0m  [4/21], [94mLoss[0m : 2.19698
[1mStep[0m  [6/21], [94mLoss[0m : 2.38400
[1mStep[0m  [8/21], [94mLoss[0m : 2.36684
[1mStep[0m  [10/21], [94mLoss[0m : 2.55250
[1mStep[0m  [12/21], [94mLoss[0m : 2.48122
[1mStep[0m  [14/21], [94mLoss[0m : 2.52537
[1mStep[0m  [16/21], [94mLoss[0m : 2.41802
[1mStep[0m  [18/21], [94mLoss[0m : 2.53434
[1mStep[0m  [20/21], [94mLoss[0m : 2.47197

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26669
[1mStep[0m  [2/21], [94mLoss[0m : 2.66657
[1mStep[0m  [4/21], [94mLoss[0m : 2.36966
[1mStep[0m  [6/21], [94mLoss[0m : 2.42886
[1mStep[0m  [8/21], [94mLoss[0m : 2.45415
[1mStep[0m  [10/21], [94mLoss[0m : 2.44772
[1mStep[0m  [12/21], [94mLoss[0m : 2.39326
[1mStep[0m  [14/21], [94mLoss[0m : 2.40287
[1mStep[0m  [16/21], [94mLoss[0m : 2.26048
[1mStep[0m  [18/21], [94mLoss[0m : 2.39279
[1mStep[0m  [20/21], [94mLoss[0m : 2.48450

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.410, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30586
[1mStep[0m  [2/21], [94mLoss[0m : 2.48581
[1mStep[0m  [4/21], [94mLoss[0m : 2.36702
[1mStep[0m  [6/21], [94mLoss[0m : 2.46961
[1mStep[0m  [8/21], [94mLoss[0m : 2.47638
[1mStep[0m  [10/21], [94mLoss[0m : 2.46498
[1mStep[0m  [12/21], [94mLoss[0m : 2.40649
[1mStep[0m  [14/21], [94mLoss[0m : 2.28881
[1mStep[0m  [16/21], [94mLoss[0m : 2.45707
[1mStep[0m  [18/21], [94mLoss[0m : 2.55013
[1mStep[0m  [20/21], [94mLoss[0m : 2.27959

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.385
====================================

Phase 1 - Evaluation MAE:  2.3853944710322788
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.38131
[1mStep[0m  [2/21], [94mLoss[0m : 2.57453
[1mStep[0m  [4/21], [94mLoss[0m : 2.46060
[1mStep[0m  [6/21], [94mLoss[0m : 2.49211
[1mStep[0m  [8/21], [94mLoss[0m : 2.40498
[1mStep[0m  [10/21], [94mLoss[0m : 2.39757
[1mStep[0m  [12/21], [94mLoss[0m : 2.55258
[1mStep[0m  [14/21], [94mLoss[0m : 2.57939
[1mStep[0m  [16/21], [94mLoss[0m : 2.39087
[1mStep[0m  [18/21], [94mLoss[0m : 2.48386
[1mStep[0m  [20/21], [94mLoss[0m : 2.49549

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47781
[1mStep[0m  [2/21], [94mLoss[0m : 2.51409
[1mStep[0m  [4/21], [94mLoss[0m : 2.42496
[1mStep[0m  [6/21], [94mLoss[0m : 2.40578
[1mStep[0m  [8/21], [94mLoss[0m : 2.39310
[1mStep[0m  [10/21], [94mLoss[0m : 2.53751
[1mStep[0m  [12/21], [94mLoss[0m : 2.49924
[1mStep[0m  [14/21], [94mLoss[0m : 2.30848
[1mStep[0m  [16/21], [94mLoss[0m : 2.38816
[1mStep[0m  [18/21], [94mLoss[0m : 2.50057
[1mStep[0m  [20/21], [94mLoss[0m : 2.54248

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.857, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44805
[1mStep[0m  [2/21], [94mLoss[0m : 2.64148
[1mStep[0m  [4/21], [94mLoss[0m : 2.56854
[1mStep[0m  [6/21], [94mLoss[0m : 2.38304
[1mStep[0m  [8/21], [94mLoss[0m : 2.46279
[1mStep[0m  [10/21], [94mLoss[0m : 2.45253
[1mStep[0m  [12/21], [94mLoss[0m : 2.43284
[1mStep[0m  [14/21], [94mLoss[0m : 2.50707
[1mStep[0m  [16/21], [94mLoss[0m : 2.34573
[1mStep[0m  [18/21], [94mLoss[0m : 2.42977
[1mStep[0m  [20/21], [94mLoss[0m : 2.32839

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60618
[1mStep[0m  [2/21], [94mLoss[0m : 2.54396
[1mStep[0m  [4/21], [94mLoss[0m : 2.34458
[1mStep[0m  [6/21], [94mLoss[0m : 2.38289
[1mStep[0m  [8/21], [94mLoss[0m : 2.31955
[1mStep[0m  [10/21], [94mLoss[0m : 2.46272
[1mStep[0m  [12/21], [94mLoss[0m : 2.38029
[1mStep[0m  [14/21], [94mLoss[0m : 2.38528
[1mStep[0m  [16/21], [94mLoss[0m : 2.46464
[1mStep[0m  [18/21], [94mLoss[0m : 2.28473
[1mStep[0m  [20/21], [94mLoss[0m : 2.29710

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34653
[1mStep[0m  [2/21], [94mLoss[0m : 2.34985
[1mStep[0m  [4/21], [94mLoss[0m : 2.31289
[1mStep[0m  [6/21], [94mLoss[0m : 2.16930
[1mStep[0m  [8/21], [94mLoss[0m : 2.40603
[1mStep[0m  [10/21], [94mLoss[0m : 2.38021
[1mStep[0m  [12/21], [94mLoss[0m : 2.26326
[1mStep[0m  [14/21], [94mLoss[0m : 2.37430
[1mStep[0m  [16/21], [94mLoss[0m : 2.42852
[1mStep[0m  [18/21], [94mLoss[0m : 2.27283
[1mStep[0m  [20/21], [94mLoss[0m : 2.35825

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19961
[1mStep[0m  [2/21], [94mLoss[0m : 2.38394
[1mStep[0m  [4/21], [94mLoss[0m : 2.43313
[1mStep[0m  [6/21], [94mLoss[0m : 2.48887
[1mStep[0m  [8/21], [94mLoss[0m : 2.32340
[1mStep[0m  [10/21], [94mLoss[0m : 2.32113
[1mStep[0m  [12/21], [94mLoss[0m : 2.33922
[1mStep[0m  [14/21], [94mLoss[0m : 2.32049
[1mStep[0m  [16/21], [94mLoss[0m : 2.27509
[1mStep[0m  [18/21], [94mLoss[0m : 2.43720
[1mStep[0m  [20/21], [94mLoss[0m : 2.18441

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30588
[1mStep[0m  [2/21], [94mLoss[0m : 2.20659
[1mStep[0m  [4/21], [94mLoss[0m : 2.24057
[1mStep[0m  [6/21], [94mLoss[0m : 2.27284
[1mStep[0m  [8/21], [94mLoss[0m : 2.27425
[1mStep[0m  [10/21], [94mLoss[0m : 2.45572
[1mStep[0m  [12/21], [94mLoss[0m : 2.15152
[1mStep[0m  [14/21], [94mLoss[0m : 2.34899
[1mStep[0m  [16/21], [94mLoss[0m : 2.27747
[1mStep[0m  [18/21], [94mLoss[0m : 2.17414
[1mStep[0m  [20/21], [94mLoss[0m : 2.30733

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12365
[1mStep[0m  [2/21], [94mLoss[0m : 2.21001
[1mStep[0m  [4/21], [94mLoss[0m : 2.28245
[1mStep[0m  [6/21], [94mLoss[0m : 2.23719
[1mStep[0m  [8/21], [94mLoss[0m : 2.28147
[1mStep[0m  [10/21], [94mLoss[0m : 2.25278
[1mStep[0m  [12/21], [94mLoss[0m : 2.19727
[1mStep[0m  [14/21], [94mLoss[0m : 2.33492
[1mStep[0m  [16/21], [94mLoss[0m : 2.38331
[1mStep[0m  [18/21], [94mLoss[0m : 2.29711
[1mStep[0m  [20/21], [94mLoss[0m : 2.23693

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.729, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35066
[1mStep[0m  [2/21], [94mLoss[0m : 2.13950
[1mStep[0m  [4/21], [94mLoss[0m : 2.21098
[1mStep[0m  [6/21], [94mLoss[0m : 2.13768
[1mStep[0m  [8/21], [94mLoss[0m : 2.15509
[1mStep[0m  [10/21], [94mLoss[0m : 2.28798
[1mStep[0m  [12/21], [94mLoss[0m : 2.27849
[1mStep[0m  [14/21], [94mLoss[0m : 2.24531
[1mStep[0m  [16/21], [94mLoss[0m : 2.43593
[1mStep[0m  [18/21], [94mLoss[0m : 2.18974
[1mStep[0m  [20/21], [94mLoss[0m : 2.17401

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.578, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09650
[1mStep[0m  [2/21], [94mLoss[0m : 2.24319
[1mStep[0m  [4/21], [94mLoss[0m : 2.14287
[1mStep[0m  [6/21], [94mLoss[0m : 2.21271
[1mStep[0m  [8/21], [94mLoss[0m : 2.21828
[1mStep[0m  [10/21], [94mLoss[0m : 2.13779
[1mStep[0m  [12/21], [94mLoss[0m : 2.16065
[1mStep[0m  [14/21], [94mLoss[0m : 2.30748
[1mStep[0m  [16/21], [94mLoss[0m : 2.15484
[1mStep[0m  [18/21], [94mLoss[0m : 2.22154
[1mStep[0m  [20/21], [94mLoss[0m : 2.20091

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18006
[1mStep[0m  [2/21], [94mLoss[0m : 2.20802
[1mStep[0m  [4/21], [94mLoss[0m : 2.17178
[1mStep[0m  [6/21], [94mLoss[0m : 2.14526
[1mStep[0m  [8/21], [94mLoss[0m : 2.22284
[1mStep[0m  [10/21], [94mLoss[0m : 2.19686
[1mStep[0m  [12/21], [94mLoss[0m : 2.38473
[1mStep[0m  [14/21], [94mLoss[0m : 2.13947
[1mStep[0m  [16/21], [94mLoss[0m : 2.18393
[1mStep[0m  [18/21], [94mLoss[0m : 2.08018
[1mStep[0m  [20/21], [94mLoss[0m : 2.09477

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08785
[1mStep[0m  [2/21], [94mLoss[0m : 2.12458
[1mStep[0m  [4/21], [94mLoss[0m : 2.13340
[1mStep[0m  [6/21], [94mLoss[0m : 2.08528
[1mStep[0m  [8/21], [94mLoss[0m : 2.19877
[1mStep[0m  [10/21], [94mLoss[0m : 2.10805
[1mStep[0m  [12/21], [94mLoss[0m : 2.09835
[1mStep[0m  [14/21], [94mLoss[0m : 2.03689
[1mStep[0m  [16/21], [94mLoss[0m : 2.02154
[1mStep[0m  [18/21], [94mLoss[0m : 2.21395
[1mStep[0m  [20/21], [94mLoss[0m : 2.10502

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.626, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07125
[1mStep[0m  [2/21], [94mLoss[0m : 2.08488
[1mStep[0m  [4/21], [94mLoss[0m : 2.06033
[1mStep[0m  [6/21], [94mLoss[0m : 2.16439
[1mStep[0m  [8/21], [94mLoss[0m : 2.15716
[1mStep[0m  [10/21], [94mLoss[0m : 2.03427
[1mStep[0m  [12/21], [94mLoss[0m : 2.18048
[1mStep[0m  [14/21], [94mLoss[0m : 2.20308
[1mStep[0m  [16/21], [94mLoss[0m : 2.30597
[1mStep[0m  [18/21], [94mLoss[0m : 2.07258
[1mStep[0m  [20/21], [94mLoss[0m : 2.16728

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15998
[1mStep[0m  [2/21], [94mLoss[0m : 2.16608
[1mStep[0m  [4/21], [94mLoss[0m : 2.18729
[1mStep[0m  [6/21], [94mLoss[0m : 2.18098
[1mStep[0m  [8/21], [94mLoss[0m : 2.04829
[1mStep[0m  [10/21], [94mLoss[0m : 2.07391
[1mStep[0m  [12/21], [94mLoss[0m : 2.11103
[1mStep[0m  [14/21], [94mLoss[0m : 2.04929
[1mStep[0m  [16/21], [94mLoss[0m : 2.31796
[1mStep[0m  [18/21], [94mLoss[0m : 2.13035
[1mStep[0m  [20/21], [94mLoss[0m : 1.94551

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08590
[1mStep[0m  [2/21], [94mLoss[0m : 1.97541
[1mStep[0m  [4/21], [94mLoss[0m : 2.01452
[1mStep[0m  [6/21], [94mLoss[0m : 2.09701
[1mStep[0m  [8/21], [94mLoss[0m : 1.89379
[1mStep[0m  [10/21], [94mLoss[0m : 2.15787
[1mStep[0m  [12/21], [94mLoss[0m : 2.06727
[1mStep[0m  [14/21], [94mLoss[0m : 2.06057
[1mStep[0m  [16/21], [94mLoss[0m : 1.98999
[1mStep[0m  [18/21], [94mLoss[0m : 2.13444
[1mStep[0m  [20/21], [94mLoss[0m : 2.18974

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.89441
[1mStep[0m  [2/21], [94mLoss[0m : 1.90911
[1mStep[0m  [4/21], [94mLoss[0m : 1.98665
[1mStep[0m  [6/21], [94mLoss[0m : 2.02454
[1mStep[0m  [8/21], [94mLoss[0m : 2.06682
[1mStep[0m  [10/21], [94mLoss[0m : 2.04063
[1mStep[0m  [12/21], [94mLoss[0m : 2.15729
[1mStep[0m  [14/21], [94mLoss[0m : 2.01461
[1mStep[0m  [16/21], [94mLoss[0m : 1.98153
[1mStep[0m  [18/21], [94mLoss[0m : 2.02693
[1mStep[0m  [20/21], [94mLoss[0m : 1.95466

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16645
[1mStep[0m  [2/21], [94mLoss[0m : 2.07921
[1mStep[0m  [4/21], [94mLoss[0m : 2.15215
[1mStep[0m  [6/21], [94mLoss[0m : 2.07769
[1mStep[0m  [8/21], [94mLoss[0m : 2.00848
[1mStep[0m  [10/21], [94mLoss[0m : 1.95978
[1mStep[0m  [12/21], [94mLoss[0m : 1.99055
[1mStep[0m  [14/21], [94mLoss[0m : 2.03169
[1mStep[0m  [16/21], [94mLoss[0m : 2.00957
[1mStep[0m  [18/21], [94mLoss[0m : 1.86640
[1mStep[0m  [20/21], [94mLoss[0m : 2.07403

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92802
[1mStep[0m  [2/21], [94mLoss[0m : 1.87256
[1mStep[0m  [4/21], [94mLoss[0m : 2.06640
[1mStep[0m  [6/21], [94mLoss[0m : 2.10867
[1mStep[0m  [8/21], [94mLoss[0m : 1.95471
[1mStep[0m  [10/21], [94mLoss[0m : 1.95365
[1mStep[0m  [12/21], [94mLoss[0m : 1.84792
[1mStep[0m  [14/21], [94mLoss[0m : 1.85587
[1mStep[0m  [16/21], [94mLoss[0m : 1.97043
[1mStep[0m  [18/21], [94mLoss[0m : 1.96742
[1mStep[0m  [20/21], [94mLoss[0m : 2.00307

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93997
[1mStep[0m  [2/21], [94mLoss[0m : 1.95109
[1mStep[0m  [4/21], [94mLoss[0m : 2.05503
[1mStep[0m  [6/21], [94mLoss[0m : 1.90499
[1mStep[0m  [8/21], [94mLoss[0m : 1.85387
[1mStep[0m  [10/21], [94mLoss[0m : 2.00979
[1mStep[0m  [12/21], [94mLoss[0m : 1.94909
[1mStep[0m  [14/21], [94mLoss[0m : 1.89080
[1mStep[0m  [16/21], [94mLoss[0m : 1.80892
[1mStep[0m  [18/21], [94mLoss[0m : 2.08876
[1mStep[0m  [20/21], [94mLoss[0m : 1.93728

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02310
[1mStep[0m  [2/21], [94mLoss[0m : 2.05569
[1mStep[0m  [4/21], [94mLoss[0m : 1.76709
[1mStep[0m  [6/21], [94mLoss[0m : 1.90683
[1mStep[0m  [8/21], [94mLoss[0m : 1.91647
[1mStep[0m  [10/21], [94mLoss[0m : 1.86940
[1mStep[0m  [12/21], [94mLoss[0m : 1.83117
[1mStep[0m  [14/21], [94mLoss[0m : 1.75109
[1mStep[0m  [16/21], [94mLoss[0m : 1.99606
[1mStep[0m  [18/21], [94mLoss[0m : 1.88801
[1mStep[0m  [20/21], [94mLoss[0m : 1.87872

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.481, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.70859
[1mStep[0m  [2/21], [94mLoss[0m : 1.91727
[1mStep[0m  [4/21], [94mLoss[0m : 1.90316
[1mStep[0m  [6/21], [94mLoss[0m : 1.91807
[1mStep[0m  [8/21], [94mLoss[0m : 1.84905
[1mStep[0m  [10/21], [94mLoss[0m : 1.79705
[1mStep[0m  [12/21], [94mLoss[0m : 1.91106
[1mStep[0m  [14/21], [94mLoss[0m : 1.96577
[1mStep[0m  [16/21], [94mLoss[0m : 1.85641
[1mStep[0m  [18/21], [94mLoss[0m : 1.88407
[1mStep[0m  [20/21], [94mLoss[0m : 1.96333

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91864
[1mStep[0m  [2/21], [94mLoss[0m : 1.85214
[1mStep[0m  [4/21], [94mLoss[0m : 1.90125
[1mStep[0m  [6/21], [94mLoss[0m : 1.86460
[1mStep[0m  [8/21], [94mLoss[0m : 1.82747
[1mStep[0m  [10/21], [94mLoss[0m : 1.97800
[1mStep[0m  [12/21], [94mLoss[0m : 1.85299
[1mStep[0m  [14/21], [94mLoss[0m : 1.89411
[1mStep[0m  [16/21], [94mLoss[0m : 1.90334
[1mStep[0m  [18/21], [94mLoss[0m : 1.85028
[1mStep[0m  [20/21], [94mLoss[0m : 1.77913

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86413
[1mStep[0m  [2/21], [94mLoss[0m : 1.80949
[1mStep[0m  [4/21], [94mLoss[0m : 1.88090
[1mStep[0m  [6/21], [94mLoss[0m : 1.70192
[1mStep[0m  [8/21], [94mLoss[0m : 1.76972
[1mStep[0m  [10/21], [94mLoss[0m : 1.87023
[1mStep[0m  [12/21], [94mLoss[0m : 1.86149
[1mStep[0m  [14/21], [94mLoss[0m : 1.72769
[1mStep[0m  [16/21], [94mLoss[0m : 1.82617
[1mStep[0m  [18/21], [94mLoss[0m : 1.90086
[1mStep[0m  [20/21], [94mLoss[0m : 1.86440

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.502, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.62464
[1mStep[0m  [2/21], [94mLoss[0m : 1.85095
[1mStep[0m  [4/21], [94mLoss[0m : 1.70950
[1mStep[0m  [6/21], [94mLoss[0m : 1.73551
[1mStep[0m  [8/21], [94mLoss[0m : 1.70982
[1mStep[0m  [10/21], [94mLoss[0m : 1.75682
[1mStep[0m  [12/21], [94mLoss[0m : 1.74151
[1mStep[0m  [14/21], [94mLoss[0m : 1.83782
[1mStep[0m  [16/21], [94mLoss[0m : 1.86599
[1mStep[0m  [18/21], [94mLoss[0m : 1.86293
[1mStep[0m  [20/21], [94mLoss[0m : 1.74435

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.449, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.76435
[1mStep[0m  [2/21], [94mLoss[0m : 1.81360
[1mStep[0m  [4/21], [94mLoss[0m : 1.69186
[1mStep[0m  [6/21], [94mLoss[0m : 1.88298
[1mStep[0m  [8/21], [94mLoss[0m : 1.67737
[1mStep[0m  [10/21], [94mLoss[0m : 1.74842
[1mStep[0m  [12/21], [94mLoss[0m : 1.78103
[1mStep[0m  [14/21], [94mLoss[0m : 1.80909
[1mStep[0m  [16/21], [94mLoss[0m : 1.68984
[1mStep[0m  [18/21], [94mLoss[0m : 1.86406
[1mStep[0m  [20/21], [94mLoss[0m : 1.84248

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.500, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.66973
[1mStep[0m  [2/21], [94mLoss[0m : 1.59455
[1mStep[0m  [4/21], [94mLoss[0m : 1.74032
[1mStep[0m  [6/21], [94mLoss[0m : 1.80555
[1mStep[0m  [8/21], [94mLoss[0m : 1.73791
[1mStep[0m  [10/21], [94mLoss[0m : 1.70486
[1mStep[0m  [12/21], [94mLoss[0m : 1.80758
[1mStep[0m  [14/21], [94mLoss[0m : 1.65198
[1mStep[0m  [16/21], [94mLoss[0m : 1.77592
[1mStep[0m  [18/21], [94mLoss[0m : 1.73882
[1mStep[0m  [20/21], [94mLoss[0m : 1.78516

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77170
[1mStep[0m  [2/21], [94mLoss[0m : 1.68520
[1mStep[0m  [4/21], [94mLoss[0m : 1.76055
[1mStep[0m  [6/21], [94mLoss[0m : 1.63999
[1mStep[0m  [8/21], [94mLoss[0m : 1.76506
[1mStep[0m  [10/21], [94mLoss[0m : 1.72154
[1mStep[0m  [12/21], [94mLoss[0m : 1.65423
[1mStep[0m  [14/21], [94mLoss[0m : 1.77365
[1mStep[0m  [16/21], [94mLoss[0m : 1.62191
[1mStep[0m  [18/21], [94mLoss[0m : 1.80293
[1mStep[0m  [20/21], [94mLoss[0m : 1.89611

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.746, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.66984
[1mStep[0m  [2/21], [94mLoss[0m : 1.61662
[1mStep[0m  [4/21], [94mLoss[0m : 1.71351
[1mStep[0m  [6/21], [94mLoss[0m : 1.70532
[1mStep[0m  [8/21], [94mLoss[0m : 1.77453
[1mStep[0m  [10/21], [94mLoss[0m : 1.68513
[1mStep[0m  [12/21], [94mLoss[0m : 1.74288
[1mStep[0m  [14/21], [94mLoss[0m : 1.73564
[1mStep[0m  [16/21], [94mLoss[0m : 1.63908
[1mStep[0m  [18/21], [94mLoss[0m : 1.72173
[1mStep[0m  [20/21], [94mLoss[0m : 1.81671

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.74999
[1mStep[0m  [2/21], [94mLoss[0m : 1.69198
[1mStep[0m  [4/21], [94mLoss[0m : 1.64678
[1mStep[0m  [6/21], [94mLoss[0m : 1.71884
[1mStep[0m  [8/21], [94mLoss[0m : 1.68521
[1mStep[0m  [10/21], [94mLoss[0m : 1.84675
[1mStep[0m  [12/21], [94mLoss[0m : 1.57952
[1mStep[0m  [14/21], [94mLoss[0m : 1.66737
[1mStep[0m  [16/21], [94mLoss[0m : 1.60900
[1mStep[0m  [18/21], [94mLoss[0m : 1.82814
[1mStep[0m  [20/21], [94mLoss[0m : 1.67332

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.64833
[1mStep[0m  [2/21], [94mLoss[0m : 1.51260
[1mStep[0m  [4/21], [94mLoss[0m : 1.63403
[1mStep[0m  [6/21], [94mLoss[0m : 1.65403
[1mStep[0m  [8/21], [94mLoss[0m : 1.75205
[1mStep[0m  [10/21], [94mLoss[0m : 1.79798
[1mStep[0m  [12/21], [94mLoss[0m : 1.70386
[1mStep[0m  [14/21], [94mLoss[0m : 1.60493
[1mStep[0m  [16/21], [94mLoss[0m : 1.62605
[1mStep[0m  [18/21], [94mLoss[0m : 1.68308
[1mStep[0m  [20/21], [94mLoss[0m : 1.67951

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.553
====================================

Phase 2 - Evaluation MAE:  2.553121634892055
MAE score P1      2.385394
MAE score P2      2.553122
loss              1.664561
learning_rate         0.01
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.89221
[1mStep[0m  [2/21], [94mLoss[0m : 10.43650
[1mStep[0m  [4/21], [94mLoss[0m : 9.58842
[1mStep[0m  [6/21], [94mLoss[0m : 8.33699
[1mStep[0m  [8/21], [94mLoss[0m : 6.65219
[1mStep[0m  [10/21], [94mLoss[0m : 4.98600
[1mStep[0m  [12/21], [94mLoss[0m : 3.76435
[1mStep[0m  [14/21], [94mLoss[0m : 3.26272
[1mStep[0m  [16/21], [94mLoss[0m : 2.87372
[1mStep[0m  [18/21], [94mLoss[0m : 2.87313
[1mStep[0m  [20/21], [94mLoss[0m : 3.40403

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.038, [92mTest[0m: 10.830, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.37715
[1mStep[0m  [2/21], [94mLoss[0m : 3.44679
[1mStep[0m  [4/21], [94mLoss[0m : 3.29578
[1mStep[0m  [6/21], [94mLoss[0m : 3.20529
[1mStep[0m  [8/21], [94mLoss[0m : 2.88054
[1mStep[0m  [10/21], [94mLoss[0m : 2.63078
[1mStep[0m  [12/21], [94mLoss[0m : 2.66552
[1mStep[0m  [14/21], [94mLoss[0m : 2.59670
[1mStep[0m  [16/21], [94mLoss[0m : 2.54613
[1mStep[0m  [18/21], [94mLoss[0m : 2.59954
[1mStep[0m  [20/21], [94mLoss[0m : 2.65259

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.905, [92mTest[0m: 3.572, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53781
[1mStep[0m  [2/21], [94mLoss[0m : 2.61744
[1mStep[0m  [4/21], [94mLoss[0m : 2.73606
[1mStep[0m  [6/21], [94mLoss[0m : 2.61407
[1mStep[0m  [8/21], [94mLoss[0m : 2.42694
[1mStep[0m  [10/21], [94mLoss[0m : 2.43722
[1mStep[0m  [12/21], [94mLoss[0m : 2.59809
[1mStep[0m  [14/21], [94mLoss[0m : 2.50633
[1mStep[0m  [16/21], [94mLoss[0m : 2.51724
[1mStep[0m  [18/21], [94mLoss[0m : 2.65876
[1mStep[0m  [20/21], [94mLoss[0m : 2.41078

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50154
[1mStep[0m  [2/21], [94mLoss[0m : 2.62285
[1mStep[0m  [4/21], [94mLoss[0m : 2.56881
[1mStep[0m  [6/21], [94mLoss[0m : 2.43708
[1mStep[0m  [8/21], [94mLoss[0m : 2.40985
[1mStep[0m  [10/21], [94mLoss[0m : 2.57269
[1mStep[0m  [12/21], [94mLoss[0m : 2.42732
[1mStep[0m  [14/21], [94mLoss[0m : 2.58446
[1mStep[0m  [16/21], [94mLoss[0m : 2.60834
[1mStep[0m  [18/21], [94mLoss[0m : 2.59512
[1mStep[0m  [20/21], [94mLoss[0m : 2.38234

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41927
[1mStep[0m  [2/21], [94mLoss[0m : 2.41703
[1mStep[0m  [4/21], [94mLoss[0m : 2.48287
[1mStep[0m  [6/21], [94mLoss[0m : 2.69132
[1mStep[0m  [8/21], [94mLoss[0m : 2.48146
[1mStep[0m  [10/21], [94mLoss[0m : 2.48331
[1mStep[0m  [12/21], [94mLoss[0m : 2.38563
[1mStep[0m  [14/21], [94mLoss[0m : 2.44308
[1mStep[0m  [16/21], [94mLoss[0m : 2.55849
[1mStep[0m  [18/21], [94mLoss[0m : 2.37552
[1mStep[0m  [20/21], [94mLoss[0m : 2.49147

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65820
[1mStep[0m  [2/21], [94mLoss[0m : 2.49848
[1mStep[0m  [4/21], [94mLoss[0m : 2.46760
[1mStep[0m  [6/21], [94mLoss[0m : 2.48152
[1mStep[0m  [8/21], [94mLoss[0m : 2.36033
[1mStep[0m  [10/21], [94mLoss[0m : 2.47884
[1mStep[0m  [12/21], [94mLoss[0m : 2.44609
[1mStep[0m  [14/21], [94mLoss[0m : 2.51652
[1mStep[0m  [16/21], [94mLoss[0m : 2.37908
[1mStep[0m  [18/21], [94mLoss[0m : 2.64010
[1mStep[0m  [20/21], [94mLoss[0m : 2.43488

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54676
[1mStep[0m  [2/21], [94mLoss[0m : 2.39871
[1mStep[0m  [4/21], [94mLoss[0m : 2.52057
[1mStep[0m  [6/21], [94mLoss[0m : 2.43768
[1mStep[0m  [8/21], [94mLoss[0m : 2.49888
[1mStep[0m  [10/21], [94mLoss[0m : 2.45070
[1mStep[0m  [12/21], [94mLoss[0m : 2.56538
[1mStep[0m  [14/21], [94mLoss[0m : 2.46603
[1mStep[0m  [16/21], [94mLoss[0m : 2.41238
[1mStep[0m  [18/21], [94mLoss[0m : 2.47939
[1mStep[0m  [20/21], [94mLoss[0m : 2.43938

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43846
[1mStep[0m  [2/21], [94mLoss[0m : 2.43409
[1mStep[0m  [4/21], [94mLoss[0m : 2.58091
[1mStep[0m  [6/21], [94mLoss[0m : 2.38009
[1mStep[0m  [8/21], [94mLoss[0m : 2.32439
[1mStep[0m  [10/21], [94mLoss[0m : 2.43660
[1mStep[0m  [12/21], [94mLoss[0m : 2.56477
[1mStep[0m  [14/21], [94mLoss[0m : 2.67909
[1mStep[0m  [16/21], [94mLoss[0m : 2.40828
[1mStep[0m  [18/21], [94mLoss[0m : 2.44351
[1mStep[0m  [20/21], [94mLoss[0m : 2.53024

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51733
[1mStep[0m  [2/21], [94mLoss[0m : 2.44370
[1mStep[0m  [4/21], [94mLoss[0m : 2.45834
[1mStep[0m  [6/21], [94mLoss[0m : 2.56327
[1mStep[0m  [8/21], [94mLoss[0m : 2.47729
[1mStep[0m  [10/21], [94mLoss[0m : 2.63385
[1mStep[0m  [12/21], [94mLoss[0m : 2.44242
[1mStep[0m  [14/21], [94mLoss[0m : 2.50558
[1mStep[0m  [16/21], [94mLoss[0m : 2.40569
[1mStep[0m  [18/21], [94mLoss[0m : 2.56119
[1mStep[0m  [20/21], [94mLoss[0m : 2.42450

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51784
[1mStep[0m  [2/21], [94mLoss[0m : 2.48948
[1mStep[0m  [4/21], [94mLoss[0m : 2.56274
[1mStep[0m  [6/21], [94mLoss[0m : 2.43629
[1mStep[0m  [8/21], [94mLoss[0m : 2.36530
[1mStep[0m  [10/21], [94mLoss[0m : 2.46922
[1mStep[0m  [12/21], [94mLoss[0m : 2.52866
[1mStep[0m  [14/21], [94mLoss[0m : 2.60318
[1mStep[0m  [16/21], [94mLoss[0m : 2.55644
[1mStep[0m  [18/21], [94mLoss[0m : 2.51083
[1mStep[0m  [20/21], [94mLoss[0m : 2.41484

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60502
[1mStep[0m  [2/21], [94mLoss[0m : 2.51303
[1mStep[0m  [4/21], [94mLoss[0m : 2.47359
[1mStep[0m  [6/21], [94mLoss[0m : 2.52912
[1mStep[0m  [8/21], [94mLoss[0m : 2.48401
[1mStep[0m  [10/21], [94mLoss[0m : 2.55359
[1mStep[0m  [12/21], [94mLoss[0m : 2.38723
[1mStep[0m  [14/21], [94mLoss[0m : 2.48959
[1mStep[0m  [16/21], [94mLoss[0m : 2.40411
[1mStep[0m  [18/21], [94mLoss[0m : 2.40145
[1mStep[0m  [20/21], [94mLoss[0m : 2.47049

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53074
[1mStep[0m  [2/21], [94mLoss[0m : 2.41583
[1mStep[0m  [4/21], [94mLoss[0m : 2.43250
[1mStep[0m  [6/21], [94mLoss[0m : 2.42308
[1mStep[0m  [8/21], [94mLoss[0m : 2.50420
[1mStep[0m  [10/21], [94mLoss[0m : 2.40395
[1mStep[0m  [12/21], [94mLoss[0m : 2.52020
[1mStep[0m  [14/21], [94mLoss[0m : 2.54348
[1mStep[0m  [16/21], [94mLoss[0m : 2.33252
[1mStep[0m  [18/21], [94mLoss[0m : 2.51707
[1mStep[0m  [20/21], [94mLoss[0m : 2.59613

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38392
[1mStep[0m  [2/21], [94mLoss[0m : 2.22322
[1mStep[0m  [4/21], [94mLoss[0m : 2.36343
[1mStep[0m  [6/21], [94mLoss[0m : 2.62535
[1mStep[0m  [8/21], [94mLoss[0m : 2.52744
[1mStep[0m  [10/21], [94mLoss[0m : 2.42810
[1mStep[0m  [12/21], [94mLoss[0m : 2.53930
[1mStep[0m  [14/21], [94mLoss[0m : 2.40505
[1mStep[0m  [16/21], [94mLoss[0m : 2.55463
[1mStep[0m  [18/21], [94mLoss[0m : 2.38009
[1mStep[0m  [20/21], [94mLoss[0m : 2.56098

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34100
[1mStep[0m  [2/21], [94mLoss[0m : 2.65386
[1mStep[0m  [4/21], [94mLoss[0m : 2.40869
[1mStep[0m  [6/21], [94mLoss[0m : 2.36245
[1mStep[0m  [8/21], [94mLoss[0m : 2.46513
[1mStep[0m  [10/21], [94mLoss[0m : 2.40217
[1mStep[0m  [12/21], [94mLoss[0m : 2.37918
[1mStep[0m  [14/21], [94mLoss[0m : 2.41172
[1mStep[0m  [16/21], [94mLoss[0m : 2.24998
[1mStep[0m  [18/21], [94mLoss[0m : 2.46526
[1mStep[0m  [20/21], [94mLoss[0m : 2.56775

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59270
[1mStep[0m  [2/21], [94mLoss[0m : 2.43626
[1mStep[0m  [4/21], [94mLoss[0m : 2.47742
[1mStep[0m  [6/21], [94mLoss[0m : 2.44700
[1mStep[0m  [8/21], [94mLoss[0m : 2.35513
[1mStep[0m  [10/21], [94mLoss[0m : 2.49638
[1mStep[0m  [12/21], [94mLoss[0m : 2.46944
[1mStep[0m  [14/21], [94mLoss[0m : 2.62539
[1mStep[0m  [16/21], [94mLoss[0m : 2.77626
[1mStep[0m  [18/21], [94mLoss[0m : 2.46018
[1mStep[0m  [20/21], [94mLoss[0m : 2.57222

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42886
[1mStep[0m  [2/21], [94mLoss[0m : 2.50906
[1mStep[0m  [4/21], [94mLoss[0m : 2.31812
[1mStep[0m  [6/21], [94mLoss[0m : 2.49666
[1mStep[0m  [8/21], [94mLoss[0m : 2.35860
[1mStep[0m  [10/21], [94mLoss[0m : 2.36509
[1mStep[0m  [12/21], [94mLoss[0m : 2.50156
[1mStep[0m  [14/21], [94mLoss[0m : 2.33812
[1mStep[0m  [16/21], [94mLoss[0m : 2.40607
[1mStep[0m  [18/21], [94mLoss[0m : 2.52598
[1mStep[0m  [20/21], [94mLoss[0m : 2.50391

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60622
[1mStep[0m  [2/21], [94mLoss[0m : 2.41124
[1mStep[0m  [4/21], [94mLoss[0m : 2.53456
[1mStep[0m  [6/21], [94mLoss[0m : 2.59372
[1mStep[0m  [8/21], [94mLoss[0m : 2.40870
[1mStep[0m  [10/21], [94mLoss[0m : 2.34555
[1mStep[0m  [12/21], [94mLoss[0m : 2.38184
[1mStep[0m  [14/21], [94mLoss[0m : 2.36771
[1mStep[0m  [16/21], [94mLoss[0m : 2.53158
[1mStep[0m  [18/21], [94mLoss[0m : 2.42138
[1mStep[0m  [20/21], [94mLoss[0m : 2.56317

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59011
[1mStep[0m  [2/21], [94mLoss[0m : 2.44321
[1mStep[0m  [4/21], [94mLoss[0m : 2.51320
[1mStep[0m  [6/21], [94mLoss[0m : 2.52219
[1mStep[0m  [8/21], [94mLoss[0m : 2.48947
[1mStep[0m  [10/21], [94mLoss[0m : 2.44073
[1mStep[0m  [12/21], [94mLoss[0m : 2.51454
[1mStep[0m  [14/21], [94mLoss[0m : 2.51989
[1mStep[0m  [16/21], [94mLoss[0m : 2.38282
[1mStep[0m  [18/21], [94mLoss[0m : 2.49054
[1mStep[0m  [20/21], [94mLoss[0m : 2.52176

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55106
[1mStep[0m  [2/21], [94mLoss[0m : 2.66888
[1mStep[0m  [4/21], [94mLoss[0m : 2.47475
[1mStep[0m  [6/21], [94mLoss[0m : 2.27822
[1mStep[0m  [8/21], [94mLoss[0m : 2.38889
[1mStep[0m  [10/21], [94mLoss[0m : 2.28152
[1mStep[0m  [12/21], [94mLoss[0m : 2.51240
[1mStep[0m  [14/21], [94mLoss[0m : 2.36892
[1mStep[0m  [16/21], [94mLoss[0m : 2.46403
[1mStep[0m  [18/21], [94mLoss[0m : 2.36364
[1mStep[0m  [20/21], [94mLoss[0m : 2.43593

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52327
[1mStep[0m  [2/21], [94mLoss[0m : 2.55133
[1mStep[0m  [4/21], [94mLoss[0m : 2.36855
[1mStep[0m  [6/21], [94mLoss[0m : 2.41593
[1mStep[0m  [8/21], [94mLoss[0m : 2.45684
[1mStep[0m  [10/21], [94mLoss[0m : 2.46860
[1mStep[0m  [12/21], [94mLoss[0m : 2.52242
[1mStep[0m  [14/21], [94mLoss[0m : 2.38289
[1mStep[0m  [16/21], [94mLoss[0m : 2.38378
[1mStep[0m  [18/21], [94mLoss[0m : 2.44603
[1mStep[0m  [20/21], [94mLoss[0m : 2.50898

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47773
[1mStep[0m  [2/21], [94mLoss[0m : 2.47788
[1mStep[0m  [4/21], [94mLoss[0m : 2.55280
[1mStep[0m  [6/21], [94mLoss[0m : 2.56630
[1mStep[0m  [8/21], [94mLoss[0m : 2.44246
[1mStep[0m  [10/21], [94mLoss[0m : 2.63908
[1mStep[0m  [12/21], [94mLoss[0m : 2.51976
[1mStep[0m  [14/21], [94mLoss[0m : 2.51984
[1mStep[0m  [16/21], [94mLoss[0m : 2.54237
[1mStep[0m  [18/21], [94mLoss[0m : 2.44610
[1mStep[0m  [20/21], [94mLoss[0m : 2.36925

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34800
[1mStep[0m  [2/21], [94mLoss[0m : 2.40593
[1mStep[0m  [4/21], [94mLoss[0m : 2.40822
[1mStep[0m  [6/21], [94mLoss[0m : 2.42549
[1mStep[0m  [8/21], [94mLoss[0m : 2.37468
[1mStep[0m  [10/21], [94mLoss[0m : 2.52309
[1mStep[0m  [12/21], [94mLoss[0m : 2.44657
[1mStep[0m  [14/21], [94mLoss[0m : 2.37542
[1mStep[0m  [16/21], [94mLoss[0m : 2.46671
[1mStep[0m  [18/21], [94mLoss[0m : 2.39657
[1mStep[0m  [20/21], [94mLoss[0m : 2.40255

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53832
[1mStep[0m  [2/21], [94mLoss[0m : 2.59076
[1mStep[0m  [4/21], [94mLoss[0m : 2.35403
[1mStep[0m  [6/21], [94mLoss[0m : 2.35080
[1mStep[0m  [8/21], [94mLoss[0m : 2.46376
[1mStep[0m  [10/21], [94mLoss[0m : 2.41870
[1mStep[0m  [12/21], [94mLoss[0m : 2.45788
[1mStep[0m  [14/21], [94mLoss[0m : 2.51594
[1mStep[0m  [16/21], [94mLoss[0m : 2.34733
[1mStep[0m  [18/21], [94mLoss[0m : 2.41983
[1mStep[0m  [20/21], [94mLoss[0m : 2.43099

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38904
[1mStep[0m  [2/21], [94mLoss[0m : 2.45935
[1mStep[0m  [4/21], [94mLoss[0m : 2.28411
[1mStep[0m  [6/21], [94mLoss[0m : 2.40202
[1mStep[0m  [8/21], [94mLoss[0m : 2.46537
[1mStep[0m  [10/21], [94mLoss[0m : 2.43243
[1mStep[0m  [12/21], [94mLoss[0m : 2.52695
[1mStep[0m  [14/21], [94mLoss[0m : 2.31076
[1mStep[0m  [16/21], [94mLoss[0m : 2.45499
[1mStep[0m  [18/21], [94mLoss[0m : 2.51967
[1mStep[0m  [20/21], [94mLoss[0m : 2.34295

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36676
[1mStep[0m  [2/21], [94mLoss[0m : 2.41141
[1mStep[0m  [4/21], [94mLoss[0m : 2.45156
[1mStep[0m  [6/21], [94mLoss[0m : 2.46010
[1mStep[0m  [8/21], [94mLoss[0m : 2.49016
[1mStep[0m  [10/21], [94mLoss[0m : 2.32277
[1mStep[0m  [12/21], [94mLoss[0m : 2.36428
[1mStep[0m  [14/21], [94mLoss[0m : 2.60237
[1mStep[0m  [16/21], [94mLoss[0m : 2.32365
[1mStep[0m  [18/21], [94mLoss[0m : 2.40623
[1mStep[0m  [20/21], [94mLoss[0m : 2.59548

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46633
[1mStep[0m  [2/21], [94mLoss[0m : 2.43812
[1mStep[0m  [4/21], [94mLoss[0m : 2.34932
[1mStep[0m  [6/21], [94mLoss[0m : 2.33970
[1mStep[0m  [8/21], [94mLoss[0m : 2.46296
[1mStep[0m  [10/21], [94mLoss[0m : 2.42744
[1mStep[0m  [12/21], [94mLoss[0m : 2.42577
[1mStep[0m  [14/21], [94mLoss[0m : 2.55534
[1mStep[0m  [16/21], [94mLoss[0m : 2.40390
[1mStep[0m  [18/21], [94mLoss[0m : 2.39427
[1mStep[0m  [20/21], [94mLoss[0m : 2.53608

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45889
[1mStep[0m  [2/21], [94mLoss[0m : 2.47171
[1mStep[0m  [4/21], [94mLoss[0m : 2.32396
[1mStep[0m  [6/21], [94mLoss[0m : 2.30351
[1mStep[0m  [8/21], [94mLoss[0m : 2.45820
[1mStep[0m  [10/21], [94mLoss[0m : 2.36988
[1mStep[0m  [12/21], [94mLoss[0m : 2.46626
[1mStep[0m  [14/21], [94mLoss[0m : 2.34577
[1mStep[0m  [16/21], [94mLoss[0m : 2.46027
[1mStep[0m  [18/21], [94mLoss[0m : 2.62225
[1mStep[0m  [20/21], [94mLoss[0m : 2.43674

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49068
[1mStep[0m  [2/21], [94mLoss[0m : 2.48683
[1mStep[0m  [4/21], [94mLoss[0m : 2.48895
[1mStep[0m  [6/21], [94mLoss[0m : 2.36371
[1mStep[0m  [8/21], [94mLoss[0m : 2.45940
[1mStep[0m  [10/21], [94mLoss[0m : 2.39001
[1mStep[0m  [12/21], [94mLoss[0m : 2.49410
[1mStep[0m  [14/21], [94mLoss[0m : 2.32251
[1mStep[0m  [16/21], [94mLoss[0m : 2.43613
[1mStep[0m  [18/21], [94mLoss[0m : 2.45873
[1mStep[0m  [20/21], [94mLoss[0m : 2.28304

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37787
[1mStep[0m  [2/21], [94mLoss[0m : 2.52596
[1mStep[0m  [4/21], [94mLoss[0m : 2.55084
[1mStep[0m  [6/21], [94mLoss[0m : 2.28041
[1mStep[0m  [8/21], [94mLoss[0m : 2.38349
[1mStep[0m  [10/21], [94mLoss[0m : 2.34944
[1mStep[0m  [12/21], [94mLoss[0m : 2.41576
[1mStep[0m  [14/21], [94mLoss[0m : 2.46886
[1mStep[0m  [16/21], [94mLoss[0m : 2.43825
[1mStep[0m  [18/21], [94mLoss[0m : 2.50695
[1mStep[0m  [20/21], [94mLoss[0m : 2.43621

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33414
[1mStep[0m  [2/21], [94mLoss[0m : 2.64027
[1mStep[0m  [4/21], [94mLoss[0m : 2.30303
[1mStep[0m  [6/21], [94mLoss[0m : 2.27303
[1mStep[0m  [8/21], [94mLoss[0m : 2.55102
[1mStep[0m  [10/21], [94mLoss[0m : 2.39138
[1mStep[0m  [12/21], [94mLoss[0m : 2.61956
[1mStep[0m  [14/21], [94mLoss[0m : 2.50754
[1mStep[0m  [16/21], [94mLoss[0m : 2.52465
[1mStep[0m  [18/21], [94mLoss[0m : 2.34050
[1mStep[0m  [20/21], [94mLoss[0m : 2.46486

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.324
====================================

Phase 1 - Evaluation MAE:  2.323736701692854
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.42013
[1mStep[0m  [2/21], [94mLoss[0m : 2.49549
[1mStep[0m  [4/21], [94mLoss[0m : 2.55859
[1mStep[0m  [6/21], [94mLoss[0m : 2.51261
[1mStep[0m  [8/21], [94mLoss[0m : 2.59783
[1mStep[0m  [10/21], [94mLoss[0m : 2.50278
[1mStep[0m  [12/21], [94mLoss[0m : 2.51477
[1mStep[0m  [14/21], [94mLoss[0m : 2.35711
[1mStep[0m  [16/21], [94mLoss[0m : 2.28795
[1mStep[0m  [18/21], [94mLoss[0m : 2.56705
[1mStep[0m  [20/21], [94mLoss[0m : 2.44873

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38062
[1mStep[0m  [2/21], [94mLoss[0m : 2.35974
[1mStep[0m  [4/21], [94mLoss[0m : 2.43623
[1mStep[0m  [6/21], [94mLoss[0m : 2.38074
[1mStep[0m  [8/21], [94mLoss[0m : 2.30210
[1mStep[0m  [10/21], [94mLoss[0m : 2.28810
[1mStep[0m  [12/21], [94mLoss[0m : 2.34814
[1mStep[0m  [14/21], [94mLoss[0m : 2.34938
[1mStep[0m  [16/21], [94mLoss[0m : 2.48854
[1mStep[0m  [18/21], [94mLoss[0m : 2.34416
[1mStep[0m  [20/21], [94mLoss[0m : 2.26315

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29132
[1mStep[0m  [2/21], [94mLoss[0m : 2.30373
[1mStep[0m  [4/21], [94mLoss[0m : 2.18521
[1mStep[0m  [6/21], [94mLoss[0m : 2.31502
[1mStep[0m  [8/21], [94mLoss[0m : 2.29604
[1mStep[0m  [10/21], [94mLoss[0m : 2.30363
[1mStep[0m  [12/21], [94mLoss[0m : 2.18433
[1mStep[0m  [14/21], [94mLoss[0m : 2.24171
[1mStep[0m  [16/21], [94mLoss[0m : 2.32795
[1mStep[0m  [18/21], [94mLoss[0m : 2.28980
[1mStep[0m  [20/21], [94mLoss[0m : 2.20793

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16443
[1mStep[0m  [2/21], [94mLoss[0m : 2.12401
[1mStep[0m  [4/21], [94mLoss[0m : 2.18408
[1mStep[0m  [6/21], [94mLoss[0m : 2.25930
[1mStep[0m  [8/21], [94mLoss[0m : 2.25342
[1mStep[0m  [10/21], [94mLoss[0m : 2.21045
[1mStep[0m  [12/21], [94mLoss[0m : 2.30596
[1mStep[0m  [14/21], [94mLoss[0m : 2.26111
[1mStep[0m  [16/21], [94mLoss[0m : 2.25800
[1mStep[0m  [18/21], [94mLoss[0m : 2.27707
[1mStep[0m  [20/21], [94mLoss[0m : 2.26487

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16330
[1mStep[0m  [2/21], [94mLoss[0m : 2.21885
[1mStep[0m  [4/21], [94mLoss[0m : 2.10549
[1mStep[0m  [6/21], [94mLoss[0m : 2.13164
[1mStep[0m  [8/21], [94mLoss[0m : 2.07863
[1mStep[0m  [10/21], [94mLoss[0m : 2.08622
[1mStep[0m  [12/21], [94mLoss[0m : 2.10842
[1mStep[0m  [14/21], [94mLoss[0m : 2.19962
[1mStep[0m  [16/21], [94mLoss[0m : 2.10935
[1mStep[0m  [18/21], [94mLoss[0m : 2.19353
[1mStep[0m  [20/21], [94mLoss[0m : 2.21165

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09452
[1mStep[0m  [2/21], [94mLoss[0m : 1.92238
[1mStep[0m  [4/21], [94mLoss[0m : 2.22948
[1mStep[0m  [6/21], [94mLoss[0m : 1.96466
[1mStep[0m  [8/21], [94mLoss[0m : 2.02107
[1mStep[0m  [10/21], [94mLoss[0m : 1.99212
[1mStep[0m  [12/21], [94mLoss[0m : 2.11496
[1mStep[0m  [14/21], [94mLoss[0m : 2.15202
[1mStep[0m  [16/21], [94mLoss[0m : 2.15175
[1mStep[0m  [18/21], [94mLoss[0m : 2.15979
[1mStep[0m  [20/21], [94mLoss[0m : 2.10652

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97714
[1mStep[0m  [2/21], [94mLoss[0m : 1.94804
[1mStep[0m  [4/21], [94mLoss[0m : 1.95462
[1mStep[0m  [6/21], [94mLoss[0m : 1.90153
[1mStep[0m  [8/21], [94mLoss[0m : 1.82190
[1mStep[0m  [10/21], [94mLoss[0m : 2.02231
[1mStep[0m  [12/21], [94mLoss[0m : 2.06675
[1mStep[0m  [14/21], [94mLoss[0m : 2.15948
[1mStep[0m  [16/21], [94mLoss[0m : 2.16105
[1mStep[0m  [18/21], [94mLoss[0m : 2.00781
[1mStep[0m  [20/21], [94mLoss[0m : 2.15534

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97866
[1mStep[0m  [2/21], [94mLoss[0m : 1.91404
[1mStep[0m  [4/21], [94mLoss[0m : 1.91706
[1mStep[0m  [6/21], [94mLoss[0m : 1.95611
[1mStep[0m  [8/21], [94mLoss[0m : 1.81729
[1mStep[0m  [10/21], [94mLoss[0m : 2.00130
[1mStep[0m  [12/21], [94mLoss[0m : 1.93771
[1mStep[0m  [14/21], [94mLoss[0m : 1.95557
[1mStep[0m  [16/21], [94mLoss[0m : 2.04667
[1mStep[0m  [18/21], [94mLoss[0m : 2.14162
[1mStep[0m  [20/21], [94mLoss[0m : 1.93416

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84797
[1mStep[0m  [2/21], [94mLoss[0m : 1.84901
[1mStep[0m  [4/21], [94mLoss[0m : 2.04907
[1mStep[0m  [6/21], [94mLoss[0m : 1.93193
[1mStep[0m  [8/21], [94mLoss[0m : 2.06555
[1mStep[0m  [10/21], [94mLoss[0m : 1.89455
[1mStep[0m  [12/21], [94mLoss[0m : 1.92770
[1mStep[0m  [14/21], [94mLoss[0m : 1.89999
[1mStep[0m  [16/21], [94mLoss[0m : 1.61815
[1mStep[0m  [18/21], [94mLoss[0m : 1.75426
[1mStep[0m  [20/21], [94mLoss[0m : 1.95715

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.904, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.85273
[1mStep[0m  [2/21], [94mLoss[0m : 1.74073
[1mStep[0m  [4/21], [94mLoss[0m : 1.87798
[1mStep[0m  [6/21], [94mLoss[0m : 1.77718
[1mStep[0m  [8/21], [94mLoss[0m : 1.75019
[1mStep[0m  [10/21], [94mLoss[0m : 1.78842
[1mStep[0m  [12/21], [94mLoss[0m : 1.78037
[1mStep[0m  [14/21], [94mLoss[0m : 1.86735
[1mStep[0m  [16/21], [94mLoss[0m : 1.91654
[1mStep[0m  [18/21], [94mLoss[0m : 1.93694
[1mStep[0m  [20/21], [94mLoss[0m : 1.84392

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.83104
[1mStep[0m  [2/21], [94mLoss[0m : 1.72959
[1mStep[0m  [4/21], [94mLoss[0m : 1.70457
[1mStep[0m  [6/21], [94mLoss[0m : 1.84394
[1mStep[0m  [8/21], [94mLoss[0m : 1.85330
[1mStep[0m  [10/21], [94mLoss[0m : 1.67938
[1mStep[0m  [12/21], [94mLoss[0m : 1.82550
[1mStep[0m  [14/21], [94mLoss[0m : 1.87072
[1mStep[0m  [16/21], [94mLoss[0m : 1.88420
[1mStep[0m  [18/21], [94mLoss[0m : 1.85085
[1mStep[0m  [20/21], [94mLoss[0m : 1.80919

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63389
[1mStep[0m  [2/21], [94mLoss[0m : 1.75812
[1mStep[0m  [4/21], [94mLoss[0m : 1.80705
[1mStep[0m  [6/21], [94mLoss[0m : 1.67358
[1mStep[0m  [8/21], [94mLoss[0m : 1.71671
[1mStep[0m  [10/21], [94mLoss[0m : 1.68581
[1mStep[0m  [12/21], [94mLoss[0m : 1.87883
[1mStep[0m  [14/21], [94mLoss[0m : 1.82308
[1mStep[0m  [16/21], [94mLoss[0m : 1.78162
[1mStep[0m  [18/21], [94mLoss[0m : 1.79501
[1mStep[0m  [20/21], [94mLoss[0m : 1.72975

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71891
[1mStep[0m  [2/21], [94mLoss[0m : 1.53311
[1mStep[0m  [4/21], [94mLoss[0m : 1.65223
[1mStep[0m  [6/21], [94mLoss[0m : 1.64644
[1mStep[0m  [8/21], [94mLoss[0m : 1.59769
[1mStep[0m  [10/21], [94mLoss[0m : 1.66768
[1mStep[0m  [12/21], [94mLoss[0m : 1.67959
[1mStep[0m  [14/21], [94mLoss[0m : 1.78115
[1mStep[0m  [16/21], [94mLoss[0m : 1.73634
[1mStep[0m  [18/21], [94mLoss[0m : 1.65750
[1mStep[0m  [20/21], [94mLoss[0m : 1.79123

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.65987
[1mStep[0m  [2/21], [94mLoss[0m : 1.57785
[1mStep[0m  [4/21], [94mLoss[0m : 1.76801
[1mStep[0m  [6/21], [94mLoss[0m : 1.72452
[1mStep[0m  [8/21], [94mLoss[0m : 1.63634
[1mStep[0m  [10/21], [94mLoss[0m : 1.59230
[1mStep[0m  [12/21], [94mLoss[0m : 1.66757
[1mStep[0m  [14/21], [94mLoss[0m : 1.70259
[1mStep[0m  [16/21], [94mLoss[0m : 1.71908
[1mStep[0m  [18/21], [94mLoss[0m : 1.74511
[1mStep[0m  [20/21], [94mLoss[0m : 1.69894

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.536, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.67432
[1mStep[0m  [2/21], [94mLoss[0m : 1.62610
[1mStep[0m  [4/21], [94mLoss[0m : 1.69813
[1mStep[0m  [6/21], [94mLoss[0m : 1.66840
[1mStep[0m  [8/21], [94mLoss[0m : 1.61764
[1mStep[0m  [10/21], [94mLoss[0m : 1.65789
[1mStep[0m  [12/21], [94mLoss[0m : 1.71080
[1mStep[0m  [14/21], [94mLoss[0m : 1.51229
[1mStep[0m  [16/21], [94mLoss[0m : 1.59467
[1mStep[0m  [18/21], [94mLoss[0m : 1.77947
[1mStep[0m  [20/21], [94mLoss[0m : 1.71736

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51555
[1mStep[0m  [2/21], [94mLoss[0m : 1.53924
[1mStep[0m  [4/21], [94mLoss[0m : 1.63107
[1mStep[0m  [6/21], [94mLoss[0m : 1.53368
[1mStep[0m  [8/21], [94mLoss[0m : 1.52197
[1mStep[0m  [10/21], [94mLoss[0m : 1.53004
[1mStep[0m  [12/21], [94mLoss[0m : 1.67582
[1mStep[0m  [14/21], [94mLoss[0m : 1.73349
[1mStep[0m  [16/21], [94mLoss[0m : 1.58178
[1mStep[0m  [18/21], [94mLoss[0m : 1.61633
[1mStep[0m  [20/21], [94mLoss[0m : 1.67391

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.603, [92mTest[0m: 2.527, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61087
[1mStep[0m  [2/21], [94mLoss[0m : 1.57394
[1mStep[0m  [4/21], [94mLoss[0m : 1.59583
[1mStep[0m  [6/21], [94mLoss[0m : 1.58068
[1mStep[0m  [8/21], [94mLoss[0m : 1.45479
[1mStep[0m  [10/21], [94mLoss[0m : 1.62049
[1mStep[0m  [12/21], [94mLoss[0m : 1.62254
[1mStep[0m  [14/21], [94mLoss[0m : 1.60484
[1mStep[0m  [16/21], [94mLoss[0m : 1.50873
[1mStep[0m  [18/21], [94mLoss[0m : 1.57726
[1mStep[0m  [20/21], [94mLoss[0m : 1.63106

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46959
[1mStep[0m  [2/21], [94mLoss[0m : 1.53896
[1mStep[0m  [4/21], [94mLoss[0m : 1.46150
[1mStep[0m  [6/21], [94mLoss[0m : 1.64377
[1mStep[0m  [8/21], [94mLoss[0m : 1.60701
[1mStep[0m  [10/21], [94mLoss[0m : 1.45624
[1mStep[0m  [12/21], [94mLoss[0m : 1.54839
[1mStep[0m  [14/21], [94mLoss[0m : 1.56278
[1mStep[0m  [16/21], [94mLoss[0m : 1.53023
[1mStep[0m  [18/21], [94mLoss[0m : 1.60756
[1mStep[0m  [20/21], [94mLoss[0m : 1.54973

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.542, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78782
[1mStep[0m  [2/21], [94mLoss[0m : 1.40281
[1mStep[0m  [4/21], [94mLoss[0m : 1.46866
[1mStep[0m  [6/21], [94mLoss[0m : 1.48196
[1mStep[0m  [8/21], [94mLoss[0m : 1.50838
[1mStep[0m  [10/21], [94mLoss[0m : 1.48282
[1mStep[0m  [12/21], [94mLoss[0m : 1.57300
[1mStep[0m  [14/21], [94mLoss[0m : 1.48609
[1mStep[0m  [16/21], [94mLoss[0m : 1.49184
[1mStep[0m  [18/21], [94mLoss[0m : 1.49280
[1mStep[0m  [20/21], [94mLoss[0m : 1.52417

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.45737
[1mStep[0m  [2/21], [94mLoss[0m : 1.41325
[1mStep[0m  [4/21], [94mLoss[0m : 1.41762
[1mStep[0m  [6/21], [94mLoss[0m : 1.44557
[1mStep[0m  [8/21], [94mLoss[0m : 1.46285
[1mStep[0m  [10/21], [94mLoss[0m : 1.63667
[1mStep[0m  [12/21], [94mLoss[0m : 1.51642
[1mStep[0m  [14/21], [94mLoss[0m : 1.47082
[1mStep[0m  [16/21], [94mLoss[0m : 1.51225
[1mStep[0m  [18/21], [94mLoss[0m : 1.56746
[1mStep[0m  [20/21], [94mLoss[0m : 1.56212

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.524, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.42347
[1mStep[0m  [2/21], [94mLoss[0m : 1.42495
[1mStep[0m  [4/21], [94mLoss[0m : 1.49408
[1mStep[0m  [6/21], [94mLoss[0m : 1.42883
[1mStep[0m  [8/21], [94mLoss[0m : 1.43447
[1mStep[0m  [10/21], [94mLoss[0m : 1.45471
[1mStep[0m  [12/21], [94mLoss[0m : 1.35074
[1mStep[0m  [14/21], [94mLoss[0m : 1.41369
[1mStep[0m  [16/21], [94mLoss[0m : 1.44892
[1mStep[0m  [18/21], [94mLoss[0m : 1.43921
[1mStep[0m  [20/21], [94mLoss[0m : 1.44294

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.436, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.32788
[1mStep[0m  [2/21], [94mLoss[0m : 1.52962
[1mStep[0m  [4/21], [94mLoss[0m : 1.46379
[1mStep[0m  [6/21], [94mLoss[0m : 1.41559
[1mStep[0m  [8/21], [94mLoss[0m : 1.39306
[1mStep[0m  [10/21], [94mLoss[0m : 1.47523
[1mStep[0m  [12/21], [94mLoss[0m : 1.46208
[1mStep[0m  [14/21], [94mLoss[0m : 1.48247
[1mStep[0m  [16/21], [94mLoss[0m : 1.38428
[1mStep[0m  [18/21], [94mLoss[0m : 1.41121
[1mStep[0m  [20/21], [94mLoss[0m : 1.55852

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.430, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43305
[1mStep[0m  [2/21], [94mLoss[0m : 1.38361
[1mStep[0m  [4/21], [94mLoss[0m : 1.40978
[1mStep[0m  [6/21], [94mLoss[0m : 1.41180
[1mStep[0m  [8/21], [94mLoss[0m : 1.39756
[1mStep[0m  [10/21], [94mLoss[0m : 1.42212
[1mStep[0m  [12/21], [94mLoss[0m : 1.47192
[1mStep[0m  [14/21], [94mLoss[0m : 1.36969
[1mStep[0m  [16/21], [94mLoss[0m : 1.37759
[1mStep[0m  [18/21], [94mLoss[0m : 1.39594
[1mStep[0m  [20/21], [94mLoss[0m : 1.35195

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.395, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.536
====================================

Phase 2 - Evaluation MAE:  2.5363289969308034
MAE score P1       2.323737
MAE score P2       2.536329
loss               1.394954
learning_rate          0.01
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay          0.001
Name: 12, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.87093
[1mStep[0m  [2/21], [94mLoss[0m : 11.00903
[1mStep[0m  [4/21], [94mLoss[0m : 10.65827
[1mStep[0m  [6/21], [94mLoss[0m : 10.65487
[1mStep[0m  [8/21], [94mLoss[0m : 10.80590
[1mStep[0m  [10/21], [94mLoss[0m : 10.69592
[1mStep[0m  [12/21], [94mLoss[0m : 10.92839
[1mStep[0m  [14/21], [94mLoss[0m : 10.57564
[1mStep[0m  [16/21], [94mLoss[0m : 10.43324
[1mStep[0m  [18/21], [94mLoss[0m : 10.63647
[1mStep[0m  [20/21], [94mLoss[0m : 10.55390

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.730, [92mTest[0m: 10.842, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.85421
[1mStep[0m  [2/21], [94mLoss[0m : 10.47416
[1mStep[0m  [4/21], [94mLoss[0m : 10.64395
[1mStep[0m  [6/21], [94mLoss[0m : 10.41262
[1mStep[0m  [8/21], [94mLoss[0m : 10.44443
[1mStep[0m  [10/21], [94mLoss[0m : 10.43040
[1mStep[0m  [12/21], [94mLoss[0m : 10.63721
[1mStep[0m  [14/21], [94mLoss[0m : 10.40587
[1mStep[0m  [16/21], [94mLoss[0m : 10.40919
[1mStep[0m  [18/21], [94mLoss[0m : 10.43138
[1mStep[0m  [20/21], [94mLoss[0m : 10.53347

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.453, [92mTest[0m: 10.553, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.13772
[1mStep[0m  [2/21], [94mLoss[0m : 10.25250
[1mStep[0m  [4/21], [94mLoss[0m : 10.12766
[1mStep[0m  [6/21], [94mLoss[0m : 10.23468
[1mStep[0m  [8/21], [94mLoss[0m : 10.32386
[1mStep[0m  [10/21], [94mLoss[0m : 9.96604
[1mStep[0m  [12/21], [94mLoss[0m : 10.11945
[1mStep[0m  [14/21], [94mLoss[0m : 10.28144
[1mStep[0m  [16/21], [94mLoss[0m : 10.02911
[1mStep[0m  [18/21], [94mLoss[0m : 10.20080
[1mStep[0m  [20/21], [94mLoss[0m : 10.17084

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.165, [92mTest[0m: 10.247, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.00027
[1mStep[0m  [2/21], [94mLoss[0m : 9.87684
[1mStep[0m  [4/21], [94mLoss[0m : 9.95314
[1mStep[0m  [6/21], [94mLoss[0m : 10.07552
[1mStep[0m  [8/21], [94mLoss[0m : 9.96838
[1mStep[0m  [10/21], [94mLoss[0m : 9.95247
[1mStep[0m  [12/21], [94mLoss[0m : 9.90059
[1mStep[0m  [14/21], [94mLoss[0m : 9.61525
[1mStep[0m  [16/21], [94mLoss[0m : 9.70778
[1mStep[0m  [18/21], [94mLoss[0m : 9.84345
[1mStep[0m  [20/21], [94mLoss[0m : 9.98047

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.869, [92mTest[0m: 9.909, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.84812
[1mStep[0m  [2/21], [94mLoss[0m : 9.60609
[1mStep[0m  [4/21], [94mLoss[0m : 9.44223
[1mStep[0m  [6/21], [94mLoss[0m : 9.90315
[1mStep[0m  [8/21], [94mLoss[0m : 9.61409
[1mStep[0m  [10/21], [94mLoss[0m : 9.63309
[1mStep[0m  [12/21], [94mLoss[0m : 9.47788
[1mStep[0m  [14/21], [94mLoss[0m : 9.47363
[1mStep[0m  [16/21], [94mLoss[0m : 9.27308
[1mStep[0m  [18/21], [94mLoss[0m : 9.60608
[1mStep[0m  [20/21], [94mLoss[0m : 9.32833

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.549, [92mTest[0m: 9.557, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.29354
[1mStep[0m  [2/21], [94mLoss[0m : 9.57076
[1mStep[0m  [4/21], [94mLoss[0m : 9.15117
[1mStep[0m  [6/21], [94mLoss[0m : 9.01732
[1mStep[0m  [8/21], [94mLoss[0m : 9.33317
[1mStep[0m  [10/21], [94mLoss[0m : 9.31232
[1mStep[0m  [12/21], [94mLoss[0m : 8.99672
[1mStep[0m  [14/21], [94mLoss[0m : 9.13313
[1mStep[0m  [16/21], [94mLoss[0m : 9.11216
[1mStep[0m  [18/21], [94mLoss[0m : 9.02557
[1mStep[0m  [20/21], [94mLoss[0m : 9.03070

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.203, [92mTest[0m: 9.186, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.00965
[1mStep[0m  [2/21], [94mLoss[0m : 9.11471
[1mStep[0m  [4/21], [94mLoss[0m : 8.88892
[1mStep[0m  [6/21], [94mLoss[0m : 9.06523
[1mStep[0m  [8/21], [94mLoss[0m : 8.80354
[1mStep[0m  [10/21], [94mLoss[0m : 8.96694
[1mStep[0m  [12/21], [94mLoss[0m : 9.02112
[1mStep[0m  [14/21], [94mLoss[0m : 8.62779
[1mStep[0m  [16/21], [94mLoss[0m : 8.42875
[1mStep[0m  [18/21], [94mLoss[0m : 8.98727
[1mStep[0m  [20/21], [94mLoss[0m : 8.55028

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.815, [92mTest[0m: 8.774, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.63665
[1mStep[0m  [2/21], [94mLoss[0m : 8.40585
[1mStep[0m  [4/21], [94mLoss[0m : 8.50422
[1mStep[0m  [6/21], [94mLoss[0m : 8.28969
[1mStep[0m  [8/21], [94mLoss[0m : 8.09071
[1mStep[0m  [10/21], [94mLoss[0m : 8.22424
[1mStep[0m  [12/21], [94mLoss[0m : 8.25986
[1mStep[0m  [14/21], [94mLoss[0m : 8.44193
[1mStep[0m  [16/21], [94mLoss[0m : 8.22212
[1mStep[0m  [18/21], [94mLoss[0m : 8.26269
[1mStep[0m  [20/21], [94mLoss[0m : 8.28422

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.361, [92mTest[0m: 8.313, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.01957
[1mStep[0m  [2/21], [94mLoss[0m : 8.27315
[1mStep[0m  [4/21], [94mLoss[0m : 8.10888
[1mStep[0m  [6/21], [94mLoss[0m : 7.92226
[1mStep[0m  [8/21], [94mLoss[0m : 8.12244
[1mStep[0m  [10/21], [94mLoss[0m : 7.90096
[1mStep[0m  [12/21], [94mLoss[0m : 7.81539
[1mStep[0m  [14/21], [94mLoss[0m : 7.68677
[1mStep[0m  [16/21], [94mLoss[0m : 7.70503
[1mStep[0m  [18/21], [94mLoss[0m : 7.48463
[1mStep[0m  [20/21], [94mLoss[0m : 7.22871

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.853, [92mTest[0m: 7.768, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.41332
[1mStep[0m  [2/21], [94mLoss[0m : 7.82047
[1mStep[0m  [4/21], [94mLoss[0m : 7.36634
[1mStep[0m  [6/21], [94mLoss[0m : 7.25442
[1mStep[0m  [8/21], [94mLoss[0m : 7.17374
[1mStep[0m  [10/21], [94mLoss[0m : 7.21424
[1mStep[0m  [12/21], [94mLoss[0m : 7.32537
[1mStep[0m  [14/21], [94mLoss[0m : 7.05688
[1mStep[0m  [16/21], [94mLoss[0m : 6.90862
[1mStep[0m  [18/21], [94mLoss[0m : 7.12903
[1mStep[0m  [20/21], [94mLoss[0m : 6.93465

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.255, [92mTest[0m: 7.149, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.85917
[1mStep[0m  [2/21], [94mLoss[0m : 6.94414
[1mStep[0m  [4/21], [94mLoss[0m : 6.84291
[1mStep[0m  [6/21], [94mLoss[0m : 6.53822
[1mStep[0m  [8/21], [94mLoss[0m : 6.56156
[1mStep[0m  [10/21], [94mLoss[0m : 6.80117
[1mStep[0m  [12/21], [94mLoss[0m : 6.51150
[1mStep[0m  [14/21], [94mLoss[0m : 6.45605
[1mStep[0m  [16/21], [94mLoss[0m : 6.39150
[1mStep[0m  [18/21], [94mLoss[0m : 6.52187
[1mStep[0m  [20/21], [94mLoss[0m : 6.14381

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.643, [92mTest[0m: 6.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.51799
[1mStep[0m  [2/21], [94mLoss[0m : 5.96704
[1mStep[0m  [4/21], [94mLoss[0m : 6.33326
[1mStep[0m  [6/21], [94mLoss[0m : 6.06718
[1mStep[0m  [8/21], [94mLoss[0m : 6.17178
[1mStep[0m  [10/21], [94mLoss[0m : 6.29695
[1mStep[0m  [12/21], [94mLoss[0m : 5.82867
[1mStep[0m  [14/21], [94mLoss[0m : 5.95788
[1mStep[0m  [16/21], [94mLoss[0m : 5.88285
[1mStep[0m  [18/21], [94mLoss[0m : 5.86677
[1mStep[0m  [20/21], [94mLoss[0m : 5.61883

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.066, [92mTest[0m: 5.750, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.56829
[1mStep[0m  [2/21], [94mLoss[0m : 5.88760
[1mStep[0m  [4/21], [94mLoss[0m : 5.85002
[1mStep[0m  [6/21], [94mLoss[0m : 5.65617
[1mStep[0m  [8/21], [94mLoss[0m : 5.74587
[1mStep[0m  [10/21], [94mLoss[0m : 5.39478
[1mStep[0m  [12/21], [94mLoss[0m : 5.46096
[1mStep[0m  [14/21], [94mLoss[0m : 5.16517
[1mStep[0m  [16/21], [94mLoss[0m : 5.17791
[1mStep[0m  [18/21], [94mLoss[0m : 5.28878
[1mStep[0m  [20/21], [94mLoss[0m : 5.18454

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.538, [92mTest[0m: 5.075, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16809
[1mStep[0m  [2/21], [94mLoss[0m : 5.02898
[1mStep[0m  [4/21], [94mLoss[0m : 5.12068
[1mStep[0m  [6/21], [94mLoss[0m : 4.89385
[1mStep[0m  [8/21], [94mLoss[0m : 4.90077
[1mStep[0m  [10/21], [94mLoss[0m : 5.11336
[1mStep[0m  [12/21], [94mLoss[0m : 4.94225
[1mStep[0m  [14/21], [94mLoss[0m : 4.96625
[1mStep[0m  [16/21], [94mLoss[0m : 4.89129
[1mStep[0m  [18/21], [94mLoss[0m : 4.89395
[1mStep[0m  [20/21], [94mLoss[0m : 4.76025

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.958, [92mTest[0m: 4.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.45487
[1mStep[0m  [2/21], [94mLoss[0m : 4.65847
[1mStep[0m  [4/21], [94mLoss[0m : 4.81644
[1mStep[0m  [6/21], [94mLoss[0m : 4.63520
[1mStep[0m  [8/21], [94mLoss[0m : 4.46626
[1mStep[0m  [10/21], [94mLoss[0m : 4.24735
[1mStep[0m  [12/21], [94mLoss[0m : 4.45207
[1mStep[0m  [14/21], [94mLoss[0m : 4.29883
[1mStep[0m  [16/21], [94mLoss[0m : 4.42375
[1mStep[0m  [18/21], [94mLoss[0m : 4.16528
[1mStep[0m  [20/21], [94mLoss[0m : 4.34471

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.447, [92mTest[0m: 3.978, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.29744
[1mStep[0m  [2/21], [94mLoss[0m : 4.32114
[1mStep[0m  [4/21], [94mLoss[0m : 3.94778
[1mStep[0m  [6/21], [94mLoss[0m : 3.94876
[1mStep[0m  [8/21], [94mLoss[0m : 4.11311
[1mStep[0m  [10/21], [94mLoss[0m : 3.92267
[1mStep[0m  [12/21], [94mLoss[0m : 3.77382
[1mStep[0m  [14/21], [94mLoss[0m : 3.87437
[1mStep[0m  [16/21], [94mLoss[0m : 3.70609
[1mStep[0m  [18/21], [94mLoss[0m : 3.86118
[1mStep[0m  [20/21], [94mLoss[0m : 3.72349

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.959, [92mTest[0m: 3.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.63135
[1mStep[0m  [2/21], [94mLoss[0m : 3.65837
[1mStep[0m  [4/21], [94mLoss[0m : 3.76925
[1mStep[0m  [6/21], [94mLoss[0m : 3.52501
[1mStep[0m  [8/21], [94mLoss[0m : 3.58712
[1mStep[0m  [10/21], [94mLoss[0m : 3.25361
[1mStep[0m  [12/21], [94mLoss[0m : 3.44471
[1mStep[0m  [14/21], [94mLoss[0m : 3.42705
[1mStep[0m  [16/21], [94mLoss[0m : 3.39252
[1mStep[0m  [18/21], [94mLoss[0m : 3.46977
[1mStep[0m  [20/21], [94mLoss[0m : 3.23137

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.495, [92mTest[0m: 3.072, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.46603
[1mStep[0m  [2/21], [94mLoss[0m : 3.12625
[1mStep[0m  [4/21], [94mLoss[0m : 3.18433
[1mStep[0m  [6/21], [94mLoss[0m : 3.00483
[1mStep[0m  [8/21], [94mLoss[0m : 3.11148
[1mStep[0m  [10/21], [94mLoss[0m : 3.15018
[1mStep[0m  [12/21], [94mLoss[0m : 3.12982
[1mStep[0m  [14/21], [94mLoss[0m : 3.17695
[1mStep[0m  [16/21], [94mLoss[0m : 3.01503
[1mStep[0m  [18/21], [94mLoss[0m : 3.11664
[1mStep[0m  [20/21], [94mLoss[0m : 2.89635

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.146, [92mTest[0m: 2.808, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.06985
[1mStep[0m  [2/21], [94mLoss[0m : 2.88935
[1mStep[0m  [4/21], [94mLoss[0m : 3.20411
[1mStep[0m  [6/21], [94mLoss[0m : 2.94373
[1mStep[0m  [8/21], [94mLoss[0m : 3.10087
[1mStep[0m  [10/21], [94mLoss[0m : 2.81614
[1mStep[0m  [12/21], [94mLoss[0m : 2.94537
[1mStep[0m  [14/21], [94mLoss[0m : 3.09312
[1mStep[0m  [16/21], [94mLoss[0m : 3.10063
[1mStep[0m  [18/21], [94mLoss[0m : 3.02924
[1mStep[0m  [20/21], [94mLoss[0m : 2.86991

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.969, [92mTest[0m: 2.597, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.89085
[1mStep[0m  [2/21], [94mLoss[0m : 2.77805
[1mStep[0m  [4/21], [94mLoss[0m : 2.98124
[1mStep[0m  [6/21], [94mLoss[0m : 2.89582
[1mStep[0m  [8/21], [94mLoss[0m : 2.72641
[1mStep[0m  [10/21], [94mLoss[0m : 3.07073
[1mStep[0m  [12/21], [94mLoss[0m : 2.80957
[1mStep[0m  [14/21], [94mLoss[0m : 2.77146
[1mStep[0m  [16/21], [94mLoss[0m : 2.98915
[1mStep[0m  [18/21], [94mLoss[0m : 2.86170
[1mStep[0m  [20/21], [94mLoss[0m : 2.73878

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.841, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70528
[1mStep[0m  [2/21], [94mLoss[0m : 2.78023
[1mStep[0m  [4/21], [94mLoss[0m : 2.63997
[1mStep[0m  [6/21], [94mLoss[0m : 2.71141
[1mStep[0m  [8/21], [94mLoss[0m : 2.73208
[1mStep[0m  [10/21], [94mLoss[0m : 2.82811
[1mStep[0m  [12/21], [94mLoss[0m : 2.78387
[1mStep[0m  [14/21], [94mLoss[0m : 3.04448
[1mStep[0m  [16/21], [94mLoss[0m : 2.78059
[1mStep[0m  [18/21], [94mLoss[0m : 2.73492
[1mStep[0m  [20/21], [94mLoss[0m : 2.78892

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.786, [92mTest[0m: 2.424, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73776
[1mStep[0m  [2/21], [94mLoss[0m : 2.63315
[1mStep[0m  [4/21], [94mLoss[0m : 2.75759
[1mStep[0m  [6/21], [94mLoss[0m : 2.60117
[1mStep[0m  [8/21], [94mLoss[0m : 2.63009
[1mStep[0m  [10/21], [94mLoss[0m : 2.74777
[1mStep[0m  [12/21], [94mLoss[0m : 2.76709
[1mStep[0m  [14/21], [94mLoss[0m : 2.68505
[1mStep[0m  [16/21], [94mLoss[0m : 2.90797
[1mStep[0m  [18/21], [94mLoss[0m : 2.69597
[1mStep[0m  [20/21], [94mLoss[0m : 2.77471

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.745, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79009
[1mStep[0m  [2/21], [94mLoss[0m : 2.83820
[1mStep[0m  [4/21], [94mLoss[0m : 2.77344
[1mStep[0m  [6/21], [94mLoss[0m : 2.81832
[1mStep[0m  [8/21], [94mLoss[0m : 2.67265
[1mStep[0m  [10/21], [94mLoss[0m : 2.62980
[1mStep[0m  [12/21], [94mLoss[0m : 2.76966
[1mStep[0m  [14/21], [94mLoss[0m : 2.65430
[1mStep[0m  [16/21], [94mLoss[0m : 2.79735
[1mStep[0m  [18/21], [94mLoss[0m : 2.92264
[1mStep[0m  [20/21], [94mLoss[0m : 2.83313

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.378, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81023
[1mStep[0m  [2/21], [94mLoss[0m : 2.56126
[1mStep[0m  [4/21], [94mLoss[0m : 2.91090
[1mStep[0m  [6/21], [94mLoss[0m : 2.74513
[1mStep[0m  [8/21], [94mLoss[0m : 2.79202
[1mStep[0m  [10/21], [94mLoss[0m : 2.77645
[1mStep[0m  [12/21], [94mLoss[0m : 2.77612
[1mStep[0m  [14/21], [94mLoss[0m : 2.78999
[1mStep[0m  [16/21], [94mLoss[0m : 2.65063
[1mStep[0m  [18/21], [94mLoss[0m : 2.83142
[1mStep[0m  [20/21], [94mLoss[0m : 2.67798

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.376, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76098
[1mStep[0m  [2/21], [94mLoss[0m : 2.60769
[1mStep[0m  [4/21], [94mLoss[0m : 2.62117
[1mStep[0m  [6/21], [94mLoss[0m : 2.68654
[1mStep[0m  [8/21], [94mLoss[0m : 2.83144
[1mStep[0m  [10/21], [94mLoss[0m : 2.74662
[1mStep[0m  [12/21], [94mLoss[0m : 2.80965
[1mStep[0m  [14/21], [94mLoss[0m : 2.68064
[1mStep[0m  [16/21], [94mLoss[0m : 2.73663
[1mStep[0m  [18/21], [94mLoss[0m : 2.78183
[1mStep[0m  [20/21], [94mLoss[0m : 2.78526

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.688, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68237
[1mStep[0m  [2/21], [94mLoss[0m : 2.84570
[1mStep[0m  [4/21], [94mLoss[0m : 2.79403
[1mStep[0m  [6/21], [94mLoss[0m : 2.76994
[1mStep[0m  [8/21], [94mLoss[0m : 2.64060
[1mStep[0m  [10/21], [94mLoss[0m : 2.62384
[1mStep[0m  [12/21], [94mLoss[0m : 2.78225
[1mStep[0m  [14/21], [94mLoss[0m : 2.97746
[1mStep[0m  [16/21], [94mLoss[0m : 2.62985
[1mStep[0m  [18/21], [94mLoss[0m : 2.63567
[1mStep[0m  [20/21], [94mLoss[0m : 2.60230

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76538
[1mStep[0m  [2/21], [94mLoss[0m : 2.77200
[1mStep[0m  [4/21], [94mLoss[0m : 2.58076
[1mStep[0m  [6/21], [94mLoss[0m : 2.71361
[1mStep[0m  [8/21], [94mLoss[0m : 2.61232
[1mStep[0m  [10/21], [94mLoss[0m : 2.63845
[1mStep[0m  [12/21], [94mLoss[0m : 2.67873
[1mStep[0m  [14/21], [94mLoss[0m : 2.62551
[1mStep[0m  [16/21], [94mLoss[0m : 2.75779
[1mStep[0m  [18/21], [94mLoss[0m : 2.73920
[1mStep[0m  [20/21], [94mLoss[0m : 2.68736

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69095
[1mStep[0m  [2/21], [94mLoss[0m : 2.62977
[1mStep[0m  [4/21], [94mLoss[0m : 2.72231
[1mStep[0m  [6/21], [94mLoss[0m : 2.80587
[1mStep[0m  [8/21], [94mLoss[0m : 2.72283
[1mStep[0m  [10/21], [94mLoss[0m : 2.66821
[1mStep[0m  [12/21], [94mLoss[0m : 2.56017
[1mStep[0m  [14/21], [94mLoss[0m : 2.74596
[1mStep[0m  [16/21], [94mLoss[0m : 2.83106
[1mStep[0m  [18/21], [94mLoss[0m : 2.91083
[1mStep[0m  [20/21], [94mLoss[0m : 2.65119

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53792
[1mStep[0m  [2/21], [94mLoss[0m : 2.76165
[1mStep[0m  [4/21], [94mLoss[0m : 2.53264
[1mStep[0m  [6/21], [94mLoss[0m : 2.75566
[1mStep[0m  [8/21], [94mLoss[0m : 2.63334
[1mStep[0m  [10/21], [94mLoss[0m : 2.74646
[1mStep[0m  [12/21], [94mLoss[0m : 2.82471
[1mStep[0m  [14/21], [94mLoss[0m : 2.62961
[1mStep[0m  [16/21], [94mLoss[0m : 2.57997
[1mStep[0m  [18/21], [94mLoss[0m : 2.63133
[1mStep[0m  [20/21], [94mLoss[0m : 2.78414

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.87199
[1mStep[0m  [2/21], [94mLoss[0m : 2.71537
[1mStep[0m  [4/21], [94mLoss[0m : 2.56433
[1mStep[0m  [6/21], [94mLoss[0m : 2.85070
[1mStep[0m  [8/21], [94mLoss[0m : 2.50239
[1mStep[0m  [10/21], [94mLoss[0m : 2.74861
[1mStep[0m  [12/21], [94mLoss[0m : 2.68975
[1mStep[0m  [14/21], [94mLoss[0m : 2.67708
[1mStep[0m  [16/21], [94mLoss[0m : 2.72330
[1mStep[0m  [18/21], [94mLoss[0m : 2.76904
[1mStep[0m  [20/21], [94mLoss[0m : 2.60050

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.684, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.358
====================================

Phase 1 - Evaluation MAE:  2.3581344059535434
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.64227
[1mStep[0m  [2/21], [94mLoss[0m : 2.75723
[1mStep[0m  [4/21], [94mLoss[0m : 2.66046
[1mStep[0m  [6/21], [94mLoss[0m : 2.84422
[1mStep[0m  [8/21], [94mLoss[0m : 2.77601
[1mStep[0m  [10/21], [94mLoss[0m : 2.77557
[1mStep[0m  [12/21], [94mLoss[0m : 2.73701
[1mStep[0m  [14/21], [94mLoss[0m : 2.63935
[1mStep[0m  [16/21], [94mLoss[0m : 2.70962
[1mStep[0m  [18/21], [94mLoss[0m : 2.64740
[1mStep[0m  [20/21], [94mLoss[0m : 2.70212

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76658
[1mStep[0m  [2/21], [94mLoss[0m : 2.57595
[1mStep[0m  [4/21], [94mLoss[0m : 2.65731
[1mStep[0m  [6/21], [94mLoss[0m : 2.78215
[1mStep[0m  [8/21], [94mLoss[0m : 2.84931
[1mStep[0m  [10/21], [94mLoss[0m : 2.62165
[1mStep[0m  [12/21], [94mLoss[0m : 2.48774
[1mStep[0m  [14/21], [94mLoss[0m : 2.71852
[1mStep[0m  [16/21], [94mLoss[0m : 2.77123
[1mStep[0m  [18/21], [94mLoss[0m : 2.56473
[1mStep[0m  [20/21], [94mLoss[0m : 2.72289

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67366
[1mStep[0m  [2/21], [94mLoss[0m : 2.70515
[1mStep[0m  [4/21], [94mLoss[0m : 2.43662
[1mStep[0m  [6/21], [94mLoss[0m : 2.58807
[1mStep[0m  [8/21], [94mLoss[0m : 2.73000
[1mStep[0m  [10/21], [94mLoss[0m : 2.59988
[1mStep[0m  [12/21], [94mLoss[0m : 2.60359
[1mStep[0m  [14/21], [94mLoss[0m : 2.59219
[1mStep[0m  [16/21], [94mLoss[0m : 2.39962
[1mStep[0m  [18/21], [94mLoss[0m : 2.66321
[1mStep[0m  [20/21], [94mLoss[0m : 2.66782

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49160
[1mStep[0m  [2/21], [94mLoss[0m : 2.75243
[1mStep[0m  [4/21], [94mLoss[0m : 2.63334
[1mStep[0m  [6/21], [94mLoss[0m : 2.65539
[1mStep[0m  [8/21], [94mLoss[0m : 2.72960
[1mStep[0m  [10/21], [94mLoss[0m : 2.65142
[1mStep[0m  [12/21], [94mLoss[0m : 2.68137
[1mStep[0m  [14/21], [94mLoss[0m : 2.54688
[1mStep[0m  [16/21], [94mLoss[0m : 2.59785
[1mStep[0m  [18/21], [94mLoss[0m : 2.50527
[1mStep[0m  [20/21], [94mLoss[0m : 2.61429

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66267
[1mStep[0m  [2/21], [94mLoss[0m : 2.64633
[1mStep[0m  [4/21], [94mLoss[0m : 2.71185
[1mStep[0m  [6/21], [94mLoss[0m : 2.66444
[1mStep[0m  [8/21], [94mLoss[0m : 2.58917
[1mStep[0m  [10/21], [94mLoss[0m : 2.55619
[1mStep[0m  [12/21], [94mLoss[0m : 2.69740
[1mStep[0m  [14/21], [94mLoss[0m : 2.72511
[1mStep[0m  [16/21], [94mLoss[0m : 2.66341
[1mStep[0m  [18/21], [94mLoss[0m : 2.54348
[1mStep[0m  [20/21], [94mLoss[0m : 2.71212

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61903
[1mStep[0m  [2/21], [94mLoss[0m : 2.55362
[1mStep[0m  [4/21], [94mLoss[0m : 2.51746
[1mStep[0m  [6/21], [94mLoss[0m : 2.58188
[1mStep[0m  [8/21], [94mLoss[0m : 2.61859
[1mStep[0m  [10/21], [94mLoss[0m : 2.38077
[1mStep[0m  [12/21], [94mLoss[0m : 2.66932
[1mStep[0m  [14/21], [94mLoss[0m : 2.63686
[1mStep[0m  [16/21], [94mLoss[0m : 2.44967
[1mStep[0m  [18/21], [94mLoss[0m : 2.66103
[1mStep[0m  [20/21], [94mLoss[0m : 2.60579

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55671
[1mStep[0m  [2/21], [94mLoss[0m : 2.58534
[1mStep[0m  [4/21], [94mLoss[0m : 2.41402
[1mStep[0m  [6/21], [94mLoss[0m : 2.67295
[1mStep[0m  [8/21], [94mLoss[0m : 2.56818
[1mStep[0m  [10/21], [94mLoss[0m : 2.47350
[1mStep[0m  [12/21], [94mLoss[0m : 2.53766
[1mStep[0m  [14/21], [94mLoss[0m : 2.57464
[1mStep[0m  [16/21], [94mLoss[0m : 2.50397
[1mStep[0m  [18/21], [94mLoss[0m : 2.52486
[1mStep[0m  [20/21], [94mLoss[0m : 2.75193

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56417
[1mStep[0m  [2/21], [94mLoss[0m : 2.51646
[1mStep[0m  [4/21], [94mLoss[0m : 2.53084
[1mStep[0m  [6/21], [94mLoss[0m : 2.64422
[1mStep[0m  [8/21], [94mLoss[0m : 2.46809
[1mStep[0m  [10/21], [94mLoss[0m : 2.68203
[1mStep[0m  [12/21], [94mLoss[0m : 2.60431
[1mStep[0m  [14/21], [94mLoss[0m : 2.34541
[1mStep[0m  [16/21], [94mLoss[0m : 2.43323
[1mStep[0m  [18/21], [94mLoss[0m : 2.67839
[1mStep[0m  [20/21], [94mLoss[0m : 2.35305

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67037
[1mStep[0m  [2/21], [94mLoss[0m : 2.49436
[1mStep[0m  [4/21], [94mLoss[0m : 2.57674
[1mStep[0m  [6/21], [94mLoss[0m : 2.54444
[1mStep[0m  [8/21], [94mLoss[0m : 2.53946
[1mStep[0m  [10/21], [94mLoss[0m : 2.43803
[1mStep[0m  [12/21], [94mLoss[0m : 2.58681
[1mStep[0m  [14/21], [94mLoss[0m : 2.37328
[1mStep[0m  [16/21], [94mLoss[0m : 2.45476
[1mStep[0m  [18/21], [94mLoss[0m : 2.45339
[1mStep[0m  [20/21], [94mLoss[0m : 2.55910

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37715
[1mStep[0m  [2/21], [94mLoss[0m : 2.45643
[1mStep[0m  [4/21], [94mLoss[0m : 2.58490
[1mStep[0m  [6/21], [94mLoss[0m : 2.59135
[1mStep[0m  [8/21], [94mLoss[0m : 2.51503
[1mStep[0m  [10/21], [94mLoss[0m : 2.46232
[1mStep[0m  [12/21], [94mLoss[0m : 2.47701
[1mStep[0m  [14/21], [94mLoss[0m : 2.47851
[1mStep[0m  [16/21], [94mLoss[0m : 2.54872
[1mStep[0m  [18/21], [94mLoss[0m : 2.48979
[1mStep[0m  [20/21], [94mLoss[0m : 2.54125

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49260
[1mStep[0m  [2/21], [94mLoss[0m : 2.63443
[1mStep[0m  [4/21], [94mLoss[0m : 2.27275
[1mStep[0m  [6/21], [94mLoss[0m : 2.39887
[1mStep[0m  [8/21], [94mLoss[0m : 2.56859
[1mStep[0m  [10/21], [94mLoss[0m : 2.46978
[1mStep[0m  [12/21], [94mLoss[0m : 2.54576
[1mStep[0m  [14/21], [94mLoss[0m : 2.48845
[1mStep[0m  [16/21], [94mLoss[0m : 2.61721
[1mStep[0m  [18/21], [94mLoss[0m : 2.43945
[1mStep[0m  [20/21], [94mLoss[0m : 2.46758

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29931
[1mStep[0m  [2/21], [94mLoss[0m : 2.56678
[1mStep[0m  [4/21], [94mLoss[0m : 2.39350
[1mStep[0m  [6/21], [94mLoss[0m : 2.32650
[1mStep[0m  [8/21], [94mLoss[0m : 2.42091
[1mStep[0m  [10/21], [94mLoss[0m : 2.42173
[1mStep[0m  [12/21], [94mLoss[0m : 2.48803
[1mStep[0m  [14/21], [94mLoss[0m : 2.55217
[1mStep[0m  [16/21], [94mLoss[0m : 2.52335
[1mStep[0m  [18/21], [94mLoss[0m : 2.49895
[1mStep[0m  [20/21], [94mLoss[0m : 2.52719

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32311
[1mStep[0m  [2/21], [94mLoss[0m : 2.49331
[1mStep[0m  [4/21], [94mLoss[0m : 2.45973
[1mStep[0m  [6/21], [94mLoss[0m : 2.31407
[1mStep[0m  [8/21], [94mLoss[0m : 2.50885
[1mStep[0m  [10/21], [94mLoss[0m : 2.41013
[1mStep[0m  [12/21], [94mLoss[0m : 2.38900
[1mStep[0m  [14/21], [94mLoss[0m : 2.48804
[1mStep[0m  [16/21], [94mLoss[0m : 2.33756
[1mStep[0m  [18/21], [94mLoss[0m : 2.51194
[1mStep[0m  [20/21], [94mLoss[0m : 2.32973

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31120
[1mStep[0m  [2/21], [94mLoss[0m : 2.38098
[1mStep[0m  [4/21], [94mLoss[0m : 2.47775
[1mStep[0m  [6/21], [94mLoss[0m : 2.37487
[1mStep[0m  [8/21], [94mLoss[0m : 2.37079
[1mStep[0m  [10/21], [94mLoss[0m : 2.75265
[1mStep[0m  [12/21], [94mLoss[0m : 2.44236
[1mStep[0m  [14/21], [94mLoss[0m : 2.28436
[1mStep[0m  [16/21], [94mLoss[0m : 2.42764
[1mStep[0m  [18/21], [94mLoss[0m : 2.48840
[1mStep[0m  [20/21], [94mLoss[0m : 2.33695

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40875
[1mStep[0m  [2/21], [94mLoss[0m : 2.40808
[1mStep[0m  [4/21], [94mLoss[0m : 2.34511
[1mStep[0m  [6/21], [94mLoss[0m : 2.40213
[1mStep[0m  [8/21], [94mLoss[0m : 2.53869
[1mStep[0m  [10/21], [94mLoss[0m : 2.34259
[1mStep[0m  [12/21], [94mLoss[0m : 2.46441
[1mStep[0m  [14/21], [94mLoss[0m : 2.24327
[1mStep[0m  [16/21], [94mLoss[0m : 2.37490
[1mStep[0m  [18/21], [94mLoss[0m : 2.40086
[1mStep[0m  [20/21], [94mLoss[0m : 2.36512

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24786
[1mStep[0m  [2/21], [94mLoss[0m : 2.35189
[1mStep[0m  [4/21], [94mLoss[0m : 2.36667
[1mStep[0m  [6/21], [94mLoss[0m : 2.22543
[1mStep[0m  [8/21], [94mLoss[0m : 2.24843
[1mStep[0m  [10/21], [94mLoss[0m : 2.16640
[1mStep[0m  [12/21], [94mLoss[0m : 2.39395
[1mStep[0m  [14/21], [94mLoss[0m : 2.42189
[1mStep[0m  [16/21], [94mLoss[0m : 2.33078
[1mStep[0m  [18/21], [94mLoss[0m : 2.34279
[1mStep[0m  [20/21], [94mLoss[0m : 2.58484

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29434
[1mStep[0m  [2/21], [94mLoss[0m : 2.44742
[1mStep[0m  [4/21], [94mLoss[0m : 2.30334
[1mStep[0m  [6/21], [94mLoss[0m : 2.47456
[1mStep[0m  [8/21], [94mLoss[0m : 2.26148
[1mStep[0m  [10/21], [94mLoss[0m : 2.26629
[1mStep[0m  [12/21], [94mLoss[0m : 2.36126
[1mStep[0m  [14/21], [94mLoss[0m : 2.35825
[1mStep[0m  [16/21], [94mLoss[0m : 2.44611
[1mStep[0m  [18/21], [94mLoss[0m : 2.34272
[1mStep[0m  [20/21], [94mLoss[0m : 2.43758

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19765
[1mStep[0m  [2/21], [94mLoss[0m : 2.39137
[1mStep[0m  [4/21], [94mLoss[0m : 2.41170
[1mStep[0m  [6/21], [94mLoss[0m : 2.29835
[1mStep[0m  [8/21], [94mLoss[0m : 2.29607
[1mStep[0m  [10/21], [94mLoss[0m : 2.39511
[1mStep[0m  [12/21], [94mLoss[0m : 2.41575
[1mStep[0m  [14/21], [94mLoss[0m : 2.30284
[1mStep[0m  [16/21], [94mLoss[0m : 2.41186
[1mStep[0m  [18/21], [94mLoss[0m : 2.28932
[1mStep[0m  [20/21], [94mLoss[0m : 2.35056

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36480
[1mStep[0m  [2/21], [94mLoss[0m : 2.32529
[1mStep[0m  [4/21], [94mLoss[0m : 2.23876
[1mStep[0m  [6/21], [94mLoss[0m : 2.16708
[1mStep[0m  [8/21], [94mLoss[0m : 2.35396
[1mStep[0m  [10/21], [94mLoss[0m : 2.22662
[1mStep[0m  [12/21], [94mLoss[0m : 2.17573
[1mStep[0m  [14/21], [94mLoss[0m : 2.31810
[1mStep[0m  [16/21], [94mLoss[0m : 2.22091
[1mStep[0m  [18/21], [94mLoss[0m : 2.21393
[1mStep[0m  [20/21], [94mLoss[0m : 2.33895

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40361
[1mStep[0m  [2/21], [94mLoss[0m : 2.20662
[1mStep[0m  [4/21], [94mLoss[0m : 2.33501
[1mStep[0m  [6/21], [94mLoss[0m : 2.26077
[1mStep[0m  [8/21], [94mLoss[0m : 2.27553
[1mStep[0m  [10/21], [94mLoss[0m : 2.27562
[1mStep[0m  [12/21], [94mLoss[0m : 2.28222
[1mStep[0m  [14/21], [94mLoss[0m : 2.25225
[1mStep[0m  [16/21], [94mLoss[0m : 2.15194
[1mStep[0m  [18/21], [94mLoss[0m : 2.25766
[1mStep[0m  [20/21], [94mLoss[0m : 2.21150

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.266, [92mTest[0m: 2.419, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28112
[1mStep[0m  [2/21], [94mLoss[0m : 2.30994
[1mStep[0m  [4/21], [94mLoss[0m : 2.19644
[1mStep[0m  [6/21], [94mLoss[0m : 2.14277
[1mStep[0m  [8/21], [94mLoss[0m : 2.29379
[1mStep[0m  [10/21], [94mLoss[0m : 2.08040
[1mStep[0m  [12/21], [94mLoss[0m : 2.36560
[1mStep[0m  [14/21], [94mLoss[0m : 2.13606
[1mStep[0m  [16/21], [94mLoss[0m : 2.26096
[1mStep[0m  [18/21], [94mLoss[0m : 2.24854
[1mStep[0m  [20/21], [94mLoss[0m : 2.35440

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.234, [92mTest[0m: 2.479, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23875
[1mStep[0m  [2/21], [94mLoss[0m : 2.28090
[1mStep[0m  [4/21], [94mLoss[0m : 2.07235
[1mStep[0m  [6/21], [94mLoss[0m : 2.30582
[1mStep[0m  [8/21], [94mLoss[0m : 2.27592
[1mStep[0m  [10/21], [94mLoss[0m : 2.39324
[1mStep[0m  [12/21], [94mLoss[0m : 2.30793
[1mStep[0m  [14/21], [94mLoss[0m : 2.28436
[1mStep[0m  [16/21], [94mLoss[0m : 2.10661
[1mStep[0m  [18/21], [94mLoss[0m : 2.06317
[1mStep[0m  [20/21], [94mLoss[0m : 2.05115

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18081
[1mStep[0m  [2/21], [94mLoss[0m : 2.14476
[1mStep[0m  [4/21], [94mLoss[0m : 2.25644
[1mStep[0m  [6/21], [94mLoss[0m : 2.28126
[1mStep[0m  [8/21], [94mLoss[0m : 2.22783
[1mStep[0m  [10/21], [94mLoss[0m : 2.16616
[1mStep[0m  [12/21], [94mLoss[0m : 2.06787
[1mStep[0m  [14/21], [94mLoss[0m : 2.16845
[1mStep[0m  [16/21], [94mLoss[0m : 2.34113
[1mStep[0m  [18/21], [94mLoss[0m : 2.10514
[1mStep[0m  [20/21], [94mLoss[0m : 2.23754

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.414, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07903
[1mStep[0m  [2/21], [94mLoss[0m : 2.21341
[1mStep[0m  [4/21], [94mLoss[0m : 2.15201
[1mStep[0m  [6/21], [94mLoss[0m : 2.18578
[1mStep[0m  [8/21], [94mLoss[0m : 2.11479
[1mStep[0m  [10/21], [94mLoss[0m : 2.13764
[1mStep[0m  [12/21], [94mLoss[0m : 2.26041
[1mStep[0m  [14/21], [94mLoss[0m : 2.19237
[1mStep[0m  [16/21], [94mLoss[0m : 2.15130
[1mStep[0m  [18/21], [94mLoss[0m : 2.20799
[1mStep[0m  [20/21], [94mLoss[0m : 2.08719

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.442, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13663
[1mStep[0m  [2/21], [94mLoss[0m : 2.18657
[1mStep[0m  [4/21], [94mLoss[0m : 2.22171
[1mStep[0m  [6/21], [94mLoss[0m : 2.19077
[1mStep[0m  [8/21], [94mLoss[0m : 2.23140
[1mStep[0m  [10/21], [94mLoss[0m : 2.05342
[1mStep[0m  [12/21], [94mLoss[0m : 2.07731
[1mStep[0m  [14/21], [94mLoss[0m : 2.08609
[1mStep[0m  [16/21], [94mLoss[0m : 2.14281
[1mStep[0m  [18/21], [94mLoss[0m : 2.09483
[1mStep[0m  [20/21], [94mLoss[0m : 2.15550

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13647
[1mStep[0m  [2/21], [94mLoss[0m : 2.18753
[1mStep[0m  [4/21], [94mLoss[0m : 2.20454
[1mStep[0m  [6/21], [94mLoss[0m : 2.08677
[1mStep[0m  [8/21], [94mLoss[0m : 2.15639
[1mStep[0m  [10/21], [94mLoss[0m : 1.99621
[1mStep[0m  [12/21], [94mLoss[0m : 2.07696
[1mStep[0m  [14/21], [94mLoss[0m : 2.18997
[1mStep[0m  [16/21], [94mLoss[0m : 2.10359
[1mStep[0m  [18/21], [94mLoss[0m : 2.05732
[1mStep[0m  [20/21], [94mLoss[0m : 2.09013

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.441, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06003
[1mStep[0m  [2/21], [94mLoss[0m : 2.18553
[1mStep[0m  [4/21], [94mLoss[0m : 1.99918
[1mStep[0m  [6/21], [94mLoss[0m : 2.15744
[1mStep[0m  [8/21], [94mLoss[0m : 2.05709
[1mStep[0m  [10/21], [94mLoss[0m : 2.14697
[1mStep[0m  [12/21], [94mLoss[0m : 2.04830
[1mStep[0m  [14/21], [94mLoss[0m : 2.06431
[1mStep[0m  [16/21], [94mLoss[0m : 2.12654
[1mStep[0m  [18/21], [94mLoss[0m : 2.19819
[1mStep[0m  [20/21], [94mLoss[0m : 2.29377

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.402, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17520
[1mStep[0m  [2/21], [94mLoss[0m : 2.21684
[1mStep[0m  [4/21], [94mLoss[0m : 2.04304
[1mStep[0m  [6/21], [94mLoss[0m : 2.14194
[1mStep[0m  [8/21], [94mLoss[0m : 2.11104
[1mStep[0m  [10/21], [94mLoss[0m : 2.14521
[1mStep[0m  [12/21], [94mLoss[0m : 2.14859
[1mStep[0m  [14/21], [94mLoss[0m : 2.07924
[1mStep[0m  [16/21], [94mLoss[0m : 2.19948
[1mStep[0m  [18/21], [94mLoss[0m : 1.94569
[1mStep[0m  [20/21], [94mLoss[0m : 2.08788

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.445, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15965
[1mStep[0m  [2/21], [94mLoss[0m : 2.02019
[1mStep[0m  [4/21], [94mLoss[0m : 2.11583
[1mStep[0m  [6/21], [94mLoss[0m : 2.16276
[1mStep[0m  [8/21], [94mLoss[0m : 2.12417
[1mStep[0m  [10/21], [94mLoss[0m : 2.06359
[1mStep[0m  [12/21], [94mLoss[0m : 2.01843
[1mStep[0m  [14/21], [94mLoss[0m : 2.05395
[1mStep[0m  [16/21], [94mLoss[0m : 2.10593
[1mStep[0m  [18/21], [94mLoss[0m : 2.15834
[1mStep[0m  [20/21], [94mLoss[0m : 2.08928

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.404, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.00332
[1mStep[0m  [2/21], [94mLoss[0m : 2.12194
[1mStep[0m  [4/21], [94mLoss[0m : 1.89124
[1mStep[0m  [6/21], [94mLoss[0m : 2.02736
[1mStep[0m  [8/21], [94mLoss[0m : 2.16010
[1mStep[0m  [10/21], [94mLoss[0m : 2.05098
[1mStep[0m  [12/21], [94mLoss[0m : 2.17394
[1mStep[0m  [14/21], [94mLoss[0m : 1.87202
[1mStep[0m  [16/21], [94mLoss[0m : 2.00439
[1mStep[0m  [18/21], [94mLoss[0m : 2.09538
[1mStep[0m  [20/21], [94mLoss[0m : 2.00787

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.399, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.430
====================================

Phase 2 - Evaluation MAE:  2.430095706667219
MAE score P1       2.358134
MAE score P2       2.430096
loss               2.051002
learning_rate          0.01
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay          0.001
Name: 13, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.28868
[1mStep[0m  [4/42], [94mLoss[0m : 8.80227
[1mStep[0m  [8/42], [94mLoss[0m : 6.57751
[1mStep[0m  [12/42], [94mLoss[0m : 4.68318
[1mStep[0m  [16/42], [94mLoss[0m : 3.36819
[1mStep[0m  [20/42], [94mLoss[0m : 3.00081
[1mStep[0m  [24/42], [94mLoss[0m : 2.57882
[1mStep[0m  [28/42], [94mLoss[0m : 2.40285
[1mStep[0m  [32/42], [94mLoss[0m : 2.72827
[1mStep[0m  [36/42], [94mLoss[0m : 2.51127
[1mStep[0m  [40/42], [94mLoss[0m : 2.49694

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.348, [92mTest[0m: 10.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39863
[1mStep[0m  [4/42], [94mLoss[0m : 2.37167
[1mStep[0m  [8/42], [94mLoss[0m : 2.68960
[1mStep[0m  [12/42], [94mLoss[0m : 2.38147
[1mStep[0m  [16/42], [94mLoss[0m : 2.64302
[1mStep[0m  [20/42], [94mLoss[0m : 2.37549
[1mStep[0m  [24/42], [94mLoss[0m : 2.83004
[1mStep[0m  [28/42], [94mLoss[0m : 2.36918
[1mStep[0m  [32/42], [94mLoss[0m : 2.48726
[1mStep[0m  [36/42], [94mLoss[0m : 2.64846
[1mStep[0m  [40/42], [94mLoss[0m : 2.47049

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41562
[1mStep[0m  [4/42], [94mLoss[0m : 2.46628
[1mStep[0m  [8/42], [94mLoss[0m : 2.36781
[1mStep[0m  [12/42], [94mLoss[0m : 2.53522
[1mStep[0m  [16/42], [94mLoss[0m : 2.31506
[1mStep[0m  [20/42], [94mLoss[0m : 2.29123
[1mStep[0m  [24/42], [94mLoss[0m : 2.34451
[1mStep[0m  [28/42], [94mLoss[0m : 2.63776
[1mStep[0m  [32/42], [94mLoss[0m : 2.70171
[1mStep[0m  [36/42], [94mLoss[0m : 2.44531
[1mStep[0m  [40/42], [94mLoss[0m : 2.35454

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30319
[1mStep[0m  [4/42], [94mLoss[0m : 2.48624
[1mStep[0m  [8/42], [94mLoss[0m : 2.51543
[1mStep[0m  [12/42], [94mLoss[0m : 2.34043
[1mStep[0m  [16/42], [94mLoss[0m : 2.58159
[1mStep[0m  [20/42], [94mLoss[0m : 2.28689
[1mStep[0m  [24/42], [94mLoss[0m : 2.66142
[1mStep[0m  [28/42], [94mLoss[0m : 2.40234
[1mStep[0m  [32/42], [94mLoss[0m : 2.44099
[1mStep[0m  [36/42], [94mLoss[0m : 2.43445
[1mStep[0m  [40/42], [94mLoss[0m : 2.57188

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54564
[1mStep[0m  [4/42], [94mLoss[0m : 2.63467
[1mStep[0m  [8/42], [94mLoss[0m : 2.53618
[1mStep[0m  [12/42], [94mLoss[0m : 2.23137
[1mStep[0m  [16/42], [94mLoss[0m : 2.45913
[1mStep[0m  [20/42], [94mLoss[0m : 2.37915
[1mStep[0m  [24/42], [94mLoss[0m : 2.37622
[1mStep[0m  [28/42], [94mLoss[0m : 2.42133
[1mStep[0m  [32/42], [94mLoss[0m : 2.59180
[1mStep[0m  [36/42], [94mLoss[0m : 2.50337
[1mStep[0m  [40/42], [94mLoss[0m : 2.30195

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48758
[1mStep[0m  [4/42], [94mLoss[0m : 2.36605
[1mStep[0m  [8/42], [94mLoss[0m : 2.47926
[1mStep[0m  [12/42], [94mLoss[0m : 2.49712
[1mStep[0m  [16/42], [94mLoss[0m : 2.44325
[1mStep[0m  [20/42], [94mLoss[0m : 2.33816
[1mStep[0m  [24/42], [94mLoss[0m : 2.37868
[1mStep[0m  [28/42], [94mLoss[0m : 2.54690
[1mStep[0m  [32/42], [94mLoss[0m : 2.29738
[1mStep[0m  [36/42], [94mLoss[0m : 2.42750
[1mStep[0m  [40/42], [94mLoss[0m : 2.45742

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56292
[1mStep[0m  [4/42], [94mLoss[0m : 2.43505
[1mStep[0m  [8/42], [94mLoss[0m : 2.32286
[1mStep[0m  [12/42], [94mLoss[0m : 2.43424
[1mStep[0m  [16/42], [94mLoss[0m : 2.37947
[1mStep[0m  [20/42], [94mLoss[0m : 2.46983
[1mStep[0m  [24/42], [94mLoss[0m : 2.42483
[1mStep[0m  [28/42], [94mLoss[0m : 2.24819
[1mStep[0m  [32/42], [94mLoss[0m : 2.42090
[1mStep[0m  [36/42], [94mLoss[0m : 2.28541
[1mStep[0m  [40/42], [94mLoss[0m : 2.47195

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33874
[1mStep[0m  [4/42], [94mLoss[0m : 2.29473
[1mStep[0m  [8/42], [94mLoss[0m : 2.35821
[1mStep[0m  [12/42], [94mLoss[0m : 2.40182
[1mStep[0m  [16/42], [94mLoss[0m : 2.27613
[1mStep[0m  [20/42], [94mLoss[0m : 2.52632
[1mStep[0m  [24/42], [94mLoss[0m : 2.34692
[1mStep[0m  [28/42], [94mLoss[0m : 2.37621
[1mStep[0m  [32/42], [94mLoss[0m : 2.38577
[1mStep[0m  [36/42], [94mLoss[0m : 2.47793
[1mStep[0m  [40/42], [94mLoss[0m : 2.24858

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44156
[1mStep[0m  [4/42], [94mLoss[0m : 2.35341
[1mStep[0m  [8/42], [94mLoss[0m : 2.25363
[1mStep[0m  [12/42], [94mLoss[0m : 2.27532
[1mStep[0m  [16/42], [94mLoss[0m : 2.41591
[1mStep[0m  [20/42], [94mLoss[0m : 2.52390
[1mStep[0m  [24/42], [94mLoss[0m : 2.50051
[1mStep[0m  [28/42], [94mLoss[0m : 2.42976
[1mStep[0m  [32/42], [94mLoss[0m : 2.31790
[1mStep[0m  [36/42], [94mLoss[0m : 2.45647
[1mStep[0m  [40/42], [94mLoss[0m : 2.31527

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51173
[1mStep[0m  [4/42], [94mLoss[0m : 2.53480
[1mStep[0m  [8/42], [94mLoss[0m : 2.33141
[1mStep[0m  [12/42], [94mLoss[0m : 2.35054
[1mStep[0m  [16/42], [94mLoss[0m : 2.37553
[1mStep[0m  [20/42], [94mLoss[0m : 2.36333
[1mStep[0m  [24/42], [94mLoss[0m : 2.22640
[1mStep[0m  [28/42], [94mLoss[0m : 2.36520
[1mStep[0m  [32/42], [94mLoss[0m : 2.57263
[1mStep[0m  [36/42], [94mLoss[0m : 2.51855
[1mStep[0m  [40/42], [94mLoss[0m : 2.48372

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35895
[1mStep[0m  [4/42], [94mLoss[0m : 2.52110
[1mStep[0m  [8/42], [94mLoss[0m : 2.27909
[1mStep[0m  [12/42], [94mLoss[0m : 2.68285
[1mStep[0m  [16/42], [94mLoss[0m : 2.40613
[1mStep[0m  [20/42], [94mLoss[0m : 2.39747
[1mStep[0m  [24/42], [94mLoss[0m : 2.52937
[1mStep[0m  [28/42], [94mLoss[0m : 2.26659
[1mStep[0m  [32/42], [94mLoss[0m : 2.32948
[1mStep[0m  [36/42], [94mLoss[0m : 2.43278
[1mStep[0m  [40/42], [94mLoss[0m : 2.46753

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45886
[1mStep[0m  [4/42], [94mLoss[0m : 2.44083
[1mStep[0m  [8/42], [94mLoss[0m : 2.44762
[1mStep[0m  [12/42], [94mLoss[0m : 2.34935
[1mStep[0m  [16/42], [94mLoss[0m : 2.52167
[1mStep[0m  [20/42], [94mLoss[0m : 2.48505
[1mStep[0m  [24/42], [94mLoss[0m : 2.58343
[1mStep[0m  [28/42], [94mLoss[0m : 2.25618
[1mStep[0m  [32/42], [94mLoss[0m : 2.37939
[1mStep[0m  [36/42], [94mLoss[0m : 2.42220
[1mStep[0m  [40/42], [94mLoss[0m : 2.42203

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51860
[1mStep[0m  [4/42], [94mLoss[0m : 2.27744
[1mStep[0m  [8/42], [94mLoss[0m : 2.47944
[1mStep[0m  [12/42], [94mLoss[0m : 2.59567
[1mStep[0m  [16/42], [94mLoss[0m : 2.47177
[1mStep[0m  [20/42], [94mLoss[0m : 2.36551
[1mStep[0m  [24/42], [94mLoss[0m : 2.54928
[1mStep[0m  [28/42], [94mLoss[0m : 2.50541
[1mStep[0m  [32/42], [94mLoss[0m : 2.39800
[1mStep[0m  [36/42], [94mLoss[0m : 2.41297
[1mStep[0m  [40/42], [94mLoss[0m : 2.35286

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47634
[1mStep[0m  [4/42], [94mLoss[0m : 2.34868
[1mStep[0m  [8/42], [94mLoss[0m : 2.52354
[1mStep[0m  [12/42], [94mLoss[0m : 2.30048
[1mStep[0m  [16/42], [94mLoss[0m : 2.43527
[1mStep[0m  [20/42], [94mLoss[0m : 2.56461
[1mStep[0m  [24/42], [94mLoss[0m : 2.45528
[1mStep[0m  [28/42], [94mLoss[0m : 2.49540
[1mStep[0m  [32/42], [94mLoss[0m : 2.36652
[1mStep[0m  [36/42], [94mLoss[0m : 2.64301
[1mStep[0m  [40/42], [94mLoss[0m : 2.31944

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38232
[1mStep[0m  [4/42], [94mLoss[0m : 2.47925
[1mStep[0m  [8/42], [94mLoss[0m : 2.64573
[1mStep[0m  [12/42], [94mLoss[0m : 2.62503
[1mStep[0m  [16/42], [94mLoss[0m : 2.57667
[1mStep[0m  [20/42], [94mLoss[0m : 2.26570
[1mStep[0m  [24/42], [94mLoss[0m : 2.38705
[1mStep[0m  [28/42], [94mLoss[0m : 2.25720
[1mStep[0m  [32/42], [94mLoss[0m : 2.43773
[1mStep[0m  [36/42], [94mLoss[0m : 2.31427
[1mStep[0m  [40/42], [94mLoss[0m : 2.19158

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46958
[1mStep[0m  [4/42], [94mLoss[0m : 2.41604
[1mStep[0m  [8/42], [94mLoss[0m : 2.54992
[1mStep[0m  [12/42], [94mLoss[0m : 2.54588
[1mStep[0m  [16/42], [94mLoss[0m : 2.49801
[1mStep[0m  [20/42], [94mLoss[0m : 2.54573
[1mStep[0m  [24/42], [94mLoss[0m : 2.42562
[1mStep[0m  [28/42], [94mLoss[0m : 2.53871
[1mStep[0m  [32/42], [94mLoss[0m : 2.44018
[1mStep[0m  [36/42], [94mLoss[0m : 2.32078
[1mStep[0m  [40/42], [94mLoss[0m : 2.38597

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52841
[1mStep[0m  [4/42], [94mLoss[0m : 2.32142
[1mStep[0m  [8/42], [94mLoss[0m : 2.52855
[1mStep[0m  [12/42], [94mLoss[0m : 2.42843
[1mStep[0m  [16/42], [94mLoss[0m : 2.40519
[1mStep[0m  [20/42], [94mLoss[0m : 2.31011
[1mStep[0m  [24/42], [94mLoss[0m : 2.45682
[1mStep[0m  [28/42], [94mLoss[0m : 2.30226
[1mStep[0m  [32/42], [94mLoss[0m : 2.39310
[1mStep[0m  [36/42], [94mLoss[0m : 2.51508
[1mStep[0m  [40/42], [94mLoss[0m : 2.29598

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25746
[1mStep[0m  [4/42], [94mLoss[0m : 2.44088
[1mStep[0m  [8/42], [94mLoss[0m : 2.22617
[1mStep[0m  [12/42], [94mLoss[0m : 2.54481
[1mStep[0m  [16/42], [94mLoss[0m : 2.38434
[1mStep[0m  [20/42], [94mLoss[0m : 2.30274
[1mStep[0m  [24/42], [94mLoss[0m : 2.53195
[1mStep[0m  [28/42], [94mLoss[0m : 2.50756
[1mStep[0m  [32/42], [94mLoss[0m : 2.64962
[1mStep[0m  [36/42], [94mLoss[0m : 2.39971
[1mStep[0m  [40/42], [94mLoss[0m : 2.41814

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56118
[1mStep[0m  [4/42], [94mLoss[0m : 2.45668
[1mStep[0m  [8/42], [94mLoss[0m : 2.30529
[1mStep[0m  [12/42], [94mLoss[0m : 2.30002
[1mStep[0m  [16/42], [94mLoss[0m : 2.68039
[1mStep[0m  [20/42], [94mLoss[0m : 2.26955
[1mStep[0m  [24/42], [94mLoss[0m : 2.36427
[1mStep[0m  [28/42], [94mLoss[0m : 2.32274
[1mStep[0m  [32/42], [94mLoss[0m : 2.64011
[1mStep[0m  [36/42], [94mLoss[0m : 2.17597
[1mStep[0m  [40/42], [94mLoss[0m : 2.49447

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68963
[1mStep[0m  [4/42], [94mLoss[0m : 2.12701
[1mStep[0m  [8/42], [94mLoss[0m : 2.47586
[1mStep[0m  [12/42], [94mLoss[0m : 2.31660
[1mStep[0m  [16/42], [94mLoss[0m : 2.34945
[1mStep[0m  [20/42], [94mLoss[0m : 2.29700
[1mStep[0m  [24/42], [94mLoss[0m : 2.42510
[1mStep[0m  [28/42], [94mLoss[0m : 2.41111
[1mStep[0m  [32/42], [94mLoss[0m : 2.56174
[1mStep[0m  [36/42], [94mLoss[0m : 2.27373
[1mStep[0m  [40/42], [94mLoss[0m : 2.40380

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30673
[1mStep[0m  [4/42], [94mLoss[0m : 2.28444
[1mStep[0m  [8/42], [94mLoss[0m : 2.29682
[1mStep[0m  [12/42], [94mLoss[0m : 2.51660
[1mStep[0m  [16/42], [94mLoss[0m : 2.49816
[1mStep[0m  [20/42], [94mLoss[0m : 2.47511
[1mStep[0m  [24/42], [94mLoss[0m : 2.28210
[1mStep[0m  [28/42], [94mLoss[0m : 2.57494
[1mStep[0m  [32/42], [94mLoss[0m : 2.40344
[1mStep[0m  [36/42], [94mLoss[0m : 2.35037
[1mStep[0m  [40/42], [94mLoss[0m : 2.55194

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52798
[1mStep[0m  [4/42], [94mLoss[0m : 2.35714
[1mStep[0m  [8/42], [94mLoss[0m : 2.28521
[1mStep[0m  [12/42], [94mLoss[0m : 2.47755
[1mStep[0m  [16/42], [94mLoss[0m : 2.53968
[1mStep[0m  [20/42], [94mLoss[0m : 2.32359
[1mStep[0m  [24/42], [94mLoss[0m : 2.27417
[1mStep[0m  [28/42], [94mLoss[0m : 2.38693
[1mStep[0m  [32/42], [94mLoss[0m : 2.47366
[1mStep[0m  [36/42], [94mLoss[0m : 2.27791
[1mStep[0m  [40/42], [94mLoss[0m : 2.50680

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54472
[1mStep[0m  [4/42], [94mLoss[0m : 2.49514
[1mStep[0m  [8/42], [94mLoss[0m : 2.65949
[1mStep[0m  [12/42], [94mLoss[0m : 2.58071
[1mStep[0m  [16/42], [94mLoss[0m : 2.58604
[1mStep[0m  [20/42], [94mLoss[0m : 2.20260
[1mStep[0m  [24/42], [94mLoss[0m : 2.31212
[1mStep[0m  [28/42], [94mLoss[0m : 2.46719
[1mStep[0m  [32/42], [94mLoss[0m : 2.39256
[1mStep[0m  [36/42], [94mLoss[0m : 2.54162
[1mStep[0m  [40/42], [94mLoss[0m : 2.38978

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45845
[1mStep[0m  [4/42], [94mLoss[0m : 2.38245
[1mStep[0m  [8/42], [94mLoss[0m : 2.25523
[1mStep[0m  [12/42], [94mLoss[0m : 2.49884
[1mStep[0m  [16/42], [94mLoss[0m : 2.11767
[1mStep[0m  [20/42], [94mLoss[0m : 2.44363
[1mStep[0m  [24/42], [94mLoss[0m : 2.18855
[1mStep[0m  [28/42], [94mLoss[0m : 2.50949
[1mStep[0m  [32/42], [94mLoss[0m : 2.55863
[1mStep[0m  [36/42], [94mLoss[0m : 2.30439
[1mStep[0m  [40/42], [94mLoss[0m : 2.42892

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45827
[1mStep[0m  [4/42], [94mLoss[0m : 2.81387
[1mStep[0m  [8/42], [94mLoss[0m : 2.44898
[1mStep[0m  [12/42], [94mLoss[0m : 2.51994
[1mStep[0m  [16/42], [94mLoss[0m : 2.52816
[1mStep[0m  [20/42], [94mLoss[0m : 2.52665
[1mStep[0m  [24/42], [94mLoss[0m : 2.35133
[1mStep[0m  [28/42], [94mLoss[0m : 2.53231
[1mStep[0m  [32/42], [94mLoss[0m : 2.19441
[1mStep[0m  [36/42], [94mLoss[0m : 2.44256
[1mStep[0m  [40/42], [94mLoss[0m : 2.34869

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49893
[1mStep[0m  [4/42], [94mLoss[0m : 2.20627
[1mStep[0m  [8/42], [94mLoss[0m : 2.42092
[1mStep[0m  [12/42], [94mLoss[0m : 2.35140
[1mStep[0m  [16/42], [94mLoss[0m : 2.28326
[1mStep[0m  [20/42], [94mLoss[0m : 2.66716
[1mStep[0m  [24/42], [94mLoss[0m : 2.77245
[1mStep[0m  [28/42], [94mLoss[0m : 2.25706
[1mStep[0m  [32/42], [94mLoss[0m : 2.61696
[1mStep[0m  [36/42], [94mLoss[0m : 2.45055
[1mStep[0m  [40/42], [94mLoss[0m : 2.40264

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28013
[1mStep[0m  [4/42], [94mLoss[0m : 2.35294
[1mStep[0m  [8/42], [94mLoss[0m : 2.51482
[1mStep[0m  [12/42], [94mLoss[0m : 2.31256
[1mStep[0m  [16/42], [94mLoss[0m : 2.32254
[1mStep[0m  [20/42], [94mLoss[0m : 2.40002
[1mStep[0m  [24/42], [94mLoss[0m : 2.29646
[1mStep[0m  [28/42], [94mLoss[0m : 2.29981
[1mStep[0m  [32/42], [94mLoss[0m : 2.60761
[1mStep[0m  [36/42], [94mLoss[0m : 2.45016
[1mStep[0m  [40/42], [94mLoss[0m : 2.38076

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38696
[1mStep[0m  [4/42], [94mLoss[0m : 2.61969
[1mStep[0m  [8/42], [94mLoss[0m : 2.32330
[1mStep[0m  [12/42], [94mLoss[0m : 2.42467
[1mStep[0m  [16/42], [94mLoss[0m : 2.28146
[1mStep[0m  [20/42], [94mLoss[0m : 2.20526
[1mStep[0m  [24/42], [94mLoss[0m : 2.49422
[1mStep[0m  [28/42], [94mLoss[0m : 2.58861
[1mStep[0m  [32/42], [94mLoss[0m : 2.30193
[1mStep[0m  [36/42], [94mLoss[0m : 2.41838
[1mStep[0m  [40/42], [94mLoss[0m : 2.40874

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35531
[1mStep[0m  [4/42], [94mLoss[0m : 2.46893
[1mStep[0m  [8/42], [94mLoss[0m : 2.34403
[1mStep[0m  [12/42], [94mLoss[0m : 2.20915
[1mStep[0m  [16/42], [94mLoss[0m : 2.39719
[1mStep[0m  [20/42], [94mLoss[0m : 2.45247
[1mStep[0m  [24/42], [94mLoss[0m : 2.51207
[1mStep[0m  [28/42], [94mLoss[0m : 2.49476
[1mStep[0m  [32/42], [94mLoss[0m : 2.37488
[1mStep[0m  [36/42], [94mLoss[0m : 2.53057
[1mStep[0m  [40/42], [94mLoss[0m : 2.50271

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52196
[1mStep[0m  [4/42], [94mLoss[0m : 2.45555
[1mStep[0m  [8/42], [94mLoss[0m : 2.58310
[1mStep[0m  [12/42], [94mLoss[0m : 2.47268
[1mStep[0m  [16/42], [94mLoss[0m : 2.53633
[1mStep[0m  [20/42], [94mLoss[0m : 2.63809
[1mStep[0m  [24/42], [94mLoss[0m : 2.35354
[1mStep[0m  [28/42], [94mLoss[0m : 2.39125
[1mStep[0m  [32/42], [94mLoss[0m : 2.25419
[1mStep[0m  [36/42], [94mLoss[0m : 2.50099
[1mStep[0m  [40/42], [94mLoss[0m : 2.34844

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.323
====================================

Phase 1 - Evaluation MAE:  2.3233853919165477
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.40191
[1mStep[0m  [4/42], [94mLoss[0m : 2.16172
[1mStep[0m  [8/42], [94mLoss[0m : 2.42971
[1mStep[0m  [12/42], [94mLoss[0m : 2.42674
[1mStep[0m  [16/42], [94mLoss[0m : 2.36966
[1mStep[0m  [20/42], [94mLoss[0m : 2.45903
[1mStep[0m  [24/42], [94mLoss[0m : 2.30575
[1mStep[0m  [28/42], [94mLoss[0m : 2.45717
[1mStep[0m  [32/42], [94mLoss[0m : 2.49871
[1mStep[0m  [36/42], [94mLoss[0m : 2.53149
[1mStep[0m  [40/42], [94mLoss[0m : 2.26511

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40337
[1mStep[0m  [4/42], [94mLoss[0m : 2.44695
[1mStep[0m  [8/42], [94mLoss[0m : 2.15138
[1mStep[0m  [12/42], [94mLoss[0m : 2.55463
[1mStep[0m  [16/42], [94mLoss[0m : 2.43628
[1mStep[0m  [20/42], [94mLoss[0m : 2.45191
[1mStep[0m  [24/42], [94mLoss[0m : 2.50195
[1mStep[0m  [28/42], [94mLoss[0m : 2.18381
[1mStep[0m  [32/42], [94mLoss[0m : 2.27076
[1mStep[0m  [36/42], [94mLoss[0m : 2.11688
[1mStep[0m  [40/42], [94mLoss[0m : 2.43231

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23973
[1mStep[0m  [4/42], [94mLoss[0m : 2.21764
[1mStep[0m  [8/42], [94mLoss[0m : 2.33265
[1mStep[0m  [12/42], [94mLoss[0m : 2.08511
[1mStep[0m  [16/42], [94mLoss[0m : 2.30175
[1mStep[0m  [20/42], [94mLoss[0m : 2.09093
[1mStep[0m  [24/42], [94mLoss[0m : 2.46209
[1mStep[0m  [28/42], [94mLoss[0m : 2.41089
[1mStep[0m  [32/42], [94mLoss[0m : 2.16124
[1mStep[0m  [36/42], [94mLoss[0m : 2.21259
[1mStep[0m  [40/42], [94mLoss[0m : 2.40014

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42419
[1mStep[0m  [4/42], [94mLoss[0m : 2.33952
[1mStep[0m  [8/42], [94mLoss[0m : 2.17774
[1mStep[0m  [12/42], [94mLoss[0m : 2.31908
[1mStep[0m  [16/42], [94mLoss[0m : 2.30467
[1mStep[0m  [20/42], [94mLoss[0m : 2.25590
[1mStep[0m  [24/42], [94mLoss[0m : 2.34452
[1mStep[0m  [28/42], [94mLoss[0m : 2.39812
[1mStep[0m  [32/42], [94mLoss[0m : 2.07993
[1mStep[0m  [36/42], [94mLoss[0m : 2.27580
[1mStep[0m  [40/42], [94mLoss[0m : 2.30833

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16688
[1mStep[0m  [4/42], [94mLoss[0m : 2.32765
[1mStep[0m  [8/42], [94mLoss[0m : 2.31209
[1mStep[0m  [12/42], [94mLoss[0m : 2.32033
[1mStep[0m  [16/42], [94mLoss[0m : 2.34716
[1mStep[0m  [20/42], [94mLoss[0m : 2.06467
[1mStep[0m  [24/42], [94mLoss[0m : 2.27884
[1mStep[0m  [28/42], [94mLoss[0m : 2.21920
[1mStep[0m  [32/42], [94mLoss[0m : 2.14495
[1mStep[0m  [36/42], [94mLoss[0m : 2.20079
[1mStep[0m  [40/42], [94mLoss[0m : 2.30615

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17060
[1mStep[0m  [4/42], [94mLoss[0m : 2.09156
[1mStep[0m  [8/42], [94mLoss[0m : 2.08057
[1mStep[0m  [12/42], [94mLoss[0m : 2.14100
[1mStep[0m  [16/42], [94mLoss[0m : 2.19439
[1mStep[0m  [20/42], [94mLoss[0m : 2.42687
[1mStep[0m  [24/42], [94mLoss[0m : 2.17251
[1mStep[0m  [28/42], [94mLoss[0m : 2.17298
[1mStep[0m  [32/42], [94mLoss[0m : 2.27157
[1mStep[0m  [36/42], [94mLoss[0m : 2.42828
[1mStep[0m  [40/42], [94mLoss[0m : 2.34378

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04183
[1mStep[0m  [4/42], [94mLoss[0m : 2.03514
[1mStep[0m  [8/42], [94mLoss[0m : 2.14873
[1mStep[0m  [12/42], [94mLoss[0m : 2.20223
[1mStep[0m  [16/42], [94mLoss[0m : 1.94317
[1mStep[0m  [20/42], [94mLoss[0m : 2.09529
[1mStep[0m  [24/42], [94mLoss[0m : 1.81320
[1mStep[0m  [28/42], [94mLoss[0m : 2.28403
[1mStep[0m  [32/42], [94mLoss[0m : 2.00234
[1mStep[0m  [36/42], [94mLoss[0m : 2.34254
[1mStep[0m  [40/42], [94mLoss[0m : 2.09247

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05393
[1mStep[0m  [4/42], [94mLoss[0m : 2.17904
[1mStep[0m  [8/42], [94mLoss[0m : 2.00796
[1mStep[0m  [12/42], [94mLoss[0m : 1.82483
[1mStep[0m  [16/42], [94mLoss[0m : 1.97309
[1mStep[0m  [20/42], [94mLoss[0m : 2.00418
[1mStep[0m  [24/42], [94mLoss[0m : 1.88107
[1mStep[0m  [28/42], [94mLoss[0m : 1.89935
[1mStep[0m  [32/42], [94mLoss[0m : 1.99037
[1mStep[0m  [36/42], [94mLoss[0m : 2.04592
[1mStep[0m  [40/42], [94mLoss[0m : 1.92916

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76108
[1mStep[0m  [4/42], [94mLoss[0m : 1.88150
[1mStep[0m  [8/42], [94mLoss[0m : 2.05681
[1mStep[0m  [12/42], [94mLoss[0m : 1.91246
[1mStep[0m  [16/42], [94mLoss[0m : 1.88227
[1mStep[0m  [20/42], [94mLoss[0m : 2.08930
[1mStep[0m  [24/42], [94mLoss[0m : 2.11211
[1mStep[0m  [28/42], [94mLoss[0m : 1.89774
[1mStep[0m  [32/42], [94mLoss[0m : 1.86672
[1mStep[0m  [36/42], [94mLoss[0m : 1.95143
[1mStep[0m  [40/42], [94mLoss[0m : 1.91733

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90256
[1mStep[0m  [4/42], [94mLoss[0m : 1.86475
[1mStep[0m  [8/42], [94mLoss[0m : 2.15449
[1mStep[0m  [12/42], [94mLoss[0m : 1.87717
[1mStep[0m  [16/42], [94mLoss[0m : 1.78875
[1mStep[0m  [20/42], [94mLoss[0m : 1.74381
[1mStep[0m  [24/42], [94mLoss[0m : 1.79821
[1mStep[0m  [28/42], [94mLoss[0m : 1.88734
[1mStep[0m  [32/42], [94mLoss[0m : 1.87215
[1mStep[0m  [36/42], [94mLoss[0m : 1.91554
[1mStep[0m  [40/42], [94mLoss[0m : 1.78764

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94598
[1mStep[0m  [4/42], [94mLoss[0m : 1.87624
[1mStep[0m  [8/42], [94mLoss[0m : 1.77234
[1mStep[0m  [12/42], [94mLoss[0m : 1.83156
[1mStep[0m  [16/42], [94mLoss[0m : 1.87338
[1mStep[0m  [20/42], [94mLoss[0m : 1.89809
[1mStep[0m  [24/42], [94mLoss[0m : 1.78401
[1mStep[0m  [28/42], [94mLoss[0m : 1.90917
[1mStep[0m  [32/42], [94mLoss[0m : 1.64620
[1mStep[0m  [36/42], [94mLoss[0m : 2.06293
[1mStep[0m  [40/42], [94mLoss[0m : 1.97566

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87170
[1mStep[0m  [4/42], [94mLoss[0m : 1.72505
[1mStep[0m  [8/42], [94mLoss[0m : 1.76836
[1mStep[0m  [12/42], [94mLoss[0m : 1.70744
[1mStep[0m  [16/42], [94mLoss[0m : 1.78776
[1mStep[0m  [20/42], [94mLoss[0m : 1.69034
[1mStep[0m  [24/42], [94mLoss[0m : 1.87703
[1mStep[0m  [28/42], [94mLoss[0m : 1.86570
[1mStep[0m  [32/42], [94mLoss[0m : 2.06763
[1mStep[0m  [36/42], [94mLoss[0m : 1.82468
[1mStep[0m  [40/42], [94mLoss[0m : 1.77073

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79155
[1mStep[0m  [4/42], [94mLoss[0m : 1.86009
[1mStep[0m  [8/42], [94mLoss[0m : 1.69163
[1mStep[0m  [12/42], [94mLoss[0m : 1.78188
[1mStep[0m  [16/42], [94mLoss[0m : 1.92655
[1mStep[0m  [20/42], [94mLoss[0m : 1.67782
[1mStep[0m  [24/42], [94mLoss[0m : 1.80847
[1mStep[0m  [28/42], [94mLoss[0m : 1.73001
[1mStep[0m  [32/42], [94mLoss[0m : 1.86876
[1mStep[0m  [36/42], [94mLoss[0m : 1.84160
[1mStep[0m  [40/42], [94mLoss[0m : 1.77726

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64752
[1mStep[0m  [4/42], [94mLoss[0m : 1.71345
[1mStep[0m  [8/42], [94mLoss[0m : 1.80370
[1mStep[0m  [12/42], [94mLoss[0m : 1.67464
[1mStep[0m  [16/42], [94mLoss[0m : 1.91481
[1mStep[0m  [20/42], [94mLoss[0m : 1.68810
[1mStep[0m  [24/42], [94mLoss[0m : 1.62354
[1mStep[0m  [28/42], [94mLoss[0m : 1.72547
[1mStep[0m  [32/42], [94mLoss[0m : 1.71085
[1mStep[0m  [36/42], [94mLoss[0m : 1.92742
[1mStep[0m  [40/42], [94mLoss[0m : 1.80583

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76584
[1mStep[0m  [4/42], [94mLoss[0m : 1.65878
[1mStep[0m  [8/42], [94mLoss[0m : 1.67606
[1mStep[0m  [12/42], [94mLoss[0m : 1.78049
[1mStep[0m  [16/42], [94mLoss[0m : 1.64465
[1mStep[0m  [20/42], [94mLoss[0m : 1.74929
[1mStep[0m  [24/42], [94mLoss[0m : 1.71139
[1mStep[0m  [28/42], [94mLoss[0m : 1.52310
[1mStep[0m  [32/42], [94mLoss[0m : 1.71214
[1mStep[0m  [36/42], [94mLoss[0m : 1.77472
[1mStep[0m  [40/42], [94mLoss[0m : 1.73428

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.559, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81469
[1mStep[0m  [4/42], [94mLoss[0m : 1.69409
[1mStep[0m  [8/42], [94mLoss[0m : 1.85187
[1mStep[0m  [12/42], [94mLoss[0m : 1.60418
[1mStep[0m  [16/42], [94mLoss[0m : 1.83156
[1mStep[0m  [20/42], [94mLoss[0m : 1.68518
[1mStep[0m  [24/42], [94mLoss[0m : 1.68807
[1mStep[0m  [28/42], [94mLoss[0m : 1.73282
[1mStep[0m  [32/42], [94mLoss[0m : 1.67132
[1mStep[0m  [36/42], [94mLoss[0m : 1.70794
[1mStep[0m  [40/42], [94mLoss[0m : 1.65156

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64924
[1mStep[0m  [4/42], [94mLoss[0m : 1.80519
[1mStep[0m  [8/42], [94mLoss[0m : 1.48358
[1mStep[0m  [12/42], [94mLoss[0m : 1.66400
[1mStep[0m  [16/42], [94mLoss[0m : 1.52188
[1mStep[0m  [20/42], [94mLoss[0m : 1.61684
[1mStep[0m  [24/42], [94mLoss[0m : 1.69949
[1mStep[0m  [28/42], [94mLoss[0m : 1.79628
[1mStep[0m  [32/42], [94mLoss[0m : 1.56916
[1mStep[0m  [36/42], [94mLoss[0m : 1.67290
[1mStep[0m  [40/42], [94mLoss[0m : 1.66886

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.649, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48552
[1mStep[0m  [4/42], [94mLoss[0m : 1.47221
[1mStep[0m  [8/42], [94mLoss[0m : 1.49846
[1mStep[0m  [12/42], [94mLoss[0m : 1.57487
[1mStep[0m  [16/42], [94mLoss[0m : 1.50309
[1mStep[0m  [20/42], [94mLoss[0m : 1.61271
[1mStep[0m  [24/42], [94mLoss[0m : 1.79154
[1mStep[0m  [28/42], [94mLoss[0m : 1.80203
[1mStep[0m  [32/42], [94mLoss[0m : 1.55865
[1mStep[0m  [36/42], [94mLoss[0m : 1.55847
[1mStep[0m  [40/42], [94mLoss[0m : 1.65481

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.576, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43122
[1mStep[0m  [4/42], [94mLoss[0m : 1.65057
[1mStep[0m  [8/42], [94mLoss[0m : 1.46040
[1mStep[0m  [12/42], [94mLoss[0m : 1.46288
[1mStep[0m  [16/42], [94mLoss[0m : 1.55191
[1mStep[0m  [20/42], [94mLoss[0m : 1.43273
[1mStep[0m  [24/42], [94mLoss[0m : 1.69884
[1mStep[0m  [28/42], [94mLoss[0m : 1.65187
[1mStep[0m  [32/42], [94mLoss[0m : 1.59298
[1mStep[0m  [36/42], [94mLoss[0m : 1.48087
[1mStep[0m  [40/42], [94mLoss[0m : 1.54016

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59552
[1mStep[0m  [4/42], [94mLoss[0m : 1.54413
[1mStep[0m  [8/42], [94mLoss[0m : 1.30776
[1mStep[0m  [12/42], [94mLoss[0m : 1.45602
[1mStep[0m  [16/42], [94mLoss[0m : 1.56932
[1mStep[0m  [20/42], [94mLoss[0m : 1.58379
[1mStep[0m  [24/42], [94mLoss[0m : 1.55941
[1mStep[0m  [28/42], [94mLoss[0m : 1.39547
[1mStep[0m  [32/42], [94mLoss[0m : 1.71441
[1mStep[0m  [36/42], [94mLoss[0m : 1.56622
[1mStep[0m  [40/42], [94mLoss[0m : 1.52383

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.524, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56641
[1mStep[0m  [4/42], [94mLoss[0m : 1.35971
[1mStep[0m  [8/42], [94mLoss[0m : 1.33825
[1mStep[0m  [12/42], [94mLoss[0m : 1.50499
[1mStep[0m  [16/42], [94mLoss[0m : 1.40848
[1mStep[0m  [20/42], [94mLoss[0m : 1.48195
[1mStep[0m  [24/42], [94mLoss[0m : 1.40243
[1mStep[0m  [28/42], [94mLoss[0m : 1.62903
[1mStep[0m  [32/42], [94mLoss[0m : 1.62025
[1mStep[0m  [36/42], [94mLoss[0m : 1.54734
[1mStep[0m  [40/42], [94mLoss[0m : 1.46744

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.28519
[1mStep[0m  [4/42], [94mLoss[0m : 1.45424
[1mStep[0m  [8/42], [94mLoss[0m : 1.46494
[1mStep[0m  [12/42], [94mLoss[0m : 1.54517
[1mStep[0m  [16/42], [94mLoss[0m : 1.58871
[1mStep[0m  [20/42], [94mLoss[0m : 1.47921
[1mStep[0m  [24/42], [94mLoss[0m : 1.46975
[1mStep[0m  [28/42], [94mLoss[0m : 1.48907
[1mStep[0m  [32/42], [94mLoss[0m : 1.37994
[1mStep[0m  [36/42], [94mLoss[0m : 1.44172
[1mStep[0m  [40/42], [94mLoss[0m : 1.56380

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36797
[1mStep[0m  [4/42], [94mLoss[0m : 1.24674
[1mStep[0m  [8/42], [94mLoss[0m : 1.40826
[1mStep[0m  [12/42], [94mLoss[0m : 1.34839
[1mStep[0m  [16/42], [94mLoss[0m : 1.21573
[1mStep[0m  [20/42], [94mLoss[0m : 1.37818
[1mStep[0m  [24/42], [94mLoss[0m : 1.56695
[1mStep[0m  [28/42], [94mLoss[0m : 1.53505
[1mStep[0m  [32/42], [94mLoss[0m : 1.41812
[1mStep[0m  [36/42], [94mLoss[0m : 1.46926
[1mStep[0m  [40/42], [94mLoss[0m : 1.49947

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43485
[1mStep[0m  [4/42], [94mLoss[0m : 1.37704
[1mStep[0m  [8/42], [94mLoss[0m : 1.33076
[1mStep[0m  [12/42], [94mLoss[0m : 1.49146
[1mStep[0m  [16/42], [94mLoss[0m : 1.51250
[1mStep[0m  [20/42], [94mLoss[0m : 1.28832
[1mStep[0m  [24/42], [94mLoss[0m : 1.47122
[1mStep[0m  [28/42], [94mLoss[0m : 1.28768
[1mStep[0m  [32/42], [94mLoss[0m : 1.58637
[1mStep[0m  [36/42], [94mLoss[0m : 1.47659
[1mStep[0m  [40/42], [94mLoss[0m : 1.37120

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38043
[1mStep[0m  [4/42], [94mLoss[0m : 1.44889
[1mStep[0m  [8/42], [94mLoss[0m : 1.34991
[1mStep[0m  [12/42], [94mLoss[0m : 1.35425
[1mStep[0m  [16/42], [94mLoss[0m : 1.45068
[1mStep[0m  [20/42], [94mLoss[0m : 1.36684
[1mStep[0m  [24/42], [94mLoss[0m : 1.40935
[1mStep[0m  [28/42], [94mLoss[0m : 1.47088
[1mStep[0m  [32/42], [94mLoss[0m : 1.33720
[1mStep[0m  [36/42], [94mLoss[0m : 1.37539
[1mStep[0m  [40/42], [94mLoss[0m : 1.49305

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.393, [92mTest[0m: 2.531, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.499
====================================

Phase 2 - Evaluation MAE:  2.4986934661865234
MAE score P1      2.323385
MAE score P2      2.498693
loss              1.393427
learning_rate         0.01
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.08826
[1mStep[0m  [2/21], [94mLoss[0m : 10.93222
[1mStep[0m  [4/21], [94mLoss[0m : 10.86658
[1mStep[0m  [6/21], [94mLoss[0m : 10.64245
[1mStep[0m  [8/21], [94mLoss[0m : 10.84092
[1mStep[0m  [10/21], [94mLoss[0m : 10.26549
[1mStep[0m  [12/21], [94mLoss[0m : 10.29154
[1mStep[0m  [14/21], [94mLoss[0m : 10.01449
[1mStep[0m  [16/21], [94mLoss[0m : 9.67045
[1mStep[0m  [18/21], [94mLoss[0m : 9.41266
[1mStep[0m  [20/21], [94mLoss[0m : 9.14912

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.291, [92mTest[0m: 11.006, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.95290
[1mStep[0m  [2/21], [94mLoss[0m : 8.61926
[1mStep[0m  [4/21], [94mLoss[0m : 8.34219
[1mStep[0m  [6/21], [94mLoss[0m : 7.73953
[1mStep[0m  [8/21], [94mLoss[0m : 7.55912
[1mStep[0m  [10/21], [94mLoss[0m : 7.19955
[1mStep[0m  [12/21], [94mLoss[0m : 6.81454
[1mStep[0m  [14/21], [94mLoss[0m : 6.51312
[1mStep[0m  [16/21], [94mLoss[0m : 6.14908
[1mStep[0m  [18/21], [94mLoss[0m : 6.00841
[1mStep[0m  [20/21], [94mLoss[0m : 6.03642

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.216, [92mTest[0m: 9.194, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.39303
[1mStep[0m  [2/21], [94mLoss[0m : 5.49528
[1mStep[0m  [4/21], [94mLoss[0m : 4.91064
[1mStep[0m  [6/21], [94mLoss[0m : 5.12222
[1mStep[0m  [8/21], [94mLoss[0m : 4.64533
[1mStep[0m  [10/21], [94mLoss[0m : 4.40257
[1mStep[0m  [12/21], [94mLoss[0m : 4.10717
[1mStep[0m  [14/21], [94mLoss[0m : 3.97543
[1mStep[0m  [16/21], [94mLoss[0m : 3.86391
[1mStep[0m  [18/21], [94mLoss[0m : 3.63977
[1mStep[0m  [20/21], [94mLoss[0m : 3.30571

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.414, [92mTest[0m: 6.088, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.33213
[1mStep[0m  [2/21], [94mLoss[0m : 3.17081
[1mStep[0m  [4/21], [94mLoss[0m : 3.06824
[1mStep[0m  [6/21], [94mLoss[0m : 3.08027
[1mStep[0m  [8/21], [94mLoss[0m : 2.94193
[1mStep[0m  [10/21], [94mLoss[0m : 2.80753
[1mStep[0m  [12/21], [94mLoss[0m : 2.66052
[1mStep[0m  [14/21], [94mLoss[0m : 2.75353
[1mStep[0m  [16/21], [94mLoss[0m : 2.85581
[1mStep[0m  [18/21], [94mLoss[0m : 2.88430
[1mStep[0m  [20/21], [94mLoss[0m : 2.73290

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.917, [92mTest[0m: 3.244, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.87350
[1mStep[0m  [2/21], [94mLoss[0m : 2.68344
[1mStep[0m  [4/21], [94mLoss[0m : 2.71533
[1mStep[0m  [6/21], [94mLoss[0m : 2.75768
[1mStep[0m  [8/21], [94mLoss[0m : 2.69694
[1mStep[0m  [10/21], [94mLoss[0m : 2.71793
[1mStep[0m  [12/21], [94mLoss[0m : 2.58117
[1mStep[0m  [14/21], [94mLoss[0m : 2.60749
[1mStep[0m  [16/21], [94mLoss[0m : 2.48650
[1mStep[0m  [18/21], [94mLoss[0m : 2.55585
[1mStep[0m  [20/21], [94mLoss[0m : 2.58908

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51591
[1mStep[0m  [2/21], [94mLoss[0m : 2.52274
[1mStep[0m  [4/21], [94mLoss[0m : 2.45901
[1mStep[0m  [6/21], [94mLoss[0m : 2.61678
[1mStep[0m  [8/21], [94mLoss[0m : 2.50105
[1mStep[0m  [10/21], [94mLoss[0m : 2.58493
[1mStep[0m  [12/21], [94mLoss[0m : 2.52141
[1mStep[0m  [14/21], [94mLoss[0m : 2.61711
[1mStep[0m  [16/21], [94mLoss[0m : 2.52486
[1mStep[0m  [18/21], [94mLoss[0m : 2.61726
[1mStep[0m  [20/21], [94mLoss[0m : 2.55050

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44109
[1mStep[0m  [2/21], [94mLoss[0m : 2.48701
[1mStep[0m  [4/21], [94mLoss[0m : 2.58143
[1mStep[0m  [6/21], [94mLoss[0m : 2.49840
[1mStep[0m  [8/21], [94mLoss[0m : 2.58089
[1mStep[0m  [10/21], [94mLoss[0m : 2.44869
[1mStep[0m  [12/21], [94mLoss[0m : 2.30339
[1mStep[0m  [14/21], [94mLoss[0m : 2.46407
[1mStep[0m  [16/21], [94mLoss[0m : 2.66163
[1mStep[0m  [18/21], [94mLoss[0m : 2.47328
[1mStep[0m  [20/21], [94mLoss[0m : 2.59394

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49287
[1mStep[0m  [2/21], [94mLoss[0m : 2.40691
[1mStep[0m  [4/21], [94mLoss[0m : 2.66909
[1mStep[0m  [6/21], [94mLoss[0m : 2.64510
[1mStep[0m  [8/21], [94mLoss[0m : 2.40414
[1mStep[0m  [10/21], [94mLoss[0m : 2.58271
[1mStep[0m  [12/21], [94mLoss[0m : 2.45871
[1mStep[0m  [14/21], [94mLoss[0m : 2.64428
[1mStep[0m  [16/21], [94mLoss[0m : 2.65831
[1mStep[0m  [18/21], [94mLoss[0m : 2.59944
[1mStep[0m  [20/21], [94mLoss[0m : 2.53790

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43200
[1mStep[0m  [2/21], [94mLoss[0m : 2.39809
[1mStep[0m  [4/21], [94mLoss[0m : 2.55147
[1mStep[0m  [6/21], [94mLoss[0m : 2.56977
[1mStep[0m  [8/21], [94mLoss[0m : 2.72094
[1mStep[0m  [10/21], [94mLoss[0m : 2.56067
[1mStep[0m  [12/21], [94mLoss[0m : 2.64988
[1mStep[0m  [14/21], [94mLoss[0m : 2.41965
[1mStep[0m  [16/21], [94mLoss[0m : 2.43366
[1mStep[0m  [18/21], [94mLoss[0m : 2.64755
[1mStep[0m  [20/21], [94mLoss[0m : 2.39671

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61655
[1mStep[0m  [2/21], [94mLoss[0m : 2.42529
[1mStep[0m  [4/21], [94mLoss[0m : 2.57332
[1mStep[0m  [6/21], [94mLoss[0m : 2.44592
[1mStep[0m  [8/21], [94mLoss[0m : 2.35623
[1mStep[0m  [10/21], [94mLoss[0m : 2.44107
[1mStep[0m  [12/21], [94mLoss[0m : 2.43967
[1mStep[0m  [14/21], [94mLoss[0m : 2.61388
[1mStep[0m  [16/21], [94mLoss[0m : 2.50150
[1mStep[0m  [18/21], [94mLoss[0m : 2.46871
[1mStep[0m  [20/21], [94mLoss[0m : 2.49791

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53161
[1mStep[0m  [2/21], [94mLoss[0m : 2.42826
[1mStep[0m  [4/21], [94mLoss[0m : 2.51256
[1mStep[0m  [6/21], [94mLoss[0m : 2.51788
[1mStep[0m  [8/21], [94mLoss[0m : 2.57679
[1mStep[0m  [10/21], [94mLoss[0m : 2.64766
[1mStep[0m  [12/21], [94mLoss[0m : 2.58864
[1mStep[0m  [14/21], [94mLoss[0m : 2.45868
[1mStep[0m  [16/21], [94mLoss[0m : 2.46793
[1mStep[0m  [18/21], [94mLoss[0m : 2.34161
[1mStep[0m  [20/21], [94mLoss[0m : 2.45156

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46837
[1mStep[0m  [2/21], [94mLoss[0m : 2.40475
[1mStep[0m  [4/21], [94mLoss[0m : 2.57129
[1mStep[0m  [6/21], [94mLoss[0m : 2.43070
[1mStep[0m  [8/21], [94mLoss[0m : 2.55019
[1mStep[0m  [10/21], [94mLoss[0m : 2.50825
[1mStep[0m  [12/21], [94mLoss[0m : 2.47538
[1mStep[0m  [14/21], [94mLoss[0m : 2.49080
[1mStep[0m  [16/21], [94mLoss[0m : 2.42593
[1mStep[0m  [18/21], [94mLoss[0m : 2.47602
[1mStep[0m  [20/21], [94mLoss[0m : 2.45290

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50702
[1mStep[0m  [2/21], [94mLoss[0m : 2.58333
[1mStep[0m  [4/21], [94mLoss[0m : 2.63263
[1mStep[0m  [6/21], [94mLoss[0m : 2.53719
[1mStep[0m  [8/21], [94mLoss[0m : 2.47556
[1mStep[0m  [10/21], [94mLoss[0m : 2.38925
[1mStep[0m  [12/21], [94mLoss[0m : 2.57639
[1mStep[0m  [14/21], [94mLoss[0m : 2.51507
[1mStep[0m  [16/21], [94mLoss[0m : 2.47318
[1mStep[0m  [18/21], [94mLoss[0m : 2.41197
[1mStep[0m  [20/21], [94mLoss[0m : 2.51082

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62718
[1mStep[0m  [2/21], [94mLoss[0m : 2.48432
[1mStep[0m  [4/21], [94mLoss[0m : 2.42997
[1mStep[0m  [6/21], [94mLoss[0m : 2.51133
[1mStep[0m  [8/21], [94mLoss[0m : 2.51976
[1mStep[0m  [10/21], [94mLoss[0m : 2.48609
[1mStep[0m  [12/21], [94mLoss[0m : 2.43213
[1mStep[0m  [14/21], [94mLoss[0m : 2.53427
[1mStep[0m  [16/21], [94mLoss[0m : 2.45606
[1mStep[0m  [18/21], [94mLoss[0m : 2.57132
[1mStep[0m  [20/21], [94mLoss[0m : 2.46753

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.316, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38707
[1mStep[0m  [2/21], [94mLoss[0m : 2.42993
[1mStep[0m  [4/21], [94mLoss[0m : 2.42759
[1mStep[0m  [6/21], [94mLoss[0m : 2.54108
[1mStep[0m  [8/21], [94mLoss[0m : 2.41314
[1mStep[0m  [10/21], [94mLoss[0m : 2.67077
[1mStep[0m  [12/21], [94mLoss[0m : 2.58194
[1mStep[0m  [14/21], [94mLoss[0m : 2.50553
[1mStep[0m  [16/21], [94mLoss[0m : 2.39414
[1mStep[0m  [18/21], [94mLoss[0m : 2.67841
[1mStep[0m  [20/21], [94mLoss[0m : 2.35111

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48611
[1mStep[0m  [2/21], [94mLoss[0m : 2.50772
[1mStep[0m  [4/21], [94mLoss[0m : 2.24627
[1mStep[0m  [6/21], [94mLoss[0m : 2.50620
[1mStep[0m  [8/21], [94mLoss[0m : 2.41811
[1mStep[0m  [10/21], [94mLoss[0m : 2.43384
[1mStep[0m  [12/21], [94mLoss[0m : 2.40985
[1mStep[0m  [14/21], [94mLoss[0m : 2.30888
[1mStep[0m  [16/21], [94mLoss[0m : 2.61393
[1mStep[0m  [18/21], [94mLoss[0m : 2.47947
[1mStep[0m  [20/21], [94mLoss[0m : 2.32906

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40468
[1mStep[0m  [2/21], [94mLoss[0m : 2.59172
[1mStep[0m  [4/21], [94mLoss[0m : 2.30485
[1mStep[0m  [6/21], [94mLoss[0m : 2.36558
[1mStep[0m  [8/21], [94mLoss[0m : 2.50584
[1mStep[0m  [10/21], [94mLoss[0m : 2.29800
[1mStep[0m  [12/21], [94mLoss[0m : 2.59744
[1mStep[0m  [14/21], [94mLoss[0m : 2.51137
[1mStep[0m  [16/21], [94mLoss[0m : 2.49110
[1mStep[0m  [18/21], [94mLoss[0m : 2.47064
[1mStep[0m  [20/21], [94mLoss[0m : 2.53618

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48670
[1mStep[0m  [2/21], [94mLoss[0m : 2.50570
[1mStep[0m  [4/21], [94mLoss[0m : 2.45159
[1mStep[0m  [6/21], [94mLoss[0m : 2.50499
[1mStep[0m  [8/21], [94mLoss[0m : 2.58953
[1mStep[0m  [10/21], [94mLoss[0m : 2.48097
[1mStep[0m  [12/21], [94mLoss[0m : 2.41810
[1mStep[0m  [14/21], [94mLoss[0m : 2.49107
[1mStep[0m  [16/21], [94mLoss[0m : 2.65835
[1mStep[0m  [18/21], [94mLoss[0m : 2.54613
[1mStep[0m  [20/21], [94mLoss[0m : 2.44622

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52495
[1mStep[0m  [2/21], [94mLoss[0m : 2.49711
[1mStep[0m  [4/21], [94mLoss[0m : 2.50651
[1mStep[0m  [6/21], [94mLoss[0m : 2.33423
[1mStep[0m  [8/21], [94mLoss[0m : 2.31263
[1mStep[0m  [10/21], [94mLoss[0m : 2.63018
[1mStep[0m  [12/21], [94mLoss[0m : 2.42793
[1mStep[0m  [14/21], [94mLoss[0m : 2.37323
[1mStep[0m  [16/21], [94mLoss[0m : 2.50774
[1mStep[0m  [18/21], [94mLoss[0m : 2.64524
[1mStep[0m  [20/21], [94mLoss[0m : 2.53996

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56619
[1mStep[0m  [2/21], [94mLoss[0m : 2.47500
[1mStep[0m  [4/21], [94mLoss[0m : 2.36687
[1mStep[0m  [6/21], [94mLoss[0m : 2.70384
[1mStep[0m  [8/21], [94mLoss[0m : 2.47196
[1mStep[0m  [10/21], [94mLoss[0m : 2.45841
[1mStep[0m  [12/21], [94mLoss[0m : 2.56627
[1mStep[0m  [14/21], [94mLoss[0m : 2.53938
[1mStep[0m  [16/21], [94mLoss[0m : 2.35270
[1mStep[0m  [18/21], [94mLoss[0m : 2.45534
[1mStep[0m  [20/21], [94mLoss[0m : 2.48602

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28295
[1mStep[0m  [2/21], [94mLoss[0m : 2.60749
[1mStep[0m  [4/21], [94mLoss[0m : 2.39429
[1mStep[0m  [6/21], [94mLoss[0m : 2.39039
[1mStep[0m  [8/21], [94mLoss[0m : 2.50510
[1mStep[0m  [10/21], [94mLoss[0m : 2.52454
[1mStep[0m  [12/21], [94mLoss[0m : 2.46025
[1mStep[0m  [14/21], [94mLoss[0m : 2.35305
[1mStep[0m  [16/21], [94mLoss[0m : 2.33445
[1mStep[0m  [18/21], [94mLoss[0m : 2.46221
[1mStep[0m  [20/21], [94mLoss[0m : 2.63148

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52629
[1mStep[0m  [2/21], [94mLoss[0m : 2.40535
[1mStep[0m  [4/21], [94mLoss[0m : 2.38198
[1mStep[0m  [6/21], [94mLoss[0m : 2.55204
[1mStep[0m  [8/21], [94mLoss[0m : 2.52719
[1mStep[0m  [10/21], [94mLoss[0m : 2.42544
[1mStep[0m  [12/21], [94mLoss[0m : 2.53627
[1mStep[0m  [14/21], [94mLoss[0m : 2.31630
[1mStep[0m  [16/21], [94mLoss[0m : 2.45268
[1mStep[0m  [18/21], [94mLoss[0m : 2.40976
[1mStep[0m  [20/21], [94mLoss[0m : 2.45283

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43989
[1mStep[0m  [2/21], [94mLoss[0m : 2.34190
[1mStep[0m  [4/21], [94mLoss[0m : 2.42865
[1mStep[0m  [6/21], [94mLoss[0m : 2.43414
[1mStep[0m  [8/21], [94mLoss[0m : 2.32873
[1mStep[0m  [10/21], [94mLoss[0m : 2.21521
[1mStep[0m  [12/21], [94mLoss[0m : 2.55709
[1mStep[0m  [14/21], [94mLoss[0m : 2.49973
[1mStep[0m  [16/21], [94mLoss[0m : 2.41665
[1mStep[0m  [18/21], [94mLoss[0m : 2.53133
[1mStep[0m  [20/21], [94mLoss[0m : 2.42526

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52294
[1mStep[0m  [2/21], [94mLoss[0m : 2.52284
[1mStep[0m  [4/21], [94mLoss[0m : 2.60747
[1mStep[0m  [6/21], [94mLoss[0m : 2.48257
[1mStep[0m  [8/21], [94mLoss[0m : 2.56558
[1mStep[0m  [10/21], [94mLoss[0m : 2.36292
[1mStep[0m  [12/21], [94mLoss[0m : 2.35242
[1mStep[0m  [14/21], [94mLoss[0m : 2.41959
[1mStep[0m  [16/21], [94mLoss[0m : 2.40110
[1mStep[0m  [18/21], [94mLoss[0m : 2.40283
[1mStep[0m  [20/21], [94mLoss[0m : 2.39655

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40485
[1mStep[0m  [2/21], [94mLoss[0m : 2.44265
[1mStep[0m  [4/21], [94mLoss[0m : 2.59489
[1mStep[0m  [6/21], [94mLoss[0m : 2.35786
[1mStep[0m  [8/21], [94mLoss[0m : 2.42371
[1mStep[0m  [10/21], [94mLoss[0m : 2.51391
[1mStep[0m  [12/21], [94mLoss[0m : 2.48160
[1mStep[0m  [14/21], [94mLoss[0m : 2.49775
[1mStep[0m  [16/21], [94mLoss[0m : 2.47766
[1mStep[0m  [18/21], [94mLoss[0m : 2.55214
[1mStep[0m  [20/21], [94mLoss[0m : 2.38834

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24173
[1mStep[0m  [2/21], [94mLoss[0m : 2.38330
[1mStep[0m  [4/21], [94mLoss[0m : 2.46016
[1mStep[0m  [6/21], [94mLoss[0m : 2.49710
[1mStep[0m  [8/21], [94mLoss[0m : 2.48534
[1mStep[0m  [10/21], [94mLoss[0m : 2.51746
[1mStep[0m  [12/21], [94mLoss[0m : 2.44705
[1mStep[0m  [14/21], [94mLoss[0m : 2.47634
[1mStep[0m  [16/21], [94mLoss[0m : 2.33140
[1mStep[0m  [18/21], [94mLoss[0m : 2.42557
[1mStep[0m  [20/21], [94mLoss[0m : 2.48510

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42619
[1mStep[0m  [2/21], [94mLoss[0m : 2.55204
[1mStep[0m  [4/21], [94mLoss[0m : 2.34350
[1mStep[0m  [6/21], [94mLoss[0m : 2.33709
[1mStep[0m  [8/21], [94mLoss[0m : 2.36241
[1mStep[0m  [10/21], [94mLoss[0m : 2.57031
[1mStep[0m  [12/21], [94mLoss[0m : 2.34902
[1mStep[0m  [14/21], [94mLoss[0m : 2.47929
[1mStep[0m  [16/21], [94mLoss[0m : 2.44477
[1mStep[0m  [18/21], [94mLoss[0m : 2.45828
[1mStep[0m  [20/21], [94mLoss[0m : 2.30434

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39757
[1mStep[0m  [2/21], [94mLoss[0m : 2.42372
[1mStep[0m  [4/21], [94mLoss[0m : 2.47302
[1mStep[0m  [6/21], [94mLoss[0m : 2.48994
[1mStep[0m  [8/21], [94mLoss[0m : 2.39441
[1mStep[0m  [10/21], [94mLoss[0m : 2.32771
[1mStep[0m  [12/21], [94mLoss[0m : 2.39105
[1mStep[0m  [14/21], [94mLoss[0m : 2.40342
[1mStep[0m  [16/21], [94mLoss[0m : 2.43552
[1mStep[0m  [18/21], [94mLoss[0m : 2.51845
[1mStep[0m  [20/21], [94mLoss[0m : 2.46249

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30225
[1mStep[0m  [2/21], [94mLoss[0m : 2.41037
[1mStep[0m  [4/21], [94mLoss[0m : 2.44562
[1mStep[0m  [6/21], [94mLoss[0m : 2.44233
[1mStep[0m  [8/21], [94mLoss[0m : 2.34086
[1mStep[0m  [10/21], [94mLoss[0m : 2.42377
[1mStep[0m  [12/21], [94mLoss[0m : 2.53364
[1mStep[0m  [14/21], [94mLoss[0m : 2.41479
[1mStep[0m  [16/21], [94mLoss[0m : 2.37417
[1mStep[0m  [18/21], [94mLoss[0m : 2.53003
[1mStep[0m  [20/21], [94mLoss[0m : 2.39408

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50572
[1mStep[0m  [2/21], [94mLoss[0m : 2.47113
[1mStep[0m  [4/21], [94mLoss[0m : 2.57401
[1mStep[0m  [6/21], [94mLoss[0m : 2.52125
[1mStep[0m  [8/21], [94mLoss[0m : 2.45499
[1mStep[0m  [10/21], [94mLoss[0m : 2.31189
[1mStep[0m  [12/21], [94mLoss[0m : 2.41557
[1mStep[0m  [14/21], [94mLoss[0m : 2.54934
[1mStep[0m  [16/21], [94mLoss[0m : 2.39853
[1mStep[0m  [18/21], [94mLoss[0m : 2.51648
[1mStep[0m  [20/21], [94mLoss[0m : 2.34211

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.335
====================================

Phase 1 - Evaluation MAE:  2.335109370095389
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.33073
[1mStep[0m  [2/21], [94mLoss[0m : 2.41677
[1mStep[0m  [4/21], [94mLoss[0m : 2.39533
[1mStep[0m  [6/21], [94mLoss[0m : 2.52491
[1mStep[0m  [8/21], [94mLoss[0m : 2.45523
[1mStep[0m  [10/21], [94mLoss[0m : 2.57707
[1mStep[0m  [12/21], [94mLoss[0m : 2.55077
[1mStep[0m  [14/21], [94mLoss[0m : 2.38109
[1mStep[0m  [16/21], [94mLoss[0m : 2.50485
[1mStep[0m  [18/21], [94mLoss[0m : 2.47403
[1mStep[0m  [20/21], [94mLoss[0m : 2.46814

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27393
[1mStep[0m  [2/21], [94mLoss[0m : 2.48164
[1mStep[0m  [4/21], [94mLoss[0m : 2.46805
[1mStep[0m  [6/21], [94mLoss[0m : 2.58093
[1mStep[0m  [8/21], [94mLoss[0m : 2.47790
[1mStep[0m  [10/21], [94mLoss[0m : 2.29595
[1mStep[0m  [12/21], [94mLoss[0m : 2.35828
[1mStep[0m  [14/21], [94mLoss[0m : 2.19217
[1mStep[0m  [16/21], [94mLoss[0m : 2.35206
[1mStep[0m  [18/21], [94mLoss[0m : 2.37057
[1mStep[0m  [20/21], [94mLoss[0m : 2.37913

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.384, [92mTest[0m: 5.059, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33776
[1mStep[0m  [2/21], [94mLoss[0m : 2.27205
[1mStep[0m  [4/21], [94mLoss[0m : 2.10155
[1mStep[0m  [6/21], [94mLoss[0m : 2.26264
[1mStep[0m  [8/21], [94mLoss[0m : 2.34246
[1mStep[0m  [10/21], [94mLoss[0m : 2.23005
[1mStep[0m  [12/21], [94mLoss[0m : 2.46939
[1mStep[0m  [14/21], [94mLoss[0m : 2.31202
[1mStep[0m  [16/21], [94mLoss[0m : 2.33443
[1mStep[0m  [18/21], [94mLoss[0m : 2.31103
[1mStep[0m  [20/21], [94mLoss[0m : 2.26768

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.278, [92mTest[0m: 3.202, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25830
[1mStep[0m  [2/21], [94mLoss[0m : 2.20234
[1mStep[0m  [4/21], [94mLoss[0m : 2.19038
[1mStep[0m  [6/21], [94mLoss[0m : 2.33756
[1mStep[0m  [8/21], [94mLoss[0m : 2.08492
[1mStep[0m  [10/21], [94mLoss[0m : 2.11246
[1mStep[0m  [12/21], [94mLoss[0m : 2.24364
[1mStep[0m  [14/21], [94mLoss[0m : 2.14666
[1mStep[0m  [16/21], [94mLoss[0m : 2.21262
[1mStep[0m  [18/21], [94mLoss[0m : 2.43717
[1mStep[0m  [20/21], [94mLoss[0m : 2.25406

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.201, [92mTest[0m: 3.003, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23569
[1mStep[0m  [2/21], [94mLoss[0m : 2.07904
[1mStep[0m  [4/21], [94mLoss[0m : 2.05593
[1mStep[0m  [6/21], [94mLoss[0m : 2.08041
[1mStep[0m  [8/21], [94mLoss[0m : 2.03010
[1mStep[0m  [10/21], [94mLoss[0m : 1.98605
[1mStep[0m  [12/21], [94mLoss[0m : 2.18096
[1mStep[0m  [14/21], [94mLoss[0m : 2.20137
[1mStep[0m  [16/21], [94mLoss[0m : 2.02447
[1mStep[0m  [18/21], [94mLoss[0m : 1.99537
[1mStep[0m  [20/21], [94mLoss[0m : 2.07218

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.106, [92mTest[0m: 3.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02481
[1mStep[0m  [2/21], [94mLoss[0m : 2.07111
[1mStep[0m  [4/21], [94mLoss[0m : 2.01868
[1mStep[0m  [6/21], [94mLoss[0m : 2.00318
[1mStep[0m  [8/21], [94mLoss[0m : 2.05795
[1mStep[0m  [10/21], [94mLoss[0m : 2.05176
[1mStep[0m  [12/21], [94mLoss[0m : 2.09989
[1mStep[0m  [14/21], [94mLoss[0m : 1.94137
[1mStep[0m  [16/21], [94mLoss[0m : 2.10956
[1mStep[0m  [18/21], [94mLoss[0m : 1.98812
[1mStep[0m  [20/21], [94mLoss[0m : 2.04889

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.047, [92mTest[0m: 2.968, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02575
[1mStep[0m  [2/21], [94mLoss[0m : 1.95120
[1mStep[0m  [4/21], [94mLoss[0m : 1.88788
[1mStep[0m  [6/21], [94mLoss[0m : 1.89204
[1mStep[0m  [8/21], [94mLoss[0m : 1.90713
[1mStep[0m  [10/21], [94mLoss[0m : 2.01168
[1mStep[0m  [12/21], [94mLoss[0m : 2.02874
[1mStep[0m  [14/21], [94mLoss[0m : 2.04943
[1mStep[0m  [16/21], [94mLoss[0m : 1.89154
[1mStep[0m  [18/21], [94mLoss[0m : 1.98505
[1mStep[0m  [20/21], [94mLoss[0m : 2.07594

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.990, [92mTest[0m: 3.107, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.70898
[1mStep[0m  [2/21], [94mLoss[0m : 1.91777
[1mStep[0m  [4/21], [94mLoss[0m : 1.95008
[1mStep[0m  [6/21], [94mLoss[0m : 2.01547
[1mStep[0m  [8/21], [94mLoss[0m : 1.88879
[1mStep[0m  [10/21], [94mLoss[0m : 2.01464
[1mStep[0m  [12/21], [94mLoss[0m : 2.04622
[1mStep[0m  [14/21], [94mLoss[0m : 1.96455
[1mStep[0m  [16/21], [94mLoss[0m : 2.02108
[1mStep[0m  [18/21], [94mLoss[0m : 1.97544
[1mStep[0m  [20/21], [94mLoss[0m : 1.98790

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.576, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82712
[1mStep[0m  [2/21], [94mLoss[0m : 1.84890
[1mStep[0m  [4/21], [94mLoss[0m : 1.81703
[1mStep[0m  [6/21], [94mLoss[0m : 1.82747
[1mStep[0m  [8/21], [94mLoss[0m : 1.92896
[1mStep[0m  [10/21], [94mLoss[0m : 1.71880
[1mStep[0m  [12/21], [94mLoss[0m : 1.88550
[1mStep[0m  [14/21], [94mLoss[0m : 1.80960
[1mStep[0m  [16/21], [94mLoss[0m : 1.89437
[1mStep[0m  [18/21], [94mLoss[0m : 2.02475
[1mStep[0m  [20/21], [94mLoss[0m : 1.94823

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.79921
[1mStep[0m  [2/21], [94mLoss[0m : 1.93246
[1mStep[0m  [4/21], [94mLoss[0m : 1.67952
[1mStep[0m  [6/21], [94mLoss[0m : 1.94250
[1mStep[0m  [8/21], [94mLoss[0m : 1.83178
[1mStep[0m  [10/21], [94mLoss[0m : 1.84961
[1mStep[0m  [12/21], [94mLoss[0m : 1.83183
[1mStep[0m  [14/21], [94mLoss[0m : 1.90731
[1mStep[0m  [16/21], [94mLoss[0m : 1.73996
[1mStep[0m  [18/21], [94mLoss[0m : 1.96330
[1mStep[0m  [20/21], [94mLoss[0m : 1.84957

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.652, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71721
[1mStep[0m  [2/21], [94mLoss[0m : 1.70647
[1mStep[0m  [4/21], [94mLoss[0m : 1.73902
[1mStep[0m  [6/21], [94mLoss[0m : 1.74361
[1mStep[0m  [8/21], [94mLoss[0m : 1.67051
[1mStep[0m  [10/21], [94mLoss[0m : 1.85342
[1mStep[0m  [12/21], [94mLoss[0m : 1.82974
[1mStep[0m  [14/21], [94mLoss[0m : 1.84406
[1mStep[0m  [16/21], [94mLoss[0m : 1.74675
[1mStep[0m  [18/21], [94mLoss[0m : 1.89796
[1mStep[0m  [20/21], [94mLoss[0m : 1.94325

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.769, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.69117
[1mStep[0m  [2/21], [94mLoss[0m : 1.72060
[1mStep[0m  [4/21], [94mLoss[0m : 1.80364
[1mStep[0m  [6/21], [94mLoss[0m : 1.73019
[1mStep[0m  [8/21], [94mLoss[0m : 1.89870
[1mStep[0m  [10/21], [94mLoss[0m : 1.74156
[1mStep[0m  [12/21], [94mLoss[0m : 1.91407
[1mStep[0m  [14/21], [94mLoss[0m : 1.70295
[1mStep[0m  [16/21], [94mLoss[0m : 1.80420
[1mStep[0m  [18/21], [94mLoss[0m : 1.63044
[1mStep[0m  [20/21], [94mLoss[0m : 1.81093

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.76883
[1mStep[0m  [2/21], [94mLoss[0m : 1.56765
[1mStep[0m  [4/21], [94mLoss[0m : 1.75501
[1mStep[0m  [6/21], [94mLoss[0m : 1.67530
[1mStep[0m  [8/21], [94mLoss[0m : 1.75664
[1mStep[0m  [10/21], [94mLoss[0m : 1.80732
[1mStep[0m  [12/21], [94mLoss[0m : 1.75385
[1mStep[0m  [14/21], [94mLoss[0m : 1.71624
[1mStep[0m  [16/21], [94mLoss[0m : 1.66728
[1mStep[0m  [18/21], [94mLoss[0m : 1.73164
[1mStep[0m  [20/21], [94mLoss[0m : 1.73075

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.66336
[1mStep[0m  [2/21], [94mLoss[0m : 1.72751
[1mStep[0m  [4/21], [94mLoss[0m : 1.64785
[1mStep[0m  [6/21], [94mLoss[0m : 1.65397
[1mStep[0m  [8/21], [94mLoss[0m : 1.67959
[1mStep[0m  [10/21], [94mLoss[0m : 1.54017
[1mStep[0m  [12/21], [94mLoss[0m : 1.68252
[1mStep[0m  [14/21], [94mLoss[0m : 1.62201
[1mStep[0m  [16/21], [94mLoss[0m : 1.79767
[1mStep[0m  [18/21], [94mLoss[0m : 1.61942
[1mStep[0m  [20/21], [94mLoss[0m : 1.62144

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63944
[1mStep[0m  [2/21], [94mLoss[0m : 1.56382
[1mStep[0m  [4/21], [94mLoss[0m : 1.65490
[1mStep[0m  [6/21], [94mLoss[0m : 1.58624
[1mStep[0m  [8/21], [94mLoss[0m : 1.66204
[1mStep[0m  [10/21], [94mLoss[0m : 1.69170
[1mStep[0m  [12/21], [94mLoss[0m : 1.70427
[1mStep[0m  [14/21], [94mLoss[0m : 1.59602
[1mStep[0m  [16/21], [94mLoss[0m : 1.66242
[1mStep[0m  [18/21], [94mLoss[0m : 1.66296
[1mStep[0m  [20/21], [94mLoss[0m : 1.70222

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.717, [96mlr[0m: 0.01
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 14 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.439
====================================

Phase 2 - Evaluation MAE:  2.4387728486742293
MAE score P1      2.335109
MAE score P2      2.438773
loss              1.654019
learning_rate         0.01
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.89102
[1mStep[0m  [4/42], [94mLoss[0m : 11.17193
[1mStep[0m  [8/42], [94mLoss[0m : 11.04242
[1mStep[0m  [12/42], [94mLoss[0m : 10.58183
[1mStep[0m  [16/42], [94mLoss[0m : 10.58860
[1mStep[0m  [20/42], [94mLoss[0m : 10.48885
[1mStep[0m  [24/42], [94mLoss[0m : 10.37291
[1mStep[0m  [28/42], [94mLoss[0m : 10.25067
[1mStep[0m  [32/42], [94mLoss[0m : 10.23243
[1mStep[0m  [36/42], [94mLoss[0m : 10.45266
[1mStep[0m  [40/42], [94mLoss[0m : 10.14330

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.633, [92mTest[0m: 10.987, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64270
[1mStep[0m  [4/42], [94mLoss[0m : 10.64701
[1mStep[0m  [8/42], [94mLoss[0m : 10.08116
[1mStep[0m  [12/42], [94mLoss[0m : 10.10388
[1mStep[0m  [16/42], [94mLoss[0m : 10.61521
[1mStep[0m  [20/42], [94mLoss[0m : 9.91978
[1mStep[0m  [24/42], [94mLoss[0m : 9.90102
[1mStep[0m  [28/42], [94mLoss[0m : 9.83083
[1mStep[0m  [32/42], [94mLoss[0m : 10.09814
[1mStep[0m  [36/42], [94mLoss[0m : 9.81913
[1mStep[0m  [40/42], [94mLoss[0m : 9.90578

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.035, [92mTest[0m: 10.221, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.80952
[1mStep[0m  [4/42], [94mLoss[0m : 9.73981
[1mStep[0m  [8/42], [94mLoss[0m : 9.70890
[1mStep[0m  [12/42], [94mLoss[0m : 9.29428
[1mStep[0m  [16/42], [94mLoss[0m : 9.09451
[1mStep[0m  [20/42], [94mLoss[0m : 9.64979
[1mStep[0m  [24/42], [94mLoss[0m : 9.58071
[1mStep[0m  [28/42], [94mLoss[0m : 9.12451
[1mStep[0m  [32/42], [94mLoss[0m : 8.90926
[1mStep[0m  [36/42], [94mLoss[0m : 9.13948
[1mStep[0m  [40/42], [94mLoss[0m : 9.16332

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.346, [92mTest[0m: 9.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.85453
[1mStep[0m  [4/42], [94mLoss[0m : 8.87038
[1mStep[0m  [8/42], [94mLoss[0m : 8.70698
[1mStep[0m  [12/42], [94mLoss[0m : 8.91921
[1mStep[0m  [16/42], [94mLoss[0m : 8.48107
[1mStep[0m  [20/42], [94mLoss[0m : 8.16245
[1mStep[0m  [24/42], [94mLoss[0m : 8.14993
[1mStep[0m  [28/42], [94mLoss[0m : 8.48726
[1mStep[0m  [32/42], [94mLoss[0m : 8.23141
[1mStep[0m  [36/42], [94mLoss[0m : 8.24435
[1mStep[0m  [40/42], [94mLoss[0m : 7.79871

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.436, [92mTest[0m: 8.594, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.72688
[1mStep[0m  [4/42], [94mLoss[0m : 7.68024
[1mStep[0m  [8/42], [94mLoss[0m : 7.44769
[1mStep[0m  [12/42], [94mLoss[0m : 7.68319
[1mStep[0m  [16/42], [94mLoss[0m : 7.42220
[1mStep[0m  [20/42], [94mLoss[0m : 7.36380
[1mStep[0m  [24/42], [94mLoss[0m : 7.29042
[1mStep[0m  [28/42], [94mLoss[0m : 7.33064
[1mStep[0m  [32/42], [94mLoss[0m : 7.04669
[1mStep[0m  [36/42], [94mLoss[0m : 7.44409
[1mStep[0m  [40/42], [94mLoss[0m : 6.77462

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.335, [92mTest[0m: 7.287, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.97812
[1mStep[0m  [4/42], [94mLoss[0m : 6.49991
[1mStep[0m  [8/42], [94mLoss[0m : 6.74365
[1mStep[0m  [12/42], [94mLoss[0m : 6.22047
[1mStep[0m  [16/42], [94mLoss[0m : 6.49425
[1mStep[0m  [20/42], [94mLoss[0m : 6.43489
[1mStep[0m  [24/42], [94mLoss[0m : 6.01418
[1mStep[0m  [28/42], [94mLoss[0m : 6.23403
[1mStep[0m  [32/42], [94mLoss[0m : 5.85428
[1mStep[0m  [36/42], [94mLoss[0m : 5.79903
[1mStep[0m  [40/42], [94mLoss[0m : 5.63548

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.256, [92mTest[0m: 5.994, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.83625
[1mStep[0m  [4/42], [94mLoss[0m : 5.64291
[1mStep[0m  [8/42], [94mLoss[0m : 5.41084
[1mStep[0m  [12/42], [94mLoss[0m : 5.10947
[1mStep[0m  [16/42], [94mLoss[0m : 5.11596
[1mStep[0m  [20/42], [94mLoss[0m : 4.90832
[1mStep[0m  [24/42], [94mLoss[0m : 5.05612
[1mStep[0m  [28/42], [94mLoss[0m : 5.10544
[1mStep[0m  [32/42], [94mLoss[0m : 4.95811
[1mStep[0m  [36/42], [94mLoss[0m : 4.79112
[1mStep[0m  [40/42], [94mLoss[0m : 4.56722

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.139, [92mTest[0m: 4.837, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.61631
[1mStep[0m  [4/42], [94mLoss[0m : 4.21328
[1mStep[0m  [8/42], [94mLoss[0m : 4.13007
[1mStep[0m  [12/42], [94mLoss[0m : 4.28561
[1mStep[0m  [16/42], [94mLoss[0m : 4.14670
[1mStep[0m  [20/42], [94mLoss[0m : 3.68600
[1mStep[0m  [24/42], [94mLoss[0m : 3.96902
[1mStep[0m  [28/42], [94mLoss[0m : 3.91657
[1mStep[0m  [32/42], [94mLoss[0m : 3.87217
[1mStep[0m  [36/42], [94mLoss[0m : 3.58716
[1mStep[0m  [40/42], [94mLoss[0m : 3.50130

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.987, [92mTest[0m: 3.682, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.65782
[1mStep[0m  [4/42], [94mLoss[0m : 3.38220
[1mStep[0m  [8/42], [94mLoss[0m : 3.16379
[1mStep[0m  [12/42], [94mLoss[0m : 3.18412
[1mStep[0m  [16/42], [94mLoss[0m : 3.18415
[1mStep[0m  [20/42], [94mLoss[0m : 3.24579
[1mStep[0m  [24/42], [94mLoss[0m : 2.96986
[1mStep[0m  [28/42], [94mLoss[0m : 2.59061
[1mStep[0m  [32/42], [94mLoss[0m : 2.86683
[1mStep[0m  [36/42], [94mLoss[0m : 2.75196
[1mStep[0m  [40/42], [94mLoss[0m : 2.74666

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.062, [92mTest[0m: 2.920, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64109
[1mStep[0m  [4/42], [94mLoss[0m : 2.83860
[1mStep[0m  [8/42], [94mLoss[0m : 3.02085
[1mStep[0m  [12/42], [94mLoss[0m : 2.62285
[1mStep[0m  [16/42], [94mLoss[0m : 2.93489
[1mStep[0m  [20/42], [94mLoss[0m : 2.65946
[1mStep[0m  [24/42], [94mLoss[0m : 2.70544
[1mStep[0m  [28/42], [94mLoss[0m : 2.67568
[1mStep[0m  [32/42], [94mLoss[0m : 2.67883
[1mStep[0m  [36/42], [94mLoss[0m : 2.83063
[1mStep[0m  [40/42], [94mLoss[0m : 2.39381

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60848
[1mStep[0m  [4/42], [94mLoss[0m : 2.61175
[1mStep[0m  [8/42], [94mLoss[0m : 2.70711
[1mStep[0m  [12/42], [94mLoss[0m : 2.54198
[1mStep[0m  [16/42], [94mLoss[0m : 2.75670
[1mStep[0m  [20/42], [94mLoss[0m : 2.47735
[1mStep[0m  [24/42], [94mLoss[0m : 2.57466
[1mStep[0m  [28/42], [94mLoss[0m : 2.29987
[1mStep[0m  [32/42], [94mLoss[0m : 2.68889
[1mStep[0m  [36/42], [94mLoss[0m : 2.41748
[1mStep[0m  [40/42], [94mLoss[0m : 2.58174

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85215
[1mStep[0m  [4/42], [94mLoss[0m : 2.61185
[1mStep[0m  [8/42], [94mLoss[0m : 2.44619
[1mStep[0m  [12/42], [94mLoss[0m : 2.50268
[1mStep[0m  [16/42], [94mLoss[0m : 2.62687
[1mStep[0m  [20/42], [94mLoss[0m : 2.44927
[1mStep[0m  [24/42], [94mLoss[0m : 2.66563
[1mStep[0m  [28/42], [94mLoss[0m : 2.48091
[1mStep[0m  [32/42], [94mLoss[0m : 2.57146
[1mStep[0m  [36/42], [94mLoss[0m : 2.70188
[1mStep[0m  [40/42], [94mLoss[0m : 2.72758

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53749
[1mStep[0m  [4/42], [94mLoss[0m : 2.56229
[1mStep[0m  [8/42], [94mLoss[0m : 2.39705
[1mStep[0m  [12/42], [94mLoss[0m : 2.70676
[1mStep[0m  [16/42], [94mLoss[0m : 2.47410
[1mStep[0m  [20/42], [94mLoss[0m : 2.61712
[1mStep[0m  [24/42], [94mLoss[0m : 2.48192
[1mStep[0m  [28/42], [94mLoss[0m : 2.80728
[1mStep[0m  [32/42], [94mLoss[0m : 2.68710
[1mStep[0m  [36/42], [94mLoss[0m : 2.60142
[1mStep[0m  [40/42], [94mLoss[0m : 2.79085

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49966
[1mStep[0m  [4/42], [94mLoss[0m : 2.66090
[1mStep[0m  [8/42], [94mLoss[0m : 2.55840
[1mStep[0m  [12/42], [94mLoss[0m : 2.53703
[1mStep[0m  [16/42], [94mLoss[0m : 2.36071
[1mStep[0m  [20/42], [94mLoss[0m : 2.62877
[1mStep[0m  [24/42], [94mLoss[0m : 2.72892
[1mStep[0m  [28/42], [94mLoss[0m : 2.56144
[1mStep[0m  [32/42], [94mLoss[0m : 2.55910
[1mStep[0m  [36/42], [94mLoss[0m : 2.49910
[1mStep[0m  [40/42], [94mLoss[0m : 2.74512

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62542
[1mStep[0m  [4/42], [94mLoss[0m : 2.38570
[1mStep[0m  [8/42], [94mLoss[0m : 2.61048
[1mStep[0m  [12/42], [94mLoss[0m : 2.30931
[1mStep[0m  [16/42], [94mLoss[0m : 2.67665
[1mStep[0m  [20/42], [94mLoss[0m : 2.61307
[1mStep[0m  [24/42], [94mLoss[0m : 2.62358
[1mStep[0m  [28/42], [94mLoss[0m : 2.86334
[1mStep[0m  [32/42], [94mLoss[0m : 2.34037
[1mStep[0m  [36/42], [94mLoss[0m : 2.58434
[1mStep[0m  [40/42], [94mLoss[0m : 2.72919

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63891
[1mStep[0m  [4/42], [94mLoss[0m : 2.75018
[1mStep[0m  [8/42], [94mLoss[0m : 2.56928
[1mStep[0m  [12/42], [94mLoss[0m : 2.47264
[1mStep[0m  [16/42], [94mLoss[0m : 2.38308
[1mStep[0m  [20/42], [94mLoss[0m : 2.54175
[1mStep[0m  [24/42], [94mLoss[0m : 2.31693
[1mStep[0m  [28/42], [94mLoss[0m : 2.61822
[1mStep[0m  [32/42], [94mLoss[0m : 2.48369
[1mStep[0m  [36/42], [94mLoss[0m : 2.65052
[1mStep[0m  [40/42], [94mLoss[0m : 2.47392

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49496
[1mStep[0m  [4/42], [94mLoss[0m : 2.46203
[1mStep[0m  [8/42], [94mLoss[0m : 2.57569
[1mStep[0m  [12/42], [94mLoss[0m : 2.48578
[1mStep[0m  [16/42], [94mLoss[0m : 2.66425
[1mStep[0m  [20/42], [94mLoss[0m : 2.65177
[1mStep[0m  [24/42], [94mLoss[0m : 2.60369
[1mStep[0m  [28/42], [94mLoss[0m : 2.82848
[1mStep[0m  [32/42], [94mLoss[0m : 2.70560
[1mStep[0m  [36/42], [94mLoss[0m : 2.53402
[1mStep[0m  [40/42], [94mLoss[0m : 2.45969

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44015
[1mStep[0m  [4/42], [94mLoss[0m : 2.62355
[1mStep[0m  [8/42], [94mLoss[0m : 2.57652
[1mStep[0m  [12/42], [94mLoss[0m : 2.62185
[1mStep[0m  [16/42], [94mLoss[0m : 2.45358
[1mStep[0m  [20/42], [94mLoss[0m : 2.37514
[1mStep[0m  [24/42], [94mLoss[0m : 2.32671
[1mStep[0m  [28/42], [94mLoss[0m : 2.60774
[1mStep[0m  [32/42], [94mLoss[0m : 2.43522
[1mStep[0m  [36/42], [94mLoss[0m : 2.49172
[1mStep[0m  [40/42], [94mLoss[0m : 2.60266

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50281
[1mStep[0m  [4/42], [94mLoss[0m : 2.43781
[1mStep[0m  [8/42], [94mLoss[0m : 2.49141
[1mStep[0m  [12/42], [94mLoss[0m : 2.56210
[1mStep[0m  [16/42], [94mLoss[0m : 2.66959
[1mStep[0m  [20/42], [94mLoss[0m : 2.51509
[1mStep[0m  [24/42], [94mLoss[0m : 2.54621
[1mStep[0m  [28/42], [94mLoss[0m : 2.41051
[1mStep[0m  [32/42], [94mLoss[0m : 2.39149
[1mStep[0m  [36/42], [94mLoss[0m : 2.52973
[1mStep[0m  [40/42], [94mLoss[0m : 2.49897

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42950
[1mStep[0m  [4/42], [94mLoss[0m : 2.59229
[1mStep[0m  [8/42], [94mLoss[0m : 2.46508
[1mStep[0m  [12/42], [94mLoss[0m : 2.46111
[1mStep[0m  [16/42], [94mLoss[0m : 2.33709
[1mStep[0m  [20/42], [94mLoss[0m : 2.56625
[1mStep[0m  [24/42], [94mLoss[0m : 2.50570
[1mStep[0m  [28/42], [94mLoss[0m : 2.65277
[1mStep[0m  [32/42], [94mLoss[0m : 2.52566
[1mStep[0m  [36/42], [94mLoss[0m : 2.35006
[1mStep[0m  [40/42], [94mLoss[0m : 2.38462

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48028
[1mStep[0m  [4/42], [94mLoss[0m : 2.63990
[1mStep[0m  [8/42], [94mLoss[0m : 2.53170
[1mStep[0m  [12/42], [94mLoss[0m : 2.55640
[1mStep[0m  [16/42], [94mLoss[0m : 2.48696
[1mStep[0m  [20/42], [94mLoss[0m : 2.48881
[1mStep[0m  [24/42], [94mLoss[0m : 2.49355
[1mStep[0m  [28/42], [94mLoss[0m : 2.62188
[1mStep[0m  [32/42], [94mLoss[0m : 2.40342
[1mStep[0m  [36/42], [94mLoss[0m : 2.46782
[1mStep[0m  [40/42], [94mLoss[0m : 2.46922

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.363, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76896
[1mStep[0m  [4/42], [94mLoss[0m : 2.91699
[1mStep[0m  [8/42], [94mLoss[0m : 2.66373
[1mStep[0m  [12/42], [94mLoss[0m : 2.63607
[1mStep[0m  [16/42], [94mLoss[0m : 2.39839
[1mStep[0m  [20/42], [94mLoss[0m : 2.43904
[1mStep[0m  [24/42], [94mLoss[0m : 2.27962
[1mStep[0m  [28/42], [94mLoss[0m : 2.47515
[1mStep[0m  [32/42], [94mLoss[0m : 2.43855
[1mStep[0m  [36/42], [94mLoss[0m : 2.72257
[1mStep[0m  [40/42], [94mLoss[0m : 2.67059

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58243
[1mStep[0m  [4/42], [94mLoss[0m : 2.49842
[1mStep[0m  [8/42], [94mLoss[0m : 2.53128
[1mStep[0m  [12/42], [94mLoss[0m : 2.63748
[1mStep[0m  [16/42], [94mLoss[0m : 2.38647
[1mStep[0m  [20/42], [94mLoss[0m : 2.50118
[1mStep[0m  [24/42], [94mLoss[0m : 2.46048
[1mStep[0m  [28/42], [94mLoss[0m : 2.48483
[1mStep[0m  [32/42], [94mLoss[0m : 2.39246
[1mStep[0m  [36/42], [94mLoss[0m : 2.48135
[1mStep[0m  [40/42], [94mLoss[0m : 2.43446

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57546
[1mStep[0m  [4/42], [94mLoss[0m : 2.42478
[1mStep[0m  [8/42], [94mLoss[0m : 2.42334
[1mStep[0m  [12/42], [94mLoss[0m : 2.69758
[1mStep[0m  [16/42], [94mLoss[0m : 2.36704
[1mStep[0m  [20/42], [94mLoss[0m : 2.32033
[1mStep[0m  [24/42], [94mLoss[0m : 2.25418
[1mStep[0m  [28/42], [94mLoss[0m : 2.61918
[1mStep[0m  [32/42], [94mLoss[0m : 2.55658
[1mStep[0m  [36/42], [94mLoss[0m : 2.49197
[1mStep[0m  [40/42], [94mLoss[0m : 2.41914

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84934
[1mStep[0m  [4/42], [94mLoss[0m : 2.50199
[1mStep[0m  [8/42], [94mLoss[0m : 2.44014
[1mStep[0m  [12/42], [94mLoss[0m : 2.39199
[1mStep[0m  [16/42], [94mLoss[0m : 2.48658
[1mStep[0m  [20/42], [94mLoss[0m : 2.27918
[1mStep[0m  [24/42], [94mLoss[0m : 2.55006
[1mStep[0m  [28/42], [94mLoss[0m : 2.27371
[1mStep[0m  [32/42], [94mLoss[0m : 2.53157
[1mStep[0m  [36/42], [94mLoss[0m : 2.42872
[1mStep[0m  [40/42], [94mLoss[0m : 2.41953

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51088
[1mStep[0m  [4/42], [94mLoss[0m : 2.33572
[1mStep[0m  [8/42], [94mLoss[0m : 2.52004
[1mStep[0m  [12/42], [94mLoss[0m : 2.48051
[1mStep[0m  [16/42], [94mLoss[0m : 2.51003
[1mStep[0m  [20/42], [94mLoss[0m : 2.65421
[1mStep[0m  [24/42], [94mLoss[0m : 2.46849
[1mStep[0m  [28/42], [94mLoss[0m : 2.57574
[1mStep[0m  [32/42], [94mLoss[0m : 2.47753
[1mStep[0m  [36/42], [94mLoss[0m : 2.51285
[1mStep[0m  [40/42], [94mLoss[0m : 2.21107

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48974
[1mStep[0m  [4/42], [94mLoss[0m : 2.26506
[1mStep[0m  [8/42], [94mLoss[0m : 2.36265
[1mStep[0m  [12/42], [94mLoss[0m : 2.45562
[1mStep[0m  [16/42], [94mLoss[0m : 2.45507
[1mStep[0m  [20/42], [94mLoss[0m : 2.62968
[1mStep[0m  [24/42], [94mLoss[0m : 2.72604
[1mStep[0m  [28/42], [94mLoss[0m : 2.40121
[1mStep[0m  [32/42], [94mLoss[0m : 2.41331
[1mStep[0m  [36/42], [94mLoss[0m : 2.56009
[1mStep[0m  [40/42], [94mLoss[0m : 2.48740

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36357
[1mStep[0m  [4/42], [94mLoss[0m : 2.73780
[1mStep[0m  [8/42], [94mLoss[0m : 2.39571
[1mStep[0m  [12/42], [94mLoss[0m : 2.36407
[1mStep[0m  [16/42], [94mLoss[0m : 2.41985
[1mStep[0m  [20/42], [94mLoss[0m : 2.30996
[1mStep[0m  [24/42], [94mLoss[0m : 2.56391
[1mStep[0m  [28/42], [94mLoss[0m : 2.61932
[1mStep[0m  [32/42], [94mLoss[0m : 2.55053
[1mStep[0m  [36/42], [94mLoss[0m : 2.50534
[1mStep[0m  [40/42], [94mLoss[0m : 2.39676

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.364, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45347
[1mStep[0m  [4/42], [94mLoss[0m : 2.41193
[1mStep[0m  [8/42], [94mLoss[0m : 2.44487
[1mStep[0m  [12/42], [94mLoss[0m : 2.40068
[1mStep[0m  [16/42], [94mLoss[0m : 2.51960
[1mStep[0m  [20/42], [94mLoss[0m : 2.54477
[1mStep[0m  [24/42], [94mLoss[0m : 2.50568
[1mStep[0m  [28/42], [94mLoss[0m : 2.28271
[1mStep[0m  [32/42], [94mLoss[0m : 2.42815
[1mStep[0m  [36/42], [94mLoss[0m : 2.41400
[1mStep[0m  [40/42], [94mLoss[0m : 2.72282

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59567
[1mStep[0m  [4/42], [94mLoss[0m : 2.40603
[1mStep[0m  [8/42], [94mLoss[0m : 2.34034
[1mStep[0m  [12/42], [94mLoss[0m : 2.45996
[1mStep[0m  [16/42], [94mLoss[0m : 2.49991
[1mStep[0m  [20/42], [94mLoss[0m : 2.50982
[1mStep[0m  [24/42], [94mLoss[0m : 2.38657
[1mStep[0m  [28/42], [94mLoss[0m : 2.47073
[1mStep[0m  [32/42], [94mLoss[0m : 2.71118
[1mStep[0m  [36/42], [94mLoss[0m : 2.47367
[1mStep[0m  [40/42], [94mLoss[0m : 2.57679

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.338
====================================

Phase 1 - Evaluation MAE:  2.3382792132241383
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.47476
[1mStep[0m  [4/42], [94mLoss[0m : 2.45929
[1mStep[0m  [8/42], [94mLoss[0m : 2.78658
[1mStep[0m  [12/42], [94mLoss[0m : 2.47403
[1mStep[0m  [16/42], [94mLoss[0m : 2.65284
[1mStep[0m  [20/42], [94mLoss[0m : 2.56169
[1mStep[0m  [24/42], [94mLoss[0m : 2.66039
[1mStep[0m  [28/42], [94mLoss[0m : 2.40159
[1mStep[0m  [32/42], [94mLoss[0m : 2.50009
[1mStep[0m  [36/42], [94mLoss[0m : 2.28197
[1mStep[0m  [40/42], [94mLoss[0m : 2.45071

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55415
[1mStep[0m  [4/42], [94mLoss[0m : 2.55919
[1mStep[0m  [8/42], [94mLoss[0m : 2.56565
[1mStep[0m  [12/42], [94mLoss[0m : 2.37896
[1mStep[0m  [16/42], [94mLoss[0m : 2.56481
[1mStep[0m  [20/42], [94mLoss[0m : 2.74633
[1mStep[0m  [24/42], [94mLoss[0m : 2.65346
[1mStep[0m  [28/42], [94mLoss[0m : 2.60432
[1mStep[0m  [32/42], [94mLoss[0m : 2.46319
[1mStep[0m  [36/42], [94mLoss[0m : 2.62037
[1mStep[0m  [40/42], [94mLoss[0m : 2.40299

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48133
[1mStep[0m  [4/42], [94mLoss[0m : 2.48338
[1mStep[0m  [8/42], [94mLoss[0m : 2.27722
[1mStep[0m  [12/42], [94mLoss[0m : 2.35534
[1mStep[0m  [16/42], [94mLoss[0m : 2.62200
[1mStep[0m  [20/42], [94mLoss[0m : 2.52311
[1mStep[0m  [24/42], [94mLoss[0m : 2.42062
[1mStep[0m  [28/42], [94mLoss[0m : 2.43321
[1mStep[0m  [32/42], [94mLoss[0m : 2.37849
[1mStep[0m  [36/42], [94mLoss[0m : 2.44743
[1mStep[0m  [40/42], [94mLoss[0m : 2.30155

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64329
[1mStep[0m  [4/42], [94mLoss[0m : 2.44724
[1mStep[0m  [8/42], [94mLoss[0m : 2.41340
[1mStep[0m  [12/42], [94mLoss[0m : 2.21373
[1mStep[0m  [16/42], [94mLoss[0m : 2.36725
[1mStep[0m  [20/42], [94mLoss[0m : 2.50178
[1mStep[0m  [24/42], [94mLoss[0m : 2.37075
[1mStep[0m  [28/42], [94mLoss[0m : 2.50584
[1mStep[0m  [32/42], [94mLoss[0m : 2.44505
[1mStep[0m  [36/42], [94mLoss[0m : 2.43778
[1mStep[0m  [40/42], [94mLoss[0m : 2.64459

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.556, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15599
[1mStep[0m  [4/42], [94mLoss[0m : 2.52152
[1mStep[0m  [8/42], [94mLoss[0m : 2.59115
[1mStep[0m  [12/42], [94mLoss[0m : 2.32825
[1mStep[0m  [16/42], [94mLoss[0m : 2.20326
[1mStep[0m  [20/42], [94mLoss[0m : 2.33787
[1mStep[0m  [24/42], [94mLoss[0m : 2.26484
[1mStep[0m  [28/42], [94mLoss[0m : 2.32076
[1mStep[0m  [32/42], [94mLoss[0m : 2.49725
[1mStep[0m  [36/42], [94mLoss[0m : 2.56271
[1mStep[0m  [40/42], [94mLoss[0m : 2.39964

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30466
[1mStep[0m  [4/42], [94mLoss[0m : 2.35238
[1mStep[0m  [8/42], [94mLoss[0m : 2.27550
[1mStep[0m  [12/42], [94mLoss[0m : 2.27998
[1mStep[0m  [16/42], [94mLoss[0m : 2.36298
[1mStep[0m  [20/42], [94mLoss[0m : 2.08956
[1mStep[0m  [24/42], [94mLoss[0m : 2.32478
[1mStep[0m  [28/42], [94mLoss[0m : 2.37371
[1mStep[0m  [32/42], [94mLoss[0m : 2.31416
[1mStep[0m  [36/42], [94mLoss[0m : 2.36231
[1mStep[0m  [40/42], [94mLoss[0m : 2.58541

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27896
[1mStep[0m  [4/42], [94mLoss[0m : 2.14127
[1mStep[0m  [8/42], [94mLoss[0m : 2.25428
[1mStep[0m  [12/42], [94mLoss[0m : 2.27867
[1mStep[0m  [16/42], [94mLoss[0m : 2.33170
[1mStep[0m  [20/42], [94mLoss[0m : 2.30117
[1mStep[0m  [24/42], [94mLoss[0m : 2.19779
[1mStep[0m  [28/42], [94mLoss[0m : 2.30977
[1mStep[0m  [32/42], [94mLoss[0m : 2.13816
[1mStep[0m  [36/42], [94mLoss[0m : 2.42064
[1mStep[0m  [40/42], [94mLoss[0m : 2.33565

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37372
[1mStep[0m  [4/42], [94mLoss[0m : 2.33963
[1mStep[0m  [8/42], [94mLoss[0m : 2.19911
[1mStep[0m  [12/42], [94mLoss[0m : 2.32911
[1mStep[0m  [16/42], [94mLoss[0m : 2.36888
[1mStep[0m  [20/42], [94mLoss[0m : 2.19997
[1mStep[0m  [24/42], [94mLoss[0m : 2.37320
[1mStep[0m  [28/42], [94mLoss[0m : 2.13250
[1mStep[0m  [32/42], [94mLoss[0m : 2.24163
[1mStep[0m  [36/42], [94mLoss[0m : 2.15194
[1mStep[0m  [40/42], [94mLoss[0m : 2.25175

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26802
[1mStep[0m  [4/42], [94mLoss[0m : 2.27200
[1mStep[0m  [8/42], [94mLoss[0m : 2.17651
[1mStep[0m  [12/42], [94mLoss[0m : 2.34822
[1mStep[0m  [16/42], [94mLoss[0m : 2.19590
[1mStep[0m  [20/42], [94mLoss[0m : 2.13066
[1mStep[0m  [24/42], [94mLoss[0m : 2.43862
[1mStep[0m  [28/42], [94mLoss[0m : 2.22979
[1mStep[0m  [32/42], [94mLoss[0m : 2.01271
[1mStep[0m  [36/42], [94mLoss[0m : 2.06460
[1mStep[0m  [40/42], [94mLoss[0m : 2.30974

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36767
[1mStep[0m  [4/42], [94mLoss[0m : 2.07180
[1mStep[0m  [8/42], [94mLoss[0m : 2.10891
[1mStep[0m  [12/42], [94mLoss[0m : 2.21329
[1mStep[0m  [16/42], [94mLoss[0m : 2.00163
[1mStep[0m  [20/42], [94mLoss[0m : 2.10879
[1mStep[0m  [24/42], [94mLoss[0m : 2.03135
[1mStep[0m  [28/42], [94mLoss[0m : 2.40041
[1mStep[0m  [32/42], [94mLoss[0m : 2.01448
[1mStep[0m  [36/42], [94mLoss[0m : 2.31210
[1mStep[0m  [40/42], [94mLoss[0m : 2.22367

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09285
[1mStep[0m  [4/42], [94mLoss[0m : 2.15814
[1mStep[0m  [8/42], [94mLoss[0m : 2.39260
[1mStep[0m  [12/42], [94mLoss[0m : 2.14325
[1mStep[0m  [16/42], [94mLoss[0m : 2.11033
[1mStep[0m  [20/42], [94mLoss[0m : 2.09829
[1mStep[0m  [24/42], [94mLoss[0m : 2.08387
[1mStep[0m  [28/42], [94mLoss[0m : 2.00495
[1mStep[0m  [32/42], [94mLoss[0m : 2.06798
[1mStep[0m  [36/42], [94mLoss[0m : 2.08935
[1mStep[0m  [40/42], [94mLoss[0m : 2.20533

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13519
[1mStep[0m  [4/42], [94mLoss[0m : 2.14760
[1mStep[0m  [8/42], [94mLoss[0m : 2.28762
[1mStep[0m  [12/42], [94mLoss[0m : 2.08261
[1mStep[0m  [16/42], [94mLoss[0m : 2.06356
[1mStep[0m  [20/42], [94mLoss[0m : 2.00005
[1mStep[0m  [24/42], [94mLoss[0m : 2.30789
[1mStep[0m  [28/42], [94mLoss[0m : 2.10937
[1mStep[0m  [32/42], [94mLoss[0m : 1.99410
[1mStep[0m  [36/42], [94mLoss[0m : 1.97571
[1mStep[0m  [40/42], [94mLoss[0m : 2.35204

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92795
[1mStep[0m  [4/42], [94mLoss[0m : 1.99939
[1mStep[0m  [8/42], [94mLoss[0m : 2.05952
[1mStep[0m  [12/42], [94mLoss[0m : 2.09366
[1mStep[0m  [16/42], [94mLoss[0m : 1.95062
[1mStep[0m  [20/42], [94mLoss[0m : 1.96730
[1mStep[0m  [24/42], [94mLoss[0m : 2.06196
[1mStep[0m  [28/42], [94mLoss[0m : 1.93870
[1mStep[0m  [32/42], [94mLoss[0m : 2.03733
[1mStep[0m  [36/42], [94mLoss[0m : 2.31767
[1mStep[0m  [40/42], [94mLoss[0m : 2.10812

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15590
[1mStep[0m  [4/42], [94mLoss[0m : 2.10070
[1mStep[0m  [8/42], [94mLoss[0m : 2.11928
[1mStep[0m  [12/42], [94mLoss[0m : 1.94617
[1mStep[0m  [16/42], [94mLoss[0m : 2.10035
[1mStep[0m  [20/42], [94mLoss[0m : 2.03242
[1mStep[0m  [24/42], [94mLoss[0m : 2.00162
[1mStep[0m  [28/42], [94mLoss[0m : 2.03913
[1mStep[0m  [32/42], [94mLoss[0m : 2.03593
[1mStep[0m  [36/42], [94mLoss[0m : 2.25162
[1mStep[0m  [40/42], [94mLoss[0m : 2.00514

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.581, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11144
[1mStep[0m  [4/42], [94mLoss[0m : 1.97841
[1mStep[0m  [8/42], [94mLoss[0m : 2.11010
[1mStep[0m  [12/42], [94mLoss[0m : 2.13893
[1mStep[0m  [16/42], [94mLoss[0m : 1.90607
[1mStep[0m  [20/42], [94mLoss[0m : 1.88695
[1mStep[0m  [24/42], [94mLoss[0m : 2.30607
[1mStep[0m  [28/42], [94mLoss[0m : 2.10741
[1mStep[0m  [32/42], [94mLoss[0m : 2.00566
[1mStep[0m  [36/42], [94mLoss[0m : 1.95095
[1mStep[0m  [40/42], [94mLoss[0m : 2.04421

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.588, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06416
[1mStep[0m  [4/42], [94mLoss[0m : 1.99425
[1mStep[0m  [8/42], [94mLoss[0m : 1.84827
[1mStep[0m  [12/42], [94mLoss[0m : 1.82825
[1mStep[0m  [16/42], [94mLoss[0m : 2.00029
[1mStep[0m  [20/42], [94mLoss[0m : 1.94671
[1mStep[0m  [24/42], [94mLoss[0m : 2.09448
[1mStep[0m  [28/42], [94mLoss[0m : 1.92953
[1mStep[0m  [32/42], [94mLoss[0m : 1.95816
[1mStep[0m  [36/42], [94mLoss[0m : 1.85230
[1mStep[0m  [40/42], [94mLoss[0m : 1.98676

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83603
[1mStep[0m  [4/42], [94mLoss[0m : 2.06630
[1mStep[0m  [8/42], [94mLoss[0m : 2.01511
[1mStep[0m  [12/42], [94mLoss[0m : 1.80328
[1mStep[0m  [16/42], [94mLoss[0m : 1.96703
[1mStep[0m  [20/42], [94mLoss[0m : 1.83478
[1mStep[0m  [24/42], [94mLoss[0m : 1.88495
[1mStep[0m  [28/42], [94mLoss[0m : 2.07732
[1mStep[0m  [32/42], [94mLoss[0m : 1.97140
[1mStep[0m  [36/42], [94mLoss[0m : 1.97401
[1mStep[0m  [40/42], [94mLoss[0m : 1.84060

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83308
[1mStep[0m  [4/42], [94mLoss[0m : 1.96861
[1mStep[0m  [8/42], [94mLoss[0m : 1.95063
[1mStep[0m  [12/42], [94mLoss[0m : 1.94242
[1mStep[0m  [16/42], [94mLoss[0m : 1.86503
[1mStep[0m  [20/42], [94mLoss[0m : 1.89300
[1mStep[0m  [24/42], [94mLoss[0m : 1.88665
[1mStep[0m  [28/42], [94mLoss[0m : 1.81294
[1mStep[0m  [32/42], [94mLoss[0m : 1.76877
[1mStep[0m  [36/42], [94mLoss[0m : 1.98981
[1mStep[0m  [40/42], [94mLoss[0m : 1.96022

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87219
[1mStep[0m  [4/42], [94mLoss[0m : 1.84365
[1mStep[0m  [8/42], [94mLoss[0m : 2.00594
[1mStep[0m  [12/42], [94mLoss[0m : 1.92079
[1mStep[0m  [16/42], [94mLoss[0m : 1.77980
[1mStep[0m  [20/42], [94mLoss[0m : 1.94849
[1mStep[0m  [24/42], [94mLoss[0m : 1.66496
[1mStep[0m  [28/42], [94mLoss[0m : 1.82993
[1mStep[0m  [32/42], [94mLoss[0m : 1.85254
[1mStep[0m  [36/42], [94mLoss[0m : 1.81859
[1mStep[0m  [40/42], [94mLoss[0m : 1.81375

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86318
[1mStep[0m  [4/42], [94mLoss[0m : 1.77639
[1mStep[0m  [8/42], [94mLoss[0m : 1.94542
[1mStep[0m  [12/42], [94mLoss[0m : 1.77900
[1mStep[0m  [16/42], [94mLoss[0m : 1.73294
[1mStep[0m  [20/42], [94mLoss[0m : 1.92595
[1mStep[0m  [24/42], [94mLoss[0m : 1.74337
[1mStep[0m  [28/42], [94mLoss[0m : 1.96616
[1mStep[0m  [32/42], [94mLoss[0m : 1.95897
[1mStep[0m  [36/42], [94mLoss[0m : 1.85112
[1mStep[0m  [40/42], [94mLoss[0m : 1.81572

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.574, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83492
[1mStep[0m  [4/42], [94mLoss[0m : 1.94520
[1mStep[0m  [8/42], [94mLoss[0m : 1.74583
[1mStep[0m  [12/42], [94mLoss[0m : 1.71395
[1mStep[0m  [16/42], [94mLoss[0m : 1.79772
[1mStep[0m  [20/42], [94mLoss[0m : 1.78911
[1mStep[0m  [24/42], [94mLoss[0m : 1.61474
[1mStep[0m  [28/42], [94mLoss[0m : 1.64047
[1mStep[0m  [32/42], [94mLoss[0m : 1.77224
[1mStep[0m  [36/42], [94mLoss[0m : 1.74594
[1mStep[0m  [40/42], [94mLoss[0m : 1.90691

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66742
[1mStep[0m  [4/42], [94mLoss[0m : 1.81239
[1mStep[0m  [8/42], [94mLoss[0m : 1.79584
[1mStep[0m  [12/42], [94mLoss[0m : 1.85322
[1mStep[0m  [16/42], [94mLoss[0m : 1.74940
[1mStep[0m  [20/42], [94mLoss[0m : 1.79146
[1mStep[0m  [24/42], [94mLoss[0m : 1.90791
[1mStep[0m  [28/42], [94mLoss[0m : 1.85296
[1mStep[0m  [32/42], [94mLoss[0m : 1.70675
[1mStep[0m  [36/42], [94mLoss[0m : 2.06959
[1mStep[0m  [40/42], [94mLoss[0m : 1.53680

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.777, [92mTest[0m: 2.618, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83573
[1mStep[0m  [4/42], [94mLoss[0m : 1.80334
[1mStep[0m  [8/42], [94mLoss[0m : 1.95974
[1mStep[0m  [12/42], [94mLoss[0m : 1.59864
[1mStep[0m  [16/42], [94mLoss[0m : 1.73012
[1mStep[0m  [20/42], [94mLoss[0m : 1.78081
[1mStep[0m  [24/42], [94mLoss[0m : 1.71731
[1mStep[0m  [28/42], [94mLoss[0m : 1.77043
[1mStep[0m  [32/42], [94mLoss[0m : 1.85559
[1mStep[0m  [36/42], [94mLoss[0m : 1.83104
[1mStep[0m  [40/42], [94mLoss[0m : 1.68824

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76090
[1mStep[0m  [4/42], [94mLoss[0m : 1.67426
[1mStep[0m  [8/42], [94mLoss[0m : 1.83710
[1mStep[0m  [12/42], [94mLoss[0m : 1.75771
[1mStep[0m  [16/42], [94mLoss[0m : 1.79237
[1mStep[0m  [20/42], [94mLoss[0m : 1.78166
[1mStep[0m  [24/42], [94mLoss[0m : 1.66383
[1mStep[0m  [28/42], [94mLoss[0m : 1.63774
[1mStep[0m  [32/42], [94mLoss[0m : 1.76956
[1mStep[0m  [36/42], [94mLoss[0m : 1.59536
[1mStep[0m  [40/42], [94mLoss[0m : 1.60365

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.669, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69295
[1mStep[0m  [4/42], [94mLoss[0m : 1.65087
[1mStep[0m  [8/42], [94mLoss[0m : 1.77832
[1mStep[0m  [12/42], [94mLoss[0m : 1.79362
[1mStep[0m  [16/42], [94mLoss[0m : 1.67890
[1mStep[0m  [20/42], [94mLoss[0m : 1.69401
[1mStep[0m  [24/42], [94mLoss[0m : 1.76252
[1mStep[0m  [28/42], [94mLoss[0m : 1.83793
[1mStep[0m  [32/42], [94mLoss[0m : 1.70020
[1mStep[0m  [36/42], [94mLoss[0m : 1.51565
[1mStep[0m  [40/42], [94mLoss[0m : 1.64467

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.637, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85183
[1mStep[0m  [4/42], [94mLoss[0m : 1.64322
[1mStep[0m  [8/42], [94mLoss[0m : 1.68018
[1mStep[0m  [12/42], [94mLoss[0m : 1.66700
[1mStep[0m  [16/42], [94mLoss[0m : 1.65938
[1mStep[0m  [20/42], [94mLoss[0m : 1.64843
[1mStep[0m  [24/42], [94mLoss[0m : 1.69745
[1mStep[0m  [28/42], [94mLoss[0m : 1.78450
[1mStep[0m  [32/42], [94mLoss[0m : 1.70883
[1mStep[0m  [36/42], [94mLoss[0m : 1.55050
[1mStep[0m  [40/42], [94mLoss[0m : 1.61121

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59692
[1mStep[0m  [4/42], [94mLoss[0m : 1.65188
[1mStep[0m  [8/42], [94mLoss[0m : 1.80063
[1mStep[0m  [12/42], [94mLoss[0m : 1.62976
[1mStep[0m  [16/42], [94mLoss[0m : 1.74192
[1mStep[0m  [20/42], [94mLoss[0m : 1.83077
[1mStep[0m  [24/42], [94mLoss[0m : 1.56749
[1mStep[0m  [28/42], [94mLoss[0m : 1.54346
[1mStep[0m  [32/42], [94mLoss[0m : 1.61761
[1mStep[0m  [36/42], [94mLoss[0m : 1.77188
[1mStep[0m  [40/42], [94mLoss[0m : 1.59715

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.458, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51813
[1mStep[0m  [4/42], [94mLoss[0m : 1.51078
[1mStep[0m  [8/42], [94mLoss[0m : 1.54652
[1mStep[0m  [12/42], [94mLoss[0m : 1.51646
[1mStep[0m  [16/42], [94mLoss[0m : 1.49783
[1mStep[0m  [20/42], [94mLoss[0m : 1.70345
[1mStep[0m  [24/42], [94mLoss[0m : 1.66865
[1mStep[0m  [28/42], [94mLoss[0m : 1.63227
[1mStep[0m  [32/42], [94mLoss[0m : 1.56404
[1mStep[0m  [36/42], [94mLoss[0m : 1.65746
[1mStep[0m  [40/42], [94mLoss[0m : 1.68771

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.446, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58783
[1mStep[0m  [4/42], [94mLoss[0m : 1.51171
[1mStep[0m  [8/42], [94mLoss[0m : 1.62951
[1mStep[0m  [12/42], [94mLoss[0m : 1.52728
[1mStep[0m  [16/42], [94mLoss[0m : 1.66069
[1mStep[0m  [20/42], [94mLoss[0m : 1.57240
[1mStep[0m  [24/42], [94mLoss[0m : 1.67880
[1mStep[0m  [28/42], [94mLoss[0m : 1.63151
[1mStep[0m  [32/42], [94mLoss[0m : 1.70876
[1mStep[0m  [36/42], [94mLoss[0m : 1.67825
[1mStep[0m  [40/42], [94mLoss[0m : 1.73607

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.536, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69429
[1mStep[0m  [4/42], [94mLoss[0m : 1.68399
[1mStep[0m  [8/42], [94mLoss[0m : 1.50927
[1mStep[0m  [12/42], [94mLoss[0m : 1.53241
[1mStep[0m  [16/42], [94mLoss[0m : 1.45259
[1mStep[0m  [20/42], [94mLoss[0m : 1.52662
[1mStep[0m  [24/42], [94mLoss[0m : 1.59468
[1mStep[0m  [28/42], [94mLoss[0m : 1.73165
[1mStep[0m  [32/42], [94mLoss[0m : 1.70437
[1mStep[0m  [36/42], [94mLoss[0m : 1.55925
[1mStep[0m  [40/42], [94mLoss[0m : 1.64174

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.513
====================================

Phase 2 - Evaluation MAE:  2.513096128191267
MAE score P1        2.338279
MAE score P2        2.513096
loss                1.596125
learning_rate           0.01
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay           0.001
Name: 16, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.81733
[1mStep[0m  [4/42], [94mLoss[0m : 10.60277
[1mStep[0m  [8/42], [94mLoss[0m : 10.05365
[1mStep[0m  [12/42], [94mLoss[0m : 9.05456
[1mStep[0m  [16/42], [94mLoss[0m : 8.65140
[1mStep[0m  [20/42], [94mLoss[0m : 8.10169
[1mStep[0m  [24/42], [94mLoss[0m : 7.18844
[1mStep[0m  [28/42], [94mLoss[0m : 6.37871
[1mStep[0m  [32/42], [94mLoss[0m : 5.25180
[1mStep[0m  [36/42], [94mLoss[0m : 5.42568
[1mStep[0m  [40/42], [94mLoss[0m : 4.07650

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.616, [92mTest[0m: 10.913, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.21452
[1mStep[0m  [4/42], [94mLoss[0m : 3.65780
[1mStep[0m  [8/42], [94mLoss[0m : 3.06403
[1mStep[0m  [12/42], [94mLoss[0m : 3.26554
[1mStep[0m  [16/42], [94mLoss[0m : 3.08021
[1mStep[0m  [20/42], [94mLoss[0m : 2.99983
[1mStep[0m  [24/42], [94mLoss[0m : 2.57110
[1mStep[0m  [28/42], [94mLoss[0m : 2.85725
[1mStep[0m  [32/42], [94mLoss[0m : 2.73408
[1mStep[0m  [36/42], [94mLoss[0m : 3.03808
[1mStep[0m  [40/42], [94mLoss[0m : 2.67188

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.120, [92mTest[0m: 6.277, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82254
[1mStep[0m  [4/42], [94mLoss[0m : 2.83087
[1mStep[0m  [8/42], [94mLoss[0m : 2.66987
[1mStep[0m  [12/42], [94mLoss[0m : 2.73139
[1mStep[0m  [16/42], [94mLoss[0m : 2.59818
[1mStep[0m  [20/42], [94mLoss[0m : 2.56021
[1mStep[0m  [24/42], [94mLoss[0m : 2.74188
[1mStep[0m  [28/42], [94mLoss[0m : 2.92488
[1mStep[0m  [32/42], [94mLoss[0m : 2.62500
[1mStep[0m  [36/42], [94mLoss[0m : 3.17719
[1mStep[0m  [40/42], [94mLoss[0m : 2.62163

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.799, [92mTest[0m: 3.085, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92144
[1mStep[0m  [4/42], [94mLoss[0m : 3.05663
[1mStep[0m  [8/42], [94mLoss[0m : 2.74151
[1mStep[0m  [12/42], [94mLoss[0m : 2.74170
[1mStep[0m  [16/42], [94mLoss[0m : 2.83092
[1mStep[0m  [20/42], [94mLoss[0m : 2.79517
[1mStep[0m  [24/42], [94mLoss[0m : 2.68216
[1mStep[0m  [28/42], [94mLoss[0m : 2.61073
[1mStep[0m  [32/42], [94mLoss[0m : 2.89237
[1mStep[0m  [36/42], [94mLoss[0m : 2.90565
[1mStep[0m  [40/42], [94mLoss[0m : 2.67591

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.774, [92mTest[0m: 2.785, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66919
[1mStep[0m  [4/42], [94mLoss[0m : 2.59940
[1mStep[0m  [8/42], [94mLoss[0m : 2.82742
[1mStep[0m  [12/42], [94mLoss[0m : 2.87903
[1mStep[0m  [16/42], [94mLoss[0m : 2.62266
[1mStep[0m  [20/42], [94mLoss[0m : 2.58978
[1mStep[0m  [24/42], [94mLoss[0m : 2.62945
[1mStep[0m  [28/42], [94mLoss[0m : 2.64862
[1mStep[0m  [32/42], [94mLoss[0m : 2.53121
[1mStep[0m  [36/42], [94mLoss[0m : 2.70531
[1mStep[0m  [40/42], [94mLoss[0m : 2.64743

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64527
[1mStep[0m  [4/42], [94mLoss[0m : 2.45613
[1mStep[0m  [8/42], [94mLoss[0m : 2.68764
[1mStep[0m  [12/42], [94mLoss[0m : 2.61206
[1mStep[0m  [16/42], [94mLoss[0m : 2.71172
[1mStep[0m  [20/42], [94mLoss[0m : 2.59684
[1mStep[0m  [24/42], [94mLoss[0m : 3.10417
[1mStep[0m  [28/42], [94mLoss[0m : 2.73483
[1mStep[0m  [32/42], [94mLoss[0m : 2.66858
[1mStep[0m  [36/42], [94mLoss[0m : 2.67267
[1mStep[0m  [40/42], [94mLoss[0m : 2.47027

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73686
[1mStep[0m  [4/42], [94mLoss[0m : 2.47813
[1mStep[0m  [8/42], [94mLoss[0m : 2.64249
[1mStep[0m  [12/42], [94mLoss[0m : 2.71910
[1mStep[0m  [16/42], [94mLoss[0m : 2.67986
[1mStep[0m  [20/42], [94mLoss[0m : 2.56906
[1mStep[0m  [24/42], [94mLoss[0m : 2.66493
[1mStep[0m  [28/42], [94mLoss[0m : 2.87018
[1mStep[0m  [32/42], [94mLoss[0m : 2.58654
[1mStep[0m  [36/42], [94mLoss[0m : 2.63742
[1mStep[0m  [40/42], [94mLoss[0m : 2.74705

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54534
[1mStep[0m  [4/42], [94mLoss[0m : 2.96639
[1mStep[0m  [8/42], [94mLoss[0m : 2.70388
[1mStep[0m  [12/42], [94mLoss[0m : 2.63723
[1mStep[0m  [16/42], [94mLoss[0m : 2.80003
[1mStep[0m  [20/42], [94mLoss[0m : 2.78933
[1mStep[0m  [24/42], [94mLoss[0m : 2.39923
[1mStep[0m  [28/42], [94mLoss[0m : 2.59909
[1mStep[0m  [32/42], [94mLoss[0m : 2.31837
[1mStep[0m  [36/42], [94mLoss[0m : 2.88380
[1mStep[0m  [40/42], [94mLoss[0m : 2.49972

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53029
[1mStep[0m  [4/42], [94mLoss[0m : 2.51000
[1mStep[0m  [8/42], [94mLoss[0m : 2.62570
[1mStep[0m  [12/42], [94mLoss[0m : 2.66864
[1mStep[0m  [16/42], [94mLoss[0m : 2.36104
[1mStep[0m  [20/42], [94mLoss[0m : 2.65775
[1mStep[0m  [24/42], [94mLoss[0m : 2.52366
[1mStep[0m  [28/42], [94mLoss[0m : 2.61442
[1mStep[0m  [32/42], [94mLoss[0m : 2.81374
[1mStep[0m  [36/42], [94mLoss[0m : 2.77750
[1mStep[0m  [40/42], [94mLoss[0m : 2.66575

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73738
[1mStep[0m  [4/42], [94mLoss[0m : 2.65545
[1mStep[0m  [8/42], [94mLoss[0m : 2.55375
[1mStep[0m  [12/42], [94mLoss[0m : 2.82218
[1mStep[0m  [16/42], [94mLoss[0m : 2.54394
[1mStep[0m  [20/42], [94mLoss[0m : 2.69403
[1mStep[0m  [24/42], [94mLoss[0m : 2.55355
[1mStep[0m  [28/42], [94mLoss[0m : 2.61038
[1mStep[0m  [32/42], [94mLoss[0m : 2.56850
[1mStep[0m  [36/42], [94mLoss[0m : 2.70527
[1mStep[0m  [40/42], [94mLoss[0m : 2.74162

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70829
[1mStep[0m  [4/42], [94mLoss[0m : 2.73050
[1mStep[0m  [8/42], [94mLoss[0m : 2.51145
[1mStep[0m  [12/42], [94mLoss[0m : 2.59473
[1mStep[0m  [16/42], [94mLoss[0m : 2.52472
[1mStep[0m  [20/42], [94mLoss[0m : 2.69538
[1mStep[0m  [24/42], [94mLoss[0m : 2.73860
[1mStep[0m  [28/42], [94mLoss[0m : 2.76522
[1mStep[0m  [32/42], [94mLoss[0m : 2.41076
[1mStep[0m  [36/42], [94mLoss[0m : 2.73456
[1mStep[0m  [40/42], [94mLoss[0m : 2.74288

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46122
[1mStep[0m  [4/42], [94mLoss[0m : 2.59656
[1mStep[0m  [8/42], [94mLoss[0m : 2.62045
[1mStep[0m  [12/42], [94mLoss[0m : 2.63393
[1mStep[0m  [16/42], [94mLoss[0m : 2.82339
[1mStep[0m  [20/42], [94mLoss[0m : 2.75722
[1mStep[0m  [24/42], [94mLoss[0m : 2.44179
[1mStep[0m  [28/42], [94mLoss[0m : 2.77196
[1mStep[0m  [32/42], [94mLoss[0m : 2.45677
[1mStep[0m  [36/42], [94mLoss[0m : 2.62562
[1mStep[0m  [40/42], [94mLoss[0m : 2.64237

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41828
[1mStep[0m  [4/42], [94mLoss[0m : 2.48040
[1mStep[0m  [8/42], [94mLoss[0m : 2.48558
[1mStep[0m  [12/42], [94mLoss[0m : 2.57773
[1mStep[0m  [16/42], [94mLoss[0m : 2.54366
[1mStep[0m  [20/42], [94mLoss[0m : 2.64380
[1mStep[0m  [24/42], [94mLoss[0m : 2.77958
[1mStep[0m  [28/42], [94mLoss[0m : 2.55541
[1mStep[0m  [32/42], [94mLoss[0m : 2.75559
[1mStep[0m  [36/42], [94mLoss[0m : 2.37045
[1mStep[0m  [40/42], [94mLoss[0m : 2.61841

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74406
[1mStep[0m  [4/42], [94mLoss[0m : 2.45313
[1mStep[0m  [8/42], [94mLoss[0m : 2.70603
[1mStep[0m  [12/42], [94mLoss[0m : 2.68645
[1mStep[0m  [16/42], [94mLoss[0m : 2.48037
[1mStep[0m  [20/42], [94mLoss[0m : 2.52430
[1mStep[0m  [24/42], [94mLoss[0m : 2.55862
[1mStep[0m  [28/42], [94mLoss[0m : 2.55018
[1mStep[0m  [32/42], [94mLoss[0m : 2.55661
[1mStep[0m  [36/42], [94mLoss[0m : 2.67832
[1mStep[0m  [40/42], [94mLoss[0m : 2.59027

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43935
[1mStep[0m  [4/42], [94mLoss[0m : 2.72813
[1mStep[0m  [8/42], [94mLoss[0m : 2.73137
[1mStep[0m  [12/42], [94mLoss[0m : 2.47893
[1mStep[0m  [16/42], [94mLoss[0m : 2.37918
[1mStep[0m  [20/42], [94mLoss[0m : 2.47361
[1mStep[0m  [24/42], [94mLoss[0m : 2.59675
[1mStep[0m  [28/42], [94mLoss[0m : 2.47128
[1mStep[0m  [32/42], [94mLoss[0m : 2.35774
[1mStep[0m  [36/42], [94mLoss[0m : 2.46732
[1mStep[0m  [40/42], [94mLoss[0m : 2.40125

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51679
[1mStep[0m  [4/42], [94mLoss[0m : 2.59961
[1mStep[0m  [8/42], [94mLoss[0m : 2.65177
[1mStep[0m  [12/42], [94mLoss[0m : 2.72259
[1mStep[0m  [16/42], [94mLoss[0m : 2.54066
[1mStep[0m  [20/42], [94mLoss[0m : 2.64930
[1mStep[0m  [24/42], [94mLoss[0m : 2.55156
[1mStep[0m  [28/42], [94mLoss[0m : 2.55879
[1mStep[0m  [32/42], [94mLoss[0m : 2.49643
[1mStep[0m  [36/42], [94mLoss[0m : 2.63660
[1mStep[0m  [40/42], [94mLoss[0m : 2.58476

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51002
[1mStep[0m  [4/42], [94mLoss[0m : 2.59094
[1mStep[0m  [8/42], [94mLoss[0m : 2.68454
[1mStep[0m  [12/42], [94mLoss[0m : 2.48659
[1mStep[0m  [16/42], [94mLoss[0m : 2.29534
[1mStep[0m  [20/42], [94mLoss[0m : 2.58997
[1mStep[0m  [24/42], [94mLoss[0m : 2.54868
[1mStep[0m  [28/42], [94mLoss[0m : 2.39515
[1mStep[0m  [32/42], [94mLoss[0m : 2.49834
[1mStep[0m  [36/42], [94mLoss[0m : 2.75223
[1mStep[0m  [40/42], [94mLoss[0m : 2.33479

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55195
[1mStep[0m  [4/42], [94mLoss[0m : 2.61341
[1mStep[0m  [8/42], [94mLoss[0m : 2.51642
[1mStep[0m  [12/42], [94mLoss[0m : 2.86554
[1mStep[0m  [16/42], [94mLoss[0m : 2.50114
[1mStep[0m  [20/42], [94mLoss[0m : 2.54769
[1mStep[0m  [24/42], [94mLoss[0m : 2.68906
[1mStep[0m  [28/42], [94mLoss[0m : 2.68666
[1mStep[0m  [32/42], [94mLoss[0m : 2.43522
[1mStep[0m  [36/42], [94mLoss[0m : 2.47452
[1mStep[0m  [40/42], [94mLoss[0m : 2.80749

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57127
[1mStep[0m  [4/42], [94mLoss[0m : 2.18889
[1mStep[0m  [8/42], [94mLoss[0m : 2.37495
[1mStep[0m  [12/42], [94mLoss[0m : 2.58363
[1mStep[0m  [16/42], [94mLoss[0m : 2.64377
[1mStep[0m  [20/42], [94mLoss[0m : 2.53379
[1mStep[0m  [24/42], [94mLoss[0m : 2.52039
[1mStep[0m  [28/42], [94mLoss[0m : 2.77422
[1mStep[0m  [32/42], [94mLoss[0m : 2.62384
[1mStep[0m  [36/42], [94mLoss[0m : 2.53651
[1mStep[0m  [40/42], [94mLoss[0m : 2.71055

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46897
[1mStep[0m  [4/42], [94mLoss[0m : 2.54092
[1mStep[0m  [8/42], [94mLoss[0m : 2.53449
[1mStep[0m  [12/42], [94mLoss[0m : 2.41530
[1mStep[0m  [16/42], [94mLoss[0m : 2.52989
[1mStep[0m  [20/42], [94mLoss[0m : 2.69276
[1mStep[0m  [24/42], [94mLoss[0m : 2.48948
[1mStep[0m  [28/42], [94mLoss[0m : 2.55766
[1mStep[0m  [32/42], [94mLoss[0m : 2.66921
[1mStep[0m  [36/42], [94mLoss[0m : 2.58410
[1mStep[0m  [40/42], [94mLoss[0m : 2.53361

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24363
[1mStep[0m  [4/42], [94mLoss[0m : 2.61719
[1mStep[0m  [8/42], [94mLoss[0m : 2.41565
[1mStep[0m  [12/42], [94mLoss[0m : 2.51373
[1mStep[0m  [16/42], [94mLoss[0m : 2.27798
[1mStep[0m  [20/42], [94mLoss[0m : 2.39234
[1mStep[0m  [24/42], [94mLoss[0m : 2.50230
[1mStep[0m  [28/42], [94mLoss[0m : 2.42064
[1mStep[0m  [32/42], [94mLoss[0m : 2.34913
[1mStep[0m  [36/42], [94mLoss[0m : 2.80243
[1mStep[0m  [40/42], [94mLoss[0m : 2.66608

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50439
[1mStep[0m  [4/42], [94mLoss[0m : 2.71128
[1mStep[0m  [8/42], [94mLoss[0m : 2.39881
[1mStep[0m  [12/42], [94mLoss[0m : 2.53811
[1mStep[0m  [16/42], [94mLoss[0m : 2.71455
[1mStep[0m  [20/42], [94mLoss[0m : 2.53729
[1mStep[0m  [24/42], [94mLoss[0m : 2.54238
[1mStep[0m  [28/42], [94mLoss[0m : 2.32896
[1mStep[0m  [32/42], [94mLoss[0m : 2.30469
[1mStep[0m  [36/42], [94mLoss[0m : 2.51466
[1mStep[0m  [40/42], [94mLoss[0m : 2.61436

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56663
[1mStep[0m  [4/42], [94mLoss[0m : 2.51556
[1mStep[0m  [8/42], [94mLoss[0m : 2.50194
[1mStep[0m  [12/42], [94mLoss[0m : 2.46004
[1mStep[0m  [16/42], [94mLoss[0m : 2.58754
[1mStep[0m  [20/42], [94mLoss[0m : 2.67492
[1mStep[0m  [24/42], [94mLoss[0m : 2.40449
[1mStep[0m  [28/42], [94mLoss[0m : 2.76211
[1mStep[0m  [32/42], [94mLoss[0m : 2.26277
[1mStep[0m  [36/42], [94mLoss[0m : 2.45902
[1mStep[0m  [40/42], [94mLoss[0m : 2.55922

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55690
[1mStep[0m  [4/42], [94mLoss[0m : 2.78779
[1mStep[0m  [8/42], [94mLoss[0m : 2.57363
[1mStep[0m  [12/42], [94mLoss[0m : 2.39772
[1mStep[0m  [16/42], [94mLoss[0m : 2.21654
[1mStep[0m  [20/42], [94mLoss[0m : 2.34517
[1mStep[0m  [24/42], [94mLoss[0m : 2.37236
[1mStep[0m  [28/42], [94mLoss[0m : 2.54994
[1mStep[0m  [32/42], [94mLoss[0m : 2.51149
[1mStep[0m  [36/42], [94mLoss[0m : 2.53554
[1mStep[0m  [40/42], [94mLoss[0m : 2.55373

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65937
[1mStep[0m  [4/42], [94mLoss[0m : 2.37368
[1mStep[0m  [8/42], [94mLoss[0m : 2.42670
[1mStep[0m  [12/42], [94mLoss[0m : 2.57593
[1mStep[0m  [16/42], [94mLoss[0m : 2.62343
[1mStep[0m  [20/42], [94mLoss[0m : 2.63614
[1mStep[0m  [24/42], [94mLoss[0m : 2.43083
[1mStep[0m  [28/42], [94mLoss[0m : 2.48130
[1mStep[0m  [32/42], [94mLoss[0m : 2.47037
[1mStep[0m  [36/42], [94mLoss[0m : 2.49091
[1mStep[0m  [40/42], [94mLoss[0m : 2.26789

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54357
[1mStep[0m  [4/42], [94mLoss[0m : 2.62024
[1mStep[0m  [8/42], [94mLoss[0m : 2.46282
[1mStep[0m  [12/42], [94mLoss[0m : 2.69727
[1mStep[0m  [16/42], [94mLoss[0m : 2.32674
[1mStep[0m  [20/42], [94mLoss[0m : 2.95022
[1mStep[0m  [24/42], [94mLoss[0m : 2.53321
[1mStep[0m  [28/42], [94mLoss[0m : 2.36952
[1mStep[0m  [32/42], [94mLoss[0m : 2.32830
[1mStep[0m  [36/42], [94mLoss[0m : 2.44653
[1mStep[0m  [40/42], [94mLoss[0m : 2.43658

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48933
[1mStep[0m  [4/42], [94mLoss[0m : 2.47769
[1mStep[0m  [8/42], [94mLoss[0m : 2.63624
[1mStep[0m  [12/42], [94mLoss[0m : 2.38946
[1mStep[0m  [16/42], [94mLoss[0m : 2.44916
[1mStep[0m  [20/42], [94mLoss[0m : 2.45929
[1mStep[0m  [24/42], [94mLoss[0m : 2.50656
[1mStep[0m  [28/42], [94mLoss[0m : 2.58701
[1mStep[0m  [32/42], [94mLoss[0m : 2.37633
[1mStep[0m  [36/42], [94mLoss[0m : 2.56531
[1mStep[0m  [40/42], [94mLoss[0m : 2.28959

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.310, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50568
[1mStep[0m  [4/42], [94mLoss[0m : 2.55652
[1mStep[0m  [8/42], [94mLoss[0m : 2.52133
[1mStep[0m  [12/42], [94mLoss[0m : 2.30616
[1mStep[0m  [16/42], [94mLoss[0m : 2.71997
[1mStep[0m  [20/42], [94mLoss[0m : 2.42972
[1mStep[0m  [24/42], [94mLoss[0m : 2.22367
[1mStep[0m  [28/42], [94mLoss[0m : 2.68967
[1mStep[0m  [32/42], [94mLoss[0m : 2.50539
[1mStep[0m  [36/42], [94mLoss[0m : 2.35614
[1mStep[0m  [40/42], [94mLoss[0m : 2.47921

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49753
[1mStep[0m  [4/42], [94mLoss[0m : 2.46575
[1mStep[0m  [8/42], [94mLoss[0m : 2.31929
[1mStep[0m  [12/42], [94mLoss[0m : 2.47079
[1mStep[0m  [16/42], [94mLoss[0m : 2.33403
[1mStep[0m  [20/42], [94mLoss[0m : 2.55594
[1mStep[0m  [24/42], [94mLoss[0m : 2.41440
[1mStep[0m  [28/42], [94mLoss[0m : 2.66968
[1mStep[0m  [32/42], [94mLoss[0m : 2.39090
[1mStep[0m  [36/42], [94mLoss[0m : 2.68619
[1mStep[0m  [40/42], [94mLoss[0m : 2.51450

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45831
[1mStep[0m  [4/42], [94mLoss[0m : 2.45041
[1mStep[0m  [8/42], [94mLoss[0m : 2.59125
[1mStep[0m  [12/42], [94mLoss[0m : 2.41845
[1mStep[0m  [16/42], [94mLoss[0m : 2.45214
[1mStep[0m  [20/42], [94mLoss[0m : 2.54578
[1mStep[0m  [24/42], [94mLoss[0m : 2.54315
[1mStep[0m  [28/42], [94mLoss[0m : 2.58681
[1mStep[0m  [32/42], [94mLoss[0m : 2.42531
[1mStep[0m  [36/42], [94mLoss[0m : 2.58471
[1mStep[0m  [40/42], [94mLoss[0m : 2.21049

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.316
====================================

Phase 1 - Evaluation MAE:  2.3162425586155484
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.54426
[1mStep[0m  [4/42], [94mLoss[0m : 2.49914
[1mStep[0m  [8/42], [94mLoss[0m : 2.65342
[1mStep[0m  [12/42], [94mLoss[0m : 2.38423
[1mStep[0m  [16/42], [94mLoss[0m : 2.46899
[1mStep[0m  [20/42], [94mLoss[0m : 2.51945
[1mStep[0m  [24/42], [94mLoss[0m : 2.64390
[1mStep[0m  [28/42], [94mLoss[0m : 2.44566
[1mStep[0m  [32/42], [94mLoss[0m : 2.31194
[1mStep[0m  [36/42], [94mLoss[0m : 2.64912
[1mStep[0m  [40/42], [94mLoss[0m : 2.55266

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.318, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60403
[1mStep[0m  [4/42], [94mLoss[0m : 2.34808
[1mStep[0m  [8/42], [94mLoss[0m : 2.45341
[1mStep[0m  [12/42], [94mLoss[0m : 2.47144
[1mStep[0m  [16/42], [94mLoss[0m : 2.51434
[1mStep[0m  [20/42], [94mLoss[0m : 2.56556
[1mStep[0m  [24/42], [94mLoss[0m : 2.52824
[1mStep[0m  [28/42], [94mLoss[0m : 2.50436
[1mStep[0m  [32/42], [94mLoss[0m : 2.42553
[1mStep[0m  [36/42], [94mLoss[0m : 2.44954
[1mStep[0m  [40/42], [94mLoss[0m : 2.69741

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35929
[1mStep[0m  [4/42], [94mLoss[0m : 2.56060
[1mStep[0m  [8/42], [94mLoss[0m : 2.27305
[1mStep[0m  [12/42], [94mLoss[0m : 2.51101
[1mStep[0m  [16/42], [94mLoss[0m : 2.56572
[1mStep[0m  [20/42], [94mLoss[0m : 2.54756
[1mStep[0m  [24/42], [94mLoss[0m : 2.48992
[1mStep[0m  [28/42], [94mLoss[0m : 2.46373
[1mStep[0m  [32/42], [94mLoss[0m : 2.23910
[1mStep[0m  [36/42], [94mLoss[0m : 2.53583
[1mStep[0m  [40/42], [94mLoss[0m : 2.42468

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50692
[1mStep[0m  [4/42], [94mLoss[0m : 2.40387
[1mStep[0m  [8/42], [94mLoss[0m : 2.48624
[1mStep[0m  [12/42], [94mLoss[0m : 2.42045
[1mStep[0m  [16/42], [94mLoss[0m : 2.34039
[1mStep[0m  [20/42], [94mLoss[0m : 2.54777
[1mStep[0m  [24/42], [94mLoss[0m : 2.50096
[1mStep[0m  [28/42], [94mLoss[0m : 2.40917
[1mStep[0m  [32/42], [94mLoss[0m : 2.18813
[1mStep[0m  [36/42], [94mLoss[0m : 2.35617
[1mStep[0m  [40/42], [94mLoss[0m : 2.44803

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30213
[1mStep[0m  [4/42], [94mLoss[0m : 2.39894
[1mStep[0m  [8/42], [94mLoss[0m : 2.35269
[1mStep[0m  [12/42], [94mLoss[0m : 2.32776
[1mStep[0m  [16/42], [94mLoss[0m : 2.25209
[1mStep[0m  [20/42], [94mLoss[0m : 2.16641
[1mStep[0m  [24/42], [94mLoss[0m : 2.32529
[1mStep[0m  [28/42], [94mLoss[0m : 2.23366
[1mStep[0m  [32/42], [94mLoss[0m : 2.32215
[1mStep[0m  [36/42], [94mLoss[0m : 2.43653
[1mStep[0m  [40/42], [94mLoss[0m : 2.34651

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47987
[1mStep[0m  [4/42], [94mLoss[0m : 2.19232
[1mStep[0m  [8/42], [94mLoss[0m : 2.17291
[1mStep[0m  [12/42], [94mLoss[0m : 2.29629
[1mStep[0m  [16/42], [94mLoss[0m : 2.25185
[1mStep[0m  [20/42], [94mLoss[0m : 2.11333
[1mStep[0m  [24/42], [94mLoss[0m : 2.28337
[1mStep[0m  [28/42], [94mLoss[0m : 2.37884
[1mStep[0m  [32/42], [94mLoss[0m : 2.18706
[1mStep[0m  [36/42], [94mLoss[0m : 2.26179
[1mStep[0m  [40/42], [94mLoss[0m : 2.22606

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12323
[1mStep[0m  [4/42], [94mLoss[0m : 2.26329
[1mStep[0m  [8/42], [94mLoss[0m : 2.10832
[1mStep[0m  [12/42], [94mLoss[0m : 2.21715
[1mStep[0m  [16/42], [94mLoss[0m : 2.54087
[1mStep[0m  [20/42], [94mLoss[0m : 2.27082
[1mStep[0m  [24/42], [94mLoss[0m : 2.30985
[1mStep[0m  [28/42], [94mLoss[0m : 2.39300
[1mStep[0m  [32/42], [94mLoss[0m : 2.36704
[1mStep[0m  [36/42], [94mLoss[0m : 2.34098
[1mStep[0m  [40/42], [94mLoss[0m : 2.25508

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06361
[1mStep[0m  [4/42], [94mLoss[0m : 2.27678
[1mStep[0m  [8/42], [94mLoss[0m : 2.22459
[1mStep[0m  [12/42], [94mLoss[0m : 2.19102
[1mStep[0m  [16/42], [94mLoss[0m : 2.39151
[1mStep[0m  [20/42], [94mLoss[0m : 2.26943
[1mStep[0m  [24/42], [94mLoss[0m : 1.93827
[1mStep[0m  [28/42], [94mLoss[0m : 2.42844
[1mStep[0m  [32/42], [94mLoss[0m : 2.16736
[1mStep[0m  [36/42], [94mLoss[0m : 1.93651
[1mStep[0m  [40/42], [94mLoss[0m : 2.18506

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13040
[1mStep[0m  [4/42], [94mLoss[0m : 2.13869
[1mStep[0m  [8/42], [94mLoss[0m : 2.10533
[1mStep[0m  [12/42], [94mLoss[0m : 2.02996
[1mStep[0m  [16/42], [94mLoss[0m : 2.13494
[1mStep[0m  [20/42], [94mLoss[0m : 2.23363
[1mStep[0m  [24/42], [94mLoss[0m : 2.33481
[1mStep[0m  [28/42], [94mLoss[0m : 2.28820
[1mStep[0m  [32/42], [94mLoss[0m : 2.21482
[1mStep[0m  [36/42], [94mLoss[0m : 2.02651
[1mStep[0m  [40/42], [94mLoss[0m : 2.24525

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.197, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17656
[1mStep[0m  [4/42], [94mLoss[0m : 2.21123
[1mStep[0m  [8/42], [94mLoss[0m : 2.17213
[1mStep[0m  [12/42], [94mLoss[0m : 2.16500
[1mStep[0m  [16/42], [94mLoss[0m : 2.15651
[1mStep[0m  [20/42], [94mLoss[0m : 2.22957
[1mStep[0m  [24/42], [94mLoss[0m : 2.26681
[1mStep[0m  [28/42], [94mLoss[0m : 2.21242
[1mStep[0m  [32/42], [94mLoss[0m : 2.37747
[1mStep[0m  [36/42], [94mLoss[0m : 2.10395
[1mStep[0m  [40/42], [94mLoss[0m : 2.09868

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19640
[1mStep[0m  [4/42], [94mLoss[0m : 2.27867
[1mStep[0m  [8/42], [94mLoss[0m : 2.07178
[1mStep[0m  [12/42], [94mLoss[0m : 2.05095
[1mStep[0m  [16/42], [94mLoss[0m : 2.03733
[1mStep[0m  [20/42], [94mLoss[0m : 2.22180
[1mStep[0m  [24/42], [94mLoss[0m : 2.05666
[1mStep[0m  [28/42], [94mLoss[0m : 2.03440
[1mStep[0m  [32/42], [94mLoss[0m : 2.33681
[1mStep[0m  [36/42], [94mLoss[0m : 2.10527
[1mStep[0m  [40/42], [94mLoss[0m : 2.09330

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94934
[1mStep[0m  [4/42], [94mLoss[0m : 1.89693
[1mStep[0m  [8/42], [94mLoss[0m : 2.02694
[1mStep[0m  [12/42], [94mLoss[0m : 2.26587
[1mStep[0m  [16/42], [94mLoss[0m : 1.94798
[1mStep[0m  [20/42], [94mLoss[0m : 2.08287
[1mStep[0m  [24/42], [94mLoss[0m : 2.03793
[1mStep[0m  [28/42], [94mLoss[0m : 1.77238
[1mStep[0m  [32/42], [94mLoss[0m : 1.89102
[1mStep[0m  [36/42], [94mLoss[0m : 2.18004
[1mStep[0m  [40/42], [94mLoss[0m : 2.14760

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13963
[1mStep[0m  [4/42], [94mLoss[0m : 2.01830
[1mStep[0m  [8/42], [94mLoss[0m : 2.09558
[1mStep[0m  [12/42], [94mLoss[0m : 1.88502
[1mStep[0m  [16/42], [94mLoss[0m : 1.98506
[1mStep[0m  [20/42], [94mLoss[0m : 2.01234
[1mStep[0m  [24/42], [94mLoss[0m : 1.80796
[1mStep[0m  [28/42], [94mLoss[0m : 1.99414
[1mStep[0m  [32/42], [94mLoss[0m : 2.18867
[1mStep[0m  [36/42], [94mLoss[0m : 1.98023
[1mStep[0m  [40/42], [94mLoss[0m : 2.26248

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88353
[1mStep[0m  [4/42], [94mLoss[0m : 1.84490
[1mStep[0m  [8/42], [94mLoss[0m : 2.04116
[1mStep[0m  [12/42], [94mLoss[0m : 1.87688
[1mStep[0m  [16/42], [94mLoss[0m : 2.11223
[1mStep[0m  [20/42], [94mLoss[0m : 1.92181
[1mStep[0m  [24/42], [94mLoss[0m : 2.02139
[1mStep[0m  [28/42], [94mLoss[0m : 2.01037
[1mStep[0m  [32/42], [94mLoss[0m : 2.14530
[1mStep[0m  [36/42], [94mLoss[0m : 1.90785
[1mStep[0m  [40/42], [94mLoss[0m : 2.05344

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13012
[1mStep[0m  [4/42], [94mLoss[0m : 1.93050
[1mStep[0m  [8/42], [94mLoss[0m : 2.07076
[1mStep[0m  [12/42], [94mLoss[0m : 1.80260
[1mStep[0m  [16/42], [94mLoss[0m : 1.97720
[1mStep[0m  [20/42], [94mLoss[0m : 2.10126
[1mStep[0m  [24/42], [94mLoss[0m : 1.91694
[1mStep[0m  [28/42], [94mLoss[0m : 1.96427
[1mStep[0m  [32/42], [94mLoss[0m : 2.04858
[1mStep[0m  [36/42], [94mLoss[0m : 1.90512
[1mStep[0m  [40/42], [94mLoss[0m : 2.06746

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92483
[1mStep[0m  [4/42], [94mLoss[0m : 2.12160
[1mStep[0m  [8/42], [94mLoss[0m : 2.04247
[1mStep[0m  [12/42], [94mLoss[0m : 1.96825
[1mStep[0m  [16/42], [94mLoss[0m : 1.98890
[1mStep[0m  [20/42], [94mLoss[0m : 1.87116
[1mStep[0m  [24/42], [94mLoss[0m : 1.98796
[1mStep[0m  [28/42], [94mLoss[0m : 1.96564
[1mStep[0m  [32/42], [94mLoss[0m : 2.09943
[1mStep[0m  [36/42], [94mLoss[0m : 1.91905
[1mStep[0m  [40/42], [94mLoss[0m : 1.94375

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.923, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69873
[1mStep[0m  [4/42], [94mLoss[0m : 1.87514
[1mStep[0m  [8/42], [94mLoss[0m : 1.63642
[1mStep[0m  [12/42], [94mLoss[0m : 1.84765
[1mStep[0m  [16/42], [94mLoss[0m : 1.79529
[1mStep[0m  [20/42], [94mLoss[0m : 1.94047
[1mStep[0m  [24/42], [94mLoss[0m : 2.01851
[1mStep[0m  [28/42], [94mLoss[0m : 1.74246
[1mStep[0m  [32/42], [94mLoss[0m : 1.83403
[1mStep[0m  [36/42], [94mLoss[0m : 1.95936
[1mStep[0m  [40/42], [94mLoss[0m : 1.83646

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78528
[1mStep[0m  [4/42], [94mLoss[0m : 1.65556
[1mStep[0m  [8/42], [94mLoss[0m : 1.66035
[1mStep[0m  [12/42], [94mLoss[0m : 1.87266
[1mStep[0m  [16/42], [94mLoss[0m : 1.72754
[1mStep[0m  [20/42], [94mLoss[0m : 1.78490
[1mStep[0m  [24/42], [94mLoss[0m : 2.02442
[1mStep[0m  [28/42], [94mLoss[0m : 1.81114
[1mStep[0m  [32/42], [94mLoss[0m : 1.89716
[1mStep[0m  [36/42], [94mLoss[0m : 1.80414
[1mStep[0m  [40/42], [94mLoss[0m : 1.92454

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77576
[1mStep[0m  [4/42], [94mLoss[0m : 1.72242
[1mStep[0m  [8/42], [94mLoss[0m : 1.73176
[1mStep[0m  [12/42], [94mLoss[0m : 1.81920
[1mStep[0m  [16/42], [94mLoss[0m : 1.87969
[1mStep[0m  [20/42], [94mLoss[0m : 1.55355
[1mStep[0m  [24/42], [94mLoss[0m : 1.69923
[1mStep[0m  [28/42], [94mLoss[0m : 1.89808
[1mStep[0m  [32/42], [94mLoss[0m : 1.86869
[1mStep[0m  [36/42], [94mLoss[0m : 1.93765
[1mStep[0m  [40/42], [94mLoss[0m : 1.86109

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60773
[1mStep[0m  [4/42], [94mLoss[0m : 1.64251
[1mStep[0m  [8/42], [94mLoss[0m : 1.96929
[1mStep[0m  [12/42], [94mLoss[0m : 1.79675
[1mStep[0m  [16/42], [94mLoss[0m : 1.87680
[1mStep[0m  [20/42], [94mLoss[0m : 1.86275
[1mStep[0m  [24/42], [94mLoss[0m : 1.76537
[1mStep[0m  [28/42], [94mLoss[0m : 1.79451
[1mStep[0m  [32/42], [94mLoss[0m : 1.81019
[1mStep[0m  [36/42], [94mLoss[0m : 1.72699
[1mStep[0m  [40/42], [94mLoss[0m : 1.65915

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.461, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73774
[1mStep[0m  [4/42], [94mLoss[0m : 1.76377
[1mStep[0m  [8/42], [94mLoss[0m : 1.71027
[1mStep[0m  [12/42], [94mLoss[0m : 1.57604
[1mStep[0m  [16/42], [94mLoss[0m : 1.93971
[1mStep[0m  [20/42], [94mLoss[0m : 1.69839
[1mStep[0m  [24/42], [94mLoss[0m : 1.54330
[1mStep[0m  [28/42], [94mLoss[0m : 1.81179
[1mStep[0m  [32/42], [94mLoss[0m : 1.71545
[1mStep[0m  [36/42], [94mLoss[0m : 1.70298
[1mStep[0m  [40/42], [94mLoss[0m : 1.79610

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77005
[1mStep[0m  [4/42], [94mLoss[0m : 1.52247
[1mStep[0m  [8/42], [94mLoss[0m : 1.79473
[1mStep[0m  [12/42], [94mLoss[0m : 1.65621
[1mStep[0m  [16/42], [94mLoss[0m : 1.75740
[1mStep[0m  [20/42], [94mLoss[0m : 1.73850
[1mStep[0m  [24/42], [94mLoss[0m : 1.56915
[1mStep[0m  [28/42], [94mLoss[0m : 1.49559
[1mStep[0m  [32/42], [94mLoss[0m : 1.78863
[1mStep[0m  [36/42], [94mLoss[0m : 1.64177
[1mStep[0m  [40/42], [94mLoss[0m : 1.69888

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.468, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61346
[1mStep[0m  [4/42], [94mLoss[0m : 1.57187
[1mStep[0m  [8/42], [94mLoss[0m : 1.55490
[1mStep[0m  [12/42], [94mLoss[0m : 1.66548
[1mStep[0m  [16/42], [94mLoss[0m : 1.77501
[1mStep[0m  [20/42], [94mLoss[0m : 1.74705
[1mStep[0m  [24/42], [94mLoss[0m : 1.57803
[1mStep[0m  [28/42], [94mLoss[0m : 1.61789
[1mStep[0m  [32/42], [94mLoss[0m : 1.69647
[1mStep[0m  [36/42], [94mLoss[0m : 1.72794
[1mStep[0m  [40/42], [94mLoss[0m : 1.80415

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67862
[1mStep[0m  [4/42], [94mLoss[0m : 1.65331
[1mStep[0m  [8/42], [94mLoss[0m : 1.49611
[1mStep[0m  [12/42], [94mLoss[0m : 1.65531
[1mStep[0m  [16/42], [94mLoss[0m : 1.83448
[1mStep[0m  [20/42], [94mLoss[0m : 1.59179
[1mStep[0m  [24/42], [94mLoss[0m : 1.52564
[1mStep[0m  [28/42], [94mLoss[0m : 1.68665
[1mStep[0m  [32/42], [94mLoss[0m : 1.58706
[1mStep[0m  [36/42], [94mLoss[0m : 1.64947
[1mStep[0m  [40/42], [94mLoss[0m : 1.52388

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.667, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60695
[1mStep[0m  [4/42], [94mLoss[0m : 1.66171
[1mStep[0m  [8/42], [94mLoss[0m : 1.62331
[1mStep[0m  [12/42], [94mLoss[0m : 1.54361
[1mStep[0m  [16/42], [94mLoss[0m : 1.79642
[1mStep[0m  [20/42], [94mLoss[0m : 1.56932
[1mStep[0m  [24/42], [94mLoss[0m : 1.66049
[1mStep[0m  [28/42], [94mLoss[0m : 1.63348
[1mStep[0m  [32/42], [94mLoss[0m : 1.62342
[1mStep[0m  [36/42], [94mLoss[0m : 1.69106
[1mStep[0m  [40/42], [94mLoss[0m : 1.71350

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.662, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57522
[1mStep[0m  [4/42], [94mLoss[0m : 1.55981
[1mStep[0m  [8/42], [94mLoss[0m : 1.84071
[1mStep[0m  [12/42], [94mLoss[0m : 1.59980
[1mStep[0m  [16/42], [94mLoss[0m : 1.68795
[1mStep[0m  [20/42], [94mLoss[0m : 1.54258
[1mStep[0m  [24/42], [94mLoss[0m : 1.51043
[1mStep[0m  [28/42], [94mLoss[0m : 1.66013
[1mStep[0m  [32/42], [94mLoss[0m : 1.68685
[1mStep[0m  [36/42], [94mLoss[0m : 1.69832
[1mStep[0m  [40/42], [94mLoss[0m : 1.67731

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.614, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67907
[1mStep[0m  [4/42], [94mLoss[0m : 1.65954
[1mStep[0m  [8/42], [94mLoss[0m : 1.60587
[1mStep[0m  [12/42], [94mLoss[0m : 1.68733
[1mStep[0m  [16/42], [94mLoss[0m : 1.74653
[1mStep[0m  [20/42], [94mLoss[0m : 1.49030
[1mStep[0m  [24/42], [94mLoss[0m : 1.55839
[1mStep[0m  [28/42], [94mLoss[0m : 1.65070
[1mStep[0m  [32/42], [94mLoss[0m : 1.75105
[1mStep[0m  [36/42], [94mLoss[0m : 1.66588
[1mStep[0m  [40/42], [94mLoss[0m : 1.56226

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.608, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61246
[1mStep[0m  [4/42], [94mLoss[0m : 1.42895
[1mStep[0m  [8/42], [94mLoss[0m : 1.56009
[1mStep[0m  [12/42], [94mLoss[0m : 1.50301
[1mStep[0m  [16/42], [94mLoss[0m : 1.52491
[1mStep[0m  [20/42], [94mLoss[0m : 1.65963
[1mStep[0m  [24/42], [94mLoss[0m : 1.98941
[1mStep[0m  [28/42], [94mLoss[0m : 1.50580
[1mStep[0m  [32/42], [94mLoss[0m : 1.57859
[1mStep[0m  [36/42], [94mLoss[0m : 1.53354
[1mStep[0m  [40/42], [94mLoss[0m : 1.40942

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43865
[1mStep[0m  [4/42], [94mLoss[0m : 1.64065
[1mStep[0m  [8/42], [94mLoss[0m : 1.54354
[1mStep[0m  [12/42], [94mLoss[0m : 1.65232
[1mStep[0m  [16/42], [94mLoss[0m : 1.52113
[1mStep[0m  [20/42], [94mLoss[0m : 1.48422
[1mStep[0m  [24/42], [94mLoss[0m : 1.60279
[1mStep[0m  [28/42], [94mLoss[0m : 1.64770
[1mStep[0m  [32/42], [94mLoss[0m : 1.57256
[1mStep[0m  [36/42], [94mLoss[0m : 1.56622
[1mStep[0m  [40/42], [94mLoss[0m : 1.54053

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.530, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45620
[1mStep[0m  [4/42], [94mLoss[0m : 1.57728
[1mStep[0m  [8/42], [94mLoss[0m : 1.53214
[1mStep[0m  [12/42], [94mLoss[0m : 1.63875
[1mStep[0m  [16/42], [94mLoss[0m : 1.66440
[1mStep[0m  [20/42], [94mLoss[0m : 1.61896
[1mStep[0m  [24/42], [94mLoss[0m : 1.54863
[1mStep[0m  [28/42], [94mLoss[0m : 1.47508
[1mStep[0m  [32/42], [94mLoss[0m : 1.46637
[1mStep[0m  [36/42], [94mLoss[0m : 1.48582
[1mStep[0m  [40/42], [94mLoss[0m : 1.71181

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.512, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.493716938155038
MAE score P1       2.316243
MAE score P2       2.493717
loss               1.580438
learning_rate          0.01
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 17, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.14843
[1mStep[0m  [2/21], [94mLoss[0m : 10.29990
[1mStep[0m  [4/21], [94mLoss[0m : 9.06933
[1mStep[0m  [6/21], [94mLoss[0m : 7.35022
[1mStep[0m  [8/21], [94mLoss[0m : 6.08761
[1mStep[0m  [10/21], [94mLoss[0m : 5.19043
[1mStep[0m  [12/21], [94mLoss[0m : 4.00161
[1mStep[0m  [14/21], [94mLoss[0m : 3.55512
[1mStep[0m  [16/21], [94mLoss[0m : 3.14003
[1mStep[0m  [18/21], [94mLoss[0m : 2.72054
[1mStep[0m  [20/21], [94mLoss[0m : 2.87886

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.932, [92mTest[0m: 11.036, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79045
[1mStep[0m  [2/21], [94mLoss[0m : 2.70898
[1mStep[0m  [4/21], [94mLoss[0m : 2.69486
[1mStep[0m  [6/21], [94mLoss[0m : 2.75758
[1mStep[0m  [8/21], [94mLoss[0m : 2.57505
[1mStep[0m  [10/21], [94mLoss[0m : 2.66708
[1mStep[0m  [12/21], [94mLoss[0m : 2.63882
[1mStep[0m  [14/21], [94mLoss[0m : 2.75365
[1mStep[0m  [16/21], [94mLoss[0m : 2.62969
[1mStep[0m  [18/21], [94mLoss[0m : 2.67224
[1mStep[0m  [20/21], [94mLoss[0m : 2.49824

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.664, [92mTest[0m: 4.753, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60881
[1mStep[0m  [2/21], [94mLoss[0m : 2.55011
[1mStep[0m  [4/21], [94mLoss[0m : 2.53421
[1mStep[0m  [6/21], [94mLoss[0m : 2.66922
[1mStep[0m  [8/21], [94mLoss[0m : 2.64022
[1mStep[0m  [10/21], [94mLoss[0m : 2.62669
[1mStep[0m  [12/21], [94mLoss[0m : 2.60484
[1mStep[0m  [14/21], [94mLoss[0m : 2.52627
[1mStep[0m  [16/21], [94mLoss[0m : 2.54999
[1mStep[0m  [18/21], [94mLoss[0m : 2.57532
[1mStep[0m  [20/21], [94mLoss[0m : 2.40216

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.603, [92mTest[0m: 3.035, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59526
[1mStep[0m  [2/21], [94mLoss[0m : 2.72520
[1mStep[0m  [4/21], [94mLoss[0m : 2.70586
[1mStep[0m  [6/21], [94mLoss[0m : 2.45772
[1mStep[0m  [8/21], [94mLoss[0m : 2.51262
[1mStep[0m  [10/21], [94mLoss[0m : 2.42995
[1mStep[0m  [12/21], [94mLoss[0m : 2.71920
[1mStep[0m  [14/21], [94mLoss[0m : 2.64645
[1mStep[0m  [16/21], [94mLoss[0m : 2.61049
[1mStep[0m  [18/21], [94mLoss[0m : 2.59619
[1mStep[0m  [20/21], [94mLoss[0m : 2.59603

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.721, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46419
[1mStep[0m  [2/21], [94mLoss[0m : 2.52728
[1mStep[0m  [4/21], [94mLoss[0m : 2.53954
[1mStep[0m  [6/21], [94mLoss[0m : 2.67410
[1mStep[0m  [8/21], [94mLoss[0m : 2.50538
[1mStep[0m  [10/21], [94mLoss[0m : 2.53764
[1mStep[0m  [12/21], [94mLoss[0m : 2.41758
[1mStep[0m  [14/21], [94mLoss[0m : 2.49258
[1mStep[0m  [16/21], [94mLoss[0m : 2.73631
[1mStep[0m  [18/21], [94mLoss[0m : 2.69176
[1mStep[0m  [20/21], [94mLoss[0m : 2.58453

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.654, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64530
[1mStep[0m  [2/21], [94mLoss[0m : 2.55321
[1mStep[0m  [4/21], [94mLoss[0m : 2.31717
[1mStep[0m  [6/21], [94mLoss[0m : 2.66311
[1mStep[0m  [8/21], [94mLoss[0m : 2.56159
[1mStep[0m  [10/21], [94mLoss[0m : 2.54533
[1mStep[0m  [12/21], [94mLoss[0m : 2.43391
[1mStep[0m  [14/21], [94mLoss[0m : 2.63197
[1mStep[0m  [16/21], [94mLoss[0m : 2.47174
[1mStep[0m  [18/21], [94mLoss[0m : 2.50574
[1mStep[0m  [20/21], [94mLoss[0m : 2.59928

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.623, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54201
[1mStep[0m  [2/21], [94mLoss[0m : 2.40989
[1mStep[0m  [4/21], [94mLoss[0m : 2.62217
[1mStep[0m  [6/21], [94mLoss[0m : 2.43781
[1mStep[0m  [8/21], [94mLoss[0m : 2.70022
[1mStep[0m  [10/21], [94mLoss[0m : 2.55094
[1mStep[0m  [12/21], [94mLoss[0m : 2.46485
[1mStep[0m  [14/21], [94mLoss[0m : 2.58529
[1mStep[0m  [16/21], [94mLoss[0m : 2.48023
[1mStep[0m  [18/21], [94mLoss[0m : 2.54832
[1mStep[0m  [20/21], [94mLoss[0m : 2.37356

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.601, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56930
[1mStep[0m  [2/21], [94mLoss[0m : 2.58360
[1mStep[0m  [4/21], [94mLoss[0m : 2.49539
[1mStep[0m  [6/21], [94mLoss[0m : 2.56791
[1mStep[0m  [8/21], [94mLoss[0m : 2.52714
[1mStep[0m  [10/21], [94mLoss[0m : 2.67086
[1mStep[0m  [12/21], [94mLoss[0m : 2.41381
[1mStep[0m  [14/21], [94mLoss[0m : 2.47805
[1mStep[0m  [16/21], [94mLoss[0m : 2.55709
[1mStep[0m  [18/21], [94mLoss[0m : 2.56848
[1mStep[0m  [20/21], [94mLoss[0m : 2.42438

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55337
[1mStep[0m  [2/21], [94mLoss[0m : 2.52409
[1mStep[0m  [4/21], [94mLoss[0m : 2.34533
[1mStep[0m  [6/21], [94mLoss[0m : 2.46166
[1mStep[0m  [8/21], [94mLoss[0m : 2.54257
[1mStep[0m  [10/21], [94mLoss[0m : 2.44941
[1mStep[0m  [12/21], [94mLoss[0m : 2.56807
[1mStep[0m  [14/21], [94mLoss[0m : 2.41130
[1mStep[0m  [16/21], [94mLoss[0m : 2.58480
[1mStep[0m  [18/21], [94mLoss[0m : 2.48097
[1mStep[0m  [20/21], [94mLoss[0m : 2.63645

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46579
[1mStep[0m  [2/21], [94mLoss[0m : 2.46819
[1mStep[0m  [4/21], [94mLoss[0m : 2.47060
[1mStep[0m  [6/21], [94mLoss[0m : 2.55386
[1mStep[0m  [8/21], [94mLoss[0m : 2.46775
[1mStep[0m  [10/21], [94mLoss[0m : 2.32793
[1mStep[0m  [12/21], [94mLoss[0m : 2.40831
[1mStep[0m  [14/21], [94mLoss[0m : 2.60938
[1mStep[0m  [16/21], [94mLoss[0m : 2.64029
[1mStep[0m  [18/21], [94mLoss[0m : 2.41043
[1mStep[0m  [20/21], [94mLoss[0m : 2.54247

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.585, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51766
[1mStep[0m  [2/21], [94mLoss[0m : 2.38959
[1mStep[0m  [4/21], [94mLoss[0m : 2.43930
[1mStep[0m  [6/21], [94mLoss[0m : 2.56422
[1mStep[0m  [8/21], [94mLoss[0m : 2.43236
[1mStep[0m  [10/21], [94mLoss[0m : 2.36918
[1mStep[0m  [12/21], [94mLoss[0m : 2.67886
[1mStep[0m  [14/21], [94mLoss[0m : 2.47872
[1mStep[0m  [16/21], [94mLoss[0m : 2.56122
[1mStep[0m  [18/21], [94mLoss[0m : 2.54011
[1mStep[0m  [20/21], [94mLoss[0m : 2.50472

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.575, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33803
[1mStep[0m  [2/21], [94mLoss[0m : 2.48265
[1mStep[0m  [4/21], [94mLoss[0m : 2.43337
[1mStep[0m  [6/21], [94mLoss[0m : 2.33726
[1mStep[0m  [8/21], [94mLoss[0m : 2.43409
[1mStep[0m  [10/21], [94mLoss[0m : 2.52813
[1mStep[0m  [12/21], [94mLoss[0m : 2.32049
[1mStep[0m  [14/21], [94mLoss[0m : 2.52203
[1mStep[0m  [16/21], [94mLoss[0m : 2.44913
[1mStep[0m  [18/21], [94mLoss[0m : 2.49841
[1mStep[0m  [20/21], [94mLoss[0m : 2.38445

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.530, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42280
[1mStep[0m  [2/21], [94mLoss[0m : 2.55625
[1mStep[0m  [4/21], [94mLoss[0m : 2.33323
[1mStep[0m  [6/21], [94mLoss[0m : 2.39829
[1mStep[0m  [8/21], [94mLoss[0m : 2.46338
[1mStep[0m  [10/21], [94mLoss[0m : 2.55679
[1mStep[0m  [12/21], [94mLoss[0m : 2.39910
[1mStep[0m  [14/21], [94mLoss[0m : 2.55958
[1mStep[0m  [16/21], [94mLoss[0m : 2.56010
[1mStep[0m  [18/21], [94mLoss[0m : 2.46963
[1mStep[0m  [20/21], [94mLoss[0m : 2.44772

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55239
[1mStep[0m  [2/21], [94mLoss[0m : 2.36426
[1mStep[0m  [4/21], [94mLoss[0m : 2.40730
[1mStep[0m  [6/21], [94mLoss[0m : 2.43048
[1mStep[0m  [8/21], [94mLoss[0m : 2.43468
[1mStep[0m  [10/21], [94mLoss[0m : 2.43996
[1mStep[0m  [12/21], [94mLoss[0m : 2.58141
[1mStep[0m  [14/21], [94mLoss[0m : 2.44669
[1mStep[0m  [16/21], [94mLoss[0m : 2.44791
[1mStep[0m  [18/21], [94mLoss[0m : 2.39970
[1mStep[0m  [20/21], [94mLoss[0m : 2.44093

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.489, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53374
[1mStep[0m  [2/21], [94mLoss[0m : 2.46386
[1mStep[0m  [4/21], [94mLoss[0m : 2.41544
[1mStep[0m  [6/21], [94mLoss[0m : 2.50484
[1mStep[0m  [8/21], [94mLoss[0m : 2.47726
[1mStep[0m  [10/21], [94mLoss[0m : 2.41332
[1mStep[0m  [12/21], [94mLoss[0m : 2.40871
[1mStep[0m  [14/21], [94mLoss[0m : 2.34843
[1mStep[0m  [16/21], [94mLoss[0m : 2.44911
[1mStep[0m  [18/21], [94mLoss[0m : 2.37990
[1mStep[0m  [20/21], [94mLoss[0m : 2.38254

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36073
[1mStep[0m  [2/21], [94mLoss[0m : 2.38340
[1mStep[0m  [4/21], [94mLoss[0m : 2.48104
[1mStep[0m  [6/21], [94mLoss[0m : 2.46892
[1mStep[0m  [8/21], [94mLoss[0m : 2.46742
[1mStep[0m  [10/21], [94mLoss[0m : 2.52020
[1mStep[0m  [12/21], [94mLoss[0m : 2.47014
[1mStep[0m  [14/21], [94mLoss[0m : 2.53086
[1mStep[0m  [16/21], [94mLoss[0m : 2.49010
[1mStep[0m  [18/21], [94mLoss[0m : 2.44768
[1mStep[0m  [20/21], [94mLoss[0m : 2.42891

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59988
[1mStep[0m  [2/21], [94mLoss[0m : 2.54073
[1mStep[0m  [4/21], [94mLoss[0m : 2.39657
[1mStep[0m  [6/21], [94mLoss[0m : 2.28358
[1mStep[0m  [8/21], [94mLoss[0m : 2.63071
[1mStep[0m  [10/21], [94mLoss[0m : 2.32347
[1mStep[0m  [12/21], [94mLoss[0m : 2.57823
[1mStep[0m  [14/21], [94mLoss[0m : 2.46746
[1mStep[0m  [16/21], [94mLoss[0m : 2.57063
[1mStep[0m  [18/21], [94mLoss[0m : 2.40789
[1mStep[0m  [20/21], [94mLoss[0m : 2.31531

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.558, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44326
[1mStep[0m  [2/21], [94mLoss[0m : 2.32354
[1mStep[0m  [4/21], [94mLoss[0m : 2.39566
[1mStep[0m  [6/21], [94mLoss[0m : 2.35214
[1mStep[0m  [8/21], [94mLoss[0m : 2.42633
[1mStep[0m  [10/21], [94mLoss[0m : 2.43726
[1mStep[0m  [12/21], [94mLoss[0m : 2.49291
[1mStep[0m  [14/21], [94mLoss[0m : 2.42871
[1mStep[0m  [16/21], [94mLoss[0m : 2.43600
[1mStep[0m  [18/21], [94mLoss[0m : 2.52703
[1mStep[0m  [20/21], [94mLoss[0m : 2.50221

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42132
[1mStep[0m  [2/21], [94mLoss[0m : 2.45452
[1mStep[0m  [4/21], [94mLoss[0m : 2.27987
[1mStep[0m  [6/21], [94mLoss[0m : 2.51790
[1mStep[0m  [8/21], [94mLoss[0m : 2.40797
[1mStep[0m  [10/21], [94mLoss[0m : 2.41879
[1mStep[0m  [12/21], [94mLoss[0m : 2.47385
[1mStep[0m  [14/21], [94mLoss[0m : 2.45882
[1mStep[0m  [16/21], [94mLoss[0m : 2.20139
[1mStep[0m  [18/21], [94mLoss[0m : 2.36803
[1mStep[0m  [20/21], [94mLoss[0m : 2.47828

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32414
[1mStep[0m  [2/21], [94mLoss[0m : 2.42586
[1mStep[0m  [4/21], [94mLoss[0m : 2.30628
[1mStep[0m  [6/21], [94mLoss[0m : 2.63622
[1mStep[0m  [8/21], [94mLoss[0m : 2.41927
[1mStep[0m  [10/21], [94mLoss[0m : 2.46258
[1mStep[0m  [12/21], [94mLoss[0m : 2.50625
[1mStep[0m  [14/21], [94mLoss[0m : 2.46159
[1mStep[0m  [16/21], [94mLoss[0m : 2.58662
[1mStep[0m  [18/21], [94mLoss[0m : 2.44060
[1mStep[0m  [20/21], [94mLoss[0m : 2.40334

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50818
[1mStep[0m  [2/21], [94mLoss[0m : 2.33238
[1mStep[0m  [4/21], [94mLoss[0m : 2.31253
[1mStep[0m  [6/21], [94mLoss[0m : 2.40917
[1mStep[0m  [8/21], [94mLoss[0m : 2.39629
[1mStep[0m  [10/21], [94mLoss[0m : 2.46709
[1mStep[0m  [12/21], [94mLoss[0m : 2.37769
[1mStep[0m  [14/21], [94mLoss[0m : 2.45917
[1mStep[0m  [16/21], [94mLoss[0m : 2.38312
[1mStep[0m  [18/21], [94mLoss[0m : 2.40002
[1mStep[0m  [20/21], [94mLoss[0m : 2.42349

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.432, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58505
[1mStep[0m  [2/21], [94mLoss[0m : 2.37065
[1mStep[0m  [4/21], [94mLoss[0m : 2.51409
[1mStep[0m  [6/21], [94mLoss[0m : 2.53833
[1mStep[0m  [8/21], [94mLoss[0m : 2.47299
[1mStep[0m  [10/21], [94mLoss[0m : 2.38512
[1mStep[0m  [12/21], [94mLoss[0m : 2.54507
[1mStep[0m  [14/21], [94mLoss[0m : 2.25608
[1mStep[0m  [16/21], [94mLoss[0m : 2.47238
[1mStep[0m  [18/21], [94mLoss[0m : 2.47043
[1mStep[0m  [20/21], [94mLoss[0m : 2.40271

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.481, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41996
[1mStep[0m  [2/21], [94mLoss[0m : 2.50866
[1mStep[0m  [4/21], [94mLoss[0m : 2.46937
[1mStep[0m  [6/21], [94mLoss[0m : 2.37099
[1mStep[0m  [8/21], [94mLoss[0m : 2.53937
[1mStep[0m  [10/21], [94mLoss[0m : 2.44001
[1mStep[0m  [12/21], [94mLoss[0m : 2.33374
[1mStep[0m  [14/21], [94mLoss[0m : 2.49082
[1mStep[0m  [16/21], [94mLoss[0m : 2.51784
[1mStep[0m  [18/21], [94mLoss[0m : 2.45080
[1mStep[0m  [20/21], [94mLoss[0m : 2.42241

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.434, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35678
[1mStep[0m  [2/21], [94mLoss[0m : 2.48059
[1mStep[0m  [4/21], [94mLoss[0m : 2.44626
[1mStep[0m  [6/21], [94mLoss[0m : 2.25279
[1mStep[0m  [8/21], [94mLoss[0m : 2.40689
[1mStep[0m  [10/21], [94mLoss[0m : 2.39567
[1mStep[0m  [12/21], [94mLoss[0m : 2.30124
[1mStep[0m  [14/21], [94mLoss[0m : 2.38487
[1mStep[0m  [16/21], [94mLoss[0m : 2.44266
[1mStep[0m  [18/21], [94mLoss[0m : 2.46232
[1mStep[0m  [20/21], [94mLoss[0m : 2.50990

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33790
[1mStep[0m  [2/21], [94mLoss[0m : 2.32896
[1mStep[0m  [4/21], [94mLoss[0m : 2.45521
[1mStep[0m  [6/21], [94mLoss[0m : 2.45657
[1mStep[0m  [8/21], [94mLoss[0m : 2.36437
[1mStep[0m  [10/21], [94mLoss[0m : 2.39724
[1mStep[0m  [12/21], [94mLoss[0m : 2.39309
[1mStep[0m  [14/21], [94mLoss[0m : 2.44808
[1mStep[0m  [16/21], [94mLoss[0m : 2.52113
[1mStep[0m  [18/21], [94mLoss[0m : 2.34093
[1mStep[0m  [20/21], [94mLoss[0m : 2.67252

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.443, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36620
[1mStep[0m  [2/21], [94mLoss[0m : 2.41666
[1mStep[0m  [4/21], [94mLoss[0m : 2.33672
[1mStep[0m  [6/21], [94mLoss[0m : 2.56104
[1mStep[0m  [8/21], [94mLoss[0m : 2.43333
[1mStep[0m  [10/21], [94mLoss[0m : 2.38430
[1mStep[0m  [12/21], [94mLoss[0m : 2.35199
[1mStep[0m  [14/21], [94mLoss[0m : 2.52851
[1mStep[0m  [16/21], [94mLoss[0m : 2.36077
[1mStep[0m  [18/21], [94mLoss[0m : 2.42631
[1mStep[0m  [20/21], [94mLoss[0m : 2.33390

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.445, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25218
[1mStep[0m  [2/21], [94mLoss[0m : 2.45605
[1mStep[0m  [4/21], [94mLoss[0m : 2.32200
[1mStep[0m  [6/21], [94mLoss[0m : 2.64237
[1mStep[0m  [8/21], [94mLoss[0m : 2.41467
[1mStep[0m  [10/21], [94mLoss[0m : 2.30592
[1mStep[0m  [12/21], [94mLoss[0m : 2.34764
[1mStep[0m  [14/21], [94mLoss[0m : 2.50164
[1mStep[0m  [16/21], [94mLoss[0m : 2.49124
[1mStep[0m  [18/21], [94mLoss[0m : 2.39876
[1mStep[0m  [20/21], [94mLoss[0m : 2.46095

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.425, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51270
[1mStep[0m  [2/21], [94mLoss[0m : 2.48228
[1mStep[0m  [4/21], [94mLoss[0m : 2.47401
[1mStep[0m  [6/21], [94mLoss[0m : 2.40581
[1mStep[0m  [8/21], [94mLoss[0m : 2.35586
[1mStep[0m  [10/21], [94mLoss[0m : 2.38836
[1mStep[0m  [12/21], [94mLoss[0m : 2.35815
[1mStep[0m  [14/21], [94mLoss[0m : 2.31187
[1mStep[0m  [16/21], [94mLoss[0m : 2.36247
[1mStep[0m  [18/21], [94mLoss[0m : 2.39379
[1mStep[0m  [20/21], [94mLoss[0m : 2.42836

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.461, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31445
[1mStep[0m  [2/21], [94mLoss[0m : 2.21340
[1mStep[0m  [4/21], [94mLoss[0m : 2.49498
[1mStep[0m  [6/21], [94mLoss[0m : 2.34356
[1mStep[0m  [8/21], [94mLoss[0m : 2.40919
[1mStep[0m  [10/21], [94mLoss[0m : 2.53734
[1mStep[0m  [12/21], [94mLoss[0m : 2.35223
[1mStep[0m  [14/21], [94mLoss[0m : 2.53371
[1mStep[0m  [16/21], [94mLoss[0m : 2.42667
[1mStep[0m  [18/21], [94mLoss[0m : 2.46880
[1mStep[0m  [20/21], [94mLoss[0m : 2.32773

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23238
[1mStep[0m  [2/21], [94mLoss[0m : 2.28901
[1mStep[0m  [4/21], [94mLoss[0m : 2.30723
[1mStep[0m  [6/21], [94mLoss[0m : 2.30623
[1mStep[0m  [8/21], [94mLoss[0m : 2.36326
[1mStep[0m  [10/21], [94mLoss[0m : 2.41102
[1mStep[0m  [12/21], [94mLoss[0m : 2.47571
[1mStep[0m  [14/21], [94mLoss[0m : 2.43396
[1mStep[0m  [16/21], [94mLoss[0m : 2.48236
[1mStep[0m  [18/21], [94mLoss[0m : 2.52683
[1mStep[0m  [20/21], [94mLoss[0m : 2.49138

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.470, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.452
====================================

Phase 1 - Evaluation MAE:  2.452094520841326
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.36335
[1mStep[0m  [2/21], [94mLoss[0m : 2.45535
[1mStep[0m  [4/21], [94mLoss[0m : 2.59796
[1mStep[0m  [6/21], [94mLoss[0m : 2.57452
[1mStep[0m  [8/21], [94mLoss[0m : 2.47924
[1mStep[0m  [10/21], [94mLoss[0m : 2.42867
[1mStep[0m  [12/21], [94mLoss[0m : 2.68427
[1mStep[0m  [14/21], [94mLoss[0m : 2.46454
[1mStep[0m  [16/21], [94mLoss[0m : 2.36423
[1mStep[0m  [18/21], [94mLoss[0m : 2.46518
[1mStep[0m  [20/21], [94mLoss[0m : 2.48761

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37371
[1mStep[0m  [2/21], [94mLoss[0m : 2.37992
[1mStep[0m  [4/21], [94mLoss[0m : 2.51830
[1mStep[0m  [6/21], [94mLoss[0m : 2.25185
[1mStep[0m  [8/21], [94mLoss[0m : 2.43871
[1mStep[0m  [10/21], [94mLoss[0m : 2.40806
[1mStep[0m  [12/21], [94mLoss[0m : 2.52992
[1mStep[0m  [14/21], [94mLoss[0m : 2.38510
[1mStep[0m  [16/21], [94mLoss[0m : 2.49778
[1mStep[0m  [18/21], [94mLoss[0m : 2.41016
[1mStep[0m  [20/21], [94mLoss[0m : 2.36188

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.960, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25410
[1mStep[0m  [2/21], [94mLoss[0m : 2.62012
[1mStep[0m  [4/21], [94mLoss[0m : 2.41197
[1mStep[0m  [6/21], [94mLoss[0m : 2.37199
[1mStep[0m  [8/21], [94mLoss[0m : 2.34126
[1mStep[0m  [10/21], [94mLoss[0m : 2.43048
[1mStep[0m  [12/21], [94mLoss[0m : 2.23962
[1mStep[0m  [14/21], [94mLoss[0m : 2.44767
[1mStep[0m  [16/21], [94mLoss[0m : 2.38119
[1mStep[0m  [18/21], [94mLoss[0m : 2.34406
[1mStep[0m  [20/21], [94mLoss[0m : 2.36058

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35535
[1mStep[0m  [2/21], [94mLoss[0m : 2.44020
[1mStep[0m  [4/21], [94mLoss[0m : 2.46798
[1mStep[0m  [6/21], [94mLoss[0m : 2.27426
[1mStep[0m  [8/21], [94mLoss[0m : 2.39705
[1mStep[0m  [10/21], [94mLoss[0m : 2.37145
[1mStep[0m  [12/21], [94mLoss[0m : 2.39205
[1mStep[0m  [14/21], [94mLoss[0m : 2.33745
[1mStep[0m  [16/21], [94mLoss[0m : 2.35591
[1mStep[0m  [18/21], [94mLoss[0m : 2.19037
[1mStep[0m  [20/21], [94mLoss[0m : 2.24256

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31096
[1mStep[0m  [2/21], [94mLoss[0m : 2.41785
[1mStep[0m  [4/21], [94mLoss[0m : 2.39273
[1mStep[0m  [6/21], [94mLoss[0m : 2.15787
[1mStep[0m  [8/21], [94mLoss[0m : 2.34437
[1mStep[0m  [10/21], [94mLoss[0m : 2.36344
[1mStep[0m  [12/21], [94mLoss[0m : 2.09237
[1mStep[0m  [14/21], [94mLoss[0m : 2.47922
[1mStep[0m  [16/21], [94mLoss[0m : 2.39170
[1mStep[0m  [18/21], [94mLoss[0m : 2.27351
[1mStep[0m  [20/21], [94mLoss[0m : 2.21226

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15516
[1mStep[0m  [2/21], [94mLoss[0m : 2.26368
[1mStep[0m  [4/21], [94mLoss[0m : 2.23731
[1mStep[0m  [6/21], [94mLoss[0m : 2.32723
[1mStep[0m  [8/21], [94mLoss[0m : 2.11517
[1mStep[0m  [10/21], [94mLoss[0m : 2.24271
[1mStep[0m  [12/21], [94mLoss[0m : 2.35137
[1mStep[0m  [14/21], [94mLoss[0m : 2.21876
[1mStep[0m  [16/21], [94mLoss[0m : 2.31512
[1mStep[0m  [18/21], [94mLoss[0m : 2.35740
[1mStep[0m  [20/21], [94mLoss[0m : 2.53411

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28568
[1mStep[0m  [2/21], [94mLoss[0m : 2.15648
[1mStep[0m  [4/21], [94mLoss[0m : 2.18311
[1mStep[0m  [6/21], [94mLoss[0m : 2.25332
[1mStep[0m  [8/21], [94mLoss[0m : 2.26026
[1mStep[0m  [10/21], [94mLoss[0m : 2.29243
[1mStep[0m  [12/21], [94mLoss[0m : 2.24999
[1mStep[0m  [14/21], [94mLoss[0m : 2.13239
[1mStep[0m  [16/21], [94mLoss[0m : 2.16786
[1mStep[0m  [18/21], [94mLoss[0m : 2.32560
[1mStep[0m  [20/21], [94mLoss[0m : 2.31378

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34025
[1mStep[0m  [2/21], [94mLoss[0m : 2.14520
[1mStep[0m  [4/21], [94mLoss[0m : 2.28107
[1mStep[0m  [6/21], [94mLoss[0m : 2.15971
[1mStep[0m  [8/21], [94mLoss[0m : 2.16583
[1mStep[0m  [10/21], [94mLoss[0m : 2.38503
[1mStep[0m  [12/21], [94mLoss[0m : 2.17599
[1mStep[0m  [14/21], [94mLoss[0m : 2.29170
[1mStep[0m  [16/21], [94mLoss[0m : 2.21420
[1mStep[0m  [18/21], [94mLoss[0m : 2.08445
[1mStep[0m  [20/21], [94mLoss[0m : 2.08049

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27509
[1mStep[0m  [2/21], [94mLoss[0m : 2.24615
[1mStep[0m  [4/21], [94mLoss[0m : 2.23395
[1mStep[0m  [6/21], [94mLoss[0m : 2.34419
[1mStep[0m  [8/21], [94mLoss[0m : 2.18676
[1mStep[0m  [10/21], [94mLoss[0m : 2.06456
[1mStep[0m  [12/21], [94mLoss[0m : 2.03138
[1mStep[0m  [14/21], [94mLoss[0m : 2.19797
[1mStep[0m  [16/21], [94mLoss[0m : 2.22241
[1mStep[0m  [18/21], [94mLoss[0m : 2.02647
[1mStep[0m  [20/21], [94mLoss[0m : 2.18759

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.152, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.00803
[1mStep[0m  [2/21], [94mLoss[0m : 2.15184
[1mStep[0m  [4/21], [94mLoss[0m : 2.13144
[1mStep[0m  [6/21], [94mLoss[0m : 2.09037
[1mStep[0m  [8/21], [94mLoss[0m : 2.00107
[1mStep[0m  [10/21], [94mLoss[0m : 2.16196
[1mStep[0m  [12/21], [94mLoss[0m : 2.17462
[1mStep[0m  [14/21], [94mLoss[0m : 2.22803
[1mStep[0m  [16/21], [94mLoss[0m : 2.15274
[1mStep[0m  [18/21], [94mLoss[0m : 2.27203
[1mStep[0m  [20/21], [94mLoss[0m : 2.07629

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10278
[1mStep[0m  [2/21], [94mLoss[0m : 2.14165
[1mStep[0m  [4/21], [94mLoss[0m : 2.01551
[1mStep[0m  [6/21], [94mLoss[0m : 2.02152
[1mStep[0m  [8/21], [94mLoss[0m : 1.96925
[1mStep[0m  [10/21], [94mLoss[0m : 1.85407
[1mStep[0m  [12/21], [94mLoss[0m : 1.94066
[1mStep[0m  [14/21], [94mLoss[0m : 2.06105
[1mStep[0m  [16/21], [94mLoss[0m : 2.16867
[1mStep[0m  [18/21], [94mLoss[0m : 2.00489
[1mStep[0m  [20/21], [94mLoss[0m : 2.06072

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.03371
[1mStep[0m  [2/21], [94mLoss[0m : 1.95444
[1mStep[0m  [4/21], [94mLoss[0m : 2.11052
[1mStep[0m  [6/21], [94mLoss[0m : 1.94953
[1mStep[0m  [8/21], [94mLoss[0m : 2.02833
[1mStep[0m  [10/21], [94mLoss[0m : 2.08073
[1mStep[0m  [12/21], [94mLoss[0m : 2.04748
[1mStep[0m  [14/21], [94mLoss[0m : 1.94780
[1mStep[0m  [16/21], [94mLoss[0m : 2.03120
[1mStep[0m  [18/21], [94mLoss[0m : 2.04097
[1mStep[0m  [20/21], [94mLoss[0m : 2.11977

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93357
[1mStep[0m  [2/21], [94mLoss[0m : 1.99370
[1mStep[0m  [4/21], [94mLoss[0m : 1.85870
[1mStep[0m  [6/21], [94mLoss[0m : 2.11251
[1mStep[0m  [8/21], [94mLoss[0m : 2.02092
[1mStep[0m  [10/21], [94mLoss[0m : 1.95467
[1mStep[0m  [12/21], [94mLoss[0m : 2.15942
[1mStep[0m  [14/21], [94mLoss[0m : 1.94120
[1mStep[0m  [16/21], [94mLoss[0m : 1.90022
[1mStep[0m  [18/21], [94mLoss[0m : 1.95846
[1mStep[0m  [20/21], [94mLoss[0m : 2.10325

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.975, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88888
[1mStep[0m  [2/21], [94mLoss[0m : 1.92811
[1mStep[0m  [4/21], [94mLoss[0m : 1.90402
[1mStep[0m  [6/21], [94mLoss[0m : 1.94991
[1mStep[0m  [8/21], [94mLoss[0m : 1.90637
[1mStep[0m  [10/21], [94mLoss[0m : 1.98677
[1mStep[0m  [12/21], [94mLoss[0m : 1.88889
[1mStep[0m  [14/21], [94mLoss[0m : 1.93560
[1mStep[0m  [16/21], [94mLoss[0m : 1.92963
[1mStep[0m  [18/21], [94mLoss[0m : 1.83120
[1mStep[0m  [20/21], [94mLoss[0m : 1.95806

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.505, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88884
[1mStep[0m  [2/21], [94mLoss[0m : 1.82696
[1mStep[0m  [4/21], [94mLoss[0m : 1.85956
[1mStep[0m  [6/21], [94mLoss[0m : 1.95359
[1mStep[0m  [8/21], [94mLoss[0m : 1.86270
[1mStep[0m  [10/21], [94mLoss[0m : 1.85351
[1mStep[0m  [12/21], [94mLoss[0m : 1.92695
[1mStep[0m  [14/21], [94mLoss[0m : 1.97131
[1mStep[0m  [16/21], [94mLoss[0m : 1.89915
[1mStep[0m  [18/21], [94mLoss[0m : 1.89183
[1mStep[0m  [20/21], [94mLoss[0m : 1.97170

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.571, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92703
[1mStep[0m  [2/21], [94mLoss[0m : 1.78711
[1mStep[0m  [4/21], [94mLoss[0m : 1.87414
[1mStep[0m  [6/21], [94mLoss[0m : 1.83419
[1mStep[0m  [8/21], [94mLoss[0m : 1.78890
[1mStep[0m  [10/21], [94mLoss[0m : 1.90935
[1mStep[0m  [12/21], [94mLoss[0m : 1.93097
[1mStep[0m  [14/21], [94mLoss[0m : 1.92300
[1mStep[0m  [16/21], [94mLoss[0m : 1.94645
[1mStep[0m  [18/21], [94mLoss[0m : 1.97487
[1mStep[0m  [20/21], [94mLoss[0m : 1.83145

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88162
[1mStep[0m  [2/21], [94mLoss[0m : 1.69680
[1mStep[0m  [4/21], [94mLoss[0m : 1.72394
[1mStep[0m  [6/21], [94mLoss[0m : 1.74408
[1mStep[0m  [8/21], [94mLoss[0m : 1.73807
[1mStep[0m  [10/21], [94mLoss[0m : 1.74473
[1mStep[0m  [12/21], [94mLoss[0m : 1.88386
[1mStep[0m  [14/21], [94mLoss[0m : 1.76839
[1mStep[0m  [16/21], [94mLoss[0m : 1.73431
[1mStep[0m  [18/21], [94mLoss[0m : 1.72251
[1mStep[0m  [20/21], [94mLoss[0m : 1.78287

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.626, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71571
[1mStep[0m  [2/21], [94mLoss[0m : 1.78041
[1mStep[0m  [4/21], [94mLoss[0m : 1.78417
[1mStep[0m  [6/21], [94mLoss[0m : 1.79321
[1mStep[0m  [8/21], [94mLoss[0m : 1.72574
[1mStep[0m  [10/21], [94mLoss[0m : 1.81872
[1mStep[0m  [12/21], [94mLoss[0m : 1.86547
[1mStep[0m  [14/21], [94mLoss[0m : 1.87039
[1mStep[0m  [16/21], [94mLoss[0m : 1.78655
[1mStep[0m  [18/21], [94mLoss[0m : 1.97948
[1mStep[0m  [20/21], [94mLoss[0m : 1.75854

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.529, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63366
[1mStep[0m  [2/21], [94mLoss[0m : 1.80352
[1mStep[0m  [4/21], [94mLoss[0m : 1.75224
[1mStep[0m  [6/21], [94mLoss[0m : 1.91598
[1mStep[0m  [8/21], [94mLoss[0m : 1.65812
[1mStep[0m  [10/21], [94mLoss[0m : 1.82883
[1mStep[0m  [12/21], [94mLoss[0m : 1.77341
[1mStep[0m  [14/21], [94mLoss[0m : 1.85033
[1mStep[0m  [16/21], [94mLoss[0m : 1.78022
[1mStep[0m  [18/21], [94mLoss[0m : 1.93013
[1mStep[0m  [20/21], [94mLoss[0m : 1.68737

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.58606
[1mStep[0m  [2/21], [94mLoss[0m : 1.75821
[1mStep[0m  [4/21], [94mLoss[0m : 1.75617
[1mStep[0m  [6/21], [94mLoss[0m : 1.79890
[1mStep[0m  [8/21], [94mLoss[0m : 1.77685
[1mStep[0m  [10/21], [94mLoss[0m : 1.87826
[1mStep[0m  [12/21], [94mLoss[0m : 1.68211
[1mStep[0m  [14/21], [94mLoss[0m : 1.82057
[1mStep[0m  [16/21], [94mLoss[0m : 1.67337
[1mStep[0m  [18/21], [94mLoss[0m : 1.79557
[1mStep[0m  [20/21], [94mLoss[0m : 1.71041

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.734, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61833
[1mStep[0m  [2/21], [94mLoss[0m : 1.70499
[1mStep[0m  [4/21], [94mLoss[0m : 1.73626
[1mStep[0m  [6/21], [94mLoss[0m : 1.60972
[1mStep[0m  [8/21], [94mLoss[0m : 1.73118
[1mStep[0m  [10/21], [94mLoss[0m : 1.66503
[1mStep[0m  [12/21], [94mLoss[0m : 1.64035
[1mStep[0m  [14/21], [94mLoss[0m : 1.74810
[1mStep[0m  [16/21], [94mLoss[0m : 1.72244
[1mStep[0m  [18/21], [94mLoss[0m : 1.74423
[1mStep[0m  [20/21], [94mLoss[0m : 1.77790

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71065
[1mStep[0m  [2/21], [94mLoss[0m : 1.65118
[1mStep[0m  [4/21], [94mLoss[0m : 1.71621
[1mStep[0m  [6/21], [94mLoss[0m : 1.70654
[1mStep[0m  [8/21], [94mLoss[0m : 1.63973
[1mStep[0m  [10/21], [94mLoss[0m : 1.65817
[1mStep[0m  [12/21], [94mLoss[0m : 1.63488
[1mStep[0m  [14/21], [94mLoss[0m : 1.68944
[1mStep[0m  [16/21], [94mLoss[0m : 1.70406
[1mStep[0m  [18/21], [94mLoss[0m : 1.70199
[1mStep[0m  [20/21], [94mLoss[0m : 1.60127

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.726, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.73896
[1mStep[0m  [2/21], [94mLoss[0m : 1.64043
[1mStep[0m  [4/21], [94mLoss[0m : 1.56360
[1mStep[0m  [6/21], [94mLoss[0m : 1.63241
[1mStep[0m  [8/21], [94mLoss[0m : 1.60969
[1mStep[0m  [10/21], [94mLoss[0m : 1.56358
[1mStep[0m  [12/21], [94mLoss[0m : 1.63475
[1mStep[0m  [14/21], [94mLoss[0m : 1.66000
[1mStep[0m  [16/21], [94mLoss[0m : 1.55560
[1mStep[0m  [18/21], [94mLoss[0m : 1.58971
[1mStep[0m  [20/21], [94mLoss[0m : 1.59802

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.640, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.56728
[1mStep[0m  [2/21], [94mLoss[0m : 1.62353
[1mStep[0m  [4/21], [94mLoss[0m : 1.59246
[1mStep[0m  [6/21], [94mLoss[0m : 1.59926
[1mStep[0m  [8/21], [94mLoss[0m : 1.69946
[1mStep[0m  [10/21], [94mLoss[0m : 1.66754
[1mStep[0m  [12/21], [94mLoss[0m : 1.69129
[1mStep[0m  [14/21], [94mLoss[0m : 1.61954
[1mStep[0m  [16/21], [94mLoss[0m : 1.59716
[1mStep[0m  [18/21], [94mLoss[0m : 1.55777
[1mStep[0m  [20/21], [94mLoss[0m : 1.65148

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.554, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51189
[1mStep[0m  [2/21], [94mLoss[0m : 1.56202
[1mStep[0m  [4/21], [94mLoss[0m : 1.65420
[1mStep[0m  [6/21], [94mLoss[0m : 1.54364
[1mStep[0m  [8/21], [94mLoss[0m : 1.68439
[1mStep[0m  [10/21], [94mLoss[0m : 1.65905
[1mStep[0m  [12/21], [94mLoss[0m : 1.60410
[1mStep[0m  [14/21], [94mLoss[0m : 1.71811
[1mStep[0m  [16/21], [94mLoss[0m : 1.55968
[1mStep[0m  [18/21], [94mLoss[0m : 1.61984
[1mStep[0m  [20/21], [94mLoss[0m : 1.64044

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.635, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.659
====================================

Phase 2 - Evaluation MAE:  2.6589463438306535
MAE score P1      2.452095
MAE score P2      2.658946
loss              1.610733
learning_rate         0.01
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 18, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.25305
[1mStep[0m  [2/21], [94mLoss[0m : 10.28447
[1mStep[0m  [4/21], [94mLoss[0m : 9.05272
[1mStep[0m  [6/21], [94mLoss[0m : 6.75848
[1mStep[0m  [8/21], [94mLoss[0m : 4.04840
[1mStep[0m  [10/21], [94mLoss[0m : 2.92790
[1mStep[0m  [12/21], [94mLoss[0m : 3.47803
[1mStep[0m  [14/21], [94mLoss[0m : 4.12391
[1mStep[0m  [16/21], [94mLoss[0m : 4.46146
[1mStep[0m  [18/21], [94mLoss[0m : 4.19470
[1mStep[0m  [20/21], [94mLoss[0m : 3.58742

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.699, [92mTest[0m: 11.031, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.16773
[1mStep[0m  [2/21], [94mLoss[0m : 2.57993
[1mStep[0m  [4/21], [94mLoss[0m : 2.63656
[1mStep[0m  [6/21], [94mLoss[0m : 2.70951
[1mStep[0m  [8/21], [94mLoss[0m : 2.98372
[1mStep[0m  [10/21], [94mLoss[0m : 2.74174
[1mStep[0m  [12/21], [94mLoss[0m : 2.64454
[1mStep[0m  [14/21], [94mLoss[0m : 2.65670
[1mStep[0m  [16/21], [94mLoss[0m : 2.48132
[1mStep[0m  [18/21], [94mLoss[0m : 2.64597
[1mStep[0m  [20/21], [94mLoss[0m : 2.54422

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.738, [92mTest[0m: 3.237, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59260
[1mStep[0m  [2/21], [94mLoss[0m : 2.60813
[1mStep[0m  [4/21], [94mLoss[0m : 2.52090
[1mStep[0m  [6/21], [94mLoss[0m : 2.55406
[1mStep[0m  [8/21], [94mLoss[0m : 2.44851
[1mStep[0m  [10/21], [94mLoss[0m : 2.39255
[1mStep[0m  [12/21], [94mLoss[0m : 2.50042
[1mStep[0m  [14/21], [94mLoss[0m : 2.60272
[1mStep[0m  [16/21], [94mLoss[0m : 2.50085
[1mStep[0m  [18/21], [94mLoss[0m : 2.50787
[1mStep[0m  [20/21], [94mLoss[0m : 2.52744

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.651, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41712
[1mStep[0m  [2/21], [94mLoss[0m : 2.51280
[1mStep[0m  [4/21], [94mLoss[0m : 2.58979
[1mStep[0m  [6/21], [94mLoss[0m : 2.34657
[1mStep[0m  [8/21], [94mLoss[0m : 2.41710
[1mStep[0m  [10/21], [94mLoss[0m : 2.49706
[1mStep[0m  [12/21], [94mLoss[0m : 2.46049
[1mStep[0m  [14/21], [94mLoss[0m : 2.28418
[1mStep[0m  [16/21], [94mLoss[0m : 2.33559
[1mStep[0m  [18/21], [94mLoss[0m : 2.47484
[1mStep[0m  [20/21], [94mLoss[0m : 2.46192

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32835
[1mStep[0m  [2/21], [94mLoss[0m : 2.42958
[1mStep[0m  [4/21], [94mLoss[0m : 2.53207
[1mStep[0m  [6/21], [94mLoss[0m : 2.53672
[1mStep[0m  [8/21], [94mLoss[0m : 2.52997
[1mStep[0m  [10/21], [94mLoss[0m : 2.39279
[1mStep[0m  [12/21], [94mLoss[0m : 2.48944
[1mStep[0m  [14/21], [94mLoss[0m : 2.46539
[1mStep[0m  [16/21], [94mLoss[0m : 2.41704
[1mStep[0m  [18/21], [94mLoss[0m : 2.50799
[1mStep[0m  [20/21], [94mLoss[0m : 2.45662

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31484
[1mStep[0m  [2/21], [94mLoss[0m : 2.43083
[1mStep[0m  [4/21], [94mLoss[0m : 2.46461
[1mStep[0m  [6/21], [94mLoss[0m : 2.55171
[1mStep[0m  [8/21], [94mLoss[0m : 2.54706
[1mStep[0m  [10/21], [94mLoss[0m : 2.46426
[1mStep[0m  [12/21], [94mLoss[0m : 2.38642
[1mStep[0m  [14/21], [94mLoss[0m : 2.36220
[1mStep[0m  [16/21], [94mLoss[0m : 2.29307
[1mStep[0m  [18/21], [94mLoss[0m : 2.50651
[1mStep[0m  [20/21], [94mLoss[0m : 2.49574

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32008
[1mStep[0m  [2/21], [94mLoss[0m : 2.61280
[1mStep[0m  [4/21], [94mLoss[0m : 2.46958
[1mStep[0m  [6/21], [94mLoss[0m : 2.55676
[1mStep[0m  [8/21], [94mLoss[0m : 2.45955
[1mStep[0m  [10/21], [94mLoss[0m : 2.42284
[1mStep[0m  [12/21], [94mLoss[0m : 2.33448
[1mStep[0m  [14/21], [94mLoss[0m : 2.33094
[1mStep[0m  [16/21], [94mLoss[0m : 2.48744
[1mStep[0m  [18/21], [94mLoss[0m : 2.55098
[1mStep[0m  [20/21], [94mLoss[0m : 2.42998

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43193
[1mStep[0m  [2/21], [94mLoss[0m : 2.44298
[1mStep[0m  [4/21], [94mLoss[0m : 2.48638
[1mStep[0m  [6/21], [94mLoss[0m : 2.34555
[1mStep[0m  [8/21], [94mLoss[0m : 2.42907
[1mStep[0m  [10/21], [94mLoss[0m : 2.39137
[1mStep[0m  [12/21], [94mLoss[0m : 2.49655
[1mStep[0m  [14/21], [94mLoss[0m : 2.42507
[1mStep[0m  [16/21], [94mLoss[0m : 2.43037
[1mStep[0m  [18/21], [94mLoss[0m : 2.52357
[1mStep[0m  [20/21], [94mLoss[0m : 2.46785

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40641
[1mStep[0m  [2/21], [94mLoss[0m : 2.46078
[1mStep[0m  [4/21], [94mLoss[0m : 2.43254
[1mStep[0m  [6/21], [94mLoss[0m : 2.37243
[1mStep[0m  [8/21], [94mLoss[0m : 2.48045
[1mStep[0m  [10/21], [94mLoss[0m : 2.49724
[1mStep[0m  [12/21], [94mLoss[0m : 2.43789
[1mStep[0m  [14/21], [94mLoss[0m : 2.32118
[1mStep[0m  [16/21], [94mLoss[0m : 2.59987
[1mStep[0m  [18/21], [94mLoss[0m : 2.62318
[1mStep[0m  [20/21], [94mLoss[0m : 2.51495

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45457
[1mStep[0m  [2/21], [94mLoss[0m : 2.34291
[1mStep[0m  [4/21], [94mLoss[0m : 2.47121
[1mStep[0m  [6/21], [94mLoss[0m : 2.48247
[1mStep[0m  [8/21], [94mLoss[0m : 2.37807
[1mStep[0m  [10/21], [94mLoss[0m : 2.28975
[1mStep[0m  [12/21], [94mLoss[0m : 2.40512
[1mStep[0m  [14/21], [94mLoss[0m : 2.33470
[1mStep[0m  [16/21], [94mLoss[0m : 2.41765
[1mStep[0m  [18/21], [94mLoss[0m : 2.42424
[1mStep[0m  [20/21], [94mLoss[0m : 2.37598

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34043
[1mStep[0m  [2/21], [94mLoss[0m : 2.43827
[1mStep[0m  [4/21], [94mLoss[0m : 2.49317
[1mStep[0m  [6/21], [94mLoss[0m : 2.49775
[1mStep[0m  [8/21], [94mLoss[0m : 2.21886
[1mStep[0m  [10/21], [94mLoss[0m : 2.42451
[1mStep[0m  [12/21], [94mLoss[0m : 2.42798
[1mStep[0m  [14/21], [94mLoss[0m : 2.35091
[1mStep[0m  [16/21], [94mLoss[0m : 2.42128
[1mStep[0m  [18/21], [94mLoss[0m : 2.50614
[1mStep[0m  [20/21], [94mLoss[0m : 2.42724

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32353
[1mStep[0m  [2/21], [94mLoss[0m : 2.52403
[1mStep[0m  [4/21], [94mLoss[0m : 2.53711
[1mStep[0m  [6/21], [94mLoss[0m : 2.38969
[1mStep[0m  [8/21], [94mLoss[0m : 2.48059
[1mStep[0m  [10/21], [94mLoss[0m : 2.48114
[1mStep[0m  [12/21], [94mLoss[0m : 2.43457
[1mStep[0m  [14/21], [94mLoss[0m : 2.44621
[1mStep[0m  [16/21], [94mLoss[0m : 2.58848
[1mStep[0m  [18/21], [94mLoss[0m : 2.35677
[1mStep[0m  [20/21], [94mLoss[0m : 2.41967

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42364
[1mStep[0m  [2/21], [94mLoss[0m : 2.49138
[1mStep[0m  [4/21], [94mLoss[0m : 2.34506
[1mStep[0m  [6/21], [94mLoss[0m : 2.41819
[1mStep[0m  [8/21], [94mLoss[0m : 2.41321
[1mStep[0m  [10/21], [94mLoss[0m : 2.58562
[1mStep[0m  [12/21], [94mLoss[0m : 2.27485
[1mStep[0m  [14/21], [94mLoss[0m : 2.44567
[1mStep[0m  [16/21], [94mLoss[0m : 2.37526
[1mStep[0m  [18/21], [94mLoss[0m : 2.52857
[1mStep[0m  [20/21], [94mLoss[0m : 2.45791

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31852
[1mStep[0m  [2/21], [94mLoss[0m : 2.60729
[1mStep[0m  [4/21], [94mLoss[0m : 2.48774
[1mStep[0m  [6/21], [94mLoss[0m : 2.48739
[1mStep[0m  [8/21], [94mLoss[0m : 2.47459
[1mStep[0m  [10/21], [94mLoss[0m : 2.38288
[1mStep[0m  [12/21], [94mLoss[0m : 2.45821
[1mStep[0m  [14/21], [94mLoss[0m : 2.25165
[1mStep[0m  [16/21], [94mLoss[0m : 2.49728
[1mStep[0m  [18/21], [94mLoss[0m : 2.28253
[1mStep[0m  [20/21], [94mLoss[0m : 2.51337

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31755
[1mStep[0m  [2/21], [94mLoss[0m : 2.34758
[1mStep[0m  [4/21], [94mLoss[0m : 2.46432
[1mStep[0m  [6/21], [94mLoss[0m : 2.28186
[1mStep[0m  [8/21], [94mLoss[0m : 2.41961
[1mStep[0m  [10/21], [94mLoss[0m : 2.49986
[1mStep[0m  [12/21], [94mLoss[0m : 2.44092
[1mStep[0m  [14/21], [94mLoss[0m : 2.45652
[1mStep[0m  [16/21], [94mLoss[0m : 2.34605
[1mStep[0m  [18/21], [94mLoss[0m : 2.51204
[1mStep[0m  [20/21], [94mLoss[0m : 2.60812

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39872
[1mStep[0m  [2/21], [94mLoss[0m : 2.40404
[1mStep[0m  [4/21], [94mLoss[0m : 2.33478
[1mStep[0m  [6/21], [94mLoss[0m : 2.42664
[1mStep[0m  [8/21], [94mLoss[0m : 2.45387
[1mStep[0m  [10/21], [94mLoss[0m : 2.39779
[1mStep[0m  [12/21], [94mLoss[0m : 2.60627
[1mStep[0m  [14/21], [94mLoss[0m : 2.48309
[1mStep[0m  [16/21], [94mLoss[0m : 2.34280
[1mStep[0m  [18/21], [94mLoss[0m : 2.44835
[1mStep[0m  [20/21], [94mLoss[0m : 2.42246

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29873
[1mStep[0m  [2/21], [94mLoss[0m : 2.48384
[1mStep[0m  [4/21], [94mLoss[0m : 2.44772
[1mStep[0m  [6/21], [94mLoss[0m : 2.43778
[1mStep[0m  [8/21], [94mLoss[0m : 2.54608
[1mStep[0m  [10/21], [94mLoss[0m : 2.48356
[1mStep[0m  [12/21], [94mLoss[0m : 2.54936
[1mStep[0m  [14/21], [94mLoss[0m : 2.42693
[1mStep[0m  [16/21], [94mLoss[0m : 2.44015
[1mStep[0m  [18/21], [94mLoss[0m : 2.30522
[1mStep[0m  [20/21], [94mLoss[0m : 2.31160

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38470
[1mStep[0m  [2/21], [94mLoss[0m : 2.51679
[1mStep[0m  [4/21], [94mLoss[0m : 2.39841
[1mStep[0m  [6/21], [94mLoss[0m : 2.39047
[1mStep[0m  [8/21], [94mLoss[0m : 2.45458
[1mStep[0m  [10/21], [94mLoss[0m : 2.52654
[1mStep[0m  [12/21], [94mLoss[0m : 2.46950
[1mStep[0m  [14/21], [94mLoss[0m : 2.43928
[1mStep[0m  [16/21], [94mLoss[0m : 2.47902
[1mStep[0m  [18/21], [94mLoss[0m : 2.48488
[1mStep[0m  [20/21], [94mLoss[0m : 2.35937

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47841
[1mStep[0m  [2/21], [94mLoss[0m : 2.45408
[1mStep[0m  [4/21], [94mLoss[0m : 2.33227
[1mStep[0m  [6/21], [94mLoss[0m : 2.41776
[1mStep[0m  [8/21], [94mLoss[0m : 2.41040
[1mStep[0m  [10/21], [94mLoss[0m : 2.43016
[1mStep[0m  [12/21], [94mLoss[0m : 2.44089
[1mStep[0m  [14/21], [94mLoss[0m : 2.61553
[1mStep[0m  [16/21], [94mLoss[0m : 2.47653
[1mStep[0m  [18/21], [94mLoss[0m : 2.46004
[1mStep[0m  [20/21], [94mLoss[0m : 2.57451

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49475
[1mStep[0m  [2/21], [94mLoss[0m : 2.55448
[1mStep[0m  [4/21], [94mLoss[0m : 2.41327
[1mStep[0m  [6/21], [94mLoss[0m : 2.43396
[1mStep[0m  [8/21], [94mLoss[0m : 2.39176
[1mStep[0m  [10/21], [94mLoss[0m : 2.37043
[1mStep[0m  [12/21], [94mLoss[0m : 2.39823
[1mStep[0m  [14/21], [94mLoss[0m : 2.48815
[1mStep[0m  [16/21], [94mLoss[0m : 2.49199
[1mStep[0m  [18/21], [94mLoss[0m : 2.43299
[1mStep[0m  [20/21], [94mLoss[0m : 2.43625

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56525
[1mStep[0m  [2/21], [94mLoss[0m : 2.38623
[1mStep[0m  [4/21], [94mLoss[0m : 2.44300
[1mStep[0m  [6/21], [94mLoss[0m : 2.45758
[1mStep[0m  [8/21], [94mLoss[0m : 2.53744
[1mStep[0m  [10/21], [94mLoss[0m : 2.45362
[1mStep[0m  [12/21], [94mLoss[0m : 2.32818
[1mStep[0m  [14/21], [94mLoss[0m : 2.47602
[1mStep[0m  [16/21], [94mLoss[0m : 2.48603
[1mStep[0m  [18/21], [94mLoss[0m : 2.40682
[1mStep[0m  [20/21], [94mLoss[0m : 2.35055

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30488
[1mStep[0m  [2/21], [94mLoss[0m : 2.38682
[1mStep[0m  [4/21], [94mLoss[0m : 2.46120
[1mStep[0m  [6/21], [94mLoss[0m : 2.46265
[1mStep[0m  [8/21], [94mLoss[0m : 2.51450
[1mStep[0m  [10/21], [94mLoss[0m : 2.50100
[1mStep[0m  [12/21], [94mLoss[0m : 2.47717
[1mStep[0m  [14/21], [94mLoss[0m : 2.46845
[1mStep[0m  [16/21], [94mLoss[0m : 2.51300
[1mStep[0m  [18/21], [94mLoss[0m : 2.45624
[1mStep[0m  [20/21], [94mLoss[0m : 2.59786

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55840
[1mStep[0m  [2/21], [94mLoss[0m : 2.32572
[1mStep[0m  [4/21], [94mLoss[0m : 2.50534
[1mStep[0m  [6/21], [94mLoss[0m : 2.30509
[1mStep[0m  [8/21], [94mLoss[0m : 2.58155
[1mStep[0m  [10/21], [94mLoss[0m : 2.48738
[1mStep[0m  [12/21], [94mLoss[0m : 2.39082
[1mStep[0m  [14/21], [94mLoss[0m : 2.45156
[1mStep[0m  [16/21], [94mLoss[0m : 2.42083
[1mStep[0m  [18/21], [94mLoss[0m : 2.48002
[1mStep[0m  [20/21], [94mLoss[0m : 2.21188

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37630
[1mStep[0m  [2/21], [94mLoss[0m : 2.59407
[1mStep[0m  [4/21], [94mLoss[0m : 2.51566
[1mStep[0m  [6/21], [94mLoss[0m : 2.35435
[1mStep[0m  [8/21], [94mLoss[0m : 2.30289
[1mStep[0m  [10/21], [94mLoss[0m : 2.62461
[1mStep[0m  [12/21], [94mLoss[0m : 2.28470
[1mStep[0m  [14/21], [94mLoss[0m : 2.35044
[1mStep[0m  [16/21], [94mLoss[0m : 2.34948
[1mStep[0m  [18/21], [94mLoss[0m : 2.50321
[1mStep[0m  [20/21], [94mLoss[0m : 2.47524

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31486
[1mStep[0m  [2/21], [94mLoss[0m : 2.40976
[1mStep[0m  [4/21], [94mLoss[0m : 2.32001
[1mStep[0m  [6/21], [94mLoss[0m : 2.42562
[1mStep[0m  [8/21], [94mLoss[0m : 2.38506
[1mStep[0m  [10/21], [94mLoss[0m : 2.57658
[1mStep[0m  [12/21], [94mLoss[0m : 2.49726
[1mStep[0m  [14/21], [94mLoss[0m : 2.59035
[1mStep[0m  [16/21], [94mLoss[0m : 2.41934
[1mStep[0m  [18/21], [94mLoss[0m : 2.47451
[1mStep[0m  [20/21], [94mLoss[0m : 2.41516

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27011
[1mStep[0m  [2/21], [94mLoss[0m : 2.46360
[1mStep[0m  [4/21], [94mLoss[0m : 2.41693
[1mStep[0m  [6/21], [94mLoss[0m : 2.34823
[1mStep[0m  [8/21], [94mLoss[0m : 2.54863
[1mStep[0m  [10/21], [94mLoss[0m : 2.40443
[1mStep[0m  [12/21], [94mLoss[0m : 2.42669
[1mStep[0m  [14/21], [94mLoss[0m : 2.39808
[1mStep[0m  [16/21], [94mLoss[0m : 2.45951
[1mStep[0m  [18/21], [94mLoss[0m : 2.46905
[1mStep[0m  [20/21], [94mLoss[0m : 2.41392

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23401
[1mStep[0m  [2/21], [94mLoss[0m : 2.48608
[1mStep[0m  [4/21], [94mLoss[0m : 2.39217
[1mStep[0m  [6/21], [94mLoss[0m : 2.51301
[1mStep[0m  [8/21], [94mLoss[0m : 2.58997
[1mStep[0m  [10/21], [94mLoss[0m : 2.44252
[1mStep[0m  [12/21], [94mLoss[0m : 2.39528
[1mStep[0m  [14/21], [94mLoss[0m : 2.40710
[1mStep[0m  [16/21], [94mLoss[0m : 2.36123
[1mStep[0m  [18/21], [94mLoss[0m : 2.42146
[1mStep[0m  [20/21], [94mLoss[0m : 2.43635

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37833
[1mStep[0m  [2/21], [94mLoss[0m : 2.54904
[1mStep[0m  [4/21], [94mLoss[0m : 2.41297
[1mStep[0m  [6/21], [94mLoss[0m : 2.53557
[1mStep[0m  [8/21], [94mLoss[0m : 2.52600
[1mStep[0m  [10/21], [94mLoss[0m : 2.42714
[1mStep[0m  [12/21], [94mLoss[0m : 2.44068
[1mStep[0m  [14/21], [94mLoss[0m : 2.31480
[1mStep[0m  [16/21], [94mLoss[0m : 2.44397
[1mStep[0m  [18/21], [94mLoss[0m : 2.38580
[1mStep[0m  [20/21], [94mLoss[0m : 2.36633

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40815
[1mStep[0m  [2/21], [94mLoss[0m : 2.46444
[1mStep[0m  [4/21], [94mLoss[0m : 2.25607
[1mStep[0m  [6/21], [94mLoss[0m : 2.51227
[1mStep[0m  [8/21], [94mLoss[0m : 2.49483
[1mStep[0m  [10/21], [94mLoss[0m : 2.36959
[1mStep[0m  [12/21], [94mLoss[0m : 2.51531
[1mStep[0m  [14/21], [94mLoss[0m : 2.32125
[1mStep[0m  [16/21], [94mLoss[0m : 2.29995
[1mStep[0m  [18/21], [94mLoss[0m : 2.40656
[1mStep[0m  [20/21], [94mLoss[0m : 2.45140

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43763
[1mStep[0m  [2/21], [94mLoss[0m : 2.47950
[1mStep[0m  [4/21], [94mLoss[0m : 2.50156
[1mStep[0m  [6/21], [94mLoss[0m : 2.48013
[1mStep[0m  [8/21], [94mLoss[0m : 2.30311
[1mStep[0m  [10/21], [94mLoss[0m : 2.42451
[1mStep[0m  [12/21], [94mLoss[0m : 2.44101
[1mStep[0m  [14/21], [94mLoss[0m : 2.45955
[1mStep[0m  [16/21], [94mLoss[0m : 2.51278
[1mStep[0m  [18/21], [94mLoss[0m : 2.41927
[1mStep[0m  [20/21], [94mLoss[0m : 2.31957

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.3245663983481273
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.46135
[1mStep[0m  [2/21], [94mLoss[0m : 2.42848
[1mStep[0m  [4/21], [94mLoss[0m : 2.32024
[1mStep[0m  [6/21], [94mLoss[0m : 2.48263
[1mStep[0m  [8/21], [94mLoss[0m : 2.47617
[1mStep[0m  [10/21], [94mLoss[0m : 2.44451
[1mStep[0m  [12/21], [94mLoss[0m : 2.62888
[1mStep[0m  [14/21], [94mLoss[0m : 2.42215
[1mStep[0m  [16/21], [94mLoss[0m : 2.61181
[1mStep[0m  [18/21], [94mLoss[0m : 2.41295
[1mStep[0m  [20/21], [94mLoss[0m : 2.42823

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45522
[1mStep[0m  [2/21], [94mLoss[0m : 2.49497
[1mStep[0m  [4/21], [94mLoss[0m : 2.42960
[1mStep[0m  [6/21], [94mLoss[0m : 2.24289
[1mStep[0m  [8/21], [94mLoss[0m : 2.25747
[1mStep[0m  [10/21], [94mLoss[0m : 2.35366
[1mStep[0m  [12/21], [94mLoss[0m : 2.36109
[1mStep[0m  [14/21], [94mLoss[0m : 2.36398
[1mStep[0m  [16/21], [94mLoss[0m : 2.29580
[1mStep[0m  [18/21], [94mLoss[0m : 2.33746
[1mStep[0m  [20/21], [94mLoss[0m : 2.28551

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.378, [92mTest[0m: 3.249, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28504
[1mStep[0m  [2/21], [94mLoss[0m : 2.18971
[1mStep[0m  [4/21], [94mLoss[0m : 2.33981
[1mStep[0m  [6/21], [94mLoss[0m : 2.18730
[1mStep[0m  [8/21], [94mLoss[0m : 2.24222
[1mStep[0m  [10/21], [94mLoss[0m : 2.18577
[1mStep[0m  [12/21], [94mLoss[0m : 2.28489
[1mStep[0m  [14/21], [94mLoss[0m : 2.25655
[1mStep[0m  [16/21], [94mLoss[0m : 2.25791
[1mStep[0m  [18/21], [94mLoss[0m : 2.30418
[1mStep[0m  [20/21], [94mLoss[0m : 2.26231

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.285, [92mTest[0m: 3.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16978
[1mStep[0m  [2/21], [94mLoss[0m : 2.15550
[1mStep[0m  [4/21], [94mLoss[0m : 2.04537
[1mStep[0m  [6/21], [94mLoss[0m : 2.20379
[1mStep[0m  [8/21], [94mLoss[0m : 2.17374
[1mStep[0m  [10/21], [94mLoss[0m : 2.15241
[1mStep[0m  [12/21], [94mLoss[0m : 2.10211
[1mStep[0m  [14/21], [94mLoss[0m : 2.19145
[1mStep[0m  [16/21], [94mLoss[0m : 2.27665
[1mStep[0m  [18/21], [94mLoss[0m : 2.08981
[1mStep[0m  [20/21], [94mLoss[0m : 2.23875

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.177, [92mTest[0m: 3.110, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.04698
[1mStep[0m  [2/21], [94mLoss[0m : 1.96330
[1mStep[0m  [4/21], [94mLoss[0m : 2.22013
[1mStep[0m  [6/21], [94mLoss[0m : 2.09743
[1mStep[0m  [8/21], [94mLoss[0m : 2.03106
[1mStep[0m  [10/21], [94mLoss[0m : 2.19337
[1mStep[0m  [12/21], [94mLoss[0m : 2.22545
[1mStep[0m  [14/21], [94mLoss[0m : 2.16338
[1mStep[0m  [16/21], [94mLoss[0m : 2.15768
[1mStep[0m  [18/21], [94mLoss[0m : 2.07252
[1mStep[0m  [20/21], [94mLoss[0m : 2.21865

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.685, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95841
[1mStep[0m  [2/21], [94mLoss[0m : 2.13834
[1mStep[0m  [4/21], [94mLoss[0m : 2.08012
[1mStep[0m  [6/21], [94mLoss[0m : 2.02159
[1mStep[0m  [8/21], [94mLoss[0m : 2.07569
[1mStep[0m  [10/21], [94mLoss[0m : 1.94697
[1mStep[0m  [12/21], [94mLoss[0m : 2.06793
[1mStep[0m  [14/21], [94mLoss[0m : 2.10012
[1mStep[0m  [16/21], [94mLoss[0m : 2.04801
[1mStep[0m  [18/21], [94mLoss[0m : 2.09995
[1mStep[0m  [20/21], [94mLoss[0m : 2.22623

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15888
[1mStep[0m  [2/21], [94mLoss[0m : 1.94775
[1mStep[0m  [4/21], [94mLoss[0m : 1.86793
[1mStep[0m  [6/21], [94mLoss[0m : 2.01008
[1mStep[0m  [8/21], [94mLoss[0m : 2.07775
[1mStep[0m  [10/21], [94mLoss[0m : 2.00819
[1mStep[0m  [12/21], [94mLoss[0m : 1.97259
[1mStep[0m  [14/21], [94mLoss[0m : 1.99333
[1mStep[0m  [16/21], [94mLoss[0m : 2.02808
[1mStep[0m  [18/21], [94mLoss[0m : 1.96182
[1mStep[0m  [20/21], [94mLoss[0m : 2.10029

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98744
[1mStep[0m  [2/21], [94mLoss[0m : 1.91333
[1mStep[0m  [4/21], [94mLoss[0m : 1.84838
[1mStep[0m  [6/21], [94mLoss[0m : 1.90463
[1mStep[0m  [8/21], [94mLoss[0m : 1.93953
[1mStep[0m  [10/21], [94mLoss[0m : 1.90356
[1mStep[0m  [12/21], [94mLoss[0m : 1.87318
[1mStep[0m  [14/21], [94mLoss[0m : 1.94956
[1mStep[0m  [16/21], [94mLoss[0m : 1.96189
[1mStep[0m  [18/21], [94mLoss[0m : 1.97088
[1mStep[0m  [20/21], [94mLoss[0m : 1.97237

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.733, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.80268
[1mStep[0m  [2/21], [94mLoss[0m : 1.88255
[1mStep[0m  [4/21], [94mLoss[0m : 1.75192
[1mStep[0m  [6/21], [94mLoss[0m : 1.82026
[1mStep[0m  [8/21], [94mLoss[0m : 1.94793
[1mStep[0m  [10/21], [94mLoss[0m : 1.93509
[1mStep[0m  [12/21], [94mLoss[0m : 1.87086
[1mStep[0m  [14/21], [94mLoss[0m : 1.92813
[1mStep[0m  [16/21], [94mLoss[0m : 1.83735
[1mStep[0m  [18/21], [94mLoss[0m : 1.75443
[1mStep[0m  [20/21], [94mLoss[0m : 1.95611

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.689, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84654
[1mStep[0m  [2/21], [94mLoss[0m : 1.79359
[1mStep[0m  [4/21], [94mLoss[0m : 1.84468
[1mStep[0m  [6/21], [94mLoss[0m : 1.84656
[1mStep[0m  [8/21], [94mLoss[0m : 1.74048
[1mStep[0m  [10/21], [94mLoss[0m : 1.92699
[1mStep[0m  [12/21], [94mLoss[0m : 1.82029
[1mStep[0m  [14/21], [94mLoss[0m : 1.90431
[1mStep[0m  [16/21], [94mLoss[0m : 1.87735
[1mStep[0m  [18/21], [94mLoss[0m : 1.90467
[1mStep[0m  [20/21], [94mLoss[0m : 1.76000

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.536, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.89976
[1mStep[0m  [2/21], [94mLoss[0m : 1.79983
[1mStep[0m  [4/21], [94mLoss[0m : 1.75476
[1mStep[0m  [6/21], [94mLoss[0m : 1.86604
[1mStep[0m  [8/21], [94mLoss[0m : 1.82007
[1mStep[0m  [10/21], [94mLoss[0m : 1.81672
[1mStep[0m  [12/21], [94mLoss[0m : 1.65145
[1mStep[0m  [14/21], [94mLoss[0m : 1.69824
[1mStep[0m  [16/21], [94mLoss[0m : 1.90611
[1mStep[0m  [18/21], [94mLoss[0m : 1.85970
[1mStep[0m  [20/21], [94mLoss[0m : 1.85899

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.62700
[1mStep[0m  [2/21], [94mLoss[0m : 1.62647
[1mStep[0m  [4/21], [94mLoss[0m : 1.76609
[1mStep[0m  [6/21], [94mLoss[0m : 1.79368
[1mStep[0m  [8/21], [94mLoss[0m : 1.69199
[1mStep[0m  [10/21], [94mLoss[0m : 1.83196
[1mStep[0m  [12/21], [94mLoss[0m : 1.81550
[1mStep[0m  [14/21], [94mLoss[0m : 1.70541
[1mStep[0m  [16/21], [94mLoss[0m : 1.89912
[1mStep[0m  [18/21], [94mLoss[0m : 1.79154
[1mStep[0m  [20/21], [94mLoss[0m : 1.72459

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.66991
[1mStep[0m  [2/21], [94mLoss[0m : 1.69098
[1mStep[0m  [4/21], [94mLoss[0m : 1.76652
[1mStep[0m  [6/21], [94mLoss[0m : 1.73015
[1mStep[0m  [8/21], [94mLoss[0m : 1.69184
[1mStep[0m  [10/21], [94mLoss[0m : 1.64505
[1mStep[0m  [12/21], [94mLoss[0m : 1.84281
[1mStep[0m  [14/21], [94mLoss[0m : 1.74474
[1mStep[0m  [16/21], [94mLoss[0m : 1.77198
[1mStep[0m  [18/21], [94mLoss[0m : 1.79497
[1mStep[0m  [20/21], [94mLoss[0m : 1.80238

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.65213
[1mStep[0m  [2/21], [94mLoss[0m : 1.69451
[1mStep[0m  [4/21], [94mLoss[0m : 1.73963
[1mStep[0m  [6/21], [94mLoss[0m : 1.66182
[1mStep[0m  [8/21], [94mLoss[0m : 1.74113
[1mStep[0m  [10/21], [94mLoss[0m : 1.62423
[1mStep[0m  [12/21], [94mLoss[0m : 1.62617
[1mStep[0m  [14/21], [94mLoss[0m : 1.76519
[1mStep[0m  [16/21], [94mLoss[0m : 1.78454
[1mStep[0m  [18/21], [94mLoss[0m : 1.78804
[1mStep[0m  [20/21], [94mLoss[0m : 1.81037

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55541
[1mStep[0m  [2/21], [94mLoss[0m : 1.64074
[1mStep[0m  [4/21], [94mLoss[0m : 1.72826
[1mStep[0m  [6/21], [94mLoss[0m : 1.66363
[1mStep[0m  [8/21], [94mLoss[0m : 1.68890
[1mStep[0m  [10/21], [94mLoss[0m : 1.79040
[1mStep[0m  [12/21], [94mLoss[0m : 1.70006
[1mStep[0m  [14/21], [94mLoss[0m : 1.73970
[1mStep[0m  [16/21], [94mLoss[0m : 1.64939
[1mStep[0m  [18/21], [94mLoss[0m : 1.67374
[1mStep[0m  [20/21], [94mLoss[0m : 1.73105

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63153
[1mStep[0m  [2/21], [94mLoss[0m : 1.55360
[1mStep[0m  [4/21], [94mLoss[0m : 1.65743
[1mStep[0m  [6/21], [94mLoss[0m : 1.74004
[1mStep[0m  [8/21], [94mLoss[0m : 1.69670
[1mStep[0m  [10/21], [94mLoss[0m : 1.60387
[1mStep[0m  [12/21], [94mLoss[0m : 1.51523
[1mStep[0m  [14/21], [94mLoss[0m : 1.56975
[1mStep[0m  [16/21], [94mLoss[0m : 1.60315
[1mStep[0m  [18/21], [94mLoss[0m : 1.76495
[1mStep[0m  [20/21], [94mLoss[0m : 1.75361

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.54764
[1mStep[0m  [2/21], [94mLoss[0m : 1.42973
[1mStep[0m  [4/21], [94mLoss[0m : 1.73229
[1mStep[0m  [6/21], [94mLoss[0m : 1.62431
[1mStep[0m  [8/21], [94mLoss[0m : 1.61395
[1mStep[0m  [10/21], [94mLoss[0m : 1.59359
[1mStep[0m  [12/21], [94mLoss[0m : 1.64221
[1mStep[0m  [14/21], [94mLoss[0m : 1.53768
[1mStep[0m  [16/21], [94mLoss[0m : 1.70185
[1mStep[0m  [18/21], [94mLoss[0m : 1.51941
[1mStep[0m  [20/21], [94mLoss[0m : 1.61784

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57115
[1mStep[0m  [2/21], [94mLoss[0m : 1.66589
[1mStep[0m  [4/21], [94mLoss[0m : 1.49735
[1mStep[0m  [6/21], [94mLoss[0m : 1.60162
[1mStep[0m  [8/21], [94mLoss[0m : 1.64739
[1mStep[0m  [10/21], [94mLoss[0m : 1.49593
[1mStep[0m  [12/21], [94mLoss[0m : 1.61087
[1mStep[0m  [14/21], [94mLoss[0m : 1.65605
[1mStep[0m  [16/21], [94mLoss[0m : 1.68549
[1mStep[0m  [18/21], [94mLoss[0m : 1.70550
[1mStep[0m  [20/21], [94mLoss[0m : 1.64894

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.536, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.52717
[1mStep[0m  [2/21], [94mLoss[0m : 1.50394
[1mStep[0m  [4/21], [94mLoss[0m : 1.62765
[1mStep[0m  [6/21], [94mLoss[0m : 1.55092
[1mStep[0m  [8/21], [94mLoss[0m : 1.67689
[1mStep[0m  [10/21], [94mLoss[0m : 1.55149
[1mStep[0m  [12/21], [94mLoss[0m : 1.63558
[1mStep[0m  [14/21], [94mLoss[0m : 1.55942
[1mStep[0m  [16/21], [94mLoss[0m : 1.54424
[1mStep[0m  [18/21], [94mLoss[0m : 1.66231
[1mStep[0m  [20/21], [94mLoss[0m : 1.56588

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51595
[1mStep[0m  [2/21], [94mLoss[0m : 1.54201
[1mStep[0m  [4/21], [94mLoss[0m : 1.49302
[1mStep[0m  [6/21], [94mLoss[0m : 1.57240
[1mStep[0m  [8/21], [94mLoss[0m : 1.62037
[1mStep[0m  [10/21], [94mLoss[0m : 1.59359
[1mStep[0m  [12/21], [94mLoss[0m : 1.49650
[1mStep[0m  [14/21], [94mLoss[0m : 1.49535
[1mStep[0m  [16/21], [94mLoss[0m : 1.52622
[1mStep[0m  [18/21], [94mLoss[0m : 1.50825
[1mStep[0m  [20/21], [94mLoss[0m : 1.65281

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.510, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55439
[1mStep[0m  [2/21], [94mLoss[0m : 1.32400
[1mStep[0m  [4/21], [94mLoss[0m : 1.47999
[1mStep[0m  [6/21], [94mLoss[0m : 1.54005
[1mStep[0m  [8/21], [94mLoss[0m : 1.46557
[1mStep[0m  [10/21], [94mLoss[0m : 1.45444
[1mStep[0m  [12/21], [94mLoss[0m : 1.42722
[1mStep[0m  [14/21], [94mLoss[0m : 1.56206
[1mStep[0m  [16/21], [94mLoss[0m : 1.48969
[1mStep[0m  [18/21], [94mLoss[0m : 1.58875
[1mStep[0m  [20/21], [94mLoss[0m : 1.59863

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.521, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.54901
[1mStep[0m  [2/21], [94mLoss[0m : 1.41033
[1mStep[0m  [4/21], [94mLoss[0m : 1.45321
[1mStep[0m  [6/21], [94mLoss[0m : 1.43833
[1mStep[0m  [8/21], [94mLoss[0m : 1.60902
[1mStep[0m  [10/21], [94mLoss[0m : 1.45903
[1mStep[0m  [12/21], [94mLoss[0m : 1.45583
[1mStep[0m  [14/21], [94mLoss[0m : 1.52082
[1mStep[0m  [16/21], [94mLoss[0m : 1.54438
[1mStep[0m  [18/21], [94mLoss[0m : 1.51873
[1mStep[0m  [20/21], [94mLoss[0m : 1.51417

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.50027
[1mStep[0m  [2/21], [94mLoss[0m : 1.42622
[1mStep[0m  [4/21], [94mLoss[0m : 1.39581
[1mStep[0m  [6/21], [94mLoss[0m : 1.50157
[1mStep[0m  [8/21], [94mLoss[0m : 1.50745
[1mStep[0m  [10/21], [94mLoss[0m : 1.59504
[1mStep[0m  [12/21], [94mLoss[0m : 1.44622
[1mStep[0m  [14/21], [94mLoss[0m : 1.46835
[1mStep[0m  [16/21], [94mLoss[0m : 1.56505
[1mStep[0m  [18/21], [94mLoss[0m : 1.56618
[1mStep[0m  [20/21], [94mLoss[0m : 1.50756

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.510, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46771
[1mStep[0m  [2/21], [94mLoss[0m : 1.50589
[1mStep[0m  [4/21], [94mLoss[0m : 1.40842
[1mStep[0m  [6/21], [94mLoss[0m : 1.50664
[1mStep[0m  [8/21], [94mLoss[0m : 1.40774
[1mStep[0m  [10/21], [94mLoss[0m : 1.53907
[1mStep[0m  [12/21], [94mLoss[0m : 1.49639
[1mStep[0m  [14/21], [94mLoss[0m : 1.50524
[1mStep[0m  [16/21], [94mLoss[0m : 1.61120
[1mStep[0m  [18/21], [94mLoss[0m : 1.42511
[1mStep[0m  [20/21], [94mLoss[0m : 1.54178

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.425, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.47474
[1mStep[0m  [2/21], [94mLoss[0m : 1.35053
[1mStep[0m  [4/21], [94mLoss[0m : 1.43672
[1mStep[0m  [6/21], [94mLoss[0m : 1.42380
[1mStep[0m  [8/21], [94mLoss[0m : 1.42854
[1mStep[0m  [10/21], [94mLoss[0m : 1.56741
[1mStep[0m  [12/21], [94mLoss[0m : 1.43754
[1mStep[0m  [14/21], [94mLoss[0m : 1.42941
[1mStep[0m  [16/21], [94mLoss[0m : 1.52565
[1mStep[0m  [18/21], [94mLoss[0m : 1.56633
[1mStep[0m  [20/21], [94mLoss[0m : 1.45079

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.451, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.26827
[1mStep[0m  [2/21], [94mLoss[0m : 1.50881
[1mStep[0m  [4/21], [94mLoss[0m : 1.47692
[1mStep[0m  [6/21], [94mLoss[0m : 1.43340
[1mStep[0m  [8/21], [94mLoss[0m : 1.32451
[1mStep[0m  [10/21], [94mLoss[0m : 1.39337
[1mStep[0m  [12/21], [94mLoss[0m : 1.55322
[1mStep[0m  [14/21], [94mLoss[0m : 1.48527
[1mStep[0m  [16/21], [94mLoss[0m : 1.58364
[1mStep[0m  [18/21], [94mLoss[0m : 1.45568
[1mStep[0m  [20/21], [94mLoss[0m : 1.37148

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.439, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.37141
[1mStep[0m  [2/21], [94mLoss[0m : 1.58950
[1mStep[0m  [4/21], [94mLoss[0m : 1.40124
[1mStep[0m  [6/21], [94mLoss[0m : 1.45718
[1mStep[0m  [8/21], [94mLoss[0m : 1.41498
[1mStep[0m  [10/21], [94mLoss[0m : 1.55327
[1mStep[0m  [12/21], [94mLoss[0m : 1.53912
[1mStep[0m  [14/21], [94mLoss[0m : 1.43077
[1mStep[0m  [16/21], [94mLoss[0m : 1.44454
[1mStep[0m  [18/21], [94mLoss[0m : 1.53879
[1mStep[0m  [20/21], [94mLoss[0m : 1.46582

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.50047
[1mStep[0m  [2/21], [94mLoss[0m : 1.48803
[1mStep[0m  [4/21], [94mLoss[0m : 1.36743
[1mStep[0m  [6/21], [94mLoss[0m : 1.36211
[1mStep[0m  [8/21], [94mLoss[0m : 1.40306
[1mStep[0m  [10/21], [94mLoss[0m : 1.46233
[1mStep[0m  [12/21], [94mLoss[0m : 1.49674
[1mStep[0m  [14/21], [94mLoss[0m : 1.42325
[1mStep[0m  [16/21], [94mLoss[0m : 1.44283
[1mStep[0m  [18/21], [94mLoss[0m : 1.51915
[1mStep[0m  [20/21], [94mLoss[0m : 1.47165

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.454, [92mTest[0m: 2.548, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.474
====================================

Phase 2 - Evaluation MAE:  2.474067449569702
MAE score P1        2.324566
MAE score P2        2.474067
loss                1.438597
learning_rate           0.01
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.9
weight_decay            0.01
Name: 19, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.89003
[1mStep[0m  [4/42], [94mLoss[0m : 10.52530
[1mStep[0m  [8/42], [94mLoss[0m : 7.97337
[1mStep[0m  [12/42], [94mLoss[0m : 4.79530
[1mStep[0m  [16/42], [94mLoss[0m : 2.72424
[1mStep[0m  [20/42], [94mLoss[0m : 3.32897
[1mStep[0m  [24/42], [94mLoss[0m : 3.73930
[1mStep[0m  [28/42], [94mLoss[0m : 3.24463
[1mStep[0m  [32/42], [94mLoss[0m : 2.60527
[1mStep[0m  [36/42], [94mLoss[0m : 2.66727
[1mStep[0m  [40/42], [94mLoss[0m : 2.63981

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.817, [92mTest[0m: 11.527, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87071
[1mStep[0m  [4/42], [94mLoss[0m : 2.47304
[1mStep[0m  [8/42], [94mLoss[0m : 2.55584
[1mStep[0m  [12/42], [94mLoss[0m : 2.36545
[1mStep[0m  [16/42], [94mLoss[0m : 2.56646
[1mStep[0m  [20/42], [94mLoss[0m : 2.53121
[1mStep[0m  [24/42], [94mLoss[0m : 2.64230
[1mStep[0m  [28/42], [94mLoss[0m : 2.48526
[1mStep[0m  [32/42], [94mLoss[0m : 2.35052
[1mStep[0m  [36/42], [94mLoss[0m : 2.45855
[1mStep[0m  [40/42], [94mLoss[0m : 2.50946

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.520, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42835
[1mStep[0m  [4/42], [94mLoss[0m : 2.50141
[1mStep[0m  [8/42], [94mLoss[0m : 2.66133
[1mStep[0m  [12/42], [94mLoss[0m : 2.66285
[1mStep[0m  [16/42], [94mLoss[0m : 2.61705
[1mStep[0m  [20/42], [94mLoss[0m : 2.52506
[1mStep[0m  [24/42], [94mLoss[0m : 2.56795
[1mStep[0m  [28/42], [94mLoss[0m : 2.42375
[1mStep[0m  [32/42], [94mLoss[0m : 2.34507
[1mStep[0m  [36/42], [94mLoss[0m : 2.38938
[1mStep[0m  [40/42], [94mLoss[0m : 2.50007

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58297
[1mStep[0m  [4/42], [94mLoss[0m : 2.38200
[1mStep[0m  [8/42], [94mLoss[0m : 2.51821
[1mStep[0m  [12/42], [94mLoss[0m : 2.52070
[1mStep[0m  [16/42], [94mLoss[0m : 2.75071
[1mStep[0m  [20/42], [94mLoss[0m : 2.64377
[1mStep[0m  [24/42], [94mLoss[0m : 2.56563
[1mStep[0m  [28/42], [94mLoss[0m : 2.64596
[1mStep[0m  [32/42], [94mLoss[0m : 2.68053
[1mStep[0m  [36/42], [94mLoss[0m : 2.61683
[1mStep[0m  [40/42], [94mLoss[0m : 2.35285

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47901
[1mStep[0m  [4/42], [94mLoss[0m : 2.34372
[1mStep[0m  [8/42], [94mLoss[0m : 2.14589
[1mStep[0m  [12/42], [94mLoss[0m : 2.53444
[1mStep[0m  [16/42], [94mLoss[0m : 2.60410
[1mStep[0m  [20/42], [94mLoss[0m : 2.53996
[1mStep[0m  [24/42], [94mLoss[0m : 2.59585
[1mStep[0m  [28/42], [94mLoss[0m : 2.45854
[1mStep[0m  [32/42], [94mLoss[0m : 2.53471
[1mStep[0m  [36/42], [94mLoss[0m : 2.47466
[1mStep[0m  [40/42], [94mLoss[0m : 2.47699

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56297
[1mStep[0m  [4/42], [94mLoss[0m : 2.59647
[1mStep[0m  [8/42], [94mLoss[0m : 2.48127
[1mStep[0m  [12/42], [94mLoss[0m : 2.64322
[1mStep[0m  [16/42], [94mLoss[0m : 2.53661
[1mStep[0m  [20/42], [94mLoss[0m : 2.65069
[1mStep[0m  [24/42], [94mLoss[0m : 2.53580
[1mStep[0m  [28/42], [94mLoss[0m : 2.55394
[1mStep[0m  [32/42], [94mLoss[0m : 2.58558
[1mStep[0m  [36/42], [94mLoss[0m : 2.54920
[1mStep[0m  [40/42], [94mLoss[0m : 2.49652

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23293
[1mStep[0m  [4/42], [94mLoss[0m : 2.46693
[1mStep[0m  [8/42], [94mLoss[0m : 2.35795
[1mStep[0m  [12/42], [94mLoss[0m : 2.67799
[1mStep[0m  [16/42], [94mLoss[0m : 2.73030
[1mStep[0m  [20/42], [94mLoss[0m : 2.40676
[1mStep[0m  [24/42], [94mLoss[0m : 2.53387
[1mStep[0m  [28/42], [94mLoss[0m : 2.51759
[1mStep[0m  [32/42], [94mLoss[0m : 2.43573
[1mStep[0m  [36/42], [94mLoss[0m : 2.61778
[1mStep[0m  [40/42], [94mLoss[0m : 2.40317

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70925
[1mStep[0m  [4/42], [94mLoss[0m : 2.69807
[1mStep[0m  [8/42], [94mLoss[0m : 2.36704
[1mStep[0m  [12/42], [94mLoss[0m : 2.49979
[1mStep[0m  [16/42], [94mLoss[0m : 2.58567
[1mStep[0m  [20/42], [94mLoss[0m : 2.53666
[1mStep[0m  [24/42], [94mLoss[0m : 2.27925
[1mStep[0m  [28/42], [94mLoss[0m : 2.56126
[1mStep[0m  [32/42], [94mLoss[0m : 2.40131
[1mStep[0m  [36/42], [94mLoss[0m : 2.72457
[1mStep[0m  [40/42], [94mLoss[0m : 2.53380

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61490
[1mStep[0m  [4/42], [94mLoss[0m : 2.66489
[1mStep[0m  [8/42], [94mLoss[0m : 2.52414
[1mStep[0m  [12/42], [94mLoss[0m : 2.42436
[1mStep[0m  [16/42], [94mLoss[0m : 2.50180
[1mStep[0m  [20/42], [94mLoss[0m : 2.51551
[1mStep[0m  [24/42], [94mLoss[0m : 2.62724
[1mStep[0m  [28/42], [94mLoss[0m : 2.32291
[1mStep[0m  [32/42], [94mLoss[0m : 2.37454
[1mStep[0m  [36/42], [94mLoss[0m : 2.37184
[1mStep[0m  [40/42], [94mLoss[0m : 2.59250

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61956
[1mStep[0m  [4/42], [94mLoss[0m : 2.49821
[1mStep[0m  [8/42], [94mLoss[0m : 2.53419
[1mStep[0m  [12/42], [94mLoss[0m : 2.56858
[1mStep[0m  [16/42], [94mLoss[0m : 2.41424
[1mStep[0m  [20/42], [94mLoss[0m : 2.75594
[1mStep[0m  [24/42], [94mLoss[0m : 2.43192
[1mStep[0m  [28/42], [94mLoss[0m : 2.51668
[1mStep[0m  [32/42], [94mLoss[0m : 2.35134
[1mStep[0m  [36/42], [94mLoss[0m : 2.46101
[1mStep[0m  [40/42], [94mLoss[0m : 2.63778

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39010
[1mStep[0m  [4/42], [94mLoss[0m : 2.58804
[1mStep[0m  [8/42], [94mLoss[0m : 2.28887
[1mStep[0m  [12/42], [94mLoss[0m : 2.42275
[1mStep[0m  [16/42], [94mLoss[0m : 2.47144
[1mStep[0m  [20/42], [94mLoss[0m : 2.59484
[1mStep[0m  [24/42], [94mLoss[0m : 2.45097
[1mStep[0m  [28/42], [94mLoss[0m : 2.60289
[1mStep[0m  [32/42], [94mLoss[0m : 2.46828
[1mStep[0m  [36/42], [94mLoss[0m : 2.42349
[1mStep[0m  [40/42], [94mLoss[0m : 2.52658

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55689
[1mStep[0m  [4/42], [94mLoss[0m : 2.41059
[1mStep[0m  [8/42], [94mLoss[0m : 2.64856
[1mStep[0m  [12/42], [94mLoss[0m : 2.59133
[1mStep[0m  [16/42], [94mLoss[0m : 2.53364
[1mStep[0m  [20/42], [94mLoss[0m : 2.50247
[1mStep[0m  [24/42], [94mLoss[0m : 2.47448
[1mStep[0m  [28/42], [94mLoss[0m : 2.51327
[1mStep[0m  [32/42], [94mLoss[0m : 2.54593
[1mStep[0m  [36/42], [94mLoss[0m : 2.36249
[1mStep[0m  [40/42], [94mLoss[0m : 2.57646

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44337
[1mStep[0m  [4/42], [94mLoss[0m : 2.58996
[1mStep[0m  [8/42], [94mLoss[0m : 2.30411
[1mStep[0m  [12/42], [94mLoss[0m : 2.58797
[1mStep[0m  [16/42], [94mLoss[0m : 2.52611
[1mStep[0m  [20/42], [94mLoss[0m : 2.58371
[1mStep[0m  [24/42], [94mLoss[0m : 2.65811
[1mStep[0m  [28/42], [94mLoss[0m : 2.37626
[1mStep[0m  [32/42], [94mLoss[0m : 2.44093
[1mStep[0m  [36/42], [94mLoss[0m : 2.46852
[1mStep[0m  [40/42], [94mLoss[0m : 2.64885

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53464
[1mStep[0m  [4/42], [94mLoss[0m : 2.44214
[1mStep[0m  [8/42], [94mLoss[0m : 2.28572
[1mStep[0m  [12/42], [94mLoss[0m : 2.57193
[1mStep[0m  [16/42], [94mLoss[0m : 2.44038
[1mStep[0m  [20/42], [94mLoss[0m : 2.79755
[1mStep[0m  [24/42], [94mLoss[0m : 2.25057
[1mStep[0m  [28/42], [94mLoss[0m : 2.53945
[1mStep[0m  [32/42], [94mLoss[0m : 2.57231
[1mStep[0m  [36/42], [94mLoss[0m : 2.39286
[1mStep[0m  [40/42], [94mLoss[0m : 2.49504

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58529
[1mStep[0m  [4/42], [94mLoss[0m : 2.62864
[1mStep[0m  [8/42], [94mLoss[0m : 2.57273
[1mStep[0m  [12/42], [94mLoss[0m : 2.46734
[1mStep[0m  [16/42], [94mLoss[0m : 2.67588
[1mStep[0m  [20/42], [94mLoss[0m : 2.43746
[1mStep[0m  [24/42], [94mLoss[0m : 2.31289
[1mStep[0m  [28/42], [94mLoss[0m : 2.55913
[1mStep[0m  [32/42], [94mLoss[0m : 2.52930
[1mStep[0m  [36/42], [94mLoss[0m : 2.33455
[1mStep[0m  [40/42], [94mLoss[0m : 2.45847

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40903
[1mStep[0m  [4/42], [94mLoss[0m : 2.68615
[1mStep[0m  [8/42], [94mLoss[0m : 2.21089
[1mStep[0m  [12/42], [94mLoss[0m : 2.40389
[1mStep[0m  [16/42], [94mLoss[0m : 2.37641
[1mStep[0m  [20/42], [94mLoss[0m : 2.42088
[1mStep[0m  [24/42], [94mLoss[0m : 2.55862
[1mStep[0m  [28/42], [94mLoss[0m : 2.57219
[1mStep[0m  [32/42], [94mLoss[0m : 2.54132
[1mStep[0m  [36/42], [94mLoss[0m : 2.57591
[1mStep[0m  [40/42], [94mLoss[0m : 2.45087

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41537
[1mStep[0m  [4/42], [94mLoss[0m : 2.45883
[1mStep[0m  [8/42], [94mLoss[0m : 2.33847
[1mStep[0m  [12/42], [94mLoss[0m : 2.51596
[1mStep[0m  [16/42], [94mLoss[0m : 2.49934
[1mStep[0m  [20/42], [94mLoss[0m : 2.38706
[1mStep[0m  [24/42], [94mLoss[0m : 2.59358
[1mStep[0m  [28/42], [94mLoss[0m : 2.57238
[1mStep[0m  [32/42], [94mLoss[0m : 2.24121
[1mStep[0m  [36/42], [94mLoss[0m : 2.47905
[1mStep[0m  [40/42], [94mLoss[0m : 2.52538

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59685
[1mStep[0m  [4/42], [94mLoss[0m : 2.47467
[1mStep[0m  [8/42], [94mLoss[0m : 2.17567
[1mStep[0m  [12/42], [94mLoss[0m : 2.40202
[1mStep[0m  [16/42], [94mLoss[0m : 2.56000
[1mStep[0m  [20/42], [94mLoss[0m : 2.44519
[1mStep[0m  [24/42], [94mLoss[0m : 2.35719
[1mStep[0m  [28/42], [94mLoss[0m : 2.44212
[1mStep[0m  [32/42], [94mLoss[0m : 2.32872
[1mStep[0m  [36/42], [94mLoss[0m : 2.44377
[1mStep[0m  [40/42], [94mLoss[0m : 2.25751

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53662
[1mStep[0m  [4/42], [94mLoss[0m : 2.28535
[1mStep[0m  [8/42], [94mLoss[0m : 2.53806
[1mStep[0m  [12/42], [94mLoss[0m : 2.44024
[1mStep[0m  [16/42], [94mLoss[0m : 2.53405
[1mStep[0m  [20/42], [94mLoss[0m : 2.47583
[1mStep[0m  [24/42], [94mLoss[0m : 2.40828
[1mStep[0m  [28/42], [94mLoss[0m : 2.44818
[1mStep[0m  [32/42], [94mLoss[0m : 2.30404
[1mStep[0m  [36/42], [94mLoss[0m : 2.26579
[1mStep[0m  [40/42], [94mLoss[0m : 2.49829

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38184
[1mStep[0m  [4/42], [94mLoss[0m : 2.32592
[1mStep[0m  [8/42], [94mLoss[0m : 2.76909
[1mStep[0m  [12/42], [94mLoss[0m : 2.51072
[1mStep[0m  [16/42], [94mLoss[0m : 2.44482
[1mStep[0m  [20/42], [94mLoss[0m : 2.34585
[1mStep[0m  [24/42], [94mLoss[0m : 2.41236
[1mStep[0m  [28/42], [94mLoss[0m : 2.53709
[1mStep[0m  [32/42], [94mLoss[0m : 2.57857
[1mStep[0m  [36/42], [94mLoss[0m : 2.41156
[1mStep[0m  [40/42], [94mLoss[0m : 2.59573

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34370
[1mStep[0m  [4/42], [94mLoss[0m : 2.57041
[1mStep[0m  [8/42], [94mLoss[0m : 2.52558
[1mStep[0m  [12/42], [94mLoss[0m : 2.65451
[1mStep[0m  [16/42], [94mLoss[0m : 2.24343
[1mStep[0m  [20/42], [94mLoss[0m : 2.55515
[1mStep[0m  [24/42], [94mLoss[0m : 2.53923
[1mStep[0m  [28/42], [94mLoss[0m : 2.47009
[1mStep[0m  [32/42], [94mLoss[0m : 2.41603
[1mStep[0m  [36/42], [94mLoss[0m : 2.28015
[1mStep[0m  [40/42], [94mLoss[0m : 2.52629

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41124
[1mStep[0m  [4/42], [94mLoss[0m : 2.42463
[1mStep[0m  [8/42], [94mLoss[0m : 2.59712
[1mStep[0m  [12/42], [94mLoss[0m : 2.67706
[1mStep[0m  [16/42], [94mLoss[0m : 2.30134
[1mStep[0m  [20/42], [94mLoss[0m : 2.29354
[1mStep[0m  [24/42], [94mLoss[0m : 2.41117
[1mStep[0m  [28/42], [94mLoss[0m : 2.47815
[1mStep[0m  [32/42], [94mLoss[0m : 2.27293
[1mStep[0m  [36/42], [94mLoss[0m : 2.35089
[1mStep[0m  [40/42], [94mLoss[0m : 2.61426

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37939
[1mStep[0m  [4/42], [94mLoss[0m : 2.43227
[1mStep[0m  [8/42], [94mLoss[0m : 2.53340
[1mStep[0m  [12/42], [94mLoss[0m : 2.56508
[1mStep[0m  [16/42], [94mLoss[0m : 2.49579
[1mStep[0m  [20/42], [94mLoss[0m : 2.50985
[1mStep[0m  [24/42], [94mLoss[0m : 2.24219
[1mStep[0m  [28/42], [94mLoss[0m : 2.37203
[1mStep[0m  [32/42], [94mLoss[0m : 2.46045
[1mStep[0m  [36/42], [94mLoss[0m : 2.48533
[1mStep[0m  [40/42], [94mLoss[0m : 2.48882

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49721
[1mStep[0m  [4/42], [94mLoss[0m : 2.72597
[1mStep[0m  [8/42], [94mLoss[0m : 2.62650
[1mStep[0m  [12/42], [94mLoss[0m : 2.54473
[1mStep[0m  [16/42], [94mLoss[0m : 2.46360
[1mStep[0m  [20/42], [94mLoss[0m : 2.54472
[1mStep[0m  [24/42], [94mLoss[0m : 2.23058
[1mStep[0m  [28/42], [94mLoss[0m : 2.70247
[1mStep[0m  [32/42], [94mLoss[0m : 2.65906
[1mStep[0m  [36/42], [94mLoss[0m : 2.50344
[1mStep[0m  [40/42], [94mLoss[0m : 2.48930

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53144
[1mStep[0m  [4/42], [94mLoss[0m : 2.37512
[1mStep[0m  [8/42], [94mLoss[0m : 2.47433
[1mStep[0m  [12/42], [94mLoss[0m : 2.55703
[1mStep[0m  [16/42], [94mLoss[0m : 2.50386
[1mStep[0m  [20/42], [94mLoss[0m : 2.52143
[1mStep[0m  [24/42], [94mLoss[0m : 2.30763
[1mStep[0m  [28/42], [94mLoss[0m : 2.60791
[1mStep[0m  [32/42], [94mLoss[0m : 2.64615
[1mStep[0m  [36/42], [94mLoss[0m : 2.32702
[1mStep[0m  [40/42], [94mLoss[0m : 2.42035

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37531
[1mStep[0m  [4/42], [94mLoss[0m : 2.35332
[1mStep[0m  [8/42], [94mLoss[0m : 2.45603
[1mStep[0m  [12/42], [94mLoss[0m : 2.30897
[1mStep[0m  [16/42], [94mLoss[0m : 2.83145
[1mStep[0m  [20/42], [94mLoss[0m : 2.30226
[1mStep[0m  [24/42], [94mLoss[0m : 2.29775
[1mStep[0m  [28/42], [94mLoss[0m : 2.37611
[1mStep[0m  [32/42], [94mLoss[0m : 2.55751
[1mStep[0m  [36/42], [94mLoss[0m : 2.38339
[1mStep[0m  [40/42], [94mLoss[0m : 2.36339

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38429
[1mStep[0m  [4/42], [94mLoss[0m : 2.43113
[1mStep[0m  [8/42], [94mLoss[0m : 2.71938
[1mStep[0m  [12/42], [94mLoss[0m : 2.58749
[1mStep[0m  [16/42], [94mLoss[0m : 2.49907
[1mStep[0m  [20/42], [94mLoss[0m : 2.54671
[1mStep[0m  [24/42], [94mLoss[0m : 2.36973
[1mStep[0m  [28/42], [94mLoss[0m : 2.37880
[1mStep[0m  [32/42], [94mLoss[0m : 2.59812
[1mStep[0m  [36/42], [94mLoss[0m : 2.34509
[1mStep[0m  [40/42], [94mLoss[0m : 2.36856

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40026
[1mStep[0m  [4/42], [94mLoss[0m : 2.59381
[1mStep[0m  [8/42], [94mLoss[0m : 2.55887
[1mStep[0m  [12/42], [94mLoss[0m : 2.58849
[1mStep[0m  [16/42], [94mLoss[0m : 2.54805
[1mStep[0m  [20/42], [94mLoss[0m : 2.21433
[1mStep[0m  [24/42], [94mLoss[0m : 2.53479
[1mStep[0m  [28/42], [94mLoss[0m : 2.42876
[1mStep[0m  [32/42], [94mLoss[0m : 2.55059
[1mStep[0m  [36/42], [94mLoss[0m : 2.52233
[1mStep[0m  [40/42], [94mLoss[0m : 2.47078

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39077
[1mStep[0m  [4/42], [94mLoss[0m : 2.60784
[1mStep[0m  [8/42], [94mLoss[0m : 2.30429
[1mStep[0m  [12/42], [94mLoss[0m : 2.34070
[1mStep[0m  [16/42], [94mLoss[0m : 2.49123
[1mStep[0m  [20/42], [94mLoss[0m : 2.53491
[1mStep[0m  [24/42], [94mLoss[0m : 2.53415
[1mStep[0m  [28/42], [94mLoss[0m : 2.70203
[1mStep[0m  [32/42], [94mLoss[0m : 2.38369
[1mStep[0m  [36/42], [94mLoss[0m : 2.42678
[1mStep[0m  [40/42], [94mLoss[0m : 2.45087

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63347
[1mStep[0m  [4/42], [94mLoss[0m : 2.42313
[1mStep[0m  [8/42], [94mLoss[0m : 2.19947
[1mStep[0m  [12/42], [94mLoss[0m : 2.22418
[1mStep[0m  [16/42], [94mLoss[0m : 2.70730
[1mStep[0m  [20/42], [94mLoss[0m : 2.43404
[1mStep[0m  [24/42], [94mLoss[0m : 2.52769
[1mStep[0m  [28/42], [94mLoss[0m : 2.59891
[1mStep[0m  [32/42], [94mLoss[0m : 2.39711
[1mStep[0m  [36/42], [94mLoss[0m : 2.35460
[1mStep[0m  [40/42], [94mLoss[0m : 2.57528

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.315
====================================

Phase 1 - Evaluation MAE:  2.3152266911097934
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.53730
[1mStep[0m  [4/42], [94mLoss[0m : 2.42380
[1mStep[0m  [8/42], [94mLoss[0m : 2.48076
[1mStep[0m  [12/42], [94mLoss[0m : 2.54239
[1mStep[0m  [16/42], [94mLoss[0m : 2.55269
[1mStep[0m  [20/42], [94mLoss[0m : 2.45146
[1mStep[0m  [24/42], [94mLoss[0m : 2.64730
[1mStep[0m  [28/42], [94mLoss[0m : 2.42957
[1mStep[0m  [32/42], [94mLoss[0m : 2.53008
[1mStep[0m  [36/42], [94mLoss[0m : 2.53282
[1mStep[0m  [40/42], [94mLoss[0m : 2.46789

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56293
[1mStep[0m  [4/42], [94mLoss[0m : 2.39238
[1mStep[0m  [8/42], [94mLoss[0m : 2.38283
[1mStep[0m  [12/42], [94mLoss[0m : 2.20969
[1mStep[0m  [16/42], [94mLoss[0m : 2.32375
[1mStep[0m  [20/42], [94mLoss[0m : 2.38041
[1mStep[0m  [24/42], [94mLoss[0m : 2.27832
[1mStep[0m  [28/42], [94mLoss[0m : 2.26877
[1mStep[0m  [32/42], [94mLoss[0m : 2.49164
[1mStep[0m  [36/42], [94mLoss[0m : 2.21645
[1mStep[0m  [40/42], [94mLoss[0m : 2.47745

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32753
[1mStep[0m  [4/42], [94mLoss[0m : 2.39562
[1mStep[0m  [8/42], [94mLoss[0m : 2.31910
[1mStep[0m  [12/42], [94mLoss[0m : 2.36739
[1mStep[0m  [16/42], [94mLoss[0m : 2.39262
[1mStep[0m  [20/42], [94mLoss[0m : 2.29266
[1mStep[0m  [24/42], [94mLoss[0m : 2.25655
[1mStep[0m  [28/42], [94mLoss[0m : 2.27631
[1mStep[0m  [32/42], [94mLoss[0m : 2.30998
[1mStep[0m  [36/42], [94mLoss[0m : 2.33382
[1mStep[0m  [40/42], [94mLoss[0m : 2.33496

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20905
[1mStep[0m  [4/42], [94mLoss[0m : 2.12585
[1mStep[0m  [8/42], [94mLoss[0m : 2.29032
[1mStep[0m  [12/42], [94mLoss[0m : 2.09700
[1mStep[0m  [16/42], [94mLoss[0m : 2.39620
[1mStep[0m  [20/42], [94mLoss[0m : 2.11835
[1mStep[0m  [24/42], [94mLoss[0m : 2.14875
[1mStep[0m  [28/42], [94mLoss[0m : 2.14047
[1mStep[0m  [32/42], [94mLoss[0m : 2.48463
[1mStep[0m  [36/42], [94mLoss[0m : 2.28618
[1mStep[0m  [40/42], [94mLoss[0m : 2.13174

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05484
[1mStep[0m  [4/42], [94mLoss[0m : 2.09777
[1mStep[0m  [8/42], [94mLoss[0m : 2.28348
[1mStep[0m  [12/42], [94mLoss[0m : 2.16575
[1mStep[0m  [16/42], [94mLoss[0m : 2.25732
[1mStep[0m  [20/42], [94mLoss[0m : 1.98336
[1mStep[0m  [24/42], [94mLoss[0m : 2.09679
[1mStep[0m  [28/42], [94mLoss[0m : 2.05930
[1mStep[0m  [32/42], [94mLoss[0m : 2.01013
[1mStep[0m  [36/42], [94mLoss[0m : 2.17211
[1mStep[0m  [40/42], [94mLoss[0m : 2.15130

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99033
[1mStep[0m  [4/42], [94mLoss[0m : 2.01663
[1mStep[0m  [8/42], [94mLoss[0m : 2.16115
[1mStep[0m  [12/42], [94mLoss[0m : 2.07808
[1mStep[0m  [16/42], [94mLoss[0m : 2.09392
[1mStep[0m  [20/42], [94mLoss[0m : 2.13923
[1mStep[0m  [24/42], [94mLoss[0m : 2.12246
[1mStep[0m  [28/42], [94mLoss[0m : 2.10393
[1mStep[0m  [32/42], [94mLoss[0m : 2.01548
[1mStep[0m  [36/42], [94mLoss[0m : 2.26715
[1mStep[0m  [40/42], [94mLoss[0m : 2.06133

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08205
[1mStep[0m  [4/42], [94mLoss[0m : 1.93431
[1mStep[0m  [8/42], [94mLoss[0m : 2.02179
[1mStep[0m  [12/42], [94mLoss[0m : 1.94521
[1mStep[0m  [16/42], [94mLoss[0m : 1.86793
[1mStep[0m  [20/42], [94mLoss[0m : 1.93890
[1mStep[0m  [24/42], [94mLoss[0m : 1.94001
[1mStep[0m  [28/42], [94mLoss[0m : 2.06704
[1mStep[0m  [32/42], [94mLoss[0m : 1.97135
[1mStep[0m  [36/42], [94mLoss[0m : 2.03530
[1mStep[0m  [40/42], [94mLoss[0m : 2.09054

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92529
[1mStep[0m  [4/42], [94mLoss[0m : 1.80356
[1mStep[0m  [8/42], [94mLoss[0m : 1.67969
[1mStep[0m  [12/42], [94mLoss[0m : 1.89847
[1mStep[0m  [16/42], [94mLoss[0m : 1.94649
[1mStep[0m  [20/42], [94mLoss[0m : 1.90546
[1mStep[0m  [24/42], [94mLoss[0m : 2.10639
[1mStep[0m  [28/42], [94mLoss[0m : 1.94875
[1mStep[0m  [32/42], [94mLoss[0m : 1.77360
[1mStep[0m  [36/42], [94mLoss[0m : 2.14063
[1mStep[0m  [40/42], [94mLoss[0m : 2.05323

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91269
[1mStep[0m  [4/42], [94mLoss[0m : 1.90324
[1mStep[0m  [8/42], [94mLoss[0m : 1.76046
[1mStep[0m  [12/42], [94mLoss[0m : 1.75784
[1mStep[0m  [16/42], [94mLoss[0m : 1.87708
[1mStep[0m  [20/42], [94mLoss[0m : 1.77980
[1mStep[0m  [24/42], [94mLoss[0m : 1.86357
[1mStep[0m  [28/42], [94mLoss[0m : 2.00127
[1mStep[0m  [32/42], [94mLoss[0m : 1.89461
[1mStep[0m  [36/42], [94mLoss[0m : 2.12173
[1mStep[0m  [40/42], [94mLoss[0m : 2.03325

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72534
[1mStep[0m  [4/42], [94mLoss[0m : 1.72315
[1mStep[0m  [8/42], [94mLoss[0m : 1.81309
[1mStep[0m  [12/42], [94mLoss[0m : 1.92630
[1mStep[0m  [16/42], [94mLoss[0m : 1.92450
[1mStep[0m  [20/42], [94mLoss[0m : 1.91420
[1mStep[0m  [24/42], [94mLoss[0m : 2.08630
[1mStep[0m  [28/42], [94mLoss[0m : 1.87601
[1mStep[0m  [32/42], [94mLoss[0m : 1.91761
[1mStep[0m  [36/42], [94mLoss[0m : 2.06541
[1mStep[0m  [40/42], [94mLoss[0m : 2.02446

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.550, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78715
[1mStep[0m  [4/42], [94mLoss[0m : 1.77356
[1mStep[0m  [8/42], [94mLoss[0m : 1.83307
[1mStep[0m  [12/42], [94mLoss[0m : 1.86470
[1mStep[0m  [16/42], [94mLoss[0m : 1.76073
[1mStep[0m  [20/42], [94mLoss[0m : 1.79310
[1mStep[0m  [24/42], [94mLoss[0m : 1.86167
[1mStep[0m  [28/42], [94mLoss[0m : 1.67979
[1mStep[0m  [32/42], [94mLoss[0m : 1.91572
[1mStep[0m  [36/42], [94mLoss[0m : 1.94710
[1mStep[0m  [40/42], [94mLoss[0m : 1.71657

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74663
[1mStep[0m  [4/42], [94mLoss[0m : 1.79360
[1mStep[0m  [8/42], [94mLoss[0m : 1.83151
[1mStep[0m  [12/42], [94mLoss[0m : 1.75963
[1mStep[0m  [16/42], [94mLoss[0m : 1.72043
[1mStep[0m  [20/42], [94mLoss[0m : 1.51255
[1mStep[0m  [24/42], [94mLoss[0m : 1.61336
[1mStep[0m  [28/42], [94mLoss[0m : 1.63001
[1mStep[0m  [32/42], [94mLoss[0m : 1.94023
[1mStep[0m  [36/42], [94mLoss[0m : 1.89317
[1mStep[0m  [40/42], [94mLoss[0m : 1.57229

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60844
[1mStep[0m  [4/42], [94mLoss[0m : 1.57394
[1mStep[0m  [8/42], [94mLoss[0m : 1.63109
[1mStep[0m  [12/42], [94mLoss[0m : 1.71034
[1mStep[0m  [16/42], [94mLoss[0m : 1.65373
[1mStep[0m  [20/42], [94mLoss[0m : 1.76486
[1mStep[0m  [24/42], [94mLoss[0m : 1.58771
[1mStep[0m  [28/42], [94mLoss[0m : 1.56197
[1mStep[0m  [32/42], [94mLoss[0m : 1.62203
[1mStep[0m  [36/42], [94mLoss[0m : 1.66242
[1mStep[0m  [40/42], [94mLoss[0m : 1.87111

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89777
[1mStep[0m  [4/42], [94mLoss[0m : 1.66741
[1mStep[0m  [8/42], [94mLoss[0m : 1.64666
[1mStep[0m  [12/42], [94mLoss[0m : 1.71618
[1mStep[0m  [16/42], [94mLoss[0m : 1.57150
[1mStep[0m  [20/42], [94mLoss[0m : 1.65925
[1mStep[0m  [24/42], [94mLoss[0m : 1.59878
[1mStep[0m  [28/42], [94mLoss[0m : 1.66702
[1mStep[0m  [32/42], [94mLoss[0m : 1.61190
[1mStep[0m  [36/42], [94mLoss[0m : 1.81755
[1mStep[0m  [40/42], [94mLoss[0m : 1.88954

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65897
[1mStep[0m  [4/42], [94mLoss[0m : 1.62265
[1mStep[0m  [8/42], [94mLoss[0m : 1.65637
[1mStep[0m  [12/42], [94mLoss[0m : 1.60980
[1mStep[0m  [16/42], [94mLoss[0m : 1.60313
[1mStep[0m  [20/42], [94mLoss[0m : 1.71558
[1mStep[0m  [24/42], [94mLoss[0m : 1.51678
[1mStep[0m  [28/42], [94mLoss[0m : 1.64828
[1mStep[0m  [32/42], [94mLoss[0m : 1.75055
[1mStep[0m  [36/42], [94mLoss[0m : 1.70198
[1mStep[0m  [40/42], [94mLoss[0m : 1.60634

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.667, [92mTest[0m: 2.531, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65482
[1mStep[0m  [4/42], [94mLoss[0m : 1.59524
[1mStep[0m  [8/42], [94mLoss[0m : 1.52954
[1mStep[0m  [12/42], [94mLoss[0m : 1.51561
[1mStep[0m  [16/42], [94mLoss[0m : 1.68150
[1mStep[0m  [20/42], [94mLoss[0m : 1.62117
[1mStep[0m  [24/42], [94mLoss[0m : 1.68313
[1mStep[0m  [28/42], [94mLoss[0m : 1.73496
[1mStep[0m  [32/42], [94mLoss[0m : 1.58800
[1mStep[0m  [36/42], [94mLoss[0m : 1.67480
[1mStep[0m  [40/42], [94mLoss[0m : 1.63547

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42852
[1mStep[0m  [4/42], [94mLoss[0m : 1.47462
[1mStep[0m  [8/42], [94mLoss[0m : 1.56555
[1mStep[0m  [12/42], [94mLoss[0m : 1.53455
[1mStep[0m  [16/42], [94mLoss[0m : 1.53028
[1mStep[0m  [20/42], [94mLoss[0m : 1.76446
[1mStep[0m  [24/42], [94mLoss[0m : 1.59699
[1mStep[0m  [28/42], [94mLoss[0m : 1.60820
[1mStep[0m  [32/42], [94mLoss[0m : 1.68712
[1mStep[0m  [36/42], [94mLoss[0m : 1.65606
[1mStep[0m  [40/42], [94mLoss[0m : 1.70524

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58076
[1mStep[0m  [4/42], [94mLoss[0m : 1.78176
[1mStep[0m  [8/42], [94mLoss[0m : 1.70285
[1mStep[0m  [12/42], [94mLoss[0m : 1.48724
[1mStep[0m  [16/42], [94mLoss[0m : 1.50355
[1mStep[0m  [20/42], [94mLoss[0m : 1.55243
[1mStep[0m  [24/42], [94mLoss[0m : 1.56624
[1mStep[0m  [28/42], [94mLoss[0m : 1.75998
[1mStep[0m  [32/42], [94mLoss[0m : 1.45023
[1mStep[0m  [36/42], [94mLoss[0m : 1.54043
[1mStep[0m  [40/42], [94mLoss[0m : 1.70490

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.584, [92mTest[0m: 2.556, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49846
[1mStep[0m  [4/42], [94mLoss[0m : 1.62107
[1mStep[0m  [8/42], [94mLoss[0m : 1.34583
[1mStep[0m  [12/42], [94mLoss[0m : 1.46975
[1mStep[0m  [16/42], [94mLoss[0m : 1.51375
[1mStep[0m  [20/42], [94mLoss[0m : 1.51559
[1mStep[0m  [24/42], [94mLoss[0m : 1.47238
[1mStep[0m  [28/42], [94mLoss[0m : 1.44864
[1mStep[0m  [32/42], [94mLoss[0m : 1.50116
[1mStep[0m  [36/42], [94mLoss[0m : 1.58242
[1mStep[0m  [40/42], [94mLoss[0m : 1.62753

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.542, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55849
[1mStep[0m  [4/42], [94mLoss[0m : 1.58810
[1mStep[0m  [8/42], [94mLoss[0m : 1.47132
[1mStep[0m  [12/42], [94mLoss[0m : 1.55049
[1mStep[0m  [16/42], [94mLoss[0m : 1.67735
[1mStep[0m  [20/42], [94mLoss[0m : 1.50419
[1mStep[0m  [24/42], [94mLoss[0m : 1.54007
[1mStep[0m  [28/42], [94mLoss[0m : 1.50255
[1mStep[0m  [32/42], [94mLoss[0m : 1.72379
[1mStep[0m  [36/42], [94mLoss[0m : 1.60083
[1mStep[0m  [40/42], [94mLoss[0m : 1.57306

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57987
[1mStep[0m  [4/42], [94mLoss[0m : 1.38527
[1mStep[0m  [8/42], [94mLoss[0m : 1.33883
[1mStep[0m  [12/42], [94mLoss[0m : 1.44983
[1mStep[0m  [16/42], [94mLoss[0m : 1.48164
[1mStep[0m  [20/42], [94mLoss[0m : 1.49804
[1mStep[0m  [24/42], [94mLoss[0m : 1.42735
[1mStep[0m  [28/42], [94mLoss[0m : 1.43106
[1mStep[0m  [32/42], [94mLoss[0m : 1.52020
[1mStep[0m  [36/42], [94mLoss[0m : 1.61077
[1mStep[0m  [40/42], [94mLoss[0m : 1.65203

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.28138
[1mStep[0m  [4/42], [94mLoss[0m : 1.40081
[1mStep[0m  [8/42], [94mLoss[0m : 1.34232
[1mStep[0m  [12/42], [94mLoss[0m : 1.58242
[1mStep[0m  [16/42], [94mLoss[0m : 1.30879
[1mStep[0m  [20/42], [94mLoss[0m : 1.49849
[1mStep[0m  [24/42], [94mLoss[0m : 1.42422
[1mStep[0m  [28/42], [94mLoss[0m : 1.38119
[1mStep[0m  [32/42], [94mLoss[0m : 1.37228
[1mStep[0m  [36/42], [94mLoss[0m : 1.35043
[1mStep[0m  [40/42], [94mLoss[0m : 1.47019

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50256
[1mStep[0m  [4/42], [94mLoss[0m : 1.54385
[1mStep[0m  [8/42], [94mLoss[0m : 1.54971
[1mStep[0m  [12/42], [94mLoss[0m : 1.30271
[1mStep[0m  [16/42], [94mLoss[0m : 1.34783
[1mStep[0m  [20/42], [94mLoss[0m : 1.47266
[1mStep[0m  [24/42], [94mLoss[0m : 1.52869
[1mStep[0m  [28/42], [94mLoss[0m : 1.49217
[1mStep[0m  [32/42], [94mLoss[0m : 1.33598
[1mStep[0m  [36/42], [94mLoss[0m : 1.34540
[1mStep[0m  [40/42], [94mLoss[0m : 1.33201

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.435, [92mTest[0m: 2.528, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47191
[1mStep[0m  [4/42], [94mLoss[0m : 1.42772
[1mStep[0m  [8/42], [94mLoss[0m : 1.32340
[1mStep[0m  [12/42], [94mLoss[0m : 1.44441
[1mStep[0m  [16/42], [94mLoss[0m : 1.42333
[1mStep[0m  [20/42], [94mLoss[0m : 1.37693
[1mStep[0m  [24/42], [94mLoss[0m : 1.26979
[1mStep[0m  [28/42], [94mLoss[0m : 1.39420
[1mStep[0m  [32/42], [94mLoss[0m : 1.46417
[1mStep[0m  [36/42], [94mLoss[0m : 1.36475
[1mStep[0m  [40/42], [94mLoss[0m : 1.41197

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.404, [92mTest[0m: 2.531, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.35492
[1mStep[0m  [4/42], [94mLoss[0m : 1.22940
[1mStep[0m  [8/42], [94mLoss[0m : 1.33686
[1mStep[0m  [12/42], [94mLoss[0m : 1.38955
[1mStep[0m  [16/42], [94mLoss[0m : 1.46835
[1mStep[0m  [20/42], [94mLoss[0m : 1.51036
[1mStep[0m  [24/42], [94mLoss[0m : 1.40958
[1mStep[0m  [28/42], [94mLoss[0m : 1.40118
[1mStep[0m  [32/42], [94mLoss[0m : 1.47620
[1mStep[0m  [36/42], [94mLoss[0m : 1.39985
[1mStep[0m  [40/42], [94mLoss[0m : 1.32273

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.398, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.521
====================================

Phase 2 - Evaluation MAE:  2.5214337621416365
MAE score P1       2.315227
MAE score P2       2.521434
loss               1.398499
learning_rate          0.01
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay          0.001
Name: 20, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.89913
[1mStep[0m  [4/42], [94mLoss[0m : 8.97330
[1mStep[0m  [8/42], [94mLoss[0m : 6.25116
[1mStep[0m  [12/42], [94mLoss[0m : 3.12010
[1mStep[0m  [16/42], [94mLoss[0m : 3.32997
[1mStep[0m  [20/42], [94mLoss[0m : 3.85966
[1mStep[0m  [24/42], [94mLoss[0m : 3.05798
[1mStep[0m  [28/42], [94mLoss[0m : 2.76770
[1mStep[0m  [32/42], [94mLoss[0m : 2.68992
[1mStep[0m  [36/42], [94mLoss[0m : 2.90091
[1mStep[0m  [40/42], [94mLoss[0m : 2.80312

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.451, [92mTest[0m: 10.785, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53402
[1mStep[0m  [4/42], [94mLoss[0m : 2.74712
[1mStep[0m  [8/42], [94mLoss[0m : 2.86803
[1mStep[0m  [12/42], [94mLoss[0m : 2.50238
[1mStep[0m  [16/42], [94mLoss[0m : 2.91517
[1mStep[0m  [20/42], [94mLoss[0m : 2.49093
[1mStep[0m  [24/42], [94mLoss[0m : 2.86379
[1mStep[0m  [28/42], [94mLoss[0m : 2.62630
[1mStep[0m  [32/42], [94mLoss[0m : 2.58523
[1mStep[0m  [36/42], [94mLoss[0m : 2.53517
[1mStep[0m  [40/42], [94mLoss[0m : 2.65235

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.681, [92mTest[0m: 2.946, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60048
[1mStep[0m  [4/42], [94mLoss[0m : 2.52957
[1mStep[0m  [8/42], [94mLoss[0m : 2.83347
[1mStep[0m  [12/42], [94mLoss[0m : 2.62838
[1mStep[0m  [16/42], [94mLoss[0m : 2.57912
[1mStep[0m  [20/42], [94mLoss[0m : 2.52299
[1mStep[0m  [24/42], [94mLoss[0m : 2.58006
[1mStep[0m  [28/42], [94mLoss[0m : 2.43488
[1mStep[0m  [32/42], [94mLoss[0m : 2.57596
[1mStep[0m  [36/42], [94mLoss[0m : 2.62653
[1mStep[0m  [40/42], [94mLoss[0m : 2.39883

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34360
[1mStep[0m  [4/42], [94mLoss[0m : 2.49058
[1mStep[0m  [8/42], [94mLoss[0m : 2.57893
[1mStep[0m  [12/42], [94mLoss[0m : 2.70037
[1mStep[0m  [16/42], [94mLoss[0m : 2.55088
[1mStep[0m  [20/42], [94mLoss[0m : 2.58104
[1mStep[0m  [24/42], [94mLoss[0m : 2.53919
[1mStep[0m  [28/42], [94mLoss[0m : 2.74010
[1mStep[0m  [32/42], [94mLoss[0m : 2.55940
[1mStep[0m  [36/42], [94mLoss[0m : 2.61103
[1mStep[0m  [40/42], [94mLoss[0m : 2.55101

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44146
[1mStep[0m  [4/42], [94mLoss[0m : 2.57323
[1mStep[0m  [8/42], [94mLoss[0m : 2.65141
[1mStep[0m  [12/42], [94mLoss[0m : 2.58721
[1mStep[0m  [16/42], [94mLoss[0m : 2.23389
[1mStep[0m  [20/42], [94mLoss[0m : 2.53602
[1mStep[0m  [24/42], [94mLoss[0m : 2.66677
[1mStep[0m  [28/42], [94mLoss[0m : 2.47620
[1mStep[0m  [32/42], [94mLoss[0m : 2.61442
[1mStep[0m  [36/42], [94mLoss[0m : 2.58313
[1mStep[0m  [40/42], [94mLoss[0m : 2.80988

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44856
[1mStep[0m  [4/42], [94mLoss[0m : 2.53517
[1mStep[0m  [8/42], [94mLoss[0m : 2.42965
[1mStep[0m  [12/42], [94mLoss[0m : 2.55451
[1mStep[0m  [16/42], [94mLoss[0m : 2.44372
[1mStep[0m  [20/42], [94mLoss[0m : 2.62029
[1mStep[0m  [24/42], [94mLoss[0m : 2.53749
[1mStep[0m  [28/42], [94mLoss[0m : 2.40272
[1mStep[0m  [32/42], [94mLoss[0m : 2.53103
[1mStep[0m  [36/42], [94mLoss[0m : 2.34863
[1mStep[0m  [40/42], [94mLoss[0m : 2.56937

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51179
[1mStep[0m  [4/42], [94mLoss[0m : 2.47608
[1mStep[0m  [8/42], [94mLoss[0m : 2.23504
[1mStep[0m  [12/42], [94mLoss[0m : 2.47867
[1mStep[0m  [16/42], [94mLoss[0m : 2.56067
[1mStep[0m  [20/42], [94mLoss[0m : 2.46446
[1mStep[0m  [24/42], [94mLoss[0m : 2.54489
[1mStep[0m  [28/42], [94mLoss[0m : 2.49664
[1mStep[0m  [32/42], [94mLoss[0m : 2.55098
[1mStep[0m  [36/42], [94mLoss[0m : 2.60686
[1mStep[0m  [40/42], [94mLoss[0m : 2.55860

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37567
[1mStep[0m  [4/42], [94mLoss[0m : 2.30113
[1mStep[0m  [8/42], [94mLoss[0m : 2.44704
[1mStep[0m  [12/42], [94mLoss[0m : 2.47463
[1mStep[0m  [16/42], [94mLoss[0m : 2.41108
[1mStep[0m  [20/42], [94mLoss[0m : 2.39152
[1mStep[0m  [24/42], [94mLoss[0m : 2.60784
[1mStep[0m  [28/42], [94mLoss[0m : 2.40665
[1mStep[0m  [32/42], [94mLoss[0m : 2.64300
[1mStep[0m  [36/42], [94mLoss[0m : 2.44407
[1mStep[0m  [40/42], [94mLoss[0m : 2.36649

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38263
[1mStep[0m  [4/42], [94mLoss[0m : 2.22215
[1mStep[0m  [8/42], [94mLoss[0m : 2.42847
[1mStep[0m  [12/42], [94mLoss[0m : 2.57427
[1mStep[0m  [16/42], [94mLoss[0m : 2.42288
[1mStep[0m  [20/42], [94mLoss[0m : 2.62932
[1mStep[0m  [24/42], [94mLoss[0m : 2.56590
[1mStep[0m  [28/42], [94mLoss[0m : 2.49577
[1mStep[0m  [32/42], [94mLoss[0m : 2.41932
[1mStep[0m  [36/42], [94mLoss[0m : 2.29338
[1mStep[0m  [40/42], [94mLoss[0m : 2.47089

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38538
[1mStep[0m  [4/42], [94mLoss[0m : 2.40173
[1mStep[0m  [8/42], [94mLoss[0m : 2.49215
[1mStep[0m  [12/42], [94mLoss[0m : 2.59989
[1mStep[0m  [16/42], [94mLoss[0m : 2.31667
[1mStep[0m  [20/42], [94mLoss[0m : 2.55997
[1mStep[0m  [24/42], [94mLoss[0m : 2.27015
[1mStep[0m  [28/42], [94mLoss[0m : 2.68176
[1mStep[0m  [32/42], [94mLoss[0m : 2.45690
[1mStep[0m  [36/42], [94mLoss[0m : 2.37165
[1mStep[0m  [40/42], [94mLoss[0m : 2.17344

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40723
[1mStep[0m  [4/42], [94mLoss[0m : 2.46479
[1mStep[0m  [8/42], [94mLoss[0m : 2.36871
[1mStep[0m  [12/42], [94mLoss[0m : 2.37805
[1mStep[0m  [16/42], [94mLoss[0m : 2.41218
[1mStep[0m  [20/42], [94mLoss[0m : 2.34038
[1mStep[0m  [24/42], [94mLoss[0m : 2.59770
[1mStep[0m  [28/42], [94mLoss[0m : 2.31648
[1mStep[0m  [32/42], [94mLoss[0m : 2.31702
[1mStep[0m  [36/42], [94mLoss[0m : 2.56555
[1mStep[0m  [40/42], [94mLoss[0m : 2.34146

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45553
[1mStep[0m  [4/42], [94mLoss[0m : 2.44168
[1mStep[0m  [8/42], [94mLoss[0m : 2.31155
[1mStep[0m  [12/42], [94mLoss[0m : 2.37534
[1mStep[0m  [16/42], [94mLoss[0m : 2.23372
[1mStep[0m  [20/42], [94mLoss[0m : 2.42585
[1mStep[0m  [24/42], [94mLoss[0m : 2.49151
[1mStep[0m  [28/42], [94mLoss[0m : 2.58910
[1mStep[0m  [32/42], [94mLoss[0m : 2.54456
[1mStep[0m  [36/42], [94mLoss[0m : 2.32541
[1mStep[0m  [40/42], [94mLoss[0m : 2.22852

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21562
[1mStep[0m  [4/42], [94mLoss[0m : 2.41339
[1mStep[0m  [8/42], [94mLoss[0m : 2.46131
[1mStep[0m  [12/42], [94mLoss[0m : 2.63399
[1mStep[0m  [16/42], [94mLoss[0m : 2.32986
[1mStep[0m  [20/42], [94mLoss[0m : 2.08263
[1mStep[0m  [24/42], [94mLoss[0m : 2.40463
[1mStep[0m  [28/42], [94mLoss[0m : 2.33234
[1mStep[0m  [32/42], [94mLoss[0m : 2.63781
[1mStep[0m  [36/42], [94mLoss[0m : 2.47430
[1mStep[0m  [40/42], [94mLoss[0m : 2.42667

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36151
[1mStep[0m  [4/42], [94mLoss[0m : 2.39988
[1mStep[0m  [8/42], [94mLoss[0m : 2.36141
[1mStep[0m  [12/42], [94mLoss[0m : 2.32366
[1mStep[0m  [16/42], [94mLoss[0m : 2.42920
[1mStep[0m  [20/42], [94mLoss[0m : 2.50027
[1mStep[0m  [24/42], [94mLoss[0m : 2.30164
[1mStep[0m  [28/42], [94mLoss[0m : 2.33874
[1mStep[0m  [32/42], [94mLoss[0m : 2.27462
[1mStep[0m  [36/42], [94mLoss[0m : 2.30489
[1mStep[0m  [40/42], [94mLoss[0m : 2.47879

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.319, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27077
[1mStep[0m  [4/42], [94mLoss[0m : 2.37842
[1mStep[0m  [8/42], [94mLoss[0m : 2.47066
[1mStep[0m  [12/42], [94mLoss[0m : 2.18034
[1mStep[0m  [16/42], [94mLoss[0m : 2.23718
[1mStep[0m  [20/42], [94mLoss[0m : 2.11704
[1mStep[0m  [24/42], [94mLoss[0m : 2.42929
[1mStep[0m  [28/42], [94mLoss[0m : 2.23531
[1mStep[0m  [32/42], [94mLoss[0m : 2.59146
[1mStep[0m  [36/42], [94mLoss[0m : 2.39571
[1mStep[0m  [40/42], [94mLoss[0m : 2.34737

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29871
[1mStep[0m  [4/42], [94mLoss[0m : 2.27633
[1mStep[0m  [8/42], [94mLoss[0m : 2.33026
[1mStep[0m  [12/42], [94mLoss[0m : 2.26868
[1mStep[0m  [16/42], [94mLoss[0m : 2.41289
[1mStep[0m  [20/42], [94mLoss[0m : 2.39320
[1mStep[0m  [24/42], [94mLoss[0m : 2.29270
[1mStep[0m  [28/42], [94mLoss[0m : 2.34363
[1mStep[0m  [32/42], [94mLoss[0m : 2.40717
[1mStep[0m  [36/42], [94mLoss[0m : 2.37827
[1mStep[0m  [40/42], [94mLoss[0m : 2.51164

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.309, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41867
[1mStep[0m  [4/42], [94mLoss[0m : 2.35690
[1mStep[0m  [8/42], [94mLoss[0m : 2.35685
[1mStep[0m  [12/42], [94mLoss[0m : 2.38268
[1mStep[0m  [16/42], [94mLoss[0m : 2.37018
[1mStep[0m  [20/42], [94mLoss[0m : 2.30833
[1mStep[0m  [24/42], [94mLoss[0m : 2.35468
[1mStep[0m  [28/42], [94mLoss[0m : 2.25410
[1mStep[0m  [32/42], [94mLoss[0m : 2.42046
[1mStep[0m  [36/42], [94mLoss[0m : 2.56480
[1mStep[0m  [40/42], [94mLoss[0m : 2.45715

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51992
[1mStep[0m  [4/42], [94mLoss[0m : 2.39044
[1mStep[0m  [8/42], [94mLoss[0m : 2.57509
[1mStep[0m  [12/42], [94mLoss[0m : 2.33319
[1mStep[0m  [16/42], [94mLoss[0m : 2.44191
[1mStep[0m  [20/42], [94mLoss[0m : 2.32121
[1mStep[0m  [24/42], [94mLoss[0m : 2.36189
[1mStep[0m  [28/42], [94mLoss[0m : 2.39055
[1mStep[0m  [32/42], [94mLoss[0m : 2.57877
[1mStep[0m  [36/42], [94mLoss[0m : 2.31449
[1mStep[0m  [40/42], [94mLoss[0m : 2.46832

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.314, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18284
[1mStep[0m  [4/42], [94mLoss[0m : 2.30065
[1mStep[0m  [8/42], [94mLoss[0m : 2.42126
[1mStep[0m  [12/42], [94mLoss[0m : 2.23862
[1mStep[0m  [16/42], [94mLoss[0m : 2.48320
[1mStep[0m  [20/42], [94mLoss[0m : 2.19687
[1mStep[0m  [24/42], [94mLoss[0m : 2.25016
[1mStep[0m  [28/42], [94mLoss[0m : 2.41495
[1mStep[0m  [32/42], [94mLoss[0m : 2.32133
[1mStep[0m  [36/42], [94mLoss[0m : 2.13505
[1mStep[0m  [40/42], [94mLoss[0m : 2.23659

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48701
[1mStep[0m  [4/42], [94mLoss[0m : 2.30290
[1mStep[0m  [8/42], [94mLoss[0m : 2.22701
[1mStep[0m  [12/42], [94mLoss[0m : 2.28023
[1mStep[0m  [16/42], [94mLoss[0m : 2.31664
[1mStep[0m  [20/42], [94mLoss[0m : 2.27803
[1mStep[0m  [24/42], [94mLoss[0m : 2.31304
[1mStep[0m  [28/42], [94mLoss[0m : 2.49588
[1mStep[0m  [32/42], [94mLoss[0m : 2.49593
[1mStep[0m  [36/42], [94mLoss[0m : 2.41164
[1mStep[0m  [40/42], [94mLoss[0m : 2.50935

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29419
[1mStep[0m  [4/42], [94mLoss[0m : 2.33292
[1mStep[0m  [8/42], [94mLoss[0m : 2.34274
[1mStep[0m  [12/42], [94mLoss[0m : 2.26646
[1mStep[0m  [16/42], [94mLoss[0m : 2.40766
[1mStep[0m  [20/42], [94mLoss[0m : 2.49249
[1mStep[0m  [24/42], [94mLoss[0m : 2.26082
[1mStep[0m  [28/42], [94mLoss[0m : 2.21043
[1mStep[0m  [32/42], [94mLoss[0m : 2.15053
[1mStep[0m  [36/42], [94mLoss[0m : 2.33045
[1mStep[0m  [40/42], [94mLoss[0m : 2.25448

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19647
[1mStep[0m  [4/42], [94mLoss[0m : 2.44490
[1mStep[0m  [8/42], [94mLoss[0m : 2.34513
[1mStep[0m  [12/42], [94mLoss[0m : 2.48695
[1mStep[0m  [16/42], [94mLoss[0m : 2.42671
[1mStep[0m  [20/42], [94mLoss[0m : 2.26139
[1mStep[0m  [24/42], [94mLoss[0m : 2.40793
[1mStep[0m  [28/42], [94mLoss[0m : 2.24288
[1mStep[0m  [32/42], [94mLoss[0m : 2.41229
[1mStep[0m  [36/42], [94mLoss[0m : 2.28942
[1mStep[0m  [40/42], [94mLoss[0m : 2.28613

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.314, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15676
[1mStep[0m  [4/42], [94mLoss[0m : 2.47848
[1mStep[0m  [8/42], [94mLoss[0m : 2.22330
[1mStep[0m  [12/42], [94mLoss[0m : 2.12922
[1mStep[0m  [16/42], [94mLoss[0m : 2.29739
[1mStep[0m  [20/42], [94mLoss[0m : 2.27886
[1mStep[0m  [24/42], [94mLoss[0m : 2.38638
[1mStep[0m  [28/42], [94mLoss[0m : 2.33408
[1mStep[0m  [32/42], [94mLoss[0m : 2.36187
[1mStep[0m  [36/42], [94mLoss[0m : 2.20620
[1mStep[0m  [40/42], [94mLoss[0m : 2.28040

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42127
[1mStep[0m  [4/42], [94mLoss[0m : 2.03393
[1mStep[0m  [8/42], [94mLoss[0m : 2.61371
[1mStep[0m  [12/42], [94mLoss[0m : 2.37158
[1mStep[0m  [16/42], [94mLoss[0m : 2.13538
[1mStep[0m  [20/42], [94mLoss[0m : 2.41800
[1mStep[0m  [24/42], [94mLoss[0m : 2.42152
[1mStep[0m  [28/42], [94mLoss[0m : 2.25963
[1mStep[0m  [32/42], [94mLoss[0m : 2.36415
[1mStep[0m  [36/42], [94mLoss[0m : 2.18501
[1mStep[0m  [40/42], [94mLoss[0m : 2.43081

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38715
[1mStep[0m  [4/42], [94mLoss[0m : 2.33012
[1mStep[0m  [8/42], [94mLoss[0m : 2.28714
[1mStep[0m  [12/42], [94mLoss[0m : 2.44151
[1mStep[0m  [16/42], [94mLoss[0m : 2.13600
[1mStep[0m  [20/42], [94mLoss[0m : 2.19714
[1mStep[0m  [24/42], [94mLoss[0m : 2.30247
[1mStep[0m  [28/42], [94mLoss[0m : 2.20310
[1mStep[0m  [32/42], [94mLoss[0m : 2.23521
[1mStep[0m  [36/42], [94mLoss[0m : 2.27990
[1mStep[0m  [40/42], [94mLoss[0m : 2.46670

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20680
[1mStep[0m  [4/42], [94mLoss[0m : 2.30328
[1mStep[0m  [8/42], [94mLoss[0m : 2.17599
[1mStep[0m  [12/42], [94mLoss[0m : 1.98206
[1mStep[0m  [16/42], [94mLoss[0m : 2.23174
[1mStep[0m  [20/42], [94mLoss[0m : 2.05034
[1mStep[0m  [24/42], [94mLoss[0m : 2.36798
[1mStep[0m  [28/42], [94mLoss[0m : 2.25421
[1mStep[0m  [32/42], [94mLoss[0m : 2.27527
[1mStep[0m  [36/42], [94mLoss[0m : 2.21079
[1mStep[0m  [40/42], [94mLoss[0m : 2.25278

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36044
[1mStep[0m  [4/42], [94mLoss[0m : 2.30442
[1mStep[0m  [8/42], [94mLoss[0m : 2.28285
[1mStep[0m  [12/42], [94mLoss[0m : 2.21864
[1mStep[0m  [16/42], [94mLoss[0m : 2.27561
[1mStep[0m  [20/42], [94mLoss[0m : 2.32803
[1mStep[0m  [24/42], [94mLoss[0m : 2.25690
[1mStep[0m  [28/42], [94mLoss[0m : 2.18574
[1mStep[0m  [32/42], [94mLoss[0m : 2.18016
[1mStep[0m  [36/42], [94mLoss[0m : 2.09924
[1mStep[0m  [40/42], [94mLoss[0m : 2.32911

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21820
[1mStep[0m  [4/42], [94mLoss[0m : 2.30588
[1mStep[0m  [8/42], [94mLoss[0m : 2.35248
[1mStep[0m  [12/42], [94mLoss[0m : 2.16857
[1mStep[0m  [16/42], [94mLoss[0m : 2.47788
[1mStep[0m  [20/42], [94mLoss[0m : 2.35786
[1mStep[0m  [24/42], [94mLoss[0m : 2.26612
[1mStep[0m  [28/42], [94mLoss[0m : 2.17080
[1mStep[0m  [32/42], [94mLoss[0m : 2.35380
[1mStep[0m  [36/42], [94mLoss[0m : 2.22243
[1mStep[0m  [40/42], [94mLoss[0m : 2.10996

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25399
[1mStep[0m  [4/42], [94mLoss[0m : 2.21885
[1mStep[0m  [8/42], [94mLoss[0m : 2.17174
[1mStep[0m  [12/42], [94mLoss[0m : 2.40345
[1mStep[0m  [16/42], [94mLoss[0m : 2.28242
[1mStep[0m  [20/42], [94mLoss[0m : 2.31296
[1mStep[0m  [24/42], [94mLoss[0m : 2.14801
[1mStep[0m  [28/42], [94mLoss[0m : 2.34187
[1mStep[0m  [32/42], [94mLoss[0m : 2.36760
[1mStep[0m  [36/42], [94mLoss[0m : 2.32755
[1mStep[0m  [40/42], [94mLoss[0m : 2.32909

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29200
[1mStep[0m  [4/42], [94mLoss[0m : 2.23226
[1mStep[0m  [8/42], [94mLoss[0m : 2.29506
[1mStep[0m  [12/42], [94mLoss[0m : 2.22841
[1mStep[0m  [16/42], [94mLoss[0m : 2.30826
[1mStep[0m  [20/42], [94mLoss[0m : 2.37422
[1mStep[0m  [24/42], [94mLoss[0m : 2.31318
[1mStep[0m  [28/42], [94mLoss[0m : 2.18313
[1mStep[0m  [32/42], [94mLoss[0m : 2.22232
[1mStep[0m  [36/42], [94mLoss[0m : 2.15431
[1mStep[0m  [40/42], [94mLoss[0m : 2.38880

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.313, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.311
====================================

Phase 1 - Evaluation MAE:  2.311084134238107
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.19548
[1mStep[0m  [4/42], [94mLoss[0m : 2.56431
[1mStep[0m  [8/42], [94mLoss[0m : 2.29681
[1mStep[0m  [12/42], [94mLoss[0m : 2.47361
[1mStep[0m  [16/42], [94mLoss[0m : 2.47558
[1mStep[0m  [20/42], [94mLoss[0m : 2.50665
[1mStep[0m  [24/42], [94mLoss[0m : 2.31755
[1mStep[0m  [28/42], [94mLoss[0m : 2.48433
[1mStep[0m  [32/42], [94mLoss[0m : 2.47637
[1mStep[0m  [36/42], [94mLoss[0m : 2.37153
[1mStep[0m  [40/42], [94mLoss[0m : 2.56947

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.309, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57654
[1mStep[0m  [4/42], [94mLoss[0m : 2.45148
[1mStep[0m  [8/42], [94mLoss[0m : 2.13873
[1mStep[0m  [12/42], [94mLoss[0m : 2.44397
[1mStep[0m  [16/42], [94mLoss[0m : 2.49141
[1mStep[0m  [20/42], [94mLoss[0m : 2.44338
[1mStep[0m  [24/42], [94mLoss[0m : 2.30131
[1mStep[0m  [28/42], [94mLoss[0m : 2.16241
[1mStep[0m  [32/42], [94mLoss[0m : 2.39153
[1mStep[0m  [36/42], [94mLoss[0m : 2.48897
[1mStep[0m  [40/42], [94mLoss[0m : 2.29903

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21470
[1mStep[0m  [4/42], [94mLoss[0m : 1.87304
[1mStep[0m  [8/42], [94mLoss[0m : 2.06723
[1mStep[0m  [12/42], [94mLoss[0m : 2.26418
[1mStep[0m  [16/42], [94mLoss[0m : 2.22225
[1mStep[0m  [20/42], [94mLoss[0m : 2.23180
[1mStep[0m  [24/42], [94mLoss[0m : 2.15691
[1mStep[0m  [28/42], [94mLoss[0m : 2.09433
[1mStep[0m  [32/42], [94mLoss[0m : 2.23950
[1mStep[0m  [36/42], [94mLoss[0m : 2.20533
[1mStep[0m  [40/42], [94mLoss[0m : 2.29901

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22176
[1mStep[0m  [4/42], [94mLoss[0m : 2.18940
[1mStep[0m  [8/42], [94mLoss[0m : 2.17720
[1mStep[0m  [12/42], [94mLoss[0m : 2.01137
[1mStep[0m  [16/42], [94mLoss[0m : 2.20564
[1mStep[0m  [20/42], [94mLoss[0m : 2.22070
[1mStep[0m  [24/42], [94mLoss[0m : 2.13515
[1mStep[0m  [28/42], [94mLoss[0m : 2.28689
[1mStep[0m  [32/42], [94mLoss[0m : 2.34896
[1mStep[0m  [36/42], [94mLoss[0m : 2.06749
[1mStep[0m  [40/42], [94mLoss[0m : 2.16136

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05045
[1mStep[0m  [4/42], [94mLoss[0m : 1.86530
[1mStep[0m  [8/42], [94mLoss[0m : 2.07640
[1mStep[0m  [12/42], [94mLoss[0m : 2.03712
[1mStep[0m  [16/42], [94mLoss[0m : 2.05472
[1mStep[0m  [20/42], [94mLoss[0m : 1.96309
[1mStep[0m  [24/42], [94mLoss[0m : 2.00739
[1mStep[0m  [28/42], [94mLoss[0m : 2.14982
[1mStep[0m  [32/42], [94mLoss[0m : 2.03339
[1mStep[0m  [36/42], [94mLoss[0m : 2.19262
[1mStep[0m  [40/42], [94mLoss[0m : 2.24478

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.074, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98174
[1mStep[0m  [4/42], [94mLoss[0m : 1.93119
[1mStep[0m  [8/42], [94mLoss[0m : 2.04922
[1mStep[0m  [12/42], [94mLoss[0m : 1.83449
[1mStep[0m  [16/42], [94mLoss[0m : 1.77050
[1mStep[0m  [20/42], [94mLoss[0m : 2.10953
[1mStep[0m  [24/42], [94mLoss[0m : 2.01628
[1mStep[0m  [28/42], [94mLoss[0m : 2.03724
[1mStep[0m  [32/42], [94mLoss[0m : 1.99711
[1mStep[0m  [36/42], [94mLoss[0m : 2.00673
[1mStep[0m  [40/42], [94mLoss[0m : 1.97312

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82382
[1mStep[0m  [4/42], [94mLoss[0m : 1.99643
[1mStep[0m  [8/42], [94mLoss[0m : 2.07395
[1mStep[0m  [12/42], [94mLoss[0m : 1.79162
[1mStep[0m  [16/42], [94mLoss[0m : 1.71263
[1mStep[0m  [20/42], [94mLoss[0m : 1.92681
[1mStep[0m  [24/42], [94mLoss[0m : 2.02826
[1mStep[0m  [28/42], [94mLoss[0m : 1.91766
[1mStep[0m  [32/42], [94mLoss[0m : 1.90053
[1mStep[0m  [36/42], [94mLoss[0m : 2.06687
[1mStep[0m  [40/42], [94mLoss[0m : 1.89486

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84462
[1mStep[0m  [4/42], [94mLoss[0m : 1.73767
[1mStep[0m  [8/42], [94mLoss[0m : 1.81813
[1mStep[0m  [12/42], [94mLoss[0m : 1.87699
[1mStep[0m  [16/42], [94mLoss[0m : 1.80679
[1mStep[0m  [20/42], [94mLoss[0m : 1.93198
[1mStep[0m  [24/42], [94mLoss[0m : 2.05539
[1mStep[0m  [28/42], [94mLoss[0m : 1.84789
[1mStep[0m  [32/42], [94mLoss[0m : 1.87871
[1mStep[0m  [36/42], [94mLoss[0m : 1.97728
[1mStep[0m  [40/42], [94mLoss[0m : 1.91841

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78403
[1mStep[0m  [4/42], [94mLoss[0m : 1.71888
[1mStep[0m  [8/42], [94mLoss[0m : 1.65142
[1mStep[0m  [12/42], [94mLoss[0m : 1.95096
[1mStep[0m  [16/42], [94mLoss[0m : 1.79332
[1mStep[0m  [20/42], [94mLoss[0m : 1.93064
[1mStep[0m  [24/42], [94mLoss[0m : 1.74575
[1mStep[0m  [28/42], [94mLoss[0m : 1.92834
[1mStep[0m  [32/42], [94mLoss[0m : 1.81325
[1mStep[0m  [36/42], [94mLoss[0m : 1.76325
[1mStep[0m  [40/42], [94mLoss[0m : 1.94139

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74290
[1mStep[0m  [4/42], [94mLoss[0m : 1.71090
[1mStep[0m  [8/42], [94mLoss[0m : 1.72079
[1mStep[0m  [12/42], [94mLoss[0m : 1.72011
[1mStep[0m  [16/42], [94mLoss[0m : 1.78310
[1mStep[0m  [20/42], [94mLoss[0m : 1.69255
[1mStep[0m  [24/42], [94mLoss[0m : 1.69854
[1mStep[0m  [28/42], [94mLoss[0m : 1.75023
[1mStep[0m  [32/42], [94mLoss[0m : 1.80364
[1mStep[0m  [36/42], [94mLoss[0m : 1.69531
[1mStep[0m  [40/42], [94mLoss[0m : 1.93479

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.780, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73705
[1mStep[0m  [4/42], [94mLoss[0m : 1.58226
[1mStep[0m  [8/42], [94mLoss[0m : 1.62797
[1mStep[0m  [12/42], [94mLoss[0m : 1.75650
[1mStep[0m  [16/42], [94mLoss[0m : 1.70995
[1mStep[0m  [20/42], [94mLoss[0m : 1.94993
[1mStep[0m  [24/42], [94mLoss[0m : 1.70671
[1mStep[0m  [28/42], [94mLoss[0m : 1.82202
[1mStep[0m  [32/42], [94mLoss[0m : 1.79217
[1mStep[0m  [36/42], [94mLoss[0m : 1.68628
[1mStep[0m  [40/42], [94mLoss[0m : 1.64393

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61024
[1mStep[0m  [4/42], [94mLoss[0m : 1.51451
[1mStep[0m  [8/42], [94mLoss[0m : 1.50404
[1mStep[0m  [12/42], [94mLoss[0m : 1.69463
[1mStep[0m  [16/42], [94mLoss[0m : 1.74194
[1mStep[0m  [20/42], [94mLoss[0m : 1.56653
[1mStep[0m  [24/42], [94mLoss[0m : 1.58538
[1mStep[0m  [28/42], [94mLoss[0m : 1.76314
[1mStep[0m  [32/42], [94mLoss[0m : 1.48607
[1mStep[0m  [36/42], [94mLoss[0m : 1.63183
[1mStep[0m  [40/42], [94mLoss[0m : 1.64485

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66723
[1mStep[0m  [4/42], [94mLoss[0m : 1.60916
[1mStep[0m  [8/42], [94mLoss[0m : 1.65243
[1mStep[0m  [12/42], [94mLoss[0m : 1.71963
[1mStep[0m  [16/42], [94mLoss[0m : 1.57126
[1mStep[0m  [20/42], [94mLoss[0m : 1.64970
[1mStep[0m  [24/42], [94mLoss[0m : 1.47478
[1mStep[0m  [28/42], [94mLoss[0m : 1.80199
[1mStep[0m  [32/42], [94mLoss[0m : 1.74391
[1mStep[0m  [36/42], [94mLoss[0m : 1.61179
[1mStep[0m  [40/42], [94mLoss[0m : 1.60391

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.615, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.39225
[1mStep[0m  [4/42], [94mLoss[0m : 1.52059
[1mStep[0m  [8/42], [94mLoss[0m : 1.62686
[1mStep[0m  [12/42], [94mLoss[0m : 1.50307
[1mStep[0m  [16/42], [94mLoss[0m : 1.64751
[1mStep[0m  [20/42], [94mLoss[0m : 1.52263
[1mStep[0m  [24/42], [94mLoss[0m : 1.86419
[1mStep[0m  [28/42], [94mLoss[0m : 1.63798
[1mStep[0m  [32/42], [94mLoss[0m : 1.66453
[1mStep[0m  [36/42], [94mLoss[0m : 1.50618
[1mStep[0m  [40/42], [94mLoss[0m : 1.57280

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59716
[1mStep[0m  [4/42], [94mLoss[0m : 1.57496
[1mStep[0m  [8/42], [94mLoss[0m : 1.64309
[1mStep[0m  [12/42], [94mLoss[0m : 1.52231
[1mStep[0m  [16/42], [94mLoss[0m : 1.49787
[1mStep[0m  [20/42], [94mLoss[0m : 1.55575
[1mStep[0m  [24/42], [94mLoss[0m : 1.57938
[1mStep[0m  [28/42], [94mLoss[0m : 1.57711
[1mStep[0m  [32/42], [94mLoss[0m : 1.47847
[1mStep[0m  [36/42], [94mLoss[0m : 1.62360
[1mStep[0m  [40/42], [94mLoss[0m : 1.61314

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42356
[1mStep[0m  [4/42], [94mLoss[0m : 1.43513
[1mStep[0m  [8/42], [94mLoss[0m : 1.45978
[1mStep[0m  [12/42], [94mLoss[0m : 1.51601
[1mStep[0m  [16/42], [94mLoss[0m : 1.53692
[1mStep[0m  [20/42], [94mLoss[0m : 1.47397
[1mStep[0m  [24/42], [94mLoss[0m : 1.55192
[1mStep[0m  [28/42], [94mLoss[0m : 1.50517
[1mStep[0m  [32/42], [94mLoss[0m : 1.56404
[1mStep[0m  [36/42], [94mLoss[0m : 1.56769
[1mStep[0m  [40/42], [94mLoss[0m : 1.51661

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.512, [92mTest[0m: 2.529, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38984
[1mStep[0m  [4/42], [94mLoss[0m : 1.29443
[1mStep[0m  [8/42], [94mLoss[0m : 1.53178
[1mStep[0m  [12/42], [94mLoss[0m : 1.55277
[1mStep[0m  [16/42], [94mLoss[0m : 1.55340
[1mStep[0m  [20/42], [94mLoss[0m : 1.46717
[1mStep[0m  [24/42], [94mLoss[0m : 1.40632
[1mStep[0m  [28/42], [94mLoss[0m : 1.37621
[1mStep[0m  [32/42], [94mLoss[0m : 1.45295
[1mStep[0m  [36/42], [94mLoss[0m : 1.32335
[1mStep[0m  [40/42], [94mLoss[0m : 1.59887

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40319
[1mStep[0m  [4/42], [94mLoss[0m : 1.60425
[1mStep[0m  [8/42], [94mLoss[0m : 1.46589
[1mStep[0m  [12/42], [94mLoss[0m : 1.57749
[1mStep[0m  [16/42], [94mLoss[0m : 1.49058
[1mStep[0m  [20/42], [94mLoss[0m : 1.37670
[1mStep[0m  [24/42], [94mLoss[0m : 1.56747
[1mStep[0m  [28/42], [94mLoss[0m : 1.37881
[1mStep[0m  [32/42], [94mLoss[0m : 1.50503
[1mStep[0m  [36/42], [94mLoss[0m : 1.62676
[1mStep[0m  [40/42], [94mLoss[0m : 1.44341

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59805
[1mStep[0m  [4/42], [94mLoss[0m : 1.29937
[1mStep[0m  [8/42], [94mLoss[0m : 1.39902
[1mStep[0m  [12/42], [94mLoss[0m : 1.35887
[1mStep[0m  [16/42], [94mLoss[0m : 1.61888
[1mStep[0m  [20/42], [94mLoss[0m : 1.39130
[1mStep[0m  [24/42], [94mLoss[0m : 1.24871
[1mStep[0m  [28/42], [94mLoss[0m : 1.42991
[1mStep[0m  [32/42], [94mLoss[0m : 1.48059
[1mStep[0m  [36/42], [94mLoss[0m : 1.48778
[1mStep[0m  [40/42], [94mLoss[0m : 1.42615

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.445, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41453
[1mStep[0m  [4/42], [94mLoss[0m : 1.33570
[1mStep[0m  [8/42], [94mLoss[0m : 1.54875
[1mStep[0m  [12/42], [94mLoss[0m : 1.45389
[1mStep[0m  [16/42], [94mLoss[0m : 1.33335
[1mStep[0m  [20/42], [94mLoss[0m : 1.26484
[1mStep[0m  [24/42], [94mLoss[0m : 1.21364
[1mStep[0m  [28/42], [94mLoss[0m : 1.50751
[1mStep[0m  [32/42], [94mLoss[0m : 1.41462
[1mStep[0m  [36/42], [94mLoss[0m : 1.43753
[1mStep[0m  [40/42], [94mLoss[0m : 1.21905

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.409, [92mTest[0m: 2.498, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37240
[1mStep[0m  [4/42], [94mLoss[0m : 1.34025
[1mStep[0m  [8/42], [94mLoss[0m : 1.43435
[1mStep[0m  [12/42], [94mLoss[0m : 1.38407
[1mStep[0m  [16/42], [94mLoss[0m : 1.24875
[1mStep[0m  [20/42], [94mLoss[0m : 1.38430
[1mStep[0m  [24/42], [94mLoss[0m : 1.38572
[1mStep[0m  [28/42], [94mLoss[0m : 1.34302
[1mStep[0m  [32/42], [94mLoss[0m : 1.32452
[1mStep[0m  [36/42], [94mLoss[0m : 1.31246
[1mStep[0m  [40/42], [94mLoss[0m : 1.34373

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.353, [92mTest[0m: 2.539, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.489
====================================

Phase 2 - Evaluation MAE:  2.489028436797006
MAE score P1        2.311084
MAE score P2        2.489028
loss                1.352648
learning_rate           0.01
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 21, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.07065
[1mStep[0m  [4/42], [94mLoss[0m : 10.84356
[1mStep[0m  [8/42], [94mLoss[0m : 10.70741
[1mStep[0m  [12/42], [94mLoss[0m : 10.58241
[1mStep[0m  [16/42], [94mLoss[0m : 10.80546
[1mStep[0m  [20/42], [94mLoss[0m : 11.02714
[1mStep[0m  [24/42], [94mLoss[0m : 10.93522
[1mStep[0m  [28/42], [94mLoss[0m : 10.82442
[1mStep[0m  [32/42], [94mLoss[0m : 10.37953
[1mStep[0m  [36/42], [94mLoss[0m : 10.42994
[1mStep[0m  [40/42], [94mLoss[0m : 10.71870

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.641, [92mTest[0m: 10.882, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.39174
[1mStep[0m  [4/42], [94mLoss[0m : 10.54690
[1mStep[0m  [8/42], [94mLoss[0m : 10.21264
[1mStep[0m  [12/42], [94mLoss[0m : 9.97059
[1mStep[0m  [16/42], [94mLoss[0m : 10.18586
[1mStep[0m  [20/42], [94mLoss[0m : 10.05986
[1mStep[0m  [24/42], [94mLoss[0m : 10.08374
[1mStep[0m  [28/42], [94mLoss[0m : 9.59705
[1mStep[0m  [32/42], [94mLoss[0m : 9.62140
[1mStep[0m  [36/42], [94mLoss[0m : 9.75174
[1mStep[0m  [40/42], [94mLoss[0m : 9.74400

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.044, [92mTest[0m: 10.207, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.75508
[1mStep[0m  [4/42], [94mLoss[0m : 9.66729
[1mStep[0m  [8/42], [94mLoss[0m : 10.04622
[1mStep[0m  [12/42], [94mLoss[0m : 9.50205
[1mStep[0m  [16/42], [94mLoss[0m : 9.85576
[1mStep[0m  [20/42], [94mLoss[0m : 9.74346
[1mStep[0m  [24/42], [94mLoss[0m : 9.28223
[1mStep[0m  [28/42], [94mLoss[0m : 8.97816
[1mStep[0m  [32/42], [94mLoss[0m : 9.31258
[1mStep[0m  [36/42], [94mLoss[0m : 9.21756
[1mStep[0m  [40/42], [94mLoss[0m : 9.04572

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.369, [92mTest[0m: 9.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.07219
[1mStep[0m  [4/42], [94mLoss[0m : 8.72017
[1mStep[0m  [8/42], [94mLoss[0m : 8.81648
[1mStep[0m  [12/42], [94mLoss[0m : 9.04738
[1mStep[0m  [16/42], [94mLoss[0m : 8.74720
[1mStep[0m  [20/42], [94mLoss[0m : 8.73503
[1mStep[0m  [24/42], [94mLoss[0m : 8.32829
[1mStep[0m  [28/42], [94mLoss[0m : 8.07200
[1mStep[0m  [32/42], [94mLoss[0m : 8.39029
[1mStep[0m  [36/42], [94mLoss[0m : 8.15272
[1mStep[0m  [40/42], [94mLoss[0m : 8.15999

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.556, [92mTest[0m: 8.631, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.77442
[1mStep[0m  [4/42], [94mLoss[0m : 8.19815
[1mStep[0m  [8/42], [94mLoss[0m : 7.87328
[1mStep[0m  [12/42], [94mLoss[0m : 7.83834
[1mStep[0m  [16/42], [94mLoss[0m : 7.75618
[1mStep[0m  [20/42], [94mLoss[0m : 7.42157
[1mStep[0m  [24/42], [94mLoss[0m : 7.35771
[1mStep[0m  [28/42], [94mLoss[0m : 7.06386
[1mStep[0m  [32/42], [94mLoss[0m : 6.97154
[1mStep[0m  [36/42], [94mLoss[0m : 6.93708
[1mStep[0m  [40/42], [94mLoss[0m : 6.59814

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.465, [92mTest[0m: 7.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.83176
[1mStep[0m  [4/42], [94mLoss[0m : 6.64475
[1mStep[0m  [8/42], [94mLoss[0m : 6.92991
[1mStep[0m  [12/42], [94mLoss[0m : 6.29391
[1mStep[0m  [16/42], [94mLoss[0m : 6.36064
[1mStep[0m  [20/42], [94mLoss[0m : 6.25601
[1mStep[0m  [24/42], [94mLoss[0m : 6.36455
[1mStep[0m  [28/42], [94mLoss[0m : 6.20481
[1mStep[0m  [32/42], [94mLoss[0m : 6.31316
[1mStep[0m  [36/42], [94mLoss[0m : 5.93065
[1mStep[0m  [40/42], [94mLoss[0m : 5.96005

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.285, [92mTest[0m: 6.218, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.32257
[1mStep[0m  [4/42], [94mLoss[0m : 5.94959
[1mStep[0m  [8/42], [94mLoss[0m : 5.51354
[1mStep[0m  [12/42], [94mLoss[0m : 5.33542
[1mStep[0m  [16/42], [94mLoss[0m : 5.52249
[1mStep[0m  [20/42], [94mLoss[0m : 5.10146
[1mStep[0m  [24/42], [94mLoss[0m : 5.19349
[1mStep[0m  [28/42], [94mLoss[0m : 5.00368
[1mStep[0m  [32/42], [94mLoss[0m : 4.61021
[1mStep[0m  [36/42], [94mLoss[0m : 5.01150
[1mStep[0m  [40/42], [94mLoss[0m : 5.02872

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.172, [92mTest[0m: 4.854, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.71715
[1mStep[0m  [4/42], [94mLoss[0m : 4.45723
[1mStep[0m  [8/42], [94mLoss[0m : 4.45837
[1mStep[0m  [12/42], [94mLoss[0m : 4.15936
[1mStep[0m  [16/42], [94mLoss[0m : 4.22927
[1mStep[0m  [20/42], [94mLoss[0m : 4.23139
[1mStep[0m  [24/42], [94mLoss[0m : 4.11973
[1mStep[0m  [28/42], [94mLoss[0m : 3.97519
[1mStep[0m  [32/42], [94mLoss[0m : 3.80848
[1mStep[0m  [36/42], [94mLoss[0m : 3.72142
[1mStep[0m  [40/42], [94mLoss[0m : 3.79067

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.128, [92mTest[0m: 3.763, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.65898
[1mStep[0m  [4/42], [94mLoss[0m : 3.75685
[1mStep[0m  [8/42], [94mLoss[0m : 3.32226
[1mStep[0m  [12/42], [94mLoss[0m : 3.39215
[1mStep[0m  [16/42], [94mLoss[0m : 3.61672
[1mStep[0m  [20/42], [94mLoss[0m : 3.42607
[1mStep[0m  [24/42], [94mLoss[0m : 2.84380
[1mStep[0m  [28/42], [94mLoss[0m : 3.13587
[1mStep[0m  [32/42], [94mLoss[0m : 3.05698
[1mStep[0m  [36/42], [94mLoss[0m : 3.11465
[1mStep[0m  [40/42], [94mLoss[0m : 3.19207

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.319, [92mTest[0m: 2.939, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76908
[1mStep[0m  [4/42], [94mLoss[0m : 2.90802
[1mStep[0m  [8/42], [94mLoss[0m : 2.98379
[1mStep[0m  [12/42], [94mLoss[0m : 3.44717
[1mStep[0m  [16/42], [94mLoss[0m : 3.10656
[1mStep[0m  [20/42], [94mLoss[0m : 2.95391
[1mStep[0m  [24/42], [94mLoss[0m : 2.90567
[1mStep[0m  [28/42], [94mLoss[0m : 2.76432
[1mStep[0m  [32/42], [94mLoss[0m : 3.11722
[1mStep[0m  [36/42], [94mLoss[0m : 2.75665
[1mStep[0m  [40/42], [94mLoss[0m : 3.01435

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.979, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69816
[1mStep[0m  [4/42], [94mLoss[0m : 2.81055
[1mStep[0m  [8/42], [94mLoss[0m : 2.84272
[1mStep[0m  [12/42], [94mLoss[0m : 2.72805
[1mStep[0m  [16/42], [94mLoss[0m : 2.76054
[1mStep[0m  [20/42], [94mLoss[0m : 2.69245
[1mStep[0m  [24/42], [94mLoss[0m : 2.73547
[1mStep[0m  [28/42], [94mLoss[0m : 2.74169
[1mStep[0m  [32/42], [94mLoss[0m : 2.76055
[1mStep[0m  [36/42], [94mLoss[0m : 2.95645
[1mStep[0m  [40/42], [94mLoss[0m : 2.84098

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.872, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.11763
[1mStep[0m  [4/42], [94mLoss[0m : 3.12220
[1mStep[0m  [8/42], [94mLoss[0m : 3.07461
[1mStep[0m  [12/42], [94mLoss[0m : 2.89872
[1mStep[0m  [16/42], [94mLoss[0m : 2.94914
[1mStep[0m  [20/42], [94mLoss[0m : 2.73963
[1mStep[0m  [24/42], [94mLoss[0m : 2.52686
[1mStep[0m  [28/42], [94mLoss[0m : 2.90936
[1mStep[0m  [32/42], [94mLoss[0m : 2.96179
[1mStep[0m  [36/42], [94mLoss[0m : 2.75330
[1mStep[0m  [40/42], [94mLoss[0m : 2.86277

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.820, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.88345
[1mStep[0m  [4/42], [94mLoss[0m : 2.68462
[1mStep[0m  [8/42], [94mLoss[0m : 2.97915
[1mStep[0m  [12/42], [94mLoss[0m : 2.82663
[1mStep[0m  [16/42], [94mLoss[0m : 3.05202
[1mStep[0m  [20/42], [94mLoss[0m : 2.85468
[1mStep[0m  [24/42], [94mLoss[0m : 2.75213
[1mStep[0m  [28/42], [94mLoss[0m : 2.64760
[1mStep[0m  [32/42], [94mLoss[0m : 2.78296
[1mStep[0m  [36/42], [94mLoss[0m : 2.62920
[1mStep[0m  [40/42], [94mLoss[0m : 2.88509

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.802, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.17671
[1mStep[0m  [4/42], [94mLoss[0m : 2.93669
[1mStep[0m  [8/42], [94mLoss[0m : 2.76574
[1mStep[0m  [12/42], [94mLoss[0m : 2.86194
[1mStep[0m  [16/42], [94mLoss[0m : 2.87981
[1mStep[0m  [20/42], [94mLoss[0m : 2.65276
[1mStep[0m  [24/42], [94mLoss[0m : 2.99374
[1mStep[0m  [28/42], [94mLoss[0m : 2.51722
[1mStep[0m  [32/42], [94mLoss[0m : 3.05329
[1mStep[0m  [36/42], [94mLoss[0m : 2.58364
[1mStep[0m  [40/42], [94mLoss[0m : 2.71960

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.796, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79535
[1mStep[0m  [4/42], [94mLoss[0m : 2.99927
[1mStep[0m  [8/42], [94mLoss[0m : 2.73308
[1mStep[0m  [12/42], [94mLoss[0m : 2.87632
[1mStep[0m  [16/42], [94mLoss[0m : 2.90533
[1mStep[0m  [20/42], [94mLoss[0m : 2.70977
[1mStep[0m  [24/42], [94mLoss[0m : 2.59470
[1mStep[0m  [28/42], [94mLoss[0m : 2.64129
[1mStep[0m  [32/42], [94mLoss[0m : 2.69711
[1mStep[0m  [36/42], [94mLoss[0m : 2.85315
[1mStep[0m  [40/42], [94mLoss[0m : 2.81744

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86169
[1mStep[0m  [4/42], [94mLoss[0m : 2.79334
[1mStep[0m  [8/42], [94mLoss[0m : 2.73397
[1mStep[0m  [12/42], [94mLoss[0m : 2.79695
[1mStep[0m  [16/42], [94mLoss[0m : 2.86805
[1mStep[0m  [20/42], [94mLoss[0m : 2.58136
[1mStep[0m  [24/42], [94mLoss[0m : 2.77982
[1mStep[0m  [28/42], [94mLoss[0m : 2.96696
[1mStep[0m  [32/42], [94mLoss[0m : 2.74773
[1mStep[0m  [36/42], [94mLoss[0m : 2.74746
[1mStep[0m  [40/42], [94mLoss[0m : 2.64749

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.730, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.05359
[1mStep[0m  [4/42], [94mLoss[0m : 2.95458
[1mStep[0m  [8/42], [94mLoss[0m : 2.89755
[1mStep[0m  [12/42], [94mLoss[0m : 2.65654
[1mStep[0m  [16/42], [94mLoss[0m : 2.89257
[1mStep[0m  [20/42], [94mLoss[0m : 2.85831
[1mStep[0m  [24/42], [94mLoss[0m : 2.72370
[1mStep[0m  [28/42], [94mLoss[0m : 2.63965
[1mStep[0m  [32/42], [94mLoss[0m : 2.90447
[1mStep[0m  [36/42], [94mLoss[0m : 2.72742
[1mStep[0m  [40/42], [94mLoss[0m : 2.74258

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.742, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71223
[1mStep[0m  [4/42], [94mLoss[0m : 2.98446
[1mStep[0m  [8/42], [94mLoss[0m : 2.68203
[1mStep[0m  [12/42], [94mLoss[0m : 2.84419
[1mStep[0m  [16/42], [94mLoss[0m : 2.44636
[1mStep[0m  [20/42], [94mLoss[0m : 2.73651
[1mStep[0m  [24/42], [94mLoss[0m : 2.52993
[1mStep[0m  [28/42], [94mLoss[0m : 2.72009
[1mStep[0m  [32/42], [94mLoss[0m : 2.82497
[1mStep[0m  [36/42], [94mLoss[0m : 2.93389
[1mStep[0m  [40/42], [94mLoss[0m : 2.42658

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.739, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.91858
[1mStep[0m  [4/42], [94mLoss[0m : 2.92399
[1mStep[0m  [8/42], [94mLoss[0m : 2.59906
[1mStep[0m  [12/42], [94mLoss[0m : 2.81338
[1mStep[0m  [16/42], [94mLoss[0m : 2.66422
[1mStep[0m  [20/42], [94mLoss[0m : 2.57134
[1mStep[0m  [24/42], [94mLoss[0m : 2.82063
[1mStep[0m  [28/42], [94mLoss[0m : 2.68830
[1mStep[0m  [32/42], [94mLoss[0m : 2.83096
[1mStep[0m  [36/42], [94mLoss[0m : 2.60526
[1mStep[0m  [40/42], [94mLoss[0m : 2.71037

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82044
[1mStep[0m  [4/42], [94mLoss[0m : 2.65417
[1mStep[0m  [8/42], [94mLoss[0m : 2.67209
[1mStep[0m  [12/42], [94mLoss[0m : 2.60857
[1mStep[0m  [16/42], [94mLoss[0m : 2.74144
[1mStep[0m  [20/42], [94mLoss[0m : 2.83826
[1mStep[0m  [24/42], [94mLoss[0m : 2.73784
[1mStep[0m  [28/42], [94mLoss[0m : 2.48601
[1mStep[0m  [32/42], [94mLoss[0m : 2.76835
[1mStep[0m  [36/42], [94mLoss[0m : 2.50435
[1mStep[0m  [40/42], [94mLoss[0m : 2.69747

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60436
[1mStep[0m  [4/42], [94mLoss[0m : 2.72455
[1mStep[0m  [8/42], [94mLoss[0m : 2.91440
[1mStep[0m  [12/42], [94mLoss[0m : 2.75111
[1mStep[0m  [16/42], [94mLoss[0m : 2.55650
[1mStep[0m  [20/42], [94mLoss[0m : 2.83022
[1mStep[0m  [24/42], [94mLoss[0m : 2.78196
[1mStep[0m  [28/42], [94mLoss[0m : 2.78253
[1mStep[0m  [32/42], [94mLoss[0m : 2.54733
[1mStep[0m  [36/42], [94mLoss[0m : 2.87418
[1mStep[0m  [40/42], [94mLoss[0m : 2.83825

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66763
[1mStep[0m  [4/42], [94mLoss[0m : 2.47723
[1mStep[0m  [8/42], [94mLoss[0m : 2.81885
[1mStep[0m  [12/42], [94mLoss[0m : 2.86753
[1mStep[0m  [16/42], [94mLoss[0m : 2.85743
[1mStep[0m  [20/42], [94mLoss[0m : 2.53244
[1mStep[0m  [24/42], [94mLoss[0m : 2.61564
[1mStep[0m  [28/42], [94mLoss[0m : 2.56777
[1mStep[0m  [32/42], [94mLoss[0m : 2.74346
[1mStep[0m  [36/42], [94mLoss[0m : 2.45947
[1mStep[0m  [40/42], [94mLoss[0m : 2.77830

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.360, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64276
[1mStep[0m  [4/42], [94mLoss[0m : 2.63151
[1mStep[0m  [8/42], [94mLoss[0m : 2.69651
[1mStep[0m  [12/42], [94mLoss[0m : 2.53577
[1mStep[0m  [16/42], [94mLoss[0m : 2.57207
[1mStep[0m  [20/42], [94mLoss[0m : 2.73430
[1mStep[0m  [24/42], [94mLoss[0m : 2.72268
[1mStep[0m  [28/42], [94mLoss[0m : 2.90960
[1mStep[0m  [32/42], [94mLoss[0m : 2.70164
[1mStep[0m  [36/42], [94mLoss[0m : 2.50347
[1mStep[0m  [40/42], [94mLoss[0m : 2.41501

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.94729
[1mStep[0m  [4/42], [94mLoss[0m : 2.65615
[1mStep[0m  [8/42], [94mLoss[0m : 2.72484
[1mStep[0m  [12/42], [94mLoss[0m : 2.54776
[1mStep[0m  [16/42], [94mLoss[0m : 2.74856
[1mStep[0m  [20/42], [94mLoss[0m : 2.45634
[1mStep[0m  [24/42], [94mLoss[0m : 2.59280
[1mStep[0m  [28/42], [94mLoss[0m : 2.91066
[1mStep[0m  [32/42], [94mLoss[0m : 2.49705
[1mStep[0m  [36/42], [94mLoss[0m : 2.70849
[1mStep[0m  [40/42], [94mLoss[0m : 2.88388

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82415
[1mStep[0m  [4/42], [94mLoss[0m : 2.53778
[1mStep[0m  [8/42], [94mLoss[0m : 2.36880
[1mStep[0m  [12/42], [94mLoss[0m : 2.69711
[1mStep[0m  [16/42], [94mLoss[0m : 2.77360
[1mStep[0m  [20/42], [94mLoss[0m : 2.54592
[1mStep[0m  [24/42], [94mLoss[0m : 2.62124
[1mStep[0m  [28/42], [94mLoss[0m : 2.61871
[1mStep[0m  [32/42], [94mLoss[0m : 2.65377
[1mStep[0m  [36/42], [94mLoss[0m : 2.50428
[1mStep[0m  [40/42], [94mLoss[0m : 2.56381

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70573
[1mStep[0m  [4/42], [94mLoss[0m : 2.63736
[1mStep[0m  [8/42], [94mLoss[0m : 2.43519
[1mStep[0m  [12/42], [94mLoss[0m : 2.60532
[1mStep[0m  [16/42], [94mLoss[0m : 2.56741
[1mStep[0m  [20/42], [94mLoss[0m : 2.68932
[1mStep[0m  [24/42], [94mLoss[0m : 2.77246
[1mStep[0m  [28/42], [94mLoss[0m : 2.65467
[1mStep[0m  [32/42], [94mLoss[0m : 2.64262
[1mStep[0m  [36/42], [94mLoss[0m : 2.66437
[1mStep[0m  [40/42], [94mLoss[0m : 2.75223

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.88141
[1mStep[0m  [4/42], [94mLoss[0m : 2.74155
[1mStep[0m  [8/42], [94mLoss[0m : 2.78046
[1mStep[0m  [12/42], [94mLoss[0m : 2.71467
[1mStep[0m  [16/42], [94mLoss[0m : 2.59151
[1mStep[0m  [20/42], [94mLoss[0m : 2.41997
[1mStep[0m  [24/42], [94mLoss[0m : 2.51540
[1mStep[0m  [28/42], [94mLoss[0m : 2.79276
[1mStep[0m  [32/42], [94mLoss[0m : 2.42411
[1mStep[0m  [36/42], [94mLoss[0m : 2.49591
[1mStep[0m  [40/42], [94mLoss[0m : 2.47990

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57141
[1mStep[0m  [4/42], [94mLoss[0m : 2.68509
[1mStep[0m  [8/42], [94mLoss[0m : 2.74861
[1mStep[0m  [12/42], [94mLoss[0m : 2.49093
[1mStep[0m  [16/42], [94mLoss[0m : 2.53430
[1mStep[0m  [20/42], [94mLoss[0m : 2.63500
[1mStep[0m  [24/42], [94mLoss[0m : 2.61182
[1mStep[0m  [28/42], [94mLoss[0m : 2.61799
[1mStep[0m  [32/42], [94mLoss[0m : 2.55917
[1mStep[0m  [36/42], [94mLoss[0m : 2.46723
[1mStep[0m  [40/42], [94mLoss[0m : 2.61774

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86584
[1mStep[0m  [4/42], [94mLoss[0m : 2.51489
[1mStep[0m  [8/42], [94mLoss[0m : 2.72772
[1mStep[0m  [12/42], [94mLoss[0m : 2.74904
[1mStep[0m  [16/42], [94mLoss[0m : 2.64419
[1mStep[0m  [20/42], [94mLoss[0m : 2.59512
[1mStep[0m  [24/42], [94mLoss[0m : 2.58416
[1mStep[0m  [28/42], [94mLoss[0m : 2.77206
[1mStep[0m  [32/42], [94mLoss[0m : 2.78766
[1mStep[0m  [36/42], [94mLoss[0m : 2.53253
[1mStep[0m  [40/42], [94mLoss[0m : 2.58411

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61241
[1mStep[0m  [4/42], [94mLoss[0m : 2.60903
[1mStep[0m  [8/42], [94mLoss[0m : 2.26650
[1mStep[0m  [12/42], [94mLoss[0m : 2.74005
[1mStep[0m  [16/42], [94mLoss[0m : 2.79614
[1mStep[0m  [20/42], [94mLoss[0m : 2.50434
[1mStep[0m  [24/42], [94mLoss[0m : 2.45150
[1mStep[0m  [28/42], [94mLoss[0m : 2.46260
[1mStep[0m  [32/42], [94mLoss[0m : 2.61410
[1mStep[0m  [36/42], [94mLoss[0m : 2.62595
[1mStep[0m  [40/42], [94mLoss[0m : 2.63507

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.347
====================================

Phase 1 - Evaluation MAE:  2.347198281969343
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.56893
[1mStep[0m  [4/42], [94mLoss[0m : 2.74919
[1mStep[0m  [8/42], [94mLoss[0m : 2.76700
[1mStep[0m  [12/42], [94mLoss[0m : 2.50172
[1mStep[0m  [16/42], [94mLoss[0m : 2.56732
[1mStep[0m  [20/42], [94mLoss[0m : 2.79016
[1mStep[0m  [24/42], [94mLoss[0m : 2.56712
[1mStep[0m  [28/42], [94mLoss[0m : 2.92216
[1mStep[0m  [32/42], [94mLoss[0m : 2.68269
[1mStep[0m  [36/42], [94mLoss[0m : 2.61967
[1mStep[0m  [40/42], [94mLoss[0m : 2.69586

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.95518
[1mStep[0m  [4/42], [94mLoss[0m : 2.48021
[1mStep[0m  [8/42], [94mLoss[0m : 2.36909
[1mStep[0m  [12/42], [94mLoss[0m : 2.88130
[1mStep[0m  [16/42], [94mLoss[0m : 2.70460
[1mStep[0m  [20/42], [94mLoss[0m : 2.67247
[1mStep[0m  [24/42], [94mLoss[0m : 2.67638
[1mStep[0m  [28/42], [94mLoss[0m : 2.90756
[1mStep[0m  [32/42], [94mLoss[0m : 2.83931
[1mStep[0m  [36/42], [94mLoss[0m : 2.60333
[1mStep[0m  [40/42], [94mLoss[0m : 2.54059

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55213
[1mStep[0m  [4/42], [94mLoss[0m : 2.29637
[1mStep[0m  [8/42], [94mLoss[0m : 2.53918
[1mStep[0m  [12/42], [94mLoss[0m : 2.50938
[1mStep[0m  [16/42], [94mLoss[0m : 2.50787
[1mStep[0m  [20/42], [94mLoss[0m : 2.65659
[1mStep[0m  [24/42], [94mLoss[0m : 2.49266
[1mStep[0m  [28/42], [94mLoss[0m : 2.61759
[1mStep[0m  [32/42], [94mLoss[0m : 2.48814
[1mStep[0m  [36/42], [94mLoss[0m : 2.74911
[1mStep[0m  [40/42], [94mLoss[0m : 2.61428

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37641
[1mStep[0m  [4/42], [94mLoss[0m : 2.24043
[1mStep[0m  [8/42], [94mLoss[0m : 2.68730
[1mStep[0m  [12/42], [94mLoss[0m : 2.75905
[1mStep[0m  [16/42], [94mLoss[0m : 2.62562
[1mStep[0m  [20/42], [94mLoss[0m : 2.37755
[1mStep[0m  [24/42], [94mLoss[0m : 2.50314
[1mStep[0m  [28/42], [94mLoss[0m : 2.58862
[1mStep[0m  [32/42], [94mLoss[0m : 2.56963
[1mStep[0m  [36/42], [94mLoss[0m : 2.39494
[1mStep[0m  [40/42], [94mLoss[0m : 2.37081

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48173
[1mStep[0m  [4/42], [94mLoss[0m : 2.52575
[1mStep[0m  [8/42], [94mLoss[0m : 2.26523
[1mStep[0m  [12/42], [94mLoss[0m : 2.56446
[1mStep[0m  [16/42], [94mLoss[0m : 2.68260
[1mStep[0m  [20/42], [94mLoss[0m : 2.75244
[1mStep[0m  [24/42], [94mLoss[0m : 2.41375
[1mStep[0m  [28/42], [94mLoss[0m : 2.60499
[1mStep[0m  [32/42], [94mLoss[0m : 2.53800
[1mStep[0m  [36/42], [94mLoss[0m : 2.52757
[1mStep[0m  [40/42], [94mLoss[0m : 2.56311

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42354
[1mStep[0m  [4/42], [94mLoss[0m : 2.71240
[1mStep[0m  [8/42], [94mLoss[0m : 2.36680
[1mStep[0m  [12/42], [94mLoss[0m : 2.60366
[1mStep[0m  [16/42], [94mLoss[0m : 2.48168
[1mStep[0m  [20/42], [94mLoss[0m : 2.64363
[1mStep[0m  [24/42], [94mLoss[0m : 2.42921
[1mStep[0m  [28/42], [94mLoss[0m : 2.48479
[1mStep[0m  [32/42], [94mLoss[0m : 2.64141
[1mStep[0m  [36/42], [94mLoss[0m : 2.45583
[1mStep[0m  [40/42], [94mLoss[0m : 2.85709

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53108
[1mStep[0m  [4/42], [94mLoss[0m : 2.48853
[1mStep[0m  [8/42], [94mLoss[0m : 2.64284
[1mStep[0m  [12/42], [94mLoss[0m : 2.37860
[1mStep[0m  [16/42], [94mLoss[0m : 2.71592
[1mStep[0m  [20/42], [94mLoss[0m : 2.45026
[1mStep[0m  [24/42], [94mLoss[0m : 2.27935
[1mStep[0m  [28/42], [94mLoss[0m : 2.48274
[1mStep[0m  [32/42], [94mLoss[0m : 2.40536
[1mStep[0m  [36/42], [94mLoss[0m : 2.57038
[1mStep[0m  [40/42], [94mLoss[0m : 2.40694

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49680
[1mStep[0m  [4/42], [94mLoss[0m : 2.38129
[1mStep[0m  [8/42], [94mLoss[0m : 2.42411
[1mStep[0m  [12/42], [94mLoss[0m : 2.45502
[1mStep[0m  [16/42], [94mLoss[0m : 2.55901
[1mStep[0m  [20/42], [94mLoss[0m : 2.35482
[1mStep[0m  [24/42], [94mLoss[0m : 2.56049
[1mStep[0m  [28/42], [94mLoss[0m : 2.61318
[1mStep[0m  [32/42], [94mLoss[0m : 2.33107
[1mStep[0m  [36/42], [94mLoss[0m : 2.37212
[1mStep[0m  [40/42], [94mLoss[0m : 2.62462

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43857
[1mStep[0m  [4/42], [94mLoss[0m : 2.41120
[1mStep[0m  [8/42], [94mLoss[0m : 2.36463
[1mStep[0m  [12/42], [94mLoss[0m : 2.34752
[1mStep[0m  [16/42], [94mLoss[0m : 2.26294
[1mStep[0m  [20/42], [94mLoss[0m : 2.45799
[1mStep[0m  [24/42], [94mLoss[0m : 2.54816
[1mStep[0m  [28/42], [94mLoss[0m : 2.35526
[1mStep[0m  [32/42], [94mLoss[0m : 2.40987
[1mStep[0m  [36/42], [94mLoss[0m : 2.64070
[1mStep[0m  [40/42], [94mLoss[0m : 2.65819

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31488
[1mStep[0m  [4/42], [94mLoss[0m : 2.26714
[1mStep[0m  [8/42], [94mLoss[0m : 2.46015
[1mStep[0m  [12/42], [94mLoss[0m : 2.29157
[1mStep[0m  [16/42], [94mLoss[0m : 2.32862
[1mStep[0m  [20/42], [94mLoss[0m : 2.44985
[1mStep[0m  [24/42], [94mLoss[0m : 2.22027
[1mStep[0m  [28/42], [94mLoss[0m : 2.39949
[1mStep[0m  [32/42], [94mLoss[0m : 2.29258
[1mStep[0m  [36/42], [94mLoss[0m : 2.34800
[1mStep[0m  [40/42], [94mLoss[0m : 2.28493

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26793
[1mStep[0m  [4/42], [94mLoss[0m : 2.15672
[1mStep[0m  [8/42], [94mLoss[0m : 2.31143
[1mStep[0m  [12/42], [94mLoss[0m : 2.28380
[1mStep[0m  [16/42], [94mLoss[0m : 2.39947
[1mStep[0m  [20/42], [94mLoss[0m : 2.11988
[1mStep[0m  [24/42], [94mLoss[0m : 2.18279
[1mStep[0m  [28/42], [94mLoss[0m : 2.47377
[1mStep[0m  [32/42], [94mLoss[0m : 2.33832
[1mStep[0m  [36/42], [94mLoss[0m : 2.21415
[1mStep[0m  [40/42], [94mLoss[0m : 2.40412

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21911
[1mStep[0m  [4/42], [94mLoss[0m : 2.15667
[1mStep[0m  [8/42], [94mLoss[0m : 2.51635
[1mStep[0m  [12/42], [94mLoss[0m : 2.22895
[1mStep[0m  [16/42], [94mLoss[0m : 2.35939
[1mStep[0m  [20/42], [94mLoss[0m : 2.25322
[1mStep[0m  [24/42], [94mLoss[0m : 2.44780
[1mStep[0m  [28/42], [94mLoss[0m : 2.28950
[1mStep[0m  [32/42], [94mLoss[0m : 2.25457
[1mStep[0m  [36/42], [94mLoss[0m : 2.27598
[1mStep[0m  [40/42], [94mLoss[0m : 2.57181

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38533
[1mStep[0m  [4/42], [94mLoss[0m : 2.35309
[1mStep[0m  [8/42], [94mLoss[0m : 2.23018
[1mStep[0m  [12/42], [94mLoss[0m : 2.22507
[1mStep[0m  [16/42], [94mLoss[0m : 2.37383
[1mStep[0m  [20/42], [94mLoss[0m : 2.10421
[1mStep[0m  [24/42], [94mLoss[0m : 2.29345
[1mStep[0m  [28/42], [94mLoss[0m : 2.20794
[1mStep[0m  [32/42], [94mLoss[0m : 2.24158
[1mStep[0m  [36/42], [94mLoss[0m : 2.61139
[1mStep[0m  [40/42], [94mLoss[0m : 2.28870

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31270
[1mStep[0m  [4/42], [94mLoss[0m : 2.43957
[1mStep[0m  [8/42], [94mLoss[0m : 2.31516
[1mStep[0m  [12/42], [94mLoss[0m : 2.30514
[1mStep[0m  [16/42], [94mLoss[0m : 2.27161
[1mStep[0m  [20/42], [94mLoss[0m : 2.10929
[1mStep[0m  [24/42], [94mLoss[0m : 2.31072
[1mStep[0m  [28/42], [94mLoss[0m : 2.21500
[1mStep[0m  [32/42], [94mLoss[0m : 2.16453
[1mStep[0m  [36/42], [94mLoss[0m : 2.53512
[1mStep[0m  [40/42], [94mLoss[0m : 2.34038

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97932
[1mStep[0m  [4/42], [94mLoss[0m : 2.23632
[1mStep[0m  [8/42], [94mLoss[0m : 2.03021
[1mStep[0m  [12/42], [94mLoss[0m : 2.26225
[1mStep[0m  [16/42], [94mLoss[0m : 2.09010
[1mStep[0m  [20/42], [94mLoss[0m : 2.25505
[1mStep[0m  [24/42], [94mLoss[0m : 2.15893
[1mStep[0m  [28/42], [94mLoss[0m : 2.27515
[1mStep[0m  [32/42], [94mLoss[0m : 2.38393
[1mStep[0m  [36/42], [94mLoss[0m : 2.31042
[1mStep[0m  [40/42], [94mLoss[0m : 2.37711

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18405
[1mStep[0m  [4/42], [94mLoss[0m : 2.08649
[1mStep[0m  [8/42], [94mLoss[0m : 2.15366
[1mStep[0m  [12/42], [94mLoss[0m : 1.96243
[1mStep[0m  [16/42], [94mLoss[0m : 2.16504
[1mStep[0m  [20/42], [94mLoss[0m : 2.20423
[1mStep[0m  [24/42], [94mLoss[0m : 2.02613
[1mStep[0m  [28/42], [94mLoss[0m : 2.48024
[1mStep[0m  [32/42], [94mLoss[0m : 2.18635
[1mStep[0m  [36/42], [94mLoss[0m : 2.01371
[1mStep[0m  [40/42], [94mLoss[0m : 2.19724

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01269
[1mStep[0m  [4/42], [94mLoss[0m : 2.06761
[1mStep[0m  [8/42], [94mLoss[0m : 2.19583
[1mStep[0m  [12/42], [94mLoss[0m : 2.01303
[1mStep[0m  [16/42], [94mLoss[0m : 1.96225
[1mStep[0m  [20/42], [94mLoss[0m : 1.94614
[1mStep[0m  [24/42], [94mLoss[0m : 2.25070
[1mStep[0m  [28/42], [94mLoss[0m : 2.18861
[1mStep[0m  [32/42], [94mLoss[0m : 2.16175
[1mStep[0m  [36/42], [94mLoss[0m : 2.24712
[1mStep[0m  [40/42], [94mLoss[0m : 1.98114

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13745
[1mStep[0m  [4/42], [94mLoss[0m : 2.05594
[1mStep[0m  [8/42], [94mLoss[0m : 1.97161
[1mStep[0m  [12/42], [94mLoss[0m : 2.11188
[1mStep[0m  [16/42], [94mLoss[0m : 2.15435
[1mStep[0m  [20/42], [94mLoss[0m : 1.94775
[1mStep[0m  [24/42], [94mLoss[0m : 2.11653
[1mStep[0m  [28/42], [94mLoss[0m : 2.11597
[1mStep[0m  [32/42], [94mLoss[0m : 2.01065
[1mStep[0m  [36/42], [94mLoss[0m : 2.16769
[1mStep[0m  [40/42], [94mLoss[0m : 2.43682

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04525
[1mStep[0m  [4/42], [94mLoss[0m : 2.21815
[1mStep[0m  [8/42], [94mLoss[0m : 2.03547
[1mStep[0m  [12/42], [94mLoss[0m : 1.93994
[1mStep[0m  [16/42], [94mLoss[0m : 2.09322
[1mStep[0m  [20/42], [94mLoss[0m : 1.92857
[1mStep[0m  [24/42], [94mLoss[0m : 1.94881
[1mStep[0m  [28/42], [94mLoss[0m : 2.22259
[1mStep[0m  [32/42], [94mLoss[0m : 2.15180
[1mStep[0m  [36/42], [94mLoss[0m : 2.15632
[1mStep[0m  [40/42], [94mLoss[0m : 1.99000

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10004
[1mStep[0m  [4/42], [94mLoss[0m : 2.04194
[1mStep[0m  [8/42], [94mLoss[0m : 2.10651
[1mStep[0m  [12/42], [94mLoss[0m : 2.28824
[1mStep[0m  [16/42], [94mLoss[0m : 2.03118
[1mStep[0m  [20/42], [94mLoss[0m : 1.93567
[1mStep[0m  [24/42], [94mLoss[0m : 1.82627
[1mStep[0m  [28/42], [94mLoss[0m : 2.06778
[1mStep[0m  [32/42], [94mLoss[0m : 1.95837
[1mStep[0m  [36/42], [94mLoss[0m : 2.04104
[1mStep[0m  [40/42], [94mLoss[0m : 2.10027

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.062, [92mTest[0m: 2.481, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94364
[1mStep[0m  [4/42], [94mLoss[0m : 1.88838
[1mStep[0m  [8/42], [94mLoss[0m : 2.06908
[1mStep[0m  [12/42], [94mLoss[0m : 2.17801
[1mStep[0m  [16/42], [94mLoss[0m : 2.13108
[1mStep[0m  [20/42], [94mLoss[0m : 1.94378
[1mStep[0m  [24/42], [94mLoss[0m : 1.93748
[1mStep[0m  [28/42], [94mLoss[0m : 2.02085
[1mStep[0m  [32/42], [94mLoss[0m : 2.07591
[1mStep[0m  [36/42], [94mLoss[0m : 2.15939
[1mStep[0m  [40/42], [94mLoss[0m : 1.92026

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.566, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00942
[1mStep[0m  [4/42], [94mLoss[0m : 1.85798
[1mStep[0m  [8/42], [94mLoss[0m : 2.08233
[1mStep[0m  [12/42], [94mLoss[0m : 1.92710
[1mStep[0m  [16/42], [94mLoss[0m : 1.94904
[1mStep[0m  [20/42], [94mLoss[0m : 2.10467
[1mStep[0m  [24/42], [94mLoss[0m : 2.08286
[1mStep[0m  [28/42], [94mLoss[0m : 2.01561
[1mStep[0m  [32/42], [94mLoss[0m : 2.06097
[1mStep[0m  [36/42], [94mLoss[0m : 2.04565
[1mStep[0m  [40/42], [94mLoss[0m : 2.08001

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.472, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93248
[1mStep[0m  [4/42], [94mLoss[0m : 1.98032
[1mStep[0m  [8/42], [94mLoss[0m : 1.95026
[1mStep[0m  [12/42], [94mLoss[0m : 2.06645
[1mStep[0m  [16/42], [94mLoss[0m : 2.03320
[1mStep[0m  [20/42], [94mLoss[0m : 2.04461
[1mStep[0m  [24/42], [94mLoss[0m : 1.90946
[1mStep[0m  [28/42], [94mLoss[0m : 2.17779
[1mStep[0m  [32/42], [94mLoss[0m : 2.08445
[1mStep[0m  [36/42], [94mLoss[0m : 1.80226
[1mStep[0m  [40/42], [94mLoss[0m : 2.03757

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79947
[1mStep[0m  [4/42], [94mLoss[0m : 1.87082
[1mStep[0m  [8/42], [94mLoss[0m : 1.83429
[1mStep[0m  [12/42], [94mLoss[0m : 1.85458
[1mStep[0m  [16/42], [94mLoss[0m : 1.88985
[1mStep[0m  [20/42], [94mLoss[0m : 1.85166
[1mStep[0m  [24/42], [94mLoss[0m : 1.94364
[1mStep[0m  [28/42], [94mLoss[0m : 2.09206
[1mStep[0m  [32/42], [94mLoss[0m : 2.02480
[1mStep[0m  [36/42], [94mLoss[0m : 1.82992
[1mStep[0m  [40/42], [94mLoss[0m : 1.99616

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93400
[1mStep[0m  [4/42], [94mLoss[0m : 1.88114
[1mStep[0m  [8/42], [94mLoss[0m : 1.79777
[1mStep[0m  [12/42], [94mLoss[0m : 1.97336
[1mStep[0m  [16/42], [94mLoss[0m : 1.98129
[1mStep[0m  [20/42], [94mLoss[0m : 2.08076
[1mStep[0m  [24/42], [94mLoss[0m : 1.77479
[1mStep[0m  [28/42], [94mLoss[0m : 1.93158
[1mStep[0m  [32/42], [94mLoss[0m : 1.83597
[1mStep[0m  [36/42], [94mLoss[0m : 2.12164
[1mStep[0m  [40/42], [94mLoss[0m : 1.87018

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.529, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91475
[1mStep[0m  [4/42], [94mLoss[0m : 1.98768
[1mStep[0m  [8/42], [94mLoss[0m : 1.79884
[1mStep[0m  [12/42], [94mLoss[0m : 1.95408
[1mStep[0m  [16/42], [94mLoss[0m : 1.86963
[1mStep[0m  [20/42], [94mLoss[0m : 1.77467
[1mStep[0m  [24/42], [94mLoss[0m : 1.96525
[1mStep[0m  [28/42], [94mLoss[0m : 1.93588
[1mStep[0m  [32/42], [94mLoss[0m : 1.89468
[1mStep[0m  [36/42], [94mLoss[0m : 1.97774
[1mStep[0m  [40/42], [94mLoss[0m : 2.01591

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.906, [92mTest[0m: 2.493, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80669
[1mStep[0m  [4/42], [94mLoss[0m : 1.79717
[1mStep[0m  [8/42], [94mLoss[0m : 1.86610
[1mStep[0m  [12/42], [94mLoss[0m : 1.92181
[1mStep[0m  [16/42], [94mLoss[0m : 1.73546
[1mStep[0m  [20/42], [94mLoss[0m : 1.88438
[1mStep[0m  [24/42], [94mLoss[0m : 1.89488
[1mStep[0m  [28/42], [94mLoss[0m : 1.75405
[1mStep[0m  [32/42], [94mLoss[0m : 1.84513
[1mStep[0m  [36/42], [94mLoss[0m : 1.81994
[1mStep[0m  [40/42], [94mLoss[0m : 1.73264

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.532, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90136
[1mStep[0m  [4/42], [94mLoss[0m : 1.79014
[1mStep[0m  [8/42], [94mLoss[0m : 1.78952
[1mStep[0m  [12/42], [94mLoss[0m : 2.06614
[1mStep[0m  [16/42], [94mLoss[0m : 1.92062
[1mStep[0m  [20/42], [94mLoss[0m : 2.01528
[1mStep[0m  [24/42], [94mLoss[0m : 1.89093
[1mStep[0m  [28/42], [94mLoss[0m : 1.77634
[1mStep[0m  [32/42], [94mLoss[0m : 1.78709
[1mStep[0m  [36/42], [94mLoss[0m : 1.90088
[1mStep[0m  [40/42], [94mLoss[0m : 1.82523

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79964
[1mStep[0m  [4/42], [94mLoss[0m : 1.75355
[1mStep[0m  [8/42], [94mLoss[0m : 1.73755
[1mStep[0m  [12/42], [94mLoss[0m : 1.91602
[1mStep[0m  [16/42], [94mLoss[0m : 1.87208
[1mStep[0m  [20/42], [94mLoss[0m : 1.72277
[1mStep[0m  [24/42], [94mLoss[0m : 1.85026
[1mStep[0m  [28/42], [94mLoss[0m : 1.70964
[1mStep[0m  [32/42], [94mLoss[0m : 1.65351
[1mStep[0m  [36/42], [94mLoss[0m : 1.92073
[1mStep[0m  [40/42], [94mLoss[0m : 1.85898

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.488, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79242
[1mStep[0m  [4/42], [94mLoss[0m : 1.80708
[1mStep[0m  [8/42], [94mLoss[0m : 1.87632
[1mStep[0m  [12/42], [94mLoss[0m : 1.83257
[1mStep[0m  [16/42], [94mLoss[0m : 1.91959
[1mStep[0m  [20/42], [94mLoss[0m : 1.79589
[1mStep[0m  [24/42], [94mLoss[0m : 1.89641
[1mStep[0m  [28/42], [94mLoss[0m : 1.84791
[1mStep[0m  [32/42], [94mLoss[0m : 1.66575
[1mStep[0m  [36/42], [94mLoss[0m : 1.78529
[1mStep[0m  [40/42], [94mLoss[0m : 1.87786

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.507
====================================

Phase 2 - Evaluation MAE:  2.507340567452567
MAE score P1       2.347198
MAE score P2       2.507341
loss               1.785253
learning_rate          0.01
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay         0.0001
Name: 22, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 11.16185
[1mStep[0m  [2/21], [94mLoss[0m : 11.06857
[1mStep[0m  [4/21], [94mLoss[0m : 10.67661
[1mStep[0m  [6/21], [94mLoss[0m : 10.98086
[1mStep[0m  [8/21], [94mLoss[0m : 10.41466
[1mStep[0m  [10/21], [94mLoss[0m : 10.59330
[1mStep[0m  [12/21], [94mLoss[0m : 9.99251
[1mStep[0m  [14/21], [94mLoss[0m : 10.02880
[1mStep[0m  [16/21], [94mLoss[0m : 9.80455
[1mStep[0m  [18/21], [94mLoss[0m : 9.81150
[1mStep[0m  [20/21], [94mLoss[0m : 9.59787

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.374, [92mTest[0m: 10.917, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.48162
[1mStep[0m  [2/21], [94mLoss[0m : 9.14586
[1mStep[0m  [4/21], [94mLoss[0m : 9.05339
[1mStep[0m  [6/21], [94mLoss[0m : 8.77096
[1mStep[0m  [8/21], [94mLoss[0m : 8.80329
[1mStep[0m  [10/21], [94mLoss[0m : 8.18039
[1mStep[0m  [12/21], [94mLoss[0m : 8.13861
[1mStep[0m  [14/21], [94mLoss[0m : 7.63764
[1mStep[0m  [16/21], [94mLoss[0m : 7.65256
[1mStep[0m  [18/21], [94mLoss[0m : 7.26230
[1mStep[0m  [20/21], [94mLoss[0m : 7.26471

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.327, [92mTest[0m: 9.942, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.40212
[1mStep[0m  [2/21], [94mLoss[0m : 6.96032
[1mStep[0m  [4/21], [94mLoss[0m : 6.66957
[1mStep[0m  [6/21], [94mLoss[0m : 6.75490
[1mStep[0m  [8/21], [94mLoss[0m : 6.47218
[1mStep[0m  [10/21], [94mLoss[0m : 6.09718
[1mStep[0m  [12/21], [94mLoss[0m : 5.64826
[1mStep[0m  [14/21], [94mLoss[0m : 5.80242
[1mStep[0m  [16/21], [94mLoss[0m : 5.40792
[1mStep[0m  [18/21], [94mLoss[0m : 5.22734
[1mStep[0m  [20/21], [94mLoss[0m : 5.36001

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.170, [92mTest[0m: 8.104, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.24024
[1mStep[0m  [2/21], [94mLoss[0m : 4.64716
[1mStep[0m  [4/21], [94mLoss[0m : 4.55143
[1mStep[0m  [6/21], [94mLoss[0m : 4.54183
[1mStep[0m  [8/21], [94mLoss[0m : 4.43618
[1mStep[0m  [10/21], [94mLoss[0m : 4.26350
[1mStep[0m  [12/21], [94mLoss[0m : 3.84158
[1mStep[0m  [14/21], [94mLoss[0m : 3.78860
[1mStep[0m  [16/21], [94mLoss[0m : 3.88586
[1mStep[0m  [18/21], [94mLoss[0m : 3.77938
[1mStep[0m  [20/21], [94mLoss[0m : 3.48802

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.249, [92mTest[0m: 6.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.43407
[1mStep[0m  [2/21], [94mLoss[0m : 3.52888
[1mStep[0m  [4/21], [94mLoss[0m : 3.32603
[1mStep[0m  [6/21], [94mLoss[0m : 3.31435
[1mStep[0m  [8/21], [94mLoss[0m : 3.19642
[1mStep[0m  [10/21], [94mLoss[0m : 2.92371
[1mStep[0m  [12/21], [94mLoss[0m : 3.36248
[1mStep[0m  [14/21], [94mLoss[0m : 3.06805
[1mStep[0m  [16/21], [94mLoss[0m : 3.03273
[1mStep[0m  [18/21], [94mLoss[0m : 2.92188
[1mStep[0m  [20/21], [94mLoss[0m : 2.95053

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.187, [92mTest[0m: 4.528, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.96056
[1mStep[0m  [2/21], [94mLoss[0m : 2.84561
[1mStep[0m  [4/21], [94mLoss[0m : 2.82091
[1mStep[0m  [6/21], [94mLoss[0m : 2.74702
[1mStep[0m  [8/21], [94mLoss[0m : 2.97225
[1mStep[0m  [10/21], [94mLoss[0m : 2.80746
[1mStep[0m  [12/21], [94mLoss[0m : 2.94443
[1mStep[0m  [14/21], [94mLoss[0m : 2.87170
[1mStep[0m  [16/21], [94mLoss[0m : 2.82368
[1mStep[0m  [18/21], [94mLoss[0m : 2.89503
[1mStep[0m  [20/21], [94mLoss[0m : 2.72211

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.820, [92mTest[0m: 3.109, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68335
[1mStep[0m  [2/21], [94mLoss[0m : 2.88465
[1mStep[0m  [4/21], [94mLoss[0m : 2.78795
[1mStep[0m  [6/21], [94mLoss[0m : 2.86489
[1mStep[0m  [8/21], [94mLoss[0m : 2.86996
[1mStep[0m  [10/21], [94mLoss[0m : 2.64220
[1mStep[0m  [12/21], [94mLoss[0m : 2.75942
[1mStep[0m  [14/21], [94mLoss[0m : 2.66054
[1mStep[0m  [16/21], [94mLoss[0m : 2.50279
[1mStep[0m  [18/21], [94mLoss[0m : 2.68944
[1mStep[0m  [20/21], [94mLoss[0m : 2.88421

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.784, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56781
[1mStep[0m  [2/21], [94mLoss[0m : 2.66492
[1mStep[0m  [4/21], [94mLoss[0m : 2.60001
[1mStep[0m  [6/21], [94mLoss[0m : 2.71347
[1mStep[0m  [8/21], [94mLoss[0m : 2.58473
[1mStep[0m  [10/21], [94mLoss[0m : 2.92140
[1mStep[0m  [12/21], [94mLoss[0m : 2.63663
[1mStep[0m  [14/21], [94mLoss[0m : 2.69888
[1mStep[0m  [16/21], [94mLoss[0m : 2.65465
[1mStep[0m  [18/21], [94mLoss[0m : 2.54315
[1mStep[0m  [20/21], [94mLoss[0m : 2.73051

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.566, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75934
[1mStep[0m  [2/21], [94mLoss[0m : 2.61456
[1mStep[0m  [4/21], [94mLoss[0m : 2.66787
[1mStep[0m  [6/21], [94mLoss[0m : 2.78168
[1mStep[0m  [8/21], [94mLoss[0m : 2.62862
[1mStep[0m  [10/21], [94mLoss[0m : 2.61331
[1mStep[0m  [12/21], [94mLoss[0m : 2.60949
[1mStep[0m  [14/21], [94mLoss[0m : 2.69154
[1mStep[0m  [16/21], [94mLoss[0m : 2.58681
[1mStep[0m  [18/21], [94mLoss[0m : 2.69839
[1mStep[0m  [20/21], [94mLoss[0m : 2.75789

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.520, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62095
[1mStep[0m  [2/21], [94mLoss[0m : 2.70940
[1mStep[0m  [4/21], [94mLoss[0m : 2.65410
[1mStep[0m  [6/21], [94mLoss[0m : 2.69187
[1mStep[0m  [8/21], [94mLoss[0m : 2.62868
[1mStep[0m  [10/21], [94mLoss[0m : 2.52793
[1mStep[0m  [12/21], [94mLoss[0m : 2.67265
[1mStep[0m  [14/21], [94mLoss[0m : 2.65481
[1mStep[0m  [16/21], [94mLoss[0m : 2.67234
[1mStep[0m  [18/21], [94mLoss[0m : 2.56811
[1mStep[0m  [20/21], [94mLoss[0m : 2.70894

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52672
[1mStep[0m  [2/21], [94mLoss[0m : 2.63682
[1mStep[0m  [4/21], [94mLoss[0m : 2.47798
[1mStep[0m  [6/21], [94mLoss[0m : 2.53452
[1mStep[0m  [8/21], [94mLoss[0m : 2.66967
[1mStep[0m  [10/21], [94mLoss[0m : 2.59050
[1mStep[0m  [12/21], [94mLoss[0m : 2.71528
[1mStep[0m  [14/21], [94mLoss[0m : 2.63229
[1mStep[0m  [16/21], [94mLoss[0m : 2.57141
[1mStep[0m  [18/21], [94mLoss[0m : 2.46660
[1mStep[0m  [20/21], [94mLoss[0m : 2.70351

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71143
[1mStep[0m  [2/21], [94mLoss[0m : 2.62824
[1mStep[0m  [4/21], [94mLoss[0m : 2.45859
[1mStep[0m  [6/21], [94mLoss[0m : 2.59140
[1mStep[0m  [8/21], [94mLoss[0m : 2.61182
[1mStep[0m  [10/21], [94mLoss[0m : 2.60636
[1mStep[0m  [12/21], [94mLoss[0m : 2.50564
[1mStep[0m  [14/21], [94mLoss[0m : 2.73863
[1mStep[0m  [16/21], [94mLoss[0m : 2.57653
[1mStep[0m  [18/21], [94mLoss[0m : 2.65706
[1mStep[0m  [20/21], [94mLoss[0m : 2.54716

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43418
[1mStep[0m  [2/21], [94mLoss[0m : 2.57444
[1mStep[0m  [4/21], [94mLoss[0m : 2.54224
[1mStep[0m  [6/21], [94mLoss[0m : 2.48620
[1mStep[0m  [8/21], [94mLoss[0m : 2.57244
[1mStep[0m  [10/21], [94mLoss[0m : 2.44332
[1mStep[0m  [12/21], [94mLoss[0m : 2.72402
[1mStep[0m  [14/21], [94mLoss[0m : 2.65741
[1mStep[0m  [16/21], [94mLoss[0m : 2.60181
[1mStep[0m  [18/21], [94mLoss[0m : 2.61338
[1mStep[0m  [20/21], [94mLoss[0m : 2.58103

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56089
[1mStep[0m  [2/21], [94mLoss[0m : 2.51363
[1mStep[0m  [4/21], [94mLoss[0m : 2.63212
[1mStep[0m  [6/21], [94mLoss[0m : 2.64164
[1mStep[0m  [8/21], [94mLoss[0m : 2.62833
[1mStep[0m  [10/21], [94mLoss[0m : 2.60537
[1mStep[0m  [12/21], [94mLoss[0m : 2.49031
[1mStep[0m  [14/21], [94mLoss[0m : 2.59121
[1mStep[0m  [16/21], [94mLoss[0m : 2.60164
[1mStep[0m  [18/21], [94mLoss[0m : 2.48658
[1mStep[0m  [20/21], [94mLoss[0m : 2.57913

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70382
[1mStep[0m  [2/21], [94mLoss[0m : 2.56749
[1mStep[0m  [4/21], [94mLoss[0m : 2.49005
[1mStep[0m  [6/21], [94mLoss[0m : 2.63077
[1mStep[0m  [8/21], [94mLoss[0m : 2.49974
[1mStep[0m  [10/21], [94mLoss[0m : 2.47210
[1mStep[0m  [12/21], [94mLoss[0m : 2.62578
[1mStep[0m  [14/21], [94mLoss[0m : 2.70982
[1mStep[0m  [16/21], [94mLoss[0m : 2.52680
[1mStep[0m  [18/21], [94mLoss[0m : 2.57901
[1mStep[0m  [20/21], [94mLoss[0m : 2.43245

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74994
[1mStep[0m  [2/21], [94mLoss[0m : 2.44127
[1mStep[0m  [4/21], [94mLoss[0m : 2.41903
[1mStep[0m  [6/21], [94mLoss[0m : 2.53651
[1mStep[0m  [8/21], [94mLoss[0m : 2.41081
[1mStep[0m  [10/21], [94mLoss[0m : 2.43719
[1mStep[0m  [12/21], [94mLoss[0m : 2.61987
[1mStep[0m  [14/21], [94mLoss[0m : 2.68567
[1mStep[0m  [16/21], [94mLoss[0m : 2.54946
[1mStep[0m  [18/21], [94mLoss[0m : 2.49766
[1mStep[0m  [20/21], [94mLoss[0m : 2.39532

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57551
[1mStep[0m  [2/21], [94mLoss[0m : 2.49659
[1mStep[0m  [4/21], [94mLoss[0m : 2.53085
[1mStep[0m  [6/21], [94mLoss[0m : 2.49210
[1mStep[0m  [8/21], [94mLoss[0m : 2.67402
[1mStep[0m  [10/21], [94mLoss[0m : 2.79088
[1mStep[0m  [12/21], [94mLoss[0m : 2.58240
[1mStep[0m  [14/21], [94mLoss[0m : 2.46732
[1mStep[0m  [16/21], [94mLoss[0m : 2.41466
[1mStep[0m  [18/21], [94mLoss[0m : 2.44655
[1mStep[0m  [20/21], [94mLoss[0m : 2.60887

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60609
[1mStep[0m  [2/21], [94mLoss[0m : 2.58946
[1mStep[0m  [4/21], [94mLoss[0m : 2.50057
[1mStep[0m  [6/21], [94mLoss[0m : 2.44225
[1mStep[0m  [8/21], [94mLoss[0m : 2.50422
[1mStep[0m  [10/21], [94mLoss[0m : 2.61777
[1mStep[0m  [12/21], [94mLoss[0m : 2.43406
[1mStep[0m  [14/21], [94mLoss[0m : 2.73025
[1mStep[0m  [16/21], [94mLoss[0m : 2.36977
[1mStep[0m  [18/21], [94mLoss[0m : 2.60121
[1mStep[0m  [20/21], [94mLoss[0m : 2.38265

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67408
[1mStep[0m  [2/21], [94mLoss[0m : 2.60648
[1mStep[0m  [4/21], [94mLoss[0m : 2.58091
[1mStep[0m  [6/21], [94mLoss[0m : 2.49342
[1mStep[0m  [8/21], [94mLoss[0m : 2.64273
[1mStep[0m  [10/21], [94mLoss[0m : 2.42926
[1mStep[0m  [12/21], [94mLoss[0m : 2.45145
[1mStep[0m  [14/21], [94mLoss[0m : 2.47509
[1mStep[0m  [16/21], [94mLoss[0m : 2.61725
[1mStep[0m  [18/21], [94mLoss[0m : 2.59858
[1mStep[0m  [20/21], [94mLoss[0m : 2.54501

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65731
[1mStep[0m  [2/21], [94mLoss[0m : 2.64109
[1mStep[0m  [4/21], [94mLoss[0m : 2.38824
[1mStep[0m  [6/21], [94mLoss[0m : 2.61217
[1mStep[0m  [8/21], [94mLoss[0m : 2.71801
[1mStep[0m  [10/21], [94mLoss[0m : 2.38920
[1mStep[0m  [12/21], [94mLoss[0m : 2.67269
[1mStep[0m  [14/21], [94mLoss[0m : 2.41159
[1mStep[0m  [16/21], [94mLoss[0m : 2.80160
[1mStep[0m  [18/21], [94mLoss[0m : 2.55872
[1mStep[0m  [20/21], [94mLoss[0m : 2.60580

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53174
[1mStep[0m  [2/21], [94mLoss[0m : 2.50872
[1mStep[0m  [4/21], [94mLoss[0m : 2.61244
[1mStep[0m  [6/21], [94mLoss[0m : 2.48695
[1mStep[0m  [8/21], [94mLoss[0m : 2.71628
[1mStep[0m  [10/21], [94mLoss[0m : 2.47035
[1mStep[0m  [12/21], [94mLoss[0m : 2.37362
[1mStep[0m  [14/21], [94mLoss[0m : 2.59888
[1mStep[0m  [16/21], [94mLoss[0m : 2.57116
[1mStep[0m  [18/21], [94mLoss[0m : 2.45535
[1mStep[0m  [20/21], [94mLoss[0m : 2.52400

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.345, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58447
[1mStep[0m  [2/21], [94mLoss[0m : 2.53234
[1mStep[0m  [4/21], [94mLoss[0m : 2.57724
[1mStep[0m  [6/21], [94mLoss[0m : 2.49868
[1mStep[0m  [8/21], [94mLoss[0m : 2.50155
[1mStep[0m  [10/21], [94mLoss[0m : 2.49367
[1mStep[0m  [12/21], [94mLoss[0m : 2.43394
[1mStep[0m  [14/21], [94mLoss[0m : 2.54367
[1mStep[0m  [16/21], [94mLoss[0m : 2.58280
[1mStep[0m  [18/21], [94mLoss[0m : 2.54403
[1mStep[0m  [20/21], [94mLoss[0m : 2.52805

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54816
[1mStep[0m  [2/21], [94mLoss[0m : 2.49500
[1mStep[0m  [4/21], [94mLoss[0m : 2.68267
[1mStep[0m  [6/21], [94mLoss[0m : 2.58835
[1mStep[0m  [8/21], [94mLoss[0m : 2.53477
[1mStep[0m  [10/21], [94mLoss[0m : 2.49697
[1mStep[0m  [12/21], [94mLoss[0m : 2.51159
[1mStep[0m  [14/21], [94mLoss[0m : 2.38143
[1mStep[0m  [16/21], [94mLoss[0m : 2.59983
[1mStep[0m  [18/21], [94mLoss[0m : 2.53418
[1mStep[0m  [20/21], [94mLoss[0m : 2.72798

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.359, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57062
[1mStep[0m  [2/21], [94mLoss[0m : 2.55610
[1mStep[0m  [4/21], [94mLoss[0m : 2.44936
[1mStep[0m  [6/21], [94mLoss[0m : 2.47426
[1mStep[0m  [8/21], [94mLoss[0m : 2.59665
[1mStep[0m  [10/21], [94mLoss[0m : 2.62496
[1mStep[0m  [12/21], [94mLoss[0m : 2.51738
[1mStep[0m  [14/21], [94mLoss[0m : 2.37577
[1mStep[0m  [16/21], [94mLoss[0m : 2.58423
[1mStep[0m  [18/21], [94mLoss[0m : 2.47406
[1mStep[0m  [20/21], [94mLoss[0m : 2.45323

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46544
[1mStep[0m  [2/21], [94mLoss[0m : 2.42774
[1mStep[0m  [4/21], [94mLoss[0m : 2.56899
[1mStep[0m  [6/21], [94mLoss[0m : 2.58399
[1mStep[0m  [8/21], [94mLoss[0m : 2.45385
[1mStep[0m  [10/21], [94mLoss[0m : 2.53793
[1mStep[0m  [12/21], [94mLoss[0m : 2.49894
[1mStep[0m  [14/21], [94mLoss[0m : 2.46662
[1mStep[0m  [16/21], [94mLoss[0m : 2.63343
[1mStep[0m  [18/21], [94mLoss[0m : 2.55066
[1mStep[0m  [20/21], [94mLoss[0m : 2.43375

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45855
[1mStep[0m  [2/21], [94mLoss[0m : 2.35243
[1mStep[0m  [4/21], [94mLoss[0m : 2.42508
[1mStep[0m  [6/21], [94mLoss[0m : 2.56806
[1mStep[0m  [8/21], [94mLoss[0m : 2.40648
[1mStep[0m  [10/21], [94mLoss[0m : 2.68680
[1mStep[0m  [12/21], [94mLoss[0m : 2.60906
[1mStep[0m  [14/21], [94mLoss[0m : 2.51253
[1mStep[0m  [16/21], [94mLoss[0m : 2.38118
[1mStep[0m  [18/21], [94mLoss[0m : 2.46585
[1mStep[0m  [20/21], [94mLoss[0m : 2.54589

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45505
[1mStep[0m  [2/21], [94mLoss[0m : 2.46203
[1mStep[0m  [4/21], [94mLoss[0m : 2.61455
[1mStep[0m  [6/21], [94mLoss[0m : 2.46955
[1mStep[0m  [8/21], [94mLoss[0m : 2.48802
[1mStep[0m  [10/21], [94mLoss[0m : 2.33412
[1mStep[0m  [12/21], [94mLoss[0m : 2.49439
[1mStep[0m  [14/21], [94mLoss[0m : 2.46131
[1mStep[0m  [16/21], [94mLoss[0m : 2.73418
[1mStep[0m  [18/21], [94mLoss[0m : 2.48630
[1mStep[0m  [20/21], [94mLoss[0m : 2.42627

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56579
[1mStep[0m  [2/21], [94mLoss[0m : 2.48915
[1mStep[0m  [4/21], [94mLoss[0m : 2.45327
[1mStep[0m  [6/21], [94mLoss[0m : 2.57483
[1mStep[0m  [8/21], [94mLoss[0m : 2.36839
[1mStep[0m  [10/21], [94mLoss[0m : 2.48424
[1mStep[0m  [12/21], [94mLoss[0m : 2.50321
[1mStep[0m  [14/21], [94mLoss[0m : 2.61991
[1mStep[0m  [16/21], [94mLoss[0m : 2.46509
[1mStep[0m  [18/21], [94mLoss[0m : 2.51722
[1mStep[0m  [20/21], [94mLoss[0m : 2.49110

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43325
[1mStep[0m  [2/21], [94mLoss[0m : 2.44277
[1mStep[0m  [4/21], [94mLoss[0m : 2.48816
[1mStep[0m  [6/21], [94mLoss[0m : 2.50615
[1mStep[0m  [8/21], [94mLoss[0m : 2.43609
[1mStep[0m  [10/21], [94mLoss[0m : 2.65757
[1mStep[0m  [12/21], [94mLoss[0m : 2.64495
[1mStep[0m  [14/21], [94mLoss[0m : 2.36419
[1mStep[0m  [16/21], [94mLoss[0m : 2.55665
[1mStep[0m  [18/21], [94mLoss[0m : 2.41351
[1mStep[0m  [20/21], [94mLoss[0m : 2.30255

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.345, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51440
[1mStep[0m  [2/21], [94mLoss[0m : 2.57803
[1mStep[0m  [4/21], [94mLoss[0m : 2.48721
[1mStep[0m  [6/21], [94mLoss[0m : 2.38600
[1mStep[0m  [8/21], [94mLoss[0m : 2.39447
[1mStep[0m  [10/21], [94mLoss[0m : 2.59157
[1mStep[0m  [12/21], [94mLoss[0m : 2.53046
[1mStep[0m  [14/21], [94mLoss[0m : 2.53436
[1mStep[0m  [16/21], [94mLoss[0m : 2.50218
[1mStep[0m  [18/21], [94mLoss[0m : 2.40414
[1mStep[0m  [20/21], [94mLoss[0m : 2.56511

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.341
====================================

Phase 1 - Evaluation MAE:  2.341453858784267
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.44873
[1mStep[0m  [2/21], [94mLoss[0m : 2.50552
[1mStep[0m  [4/21], [94mLoss[0m : 2.67899
[1mStep[0m  [6/21], [94mLoss[0m : 2.50949
[1mStep[0m  [8/21], [94mLoss[0m : 2.40849
[1mStep[0m  [10/21], [94mLoss[0m : 2.37081
[1mStep[0m  [12/21], [94mLoss[0m : 2.68754
[1mStep[0m  [14/21], [94mLoss[0m : 2.53373
[1mStep[0m  [16/21], [94mLoss[0m : 2.59565
[1mStep[0m  [18/21], [94mLoss[0m : 2.64091
[1mStep[0m  [20/21], [94mLoss[0m : 2.57514

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48427
[1mStep[0m  [2/21], [94mLoss[0m : 2.70128
[1mStep[0m  [4/21], [94mLoss[0m : 2.52638
[1mStep[0m  [6/21], [94mLoss[0m : 2.43975
[1mStep[0m  [8/21], [94mLoss[0m : 2.60439
[1mStep[0m  [10/21], [94mLoss[0m : 2.62411
[1mStep[0m  [12/21], [94mLoss[0m : 2.59993
[1mStep[0m  [14/21], [94mLoss[0m : 2.57778
[1mStep[0m  [16/21], [94mLoss[0m : 2.79114
[1mStep[0m  [18/21], [94mLoss[0m : 2.63288
[1mStep[0m  [20/21], [94mLoss[0m : 2.50115

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.534, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44967
[1mStep[0m  [2/21], [94mLoss[0m : 2.67954
[1mStep[0m  [4/21], [94mLoss[0m : 2.59472
[1mStep[0m  [6/21], [94mLoss[0m : 2.57504
[1mStep[0m  [8/21], [94mLoss[0m : 2.48850
[1mStep[0m  [10/21], [94mLoss[0m : 2.45179
[1mStep[0m  [12/21], [94mLoss[0m : 2.57939
[1mStep[0m  [14/21], [94mLoss[0m : 2.42827
[1mStep[0m  [16/21], [94mLoss[0m : 2.53496
[1mStep[0m  [18/21], [94mLoss[0m : 2.50336
[1mStep[0m  [20/21], [94mLoss[0m : 2.48256

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60147
[1mStep[0m  [2/21], [94mLoss[0m : 2.44647
[1mStep[0m  [4/21], [94mLoss[0m : 2.59890
[1mStep[0m  [6/21], [94mLoss[0m : 2.66404
[1mStep[0m  [8/21], [94mLoss[0m : 2.60985
[1mStep[0m  [10/21], [94mLoss[0m : 2.52314
[1mStep[0m  [12/21], [94mLoss[0m : 2.62969
[1mStep[0m  [14/21], [94mLoss[0m : 2.64891
[1mStep[0m  [16/21], [94mLoss[0m : 2.59972
[1mStep[0m  [18/21], [94mLoss[0m : 2.40412
[1mStep[0m  [20/21], [94mLoss[0m : 2.46636

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53160
[1mStep[0m  [2/21], [94mLoss[0m : 2.48428
[1mStep[0m  [4/21], [94mLoss[0m : 2.47230
[1mStep[0m  [6/21], [94mLoss[0m : 2.55534
[1mStep[0m  [8/21], [94mLoss[0m : 2.46900
[1mStep[0m  [10/21], [94mLoss[0m : 2.65220
[1mStep[0m  [12/21], [94mLoss[0m : 2.52997
[1mStep[0m  [14/21], [94mLoss[0m : 2.50745
[1mStep[0m  [16/21], [94mLoss[0m : 2.41210
[1mStep[0m  [18/21], [94mLoss[0m : 2.35486
[1mStep[0m  [20/21], [94mLoss[0m : 2.38212

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49729
[1mStep[0m  [2/21], [94mLoss[0m : 2.52250
[1mStep[0m  [4/21], [94mLoss[0m : 2.54591
[1mStep[0m  [6/21], [94mLoss[0m : 2.51816
[1mStep[0m  [8/21], [94mLoss[0m : 2.64700
[1mStep[0m  [10/21], [94mLoss[0m : 2.57841
[1mStep[0m  [12/21], [94mLoss[0m : 2.43775
[1mStep[0m  [14/21], [94mLoss[0m : 2.50916
[1mStep[0m  [16/21], [94mLoss[0m : 2.36890
[1mStep[0m  [18/21], [94mLoss[0m : 2.43403
[1mStep[0m  [20/21], [94mLoss[0m : 2.50735

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39159
[1mStep[0m  [2/21], [94mLoss[0m : 2.39234
[1mStep[0m  [4/21], [94mLoss[0m : 2.51733
[1mStep[0m  [6/21], [94mLoss[0m : 2.50780
[1mStep[0m  [8/21], [94mLoss[0m : 2.60617
[1mStep[0m  [10/21], [94mLoss[0m : 2.41782
[1mStep[0m  [12/21], [94mLoss[0m : 2.51422
[1mStep[0m  [14/21], [94mLoss[0m : 2.45189
[1mStep[0m  [16/21], [94mLoss[0m : 2.53940
[1mStep[0m  [18/21], [94mLoss[0m : 2.44017
[1mStep[0m  [20/21], [94mLoss[0m : 2.47240

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.557, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44143
[1mStep[0m  [2/21], [94mLoss[0m : 2.43874
[1mStep[0m  [4/21], [94mLoss[0m : 2.54775
[1mStep[0m  [6/21], [94mLoss[0m : 2.49655
[1mStep[0m  [8/21], [94mLoss[0m : 2.31775
[1mStep[0m  [10/21], [94mLoss[0m : 2.49578
[1mStep[0m  [12/21], [94mLoss[0m : 2.59052
[1mStep[0m  [14/21], [94mLoss[0m : 2.41521
[1mStep[0m  [16/21], [94mLoss[0m : 2.38183
[1mStep[0m  [18/21], [94mLoss[0m : 2.40318
[1mStep[0m  [20/21], [94mLoss[0m : 2.53030

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.564, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69077
[1mStep[0m  [2/21], [94mLoss[0m : 2.28206
[1mStep[0m  [4/21], [94mLoss[0m : 2.38650
[1mStep[0m  [6/21], [94mLoss[0m : 2.35521
[1mStep[0m  [8/21], [94mLoss[0m : 2.46338
[1mStep[0m  [10/21], [94mLoss[0m : 2.26366
[1mStep[0m  [12/21], [94mLoss[0m : 2.46404
[1mStep[0m  [14/21], [94mLoss[0m : 2.52993
[1mStep[0m  [16/21], [94mLoss[0m : 2.43605
[1mStep[0m  [18/21], [94mLoss[0m : 2.56697
[1mStep[0m  [20/21], [94mLoss[0m : 2.31082

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38154
[1mStep[0m  [2/21], [94mLoss[0m : 2.30123
[1mStep[0m  [4/21], [94mLoss[0m : 2.46323
[1mStep[0m  [6/21], [94mLoss[0m : 2.34979
[1mStep[0m  [8/21], [94mLoss[0m : 2.51397
[1mStep[0m  [10/21], [94mLoss[0m : 2.39064
[1mStep[0m  [12/21], [94mLoss[0m : 2.46775
[1mStep[0m  [14/21], [94mLoss[0m : 2.33961
[1mStep[0m  [16/21], [94mLoss[0m : 2.40777
[1mStep[0m  [18/21], [94mLoss[0m : 2.50130
[1mStep[0m  [20/21], [94mLoss[0m : 2.47834

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41324
[1mStep[0m  [2/21], [94mLoss[0m : 2.20813
[1mStep[0m  [4/21], [94mLoss[0m : 2.56044
[1mStep[0m  [6/21], [94mLoss[0m : 2.42485
[1mStep[0m  [8/21], [94mLoss[0m : 2.42284
[1mStep[0m  [10/21], [94mLoss[0m : 2.32319
[1mStep[0m  [12/21], [94mLoss[0m : 2.48685
[1mStep[0m  [14/21], [94mLoss[0m : 2.43489
[1mStep[0m  [16/21], [94mLoss[0m : 2.51402
[1mStep[0m  [18/21], [94mLoss[0m : 2.39289
[1mStep[0m  [20/21], [94mLoss[0m : 2.33277

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.634, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41712
[1mStep[0m  [2/21], [94mLoss[0m : 2.44925
[1mStep[0m  [4/21], [94mLoss[0m : 2.41052
[1mStep[0m  [6/21], [94mLoss[0m : 2.50210
[1mStep[0m  [8/21], [94mLoss[0m : 2.40348
[1mStep[0m  [10/21], [94mLoss[0m : 2.31411
[1mStep[0m  [12/21], [94mLoss[0m : 2.34829
[1mStep[0m  [14/21], [94mLoss[0m : 2.30141
[1mStep[0m  [16/21], [94mLoss[0m : 2.37980
[1mStep[0m  [18/21], [94mLoss[0m : 2.35135
[1mStep[0m  [20/21], [94mLoss[0m : 2.35838

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39831
[1mStep[0m  [2/21], [94mLoss[0m : 2.43148
[1mStep[0m  [4/21], [94mLoss[0m : 2.39667
[1mStep[0m  [6/21], [94mLoss[0m : 2.32980
[1mStep[0m  [8/21], [94mLoss[0m : 2.21319
[1mStep[0m  [10/21], [94mLoss[0m : 2.22308
[1mStep[0m  [12/21], [94mLoss[0m : 2.22754
[1mStep[0m  [14/21], [94mLoss[0m : 2.35034
[1mStep[0m  [16/21], [94mLoss[0m : 2.39152
[1mStep[0m  [18/21], [94mLoss[0m : 2.23776
[1mStep[0m  [20/21], [94mLoss[0m : 2.36608

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.645, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32446
[1mStep[0m  [2/21], [94mLoss[0m : 2.33981
[1mStep[0m  [4/21], [94mLoss[0m : 2.31234
[1mStep[0m  [6/21], [94mLoss[0m : 2.32305
[1mStep[0m  [8/21], [94mLoss[0m : 2.22540
[1mStep[0m  [10/21], [94mLoss[0m : 2.18620
[1mStep[0m  [12/21], [94mLoss[0m : 2.24067
[1mStep[0m  [14/21], [94mLoss[0m : 2.34603
[1mStep[0m  [16/21], [94mLoss[0m : 2.25501
[1mStep[0m  [18/21], [94mLoss[0m : 2.30009
[1mStep[0m  [20/21], [94mLoss[0m : 2.31177

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.532, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36119
[1mStep[0m  [2/21], [94mLoss[0m : 2.33870
[1mStep[0m  [4/21], [94mLoss[0m : 2.30708
[1mStep[0m  [6/21], [94mLoss[0m : 2.21682
[1mStep[0m  [8/21], [94mLoss[0m : 2.19837
[1mStep[0m  [10/21], [94mLoss[0m : 2.31758
[1mStep[0m  [12/21], [94mLoss[0m : 2.33926
[1mStep[0m  [14/21], [94mLoss[0m : 2.35230
[1mStep[0m  [16/21], [94mLoss[0m : 2.34048
[1mStep[0m  [18/21], [94mLoss[0m : 2.16507
[1mStep[0m  [20/21], [94mLoss[0m : 2.18620

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.520, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16887
[1mStep[0m  [2/21], [94mLoss[0m : 2.24407
[1mStep[0m  [4/21], [94mLoss[0m : 2.33347
[1mStep[0m  [6/21], [94mLoss[0m : 2.15280
[1mStep[0m  [8/21], [94mLoss[0m : 2.30217
[1mStep[0m  [10/21], [94mLoss[0m : 2.17062
[1mStep[0m  [12/21], [94mLoss[0m : 2.23857
[1mStep[0m  [14/21], [94mLoss[0m : 2.31265
[1mStep[0m  [16/21], [94mLoss[0m : 2.25628
[1mStep[0m  [18/21], [94mLoss[0m : 2.20547
[1mStep[0m  [20/21], [94mLoss[0m : 2.31566

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31266
[1mStep[0m  [2/21], [94mLoss[0m : 2.14843
[1mStep[0m  [4/21], [94mLoss[0m : 2.12374
[1mStep[0m  [6/21], [94mLoss[0m : 2.10063
[1mStep[0m  [8/21], [94mLoss[0m : 2.23684
[1mStep[0m  [10/21], [94mLoss[0m : 2.20968
[1mStep[0m  [12/21], [94mLoss[0m : 2.22070
[1mStep[0m  [14/21], [94mLoss[0m : 2.31569
[1mStep[0m  [16/21], [94mLoss[0m : 2.26405
[1mStep[0m  [18/21], [94mLoss[0m : 2.17785
[1mStep[0m  [20/21], [94mLoss[0m : 2.42510

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18456
[1mStep[0m  [2/21], [94mLoss[0m : 2.22292
[1mStep[0m  [4/21], [94mLoss[0m : 2.22220
[1mStep[0m  [6/21], [94mLoss[0m : 2.16188
[1mStep[0m  [8/21], [94mLoss[0m : 2.22491
[1mStep[0m  [10/21], [94mLoss[0m : 2.13801
[1mStep[0m  [12/21], [94mLoss[0m : 2.29324
[1mStep[0m  [14/21], [94mLoss[0m : 2.25390
[1mStep[0m  [16/21], [94mLoss[0m : 2.10306
[1mStep[0m  [18/21], [94mLoss[0m : 2.22030
[1mStep[0m  [20/21], [94mLoss[0m : 2.19101

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.209, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18301
[1mStep[0m  [2/21], [94mLoss[0m : 2.21473
[1mStep[0m  [4/21], [94mLoss[0m : 2.21675
[1mStep[0m  [6/21], [94mLoss[0m : 2.22776
[1mStep[0m  [8/21], [94mLoss[0m : 2.13507
[1mStep[0m  [10/21], [94mLoss[0m : 2.19052
[1mStep[0m  [12/21], [94mLoss[0m : 2.32157
[1mStep[0m  [14/21], [94mLoss[0m : 2.18600
[1mStep[0m  [16/21], [94mLoss[0m : 2.29898
[1mStep[0m  [18/21], [94mLoss[0m : 2.14140
[1mStep[0m  [20/21], [94mLoss[0m : 2.22262

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.203, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11570
[1mStep[0m  [2/21], [94mLoss[0m : 2.12766
[1mStep[0m  [4/21], [94mLoss[0m : 2.12519
[1mStep[0m  [6/21], [94mLoss[0m : 2.08657
[1mStep[0m  [8/21], [94mLoss[0m : 2.03986
[1mStep[0m  [10/21], [94mLoss[0m : 2.10864
[1mStep[0m  [12/21], [94mLoss[0m : 2.14960
[1mStep[0m  [14/21], [94mLoss[0m : 2.11655
[1mStep[0m  [16/21], [94mLoss[0m : 2.30971
[1mStep[0m  [18/21], [94mLoss[0m : 2.36468
[1mStep[0m  [20/21], [94mLoss[0m : 2.38943

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01187
[1mStep[0m  [2/21], [94mLoss[0m : 2.19410
[1mStep[0m  [4/21], [94mLoss[0m : 2.08300
[1mStep[0m  [6/21], [94mLoss[0m : 2.34101
[1mStep[0m  [8/21], [94mLoss[0m : 2.03817
[1mStep[0m  [10/21], [94mLoss[0m : 2.16151
[1mStep[0m  [12/21], [94mLoss[0m : 2.19356
[1mStep[0m  [14/21], [94mLoss[0m : 2.08427
[1mStep[0m  [16/21], [94mLoss[0m : 2.14821
[1mStep[0m  [18/21], [94mLoss[0m : 2.18434
[1mStep[0m  [20/21], [94mLoss[0m : 2.24253

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17165
[1mStep[0m  [2/21], [94mLoss[0m : 2.18663
[1mStep[0m  [4/21], [94mLoss[0m : 2.09893
[1mStep[0m  [6/21], [94mLoss[0m : 2.11303
[1mStep[0m  [8/21], [94mLoss[0m : 2.00917
[1mStep[0m  [10/21], [94mLoss[0m : 2.03756
[1mStep[0m  [12/21], [94mLoss[0m : 2.05742
[1mStep[0m  [14/21], [94mLoss[0m : 2.10498
[1mStep[0m  [16/21], [94mLoss[0m : 2.06265
[1mStep[0m  [18/21], [94mLoss[0m : 2.09492
[1mStep[0m  [20/21], [94mLoss[0m : 2.14911

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01202
[1mStep[0m  [2/21], [94mLoss[0m : 2.14829
[1mStep[0m  [4/21], [94mLoss[0m : 2.10933
[1mStep[0m  [6/21], [94mLoss[0m : 2.19930
[1mStep[0m  [8/21], [94mLoss[0m : 2.23856
[1mStep[0m  [10/21], [94mLoss[0m : 1.97128
[1mStep[0m  [12/21], [94mLoss[0m : 2.12500
[1mStep[0m  [14/21], [94mLoss[0m : 2.08535
[1mStep[0m  [16/21], [94mLoss[0m : 2.08039
[1mStep[0m  [18/21], [94mLoss[0m : 1.99579
[1mStep[0m  [20/21], [94mLoss[0m : 2.14207

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.450, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18432
[1mStep[0m  [2/21], [94mLoss[0m : 2.08438
[1mStep[0m  [4/21], [94mLoss[0m : 2.07976
[1mStep[0m  [6/21], [94mLoss[0m : 1.97266
[1mStep[0m  [8/21], [94mLoss[0m : 1.94610
[1mStep[0m  [10/21], [94mLoss[0m : 2.10964
[1mStep[0m  [12/21], [94mLoss[0m : 2.06187
[1mStep[0m  [14/21], [94mLoss[0m : 2.07018
[1mStep[0m  [16/21], [94mLoss[0m : 2.21392
[1mStep[0m  [18/21], [94mLoss[0m : 2.25087
[1mStep[0m  [20/21], [94mLoss[0m : 1.97489

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.05535
[1mStep[0m  [2/21], [94mLoss[0m : 2.12480
[1mStep[0m  [4/21], [94mLoss[0m : 2.12032
[1mStep[0m  [6/21], [94mLoss[0m : 2.04133
[1mStep[0m  [8/21], [94mLoss[0m : 2.00508
[1mStep[0m  [10/21], [94mLoss[0m : 2.27765
[1mStep[0m  [12/21], [94mLoss[0m : 2.11172
[1mStep[0m  [14/21], [94mLoss[0m : 2.07853
[1mStep[0m  [16/21], [94mLoss[0m : 2.17091
[1mStep[0m  [18/21], [94mLoss[0m : 2.27699
[1mStep[0m  [20/21], [94mLoss[0m : 2.06315

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.073, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.03604
[1mStep[0m  [2/21], [94mLoss[0m : 2.05450
[1mStep[0m  [4/21], [94mLoss[0m : 2.16101
[1mStep[0m  [6/21], [94mLoss[0m : 2.11813
[1mStep[0m  [8/21], [94mLoss[0m : 1.96347
[1mStep[0m  [10/21], [94mLoss[0m : 2.09268
[1mStep[0m  [12/21], [94mLoss[0m : 2.00683
[1mStep[0m  [14/21], [94mLoss[0m : 1.87118
[1mStep[0m  [16/21], [94mLoss[0m : 1.83053
[1mStep[0m  [18/21], [94mLoss[0m : 2.11452
[1mStep[0m  [20/21], [94mLoss[0m : 2.06548

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.523, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99954
[1mStep[0m  [2/21], [94mLoss[0m : 2.02784
[1mStep[0m  [4/21], [94mLoss[0m : 1.89341
[1mStep[0m  [6/21], [94mLoss[0m : 1.94159
[1mStep[0m  [8/21], [94mLoss[0m : 1.97540
[1mStep[0m  [10/21], [94mLoss[0m : 1.94443
[1mStep[0m  [12/21], [94mLoss[0m : 2.04171
[1mStep[0m  [14/21], [94mLoss[0m : 2.08252
[1mStep[0m  [16/21], [94mLoss[0m : 1.99624
[1mStep[0m  [18/21], [94mLoss[0m : 2.17740
[1mStep[0m  [20/21], [94mLoss[0m : 2.02205

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84173
[1mStep[0m  [2/21], [94mLoss[0m : 1.90920
[1mStep[0m  [4/21], [94mLoss[0m : 2.05309
[1mStep[0m  [6/21], [94mLoss[0m : 1.99954
[1mStep[0m  [8/21], [94mLoss[0m : 2.09239
[1mStep[0m  [10/21], [94mLoss[0m : 2.07914
[1mStep[0m  [12/21], [94mLoss[0m : 2.01188
[1mStep[0m  [14/21], [94mLoss[0m : 2.00033
[1mStep[0m  [16/21], [94mLoss[0m : 2.08666
[1mStep[0m  [18/21], [94mLoss[0m : 2.09866
[1mStep[0m  [20/21], [94mLoss[0m : 2.02399

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95920
[1mStep[0m  [2/21], [94mLoss[0m : 1.97152
[1mStep[0m  [4/21], [94mLoss[0m : 2.01519
[1mStep[0m  [6/21], [94mLoss[0m : 1.96865
[1mStep[0m  [8/21], [94mLoss[0m : 2.00717
[1mStep[0m  [10/21], [94mLoss[0m : 1.99761
[1mStep[0m  [12/21], [94mLoss[0m : 2.20701
[1mStep[0m  [14/21], [94mLoss[0m : 2.00498
[1mStep[0m  [16/21], [94mLoss[0m : 2.12780
[1mStep[0m  [18/21], [94mLoss[0m : 1.92203
[1mStep[0m  [20/21], [94mLoss[0m : 2.05278

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.535, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86549
[1mStep[0m  [2/21], [94mLoss[0m : 1.82350
[1mStep[0m  [4/21], [94mLoss[0m : 1.95584
[1mStep[0m  [6/21], [94mLoss[0m : 1.86624
[1mStep[0m  [8/21], [94mLoss[0m : 1.87487
[1mStep[0m  [10/21], [94mLoss[0m : 1.98711
[1mStep[0m  [12/21], [94mLoss[0m : 1.97935
[1mStep[0m  [14/21], [94mLoss[0m : 1.94997
[1mStep[0m  [16/21], [94mLoss[0m : 1.98849
[1mStep[0m  [18/21], [94mLoss[0m : 1.98946
[1mStep[0m  [20/21], [94mLoss[0m : 2.10200

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.562, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.543
====================================

Phase 2 - Evaluation MAE:  2.543294940676008
MAE score P1       2.341454
MAE score P2       2.543295
loss               1.952078
learning_rate          0.01
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay          0.001
Name: 23, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.49516
[1mStep[0m  [2/21], [94mLoss[0m : 9.37418
[1mStep[0m  [4/21], [94mLoss[0m : 7.97208
[1mStep[0m  [6/21], [94mLoss[0m : 5.92469
[1mStep[0m  [8/21], [94mLoss[0m : 4.58723
[1mStep[0m  [10/21], [94mLoss[0m : 3.84482
[1mStep[0m  [12/21], [94mLoss[0m : 3.43604
[1mStep[0m  [14/21], [94mLoss[0m : 2.77376
[1mStep[0m  [16/21], [94mLoss[0m : 2.99905
[1mStep[0m  [18/21], [94mLoss[0m : 2.84566
[1mStep[0m  [20/21], [94mLoss[0m : 2.59759

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.069, [92mTest[0m: 10.820, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62093
[1mStep[0m  [2/21], [94mLoss[0m : 2.77381
[1mStep[0m  [4/21], [94mLoss[0m : 2.65258
[1mStep[0m  [6/21], [94mLoss[0m : 2.44488
[1mStep[0m  [8/21], [94mLoss[0m : 2.64662
[1mStep[0m  [10/21], [94mLoss[0m : 2.61028
[1mStep[0m  [12/21], [94mLoss[0m : 2.65439
[1mStep[0m  [14/21], [94mLoss[0m : 2.58553
[1mStep[0m  [16/21], [94mLoss[0m : 2.75725
[1mStep[0m  [18/21], [94mLoss[0m : 2.61207
[1mStep[0m  [20/21], [94mLoss[0m : 2.56313

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.607, [92mTest[0m: 4.669, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69426
[1mStep[0m  [2/21], [94mLoss[0m : 2.45343
[1mStep[0m  [4/21], [94mLoss[0m : 2.67576
[1mStep[0m  [6/21], [94mLoss[0m : 2.57143
[1mStep[0m  [8/21], [94mLoss[0m : 2.54160
[1mStep[0m  [10/21], [94mLoss[0m : 2.32249
[1mStep[0m  [12/21], [94mLoss[0m : 2.59609
[1mStep[0m  [14/21], [94mLoss[0m : 2.43204
[1mStep[0m  [16/21], [94mLoss[0m : 2.50805
[1mStep[0m  [18/21], [94mLoss[0m : 2.42255
[1mStep[0m  [20/21], [94mLoss[0m : 2.57685

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.515, [92mTest[0m: 3.119, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33895
[1mStep[0m  [2/21], [94mLoss[0m : 2.49607
[1mStep[0m  [4/21], [94mLoss[0m : 2.42544
[1mStep[0m  [6/21], [94mLoss[0m : 2.51036
[1mStep[0m  [8/21], [94mLoss[0m : 2.46981
[1mStep[0m  [10/21], [94mLoss[0m : 2.59460
[1mStep[0m  [12/21], [94mLoss[0m : 2.46915
[1mStep[0m  [14/21], [94mLoss[0m : 2.56914
[1mStep[0m  [16/21], [94mLoss[0m : 2.56579
[1mStep[0m  [18/21], [94mLoss[0m : 2.56899
[1mStep[0m  [20/21], [94mLoss[0m : 2.47193

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.720, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45888
[1mStep[0m  [2/21], [94mLoss[0m : 2.44262
[1mStep[0m  [4/21], [94mLoss[0m : 2.35882
[1mStep[0m  [6/21], [94mLoss[0m : 2.44777
[1mStep[0m  [8/21], [94mLoss[0m : 2.39301
[1mStep[0m  [10/21], [94mLoss[0m : 2.52662
[1mStep[0m  [12/21], [94mLoss[0m : 2.39847
[1mStep[0m  [14/21], [94mLoss[0m : 2.49255
[1mStep[0m  [16/21], [94mLoss[0m : 2.51878
[1mStep[0m  [18/21], [94mLoss[0m : 2.64451
[1mStep[0m  [20/21], [94mLoss[0m : 2.45236

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.653, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46761
[1mStep[0m  [2/21], [94mLoss[0m : 2.38646
[1mStep[0m  [4/21], [94mLoss[0m : 2.35815
[1mStep[0m  [6/21], [94mLoss[0m : 2.45451
[1mStep[0m  [8/21], [94mLoss[0m : 2.73116
[1mStep[0m  [10/21], [94mLoss[0m : 2.29277
[1mStep[0m  [12/21], [94mLoss[0m : 2.41821
[1mStep[0m  [14/21], [94mLoss[0m : 2.47033
[1mStep[0m  [16/21], [94mLoss[0m : 2.37707
[1mStep[0m  [18/21], [94mLoss[0m : 2.46637
[1mStep[0m  [20/21], [94mLoss[0m : 2.48746

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.612, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39321
[1mStep[0m  [2/21], [94mLoss[0m : 2.49375
[1mStep[0m  [4/21], [94mLoss[0m : 2.45760
[1mStep[0m  [6/21], [94mLoss[0m : 2.51529
[1mStep[0m  [8/21], [94mLoss[0m : 2.38808
[1mStep[0m  [10/21], [94mLoss[0m : 2.36958
[1mStep[0m  [12/21], [94mLoss[0m : 2.55720
[1mStep[0m  [14/21], [94mLoss[0m : 2.42611
[1mStep[0m  [16/21], [94mLoss[0m : 2.38638
[1mStep[0m  [18/21], [94mLoss[0m : 2.36969
[1mStep[0m  [20/21], [94mLoss[0m : 2.45639

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.645, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44730
[1mStep[0m  [2/21], [94mLoss[0m : 2.50336
[1mStep[0m  [4/21], [94mLoss[0m : 2.40036
[1mStep[0m  [6/21], [94mLoss[0m : 2.29868
[1mStep[0m  [8/21], [94mLoss[0m : 2.38899
[1mStep[0m  [10/21], [94mLoss[0m : 2.46040
[1mStep[0m  [12/21], [94mLoss[0m : 2.39130
[1mStep[0m  [14/21], [94mLoss[0m : 2.47365
[1mStep[0m  [16/21], [94mLoss[0m : 2.40346
[1mStep[0m  [18/21], [94mLoss[0m : 2.53751
[1mStep[0m  [20/21], [94mLoss[0m : 2.56711

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.597, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36799
[1mStep[0m  [2/21], [94mLoss[0m : 2.50116
[1mStep[0m  [4/21], [94mLoss[0m : 2.54192
[1mStep[0m  [6/21], [94mLoss[0m : 2.39650
[1mStep[0m  [8/21], [94mLoss[0m : 2.48479
[1mStep[0m  [10/21], [94mLoss[0m : 2.40401
[1mStep[0m  [12/21], [94mLoss[0m : 2.31870
[1mStep[0m  [14/21], [94mLoss[0m : 2.30467
[1mStep[0m  [16/21], [94mLoss[0m : 2.44800
[1mStep[0m  [18/21], [94mLoss[0m : 2.25976
[1mStep[0m  [20/21], [94mLoss[0m : 2.44103

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.554, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48060
[1mStep[0m  [2/21], [94mLoss[0m : 2.42893
[1mStep[0m  [4/21], [94mLoss[0m : 2.36090
[1mStep[0m  [6/21], [94mLoss[0m : 2.30030
[1mStep[0m  [8/21], [94mLoss[0m : 2.46985
[1mStep[0m  [10/21], [94mLoss[0m : 2.46927
[1mStep[0m  [12/21], [94mLoss[0m : 2.37117
[1mStep[0m  [14/21], [94mLoss[0m : 2.45298
[1mStep[0m  [16/21], [94mLoss[0m : 2.33957
[1mStep[0m  [18/21], [94mLoss[0m : 2.44758
[1mStep[0m  [20/21], [94mLoss[0m : 2.45771

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.594, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48548
[1mStep[0m  [2/21], [94mLoss[0m : 2.41856
[1mStep[0m  [4/21], [94mLoss[0m : 2.50247
[1mStep[0m  [6/21], [94mLoss[0m : 2.31851
[1mStep[0m  [8/21], [94mLoss[0m : 2.60128
[1mStep[0m  [10/21], [94mLoss[0m : 2.34280
[1mStep[0m  [12/21], [94mLoss[0m : 2.30652
[1mStep[0m  [14/21], [94mLoss[0m : 2.24972
[1mStep[0m  [16/21], [94mLoss[0m : 2.29270
[1mStep[0m  [18/21], [94mLoss[0m : 2.42096
[1mStep[0m  [20/21], [94mLoss[0m : 2.40512

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.513, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43849
[1mStep[0m  [2/21], [94mLoss[0m : 2.37770
[1mStep[0m  [4/21], [94mLoss[0m : 2.54648
[1mStep[0m  [6/21], [94mLoss[0m : 2.40359
[1mStep[0m  [8/21], [94mLoss[0m : 2.41110
[1mStep[0m  [10/21], [94mLoss[0m : 2.34377
[1mStep[0m  [12/21], [94mLoss[0m : 2.48281
[1mStep[0m  [14/21], [94mLoss[0m : 2.43443
[1mStep[0m  [16/21], [94mLoss[0m : 2.35005
[1mStep[0m  [18/21], [94mLoss[0m : 2.35647
[1mStep[0m  [20/21], [94mLoss[0m : 2.47588

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.534, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40887
[1mStep[0m  [2/21], [94mLoss[0m : 2.35895
[1mStep[0m  [4/21], [94mLoss[0m : 2.41958
[1mStep[0m  [6/21], [94mLoss[0m : 2.28123
[1mStep[0m  [8/21], [94mLoss[0m : 2.36029
[1mStep[0m  [10/21], [94mLoss[0m : 2.28338
[1mStep[0m  [12/21], [94mLoss[0m : 2.39358
[1mStep[0m  [14/21], [94mLoss[0m : 2.60054
[1mStep[0m  [16/21], [94mLoss[0m : 2.32849
[1mStep[0m  [18/21], [94mLoss[0m : 2.47784
[1mStep[0m  [20/21], [94mLoss[0m : 2.32000

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44121
[1mStep[0m  [2/21], [94mLoss[0m : 2.37546
[1mStep[0m  [4/21], [94mLoss[0m : 2.46538
[1mStep[0m  [6/21], [94mLoss[0m : 2.59767
[1mStep[0m  [8/21], [94mLoss[0m : 2.43381
[1mStep[0m  [10/21], [94mLoss[0m : 2.44076
[1mStep[0m  [12/21], [94mLoss[0m : 2.52793
[1mStep[0m  [14/21], [94mLoss[0m : 2.40518
[1mStep[0m  [16/21], [94mLoss[0m : 2.25826
[1mStep[0m  [18/21], [94mLoss[0m : 2.37406
[1mStep[0m  [20/21], [94mLoss[0m : 2.40701

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.520, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32120
[1mStep[0m  [2/21], [94mLoss[0m : 2.38043
[1mStep[0m  [4/21], [94mLoss[0m : 2.50676
[1mStep[0m  [6/21], [94mLoss[0m : 2.49059
[1mStep[0m  [8/21], [94mLoss[0m : 2.36607
[1mStep[0m  [10/21], [94mLoss[0m : 2.27788
[1mStep[0m  [12/21], [94mLoss[0m : 2.44314
[1mStep[0m  [14/21], [94mLoss[0m : 2.18334
[1mStep[0m  [16/21], [94mLoss[0m : 2.43823
[1mStep[0m  [18/21], [94mLoss[0m : 2.49750
[1mStep[0m  [20/21], [94mLoss[0m : 2.31212

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38178
[1mStep[0m  [2/21], [94mLoss[0m : 2.31873
[1mStep[0m  [4/21], [94mLoss[0m : 2.45618
[1mStep[0m  [6/21], [94mLoss[0m : 2.34736
[1mStep[0m  [8/21], [94mLoss[0m : 2.34028
[1mStep[0m  [10/21], [94mLoss[0m : 2.39169
[1mStep[0m  [12/21], [94mLoss[0m : 2.37085
[1mStep[0m  [14/21], [94mLoss[0m : 2.49675
[1mStep[0m  [16/21], [94mLoss[0m : 2.43516
[1mStep[0m  [18/21], [94mLoss[0m : 2.29513
[1mStep[0m  [20/21], [94mLoss[0m : 2.49366

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45459
[1mStep[0m  [2/21], [94mLoss[0m : 2.50017
[1mStep[0m  [4/21], [94mLoss[0m : 2.46363
[1mStep[0m  [6/21], [94mLoss[0m : 2.47097
[1mStep[0m  [8/21], [94mLoss[0m : 2.47569
[1mStep[0m  [10/21], [94mLoss[0m : 2.29562
[1mStep[0m  [12/21], [94mLoss[0m : 2.36022
[1mStep[0m  [14/21], [94mLoss[0m : 2.22054
[1mStep[0m  [16/21], [94mLoss[0m : 2.46931
[1mStep[0m  [18/21], [94mLoss[0m : 2.46623
[1mStep[0m  [20/21], [94mLoss[0m : 2.24640

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41838
[1mStep[0m  [2/21], [94mLoss[0m : 2.43236
[1mStep[0m  [4/21], [94mLoss[0m : 2.32459
[1mStep[0m  [6/21], [94mLoss[0m : 2.44641
[1mStep[0m  [8/21], [94mLoss[0m : 2.34544
[1mStep[0m  [10/21], [94mLoss[0m : 2.35314
[1mStep[0m  [12/21], [94mLoss[0m : 2.40367
[1mStep[0m  [14/21], [94mLoss[0m : 2.16309
[1mStep[0m  [16/21], [94mLoss[0m : 2.36399
[1mStep[0m  [18/21], [94mLoss[0m : 2.42344
[1mStep[0m  [20/21], [94mLoss[0m : 2.36326

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43847
[1mStep[0m  [2/21], [94mLoss[0m : 2.43559
[1mStep[0m  [4/21], [94mLoss[0m : 2.31082
[1mStep[0m  [6/21], [94mLoss[0m : 2.34522
[1mStep[0m  [8/21], [94mLoss[0m : 2.36882
[1mStep[0m  [10/21], [94mLoss[0m : 2.30894
[1mStep[0m  [12/21], [94mLoss[0m : 2.41697
[1mStep[0m  [14/21], [94mLoss[0m : 2.42924
[1mStep[0m  [16/21], [94mLoss[0m : 2.39226
[1mStep[0m  [18/21], [94mLoss[0m : 2.38531
[1mStep[0m  [20/21], [94mLoss[0m : 2.23977

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35496
[1mStep[0m  [2/21], [94mLoss[0m : 2.36759
[1mStep[0m  [4/21], [94mLoss[0m : 2.29365
[1mStep[0m  [6/21], [94mLoss[0m : 2.49831
[1mStep[0m  [8/21], [94mLoss[0m : 2.57051
[1mStep[0m  [10/21], [94mLoss[0m : 2.39220
[1mStep[0m  [12/21], [94mLoss[0m : 2.25639
[1mStep[0m  [14/21], [94mLoss[0m : 2.45764
[1mStep[0m  [16/21], [94mLoss[0m : 2.26091
[1mStep[0m  [18/21], [94mLoss[0m : 2.40197
[1mStep[0m  [20/21], [94mLoss[0m : 2.33855

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32298
[1mStep[0m  [2/21], [94mLoss[0m : 2.48292
[1mStep[0m  [4/21], [94mLoss[0m : 2.49104
[1mStep[0m  [6/21], [94mLoss[0m : 2.32730
[1mStep[0m  [8/21], [94mLoss[0m : 2.36359
[1mStep[0m  [10/21], [94mLoss[0m : 2.31845
[1mStep[0m  [12/21], [94mLoss[0m : 2.33561
[1mStep[0m  [14/21], [94mLoss[0m : 2.46437
[1mStep[0m  [16/21], [94mLoss[0m : 2.22758
[1mStep[0m  [18/21], [94mLoss[0m : 2.35359
[1mStep[0m  [20/21], [94mLoss[0m : 2.37364

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40692
[1mStep[0m  [2/21], [94mLoss[0m : 2.35881
[1mStep[0m  [4/21], [94mLoss[0m : 2.35410
[1mStep[0m  [6/21], [94mLoss[0m : 2.39583
[1mStep[0m  [8/21], [94mLoss[0m : 2.40382
[1mStep[0m  [10/21], [94mLoss[0m : 2.32679
[1mStep[0m  [12/21], [94mLoss[0m : 2.34640
[1mStep[0m  [14/21], [94mLoss[0m : 2.26359
[1mStep[0m  [16/21], [94mLoss[0m : 2.34980
[1mStep[0m  [18/21], [94mLoss[0m : 2.39705
[1mStep[0m  [20/21], [94mLoss[0m : 2.34425

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31810
[1mStep[0m  [2/21], [94mLoss[0m : 2.47684
[1mStep[0m  [4/21], [94mLoss[0m : 2.36887
[1mStep[0m  [6/21], [94mLoss[0m : 2.38800
[1mStep[0m  [8/21], [94mLoss[0m : 2.24507
[1mStep[0m  [10/21], [94mLoss[0m : 2.38519
[1mStep[0m  [12/21], [94mLoss[0m : 2.38282
[1mStep[0m  [14/21], [94mLoss[0m : 2.36423
[1mStep[0m  [16/21], [94mLoss[0m : 2.41024
[1mStep[0m  [18/21], [94mLoss[0m : 2.45797
[1mStep[0m  [20/21], [94mLoss[0m : 2.39493

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33128
[1mStep[0m  [2/21], [94mLoss[0m : 2.41130
[1mStep[0m  [4/21], [94mLoss[0m : 2.23436
[1mStep[0m  [6/21], [94mLoss[0m : 2.35011
[1mStep[0m  [8/21], [94mLoss[0m : 2.34911
[1mStep[0m  [10/21], [94mLoss[0m : 2.31882
[1mStep[0m  [12/21], [94mLoss[0m : 2.44570
[1mStep[0m  [14/21], [94mLoss[0m : 2.39131
[1mStep[0m  [16/21], [94mLoss[0m : 2.40985
[1mStep[0m  [18/21], [94mLoss[0m : 2.38892
[1mStep[0m  [20/21], [94mLoss[0m : 2.29553

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.467, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27796
[1mStep[0m  [2/21], [94mLoss[0m : 2.46467
[1mStep[0m  [4/21], [94mLoss[0m : 2.26779
[1mStep[0m  [6/21], [94mLoss[0m : 2.38314
[1mStep[0m  [8/21], [94mLoss[0m : 2.39357
[1mStep[0m  [10/21], [94mLoss[0m : 2.25782
[1mStep[0m  [12/21], [94mLoss[0m : 2.46084
[1mStep[0m  [14/21], [94mLoss[0m : 2.34393
[1mStep[0m  [16/21], [94mLoss[0m : 2.28200
[1mStep[0m  [18/21], [94mLoss[0m : 2.34152
[1mStep[0m  [20/21], [94mLoss[0m : 2.31554

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.454, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34886
[1mStep[0m  [2/21], [94mLoss[0m : 2.28854
[1mStep[0m  [4/21], [94mLoss[0m : 2.44539
[1mStep[0m  [6/21], [94mLoss[0m : 2.39487
[1mStep[0m  [8/21], [94mLoss[0m : 2.37348
[1mStep[0m  [10/21], [94mLoss[0m : 2.23456
[1mStep[0m  [12/21], [94mLoss[0m : 2.29573
[1mStep[0m  [14/21], [94mLoss[0m : 2.29188
[1mStep[0m  [16/21], [94mLoss[0m : 2.30465
[1mStep[0m  [18/21], [94mLoss[0m : 2.36460
[1mStep[0m  [20/21], [94mLoss[0m : 2.45045

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44609
[1mStep[0m  [2/21], [94mLoss[0m : 2.26388
[1mStep[0m  [4/21], [94mLoss[0m : 2.34466
[1mStep[0m  [6/21], [94mLoss[0m : 2.30006
[1mStep[0m  [8/21], [94mLoss[0m : 2.28990
[1mStep[0m  [10/21], [94mLoss[0m : 2.40208
[1mStep[0m  [12/21], [94mLoss[0m : 2.46223
[1mStep[0m  [14/21], [94mLoss[0m : 2.32609
[1mStep[0m  [16/21], [94mLoss[0m : 2.38256
[1mStep[0m  [18/21], [94mLoss[0m : 2.34621
[1mStep[0m  [20/21], [94mLoss[0m : 2.19171

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.467, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31995
[1mStep[0m  [2/21], [94mLoss[0m : 2.20680
[1mStep[0m  [4/21], [94mLoss[0m : 2.25072
[1mStep[0m  [6/21], [94mLoss[0m : 2.37823
[1mStep[0m  [8/21], [94mLoss[0m : 2.30816
[1mStep[0m  [10/21], [94mLoss[0m : 2.24446
[1mStep[0m  [12/21], [94mLoss[0m : 2.31035
[1mStep[0m  [14/21], [94mLoss[0m : 2.48554
[1mStep[0m  [16/21], [94mLoss[0m : 2.45373
[1mStep[0m  [18/21], [94mLoss[0m : 2.37404
[1mStep[0m  [20/21], [94mLoss[0m : 2.38079

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39628
[1mStep[0m  [2/21], [94mLoss[0m : 2.27768
[1mStep[0m  [4/21], [94mLoss[0m : 2.21086
[1mStep[0m  [6/21], [94mLoss[0m : 2.40803
[1mStep[0m  [8/21], [94mLoss[0m : 2.22134
[1mStep[0m  [10/21], [94mLoss[0m : 2.47774
[1mStep[0m  [12/21], [94mLoss[0m : 2.30655
[1mStep[0m  [14/21], [94mLoss[0m : 2.31751
[1mStep[0m  [16/21], [94mLoss[0m : 2.29528
[1mStep[0m  [18/21], [94mLoss[0m : 2.35862
[1mStep[0m  [20/21], [94mLoss[0m : 2.29778

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.451, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17340
[1mStep[0m  [2/21], [94mLoss[0m : 2.36930
[1mStep[0m  [4/21], [94mLoss[0m : 2.34599
[1mStep[0m  [6/21], [94mLoss[0m : 2.31726
[1mStep[0m  [8/21], [94mLoss[0m : 2.31040
[1mStep[0m  [10/21], [94mLoss[0m : 2.30084
[1mStep[0m  [12/21], [94mLoss[0m : 2.32771
[1mStep[0m  [14/21], [94mLoss[0m : 2.33627
[1mStep[0m  [16/21], [94mLoss[0m : 2.43242
[1mStep[0m  [18/21], [94mLoss[0m : 2.27164
[1mStep[0m  [20/21], [94mLoss[0m : 2.27111

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.455, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.430
====================================

Phase 1 - Evaluation MAE:  2.429899045399257
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.31825
[1mStep[0m  [2/21], [94mLoss[0m : 2.33482
[1mStep[0m  [4/21], [94mLoss[0m : 2.24921
[1mStep[0m  [6/21], [94mLoss[0m : 2.26972
[1mStep[0m  [8/21], [94mLoss[0m : 2.49955
[1mStep[0m  [10/21], [94mLoss[0m : 2.51659
[1mStep[0m  [12/21], [94mLoss[0m : 2.44645
[1mStep[0m  [14/21], [94mLoss[0m : 2.39550
[1mStep[0m  [16/21], [94mLoss[0m : 2.48730
[1mStep[0m  [18/21], [94mLoss[0m : 2.46909
[1mStep[0m  [20/21], [94mLoss[0m : 2.43974

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36615
[1mStep[0m  [2/21], [94mLoss[0m : 2.30052
[1mStep[0m  [4/21], [94mLoss[0m : 2.37211
[1mStep[0m  [6/21], [94mLoss[0m : 2.47896
[1mStep[0m  [8/21], [94mLoss[0m : 2.25262
[1mStep[0m  [10/21], [94mLoss[0m : 2.37144
[1mStep[0m  [12/21], [94mLoss[0m : 2.33649
[1mStep[0m  [14/21], [94mLoss[0m : 2.37391
[1mStep[0m  [16/21], [94mLoss[0m : 2.30279
[1mStep[0m  [18/21], [94mLoss[0m : 2.23200
[1mStep[0m  [20/21], [94mLoss[0m : 2.37171

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.722, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31082
[1mStep[0m  [2/21], [94mLoss[0m : 2.25995
[1mStep[0m  [4/21], [94mLoss[0m : 2.23735
[1mStep[0m  [6/21], [94mLoss[0m : 2.24112
[1mStep[0m  [8/21], [94mLoss[0m : 2.35537
[1mStep[0m  [10/21], [94mLoss[0m : 2.27568
[1mStep[0m  [12/21], [94mLoss[0m : 2.19527
[1mStep[0m  [14/21], [94mLoss[0m : 2.41992
[1mStep[0m  [16/21], [94mLoss[0m : 2.34844
[1mStep[0m  [18/21], [94mLoss[0m : 2.36378
[1mStep[0m  [20/21], [94mLoss[0m : 2.27854

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17510
[1mStep[0m  [2/21], [94mLoss[0m : 2.22065
[1mStep[0m  [4/21], [94mLoss[0m : 2.17057
[1mStep[0m  [6/21], [94mLoss[0m : 2.23696
[1mStep[0m  [8/21], [94mLoss[0m : 2.32657
[1mStep[0m  [10/21], [94mLoss[0m : 2.16549
[1mStep[0m  [12/21], [94mLoss[0m : 2.18244
[1mStep[0m  [14/21], [94mLoss[0m : 2.24299
[1mStep[0m  [16/21], [94mLoss[0m : 2.32939
[1mStep[0m  [18/21], [94mLoss[0m : 2.10858
[1mStep[0m  [20/21], [94mLoss[0m : 2.18769

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19359
[1mStep[0m  [2/21], [94mLoss[0m : 2.20307
[1mStep[0m  [4/21], [94mLoss[0m : 2.10942
[1mStep[0m  [6/21], [94mLoss[0m : 2.18256
[1mStep[0m  [8/21], [94mLoss[0m : 2.12430
[1mStep[0m  [10/21], [94mLoss[0m : 2.00269
[1mStep[0m  [12/21], [94mLoss[0m : 2.18266
[1mStep[0m  [14/21], [94mLoss[0m : 2.01150
[1mStep[0m  [16/21], [94mLoss[0m : 2.08993
[1mStep[0m  [18/21], [94mLoss[0m : 2.25136
[1mStep[0m  [20/21], [94mLoss[0m : 2.26050

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14175
[1mStep[0m  [2/21], [94mLoss[0m : 2.15300
[1mStep[0m  [4/21], [94mLoss[0m : 2.16256
[1mStep[0m  [6/21], [94mLoss[0m : 2.08500
[1mStep[0m  [8/21], [94mLoss[0m : 2.05814
[1mStep[0m  [10/21], [94mLoss[0m : 2.32670
[1mStep[0m  [12/21], [94mLoss[0m : 2.24014
[1mStep[0m  [14/21], [94mLoss[0m : 2.11990
[1mStep[0m  [16/21], [94mLoss[0m : 2.02886
[1mStep[0m  [18/21], [94mLoss[0m : 2.03544
[1mStep[0m  [20/21], [94mLoss[0m : 2.25039

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07947
[1mStep[0m  [2/21], [94mLoss[0m : 2.19456
[1mStep[0m  [4/21], [94mLoss[0m : 1.98943
[1mStep[0m  [6/21], [94mLoss[0m : 2.15174
[1mStep[0m  [8/21], [94mLoss[0m : 2.09850
[1mStep[0m  [10/21], [94mLoss[0m : 2.09561
[1mStep[0m  [12/21], [94mLoss[0m : 2.32715
[1mStep[0m  [14/21], [94mLoss[0m : 2.02980
[1mStep[0m  [16/21], [94mLoss[0m : 2.23656
[1mStep[0m  [18/21], [94mLoss[0m : 2.03103
[1mStep[0m  [20/21], [94mLoss[0m : 2.19866

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93950
[1mStep[0m  [2/21], [94mLoss[0m : 1.92962
[1mStep[0m  [4/21], [94mLoss[0m : 1.97602
[1mStep[0m  [6/21], [94mLoss[0m : 2.20374
[1mStep[0m  [8/21], [94mLoss[0m : 2.02462
[1mStep[0m  [10/21], [94mLoss[0m : 2.06032
[1mStep[0m  [12/21], [94mLoss[0m : 2.06055
[1mStep[0m  [14/21], [94mLoss[0m : 2.02063
[1mStep[0m  [16/21], [94mLoss[0m : 2.25305
[1mStep[0m  [18/21], [94mLoss[0m : 1.95089
[1mStep[0m  [20/21], [94mLoss[0m : 2.06544

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06188
[1mStep[0m  [2/21], [94mLoss[0m : 1.97076
[1mStep[0m  [4/21], [94mLoss[0m : 2.01399
[1mStep[0m  [6/21], [94mLoss[0m : 1.89553
[1mStep[0m  [8/21], [94mLoss[0m : 1.95469
[1mStep[0m  [10/21], [94mLoss[0m : 1.98118
[1mStep[0m  [12/21], [94mLoss[0m : 1.94216
[1mStep[0m  [14/21], [94mLoss[0m : 1.90054
[1mStep[0m  [16/21], [94mLoss[0m : 1.97653
[1mStep[0m  [18/21], [94mLoss[0m : 1.97129
[1mStep[0m  [20/21], [94mLoss[0m : 2.09549

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86993
[1mStep[0m  [2/21], [94mLoss[0m : 1.90731
[1mStep[0m  [4/21], [94mLoss[0m : 1.89568
[1mStep[0m  [6/21], [94mLoss[0m : 1.91328
[1mStep[0m  [8/21], [94mLoss[0m : 1.99964
[1mStep[0m  [10/21], [94mLoss[0m : 1.89023
[1mStep[0m  [12/21], [94mLoss[0m : 1.97083
[1mStep[0m  [14/21], [94mLoss[0m : 1.99688
[1mStep[0m  [16/21], [94mLoss[0m : 1.90768
[1mStep[0m  [18/21], [94mLoss[0m : 2.07805
[1mStep[0m  [20/21], [94mLoss[0m : 2.02790

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97153
[1mStep[0m  [2/21], [94mLoss[0m : 1.81033
[1mStep[0m  [4/21], [94mLoss[0m : 1.92897
[1mStep[0m  [6/21], [94mLoss[0m : 1.98125
[1mStep[0m  [8/21], [94mLoss[0m : 1.86585
[1mStep[0m  [10/21], [94mLoss[0m : 1.94811
[1mStep[0m  [12/21], [94mLoss[0m : 1.90704
[1mStep[0m  [14/21], [94mLoss[0m : 1.94544
[1mStep[0m  [16/21], [94mLoss[0m : 1.92130
[1mStep[0m  [18/21], [94mLoss[0m : 1.92891
[1mStep[0m  [20/21], [94mLoss[0m : 2.06837

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.83402
[1mStep[0m  [2/21], [94mLoss[0m : 1.92497
[1mStep[0m  [4/21], [94mLoss[0m : 1.90194
[1mStep[0m  [6/21], [94mLoss[0m : 1.80560
[1mStep[0m  [8/21], [94mLoss[0m : 1.86996
[1mStep[0m  [10/21], [94mLoss[0m : 1.93123
[1mStep[0m  [12/21], [94mLoss[0m : 1.81895
[1mStep[0m  [14/21], [94mLoss[0m : 1.91537
[1mStep[0m  [16/21], [94mLoss[0m : 1.79326
[1mStep[0m  [18/21], [94mLoss[0m : 1.91432
[1mStep[0m  [20/21], [94mLoss[0m : 1.78271

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82076
[1mStep[0m  [2/21], [94mLoss[0m : 1.73841
[1mStep[0m  [4/21], [94mLoss[0m : 1.80830
[1mStep[0m  [6/21], [94mLoss[0m : 1.77839
[1mStep[0m  [8/21], [94mLoss[0m : 1.77655
[1mStep[0m  [10/21], [94mLoss[0m : 1.82645
[1mStep[0m  [12/21], [94mLoss[0m : 1.80833
[1mStep[0m  [14/21], [94mLoss[0m : 1.80230
[1mStep[0m  [16/21], [94mLoss[0m : 1.90614
[1mStep[0m  [18/21], [94mLoss[0m : 1.79373
[1mStep[0m  [20/21], [94mLoss[0m : 1.89827

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.77293
[1mStep[0m  [2/21], [94mLoss[0m : 1.81878
[1mStep[0m  [4/21], [94mLoss[0m : 1.81316
[1mStep[0m  [6/21], [94mLoss[0m : 1.78221
[1mStep[0m  [8/21], [94mLoss[0m : 1.76591
[1mStep[0m  [10/21], [94mLoss[0m : 1.81464
[1mStep[0m  [12/21], [94mLoss[0m : 1.84052
[1mStep[0m  [14/21], [94mLoss[0m : 1.83348
[1mStep[0m  [16/21], [94mLoss[0m : 1.87468
[1mStep[0m  [18/21], [94mLoss[0m : 1.73308
[1mStep[0m  [20/21], [94mLoss[0m : 1.70781

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.72735
[1mStep[0m  [2/21], [94mLoss[0m : 1.84474
[1mStep[0m  [4/21], [94mLoss[0m : 1.74630
[1mStep[0m  [6/21], [94mLoss[0m : 1.82468
[1mStep[0m  [8/21], [94mLoss[0m : 1.78616
[1mStep[0m  [10/21], [94mLoss[0m : 1.78467
[1mStep[0m  [12/21], [94mLoss[0m : 1.72537
[1mStep[0m  [14/21], [94mLoss[0m : 1.73429
[1mStep[0m  [16/21], [94mLoss[0m : 1.82136
[1mStep[0m  [18/21], [94mLoss[0m : 1.71126
[1mStep[0m  [20/21], [94mLoss[0m : 1.65402

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.69065
[1mStep[0m  [2/21], [94mLoss[0m : 1.62949
[1mStep[0m  [4/21], [94mLoss[0m : 1.66763
[1mStep[0m  [6/21], [94mLoss[0m : 1.64928
[1mStep[0m  [8/21], [94mLoss[0m : 1.71561
[1mStep[0m  [10/21], [94mLoss[0m : 1.84484
[1mStep[0m  [12/21], [94mLoss[0m : 1.71401
[1mStep[0m  [14/21], [94mLoss[0m : 1.61323
[1mStep[0m  [16/21], [94mLoss[0m : 1.70281
[1mStep[0m  [18/21], [94mLoss[0m : 1.60873
[1mStep[0m  [20/21], [94mLoss[0m : 1.57932

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.686, [92mTest[0m: 2.566, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.56026
[1mStep[0m  [2/21], [94mLoss[0m : 1.57164
[1mStep[0m  [4/21], [94mLoss[0m : 1.73477
[1mStep[0m  [6/21], [94mLoss[0m : 1.73144
[1mStep[0m  [8/21], [94mLoss[0m : 1.63564
[1mStep[0m  [10/21], [94mLoss[0m : 1.65783
[1mStep[0m  [12/21], [94mLoss[0m : 1.61004
[1mStep[0m  [14/21], [94mLoss[0m : 1.69848
[1mStep[0m  [16/21], [94mLoss[0m : 1.64125
[1mStep[0m  [18/21], [94mLoss[0m : 1.66768
[1mStep[0m  [20/21], [94mLoss[0m : 1.63577

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51874
[1mStep[0m  [2/21], [94mLoss[0m : 1.69083
[1mStep[0m  [4/21], [94mLoss[0m : 1.56753
[1mStep[0m  [6/21], [94mLoss[0m : 1.53734
[1mStep[0m  [8/21], [94mLoss[0m : 1.65519
[1mStep[0m  [10/21], [94mLoss[0m : 1.52829
[1mStep[0m  [12/21], [94mLoss[0m : 1.58144
[1mStep[0m  [14/21], [94mLoss[0m : 1.75144
[1mStep[0m  [16/21], [94mLoss[0m : 1.51879
[1mStep[0m  [18/21], [94mLoss[0m : 1.67310
[1mStep[0m  [20/21], [94mLoss[0m : 1.64837

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.47616
[1mStep[0m  [2/21], [94mLoss[0m : 1.52189
[1mStep[0m  [4/21], [94mLoss[0m : 1.55991
[1mStep[0m  [6/21], [94mLoss[0m : 1.53326
[1mStep[0m  [8/21], [94mLoss[0m : 1.63582
[1mStep[0m  [10/21], [94mLoss[0m : 1.59487
[1mStep[0m  [12/21], [94mLoss[0m : 1.63864
[1mStep[0m  [14/21], [94mLoss[0m : 1.51599
[1mStep[0m  [16/21], [94mLoss[0m : 1.66820
[1mStep[0m  [18/21], [94mLoss[0m : 1.58926
[1mStep[0m  [20/21], [94mLoss[0m : 1.65203

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.47292
[1mStep[0m  [2/21], [94mLoss[0m : 1.69285
[1mStep[0m  [4/21], [94mLoss[0m : 1.64276
[1mStep[0m  [6/21], [94mLoss[0m : 1.45595
[1mStep[0m  [8/21], [94mLoss[0m : 1.57732
[1mStep[0m  [10/21], [94mLoss[0m : 1.65866
[1mStep[0m  [12/21], [94mLoss[0m : 1.59329
[1mStep[0m  [14/21], [94mLoss[0m : 1.41980
[1mStep[0m  [16/21], [94mLoss[0m : 1.71937
[1mStep[0m  [18/21], [94mLoss[0m : 1.58512
[1mStep[0m  [20/21], [94mLoss[0m : 1.57393

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63719
[1mStep[0m  [2/21], [94mLoss[0m : 1.44809
[1mStep[0m  [4/21], [94mLoss[0m : 1.39517
[1mStep[0m  [6/21], [94mLoss[0m : 1.53213
[1mStep[0m  [8/21], [94mLoss[0m : 1.53848
[1mStep[0m  [10/21], [94mLoss[0m : 1.62660
[1mStep[0m  [12/21], [94mLoss[0m : 1.59368
[1mStep[0m  [14/21], [94mLoss[0m : 1.47600
[1mStep[0m  [16/21], [94mLoss[0m : 1.55498
[1mStep[0m  [18/21], [94mLoss[0m : 1.55508
[1mStep[0m  [20/21], [94mLoss[0m : 1.48025

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.540, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46661
[1mStep[0m  [2/21], [94mLoss[0m : 1.43583
[1mStep[0m  [4/21], [94mLoss[0m : 1.48460
[1mStep[0m  [6/21], [94mLoss[0m : 1.54529
[1mStep[0m  [8/21], [94mLoss[0m : 1.61461
[1mStep[0m  [10/21], [94mLoss[0m : 1.62105
[1mStep[0m  [12/21], [94mLoss[0m : 1.53047
[1mStep[0m  [14/21], [94mLoss[0m : 1.46833
[1mStep[0m  [16/21], [94mLoss[0m : 1.47915
[1mStep[0m  [18/21], [94mLoss[0m : 1.51137
[1mStep[0m  [20/21], [94mLoss[0m : 1.52382

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.508, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.34430
[1mStep[0m  [2/21], [94mLoss[0m : 1.47353
[1mStep[0m  [4/21], [94mLoss[0m : 1.43870
[1mStep[0m  [6/21], [94mLoss[0m : 1.37609
[1mStep[0m  [8/21], [94mLoss[0m : 1.54554
[1mStep[0m  [10/21], [94mLoss[0m : 1.53084
[1mStep[0m  [12/21], [94mLoss[0m : 1.45516
[1mStep[0m  [14/21], [94mLoss[0m : 1.44306
[1mStep[0m  [16/21], [94mLoss[0m : 1.49256
[1mStep[0m  [18/21], [94mLoss[0m : 1.42508
[1mStep[0m  [20/21], [94mLoss[0m : 1.48683

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.465, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.36656
[1mStep[0m  [2/21], [94mLoss[0m : 1.39181
[1mStep[0m  [4/21], [94mLoss[0m : 1.47868
[1mStep[0m  [6/21], [94mLoss[0m : 1.54501
[1mStep[0m  [8/21], [94mLoss[0m : 1.37100
[1mStep[0m  [10/21], [94mLoss[0m : 1.32877
[1mStep[0m  [12/21], [94mLoss[0m : 1.47965
[1mStep[0m  [14/21], [94mLoss[0m : 1.42283
[1mStep[0m  [16/21], [94mLoss[0m : 1.42630
[1mStep[0m  [18/21], [94mLoss[0m : 1.45567
[1mStep[0m  [20/21], [94mLoss[0m : 1.41778

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.424, [92mTest[0m: 2.570, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.561
====================================

Phase 2 - Evaluation MAE:  2.560657262802124
MAE score P1      2.429899
MAE score P2      2.560657
loss              1.424472
learning_rate         0.01
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 24, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.24173
[1mStep[0m  [4/42], [94mLoss[0m : 10.73362
[1mStep[0m  [8/42], [94mLoss[0m : 10.85530
[1mStep[0m  [12/42], [94mLoss[0m : 10.45093
[1mStep[0m  [16/42], [94mLoss[0m : 9.95151
[1mStep[0m  [20/42], [94mLoss[0m : 9.40151
[1mStep[0m  [24/42], [94mLoss[0m : 9.33179
[1mStep[0m  [28/42], [94mLoss[0m : 8.14017
[1mStep[0m  [32/42], [94mLoss[0m : 7.55540
[1mStep[0m  [36/42], [94mLoss[0m : 6.55858
[1mStep[0m  [40/42], [94mLoss[0m : 6.03754

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.039, [92mTest[0m: 10.909, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.66802
[1mStep[0m  [4/42], [94mLoss[0m : 4.82237
[1mStep[0m  [8/42], [94mLoss[0m : 4.18923
[1mStep[0m  [12/42], [94mLoss[0m : 3.34720
[1mStep[0m  [16/42], [94mLoss[0m : 3.08148
[1mStep[0m  [20/42], [94mLoss[0m : 2.64708
[1mStep[0m  [24/42], [94mLoss[0m : 2.67027
[1mStep[0m  [28/42], [94mLoss[0m : 2.60548
[1mStep[0m  [32/42], [94mLoss[0m : 2.86242
[1mStep[0m  [36/42], [94mLoss[0m : 2.93907
[1mStep[0m  [40/42], [94mLoss[0m : 2.57008

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.293, [92mTest[0m: 6.044, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68371
[1mStep[0m  [4/42], [94mLoss[0m : 2.55792
[1mStep[0m  [8/42], [94mLoss[0m : 2.59561
[1mStep[0m  [12/42], [94mLoss[0m : 2.56033
[1mStep[0m  [16/42], [94mLoss[0m : 2.56570
[1mStep[0m  [20/42], [94mLoss[0m : 2.61767
[1mStep[0m  [24/42], [94mLoss[0m : 2.60896
[1mStep[0m  [28/42], [94mLoss[0m : 2.44722
[1mStep[0m  [32/42], [94mLoss[0m : 2.41924
[1mStep[0m  [36/42], [94mLoss[0m : 2.38377
[1mStep[0m  [40/42], [94mLoss[0m : 2.57667

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38442
[1mStep[0m  [4/42], [94mLoss[0m : 2.69464
[1mStep[0m  [8/42], [94mLoss[0m : 2.53291
[1mStep[0m  [12/42], [94mLoss[0m : 2.20331
[1mStep[0m  [16/42], [94mLoss[0m : 2.61715
[1mStep[0m  [20/42], [94mLoss[0m : 2.50550
[1mStep[0m  [24/42], [94mLoss[0m : 2.58724
[1mStep[0m  [28/42], [94mLoss[0m : 2.64300
[1mStep[0m  [32/42], [94mLoss[0m : 2.34438
[1mStep[0m  [36/42], [94mLoss[0m : 2.45322
[1mStep[0m  [40/42], [94mLoss[0m : 2.64357

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54314
[1mStep[0m  [4/42], [94mLoss[0m : 2.57796
[1mStep[0m  [8/42], [94mLoss[0m : 2.43703
[1mStep[0m  [12/42], [94mLoss[0m : 2.41958
[1mStep[0m  [16/42], [94mLoss[0m : 2.52873
[1mStep[0m  [20/42], [94mLoss[0m : 2.48550
[1mStep[0m  [24/42], [94mLoss[0m : 2.49234
[1mStep[0m  [28/42], [94mLoss[0m : 2.50465
[1mStep[0m  [32/42], [94mLoss[0m : 2.36260
[1mStep[0m  [36/42], [94mLoss[0m : 2.43804
[1mStep[0m  [40/42], [94mLoss[0m : 2.49368

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54417
[1mStep[0m  [4/42], [94mLoss[0m : 2.37900
[1mStep[0m  [8/42], [94mLoss[0m : 2.45233
[1mStep[0m  [12/42], [94mLoss[0m : 2.49258
[1mStep[0m  [16/42], [94mLoss[0m : 2.57914
[1mStep[0m  [20/42], [94mLoss[0m : 2.72489
[1mStep[0m  [24/42], [94mLoss[0m : 2.26694
[1mStep[0m  [28/42], [94mLoss[0m : 2.60396
[1mStep[0m  [32/42], [94mLoss[0m : 2.42671
[1mStep[0m  [36/42], [94mLoss[0m : 2.54767
[1mStep[0m  [40/42], [94mLoss[0m : 2.46955

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55335
[1mStep[0m  [4/42], [94mLoss[0m : 2.43900
[1mStep[0m  [8/42], [94mLoss[0m : 2.36052
[1mStep[0m  [12/42], [94mLoss[0m : 2.22202
[1mStep[0m  [16/42], [94mLoss[0m : 2.73677
[1mStep[0m  [20/42], [94mLoss[0m : 2.45957
[1mStep[0m  [24/42], [94mLoss[0m : 2.35479
[1mStep[0m  [28/42], [94mLoss[0m : 2.45907
[1mStep[0m  [32/42], [94mLoss[0m : 2.32871
[1mStep[0m  [36/42], [94mLoss[0m : 2.32349
[1mStep[0m  [40/42], [94mLoss[0m : 2.40742

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31224
[1mStep[0m  [4/42], [94mLoss[0m : 2.28476
[1mStep[0m  [8/42], [94mLoss[0m : 2.54074
[1mStep[0m  [12/42], [94mLoss[0m : 2.39801
[1mStep[0m  [16/42], [94mLoss[0m : 2.11121
[1mStep[0m  [20/42], [94mLoss[0m : 2.45276
[1mStep[0m  [24/42], [94mLoss[0m : 2.35431
[1mStep[0m  [28/42], [94mLoss[0m : 2.34458
[1mStep[0m  [32/42], [94mLoss[0m : 2.34819
[1mStep[0m  [36/42], [94mLoss[0m : 2.60822
[1mStep[0m  [40/42], [94mLoss[0m : 2.33217

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35361
[1mStep[0m  [4/42], [94mLoss[0m : 2.37742
[1mStep[0m  [8/42], [94mLoss[0m : 2.43444
[1mStep[0m  [12/42], [94mLoss[0m : 2.38223
[1mStep[0m  [16/42], [94mLoss[0m : 2.26460
[1mStep[0m  [20/42], [94mLoss[0m : 2.34250
[1mStep[0m  [24/42], [94mLoss[0m : 2.39055
[1mStep[0m  [28/42], [94mLoss[0m : 2.47703
[1mStep[0m  [32/42], [94mLoss[0m : 2.76648
[1mStep[0m  [36/42], [94mLoss[0m : 2.50370
[1mStep[0m  [40/42], [94mLoss[0m : 2.35620

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30927
[1mStep[0m  [4/42], [94mLoss[0m : 2.40327
[1mStep[0m  [8/42], [94mLoss[0m : 2.39332
[1mStep[0m  [12/42], [94mLoss[0m : 2.32070
[1mStep[0m  [16/42], [94mLoss[0m : 2.18886
[1mStep[0m  [20/42], [94mLoss[0m : 2.42168
[1mStep[0m  [24/42], [94mLoss[0m : 2.39406
[1mStep[0m  [28/42], [94mLoss[0m : 2.63207
[1mStep[0m  [32/42], [94mLoss[0m : 2.39909
[1mStep[0m  [36/42], [94mLoss[0m : 2.33933
[1mStep[0m  [40/42], [94mLoss[0m : 2.48732

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34073
[1mStep[0m  [4/42], [94mLoss[0m : 2.37741
[1mStep[0m  [8/42], [94mLoss[0m : 2.53136
[1mStep[0m  [12/42], [94mLoss[0m : 2.51567
[1mStep[0m  [16/42], [94mLoss[0m : 2.28598
[1mStep[0m  [20/42], [94mLoss[0m : 2.25089
[1mStep[0m  [24/42], [94mLoss[0m : 2.39540
[1mStep[0m  [28/42], [94mLoss[0m : 2.56115
[1mStep[0m  [32/42], [94mLoss[0m : 2.60855
[1mStep[0m  [36/42], [94mLoss[0m : 2.41464
[1mStep[0m  [40/42], [94mLoss[0m : 2.54254

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36049
[1mStep[0m  [4/42], [94mLoss[0m : 2.63367
[1mStep[0m  [8/42], [94mLoss[0m : 2.37858
[1mStep[0m  [12/42], [94mLoss[0m : 2.37916
[1mStep[0m  [16/42], [94mLoss[0m : 2.68311
[1mStep[0m  [20/42], [94mLoss[0m : 2.48273
[1mStep[0m  [24/42], [94mLoss[0m : 2.49478
[1mStep[0m  [28/42], [94mLoss[0m : 2.32553
[1mStep[0m  [32/42], [94mLoss[0m : 2.34509
[1mStep[0m  [36/42], [94mLoss[0m : 2.30417
[1mStep[0m  [40/42], [94mLoss[0m : 2.37991

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23192
[1mStep[0m  [4/42], [94mLoss[0m : 2.29944
[1mStep[0m  [8/42], [94mLoss[0m : 2.22060
[1mStep[0m  [12/42], [94mLoss[0m : 2.51086
[1mStep[0m  [16/42], [94mLoss[0m : 2.54807
[1mStep[0m  [20/42], [94mLoss[0m : 2.46327
[1mStep[0m  [24/42], [94mLoss[0m : 2.53828
[1mStep[0m  [28/42], [94mLoss[0m : 2.27061
[1mStep[0m  [32/42], [94mLoss[0m : 2.52024
[1mStep[0m  [36/42], [94mLoss[0m : 2.44534
[1mStep[0m  [40/42], [94mLoss[0m : 2.41596

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31643
[1mStep[0m  [4/42], [94mLoss[0m : 2.24921
[1mStep[0m  [8/42], [94mLoss[0m : 2.78335
[1mStep[0m  [12/42], [94mLoss[0m : 2.35185
[1mStep[0m  [16/42], [94mLoss[0m : 2.38263
[1mStep[0m  [20/42], [94mLoss[0m : 2.32153
[1mStep[0m  [24/42], [94mLoss[0m : 2.36245
[1mStep[0m  [28/42], [94mLoss[0m : 2.25440
[1mStep[0m  [32/42], [94mLoss[0m : 2.24357
[1mStep[0m  [36/42], [94mLoss[0m : 2.32892
[1mStep[0m  [40/42], [94mLoss[0m : 2.17264

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34993
[1mStep[0m  [4/42], [94mLoss[0m : 2.38059
[1mStep[0m  [8/42], [94mLoss[0m : 2.53950
[1mStep[0m  [12/42], [94mLoss[0m : 2.20137
[1mStep[0m  [16/42], [94mLoss[0m : 2.46681
[1mStep[0m  [20/42], [94mLoss[0m : 2.48872
[1mStep[0m  [24/42], [94mLoss[0m : 2.49582
[1mStep[0m  [28/42], [94mLoss[0m : 2.35627
[1mStep[0m  [32/42], [94mLoss[0m : 2.23161
[1mStep[0m  [36/42], [94mLoss[0m : 2.23285
[1mStep[0m  [40/42], [94mLoss[0m : 2.17857

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.318, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41617
[1mStep[0m  [4/42], [94mLoss[0m : 2.41065
[1mStep[0m  [8/42], [94mLoss[0m : 2.47989
[1mStep[0m  [12/42], [94mLoss[0m : 2.29676
[1mStep[0m  [16/42], [94mLoss[0m : 2.48791
[1mStep[0m  [20/42], [94mLoss[0m : 2.29302
[1mStep[0m  [24/42], [94mLoss[0m : 2.14876
[1mStep[0m  [28/42], [94mLoss[0m : 2.39443
[1mStep[0m  [32/42], [94mLoss[0m : 2.58578
[1mStep[0m  [36/42], [94mLoss[0m : 2.40775
[1mStep[0m  [40/42], [94mLoss[0m : 2.20817

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45263
[1mStep[0m  [4/42], [94mLoss[0m : 2.32282
[1mStep[0m  [8/42], [94mLoss[0m : 2.33389
[1mStep[0m  [12/42], [94mLoss[0m : 2.38284
[1mStep[0m  [16/42], [94mLoss[0m : 2.27781
[1mStep[0m  [20/42], [94mLoss[0m : 2.41475
[1mStep[0m  [24/42], [94mLoss[0m : 2.28329
[1mStep[0m  [28/42], [94mLoss[0m : 2.47537
[1mStep[0m  [32/42], [94mLoss[0m : 2.43354
[1mStep[0m  [36/42], [94mLoss[0m : 2.31560
[1mStep[0m  [40/42], [94mLoss[0m : 2.47079

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29588
[1mStep[0m  [4/42], [94mLoss[0m : 2.26687
[1mStep[0m  [8/42], [94mLoss[0m : 2.49021
[1mStep[0m  [12/42], [94mLoss[0m : 2.37716
[1mStep[0m  [16/42], [94mLoss[0m : 2.12841
[1mStep[0m  [20/42], [94mLoss[0m : 2.21182
[1mStep[0m  [24/42], [94mLoss[0m : 2.15314
[1mStep[0m  [28/42], [94mLoss[0m : 2.25597
[1mStep[0m  [32/42], [94mLoss[0m : 2.32505
[1mStep[0m  [36/42], [94mLoss[0m : 2.17810
[1mStep[0m  [40/42], [94mLoss[0m : 2.32570

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.313, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44907
[1mStep[0m  [4/42], [94mLoss[0m : 2.40989
[1mStep[0m  [8/42], [94mLoss[0m : 2.34501
[1mStep[0m  [12/42], [94mLoss[0m : 2.33632
[1mStep[0m  [16/42], [94mLoss[0m : 2.28019
[1mStep[0m  [20/42], [94mLoss[0m : 2.15865
[1mStep[0m  [24/42], [94mLoss[0m : 2.53007
[1mStep[0m  [28/42], [94mLoss[0m : 2.47383
[1mStep[0m  [32/42], [94mLoss[0m : 2.43323
[1mStep[0m  [36/42], [94mLoss[0m : 2.15563
[1mStep[0m  [40/42], [94mLoss[0m : 2.34711

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24170
[1mStep[0m  [4/42], [94mLoss[0m : 2.37340
[1mStep[0m  [8/42], [94mLoss[0m : 2.41913
[1mStep[0m  [12/42], [94mLoss[0m : 2.17048
[1mStep[0m  [16/42], [94mLoss[0m : 2.30183
[1mStep[0m  [20/42], [94mLoss[0m : 2.48965
[1mStep[0m  [24/42], [94mLoss[0m : 2.30632
[1mStep[0m  [28/42], [94mLoss[0m : 2.28133
[1mStep[0m  [32/42], [94mLoss[0m : 2.47686
[1mStep[0m  [36/42], [94mLoss[0m : 2.34967
[1mStep[0m  [40/42], [94mLoss[0m : 2.40848

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.312, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26838
[1mStep[0m  [4/42], [94mLoss[0m : 2.36718
[1mStep[0m  [8/42], [94mLoss[0m : 2.07918
[1mStep[0m  [12/42], [94mLoss[0m : 2.29578
[1mStep[0m  [16/42], [94mLoss[0m : 2.25797
[1mStep[0m  [20/42], [94mLoss[0m : 2.49449
[1mStep[0m  [24/42], [94mLoss[0m : 2.35132
[1mStep[0m  [28/42], [94mLoss[0m : 2.24484
[1mStep[0m  [32/42], [94mLoss[0m : 2.58363
[1mStep[0m  [36/42], [94mLoss[0m : 2.41127
[1mStep[0m  [40/42], [94mLoss[0m : 2.39827

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.312, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26023
[1mStep[0m  [4/42], [94mLoss[0m : 2.40006
[1mStep[0m  [8/42], [94mLoss[0m : 2.49351
[1mStep[0m  [12/42], [94mLoss[0m : 2.12360
[1mStep[0m  [16/42], [94mLoss[0m : 2.44048
[1mStep[0m  [20/42], [94mLoss[0m : 2.33303
[1mStep[0m  [24/42], [94mLoss[0m : 2.28895
[1mStep[0m  [28/42], [94mLoss[0m : 2.31406
[1mStep[0m  [32/42], [94mLoss[0m : 2.38277
[1mStep[0m  [36/42], [94mLoss[0m : 2.34860
[1mStep[0m  [40/42], [94mLoss[0m : 2.41922

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26813
[1mStep[0m  [4/42], [94mLoss[0m : 2.44993
[1mStep[0m  [8/42], [94mLoss[0m : 2.27649
[1mStep[0m  [12/42], [94mLoss[0m : 2.14121
[1mStep[0m  [16/42], [94mLoss[0m : 2.26239
[1mStep[0m  [20/42], [94mLoss[0m : 2.56143
[1mStep[0m  [24/42], [94mLoss[0m : 2.65831
[1mStep[0m  [28/42], [94mLoss[0m : 2.29302
[1mStep[0m  [32/42], [94mLoss[0m : 2.39971
[1mStep[0m  [36/42], [94mLoss[0m : 2.17509
[1mStep[0m  [40/42], [94mLoss[0m : 2.29034

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.312, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34713
[1mStep[0m  [4/42], [94mLoss[0m : 2.58660
[1mStep[0m  [8/42], [94mLoss[0m : 2.44621
[1mStep[0m  [12/42], [94mLoss[0m : 2.46359
[1mStep[0m  [16/42], [94mLoss[0m : 2.20857
[1mStep[0m  [20/42], [94mLoss[0m : 2.38104
[1mStep[0m  [24/42], [94mLoss[0m : 2.12973
[1mStep[0m  [28/42], [94mLoss[0m : 2.47678
[1mStep[0m  [32/42], [94mLoss[0m : 2.41123
[1mStep[0m  [36/42], [94mLoss[0m : 2.16122
[1mStep[0m  [40/42], [94mLoss[0m : 2.38594

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41389
[1mStep[0m  [4/42], [94mLoss[0m : 2.21440
[1mStep[0m  [8/42], [94mLoss[0m : 2.19373
[1mStep[0m  [12/42], [94mLoss[0m : 2.44957
[1mStep[0m  [16/42], [94mLoss[0m : 2.44186
[1mStep[0m  [20/42], [94mLoss[0m : 2.49393
[1mStep[0m  [24/42], [94mLoss[0m : 2.32767
[1mStep[0m  [28/42], [94mLoss[0m : 2.21915
[1mStep[0m  [32/42], [94mLoss[0m : 2.10797
[1mStep[0m  [36/42], [94mLoss[0m : 2.12899
[1mStep[0m  [40/42], [94mLoss[0m : 1.94510

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45600
[1mStep[0m  [4/42], [94mLoss[0m : 2.21855
[1mStep[0m  [8/42], [94mLoss[0m : 2.42425
[1mStep[0m  [12/42], [94mLoss[0m : 2.24319
[1mStep[0m  [16/42], [94mLoss[0m : 2.34590
[1mStep[0m  [20/42], [94mLoss[0m : 2.46513
[1mStep[0m  [24/42], [94mLoss[0m : 2.44080
[1mStep[0m  [28/42], [94mLoss[0m : 2.11402
[1mStep[0m  [32/42], [94mLoss[0m : 2.33129
[1mStep[0m  [36/42], [94mLoss[0m : 2.21177
[1mStep[0m  [40/42], [94mLoss[0m : 2.32736

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16433
[1mStep[0m  [4/42], [94mLoss[0m : 2.41482
[1mStep[0m  [8/42], [94mLoss[0m : 2.18906
[1mStep[0m  [12/42], [94mLoss[0m : 2.33435
[1mStep[0m  [16/42], [94mLoss[0m : 2.28246
[1mStep[0m  [20/42], [94mLoss[0m : 2.26074
[1mStep[0m  [24/42], [94mLoss[0m : 2.32933
[1mStep[0m  [28/42], [94mLoss[0m : 2.27853
[1mStep[0m  [32/42], [94mLoss[0m : 2.30642
[1mStep[0m  [36/42], [94mLoss[0m : 2.51264
[1mStep[0m  [40/42], [94mLoss[0m : 2.22555

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38654
[1mStep[0m  [4/42], [94mLoss[0m : 2.38897
[1mStep[0m  [8/42], [94mLoss[0m : 2.48135
[1mStep[0m  [12/42], [94mLoss[0m : 2.15665
[1mStep[0m  [16/42], [94mLoss[0m : 2.50705
[1mStep[0m  [20/42], [94mLoss[0m : 2.16778
[1mStep[0m  [24/42], [94mLoss[0m : 2.36023
[1mStep[0m  [28/42], [94mLoss[0m : 2.20585
[1mStep[0m  [32/42], [94mLoss[0m : 2.13782
[1mStep[0m  [36/42], [94mLoss[0m : 2.34092
[1mStep[0m  [40/42], [94mLoss[0m : 2.60777

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18493
[1mStep[0m  [4/42], [94mLoss[0m : 2.34559
[1mStep[0m  [8/42], [94mLoss[0m : 2.22065
[1mStep[0m  [12/42], [94mLoss[0m : 2.60790
[1mStep[0m  [16/42], [94mLoss[0m : 2.50312
[1mStep[0m  [20/42], [94mLoss[0m : 2.48822
[1mStep[0m  [24/42], [94mLoss[0m : 2.27039
[1mStep[0m  [28/42], [94mLoss[0m : 2.45539
[1mStep[0m  [32/42], [94mLoss[0m : 2.60687
[1mStep[0m  [36/42], [94mLoss[0m : 2.33657
[1mStep[0m  [40/42], [94mLoss[0m : 2.18845

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28774
[1mStep[0m  [4/42], [94mLoss[0m : 2.22631
[1mStep[0m  [8/42], [94mLoss[0m : 2.19992
[1mStep[0m  [12/42], [94mLoss[0m : 2.31283
[1mStep[0m  [16/42], [94mLoss[0m : 2.22494
[1mStep[0m  [20/42], [94mLoss[0m : 2.35565
[1mStep[0m  [24/42], [94mLoss[0m : 2.33239
[1mStep[0m  [28/42], [94mLoss[0m : 2.30931
[1mStep[0m  [32/42], [94mLoss[0m : 2.29561
[1mStep[0m  [36/42], [94mLoss[0m : 2.37013
[1mStep[0m  [40/42], [94mLoss[0m : 2.24907

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.321
====================================

Phase 1 - Evaluation MAE:  2.321413959775652
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.19005
[1mStep[0m  [4/42], [94mLoss[0m : 2.40563
[1mStep[0m  [8/42], [94mLoss[0m : 2.47232
[1mStep[0m  [12/42], [94mLoss[0m : 2.40562
[1mStep[0m  [16/42], [94mLoss[0m : 2.56198
[1mStep[0m  [20/42], [94mLoss[0m : 2.37395
[1mStep[0m  [24/42], [94mLoss[0m : 2.46219
[1mStep[0m  [28/42], [94mLoss[0m : 2.35256
[1mStep[0m  [32/42], [94mLoss[0m : 2.46754
[1mStep[0m  [36/42], [94mLoss[0m : 2.17885
[1mStep[0m  [40/42], [94mLoss[0m : 2.40688

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26423
[1mStep[0m  [4/42], [94mLoss[0m : 2.35032
[1mStep[0m  [8/42], [94mLoss[0m : 2.17208
[1mStep[0m  [12/42], [94mLoss[0m : 2.10491
[1mStep[0m  [16/42], [94mLoss[0m : 2.32084
[1mStep[0m  [20/42], [94mLoss[0m : 2.38847
[1mStep[0m  [24/42], [94mLoss[0m : 2.22181
[1mStep[0m  [28/42], [94mLoss[0m : 2.20417
[1mStep[0m  [32/42], [94mLoss[0m : 2.42341
[1mStep[0m  [36/42], [94mLoss[0m : 2.19365
[1mStep[0m  [40/42], [94mLoss[0m : 2.42404

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25701
[1mStep[0m  [4/42], [94mLoss[0m : 2.13890
[1mStep[0m  [8/42], [94mLoss[0m : 2.34288
[1mStep[0m  [12/42], [94mLoss[0m : 2.35533
[1mStep[0m  [16/42], [94mLoss[0m : 2.37537
[1mStep[0m  [20/42], [94mLoss[0m : 2.12943
[1mStep[0m  [24/42], [94mLoss[0m : 2.23692
[1mStep[0m  [28/42], [94mLoss[0m : 2.08785
[1mStep[0m  [32/42], [94mLoss[0m : 2.25235
[1mStep[0m  [36/42], [94mLoss[0m : 2.24067
[1mStep[0m  [40/42], [94mLoss[0m : 2.19922

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12435
[1mStep[0m  [4/42], [94mLoss[0m : 2.10391
[1mStep[0m  [8/42], [94mLoss[0m : 2.20286
[1mStep[0m  [12/42], [94mLoss[0m : 2.04995
[1mStep[0m  [16/42], [94mLoss[0m : 2.05262
[1mStep[0m  [20/42], [94mLoss[0m : 2.06365
[1mStep[0m  [24/42], [94mLoss[0m : 2.26694
[1mStep[0m  [28/42], [94mLoss[0m : 2.06929
[1mStep[0m  [32/42], [94mLoss[0m : 1.88983
[1mStep[0m  [36/42], [94mLoss[0m : 1.94776
[1mStep[0m  [40/42], [94mLoss[0m : 2.10940

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16194
[1mStep[0m  [4/42], [94mLoss[0m : 1.99791
[1mStep[0m  [8/42], [94mLoss[0m : 1.99599
[1mStep[0m  [12/42], [94mLoss[0m : 2.09799
[1mStep[0m  [16/42], [94mLoss[0m : 2.01911
[1mStep[0m  [20/42], [94mLoss[0m : 2.08253
[1mStep[0m  [24/42], [94mLoss[0m : 1.96898
[1mStep[0m  [28/42], [94mLoss[0m : 1.99647
[1mStep[0m  [32/42], [94mLoss[0m : 2.03030
[1mStep[0m  [36/42], [94mLoss[0m : 1.97552
[1mStep[0m  [40/42], [94mLoss[0m : 1.90452

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92080
[1mStep[0m  [4/42], [94mLoss[0m : 1.97551
[1mStep[0m  [8/42], [94mLoss[0m : 1.89533
[1mStep[0m  [12/42], [94mLoss[0m : 1.86566
[1mStep[0m  [16/42], [94mLoss[0m : 1.96579
[1mStep[0m  [20/42], [94mLoss[0m : 1.98660
[1mStep[0m  [24/42], [94mLoss[0m : 1.87577
[1mStep[0m  [28/42], [94mLoss[0m : 2.19482
[1mStep[0m  [32/42], [94mLoss[0m : 1.97870
[1mStep[0m  [36/42], [94mLoss[0m : 1.97000
[1mStep[0m  [40/42], [94mLoss[0m : 1.88686

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87047
[1mStep[0m  [4/42], [94mLoss[0m : 1.71675
[1mStep[0m  [8/42], [94mLoss[0m : 1.78244
[1mStep[0m  [12/42], [94mLoss[0m : 2.02450
[1mStep[0m  [16/42], [94mLoss[0m : 1.92772
[1mStep[0m  [20/42], [94mLoss[0m : 1.99123
[1mStep[0m  [24/42], [94mLoss[0m : 1.98768
[1mStep[0m  [28/42], [94mLoss[0m : 1.76607
[1mStep[0m  [32/42], [94mLoss[0m : 1.91216
[1mStep[0m  [36/42], [94mLoss[0m : 2.08915
[1mStep[0m  [40/42], [94mLoss[0m : 1.93943

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87747
[1mStep[0m  [4/42], [94mLoss[0m : 1.74256
[1mStep[0m  [8/42], [94mLoss[0m : 1.70739
[1mStep[0m  [12/42], [94mLoss[0m : 1.82494
[1mStep[0m  [16/42], [94mLoss[0m : 1.79076
[1mStep[0m  [20/42], [94mLoss[0m : 1.85684
[1mStep[0m  [24/42], [94mLoss[0m : 1.82262
[1mStep[0m  [28/42], [94mLoss[0m : 1.79079
[1mStep[0m  [32/42], [94mLoss[0m : 1.89989
[1mStep[0m  [36/42], [94mLoss[0m : 1.82321
[1mStep[0m  [40/42], [94mLoss[0m : 2.04067

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.826, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78461
[1mStep[0m  [4/42], [94mLoss[0m : 1.70768
[1mStep[0m  [8/42], [94mLoss[0m : 1.75263
[1mStep[0m  [12/42], [94mLoss[0m : 1.76802
[1mStep[0m  [16/42], [94mLoss[0m : 1.83545
[1mStep[0m  [20/42], [94mLoss[0m : 1.82291
[1mStep[0m  [24/42], [94mLoss[0m : 1.73650
[1mStep[0m  [28/42], [94mLoss[0m : 1.77372
[1mStep[0m  [32/42], [94mLoss[0m : 1.80814
[1mStep[0m  [36/42], [94mLoss[0m : 1.78600
[1mStep[0m  [40/42], [94mLoss[0m : 1.72116

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64953
[1mStep[0m  [4/42], [94mLoss[0m : 1.56262
[1mStep[0m  [8/42], [94mLoss[0m : 1.75982
[1mStep[0m  [12/42], [94mLoss[0m : 1.68643
[1mStep[0m  [16/42], [94mLoss[0m : 1.78424
[1mStep[0m  [20/42], [94mLoss[0m : 1.78487
[1mStep[0m  [24/42], [94mLoss[0m : 1.55875
[1mStep[0m  [28/42], [94mLoss[0m : 1.83268
[1mStep[0m  [32/42], [94mLoss[0m : 1.75743
[1mStep[0m  [36/42], [94mLoss[0m : 1.71236
[1mStep[0m  [40/42], [94mLoss[0m : 1.48608

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.703, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55692
[1mStep[0m  [4/42], [94mLoss[0m : 1.62759
[1mStep[0m  [8/42], [94mLoss[0m : 1.50378
[1mStep[0m  [12/42], [94mLoss[0m : 1.75617
[1mStep[0m  [16/42], [94mLoss[0m : 1.58670
[1mStep[0m  [20/42], [94mLoss[0m : 1.49852
[1mStep[0m  [24/42], [94mLoss[0m : 1.72516
[1mStep[0m  [28/42], [94mLoss[0m : 1.52858
[1mStep[0m  [32/42], [94mLoss[0m : 1.72607
[1mStep[0m  [36/42], [94mLoss[0m : 1.82293
[1mStep[0m  [40/42], [94mLoss[0m : 1.75657

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57366
[1mStep[0m  [4/42], [94mLoss[0m : 1.58635
[1mStep[0m  [8/42], [94mLoss[0m : 1.55329
[1mStep[0m  [12/42], [94mLoss[0m : 1.76340
[1mStep[0m  [16/42], [94mLoss[0m : 1.64155
[1mStep[0m  [20/42], [94mLoss[0m : 1.47647
[1mStep[0m  [24/42], [94mLoss[0m : 1.70354
[1mStep[0m  [28/42], [94mLoss[0m : 1.52961
[1mStep[0m  [32/42], [94mLoss[0m : 1.51470
[1mStep[0m  [36/42], [94mLoss[0m : 1.72997
[1mStep[0m  [40/42], [94mLoss[0m : 1.70390

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48665
[1mStep[0m  [4/42], [94mLoss[0m : 1.56579
[1mStep[0m  [8/42], [94mLoss[0m : 1.61089
[1mStep[0m  [12/42], [94mLoss[0m : 1.56065
[1mStep[0m  [16/42], [94mLoss[0m : 1.58691
[1mStep[0m  [20/42], [94mLoss[0m : 1.49109
[1mStep[0m  [24/42], [94mLoss[0m : 1.60395
[1mStep[0m  [28/42], [94mLoss[0m : 1.47914
[1mStep[0m  [32/42], [94mLoss[0m : 1.81133
[1mStep[0m  [36/42], [94mLoss[0m : 1.67955
[1mStep[0m  [40/42], [94mLoss[0m : 1.42275

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.513, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38732
[1mStep[0m  [4/42], [94mLoss[0m : 1.58237
[1mStep[0m  [8/42], [94mLoss[0m : 1.64341
[1mStep[0m  [12/42], [94mLoss[0m : 1.50885
[1mStep[0m  [16/42], [94mLoss[0m : 1.50571
[1mStep[0m  [20/42], [94mLoss[0m : 1.51098
[1mStep[0m  [24/42], [94mLoss[0m : 1.44667
[1mStep[0m  [28/42], [94mLoss[0m : 1.58319
[1mStep[0m  [32/42], [94mLoss[0m : 1.49766
[1mStep[0m  [36/42], [94mLoss[0m : 1.57064
[1mStep[0m  [40/42], [94mLoss[0m : 1.65690

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37012
[1mStep[0m  [4/42], [94mLoss[0m : 1.51168
[1mStep[0m  [8/42], [94mLoss[0m : 1.58811
[1mStep[0m  [12/42], [94mLoss[0m : 1.49997
[1mStep[0m  [16/42], [94mLoss[0m : 1.44281
[1mStep[0m  [20/42], [94mLoss[0m : 1.49363
[1mStep[0m  [24/42], [94mLoss[0m : 1.58620
[1mStep[0m  [28/42], [94mLoss[0m : 1.49736
[1mStep[0m  [32/42], [94mLoss[0m : 1.55001
[1mStep[0m  [36/42], [94mLoss[0m : 1.48133
[1mStep[0m  [40/42], [94mLoss[0m : 1.51050

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44492
[1mStep[0m  [4/42], [94mLoss[0m : 1.42882
[1mStep[0m  [8/42], [94mLoss[0m : 1.51650
[1mStep[0m  [12/42], [94mLoss[0m : 1.46651
[1mStep[0m  [16/42], [94mLoss[0m : 1.39024
[1mStep[0m  [20/42], [94mLoss[0m : 1.48585
[1mStep[0m  [24/42], [94mLoss[0m : 1.48475
[1mStep[0m  [28/42], [94mLoss[0m : 1.45222
[1mStep[0m  [32/42], [94mLoss[0m : 1.62872
[1mStep[0m  [36/42], [94mLoss[0m : 1.52775
[1mStep[0m  [40/42], [94mLoss[0m : 1.53843

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.494, [92mTest[0m: 2.546, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.34306
[1mStep[0m  [4/42], [94mLoss[0m : 1.45324
[1mStep[0m  [8/42], [94mLoss[0m : 1.42299
[1mStep[0m  [12/42], [94mLoss[0m : 1.31684
[1mStep[0m  [16/42], [94mLoss[0m : 1.31317
[1mStep[0m  [20/42], [94mLoss[0m : 1.59147
[1mStep[0m  [24/42], [94mLoss[0m : 1.61311
[1mStep[0m  [28/42], [94mLoss[0m : 1.45403
[1mStep[0m  [32/42], [94mLoss[0m : 1.48048
[1mStep[0m  [36/42], [94mLoss[0m : 1.62062
[1mStep[0m  [40/42], [94mLoss[0m : 1.53212

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37816
[1mStep[0m  [4/42], [94mLoss[0m : 1.45615
[1mStep[0m  [8/42], [94mLoss[0m : 1.44757
[1mStep[0m  [12/42], [94mLoss[0m : 1.29440
[1mStep[0m  [16/42], [94mLoss[0m : 1.47531
[1mStep[0m  [20/42], [94mLoss[0m : 1.45572
[1mStep[0m  [24/42], [94mLoss[0m : 1.42660
[1mStep[0m  [28/42], [94mLoss[0m : 1.46038
[1mStep[0m  [32/42], [94mLoss[0m : 1.47014
[1mStep[0m  [36/42], [94mLoss[0m : 1.45938
[1mStep[0m  [40/42], [94mLoss[0m : 1.48627

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.440, [92mTest[0m: 2.573, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.39959
[1mStep[0m  [4/42], [94mLoss[0m : 1.28002
[1mStep[0m  [8/42], [94mLoss[0m : 1.17340
[1mStep[0m  [12/42], [94mLoss[0m : 1.58485
[1mStep[0m  [16/42], [94mLoss[0m : 1.23531
[1mStep[0m  [20/42], [94mLoss[0m : 1.36894
[1mStep[0m  [24/42], [94mLoss[0m : 1.28571
[1mStep[0m  [28/42], [94mLoss[0m : 1.43375
[1mStep[0m  [32/42], [94mLoss[0m : 1.29353
[1mStep[0m  [36/42], [94mLoss[0m : 1.27861
[1mStep[0m  [40/42], [94mLoss[0m : 1.32640

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.377, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 18 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.510
====================================

Phase 2 - Evaluation MAE:  2.509774855204991
MAE score P1       2.321414
MAE score P2       2.509775
loss               1.376653
learning_rate          0.01
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 25, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 11.02811
[1mStep[0m  [2/21], [94mLoss[0m : 10.94507
[1mStep[0m  [4/21], [94mLoss[0m : 10.24098
[1mStep[0m  [6/21], [94mLoss[0m : 10.32906
[1mStep[0m  [8/21], [94mLoss[0m : 10.27006
[1mStep[0m  [10/21], [94mLoss[0m : 9.68839
[1mStep[0m  [12/21], [94mLoss[0m : 9.41911
[1mStep[0m  [14/21], [94mLoss[0m : 8.78980
[1mStep[0m  [16/21], [94mLoss[0m : 8.74249
[1mStep[0m  [18/21], [94mLoss[0m : 8.34468
[1mStep[0m  [20/21], [94mLoss[0m : 7.99722

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.601, [92mTest[0m: 11.063, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.15329
[1mStep[0m  [2/21], [94mLoss[0m : 7.95263
[1mStep[0m  [4/21], [94mLoss[0m : 7.11384
[1mStep[0m  [6/21], [94mLoss[0m : 6.96299
[1mStep[0m  [8/21], [94mLoss[0m : 6.70036
[1mStep[0m  [10/21], [94mLoss[0m : 6.46563
[1mStep[0m  [12/21], [94mLoss[0m : 6.33970
[1mStep[0m  [14/21], [94mLoss[0m : 6.09411
[1mStep[0m  [16/21], [94mLoss[0m : 5.57192
[1mStep[0m  [18/21], [94mLoss[0m : 5.24840
[1mStep[0m  [20/21], [94mLoss[0m : 4.94453

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.447, [92mTest[0m: 7.922, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.96641
[1mStep[0m  [2/21], [94mLoss[0m : 4.95645
[1mStep[0m  [4/21], [94mLoss[0m : 4.35276
[1mStep[0m  [6/21], [94mLoss[0m : 4.06296
[1mStep[0m  [8/21], [94mLoss[0m : 4.24833
[1mStep[0m  [10/21], [94mLoss[0m : 3.84994
[1mStep[0m  [12/21], [94mLoss[0m : 3.81411
[1mStep[0m  [14/21], [94mLoss[0m : 3.71015
[1mStep[0m  [16/21], [94mLoss[0m : 3.69500
[1mStep[0m  [18/21], [94mLoss[0m : 3.47463
[1mStep[0m  [20/21], [94mLoss[0m : 3.42521

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.047, [92mTest[0m: 4.879, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.48329
[1mStep[0m  [2/21], [94mLoss[0m : 3.40320
[1mStep[0m  [4/21], [94mLoss[0m : 3.29569
[1mStep[0m  [6/21], [94mLoss[0m : 3.21451
[1mStep[0m  [8/21], [94mLoss[0m : 2.99512
[1mStep[0m  [10/21], [94mLoss[0m : 3.09471
[1mStep[0m  [12/21], [94mLoss[0m : 2.71425
[1mStep[0m  [14/21], [94mLoss[0m : 3.05434
[1mStep[0m  [16/21], [94mLoss[0m : 2.98592
[1mStep[0m  [18/21], [94mLoss[0m : 2.72774
[1mStep[0m  [20/21], [94mLoss[0m : 2.88590

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.063, [92mTest[0m: 3.247, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.87702
[1mStep[0m  [2/21], [94mLoss[0m : 2.87701
[1mStep[0m  [4/21], [94mLoss[0m : 2.94090
[1mStep[0m  [6/21], [94mLoss[0m : 2.93731
[1mStep[0m  [8/21], [94mLoss[0m : 2.67299
[1mStep[0m  [10/21], [94mLoss[0m : 2.74767
[1mStep[0m  [12/21], [94mLoss[0m : 2.60062
[1mStep[0m  [14/21], [94mLoss[0m : 2.65872
[1mStep[0m  [16/21], [94mLoss[0m : 2.53025
[1mStep[0m  [18/21], [94mLoss[0m : 2.65541
[1mStep[0m  [20/21], [94mLoss[0m : 2.64671

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.750, [92mTest[0m: 2.629, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71769
[1mStep[0m  [2/21], [94mLoss[0m : 2.60547
[1mStep[0m  [4/21], [94mLoss[0m : 2.67120
[1mStep[0m  [6/21], [94mLoss[0m : 2.58395
[1mStep[0m  [8/21], [94mLoss[0m : 2.82328
[1mStep[0m  [10/21], [94mLoss[0m : 2.71404
[1mStep[0m  [12/21], [94mLoss[0m : 2.63838
[1mStep[0m  [14/21], [94mLoss[0m : 2.61173
[1mStep[0m  [16/21], [94mLoss[0m : 2.75957
[1mStep[0m  [18/21], [94mLoss[0m : 2.59629
[1mStep[0m  [20/21], [94mLoss[0m : 2.56782

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78296
[1mStep[0m  [2/21], [94mLoss[0m : 2.57178
[1mStep[0m  [4/21], [94mLoss[0m : 2.55796
[1mStep[0m  [6/21], [94mLoss[0m : 2.74465
[1mStep[0m  [8/21], [94mLoss[0m : 2.70239
[1mStep[0m  [10/21], [94mLoss[0m : 2.51672
[1mStep[0m  [12/21], [94mLoss[0m : 2.69701
[1mStep[0m  [14/21], [94mLoss[0m : 2.59413
[1mStep[0m  [16/21], [94mLoss[0m : 2.64647
[1mStep[0m  [18/21], [94mLoss[0m : 2.65403
[1mStep[0m  [20/21], [94mLoss[0m : 2.67241

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67309
[1mStep[0m  [2/21], [94mLoss[0m : 2.67995
[1mStep[0m  [4/21], [94mLoss[0m : 2.63907
[1mStep[0m  [6/21], [94mLoss[0m : 2.62278
[1mStep[0m  [8/21], [94mLoss[0m : 2.48224
[1mStep[0m  [10/21], [94mLoss[0m : 2.55592
[1mStep[0m  [12/21], [94mLoss[0m : 2.54800
[1mStep[0m  [14/21], [94mLoss[0m : 2.65417
[1mStep[0m  [16/21], [94mLoss[0m : 2.57299
[1mStep[0m  [18/21], [94mLoss[0m : 2.59005
[1mStep[0m  [20/21], [94mLoss[0m : 2.56872

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59593
[1mStep[0m  [2/21], [94mLoss[0m : 2.57211
[1mStep[0m  [4/21], [94mLoss[0m : 2.54425
[1mStep[0m  [6/21], [94mLoss[0m : 2.52821
[1mStep[0m  [8/21], [94mLoss[0m : 2.69829
[1mStep[0m  [10/21], [94mLoss[0m : 2.49311
[1mStep[0m  [12/21], [94mLoss[0m : 2.64359
[1mStep[0m  [14/21], [94mLoss[0m : 2.62612
[1mStep[0m  [16/21], [94mLoss[0m : 2.56396
[1mStep[0m  [18/21], [94mLoss[0m : 2.69911
[1mStep[0m  [20/21], [94mLoss[0m : 2.60225

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65909
[1mStep[0m  [2/21], [94mLoss[0m : 2.44595
[1mStep[0m  [4/21], [94mLoss[0m : 2.60654
[1mStep[0m  [6/21], [94mLoss[0m : 2.42881
[1mStep[0m  [8/21], [94mLoss[0m : 2.66729
[1mStep[0m  [10/21], [94mLoss[0m : 2.68046
[1mStep[0m  [12/21], [94mLoss[0m : 2.48955
[1mStep[0m  [14/21], [94mLoss[0m : 2.65135
[1mStep[0m  [16/21], [94mLoss[0m : 2.69456
[1mStep[0m  [18/21], [94mLoss[0m : 2.70165
[1mStep[0m  [20/21], [94mLoss[0m : 2.57891

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57600
[1mStep[0m  [2/21], [94mLoss[0m : 2.69746
[1mStep[0m  [4/21], [94mLoss[0m : 2.63372
[1mStep[0m  [6/21], [94mLoss[0m : 2.55479
[1mStep[0m  [8/21], [94mLoss[0m : 2.52641
[1mStep[0m  [10/21], [94mLoss[0m : 2.77797
[1mStep[0m  [12/21], [94mLoss[0m : 2.58970
[1mStep[0m  [14/21], [94mLoss[0m : 2.52271
[1mStep[0m  [16/21], [94mLoss[0m : 2.55907
[1mStep[0m  [18/21], [94mLoss[0m : 2.62421
[1mStep[0m  [20/21], [94mLoss[0m : 2.73556

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59206
[1mStep[0m  [2/21], [94mLoss[0m : 2.60348
[1mStep[0m  [4/21], [94mLoss[0m : 2.60464
[1mStep[0m  [6/21], [94mLoss[0m : 2.45551
[1mStep[0m  [8/21], [94mLoss[0m : 2.65510
[1mStep[0m  [10/21], [94mLoss[0m : 2.69030
[1mStep[0m  [12/21], [94mLoss[0m : 2.46773
[1mStep[0m  [14/21], [94mLoss[0m : 2.71670
[1mStep[0m  [16/21], [94mLoss[0m : 2.44021
[1mStep[0m  [18/21], [94mLoss[0m : 2.56950
[1mStep[0m  [20/21], [94mLoss[0m : 2.67440

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61867
[1mStep[0m  [2/21], [94mLoss[0m : 2.56298
[1mStep[0m  [4/21], [94mLoss[0m : 2.72234
[1mStep[0m  [6/21], [94mLoss[0m : 2.50496
[1mStep[0m  [8/21], [94mLoss[0m : 2.50216
[1mStep[0m  [10/21], [94mLoss[0m : 2.57912
[1mStep[0m  [12/21], [94mLoss[0m : 2.41363
[1mStep[0m  [14/21], [94mLoss[0m : 2.58576
[1mStep[0m  [16/21], [94mLoss[0m : 2.63555
[1mStep[0m  [18/21], [94mLoss[0m : 2.61571
[1mStep[0m  [20/21], [94mLoss[0m : 2.75539

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35406
[1mStep[0m  [2/21], [94mLoss[0m : 2.52193
[1mStep[0m  [4/21], [94mLoss[0m : 2.65252
[1mStep[0m  [6/21], [94mLoss[0m : 2.47970
[1mStep[0m  [8/21], [94mLoss[0m : 2.56789
[1mStep[0m  [10/21], [94mLoss[0m : 2.59420
[1mStep[0m  [12/21], [94mLoss[0m : 2.55513
[1mStep[0m  [14/21], [94mLoss[0m : 2.74781
[1mStep[0m  [16/21], [94mLoss[0m : 2.78137
[1mStep[0m  [18/21], [94mLoss[0m : 2.52670
[1mStep[0m  [20/21], [94mLoss[0m : 2.77643

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80708
[1mStep[0m  [2/21], [94mLoss[0m : 2.46150
[1mStep[0m  [4/21], [94mLoss[0m : 2.37634
[1mStep[0m  [6/21], [94mLoss[0m : 2.54205
[1mStep[0m  [8/21], [94mLoss[0m : 2.50634
[1mStep[0m  [10/21], [94mLoss[0m : 2.68891
[1mStep[0m  [12/21], [94mLoss[0m : 2.49646
[1mStep[0m  [14/21], [94mLoss[0m : 2.74420
[1mStep[0m  [16/21], [94mLoss[0m : 2.38235
[1mStep[0m  [18/21], [94mLoss[0m : 2.56405
[1mStep[0m  [20/21], [94mLoss[0m : 2.53471

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59307
[1mStep[0m  [2/21], [94mLoss[0m : 2.60069
[1mStep[0m  [4/21], [94mLoss[0m : 2.47479
[1mStep[0m  [6/21], [94mLoss[0m : 2.57055
[1mStep[0m  [8/21], [94mLoss[0m : 2.65704
[1mStep[0m  [10/21], [94mLoss[0m : 2.61495
[1mStep[0m  [12/21], [94mLoss[0m : 2.60457
[1mStep[0m  [14/21], [94mLoss[0m : 2.50008
[1mStep[0m  [16/21], [94mLoss[0m : 2.59184
[1mStep[0m  [18/21], [94mLoss[0m : 2.54221
[1mStep[0m  [20/21], [94mLoss[0m : 2.59966

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73851
[1mStep[0m  [2/21], [94mLoss[0m : 2.61058
[1mStep[0m  [4/21], [94mLoss[0m : 2.50608
[1mStep[0m  [6/21], [94mLoss[0m : 2.66809
[1mStep[0m  [8/21], [94mLoss[0m : 2.64189
[1mStep[0m  [10/21], [94mLoss[0m : 2.69760
[1mStep[0m  [12/21], [94mLoss[0m : 2.53569
[1mStep[0m  [14/21], [94mLoss[0m : 2.54266
[1mStep[0m  [16/21], [94mLoss[0m : 2.45643
[1mStep[0m  [18/21], [94mLoss[0m : 2.73405
[1mStep[0m  [20/21], [94mLoss[0m : 2.50661

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46359
[1mStep[0m  [2/21], [94mLoss[0m : 2.60876
[1mStep[0m  [4/21], [94mLoss[0m : 2.55929
[1mStep[0m  [6/21], [94mLoss[0m : 2.57605
[1mStep[0m  [8/21], [94mLoss[0m : 2.55724
[1mStep[0m  [10/21], [94mLoss[0m : 2.61613
[1mStep[0m  [12/21], [94mLoss[0m : 2.44493
[1mStep[0m  [14/21], [94mLoss[0m : 2.50819
[1mStep[0m  [16/21], [94mLoss[0m : 2.42989
[1mStep[0m  [18/21], [94mLoss[0m : 2.57055
[1mStep[0m  [20/21], [94mLoss[0m : 2.55772

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51898
[1mStep[0m  [2/21], [94mLoss[0m : 2.65967
[1mStep[0m  [4/21], [94mLoss[0m : 2.48412
[1mStep[0m  [6/21], [94mLoss[0m : 2.51146
[1mStep[0m  [8/21], [94mLoss[0m : 2.45424
[1mStep[0m  [10/21], [94mLoss[0m : 2.60033
[1mStep[0m  [12/21], [94mLoss[0m : 2.51286
[1mStep[0m  [14/21], [94mLoss[0m : 2.55844
[1mStep[0m  [16/21], [94mLoss[0m : 2.49714
[1mStep[0m  [18/21], [94mLoss[0m : 2.68821
[1mStep[0m  [20/21], [94mLoss[0m : 2.50971

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54369
[1mStep[0m  [2/21], [94mLoss[0m : 2.56567
[1mStep[0m  [4/21], [94mLoss[0m : 2.68310
[1mStep[0m  [6/21], [94mLoss[0m : 2.67775
[1mStep[0m  [8/21], [94mLoss[0m : 2.55817
[1mStep[0m  [10/21], [94mLoss[0m : 2.48846
[1mStep[0m  [12/21], [94mLoss[0m : 2.58021
[1mStep[0m  [14/21], [94mLoss[0m : 2.51377
[1mStep[0m  [16/21], [94mLoss[0m : 2.64117
[1mStep[0m  [18/21], [94mLoss[0m : 2.48829
[1mStep[0m  [20/21], [94mLoss[0m : 2.56220

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63766
[1mStep[0m  [2/21], [94mLoss[0m : 2.66560
[1mStep[0m  [4/21], [94mLoss[0m : 2.45001
[1mStep[0m  [6/21], [94mLoss[0m : 2.62879
[1mStep[0m  [8/21], [94mLoss[0m : 2.51177
[1mStep[0m  [10/21], [94mLoss[0m : 2.61478
[1mStep[0m  [12/21], [94mLoss[0m : 2.51857
[1mStep[0m  [14/21], [94mLoss[0m : 2.52902
[1mStep[0m  [16/21], [94mLoss[0m : 2.48813
[1mStep[0m  [18/21], [94mLoss[0m : 2.56701
[1mStep[0m  [20/21], [94mLoss[0m : 2.66751

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44856
[1mStep[0m  [2/21], [94mLoss[0m : 2.60026
[1mStep[0m  [4/21], [94mLoss[0m : 2.72235
[1mStep[0m  [6/21], [94mLoss[0m : 2.53993
[1mStep[0m  [8/21], [94mLoss[0m : 2.59443
[1mStep[0m  [10/21], [94mLoss[0m : 2.51509
[1mStep[0m  [12/21], [94mLoss[0m : 2.55524
[1mStep[0m  [14/21], [94mLoss[0m : 2.65332
[1mStep[0m  [16/21], [94mLoss[0m : 2.62138
[1mStep[0m  [18/21], [94mLoss[0m : 2.46899
[1mStep[0m  [20/21], [94mLoss[0m : 2.51144

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49657
[1mStep[0m  [2/21], [94mLoss[0m : 2.54769
[1mStep[0m  [4/21], [94mLoss[0m : 2.53432
[1mStep[0m  [6/21], [94mLoss[0m : 2.61377
[1mStep[0m  [8/21], [94mLoss[0m : 2.73109
[1mStep[0m  [10/21], [94mLoss[0m : 2.54056
[1mStep[0m  [12/21], [94mLoss[0m : 2.52312
[1mStep[0m  [14/21], [94mLoss[0m : 2.51809
[1mStep[0m  [16/21], [94mLoss[0m : 2.52057
[1mStep[0m  [18/21], [94mLoss[0m : 2.46288
[1mStep[0m  [20/21], [94mLoss[0m : 2.51988

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58393
[1mStep[0m  [2/21], [94mLoss[0m : 2.61546
[1mStep[0m  [4/21], [94mLoss[0m : 2.51780
[1mStep[0m  [6/21], [94mLoss[0m : 2.54893
[1mStep[0m  [8/21], [94mLoss[0m : 2.34252
[1mStep[0m  [10/21], [94mLoss[0m : 2.61895
[1mStep[0m  [12/21], [94mLoss[0m : 2.52739
[1mStep[0m  [14/21], [94mLoss[0m : 2.70159
[1mStep[0m  [16/21], [94mLoss[0m : 2.60591
[1mStep[0m  [18/21], [94mLoss[0m : 2.54580
[1mStep[0m  [20/21], [94mLoss[0m : 2.58346

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60538
[1mStep[0m  [2/21], [94mLoss[0m : 2.52968
[1mStep[0m  [4/21], [94mLoss[0m : 2.56118
[1mStep[0m  [6/21], [94mLoss[0m : 2.53019
[1mStep[0m  [8/21], [94mLoss[0m : 2.51869
[1mStep[0m  [10/21], [94mLoss[0m : 2.53675
[1mStep[0m  [12/21], [94mLoss[0m : 2.63639
[1mStep[0m  [14/21], [94mLoss[0m : 2.48637
[1mStep[0m  [16/21], [94mLoss[0m : 2.44499
[1mStep[0m  [18/21], [94mLoss[0m : 2.49214
[1mStep[0m  [20/21], [94mLoss[0m : 2.64579

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40627
[1mStep[0m  [2/21], [94mLoss[0m : 2.47948
[1mStep[0m  [4/21], [94mLoss[0m : 2.44115
[1mStep[0m  [6/21], [94mLoss[0m : 2.48406
[1mStep[0m  [8/21], [94mLoss[0m : 2.58743
[1mStep[0m  [10/21], [94mLoss[0m : 2.53808
[1mStep[0m  [12/21], [94mLoss[0m : 2.58964
[1mStep[0m  [14/21], [94mLoss[0m : 2.63412
[1mStep[0m  [16/21], [94mLoss[0m : 2.63341
[1mStep[0m  [18/21], [94mLoss[0m : 2.64289
[1mStep[0m  [20/21], [94mLoss[0m : 2.49708

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53565
[1mStep[0m  [2/21], [94mLoss[0m : 2.42193
[1mStep[0m  [4/21], [94mLoss[0m : 2.71791
[1mStep[0m  [6/21], [94mLoss[0m : 2.52610
[1mStep[0m  [8/21], [94mLoss[0m : 2.51977
[1mStep[0m  [10/21], [94mLoss[0m : 2.54021
[1mStep[0m  [12/21], [94mLoss[0m : 2.42862
[1mStep[0m  [14/21], [94mLoss[0m : 2.44132
[1mStep[0m  [16/21], [94mLoss[0m : 2.61485
[1mStep[0m  [18/21], [94mLoss[0m : 2.67724
[1mStep[0m  [20/21], [94mLoss[0m : 2.54158

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55533
[1mStep[0m  [2/21], [94mLoss[0m : 2.54973
[1mStep[0m  [4/21], [94mLoss[0m : 2.54988
[1mStep[0m  [6/21], [94mLoss[0m : 2.50731
[1mStep[0m  [8/21], [94mLoss[0m : 2.67514
[1mStep[0m  [10/21], [94mLoss[0m : 2.48729
[1mStep[0m  [12/21], [94mLoss[0m : 2.51869
[1mStep[0m  [14/21], [94mLoss[0m : 2.74475
[1mStep[0m  [16/21], [94mLoss[0m : 2.57418
[1mStep[0m  [18/21], [94mLoss[0m : 2.48857
[1mStep[0m  [20/21], [94mLoss[0m : 2.51577

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41236
[1mStep[0m  [2/21], [94mLoss[0m : 2.33040
[1mStep[0m  [4/21], [94mLoss[0m : 2.64444
[1mStep[0m  [6/21], [94mLoss[0m : 2.45447
[1mStep[0m  [8/21], [94mLoss[0m : 2.57431
[1mStep[0m  [10/21], [94mLoss[0m : 2.58254
[1mStep[0m  [12/21], [94mLoss[0m : 2.61385
[1mStep[0m  [14/21], [94mLoss[0m : 2.54880
[1mStep[0m  [16/21], [94mLoss[0m : 2.57885
[1mStep[0m  [18/21], [94mLoss[0m : 2.65016
[1mStep[0m  [20/21], [94mLoss[0m : 2.47984

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59806
[1mStep[0m  [2/21], [94mLoss[0m : 2.48851
[1mStep[0m  [4/21], [94mLoss[0m : 2.57326
[1mStep[0m  [6/21], [94mLoss[0m : 2.59717
[1mStep[0m  [8/21], [94mLoss[0m : 2.48221
[1mStep[0m  [10/21], [94mLoss[0m : 2.51289
[1mStep[0m  [12/21], [94mLoss[0m : 2.53588
[1mStep[0m  [14/21], [94mLoss[0m : 2.57910
[1mStep[0m  [16/21], [94mLoss[0m : 2.64476
[1mStep[0m  [18/21], [94mLoss[0m : 2.70101
[1mStep[0m  [20/21], [94mLoss[0m : 2.41468

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.349
====================================

Phase 1 - Evaluation MAE:  2.3492976937975203
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.59009
[1mStep[0m  [2/21], [94mLoss[0m : 2.56710
[1mStep[0m  [4/21], [94mLoss[0m : 2.50527
[1mStep[0m  [6/21], [94mLoss[0m : 2.58948
[1mStep[0m  [8/21], [94mLoss[0m : 2.61052
[1mStep[0m  [10/21], [94mLoss[0m : 2.50309
[1mStep[0m  [12/21], [94mLoss[0m : 2.48912
[1mStep[0m  [14/21], [94mLoss[0m : 2.55334
[1mStep[0m  [16/21], [94mLoss[0m : 2.52961
[1mStep[0m  [18/21], [94mLoss[0m : 2.50982
[1mStep[0m  [20/21], [94mLoss[0m : 2.51763

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48078
[1mStep[0m  [2/21], [94mLoss[0m : 2.55198
[1mStep[0m  [4/21], [94mLoss[0m : 2.61090
[1mStep[0m  [6/21], [94mLoss[0m : 2.35806
[1mStep[0m  [8/21], [94mLoss[0m : 2.50330
[1mStep[0m  [10/21], [94mLoss[0m : 2.65782
[1mStep[0m  [12/21], [94mLoss[0m : 2.68616
[1mStep[0m  [14/21], [94mLoss[0m : 2.47766
[1mStep[0m  [16/21], [94mLoss[0m : 2.45573
[1mStep[0m  [18/21], [94mLoss[0m : 2.57040
[1mStep[0m  [20/21], [94mLoss[0m : 2.52504

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51122
[1mStep[0m  [2/21], [94mLoss[0m : 2.63554
[1mStep[0m  [4/21], [94mLoss[0m : 2.60466
[1mStep[0m  [6/21], [94mLoss[0m : 2.37262
[1mStep[0m  [8/21], [94mLoss[0m : 2.57053
[1mStep[0m  [10/21], [94mLoss[0m : 2.54031
[1mStep[0m  [12/21], [94mLoss[0m : 2.52315
[1mStep[0m  [14/21], [94mLoss[0m : 2.54794
[1mStep[0m  [16/21], [94mLoss[0m : 2.68363
[1mStep[0m  [18/21], [94mLoss[0m : 2.55295
[1mStep[0m  [20/21], [94mLoss[0m : 2.43851

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47373
[1mStep[0m  [2/21], [94mLoss[0m : 2.50965
[1mStep[0m  [4/21], [94mLoss[0m : 2.42258
[1mStep[0m  [6/21], [94mLoss[0m : 2.52788
[1mStep[0m  [8/21], [94mLoss[0m : 2.46156
[1mStep[0m  [10/21], [94mLoss[0m : 2.40964
[1mStep[0m  [12/21], [94mLoss[0m : 2.55407
[1mStep[0m  [14/21], [94mLoss[0m : 2.54861
[1mStep[0m  [16/21], [94mLoss[0m : 2.57115
[1mStep[0m  [18/21], [94mLoss[0m : 2.50819
[1mStep[0m  [20/21], [94mLoss[0m : 2.47212

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46368
[1mStep[0m  [2/21], [94mLoss[0m : 2.31677
[1mStep[0m  [4/21], [94mLoss[0m : 2.43833
[1mStep[0m  [6/21], [94mLoss[0m : 2.43941
[1mStep[0m  [8/21], [94mLoss[0m : 2.62423
[1mStep[0m  [10/21], [94mLoss[0m : 2.44946
[1mStep[0m  [12/21], [94mLoss[0m : 2.45073
[1mStep[0m  [14/21], [94mLoss[0m : 2.38066
[1mStep[0m  [16/21], [94mLoss[0m : 2.67896
[1mStep[0m  [18/21], [94mLoss[0m : 2.51913
[1mStep[0m  [20/21], [94mLoss[0m : 2.54858

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33841
[1mStep[0m  [2/21], [94mLoss[0m : 2.44704
[1mStep[0m  [4/21], [94mLoss[0m : 2.40081
[1mStep[0m  [6/21], [94mLoss[0m : 2.56001
[1mStep[0m  [8/21], [94mLoss[0m : 2.60247
[1mStep[0m  [10/21], [94mLoss[0m : 2.50477
[1mStep[0m  [12/21], [94mLoss[0m : 2.46458
[1mStep[0m  [14/21], [94mLoss[0m : 2.60749
[1mStep[0m  [16/21], [94mLoss[0m : 2.52720
[1mStep[0m  [18/21], [94mLoss[0m : 2.50360
[1mStep[0m  [20/21], [94mLoss[0m : 2.60234

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40085
[1mStep[0m  [2/21], [94mLoss[0m : 2.60074
[1mStep[0m  [4/21], [94mLoss[0m : 2.66534
[1mStep[0m  [6/21], [94mLoss[0m : 2.49729
[1mStep[0m  [8/21], [94mLoss[0m : 2.47625
[1mStep[0m  [10/21], [94mLoss[0m : 2.64947
[1mStep[0m  [12/21], [94mLoss[0m : 2.54642
[1mStep[0m  [14/21], [94mLoss[0m : 2.48569
[1mStep[0m  [16/21], [94mLoss[0m : 2.39503
[1mStep[0m  [18/21], [94mLoss[0m : 2.48484
[1mStep[0m  [20/21], [94mLoss[0m : 2.51929

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44727
[1mStep[0m  [2/21], [94mLoss[0m : 2.45019
[1mStep[0m  [4/21], [94mLoss[0m : 2.54266
[1mStep[0m  [6/21], [94mLoss[0m : 2.52999
[1mStep[0m  [8/21], [94mLoss[0m : 2.60052
[1mStep[0m  [10/21], [94mLoss[0m : 2.49101
[1mStep[0m  [12/21], [94mLoss[0m : 2.30778
[1mStep[0m  [14/21], [94mLoss[0m : 2.55418
[1mStep[0m  [16/21], [94mLoss[0m : 2.56025
[1mStep[0m  [18/21], [94mLoss[0m : 2.63344
[1mStep[0m  [20/21], [94mLoss[0m : 2.38949

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44336
[1mStep[0m  [2/21], [94mLoss[0m : 2.50420
[1mStep[0m  [4/21], [94mLoss[0m : 2.57099
[1mStep[0m  [6/21], [94mLoss[0m : 2.38071
[1mStep[0m  [8/21], [94mLoss[0m : 2.50388
[1mStep[0m  [10/21], [94mLoss[0m : 2.43705
[1mStep[0m  [12/21], [94mLoss[0m : 2.58641
[1mStep[0m  [14/21], [94mLoss[0m : 2.38151
[1mStep[0m  [16/21], [94mLoss[0m : 2.48744
[1mStep[0m  [18/21], [94mLoss[0m : 2.44193
[1mStep[0m  [20/21], [94mLoss[0m : 2.50709

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53577
[1mStep[0m  [2/21], [94mLoss[0m : 2.43267
[1mStep[0m  [4/21], [94mLoss[0m : 2.55817
[1mStep[0m  [6/21], [94mLoss[0m : 2.42819
[1mStep[0m  [8/21], [94mLoss[0m : 2.61408
[1mStep[0m  [10/21], [94mLoss[0m : 2.27525
[1mStep[0m  [12/21], [94mLoss[0m : 2.43296
[1mStep[0m  [14/21], [94mLoss[0m : 2.49795
[1mStep[0m  [16/21], [94mLoss[0m : 2.37867
[1mStep[0m  [18/21], [94mLoss[0m : 2.45907
[1mStep[0m  [20/21], [94mLoss[0m : 2.69993

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45964
[1mStep[0m  [2/21], [94mLoss[0m : 2.56846
[1mStep[0m  [4/21], [94mLoss[0m : 2.39605
[1mStep[0m  [6/21], [94mLoss[0m : 2.41872
[1mStep[0m  [8/21], [94mLoss[0m : 2.33997
[1mStep[0m  [10/21], [94mLoss[0m : 2.53437
[1mStep[0m  [12/21], [94mLoss[0m : 2.49766
[1mStep[0m  [14/21], [94mLoss[0m : 2.42991
[1mStep[0m  [16/21], [94mLoss[0m : 2.55425
[1mStep[0m  [18/21], [94mLoss[0m : 2.54806
[1mStep[0m  [20/21], [94mLoss[0m : 2.43275

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.549, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19386
[1mStep[0m  [2/21], [94mLoss[0m : 2.20603
[1mStep[0m  [4/21], [94mLoss[0m : 2.45819
[1mStep[0m  [6/21], [94mLoss[0m : 2.32472
[1mStep[0m  [8/21], [94mLoss[0m : 2.39195
[1mStep[0m  [10/21], [94mLoss[0m : 2.42100
[1mStep[0m  [12/21], [94mLoss[0m : 2.53370
[1mStep[0m  [14/21], [94mLoss[0m : 2.60493
[1mStep[0m  [16/21], [94mLoss[0m : 2.39333
[1mStep[0m  [18/21], [94mLoss[0m : 2.46210
[1mStep[0m  [20/21], [94mLoss[0m : 2.31586

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48855
[1mStep[0m  [2/21], [94mLoss[0m : 2.69314
[1mStep[0m  [4/21], [94mLoss[0m : 2.24504
[1mStep[0m  [6/21], [94mLoss[0m : 2.64663
[1mStep[0m  [8/21], [94mLoss[0m : 2.37299
[1mStep[0m  [10/21], [94mLoss[0m : 2.39188
[1mStep[0m  [12/21], [94mLoss[0m : 2.35246
[1mStep[0m  [14/21], [94mLoss[0m : 2.39053
[1mStep[0m  [16/21], [94mLoss[0m : 2.50982
[1mStep[0m  [18/21], [94mLoss[0m : 2.38270
[1mStep[0m  [20/21], [94mLoss[0m : 2.56725

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.591, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39940
[1mStep[0m  [2/21], [94mLoss[0m : 2.29290
[1mStep[0m  [4/21], [94mLoss[0m : 2.37435
[1mStep[0m  [6/21], [94mLoss[0m : 2.52536
[1mStep[0m  [8/21], [94mLoss[0m : 2.22008
[1mStep[0m  [10/21], [94mLoss[0m : 2.35754
[1mStep[0m  [12/21], [94mLoss[0m : 2.37381
[1mStep[0m  [14/21], [94mLoss[0m : 2.58406
[1mStep[0m  [16/21], [94mLoss[0m : 2.35954
[1mStep[0m  [18/21], [94mLoss[0m : 2.27371
[1mStep[0m  [20/21], [94mLoss[0m : 2.44161

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.538, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32645
[1mStep[0m  [2/21], [94mLoss[0m : 2.42961
[1mStep[0m  [4/21], [94mLoss[0m : 2.41650
[1mStep[0m  [6/21], [94mLoss[0m : 2.32202
[1mStep[0m  [8/21], [94mLoss[0m : 2.43549
[1mStep[0m  [10/21], [94mLoss[0m : 2.37681
[1mStep[0m  [12/21], [94mLoss[0m : 2.38247
[1mStep[0m  [14/21], [94mLoss[0m : 2.31571
[1mStep[0m  [16/21], [94mLoss[0m : 2.52576
[1mStep[0m  [18/21], [94mLoss[0m : 2.38373
[1mStep[0m  [20/21], [94mLoss[0m : 2.28388

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.552, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48035
[1mStep[0m  [2/21], [94mLoss[0m : 2.40619
[1mStep[0m  [4/21], [94mLoss[0m : 2.52203
[1mStep[0m  [6/21], [94mLoss[0m : 2.34714
[1mStep[0m  [8/21], [94mLoss[0m : 2.39310
[1mStep[0m  [10/21], [94mLoss[0m : 2.39645
[1mStep[0m  [12/21], [94mLoss[0m : 2.38463
[1mStep[0m  [14/21], [94mLoss[0m : 2.48542
[1mStep[0m  [16/21], [94mLoss[0m : 2.44524
[1mStep[0m  [18/21], [94mLoss[0m : 2.30952
[1mStep[0m  [20/21], [94mLoss[0m : 2.20042

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.631, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36601
[1mStep[0m  [2/21], [94mLoss[0m : 2.38600
[1mStep[0m  [4/21], [94mLoss[0m : 2.48873
[1mStep[0m  [6/21], [94mLoss[0m : 2.33407
[1mStep[0m  [8/21], [94mLoss[0m : 2.49402
[1mStep[0m  [10/21], [94mLoss[0m : 2.43675
[1mStep[0m  [12/21], [94mLoss[0m : 2.33186
[1mStep[0m  [14/21], [94mLoss[0m : 2.25722
[1mStep[0m  [16/21], [94mLoss[0m : 2.14294
[1mStep[0m  [18/21], [94mLoss[0m : 2.39704
[1mStep[0m  [20/21], [94mLoss[0m : 2.25674

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.592, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29328
[1mStep[0m  [2/21], [94mLoss[0m : 2.28183
[1mStep[0m  [4/21], [94mLoss[0m : 2.52420
[1mStep[0m  [6/21], [94mLoss[0m : 2.34420
[1mStep[0m  [8/21], [94mLoss[0m : 2.36791
[1mStep[0m  [10/21], [94mLoss[0m : 2.35372
[1mStep[0m  [12/21], [94mLoss[0m : 2.30573
[1mStep[0m  [14/21], [94mLoss[0m : 2.25353
[1mStep[0m  [16/21], [94mLoss[0m : 2.36669
[1mStep[0m  [18/21], [94mLoss[0m : 2.33600
[1mStep[0m  [20/21], [94mLoss[0m : 2.52483

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.667, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37630
[1mStep[0m  [2/21], [94mLoss[0m : 2.35287
[1mStep[0m  [4/21], [94mLoss[0m : 2.33734
[1mStep[0m  [6/21], [94mLoss[0m : 2.42301
[1mStep[0m  [8/21], [94mLoss[0m : 2.38389
[1mStep[0m  [10/21], [94mLoss[0m : 2.17110
[1mStep[0m  [12/21], [94mLoss[0m : 2.30772
[1mStep[0m  [14/21], [94mLoss[0m : 2.33376
[1mStep[0m  [16/21], [94mLoss[0m : 2.29872
[1mStep[0m  [18/21], [94mLoss[0m : 2.24627
[1mStep[0m  [20/21], [94mLoss[0m : 2.38419

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.617, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28248
[1mStep[0m  [2/21], [94mLoss[0m : 2.37991
[1mStep[0m  [4/21], [94mLoss[0m : 2.43617
[1mStep[0m  [6/21], [94mLoss[0m : 2.31019
[1mStep[0m  [8/21], [94mLoss[0m : 2.24532
[1mStep[0m  [10/21], [94mLoss[0m : 2.44713
[1mStep[0m  [12/21], [94mLoss[0m : 2.35011
[1mStep[0m  [14/21], [94mLoss[0m : 2.36261
[1mStep[0m  [16/21], [94mLoss[0m : 2.17076
[1mStep[0m  [18/21], [94mLoss[0m : 2.39690
[1mStep[0m  [20/21], [94mLoss[0m : 2.05186

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.611, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34866
[1mStep[0m  [2/21], [94mLoss[0m : 2.28945
[1mStep[0m  [4/21], [94mLoss[0m : 2.27394
[1mStep[0m  [6/21], [94mLoss[0m : 2.31154
[1mStep[0m  [8/21], [94mLoss[0m : 2.35939
[1mStep[0m  [10/21], [94mLoss[0m : 2.27667
[1mStep[0m  [12/21], [94mLoss[0m : 2.08379
[1mStep[0m  [14/21], [94mLoss[0m : 2.23699
[1mStep[0m  [16/21], [94mLoss[0m : 2.31477
[1mStep[0m  [18/21], [94mLoss[0m : 2.26465
[1mStep[0m  [20/21], [94mLoss[0m : 2.45410

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.568, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33073
[1mStep[0m  [2/21], [94mLoss[0m : 2.26367
[1mStep[0m  [4/21], [94mLoss[0m : 2.16696
[1mStep[0m  [6/21], [94mLoss[0m : 2.35147
[1mStep[0m  [8/21], [94mLoss[0m : 2.37482
[1mStep[0m  [10/21], [94mLoss[0m : 2.16467
[1mStep[0m  [12/21], [94mLoss[0m : 2.34937
[1mStep[0m  [14/21], [94mLoss[0m : 2.41179
[1mStep[0m  [16/21], [94mLoss[0m : 2.25672
[1mStep[0m  [18/21], [94mLoss[0m : 2.16635
[1mStep[0m  [20/21], [94mLoss[0m : 2.22788

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.703, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36537
[1mStep[0m  [2/21], [94mLoss[0m : 2.24304
[1mStep[0m  [4/21], [94mLoss[0m : 2.13076
[1mStep[0m  [6/21], [94mLoss[0m : 2.21503
[1mStep[0m  [8/21], [94mLoss[0m : 2.10064
[1mStep[0m  [10/21], [94mLoss[0m : 2.30888
[1mStep[0m  [12/21], [94mLoss[0m : 2.24235
[1mStep[0m  [14/21], [94mLoss[0m : 2.28299
[1mStep[0m  [16/21], [94mLoss[0m : 2.25586
[1mStep[0m  [18/21], [94mLoss[0m : 2.19963
[1mStep[0m  [20/21], [94mLoss[0m : 2.24591

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.644, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.12842
[1mStep[0m  [2/21], [94mLoss[0m : 2.28588
[1mStep[0m  [4/21], [94mLoss[0m : 2.26613
[1mStep[0m  [6/21], [94mLoss[0m : 2.30592
[1mStep[0m  [8/21], [94mLoss[0m : 2.19326
[1mStep[0m  [10/21], [94mLoss[0m : 2.23844
[1mStep[0m  [12/21], [94mLoss[0m : 2.21801
[1mStep[0m  [14/21], [94mLoss[0m : 2.19652
[1mStep[0m  [16/21], [94mLoss[0m : 2.30290
[1mStep[0m  [18/21], [94mLoss[0m : 2.26108
[1mStep[0m  [20/21], [94mLoss[0m : 2.14611

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.696, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17214
[1mStep[0m  [2/21], [94mLoss[0m : 2.14910
[1mStep[0m  [4/21], [94mLoss[0m : 2.22846
[1mStep[0m  [6/21], [94mLoss[0m : 2.36232
[1mStep[0m  [8/21], [94mLoss[0m : 2.19436
[1mStep[0m  [10/21], [94mLoss[0m : 2.13083
[1mStep[0m  [12/21], [94mLoss[0m : 2.15626
[1mStep[0m  [14/21], [94mLoss[0m : 2.16410
[1mStep[0m  [16/21], [94mLoss[0m : 2.32580
[1mStep[0m  [18/21], [94mLoss[0m : 2.07964
[1mStep[0m  [20/21], [94mLoss[0m : 2.26173

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.666, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15673
[1mStep[0m  [2/21], [94mLoss[0m : 2.04015
[1mStep[0m  [4/21], [94mLoss[0m : 2.13391
[1mStep[0m  [6/21], [94mLoss[0m : 2.06618
[1mStep[0m  [8/21], [94mLoss[0m : 2.13831
[1mStep[0m  [10/21], [94mLoss[0m : 2.14010
[1mStep[0m  [12/21], [94mLoss[0m : 2.26000
[1mStep[0m  [14/21], [94mLoss[0m : 1.98602
[1mStep[0m  [16/21], [94mLoss[0m : 2.38849
[1mStep[0m  [18/21], [94mLoss[0m : 2.34776
[1mStep[0m  [20/21], [94mLoss[0m : 2.15367

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.741, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14332
[1mStep[0m  [2/21], [94mLoss[0m : 2.07957
[1mStep[0m  [4/21], [94mLoss[0m : 2.21543
[1mStep[0m  [6/21], [94mLoss[0m : 2.13385
[1mStep[0m  [8/21], [94mLoss[0m : 2.20835
[1mStep[0m  [10/21], [94mLoss[0m : 2.15105
[1mStep[0m  [12/21], [94mLoss[0m : 2.27187
[1mStep[0m  [14/21], [94mLoss[0m : 2.26453
[1mStep[0m  [16/21], [94mLoss[0m : 2.22885
[1mStep[0m  [18/21], [94mLoss[0m : 2.32251
[1mStep[0m  [20/21], [94mLoss[0m : 2.08775

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.647, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26080
[1mStep[0m  [2/21], [94mLoss[0m : 2.08939
[1mStep[0m  [4/21], [94mLoss[0m : 2.16318
[1mStep[0m  [6/21], [94mLoss[0m : 2.22336
[1mStep[0m  [8/21], [94mLoss[0m : 2.11635
[1mStep[0m  [10/21], [94mLoss[0m : 2.11160
[1mStep[0m  [12/21], [94mLoss[0m : 2.13427
[1mStep[0m  [14/21], [94mLoss[0m : 2.24437
[1mStep[0m  [16/21], [94mLoss[0m : 2.13461
[1mStep[0m  [18/21], [94mLoss[0m : 2.11586
[1mStep[0m  [20/21], [94mLoss[0m : 2.09556

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.670, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16753
[1mStep[0m  [2/21], [94mLoss[0m : 2.18970
[1mStep[0m  [4/21], [94mLoss[0m : 2.08850
[1mStep[0m  [6/21], [94mLoss[0m : 2.01929
[1mStep[0m  [8/21], [94mLoss[0m : 2.01709
[1mStep[0m  [10/21], [94mLoss[0m : 2.07355
[1mStep[0m  [12/21], [94mLoss[0m : 2.16258
[1mStep[0m  [14/21], [94mLoss[0m : 2.17581
[1mStep[0m  [16/21], [94mLoss[0m : 2.25459
[1mStep[0m  [18/21], [94mLoss[0m : 2.16766
[1mStep[0m  [20/21], [94mLoss[0m : 2.11964

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.662, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18917
[1mStep[0m  [2/21], [94mLoss[0m : 2.12121
[1mStep[0m  [4/21], [94mLoss[0m : 2.17391
[1mStep[0m  [6/21], [94mLoss[0m : 2.08249
[1mStep[0m  [8/21], [94mLoss[0m : 2.29907
[1mStep[0m  [10/21], [94mLoss[0m : 2.17704
[1mStep[0m  [12/21], [94mLoss[0m : 2.09909
[1mStep[0m  [14/21], [94mLoss[0m : 2.07103
[1mStep[0m  [16/21], [94mLoss[0m : 2.13313
[1mStep[0m  [18/21], [94mLoss[0m : 2.06588
[1mStep[0m  [20/21], [94mLoss[0m : 2.06711

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.626, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.631
====================================

Phase 2 - Evaluation MAE:  2.6308869293757846
MAE score P1       2.349298
MAE score P2       2.630887
loss               2.117561
learning_rate          0.01
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay          0.001
Name: 26, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.14057
[1mStep[0m  [4/42], [94mLoss[0m : 10.93178
[1mStep[0m  [8/42], [94mLoss[0m : 10.78634
[1mStep[0m  [12/42], [94mLoss[0m : 9.98504
[1mStep[0m  [16/42], [94mLoss[0m : 9.88208
[1mStep[0m  [20/42], [94mLoss[0m : 9.06061
[1mStep[0m  [24/42], [94mLoss[0m : 8.17755
[1mStep[0m  [28/42], [94mLoss[0m : 7.74137
[1mStep[0m  [32/42], [94mLoss[0m : 7.26803
[1mStep[0m  [36/42], [94mLoss[0m : 6.23246
[1mStep[0m  [40/42], [94mLoss[0m : 5.51824

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.726, [92mTest[0m: 10.912, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.86236
[1mStep[0m  [4/42], [94mLoss[0m : 4.85731
[1mStep[0m  [8/42], [94mLoss[0m : 4.42258
[1mStep[0m  [12/42], [94mLoss[0m : 4.56881
[1mStep[0m  [16/42], [94mLoss[0m : 4.02126
[1mStep[0m  [20/42], [94mLoss[0m : 2.98611
[1mStep[0m  [24/42], [94mLoss[0m : 3.41716
[1mStep[0m  [28/42], [94mLoss[0m : 2.96859
[1mStep[0m  [32/42], [94mLoss[0m : 2.58750
[1mStep[0m  [36/42], [94mLoss[0m : 2.99013
[1mStep[0m  [40/42], [94mLoss[0m : 2.97358

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.703, [92mTest[0m: 6.002, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55338
[1mStep[0m  [4/42], [94mLoss[0m : 2.47484
[1mStep[0m  [8/42], [94mLoss[0m : 2.69340
[1mStep[0m  [12/42], [94mLoss[0m : 2.49193
[1mStep[0m  [16/42], [94mLoss[0m : 2.52028
[1mStep[0m  [20/42], [94mLoss[0m : 2.71227
[1mStep[0m  [24/42], [94mLoss[0m : 2.81348
[1mStep[0m  [28/42], [94mLoss[0m : 2.64384
[1mStep[0m  [32/42], [94mLoss[0m : 2.63346
[1mStep[0m  [36/42], [94mLoss[0m : 2.72747
[1mStep[0m  [40/42], [94mLoss[0m : 2.36817

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.89022
[1mStep[0m  [4/42], [94mLoss[0m : 2.38341
[1mStep[0m  [8/42], [94mLoss[0m : 2.47539
[1mStep[0m  [12/42], [94mLoss[0m : 2.49966
[1mStep[0m  [16/42], [94mLoss[0m : 2.48274
[1mStep[0m  [20/42], [94mLoss[0m : 2.71220
[1mStep[0m  [24/42], [94mLoss[0m : 2.65372
[1mStep[0m  [28/42], [94mLoss[0m : 2.68087
[1mStep[0m  [32/42], [94mLoss[0m : 2.54028
[1mStep[0m  [36/42], [94mLoss[0m : 2.41872
[1mStep[0m  [40/42], [94mLoss[0m : 2.43908

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51772
[1mStep[0m  [4/42], [94mLoss[0m : 2.51549
[1mStep[0m  [8/42], [94mLoss[0m : 2.37652
[1mStep[0m  [12/42], [94mLoss[0m : 2.44570
[1mStep[0m  [16/42], [94mLoss[0m : 2.68842
[1mStep[0m  [20/42], [94mLoss[0m : 2.40791
[1mStep[0m  [24/42], [94mLoss[0m : 2.39916
[1mStep[0m  [28/42], [94mLoss[0m : 2.65035
[1mStep[0m  [32/42], [94mLoss[0m : 2.50687
[1mStep[0m  [36/42], [94mLoss[0m : 2.53184
[1mStep[0m  [40/42], [94mLoss[0m : 2.74256

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43409
[1mStep[0m  [4/42], [94mLoss[0m : 2.58547
[1mStep[0m  [8/42], [94mLoss[0m : 2.62113
[1mStep[0m  [12/42], [94mLoss[0m : 2.44316
[1mStep[0m  [16/42], [94mLoss[0m : 2.59945
[1mStep[0m  [20/42], [94mLoss[0m : 2.53214
[1mStep[0m  [24/42], [94mLoss[0m : 2.58396
[1mStep[0m  [28/42], [94mLoss[0m : 2.48005
[1mStep[0m  [32/42], [94mLoss[0m : 2.45476
[1mStep[0m  [36/42], [94mLoss[0m : 2.68365
[1mStep[0m  [40/42], [94mLoss[0m : 2.42099

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52831
[1mStep[0m  [4/42], [94mLoss[0m : 2.42689
[1mStep[0m  [8/42], [94mLoss[0m : 2.45673
[1mStep[0m  [12/42], [94mLoss[0m : 2.52477
[1mStep[0m  [16/42], [94mLoss[0m : 2.65015
[1mStep[0m  [20/42], [94mLoss[0m : 2.50360
[1mStep[0m  [24/42], [94mLoss[0m : 2.66903
[1mStep[0m  [28/42], [94mLoss[0m : 2.38450
[1mStep[0m  [32/42], [94mLoss[0m : 2.41520
[1mStep[0m  [36/42], [94mLoss[0m : 2.41997
[1mStep[0m  [40/42], [94mLoss[0m : 2.44902

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33311
[1mStep[0m  [4/42], [94mLoss[0m : 2.32978
[1mStep[0m  [8/42], [94mLoss[0m : 2.49324
[1mStep[0m  [12/42], [94mLoss[0m : 2.51442
[1mStep[0m  [16/42], [94mLoss[0m : 2.55560
[1mStep[0m  [20/42], [94mLoss[0m : 2.33785
[1mStep[0m  [24/42], [94mLoss[0m : 2.51825
[1mStep[0m  [28/42], [94mLoss[0m : 2.57761
[1mStep[0m  [32/42], [94mLoss[0m : 2.39718
[1mStep[0m  [36/42], [94mLoss[0m : 2.54677
[1mStep[0m  [40/42], [94mLoss[0m : 2.48626

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71926
[1mStep[0m  [4/42], [94mLoss[0m : 2.32943
[1mStep[0m  [8/42], [94mLoss[0m : 2.39827
[1mStep[0m  [12/42], [94mLoss[0m : 2.48998
[1mStep[0m  [16/42], [94mLoss[0m : 2.35079
[1mStep[0m  [20/42], [94mLoss[0m : 2.59580
[1mStep[0m  [24/42], [94mLoss[0m : 2.65120
[1mStep[0m  [28/42], [94mLoss[0m : 2.54340
[1mStep[0m  [32/42], [94mLoss[0m : 2.70195
[1mStep[0m  [36/42], [94mLoss[0m : 2.54756
[1mStep[0m  [40/42], [94mLoss[0m : 2.58906

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33615
[1mStep[0m  [4/42], [94mLoss[0m : 2.59729
[1mStep[0m  [8/42], [94mLoss[0m : 2.44105
[1mStep[0m  [12/42], [94mLoss[0m : 2.46309
[1mStep[0m  [16/42], [94mLoss[0m : 2.43210
[1mStep[0m  [20/42], [94mLoss[0m : 2.55371
[1mStep[0m  [24/42], [94mLoss[0m : 2.62241
[1mStep[0m  [28/42], [94mLoss[0m : 2.47508
[1mStep[0m  [32/42], [94mLoss[0m : 2.50842
[1mStep[0m  [36/42], [94mLoss[0m : 2.19145
[1mStep[0m  [40/42], [94mLoss[0m : 2.52058

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47656
[1mStep[0m  [4/42], [94mLoss[0m : 2.24366
[1mStep[0m  [8/42], [94mLoss[0m : 2.44346
[1mStep[0m  [12/42], [94mLoss[0m : 2.51239
[1mStep[0m  [16/42], [94mLoss[0m : 2.35536
[1mStep[0m  [20/42], [94mLoss[0m : 2.50308
[1mStep[0m  [24/42], [94mLoss[0m : 2.49005
[1mStep[0m  [28/42], [94mLoss[0m : 2.40395
[1mStep[0m  [32/42], [94mLoss[0m : 2.47192
[1mStep[0m  [36/42], [94mLoss[0m : 2.31492
[1mStep[0m  [40/42], [94mLoss[0m : 2.42949

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46217
[1mStep[0m  [4/42], [94mLoss[0m : 2.40639
[1mStep[0m  [8/42], [94mLoss[0m : 2.51852
[1mStep[0m  [12/42], [94mLoss[0m : 2.42304
[1mStep[0m  [16/42], [94mLoss[0m : 2.51308
[1mStep[0m  [20/42], [94mLoss[0m : 2.53532
[1mStep[0m  [24/42], [94mLoss[0m : 2.41229
[1mStep[0m  [28/42], [94mLoss[0m : 2.55302
[1mStep[0m  [32/42], [94mLoss[0m : 2.54625
[1mStep[0m  [36/42], [94mLoss[0m : 2.52959
[1mStep[0m  [40/42], [94mLoss[0m : 2.52067

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36723
[1mStep[0m  [4/42], [94mLoss[0m : 2.32699
[1mStep[0m  [8/42], [94mLoss[0m : 2.25304
[1mStep[0m  [12/42], [94mLoss[0m : 2.49425
[1mStep[0m  [16/42], [94mLoss[0m : 2.47494
[1mStep[0m  [20/42], [94mLoss[0m : 2.28409
[1mStep[0m  [24/42], [94mLoss[0m : 2.53546
[1mStep[0m  [28/42], [94mLoss[0m : 2.57005
[1mStep[0m  [32/42], [94mLoss[0m : 2.47236
[1mStep[0m  [36/42], [94mLoss[0m : 2.35944
[1mStep[0m  [40/42], [94mLoss[0m : 2.39522

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69674
[1mStep[0m  [4/42], [94mLoss[0m : 2.34645
[1mStep[0m  [8/42], [94mLoss[0m : 2.33189
[1mStep[0m  [12/42], [94mLoss[0m : 2.42968
[1mStep[0m  [16/42], [94mLoss[0m : 2.47777
[1mStep[0m  [20/42], [94mLoss[0m : 2.53381
[1mStep[0m  [24/42], [94mLoss[0m : 2.34784
[1mStep[0m  [28/42], [94mLoss[0m : 2.51753
[1mStep[0m  [32/42], [94mLoss[0m : 2.78375
[1mStep[0m  [36/42], [94mLoss[0m : 2.49870
[1mStep[0m  [40/42], [94mLoss[0m : 2.27487

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33480
[1mStep[0m  [4/42], [94mLoss[0m : 2.25825
[1mStep[0m  [8/42], [94mLoss[0m : 2.50371
[1mStep[0m  [12/42], [94mLoss[0m : 2.41884
[1mStep[0m  [16/42], [94mLoss[0m : 2.56719
[1mStep[0m  [20/42], [94mLoss[0m : 2.56082
[1mStep[0m  [24/42], [94mLoss[0m : 2.44756
[1mStep[0m  [28/42], [94mLoss[0m : 2.62797
[1mStep[0m  [32/42], [94mLoss[0m : 2.51712
[1mStep[0m  [36/42], [94mLoss[0m : 2.35185
[1mStep[0m  [40/42], [94mLoss[0m : 2.61311

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36623
[1mStep[0m  [4/42], [94mLoss[0m : 2.38374
[1mStep[0m  [8/42], [94mLoss[0m : 2.34648
[1mStep[0m  [12/42], [94mLoss[0m : 2.75230
[1mStep[0m  [16/42], [94mLoss[0m : 2.68448
[1mStep[0m  [20/42], [94mLoss[0m : 2.45723
[1mStep[0m  [24/42], [94mLoss[0m : 2.40179
[1mStep[0m  [28/42], [94mLoss[0m : 2.39089
[1mStep[0m  [32/42], [94mLoss[0m : 2.62319
[1mStep[0m  [36/42], [94mLoss[0m : 2.28092
[1mStep[0m  [40/42], [94mLoss[0m : 2.43598

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44533
[1mStep[0m  [4/42], [94mLoss[0m : 2.39376
[1mStep[0m  [8/42], [94mLoss[0m : 2.31935
[1mStep[0m  [12/42], [94mLoss[0m : 2.53472
[1mStep[0m  [16/42], [94mLoss[0m : 2.70894
[1mStep[0m  [20/42], [94mLoss[0m : 2.48363
[1mStep[0m  [24/42], [94mLoss[0m : 2.28251
[1mStep[0m  [28/42], [94mLoss[0m : 2.59135
[1mStep[0m  [32/42], [94mLoss[0m : 2.50178
[1mStep[0m  [36/42], [94mLoss[0m : 2.59358
[1mStep[0m  [40/42], [94mLoss[0m : 2.59165

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53648
[1mStep[0m  [4/42], [94mLoss[0m : 2.48908
[1mStep[0m  [8/42], [94mLoss[0m : 2.26833
[1mStep[0m  [12/42], [94mLoss[0m : 2.50897
[1mStep[0m  [16/42], [94mLoss[0m : 2.50106
[1mStep[0m  [20/42], [94mLoss[0m : 2.38728
[1mStep[0m  [24/42], [94mLoss[0m : 2.58244
[1mStep[0m  [28/42], [94mLoss[0m : 2.42896
[1mStep[0m  [32/42], [94mLoss[0m : 2.57782
[1mStep[0m  [36/42], [94mLoss[0m : 2.50214
[1mStep[0m  [40/42], [94mLoss[0m : 2.39925

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59685
[1mStep[0m  [4/42], [94mLoss[0m : 2.47576
[1mStep[0m  [8/42], [94mLoss[0m : 2.36937
[1mStep[0m  [12/42], [94mLoss[0m : 2.40376
[1mStep[0m  [16/42], [94mLoss[0m : 2.51082
[1mStep[0m  [20/42], [94mLoss[0m : 2.59082
[1mStep[0m  [24/42], [94mLoss[0m : 2.43075
[1mStep[0m  [28/42], [94mLoss[0m : 2.28240
[1mStep[0m  [32/42], [94mLoss[0m : 2.34562
[1mStep[0m  [36/42], [94mLoss[0m : 2.43465
[1mStep[0m  [40/42], [94mLoss[0m : 2.21916

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37311
[1mStep[0m  [4/42], [94mLoss[0m : 2.40330
[1mStep[0m  [8/42], [94mLoss[0m : 2.44369
[1mStep[0m  [12/42], [94mLoss[0m : 2.52700
[1mStep[0m  [16/42], [94mLoss[0m : 2.41876
[1mStep[0m  [20/42], [94mLoss[0m : 2.44022
[1mStep[0m  [24/42], [94mLoss[0m : 2.41526
[1mStep[0m  [28/42], [94mLoss[0m : 2.37173
[1mStep[0m  [32/42], [94mLoss[0m : 2.46602
[1mStep[0m  [36/42], [94mLoss[0m : 2.41283
[1mStep[0m  [40/42], [94mLoss[0m : 2.32447

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52337
[1mStep[0m  [4/42], [94mLoss[0m : 2.42420
[1mStep[0m  [8/42], [94mLoss[0m : 2.58238
[1mStep[0m  [12/42], [94mLoss[0m : 2.41634
[1mStep[0m  [16/42], [94mLoss[0m : 2.43784
[1mStep[0m  [20/42], [94mLoss[0m : 2.18953
[1mStep[0m  [24/42], [94mLoss[0m : 2.49614
[1mStep[0m  [28/42], [94mLoss[0m : 2.53850
[1mStep[0m  [32/42], [94mLoss[0m : 2.31306
[1mStep[0m  [36/42], [94mLoss[0m : 2.43418
[1mStep[0m  [40/42], [94mLoss[0m : 2.40727

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36500
[1mStep[0m  [4/42], [94mLoss[0m : 2.46516
[1mStep[0m  [8/42], [94mLoss[0m : 2.46019
[1mStep[0m  [12/42], [94mLoss[0m : 2.17713
[1mStep[0m  [16/42], [94mLoss[0m : 2.55390
[1mStep[0m  [20/42], [94mLoss[0m : 2.49501
[1mStep[0m  [24/42], [94mLoss[0m : 2.33771
[1mStep[0m  [28/42], [94mLoss[0m : 2.36699
[1mStep[0m  [32/42], [94mLoss[0m : 2.39579
[1mStep[0m  [36/42], [94mLoss[0m : 2.55930
[1mStep[0m  [40/42], [94mLoss[0m : 2.44188

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57303
[1mStep[0m  [4/42], [94mLoss[0m : 2.39057
[1mStep[0m  [8/42], [94mLoss[0m : 2.49938
[1mStep[0m  [12/42], [94mLoss[0m : 2.26456
[1mStep[0m  [16/42], [94mLoss[0m : 2.52145
[1mStep[0m  [20/42], [94mLoss[0m : 2.54204
[1mStep[0m  [24/42], [94mLoss[0m : 2.42201
[1mStep[0m  [28/42], [94mLoss[0m : 2.47889
[1mStep[0m  [32/42], [94mLoss[0m : 2.37932
[1mStep[0m  [36/42], [94mLoss[0m : 2.40503
[1mStep[0m  [40/42], [94mLoss[0m : 2.46960

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32306
[1mStep[0m  [4/42], [94mLoss[0m : 2.53683
[1mStep[0m  [8/42], [94mLoss[0m : 2.49075
[1mStep[0m  [12/42], [94mLoss[0m : 2.35925
[1mStep[0m  [16/42], [94mLoss[0m : 2.68124
[1mStep[0m  [20/42], [94mLoss[0m : 2.40264
[1mStep[0m  [24/42], [94mLoss[0m : 2.37173
[1mStep[0m  [28/42], [94mLoss[0m : 2.40132
[1mStep[0m  [32/42], [94mLoss[0m : 2.44068
[1mStep[0m  [36/42], [94mLoss[0m : 2.32449
[1mStep[0m  [40/42], [94mLoss[0m : 2.39462

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31458
[1mStep[0m  [4/42], [94mLoss[0m : 2.26658
[1mStep[0m  [8/42], [94mLoss[0m : 2.34103
[1mStep[0m  [12/42], [94mLoss[0m : 2.60051
[1mStep[0m  [16/42], [94mLoss[0m : 2.39840
[1mStep[0m  [20/42], [94mLoss[0m : 2.30256
[1mStep[0m  [24/42], [94mLoss[0m : 2.63850
[1mStep[0m  [28/42], [94mLoss[0m : 2.34455
[1mStep[0m  [32/42], [94mLoss[0m : 2.26621
[1mStep[0m  [36/42], [94mLoss[0m : 2.45977
[1mStep[0m  [40/42], [94mLoss[0m : 2.30104

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.342, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38473
[1mStep[0m  [4/42], [94mLoss[0m : 2.54858
[1mStep[0m  [8/42], [94mLoss[0m : 2.37459
[1mStep[0m  [12/42], [94mLoss[0m : 2.22229
[1mStep[0m  [16/42], [94mLoss[0m : 2.47845
[1mStep[0m  [20/42], [94mLoss[0m : 2.45400
[1mStep[0m  [24/42], [94mLoss[0m : 2.30933
[1mStep[0m  [28/42], [94mLoss[0m : 2.65193
[1mStep[0m  [32/42], [94mLoss[0m : 2.33428
[1mStep[0m  [36/42], [94mLoss[0m : 2.41071
[1mStep[0m  [40/42], [94mLoss[0m : 2.57509

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52276
[1mStep[0m  [4/42], [94mLoss[0m : 2.31594
[1mStep[0m  [8/42], [94mLoss[0m : 2.20792
[1mStep[0m  [12/42], [94mLoss[0m : 2.51003
[1mStep[0m  [16/42], [94mLoss[0m : 2.27187
[1mStep[0m  [20/42], [94mLoss[0m : 2.33185
[1mStep[0m  [24/42], [94mLoss[0m : 2.45209
[1mStep[0m  [28/42], [94mLoss[0m : 2.50192
[1mStep[0m  [32/42], [94mLoss[0m : 2.56621
[1mStep[0m  [36/42], [94mLoss[0m : 2.60652
[1mStep[0m  [40/42], [94mLoss[0m : 2.39002

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65213
[1mStep[0m  [4/42], [94mLoss[0m : 2.53606
[1mStep[0m  [8/42], [94mLoss[0m : 2.24146
[1mStep[0m  [12/42], [94mLoss[0m : 2.29476
[1mStep[0m  [16/42], [94mLoss[0m : 2.69938
[1mStep[0m  [20/42], [94mLoss[0m : 2.54017
[1mStep[0m  [24/42], [94mLoss[0m : 2.23921
[1mStep[0m  [28/42], [94mLoss[0m : 2.31489
[1mStep[0m  [32/42], [94mLoss[0m : 2.47766
[1mStep[0m  [36/42], [94mLoss[0m : 2.28618
[1mStep[0m  [40/42], [94mLoss[0m : 2.42734

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40185
[1mStep[0m  [4/42], [94mLoss[0m : 2.65951
[1mStep[0m  [8/42], [94mLoss[0m : 2.16815
[1mStep[0m  [12/42], [94mLoss[0m : 2.26884
[1mStep[0m  [16/42], [94mLoss[0m : 2.31166
[1mStep[0m  [20/42], [94mLoss[0m : 2.53789
[1mStep[0m  [24/42], [94mLoss[0m : 2.56373
[1mStep[0m  [28/42], [94mLoss[0m : 2.41869
[1mStep[0m  [32/42], [94mLoss[0m : 2.35760
[1mStep[0m  [36/42], [94mLoss[0m : 2.63887
[1mStep[0m  [40/42], [94mLoss[0m : 2.24390

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44594
[1mStep[0m  [4/42], [94mLoss[0m : 2.47111
[1mStep[0m  [8/42], [94mLoss[0m : 2.66148
[1mStep[0m  [12/42], [94mLoss[0m : 2.35511
[1mStep[0m  [16/42], [94mLoss[0m : 2.56062
[1mStep[0m  [20/42], [94mLoss[0m : 2.42556
[1mStep[0m  [24/42], [94mLoss[0m : 2.41911
[1mStep[0m  [28/42], [94mLoss[0m : 2.35920
[1mStep[0m  [32/42], [94mLoss[0m : 2.56269
[1mStep[0m  [36/42], [94mLoss[0m : 2.43958
[1mStep[0m  [40/42], [94mLoss[0m : 2.40817

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.328
====================================

Phase 1 - Evaluation MAE:  2.3277986219951083
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.61223
[1mStep[0m  [4/42], [94mLoss[0m : 2.33538
[1mStep[0m  [8/42], [94mLoss[0m : 2.50257
[1mStep[0m  [12/42], [94mLoss[0m : 2.36705
[1mStep[0m  [16/42], [94mLoss[0m : 2.52390
[1mStep[0m  [20/42], [94mLoss[0m : 2.62448
[1mStep[0m  [24/42], [94mLoss[0m : 2.31822
[1mStep[0m  [28/42], [94mLoss[0m : 2.65866
[1mStep[0m  [32/42], [94mLoss[0m : 2.37690
[1mStep[0m  [36/42], [94mLoss[0m : 2.51613
[1mStep[0m  [40/42], [94mLoss[0m : 2.56051

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30504
[1mStep[0m  [4/42], [94mLoss[0m : 2.35000
[1mStep[0m  [8/42], [94mLoss[0m : 2.41657
[1mStep[0m  [12/42], [94mLoss[0m : 2.27032
[1mStep[0m  [16/42], [94mLoss[0m : 2.33079
[1mStep[0m  [20/42], [94mLoss[0m : 2.23154
[1mStep[0m  [24/42], [94mLoss[0m : 2.39598
[1mStep[0m  [28/42], [94mLoss[0m : 2.34152
[1mStep[0m  [32/42], [94mLoss[0m : 2.31115
[1mStep[0m  [36/42], [94mLoss[0m : 2.04952
[1mStep[0m  [40/42], [94mLoss[0m : 2.41126

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98758
[1mStep[0m  [4/42], [94mLoss[0m : 2.19557
[1mStep[0m  [8/42], [94mLoss[0m : 2.09578
[1mStep[0m  [12/42], [94mLoss[0m : 1.94183
[1mStep[0m  [16/42], [94mLoss[0m : 2.22344
[1mStep[0m  [20/42], [94mLoss[0m : 2.37296
[1mStep[0m  [24/42], [94mLoss[0m : 2.39379
[1mStep[0m  [28/42], [94mLoss[0m : 1.97615
[1mStep[0m  [32/42], [94mLoss[0m : 2.33232
[1mStep[0m  [36/42], [94mLoss[0m : 2.09314
[1mStep[0m  [40/42], [94mLoss[0m : 2.24833

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12457
[1mStep[0m  [4/42], [94mLoss[0m : 1.97465
[1mStep[0m  [8/42], [94mLoss[0m : 2.16372
[1mStep[0m  [12/42], [94mLoss[0m : 2.16146
[1mStep[0m  [16/42], [94mLoss[0m : 2.10112
[1mStep[0m  [20/42], [94mLoss[0m : 2.17869
[1mStep[0m  [24/42], [94mLoss[0m : 2.29100
[1mStep[0m  [28/42], [94mLoss[0m : 2.12317
[1mStep[0m  [32/42], [94mLoss[0m : 2.27200
[1mStep[0m  [36/42], [94mLoss[0m : 2.22501
[1mStep[0m  [40/42], [94mLoss[0m : 2.24680

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06383
[1mStep[0m  [4/42], [94mLoss[0m : 2.03807
[1mStep[0m  [8/42], [94mLoss[0m : 2.00841
[1mStep[0m  [12/42], [94mLoss[0m : 1.94531
[1mStep[0m  [16/42], [94mLoss[0m : 2.05867
[1mStep[0m  [20/42], [94mLoss[0m : 2.03013
[1mStep[0m  [24/42], [94mLoss[0m : 2.11031
[1mStep[0m  [28/42], [94mLoss[0m : 1.90853
[1mStep[0m  [32/42], [94mLoss[0m : 2.04511
[1mStep[0m  [36/42], [94mLoss[0m : 2.19528
[1mStep[0m  [40/42], [94mLoss[0m : 2.22591

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31206
[1mStep[0m  [4/42], [94mLoss[0m : 1.56913
[1mStep[0m  [8/42], [94mLoss[0m : 2.04655
[1mStep[0m  [12/42], [94mLoss[0m : 2.09663
[1mStep[0m  [16/42], [94mLoss[0m : 2.11129
[1mStep[0m  [20/42], [94mLoss[0m : 1.81191
[1mStep[0m  [24/42], [94mLoss[0m : 2.05568
[1mStep[0m  [28/42], [94mLoss[0m : 1.95758
[1mStep[0m  [32/42], [94mLoss[0m : 2.13737
[1mStep[0m  [36/42], [94mLoss[0m : 1.99274
[1mStep[0m  [40/42], [94mLoss[0m : 2.08794

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88100
[1mStep[0m  [4/42], [94mLoss[0m : 1.88294
[1mStep[0m  [8/42], [94mLoss[0m : 1.98731
[1mStep[0m  [12/42], [94mLoss[0m : 1.92504
[1mStep[0m  [16/42], [94mLoss[0m : 1.86661
[1mStep[0m  [20/42], [94mLoss[0m : 2.03629
[1mStep[0m  [24/42], [94mLoss[0m : 2.34299
[1mStep[0m  [28/42], [94mLoss[0m : 2.09041
[1mStep[0m  [32/42], [94mLoss[0m : 1.99368
[1mStep[0m  [36/42], [94mLoss[0m : 1.84259
[1mStep[0m  [40/42], [94mLoss[0m : 2.09451

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90862
[1mStep[0m  [4/42], [94mLoss[0m : 1.96603
[1mStep[0m  [8/42], [94mLoss[0m : 1.93942
[1mStep[0m  [12/42], [94mLoss[0m : 1.93376
[1mStep[0m  [16/42], [94mLoss[0m : 1.90574
[1mStep[0m  [20/42], [94mLoss[0m : 1.92054
[1mStep[0m  [24/42], [94mLoss[0m : 1.93790
[1mStep[0m  [28/42], [94mLoss[0m : 1.87192
[1mStep[0m  [32/42], [94mLoss[0m : 1.97002
[1mStep[0m  [36/42], [94mLoss[0m : 1.91858
[1mStep[0m  [40/42], [94mLoss[0m : 1.89839

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80472
[1mStep[0m  [4/42], [94mLoss[0m : 1.71036
[1mStep[0m  [8/42], [94mLoss[0m : 1.85187
[1mStep[0m  [12/42], [94mLoss[0m : 1.83677
[1mStep[0m  [16/42], [94mLoss[0m : 1.72194
[1mStep[0m  [20/42], [94mLoss[0m : 2.11516
[1mStep[0m  [24/42], [94mLoss[0m : 1.91194
[1mStep[0m  [28/42], [94mLoss[0m : 1.85002
[1mStep[0m  [32/42], [94mLoss[0m : 1.82900
[1mStep[0m  [36/42], [94mLoss[0m : 1.89124
[1mStep[0m  [40/42], [94mLoss[0m : 1.96885

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02576
[1mStep[0m  [4/42], [94mLoss[0m : 1.91968
[1mStep[0m  [8/42], [94mLoss[0m : 1.78820
[1mStep[0m  [12/42], [94mLoss[0m : 2.02393
[1mStep[0m  [16/42], [94mLoss[0m : 1.80360
[1mStep[0m  [20/42], [94mLoss[0m : 1.96441
[1mStep[0m  [24/42], [94mLoss[0m : 1.70869
[1mStep[0m  [28/42], [94mLoss[0m : 1.80056
[1mStep[0m  [32/42], [94mLoss[0m : 1.94678
[1mStep[0m  [36/42], [94mLoss[0m : 1.96621
[1mStep[0m  [40/42], [94mLoss[0m : 2.13930

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75758
[1mStep[0m  [4/42], [94mLoss[0m : 1.77178
[1mStep[0m  [8/42], [94mLoss[0m : 1.99071
[1mStep[0m  [12/42], [94mLoss[0m : 1.73885
[1mStep[0m  [16/42], [94mLoss[0m : 1.79132
[1mStep[0m  [20/42], [94mLoss[0m : 1.92634
[1mStep[0m  [24/42], [94mLoss[0m : 1.85386
[1mStep[0m  [28/42], [94mLoss[0m : 1.83732
[1mStep[0m  [32/42], [94mLoss[0m : 1.86338
[1mStep[0m  [36/42], [94mLoss[0m : 2.02414
[1mStep[0m  [40/42], [94mLoss[0m : 1.97800

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83927
[1mStep[0m  [4/42], [94mLoss[0m : 1.91500
[1mStep[0m  [8/42], [94mLoss[0m : 1.77305
[1mStep[0m  [12/42], [94mLoss[0m : 1.92260
[1mStep[0m  [16/42], [94mLoss[0m : 1.72718
[1mStep[0m  [20/42], [94mLoss[0m : 1.57471
[1mStep[0m  [24/42], [94mLoss[0m : 1.65791
[1mStep[0m  [28/42], [94mLoss[0m : 1.66941
[1mStep[0m  [32/42], [94mLoss[0m : 1.89602
[1mStep[0m  [36/42], [94mLoss[0m : 1.74308
[1mStep[0m  [40/42], [94mLoss[0m : 1.94610

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76675
[1mStep[0m  [4/42], [94mLoss[0m : 2.09577
[1mStep[0m  [8/42], [94mLoss[0m : 1.57166
[1mStep[0m  [12/42], [94mLoss[0m : 1.79339
[1mStep[0m  [16/42], [94mLoss[0m : 1.61168
[1mStep[0m  [20/42], [94mLoss[0m : 1.86774
[1mStep[0m  [24/42], [94mLoss[0m : 1.84872
[1mStep[0m  [28/42], [94mLoss[0m : 1.92844
[1mStep[0m  [32/42], [94mLoss[0m : 1.73446
[1mStep[0m  [36/42], [94mLoss[0m : 1.92016
[1mStep[0m  [40/42], [94mLoss[0m : 1.96431

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76753
[1mStep[0m  [4/42], [94mLoss[0m : 1.60916
[1mStep[0m  [8/42], [94mLoss[0m : 1.79312
[1mStep[0m  [12/42], [94mLoss[0m : 1.72763
[1mStep[0m  [16/42], [94mLoss[0m : 1.72729
[1mStep[0m  [20/42], [94mLoss[0m : 1.86251
[1mStep[0m  [24/42], [94mLoss[0m : 1.82747
[1mStep[0m  [28/42], [94mLoss[0m : 1.74854
[1mStep[0m  [32/42], [94mLoss[0m : 1.96423
[1mStep[0m  [36/42], [94mLoss[0m : 1.83824
[1mStep[0m  [40/42], [94mLoss[0m : 1.80008

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71134
[1mStep[0m  [4/42], [94mLoss[0m : 1.68156
[1mStep[0m  [8/42], [94mLoss[0m : 1.59278
[1mStep[0m  [12/42], [94mLoss[0m : 1.67618
[1mStep[0m  [16/42], [94mLoss[0m : 1.82113
[1mStep[0m  [20/42], [94mLoss[0m : 1.83826
[1mStep[0m  [24/42], [94mLoss[0m : 1.73428
[1mStep[0m  [28/42], [94mLoss[0m : 1.65397
[1mStep[0m  [32/42], [94mLoss[0m : 1.79214
[1mStep[0m  [36/42], [94mLoss[0m : 1.78681
[1mStep[0m  [40/42], [94mLoss[0m : 1.76472

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58655
[1mStep[0m  [4/42], [94mLoss[0m : 1.57410
[1mStep[0m  [8/42], [94mLoss[0m : 1.68668
[1mStep[0m  [12/42], [94mLoss[0m : 1.78941
[1mStep[0m  [16/42], [94mLoss[0m : 1.75538
[1mStep[0m  [20/42], [94mLoss[0m : 1.74059
[1mStep[0m  [24/42], [94mLoss[0m : 1.91943
[1mStep[0m  [28/42], [94mLoss[0m : 1.66737
[1mStep[0m  [32/42], [94mLoss[0m : 1.73778
[1mStep[0m  [36/42], [94mLoss[0m : 1.76607
[1mStep[0m  [40/42], [94mLoss[0m : 1.67436

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78110
[1mStep[0m  [4/42], [94mLoss[0m : 1.91134
[1mStep[0m  [8/42], [94mLoss[0m : 1.53901
[1mStep[0m  [12/42], [94mLoss[0m : 1.81522
[1mStep[0m  [16/42], [94mLoss[0m : 1.61996
[1mStep[0m  [20/42], [94mLoss[0m : 1.71040
[1mStep[0m  [24/42], [94mLoss[0m : 1.84758
[1mStep[0m  [28/42], [94mLoss[0m : 1.66441
[1mStep[0m  [32/42], [94mLoss[0m : 1.70181
[1mStep[0m  [36/42], [94mLoss[0m : 1.63167
[1mStep[0m  [40/42], [94mLoss[0m : 1.83415

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73742
[1mStep[0m  [4/42], [94mLoss[0m : 1.67115
[1mStep[0m  [8/42], [94mLoss[0m : 1.58591
[1mStep[0m  [12/42], [94mLoss[0m : 1.69231
[1mStep[0m  [16/42], [94mLoss[0m : 1.71507
[1mStep[0m  [20/42], [94mLoss[0m : 1.66770
[1mStep[0m  [24/42], [94mLoss[0m : 1.58443
[1mStep[0m  [28/42], [94mLoss[0m : 1.87908
[1mStep[0m  [32/42], [94mLoss[0m : 1.65290
[1mStep[0m  [36/42], [94mLoss[0m : 1.84247
[1mStep[0m  [40/42], [94mLoss[0m : 1.92361

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62203
[1mStep[0m  [4/42], [94mLoss[0m : 1.62103
[1mStep[0m  [8/42], [94mLoss[0m : 1.45511
[1mStep[0m  [12/42], [94mLoss[0m : 1.61674
[1mStep[0m  [16/42], [94mLoss[0m : 1.62855
[1mStep[0m  [20/42], [94mLoss[0m : 1.66689
[1mStep[0m  [24/42], [94mLoss[0m : 1.57180
[1mStep[0m  [28/42], [94mLoss[0m : 1.48572
[1mStep[0m  [32/42], [94mLoss[0m : 1.76259
[1mStep[0m  [36/42], [94mLoss[0m : 1.74549
[1mStep[0m  [40/42], [94mLoss[0m : 1.84918

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.500, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63743
[1mStep[0m  [4/42], [94mLoss[0m : 1.57736
[1mStep[0m  [8/42], [94mLoss[0m : 1.56506
[1mStep[0m  [12/42], [94mLoss[0m : 1.57455
[1mStep[0m  [16/42], [94mLoss[0m : 1.76365
[1mStep[0m  [20/42], [94mLoss[0m : 1.80531
[1mStep[0m  [24/42], [94mLoss[0m : 1.77956
[1mStep[0m  [28/42], [94mLoss[0m : 1.69811
[1mStep[0m  [32/42], [94mLoss[0m : 1.82745
[1mStep[0m  [36/42], [94mLoss[0m : 1.81793
[1mStep[0m  [40/42], [94mLoss[0m : 1.65712

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59260
[1mStep[0m  [4/42], [94mLoss[0m : 1.63004
[1mStep[0m  [8/42], [94mLoss[0m : 1.53763
[1mStep[0m  [12/42], [94mLoss[0m : 1.67074
[1mStep[0m  [16/42], [94mLoss[0m : 1.55698
[1mStep[0m  [20/42], [94mLoss[0m : 1.50041
[1mStep[0m  [24/42], [94mLoss[0m : 1.62730
[1mStep[0m  [28/42], [94mLoss[0m : 1.49148
[1mStep[0m  [32/42], [94mLoss[0m : 1.51927
[1mStep[0m  [36/42], [94mLoss[0m : 1.72696
[1mStep[0m  [40/42], [94mLoss[0m : 1.69313

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.470, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51814
[1mStep[0m  [4/42], [94mLoss[0m : 1.44520
[1mStep[0m  [8/42], [94mLoss[0m : 1.58986
[1mStep[0m  [12/42], [94mLoss[0m : 1.62081
[1mStep[0m  [16/42], [94mLoss[0m : 1.52176
[1mStep[0m  [20/42], [94mLoss[0m : 1.47509
[1mStep[0m  [24/42], [94mLoss[0m : 1.49960
[1mStep[0m  [28/42], [94mLoss[0m : 1.71730
[1mStep[0m  [32/42], [94mLoss[0m : 1.65726
[1mStep[0m  [36/42], [94mLoss[0m : 1.43802
[1mStep[0m  [40/42], [94mLoss[0m : 1.66687

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47217
[1mStep[0m  [4/42], [94mLoss[0m : 1.46558
[1mStep[0m  [8/42], [94mLoss[0m : 1.57066
[1mStep[0m  [12/42], [94mLoss[0m : 1.66851
[1mStep[0m  [16/42], [94mLoss[0m : 1.69676
[1mStep[0m  [20/42], [94mLoss[0m : 1.48941
[1mStep[0m  [24/42], [94mLoss[0m : 1.54201
[1mStep[0m  [28/42], [94mLoss[0m : 1.69290
[1mStep[0m  [32/42], [94mLoss[0m : 1.61010
[1mStep[0m  [36/42], [94mLoss[0m : 1.53642
[1mStep[0m  [40/42], [94mLoss[0m : 1.51914

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.457, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.53140
[1mStep[0m  [4/42], [94mLoss[0m : 1.53730
[1mStep[0m  [8/42], [94mLoss[0m : 1.70981
[1mStep[0m  [12/42], [94mLoss[0m : 1.42947
[1mStep[0m  [16/42], [94mLoss[0m : 1.48237
[1mStep[0m  [20/42], [94mLoss[0m : 1.63289
[1mStep[0m  [24/42], [94mLoss[0m : 1.90337
[1mStep[0m  [28/42], [94mLoss[0m : 1.62428
[1mStep[0m  [32/42], [94mLoss[0m : 1.64581
[1mStep[0m  [36/42], [94mLoss[0m : 1.59255
[1mStep[0m  [40/42], [94mLoss[0m : 1.88326

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.573, [92mTest[0m: 2.464, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42129
[1mStep[0m  [4/42], [94mLoss[0m : 1.66046
[1mStep[0m  [8/42], [94mLoss[0m : 1.49985
[1mStep[0m  [12/42], [94mLoss[0m : 1.61779
[1mStep[0m  [16/42], [94mLoss[0m : 1.44075
[1mStep[0m  [20/42], [94mLoss[0m : 1.63651
[1mStep[0m  [24/42], [94mLoss[0m : 1.56267
[1mStep[0m  [28/42], [94mLoss[0m : 1.43691
[1mStep[0m  [32/42], [94mLoss[0m : 1.66396
[1mStep[0m  [36/42], [94mLoss[0m : 1.56157
[1mStep[0m  [40/42], [94mLoss[0m : 1.68562

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.525, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48938
[1mStep[0m  [4/42], [94mLoss[0m : 1.44485
[1mStep[0m  [8/42], [94mLoss[0m : 1.60116
[1mStep[0m  [12/42], [94mLoss[0m : 1.53691
[1mStep[0m  [16/42], [94mLoss[0m : 1.40978
[1mStep[0m  [20/42], [94mLoss[0m : 1.58782
[1mStep[0m  [24/42], [94mLoss[0m : 1.48520
[1mStep[0m  [28/42], [94mLoss[0m : 1.52112
[1mStep[0m  [32/42], [94mLoss[0m : 1.77513
[1mStep[0m  [36/42], [94mLoss[0m : 1.71495
[1mStep[0m  [40/42], [94mLoss[0m : 1.62333

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45047
[1mStep[0m  [4/42], [94mLoss[0m : 1.42279
[1mStep[0m  [8/42], [94mLoss[0m : 1.36290
[1mStep[0m  [12/42], [94mLoss[0m : 1.47621
[1mStep[0m  [16/42], [94mLoss[0m : 1.46252
[1mStep[0m  [20/42], [94mLoss[0m : 1.54116
[1mStep[0m  [24/42], [94mLoss[0m : 1.58483
[1mStep[0m  [28/42], [94mLoss[0m : 1.59716
[1mStep[0m  [32/42], [94mLoss[0m : 1.81494
[1mStep[0m  [36/42], [94mLoss[0m : 1.66357
[1mStep[0m  [40/42], [94mLoss[0m : 1.56203

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.528, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55978
[1mStep[0m  [4/42], [94mLoss[0m : 1.57077
[1mStep[0m  [8/42], [94mLoss[0m : 1.43178
[1mStep[0m  [12/42], [94mLoss[0m : 1.42670
[1mStep[0m  [16/42], [94mLoss[0m : 1.55827
[1mStep[0m  [20/42], [94mLoss[0m : 1.67681
[1mStep[0m  [24/42], [94mLoss[0m : 1.57895
[1mStep[0m  [28/42], [94mLoss[0m : 1.53467
[1mStep[0m  [32/42], [94mLoss[0m : 1.49495
[1mStep[0m  [36/42], [94mLoss[0m : 1.57904
[1mStep[0m  [40/42], [94mLoss[0m : 1.60018

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.546, [92mTest[0m: 2.534, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59779
[1mStep[0m  [4/42], [94mLoss[0m : 1.52537
[1mStep[0m  [8/42], [94mLoss[0m : 1.49004
[1mStep[0m  [12/42], [94mLoss[0m : 1.50277
[1mStep[0m  [16/42], [94mLoss[0m : 1.51941
[1mStep[0m  [20/42], [94mLoss[0m : 1.48839
[1mStep[0m  [24/42], [94mLoss[0m : 1.62544
[1mStep[0m  [28/42], [94mLoss[0m : 1.43087
[1mStep[0m  [32/42], [94mLoss[0m : 1.50344
[1mStep[0m  [36/42], [94mLoss[0m : 1.42947
[1mStep[0m  [40/42], [94mLoss[0m : 1.48592

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.578, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47101
[1mStep[0m  [4/42], [94mLoss[0m : 1.50834
[1mStep[0m  [8/42], [94mLoss[0m : 1.54769
[1mStep[0m  [12/42], [94mLoss[0m : 1.58161
[1mStep[0m  [16/42], [94mLoss[0m : 1.32735
[1mStep[0m  [20/42], [94mLoss[0m : 1.55899
[1mStep[0m  [24/42], [94mLoss[0m : 1.58432
[1mStep[0m  [28/42], [94mLoss[0m : 1.40732
[1mStep[0m  [32/42], [94mLoss[0m : 1.54973
[1mStep[0m  [36/42], [94mLoss[0m : 1.48264
[1mStep[0m  [40/42], [94mLoss[0m : 1.59497

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.516, [92mTest[0m: 2.539, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.500
====================================

Phase 2 - Evaluation MAE:  2.5004238230841502
MAE score P1      2.327799
MAE score P2      2.500424
loss              1.515508
learning_rate         0.01
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 27, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.86881
[1mStep[0m  [2/21], [94mLoss[0m : 10.92118
[1mStep[0m  [4/21], [94mLoss[0m : 10.91461
[1mStep[0m  [6/21], [94mLoss[0m : 10.71538
[1mStep[0m  [8/21], [94mLoss[0m : 10.56642
[1mStep[0m  [10/21], [94mLoss[0m : 10.92012
[1mStep[0m  [12/21], [94mLoss[0m : 10.60837
[1mStep[0m  [14/21], [94mLoss[0m : 10.46767
[1mStep[0m  [16/21], [94mLoss[0m : 10.77229
[1mStep[0m  [18/21], [94mLoss[0m : 10.78602
[1mStep[0m  [20/21], [94mLoss[0m : 10.55924

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.742, [92mTest[0m: 10.925, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48122
[1mStep[0m  [2/21], [94mLoss[0m : 10.40047
[1mStep[0m  [4/21], [94mLoss[0m : 10.33527
[1mStep[0m  [6/21], [94mLoss[0m : 10.04556
[1mStep[0m  [8/21], [94mLoss[0m : 10.33019
[1mStep[0m  [10/21], [94mLoss[0m : 10.05062
[1mStep[0m  [12/21], [94mLoss[0m : 10.32470
[1mStep[0m  [14/21], [94mLoss[0m : 10.13915
[1mStep[0m  [16/21], [94mLoss[0m : 10.40924
[1mStep[0m  [18/21], [94mLoss[0m : 9.94007
[1mStep[0m  [20/21], [94mLoss[0m : 9.93972

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.319, [92mTest[0m: 10.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.15910
[1mStep[0m  [2/21], [94mLoss[0m : 10.07642
[1mStep[0m  [4/21], [94mLoss[0m : 9.91627
[1mStep[0m  [6/21], [94mLoss[0m : 9.70756
[1mStep[0m  [8/21], [94mLoss[0m : 9.87636
[1mStep[0m  [10/21], [94mLoss[0m : 10.02660
[1mStep[0m  [12/21], [94mLoss[0m : 9.66402
[1mStep[0m  [14/21], [94mLoss[0m : 9.53402
[1mStep[0m  [16/21], [94mLoss[0m : 9.79067
[1mStep[0m  [18/21], [94mLoss[0m : 9.75957
[1mStep[0m  [20/21], [94mLoss[0m : 9.47105

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.865, [92mTest[0m: 9.951, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.30168
[1mStep[0m  [2/21], [94mLoss[0m : 9.40598
[1mStep[0m  [4/21], [94mLoss[0m : 9.42240
[1mStep[0m  [6/21], [94mLoss[0m : 9.67399
[1mStep[0m  [8/21], [94mLoss[0m : 9.57494
[1mStep[0m  [10/21], [94mLoss[0m : 9.35380
[1mStep[0m  [12/21], [94mLoss[0m : 9.26130
[1mStep[0m  [14/21], [94mLoss[0m : 9.05921
[1mStep[0m  [16/21], [94mLoss[0m : 9.19767
[1mStep[0m  [18/21], [94mLoss[0m : 9.23844
[1mStep[0m  [20/21], [94mLoss[0m : 8.94385

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.336, [92mTest[0m: 9.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.11710
[1mStep[0m  [2/21], [94mLoss[0m : 9.09903
[1mStep[0m  [4/21], [94mLoss[0m : 8.98241
[1mStep[0m  [6/21], [94mLoss[0m : 8.73167
[1mStep[0m  [8/21], [94mLoss[0m : 9.00655
[1mStep[0m  [10/21], [94mLoss[0m : 8.81860
[1mStep[0m  [12/21], [94mLoss[0m : 8.79965
[1mStep[0m  [14/21], [94mLoss[0m : 8.62084
[1mStep[0m  [16/21], [94mLoss[0m : 8.50738
[1mStep[0m  [18/21], [94mLoss[0m : 8.38468
[1mStep[0m  [20/21], [94mLoss[0m : 8.64840

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.774, [92mTest[0m: 8.673, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.28718
[1mStep[0m  [2/21], [94mLoss[0m : 8.34235
[1mStep[0m  [4/21], [94mLoss[0m : 8.40129
[1mStep[0m  [6/21], [94mLoss[0m : 8.47185
[1mStep[0m  [8/21], [94mLoss[0m : 8.31772
[1mStep[0m  [10/21], [94mLoss[0m : 8.29358
[1mStep[0m  [12/21], [94mLoss[0m : 7.98821
[1mStep[0m  [14/21], [94mLoss[0m : 8.06441
[1mStep[0m  [16/21], [94mLoss[0m : 7.94391
[1mStep[0m  [18/21], [94mLoss[0m : 8.00781
[1mStep[0m  [20/21], [94mLoss[0m : 7.94732

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.222, [92mTest[0m: 8.032, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.81626
[1mStep[0m  [2/21], [94mLoss[0m : 8.06916
[1mStep[0m  [4/21], [94mLoss[0m : 7.83814
[1mStep[0m  [6/21], [94mLoss[0m : 7.61213
[1mStep[0m  [8/21], [94mLoss[0m : 7.73659
[1mStep[0m  [10/21], [94mLoss[0m : 7.95430
[1mStep[0m  [12/21], [94mLoss[0m : 7.69685
[1mStep[0m  [14/21], [94mLoss[0m : 7.77614
[1mStep[0m  [16/21], [94mLoss[0m : 7.65335
[1mStep[0m  [18/21], [94mLoss[0m : 7.53191
[1mStep[0m  [20/21], [94mLoss[0m : 7.54661

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.707, [92mTest[0m: 7.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.25262
[1mStep[0m  [2/21], [94mLoss[0m : 7.33961
[1mStep[0m  [4/21], [94mLoss[0m : 7.24740
[1mStep[0m  [6/21], [94mLoss[0m : 7.22520
[1mStep[0m  [8/21], [94mLoss[0m : 7.03636
[1mStep[0m  [10/21], [94mLoss[0m : 7.30139
[1mStep[0m  [12/21], [94mLoss[0m : 7.27378
[1mStep[0m  [14/21], [94mLoss[0m : 7.07279
[1mStep[0m  [16/21], [94mLoss[0m : 7.20029
[1mStep[0m  [18/21], [94mLoss[0m : 6.85730
[1mStep[0m  [20/21], [94mLoss[0m : 6.81631

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.241, [92mTest[0m: 6.846, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.91640
[1mStep[0m  [2/21], [94mLoss[0m : 6.93518
[1mStep[0m  [4/21], [94mLoss[0m : 6.99146
[1mStep[0m  [6/21], [94mLoss[0m : 6.80942
[1mStep[0m  [8/21], [94mLoss[0m : 7.02261
[1mStep[0m  [10/21], [94mLoss[0m : 6.59481
[1mStep[0m  [12/21], [94mLoss[0m : 6.57805
[1mStep[0m  [14/21], [94mLoss[0m : 6.92425
[1mStep[0m  [16/21], [94mLoss[0m : 6.67389
[1mStep[0m  [18/21], [94mLoss[0m : 6.58213
[1mStep[0m  [20/21], [94mLoss[0m : 6.60293

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.806, [92mTest[0m: 6.273, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.60332
[1mStep[0m  [2/21], [94mLoss[0m : 6.54612
[1mStep[0m  [4/21], [94mLoss[0m : 6.35221
[1mStep[0m  [6/21], [94mLoss[0m : 6.51950
[1mStep[0m  [8/21], [94mLoss[0m : 6.48184
[1mStep[0m  [10/21], [94mLoss[0m : 6.33336
[1mStep[0m  [12/21], [94mLoss[0m : 6.03680
[1mStep[0m  [14/21], [94mLoss[0m : 5.95317
[1mStep[0m  [16/21], [94mLoss[0m : 6.24305
[1mStep[0m  [18/21], [94mLoss[0m : 6.48632
[1mStep[0m  [20/21], [94mLoss[0m : 6.05932

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.386, [92mTest[0m: 5.642, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.28089
[1mStep[0m  [2/21], [94mLoss[0m : 6.08370
[1mStep[0m  [4/21], [94mLoss[0m : 6.14071
[1mStep[0m  [6/21], [94mLoss[0m : 6.10812
[1mStep[0m  [8/21], [94mLoss[0m : 5.90305
[1mStep[0m  [10/21], [94mLoss[0m : 5.89440
[1mStep[0m  [12/21], [94mLoss[0m : 5.85476
[1mStep[0m  [14/21], [94mLoss[0m : 5.90175
[1mStep[0m  [16/21], [94mLoss[0m : 5.86739
[1mStep[0m  [18/21], [94mLoss[0m : 5.72214
[1mStep[0m  [20/21], [94mLoss[0m : 5.80667

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.948, [92mTest[0m: 5.183, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.92926
[1mStep[0m  [2/21], [94mLoss[0m : 5.51657
[1mStep[0m  [4/21], [94mLoss[0m : 5.63539
[1mStep[0m  [6/21], [94mLoss[0m : 5.43789
[1mStep[0m  [8/21], [94mLoss[0m : 5.31407
[1mStep[0m  [10/21], [94mLoss[0m : 5.61362
[1mStep[0m  [12/21], [94mLoss[0m : 5.54433
[1mStep[0m  [14/21], [94mLoss[0m : 5.10918
[1mStep[0m  [16/21], [94mLoss[0m : 5.19512
[1mStep[0m  [18/21], [94mLoss[0m : 5.29394
[1mStep[0m  [20/21], [94mLoss[0m : 5.16971

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.442, [92mTest[0m: 4.694, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.33322
[1mStep[0m  [2/21], [94mLoss[0m : 5.28434
[1mStep[0m  [4/21], [94mLoss[0m : 5.09907
[1mStep[0m  [6/21], [94mLoss[0m : 4.93306
[1mStep[0m  [8/21], [94mLoss[0m : 4.94555
[1mStep[0m  [10/21], [94mLoss[0m : 4.85631
[1mStep[0m  [12/21], [94mLoss[0m : 4.73107
[1mStep[0m  [14/21], [94mLoss[0m : 4.89034
[1mStep[0m  [16/21], [94mLoss[0m : 4.68495
[1mStep[0m  [18/21], [94mLoss[0m : 4.90576
[1mStep[0m  [20/21], [94mLoss[0m : 4.38528

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.912, [92mTest[0m: 4.106, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.74511
[1mStep[0m  [2/21], [94mLoss[0m : 4.61368
[1mStep[0m  [4/21], [94mLoss[0m : 4.60184
[1mStep[0m  [6/21], [94mLoss[0m : 4.34006
[1mStep[0m  [8/21], [94mLoss[0m : 4.36910
[1mStep[0m  [10/21], [94mLoss[0m : 4.55950
[1mStep[0m  [12/21], [94mLoss[0m : 4.14537
[1mStep[0m  [14/21], [94mLoss[0m : 4.29396
[1mStep[0m  [16/21], [94mLoss[0m : 4.34517
[1mStep[0m  [18/21], [94mLoss[0m : 4.19596
[1mStep[0m  [20/21], [94mLoss[0m : 4.28890

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.359, [92mTest[0m: 3.616, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.84935
[1mStep[0m  [2/21], [94mLoss[0m : 3.75934
[1mStep[0m  [4/21], [94mLoss[0m : 4.02368
[1mStep[0m  [6/21], [94mLoss[0m : 3.88481
[1mStep[0m  [8/21], [94mLoss[0m : 3.82506
[1mStep[0m  [10/21], [94mLoss[0m : 3.88053
[1mStep[0m  [12/21], [94mLoss[0m : 3.88955
[1mStep[0m  [14/21], [94mLoss[0m : 3.59425
[1mStep[0m  [16/21], [94mLoss[0m : 3.62550
[1mStep[0m  [18/21], [94mLoss[0m : 3.81953
[1mStep[0m  [20/21], [94mLoss[0m : 3.36186

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.772, [92mTest[0m: 3.159, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.69081
[1mStep[0m  [2/21], [94mLoss[0m : 3.28915
[1mStep[0m  [4/21], [94mLoss[0m : 3.37187
[1mStep[0m  [6/21], [94mLoss[0m : 3.16994
[1mStep[0m  [8/21], [94mLoss[0m : 3.17111
[1mStep[0m  [10/21], [94mLoss[0m : 3.25972
[1mStep[0m  [12/21], [94mLoss[0m : 3.22277
[1mStep[0m  [14/21], [94mLoss[0m : 3.27133
[1mStep[0m  [16/21], [94mLoss[0m : 3.03821
[1mStep[0m  [18/21], [94mLoss[0m : 3.27953
[1mStep[0m  [20/21], [94mLoss[0m : 3.11596

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.268, [92mTest[0m: 2.733, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.08814
[1mStep[0m  [2/21], [94mLoss[0m : 2.97684
[1mStep[0m  [4/21], [94mLoss[0m : 2.89966
[1mStep[0m  [6/21], [94mLoss[0m : 2.88271
[1mStep[0m  [8/21], [94mLoss[0m : 2.81464
[1mStep[0m  [10/21], [94mLoss[0m : 2.89344
[1mStep[0m  [12/21], [94mLoss[0m : 2.83638
[1mStep[0m  [14/21], [94mLoss[0m : 2.66765
[1mStep[0m  [16/21], [94mLoss[0m : 2.97292
[1mStep[0m  [18/21], [94mLoss[0m : 2.66589
[1mStep[0m  [20/21], [94mLoss[0m : 2.91302

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.878, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76413
[1mStep[0m  [2/21], [94mLoss[0m : 2.85243
[1mStep[0m  [4/21], [94mLoss[0m : 2.84993
[1mStep[0m  [6/21], [94mLoss[0m : 2.67614
[1mStep[0m  [8/21], [94mLoss[0m : 2.60132
[1mStep[0m  [10/21], [94mLoss[0m : 2.54141
[1mStep[0m  [12/21], [94mLoss[0m : 2.75607
[1mStep[0m  [14/21], [94mLoss[0m : 2.91847
[1mStep[0m  [16/21], [94mLoss[0m : 2.65194
[1mStep[0m  [18/21], [94mLoss[0m : 2.72367
[1mStep[0m  [20/21], [94mLoss[0m : 2.67441

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.710, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69852
[1mStep[0m  [2/21], [94mLoss[0m : 2.85410
[1mStep[0m  [4/21], [94mLoss[0m : 2.68284
[1mStep[0m  [6/21], [94mLoss[0m : 2.60790
[1mStep[0m  [8/21], [94mLoss[0m : 2.66152
[1mStep[0m  [10/21], [94mLoss[0m : 2.64320
[1mStep[0m  [12/21], [94mLoss[0m : 2.58243
[1mStep[0m  [14/21], [94mLoss[0m : 2.60381
[1mStep[0m  [16/21], [94mLoss[0m : 2.59178
[1mStep[0m  [18/21], [94mLoss[0m : 2.66249
[1mStep[0m  [20/21], [94mLoss[0m : 2.61302

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49234
[1mStep[0m  [2/21], [94mLoss[0m : 2.57870
[1mStep[0m  [4/21], [94mLoss[0m : 2.55221
[1mStep[0m  [6/21], [94mLoss[0m : 2.55609
[1mStep[0m  [8/21], [94mLoss[0m : 2.58516
[1mStep[0m  [10/21], [94mLoss[0m : 2.51808
[1mStep[0m  [12/21], [94mLoss[0m : 2.46147
[1mStep[0m  [14/21], [94mLoss[0m : 2.78398
[1mStep[0m  [16/21], [94mLoss[0m : 2.66990
[1mStep[0m  [18/21], [94mLoss[0m : 2.67452
[1mStep[0m  [20/21], [94mLoss[0m : 2.68393

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.373, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65942
[1mStep[0m  [2/21], [94mLoss[0m : 2.51584
[1mStep[0m  [4/21], [94mLoss[0m : 2.63371
[1mStep[0m  [6/21], [94mLoss[0m : 2.46285
[1mStep[0m  [8/21], [94mLoss[0m : 2.58109
[1mStep[0m  [10/21], [94mLoss[0m : 2.60081
[1mStep[0m  [12/21], [94mLoss[0m : 2.71918
[1mStep[0m  [14/21], [94mLoss[0m : 2.58436
[1mStep[0m  [16/21], [94mLoss[0m : 2.58881
[1mStep[0m  [18/21], [94mLoss[0m : 2.63530
[1mStep[0m  [20/21], [94mLoss[0m : 2.43527

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.369, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69244
[1mStep[0m  [2/21], [94mLoss[0m : 2.51908
[1mStep[0m  [4/21], [94mLoss[0m : 2.53565
[1mStep[0m  [6/21], [94mLoss[0m : 2.43738
[1mStep[0m  [8/21], [94mLoss[0m : 2.47588
[1mStep[0m  [10/21], [94mLoss[0m : 2.27254
[1mStep[0m  [12/21], [94mLoss[0m : 2.68639
[1mStep[0m  [14/21], [94mLoss[0m : 2.37412
[1mStep[0m  [16/21], [94mLoss[0m : 2.45447
[1mStep[0m  [18/21], [94mLoss[0m : 2.45941
[1mStep[0m  [20/21], [94mLoss[0m : 2.52467

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46432
[1mStep[0m  [2/21], [94mLoss[0m : 2.50237
[1mStep[0m  [4/21], [94mLoss[0m : 2.50002
[1mStep[0m  [6/21], [94mLoss[0m : 2.41928
[1mStep[0m  [8/21], [94mLoss[0m : 2.65479
[1mStep[0m  [10/21], [94mLoss[0m : 2.47262
[1mStep[0m  [12/21], [94mLoss[0m : 2.53884
[1mStep[0m  [14/21], [94mLoss[0m : 2.49005
[1mStep[0m  [16/21], [94mLoss[0m : 2.46570
[1mStep[0m  [18/21], [94mLoss[0m : 2.53038
[1mStep[0m  [20/21], [94mLoss[0m : 2.51524

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.394, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52366
[1mStep[0m  [2/21], [94mLoss[0m : 2.48285
[1mStep[0m  [4/21], [94mLoss[0m : 2.50300
[1mStep[0m  [6/21], [94mLoss[0m : 2.47108
[1mStep[0m  [8/21], [94mLoss[0m : 2.62815
[1mStep[0m  [10/21], [94mLoss[0m : 2.60740
[1mStep[0m  [12/21], [94mLoss[0m : 2.42572
[1mStep[0m  [14/21], [94mLoss[0m : 2.36402
[1mStep[0m  [16/21], [94mLoss[0m : 2.64289
[1mStep[0m  [18/21], [94mLoss[0m : 2.59170
[1mStep[0m  [20/21], [94mLoss[0m : 2.57162

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.379, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35023
[1mStep[0m  [2/21], [94mLoss[0m : 2.56445
[1mStep[0m  [4/21], [94mLoss[0m : 2.53283
[1mStep[0m  [6/21], [94mLoss[0m : 2.49876
[1mStep[0m  [8/21], [94mLoss[0m : 2.47122
[1mStep[0m  [10/21], [94mLoss[0m : 2.36881
[1mStep[0m  [12/21], [94mLoss[0m : 2.47779
[1mStep[0m  [14/21], [94mLoss[0m : 2.58479
[1mStep[0m  [16/21], [94mLoss[0m : 2.48857
[1mStep[0m  [18/21], [94mLoss[0m : 2.69401
[1mStep[0m  [20/21], [94mLoss[0m : 2.55380

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.406, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62822
[1mStep[0m  [2/21], [94mLoss[0m : 2.44630
[1mStep[0m  [4/21], [94mLoss[0m : 2.54646
[1mStep[0m  [6/21], [94mLoss[0m : 2.56833
[1mStep[0m  [8/21], [94mLoss[0m : 2.48272
[1mStep[0m  [10/21], [94mLoss[0m : 2.57229
[1mStep[0m  [12/21], [94mLoss[0m : 2.56539
[1mStep[0m  [14/21], [94mLoss[0m : 2.57904
[1mStep[0m  [16/21], [94mLoss[0m : 2.52656
[1mStep[0m  [18/21], [94mLoss[0m : 2.40935
[1mStep[0m  [20/21], [94mLoss[0m : 2.47061

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.407, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42796
[1mStep[0m  [2/21], [94mLoss[0m : 2.66625
[1mStep[0m  [4/21], [94mLoss[0m : 2.63602
[1mStep[0m  [6/21], [94mLoss[0m : 2.55703
[1mStep[0m  [8/21], [94mLoss[0m : 2.57920
[1mStep[0m  [10/21], [94mLoss[0m : 2.37025
[1mStep[0m  [12/21], [94mLoss[0m : 2.66811
[1mStep[0m  [14/21], [94mLoss[0m : 2.50740
[1mStep[0m  [16/21], [94mLoss[0m : 2.51235
[1mStep[0m  [18/21], [94mLoss[0m : 2.40279
[1mStep[0m  [20/21], [94mLoss[0m : 2.55946

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57301
[1mStep[0m  [2/21], [94mLoss[0m : 2.69685
[1mStep[0m  [4/21], [94mLoss[0m : 2.46380
[1mStep[0m  [6/21], [94mLoss[0m : 2.51640
[1mStep[0m  [8/21], [94mLoss[0m : 2.42965
[1mStep[0m  [10/21], [94mLoss[0m : 2.57022
[1mStep[0m  [12/21], [94mLoss[0m : 2.55557
[1mStep[0m  [14/21], [94mLoss[0m : 2.47755
[1mStep[0m  [16/21], [94mLoss[0m : 2.62067
[1mStep[0m  [18/21], [94mLoss[0m : 2.48551
[1mStep[0m  [20/21], [94mLoss[0m : 2.63061

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.419, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51558
[1mStep[0m  [2/21], [94mLoss[0m : 2.52854
[1mStep[0m  [4/21], [94mLoss[0m : 2.54004
[1mStep[0m  [6/21], [94mLoss[0m : 2.51450
[1mStep[0m  [8/21], [94mLoss[0m : 2.70987
[1mStep[0m  [10/21], [94mLoss[0m : 2.51692
[1mStep[0m  [12/21], [94mLoss[0m : 2.40981
[1mStep[0m  [14/21], [94mLoss[0m : 2.47896
[1mStep[0m  [16/21], [94mLoss[0m : 2.36956
[1mStep[0m  [18/21], [94mLoss[0m : 2.45058
[1mStep[0m  [20/21], [94mLoss[0m : 2.45667

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.412, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38483
[1mStep[0m  [2/21], [94mLoss[0m : 2.49704
[1mStep[0m  [4/21], [94mLoss[0m : 2.51400
[1mStep[0m  [6/21], [94mLoss[0m : 2.41813
[1mStep[0m  [8/21], [94mLoss[0m : 2.46065
[1mStep[0m  [10/21], [94mLoss[0m : 2.36649
[1mStep[0m  [12/21], [94mLoss[0m : 2.53639
[1mStep[0m  [14/21], [94mLoss[0m : 2.47132
[1mStep[0m  [16/21], [94mLoss[0m : 2.60684
[1mStep[0m  [18/21], [94mLoss[0m : 2.40356
[1mStep[0m  [20/21], [94mLoss[0m : 2.44931

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.367
====================================

Phase 1 - Evaluation MAE:  2.366856813430786
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.44947
[1mStep[0m  [2/21], [94mLoss[0m : 2.53157
[1mStep[0m  [4/21], [94mLoss[0m : 2.60377
[1mStep[0m  [6/21], [94mLoss[0m : 2.61987
[1mStep[0m  [8/21], [94mLoss[0m : 2.71076
[1mStep[0m  [10/21], [94mLoss[0m : 2.51844
[1mStep[0m  [12/21], [94mLoss[0m : 2.70034
[1mStep[0m  [14/21], [94mLoss[0m : 2.68724
[1mStep[0m  [16/21], [94mLoss[0m : 2.52742
[1mStep[0m  [18/21], [94mLoss[0m : 2.48241
[1mStep[0m  [20/21], [94mLoss[0m : 2.53975

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45703
[1mStep[0m  [2/21], [94mLoss[0m : 2.48797
[1mStep[0m  [4/21], [94mLoss[0m : 2.55684
[1mStep[0m  [6/21], [94mLoss[0m : 2.37131
[1mStep[0m  [8/21], [94mLoss[0m : 2.51751
[1mStep[0m  [10/21], [94mLoss[0m : 2.44651
[1mStep[0m  [12/21], [94mLoss[0m : 2.57159
[1mStep[0m  [14/21], [94mLoss[0m : 2.57875
[1mStep[0m  [16/21], [94mLoss[0m : 2.42180
[1mStep[0m  [18/21], [94mLoss[0m : 2.45702
[1mStep[0m  [20/21], [94mLoss[0m : 2.62275

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56251
[1mStep[0m  [2/21], [94mLoss[0m : 2.50378
[1mStep[0m  [4/21], [94mLoss[0m : 2.35979
[1mStep[0m  [6/21], [94mLoss[0m : 2.48748
[1mStep[0m  [8/21], [94mLoss[0m : 2.56664
[1mStep[0m  [10/21], [94mLoss[0m : 2.53415
[1mStep[0m  [12/21], [94mLoss[0m : 2.40393
[1mStep[0m  [14/21], [94mLoss[0m : 2.47718
[1mStep[0m  [16/21], [94mLoss[0m : 2.54395
[1mStep[0m  [18/21], [94mLoss[0m : 2.63590
[1mStep[0m  [20/21], [94mLoss[0m : 2.43267

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47047
[1mStep[0m  [2/21], [94mLoss[0m : 2.45888
[1mStep[0m  [4/21], [94mLoss[0m : 2.41744
[1mStep[0m  [6/21], [94mLoss[0m : 2.44551
[1mStep[0m  [8/21], [94mLoss[0m : 2.59224
[1mStep[0m  [10/21], [94mLoss[0m : 2.27055
[1mStep[0m  [12/21], [94mLoss[0m : 2.44651
[1mStep[0m  [14/21], [94mLoss[0m : 2.57719
[1mStep[0m  [16/21], [94mLoss[0m : 2.54314
[1mStep[0m  [18/21], [94mLoss[0m : 2.47302
[1mStep[0m  [20/21], [94mLoss[0m : 2.52565

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.530, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34815
[1mStep[0m  [2/21], [94mLoss[0m : 2.51330
[1mStep[0m  [4/21], [94mLoss[0m : 2.51481
[1mStep[0m  [6/21], [94mLoss[0m : 2.39707
[1mStep[0m  [8/21], [94mLoss[0m : 2.39008
[1mStep[0m  [10/21], [94mLoss[0m : 2.49434
[1mStep[0m  [12/21], [94mLoss[0m : 2.35508
[1mStep[0m  [14/21], [94mLoss[0m : 2.34065
[1mStep[0m  [16/21], [94mLoss[0m : 2.51565
[1mStep[0m  [18/21], [94mLoss[0m : 2.37958
[1mStep[0m  [20/21], [94mLoss[0m : 2.49152

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52768
[1mStep[0m  [2/21], [94mLoss[0m : 2.39119
[1mStep[0m  [4/21], [94mLoss[0m : 2.37319
[1mStep[0m  [6/21], [94mLoss[0m : 2.44259
[1mStep[0m  [8/21], [94mLoss[0m : 2.24667
[1mStep[0m  [10/21], [94mLoss[0m : 2.56029
[1mStep[0m  [12/21], [94mLoss[0m : 2.32012
[1mStep[0m  [14/21], [94mLoss[0m : 2.27569
[1mStep[0m  [16/21], [94mLoss[0m : 2.48107
[1mStep[0m  [18/21], [94mLoss[0m : 2.37938
[1mStep[0m  [20/21], [94mLoss[0m : 2.43837

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.529, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44245
[1mStep[0m  [2/21], [94mLoss[0m : 2.31013
[1mStep[0m  [4/21], [94mLoss[0m : 2.30304
[1mStep[0m  [6/21], [94mLoss[0m : 2.29427
[1mStep[0m  [8/21], [94mLoss[0m : 2.35954
[1mStep[0m  [10/21], [94mLoss[0m : 2.51992
[1mStep[0m  [12/21], [94mLoss[0m : 2.28543
[1mStep[0m  [14/21], [94mLoss[0m : 2.44631
[1mStep[0m  [16/21], [94mLoss[0m : 2.41864
[1mStep[0m  [18/21], [94mLoss[0m : 2.30447
[1mStep[0m  [20/21], [94mLoss[0m : 2.48409

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.587, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36616
[1mStep[0m  [2/21], [94mLoss[0m : 2.47970
[1mStep[0m  [4/21], [94mLoss[0m : 2.29838
[1mStep[0m  [6/21], [94mLoss[0m : 2.39487
[1mStep[0m  [8/21], [94mLoss[0m : 2.30663
[1mStep[0m  [10/21], [94mLoss[0m : 2.33658
[1mStep[0m  [12/21], [94mLoss[0m : 2.30012
[1mStep[0m  [14/21], [94mLoss[0m : 2.29356
[1mStep[0m  [16/21], [94mLoss[0m : 2.37457
[1mStep[0m  [18/21], [94mLoss[0m : 2.43537
[1mStep[0m  [20/21], [94mLoss[0m : 2.30230

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40474
[1mStep[0m  [2/21], [94mLoss[0m : 2.18688
[1mStep[0m  [4/21], [94mLoss[0m : 2.44107
[1mStep[0m  [6/21], [94mLoss[0m : 2.30427
[1mStep[0m  [8/21], [94mLoss[0m : 2.40877
[1mStep[0m  [10/21], [94mLoss[0m : 2.42561
[1mStep[0m  [12/21], [94mLoss[0m : 2.49093
[1mStep[0m  [14/21], [94mLoss[0m : 2.21721
[1mStep[0m  [16/21], [94mLoss[0m : 2.34562
[1mStep[0m  [18/21], [94mLoss[0m : 2.39429
[1mStep[0m  [20/21], [94mLoss[0m : 2.33862

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.570, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30083
[1mStep[0m  [2/21], [94mLoss[0m : 2.34352
[1mStep[0m  [4/21], [94mLoss[0m : 2.32589
[1mStep[0m  [6/21], [94mLoss[0m : 2.16387
[1mStep[0m  [8/21], [94mLoss[0m : 2.31562
[1mStep[0m  [10/21], [94mLoss[0m : 2.33008
[1mStep[0m  [12/21], [94mLoss[0m : 2.35461
[1mStep[0m  [14/21], [94mLoss[0m : 2.40937
[1mStep[0m  [16/21], [94mLoss[0m : 2.23103
[1mStep[0m  [18/21], [94mLoss[0m : 2.27401
[1mStep[0m  [20/21], [94mLoss[0m : 2.37208

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.289, [92mTest[0m: 2.536, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26864
[1mStep[0m  [2/21], [94mLoss[0m : 2.20646
[1mStep[0m  [4/21], [94mLoss[0m : 2.23630
[1mStep[0m  [6/21], [94mLoss[0m : 2.24918
[1mStep[0m  [8/21], [94mLoss[0m : 2.18465
[1mStep[0m  [10/21], [94mLoss[0m : 2.14802
[1mStep[0m  [12/21], [94mLoss[0m : 2.18204
[1mStep[0m  [14/21], [94mLoss[0m : 2.35771
[1mStep[0m  [16/21], [94mLoss[0m : 2.30961
[1mStep[0m  [18/21], [94mLoss[0m : 2.44277
[1mStep[0m  [20/21], [94mLoss[0m : 2.28063

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24571
[1mStep[0m  [2/21], [94mLoss[0m : 2.21068
[1mStep[0m  [4/21], [94mLoss[0m : 2.26604
[1mStep[0m  [6/21], [94mLoss[0m : 2.27399
[1mStep[0m  [8/21], [94mLoss[0m : 2.14253
[1mStep[0m  [10/21], [94mLoss[0m : 2.21532
[1mStep[0m  [12/21], [94mLoss[0m : 2.26134
[1mStep[0m  [14/21], [94mLoss[0m : 2.23125
[1mStep[0m  [16/21], [94mLoss[0m : 2.16605
[1mStep[0m  [18/21], [94mLoss[0m : 2.41587
[1mStep[0m  [20/21], [94mLoss[0m : 2.25941

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.510, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27369
[1mStep[0m  [2/21], [94mLoss[0m : 2.14113
[1mStep[0m  [4/21], [94mLoss[0m : 2.20251
[1mStep[0m  [6/21], [94mLoss[0m : 2.19205
[1mStep[0m  [8/21], [94mLoss[0m : 2.41129
[1mStep[0m  [10/21], [94mLoss[0m : 2.29299
[1mStep[0m  [12/21], [94mLoss[0m : 2.21341
[1mStep[0m  [14/21], [94mLoss[0m : 2.18237
[1mStep[0m  [16/21], [94mLoss[0m : 2.12037
[1mStep[0m  [18/21], [94mLoss[0m : 2.21132
[1mStep[0m  [20/21], [94mLoss[0m : 2.27932

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15177
[1mStep[0m  [2/21], [94mLoss[0m : 2.20140
[1mStep[0m  [4/21], [94mLoss[0m : 2.24235
[1mStep[0m  [6/21], [94mLoss[0m : 2.15369
[1mStep[0m  [8/21], [94mLoss[0m : 2.08606
[1mStep[0m  [10/21], [94mLoss[0m : 2.21454
[1mStep[0m  [12/21], [94mLoss[0m : 2.23002
[1mStep[0m  [14/21], [94mLoss[0m : 1.98521
[1mStep[0m  [16/21], [94mLoss[0m : 2.23040
[1mStep[0m  [18/21], [94mLoss[0m : 2.32031
[1mStep[0m  [20/21], [94mLoss[0m : 2.21349

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.695, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14983
[1mStep[0m  [2/21], [94mLoss[0m : 2.06353
[1mStep[0m  [4/21], [94mLoss[0m : 2.13660
[1mStep[0m  [6/21], [94mLoss[0m : 2.16994
[1mStep[0m  [8/21], [94mLoss[0m : 2.02698
[1mStep[0m  [10/21], [94mLoss[0m : 2.21716
[1mStep[0m  [12/21], [94mLoss[0m : 2.14684
[1mStep[0m  [14/21], [94mLoss[0m : 2.09864
[1mStep[0m  [16/21], [94mLoss[0m : 2.09045
[1mStep[0m  [18/21], [94mLoss[0m : 2.25640
[1mStep[0m  [20/21], [94mLoss[0m : 2.24358

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.593, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15167
[1mStep[0m  [2/21], [94mLoss[0m : 2.11427
[1mStep[0m  [4/21], [94mLoss[0m : 2.16726
[1mStep[0m  [6/21], [94mLoss[0m : 2.27226
[1mStep[0m  [8/21], [94mLoss[0m : 1.97916
[1mStep[0m  [10/21], [94mLoss[0m : 2.10503
[1mStep[0m  [12/21], [94mLoss[0m : 2.01661
[1mStep[0m  [14/21], [94mLoss[0m : 2.22537
[1mStep[0m  [16/21], [94mLoss[0m : 2.25807
[1mStep[0m  [18/21], [94mLoss[0m : 2.07952
[1mStep[0m  [20/21], [94mLoss[0m : 2.27790

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.578, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97529
[1mStep[0m  [2/21], [94mLoss[0m : 2.16520
[1mStep[0m  [4/21], [94mLoss[0m : 2.08886
[1mStep[0m  [6/21], [94mLoss[0m : 2.09201
[1mStep[0m  [8/21], [94mLoss[0m : 2.07409
[1mStep[0m  [10/21], [94mLoss[0m : 2.01127
[1mStep[0m  [12/21], [94mLoss[0m : 2.22354
[1mStep[0m  [14/21], [94mLoss[0m : 2.17592
[1mStep[0m  [16/21], [94mLoss[0m : 2.11287
[1mStep[0m  [18/21], [94mLoss[0m : 2.00265
[1mStep[0m  [20/21], [94mLoss[0m : 2.25031

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.562, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08767
[1mStep[0m  [2/21], [94mLoss[0m : 2.04176
[1mStep[0m  [4/21], [94mLoss[0m : 2.00373
[1mStep[0m  [6/21], [94mLoss[0m : 1.99297
[1mStep[0m  [8/21], [94mLoss[0m : 2.00505
[1mStep[0m  [10/21], [94mLoss[0m : 2.25543
[1mStep[0m  [12/21], [94mLoss[0m : 2.17146
[1mStep[0m  [14/21], [94mLoss[0m : 2.05291
[1mStep[0m  [16/21], [94mLoss[0m : 2.04940
[1mStep[0m  [18/21], [94mLoss[0m : 1.94013
[1mStep[0m  [20/21], [94mLoss[0m : 2.11540

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.587, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01554
[1mStep[0m  [2/21], [94mLoss[0m : 1.91928
[1mStep[0m  [4/21], [94mLoss[0m : 1.99529
[1mStep[0m  [6/21], [94mLoss[0m : 2.09917
[1mStep[0m  [8/21], [94mLoss[0m : 2.23932
[1mStep[0m  [10/21], [94mLoss[0m : 2.04467
[1mStep[0m  [12/21], [94mLoss[0m : 1.96879
[1mStep[0m  [14/21], [94mLoss[0m : 2.03577
[1mStep[0m  [16/21], [94mLoss[0m : 1.99097
[1mStep[0m  [18/21], [94mLoss[0m : 1.99721
[1mStep[0m  [20/21], [94mLoss[0m : 2.04373

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.21668
[1mStep[0m  [2/21], [94mLoss[0m : 2.00927
[1mStep[0m  [4/21], [94mLoss[0m : 2.03114
[1mStep[0m  [6/21], [94mLoss[0m : 2.07870
[1mStep[0m  [8/21], [94mLoss[0m : 2.00418
[1mStep[0m  [10/21], [94mLoss[0m : 1.86879
[1mStep[0m  [12/21], [94mLoss[0m : 1.91593
[1mStep[0m  [14/21], [94mLoss[0m : 2.05793
[1mStep[0m  [16/21], [94mLoss[0m : 1.88449
[1mStep[0m  [18/21], [94mLoss[0m : 2.02109
[1mStep[0m  [20/21], [94mLoss[0m : 1.99572

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.869, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84979
[1mStep[0m  [2/21], [94mLoss[0m : 2.02932
[1mStep[0m  [4/21], [94mLoss[0m : 1.90017
[1mStep[0m  [6/21], [94mLoss[0m : 2.10636
[1mStep[0m  [8/21], [94mLoss[0m : 2.00527
[1mStep[0m  [10/21], [94mLoss[0m : 2.08886
[1mStep[0m  [12/21], [94mLoss[0m : 2.05536
[1mStep[0m  [14/21], [94mLoss[0m : 2.07933
[1mStep[0m  [16/21], [94mLoss[0m : 2.09051
[1mStep[0m  [18/21], [94mLoss[0m : 2.01377
[1mStep[0m  [20/21], [94mLoss[0m : 2.04850

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.872, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99312
[1mStep[0m  [2/21], [94mLoss[0m : 1.79250
[1mStep[0m  [4/21], [94mLoss[0m : 1.96172
[1mStep[0m  [6/21], [94mLoss[0m : 1.82894
[1mStep[0m  [8/21], [94mLoss[0m : 1.88353
[1mStep[0m  [10/21], [94mLoss[0m : 1.87773
[1mStep[0m  [12/21], [94mLoss[0m : 2.05575
[1mStep[0m  [14/21], [94mLoss[0m : 1.96525
[1mStep[0m  [16/21], [94mLoss[0m : 1.98459
[1mStep[0m  [18/21], [94mLoss[0m : 2.03045
[1mStep[0m  [20/21], [94mLoss[0m : 1.94093

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.769, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.76368
[1mStep[0m  [2/21], [94mLoss[0m : 1.85215
[1mStep[0m  [4/21], [94mLoss[0m : 1.86073
[1mStep[0m  [6/21], [94mLoss[0m : 1.92830
[1mStep[0m  [8/21], [94mLoss[0m : 2.00780
[1mStep[0m  [10/21], [94mLoss[0m : 1.91923
[1mStep[0m  [12/21], [94mLoss[0m : 1.92284
[1mStep[0m  [14/21], [94mLoss[0m : 2.01035
[1mStep[0m  [16/21], [94mLoss[0m : 1.87456
[1mStep[0m  [18/21], [94mLoss[0m : 1.96071
[1mStep[0m  [20/21], [94mLoss[0m : 1.98926

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.941, [92mTest[0m: 2.714, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.85459
[1mStep[0m  [2/21], [94mLoss[0m : 1.99933
[1mStep[0m  [4/21], [94mLoss[0m : 1.88490
[1mStep[0m  [6/21], [94mLoss[0m : 1.91538
[1mStep[0m  [8/21], [94mLoss[0m : 1.87878
[1mStep[0m  [10/21], [94mLoss[0m : 2.04815
[1mStep[0m  [12/21], [94mLoss[0m : 1.87903
[1mStep[0m  [14/21], [94mLoss[0m : 1.78601
[1mStep[0m  [16/21], [94mLoss[0m : 2.10412
[1mStep[0m  [18/21], [94mLoss[0m : 2.02316
[1mStep[0m  [20/21], [94mLoss[0m : 1.84393

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.605, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.81199
[1mStep[0m  [2/21], [94mLoss[0m : 1.81961
[1mStep[0m  [4/21], [94mLoss[0m : 1.84793
[1mStep[0m  [6/21], [94mLoss[0m : 1.92665
[1mStep[0m  [8/21], [94mLoss[0m : 1.81030
[1mStep[0m  [10/21], [94mLoss[0m : 1.90147
[1mStep[0m  [12/21], [94mLoss[0m : 1.98090
[1mStep[0m  [14/21], [94mLoss[0m : 1.83178
[1mStep[0m  [16/21], [94mLoss[0m : 1.90700
[1mStep[0m  [18/21], [94mLoss[0m : 1.92325
[1mStep[0m  [20/21], [94mLoss[0m : 1.96576

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.696, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71683
[1mStep[0m  [2/21], [94mLoss[0m : 1.86591
[1mStep[0m  [4/21], [94mLoss[0m : 1.90837
[1mStep[0m  [6/21], [94mLoss[0m : 1.85583
[1mStep[0m  [8/21], [94mLoss[0m : 1.89342
[1mStep[0m  [10/21], [94mLoss[0m : 1.93659
[1mStep[0m  [12/21], [94mLoss[0m : 1.91132
[1mStep[0m  [14/21], [94mLoss[0m : 1.85681
[1mStep[0m  [16/21], [94mLoss[0m : 1.95449
[1mStep[0m  [18/21], [94mLoss[0m : 2.03662
[1mStep[0m  [20/21], [94mLoss[0m : 1.84035

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.559, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95538
[1mStep[0m  [2/21], [94mLoss[0m : 1.78195
[1mStep[0m  [4/21], [94mLoss[0m : 1.75393
[1mStep[0m  [6/21], [94mLoss[0m : 1.88898
[1mStep[0m  [8/21], [94mLoss[0m : 1.87021
[1mStep[0m  [10/21], [94mLoss[0m : 1.83656
[1mStep[0m  [12/21], [94mLoss[0m : 1.79026
[1mStep[0m  [14/21], [94mLoss[0m : 1.78653
[1mStep[0m  [16/21], [94mLoss[0m : 1.82356
[1mStep[0m  [18/21], [94mLoss[0m : 1.89215
[1mStep[0m  [20/21], [94mLoss[0m : 1.79139

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.851, [92mTest[0m: 2.641, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78793
[1mStep[0m  [2/21], [94mLoss[0m : 1.77343
[1mStep[0m  [4/21], [94mLoss[0m : 1.90112
[1mStep[0m  [6/21], [94mLoss[0m : 1.72501
[1mStep[0m  [8/21], [94mLoss[0m : 1.75393
[1mStep[0m  [10/21], [94mLoss[0m : 1.76243
[1mStep[0m  [12/21], [94mLoss[0m : 1.79831
[1mStep[0m  [14/21], [94mLoss[0m : 1.97187
[1mStep[0m  [16/21], [94mLoss[0m : 1.74763
[1mStep[0m  [18/21], [94mLoss[0m : 1.87179
[1mStep[0m  [20/21], [94mLoss[0m : 1.91802

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.656, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78635
[1mStep[0m  [2/21], [94mLoss[0m : 1.72889
[1mStep[0m  [4/21], [94mLoss[0m : 1.78214
[1mStep[0m  [6/21], [94mLoss[0m : 1.68331
[1mStep[0m  [8/21], [94mLoss[0m : 1.73180
[1mStep[0m  [10/21], [94mLoss[0m : 1.80311
[1mStep[0m  [12/21], [94mLoss[0m : 1.89706
[1mStep[0m  [14/21], [94mLoss[0m : 1.76267
[1mStep[0m  [16/21], [94mLoss[0m : 1.74622
[1mStep[0m  [18/21], [94mLoss[0m : 1.84945
[1mStep[0m  [20/21], [94mLoss[0m : 1.70283

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.543, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.66603
[1mStep[0m  [2/21], [94mLoss[0m : 1.78968
[1mStep[0m  [4/21], [94mLoss[0m : 1.85284
[1mStep[0m  [6/21], [94mLoss[0m : 1.84061
[1mStep[0m  [8/21], [94mLoss[0m : 1.67633
[1mStep[0m  [10/21], [94mLoss[0m : 1.73719
[1mStep[0m  [12/21], [94mLoss[0m : 1.79636
[1mStep[0m  [14/21], [94mLoss[0m : 1.87262
[1mStep[0m  [16/21], [94mLoss[0m : 1.80035
[1mStep[0m  [18/21], [94mLoss[0m : 1.81043
[1mStep[0m  [20/21], [94mLoss[0m : 1.81729

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.630, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.556
====================================

Phase 2 - Evaluation MAE:  2.5563648087637767
MAE score P1      2.366857
MAE score P2      2.556365
loss              1.783254
learning_rate         0.01
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 28, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
