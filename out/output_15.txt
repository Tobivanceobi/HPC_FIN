no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  15
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.63377
[1mStep[0m  [5/53], [94mLoss[0m : 6.62449
[1mStep[0m  [10/53], [94mLoss[0m : 2.83121
[1mStep[0m  [15/53], [94mLoss[0m : 4.39849
[1mStep[0m  [20/53], [94mLoss[0m : 3.31777
[1mStep[0m  [25/53], [94mLoss[0m : 2.62539
[1mStep[0m  [30/53], [94mLoss[0m : 3.07441
[1mStep[0m  [35/53], [94mLoss[0m : 2.62123
[1mStep[0m  [40/53], [94mLoss[0m : 2.73806
[1mStep[0m  [45/53], [94mLoss[0m : 2.84681
[1mStep[0m  [50/53], [94mLoss[0m : 2.68656

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.768, [92mTest[0m: 10.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63639
[1mStep[0m  [5/53], [94mLoss[0m : 2.71038
[1mStep[0m  [10/53], [94mLoss[0m : 2.64917
[1mStep[0m  [15/53], [94mLoss[0m : 2.79013
[1mStep[0m  [20/53], [94mLoss[0m : 2.59578
[1mStep[0m  [25/53], [94mLoss[0m : 2.65066
[1mStep[0m  [30/53], [94mLoss[0m : 2.50419
[1mStep[0m  [35/53], [94mLoss[0m : 2.55786
[1mStep[0m  [40/53], [94mLoss[0m : 2.82212
[1mStep[0m  [45/53], [94mLoss[0m : 2.37634
[1mStep[0m  [50/53], [94mLoss[0m : 2.46590

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47087
[1mStep[0m  [5/53], [94mLoss[0m : 2.65251
[1mStep[0m  [10/53], [94mLoss[0m : 2.47536
[1mStep[0m  [15/53], [94mLoss[0m : 2.46741
[1mStep[0m  [20/53], [94mLoss[0m : 2.68355
[1mStep[0m  [25/53], [94mLoss[0m : 2.50325
[1mStep[0m  [30/53], [94mLoss[0m : 2.30050
[1mStep[0m  [35/53], [94mLoss[0m : 2.59389
[1mStep[0m  [40/53], [94mLoss[0m : 2.37906
[1mStep[0m  [45/53], [94mLoss[0m : 2.46689
[1mStep[0m  [50/53], [94mLoss[0m : 2.65075

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61254
[1mStep[0m  [5/53], [94mLoss[0m : 2.48047
[1mStep[0m  [10/53], [94mLoss[0m : 2.42518
[1mStep[0m  [15/53], [94mLoss[0m : 2.54254
[1mStep[0m  [20/53], [94mLoss[0m : 2.58600
[1mStep[0m  [25/53], [94mLoss[0m : 2.54715
[1mStep[0m  [30/53], [94mLoss[0m : 2.33688
[1mStep[0m  [35/53], [94mLoss[0m : 2.58778
[1mStep[0m  [40/53], [94mLoss[0m : 2.44851
[1mStep[0m  [45/53], [94mLoss[0m : 2.45443
[1mStep[0m  [50/53], [94mLoss[0m : 2.48029

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64635
[1mStep[0m  [5/53], [94mLoss[0m : 2.61970
[1mStep[0m  [10/53], [94mLoss[0m : 2.52098
[1mStep[0m  [15/53], [94mLoss[0m : 2.69081
[1mStep[0m  [20/53], [94mLoss[0m : 2.35091
[1mStep[0m  [25/53], [94mLoss[0m : 2.21365
[1mStep[0m  [30/53], [94mLoss[0m : 2.42850
[1mStep[0m  [35/53], [94mLoss[0m : 2.41815
[1mStep[0m  [40/53], [94mLoss[0m : 2.34336
[1mStep[0m  [45/53], [94mLoss[0m : 2.68631
[1mStep[0m  [50/53], [94mLoss[0m : 2.50992

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57840
[1mStep[0m  [5/53], [94mLoss[0m : 2.62517
[1mStep[0m  [10/53], [94mLoss[0m : 2.51039
[1mStep[0m  [15/53], [94mLoss[0m : 2.41960
[1mStep[0m  [20/53], [94mLoss[0m : 2.47529
[1mStep[0m  [25/53], [94mLoss[0m : 2.49262
[1mStep[0m  [30/53], [94mLoss[0m : 2.46038
[1mStep[0m  [35/53], [94mLoss[0m : 2.41290
[1mStep[0m  [40/53], [94mLoss[0m : 2.46697
[1mStep[0m  [45/53], [94mLoss[0m : 2.49490
[1mStep[0m  [50/53], [94mLoss[0m : 2.41339

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36478
[1mStep[0m  [5/53], [94mLoss[0m : 2.29167
[1mStep[0m  [10/53], [94mLoss[0m : 2.55443
[1mStep[0m  [15/53], [94mLoss[0m : 2.56194
[1mStep[0m  [20/53], [94mLoss[0m : 2.65575
[1mStep[0m  [25/53], [94mLoss[0m : 2.61822
[1mStep[0m  [30/53], [94mLoss[0m : 2.25198
[1mStep[0m  [35/53], [94mLoss[0m : 2.49043
[1mStep[0m  [40/53], [94mLoss[0m : 2.56053
[1mStep[0m  [45/53], [94mLoss[0m : 2.72022
[1mStep[0m  [50/53], [94mLoss[0m : 2.43166

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.77578
[1mStep[0m  [5/53], [94mLoss[0m : 2.53606
[1mStep[0m  [10/53], [94mLoss[0m : 2.41037
[1mStep[0m  [15/53], [94mLoss[0m : 2.33154
[1mStep[0m  [20/53], [94mLoss[0m : 2.50676
[1mStep[0m  [25/53], [94mLoss[0m : 2.45245
[1mStep[0m  [30/53], [94mLoss[0m : 2.41752
[1mStep[0m  [35/53], [94mLoss[0m : 2.62734
[1mStep[0m  [40/53], [94mLoss[0m : 2.52257
[1mStep[0m  [45/53], [94mLoss[0m : 2.48237
[1mStep[0m  [50/53], [94mLoss[0m : 2.72074

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.75114
[1mStep[0m  [5/53], [94mLoss[0m : 2.62227
[1mStep[0m  [10/53], [94mLoss[0m : 2.34043
[1mStep[0m  [15/53], [94mLoss[0m : 2.38853
[1mStep[0m  [20/53], [94mLoss[0m : 2.53110
[1mStep[0m  [25/53], [94mLoss[0m : 2.41095
[1mStep[0m  [30/53], [94mLoss[0m : 2.53277
[1mStep[0m  [35/53], [94mLoss[0m : 2.43910
[1mStep[0m  [40/53], [94mLoss[0m : 2.48071
[1mStep[0m  [45/53], [94mLoss[0m : 2.54821
[1mStep[0m  [50/53], [94mLoss[0m : 2.38005

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44716
[1mStep[0m  [5/53], [94mLoss[0m : 2.73719
[1mStep[0m  [10/53], [94mLoss[0m : 2.33841
[1mStep[0m  [15/53], [94mLoss[0m : 2.73373
[1mStep[0m  [20/53], [94mLoss[0m : 2.37100
[1mStep[0m  [25/53], [94mLoss[0m : 2.37764
[1mStep[0m  [30/53], [94mLoss[0m : 2.36899
[1mStep[0m  [35/53], [94mLoss[0m : 2.55541
[1mStep[0m  [40/53], [94mLoss[0m : 2.29871
[1mStep[0m  [45/53], [94mLoss[0m : 2.50223
[1mStep[0m  [50/53], [94mLoss[0m : 2.61480

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.74902
[1mStep[0m  [5/53], [94mLoss[0m : 2.30522
[1mStep[0m  [10/53], [94mLoss[0m : 2.43991
[1mStep[0m  [15/53], [94mLoss[0m : 2.34085
[1mStep[0m  [20/53], [94mLoss[0m : 2.60673
[1mStep[0m  [25/53], [94mLoss[0m : 2.59591
[1mStep[0m  [30/53], [94mLoss[0m : 2.36097
[1mStep[0m  [35/53], [94mLoss[0m : 2.46426
[1mStep[0m  [40/53], [94mLoss[0m : 2.46251
[1mStep[0m  [45/53], [94mLoss[0m : 2.58267
[1mStep[0m  [50/53], [94mLoss[0m : 2.26051

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37017
[1mStep[0m  [5/53], [94mLoss[0m : 2.83089
[1mStep[0m  [10/53], [94mLoss[0m : 2.69276
[1mStep[0m  [15/53], [94mLoss[0m : 2.54049
[1mStep[0m  [20/53], [94mLoss[0m : 2.30103
[1mStep[0m  [25/53], [94mLoss[0m : 2.59397
[1mStep[0m  [30/53], [94mLoss[0m : 2.30629
[1mStep[0m  [35/53], [94mLoss[0m : 2.52175
[1mStep[0m  [40/53], [94mLoss[0m : 2.39173
[1mStep[0m  [45/53], [94mLoss[0m : 2.33212
[1mStep[0m  [50/53], [94mLoss[0m : 2.27279

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45337
[1mStep[0m  [5/53], [94mLoss[0m : 2.53652
[1mStep[0m  [10/53], [94mLoss[0m : 2.62938
[1mStep[0m  [15/53], [94mLoss[0m : 2.30580
[1mStep[0m  [20/53], [94mLoss[0m : 2.29361
[1mStep[0m  [25/53], [94mLoss[0m : 2.31703
[1mStep[0m  [30/53], [94mLoss[0m : 2.49578
[1mStep[0m  [35/53], [94mLoss[0m : 2.46043
[1mStep[0m  [40/53], [94mLoss[0m : 2.75954
[1mStep[0m  [45/53], [94mLoss[0m : 2.62248
[1mStep[0m  [50/53], [94mLoss[0m : 2.43078

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64009
[1mStep[0m  [5/53], [94mLoss[0m : 2.50201
[1mStep[0m  [10/53], [94mLoss[0m : 2.38443
[1mStep[0m  [15/53], [94mLoss[0m : 2.53538
[1mStep[0m  [20/53], [94mLoss[0m : 2.50233
[1mStep[0m  [25/53], [94mLoss[0m : 2.47706
[1mStep[0m  [30/53], [94mLoss[0m : 2.67812
[1mStep[0m  [35/53], [94mLoss[0m : 2.46350
[1mStep[0m  [40/53], [94mLoss[0m : 2.40813
[1mStep[0m  [45/53], [94mLoss[0m : 2.41622
[1mStep[0m  [50/53], [94mLoss[0m : 2.66897

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40886
[1mStep[0m  [5/53], [94mLoss[0m : 2.44806
[1mStep[0m  [10/53], [94mLoss[0m : 2.54369
[1mStep[0m  [15/53], [94mLoss[0m : 2.39183
[1mStep[0m  [20/53], [94mLoss[0m : 2.26293
[1mStep[0m  [25/53], [94mLoss[0m : 2.23898
[1mStep[0m  [30/53], [94mLoss[0m : 2.35247
[1mStep[0m  [35/53], [94mLoss[0m : 2.55417
[1mStep[0m  [40/53], [94mLoss[0m : 2.49213
[1mStep[0m  [45/53], [94mLoss[0m : 2.51959
[1mStep[0m  [50/53], [94mLoss[0m : 2.86043

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22170
[1mStep[0m  [5/53], [94mLoss[0m : 2.29364
[1mStep[0m  [10/53], [94mLoss[0m : 2.50036
[1mStep[0m  [15/53], [94mLoss[0m : 2.29964
[1mStep[0m  [20/53], [94mLoss[0m : 2.33727
[1mStep[0m  [25/53], [94mLoss[0m : 2.54871
[1mStep[0m  [30/53], [94mLoss[0m : 2.57085
[1mStep[0m  [35/53], [94mLoss[0m : 2.26188
[1mStep[0m  [40/53], [94mLoss[0m : 2.45970
[1mStep[0m  [45/53], [94mLoss[0m : 2.54527
[1mStep[0m  [50/53], [94mLoss[0m : 2.51249

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38972
[1mStep[0m  [5/53], [94mLoss[0m : 2.41365
[1mStep[0m  [10/53], [94mLoss[0m : 2.54717
[1mStep[0m  [15/53], [94mLoss[0m : 2.30795
[1mStep[0m  [20/53], [94mLoss[0m : 2.41210
[1mStep[0m  [25/53], [94mLoss[0m : 2.51263
[1mStep[0m  [30/53], [94mLoss[0m : 2.51090
[1mStep[0m  [35/53], [94mLoss[0m : 2.54019
[1mStep[0m  [40/53], [94mLoss[0m : 2.56448
[1mStep[0m  [45/53], [94mLoss[0m : 2.56625
[1mStep[0m  [50/53], [94mLoss[0m : 2.40096

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42146
[1mStep[0m  [5/53], [94mLoss[0m : 2.31085
[1mStep[0m  [10/53], [94mLoss[0m : 2.56311
[1mStep[0m  [15/53], [94mLoss[0m : 2.51245
[1mStep[0m  [20/53], [94mLoss[0m : 2.40276
[1mStep[0m  [25/53], [94mLoss[0m : 2.26388
[1mStep[0m  [30/53], [94mLoss[0m : 2.63089
[1mStep[0m  [35/53], [94mLoss[0m : 2.44166
[1mStep[0m  [40/53], [94mLoss[0m : 2.72984
[1mStep[0m  [45/53], [94mLoss[0m : 2.36579
[1mStep[0m  [50/53], [94mLoss[0m : 2.46695

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30777
[1mStep[0m  [5/53], [94mLoss[0m : 2.43684
[1mStep[0m  [10/53], [94mLoss[0m : 2.51569
[1mStep[0m  [15/53], [94mLoss[0m : 2.55675
[1mStep[0m  [20/53], [94mLoss[0m : 2.59278
[1mStep[0m  [25/53], [94mLoss[0m : 2.41395
[1mStep[0m  [30/53], [94mLoss[0m : 2.61120
[1mStep[0m  [35/53], [94mLoss[0m : 2.11734
[1mStep[0m  [40/53], [94mLoss[0m : 2.58044
[1mStep[0m  [45/53], [94mLoss[0m : 2.71421
[1mStep[0m  [50/53], [94mLoss[0m : 2.71529

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45290
[1mStep[0m  [5/53], [94mLoss[0m : 2.33001
[1mStep[0m  [10/53], [94mLoss[0m : 2.35860
[1mStep[0m  [15/53], [94mLoss[0m : 2.44039
[1mStep[0m  [20/53], [94mLoss[0m : 2.28032
[1mStep[0m  [25/53], [94mLoss[0m : 2.42644
[1mStep[0m  [30/53], [94mLoss[0m : 2.38937
[1mStep[0m  [35/53], [94mLoss[0m : 2.57094
[1mStep[0m  [40/53], [94mLoss[0m : 2.87895
[1mStep[0m  [45/53], [94mLoss[0m : 2.59929
[1mStep[0m  [50/53], [94mLoss[0m : 2.56712

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56495
[1mStep[0m  [5/53], [94mLoss[0m : 2.51904
[1mStep[0m  [10/53], [94mLoss[0m : 2.43236
[1mStep[0m  [15/53], [94mLoss[0m : 2.36831
[1mStep[0m  [20/53], [94mLoss[0m : 2.37383
[1mStep[0m  [25/53], [94mLoss[0m : 2.45132
[1mStep[0m  [30/53], [94mLoss[0m : 2.56619
[1mStep[0m  [35/53], [94mLoss[0m : 2.39320
[1mStep[0m  [40/53], [94mLoss[0m : 2.21954
[1mStep[0m  [45/53], [94mLoss[0m : 2.43719
[1mStep[0m  [50/53], [94mLoss[0m : 2.44171

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.397, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37659
[1mStep[0m  [5/53], [94mLoss[0m : 2.24283
[1mStep[0m  [10/53], [94mLoss[0m : 2.36306
[1mStep[0m  [15/53], [94mLoss[0m : 2.58962
[1mStep[0m  [20/53], [94mLoss[0m : 2.44113
[1mStep[0m  [25/53], [94mLoss[0m : 2.42600
[1mStep[0m  [30/53], [94mLoss[0m : 2.54272
[1mStep[0m  [35/53], [94mLoss[0m : 2.56668
[1mStep[0m  [40/53], [94mLoss[0m : 2.23330
[1mStep[0m  [45/53], [94mLoss[0m : 2.52772
[1mStep[0m  [50/53], [94mLoss[0m : 2.37424

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47301
[1mStep[0m  [5/53], [94mLoss[0m : 2.67516
[1mStep[0m  [10/53], [94mLoss[0m : 2.51765
[1mStep[0m  [15/53], [94mLoss[0m : 2.36198
[1mStep[0m  [20/53], [94mLoss[0m : 2.32103
[1mStep[0m  [25/53], [94mLoss[0m : 2.48730
[1mStep[0m  [30/53], [94mLoss[0m : 2.32787
[1mStep[0m  [35/53], [94mLoss[0m : 2.35716
[1mStep[0m  [40/53], [94mLoss[0m : 2.52885
[1mStep[0m  [45/53], [94mLoss[0m : 2.46914
[1mStep[0m  [50/53], [94mLoss[0m : 2.54088

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.404, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34521
[1mStep[0m  [5/53], [94mLoss[0m : 2.34502
[1mStep[0m  [10/53], [94mLoss[0m : 2.43893
[1mStep[0m  [15/53], [94mLoss[0m : 2.61919
[1mStep[0m  [20/53], [94mLoss[0m : 2.26819
[1mStep[0m  [25/53], [94mLoss[0m : 2.67196
[1mStep[0m  [30/53], [94mLoss[0m : 2.50099
[1mStep[0m  [35/53], [94mLoss[0m : 2.30749
[1mStep[0m  [40/53], [94mLoss[0m : 2.40890
[1mStep[0m  [45/53], [94mLoss[0m : 2.32214
[1mStep[0m  [50/53], [94mLoss[0m : 2.42182

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.394, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33214
[1mStep[0m  [5/53], [94mLoss[0m : 2.43784
[1mStep[0m  [10/53], [94mLoss[0m : 2.64726
[1mStep[0m  [15/53], [94mLoss[0m : 2.56240
[1mStep[0m  [20/53], [94mLoss[0m : 2.31984
[1mStep[0m  [25/53], [94mLoss[0m : 2.51118
[1mStep[0m  [30/53], [94mLoss[0m : 2.29964
[1mStep[0m  [35/53], [94mLoss[0m : 2.52802
[1mStep[0m  [40/53], [94mLoss[0m : 2.41310
[1mStep[0m  [45/53], [94mLoss[0m : 2.58701
[1mStep[0m  [50/53], [94mLoss[0m : 2.36019

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.395, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49766
[1mStep[0m  [5/53], [94mLoss[0m : 2.52624
[1mStep[0m  [10/53], [94mLoss[0m : 2.74039
[1mStep[0m  [15/53], [94mLoss[0m : 2.35087
[1mStep[0m  [20/53], [94mLoss[0m : 2.39604
[1mStep[0m  [25/53], [94mLoss[0m : 2.30719
[1mStep[0m  [30/53], [94mLoss[0m : 2.42289
[1mStep[0m  [35/53], [94mLoss[0m : 2.37364
[1mStep[0m  [40/53], [94mLoss[0m : 2.40294
[1mStep[0m  [45/53], [94mLoss[0m : 2.42619
[1mStep[0m  [50/53], [94mLoss[0m : 2.40379

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55142
[1mStep[0m  [5/53], [94mLoss[0m : 2.34629
[1mStep[0m  [10/53], [94mLoss[0m : 2.61384
[1mStep[0m  [15/53], [94mLoss[0m : 2.39856
[1mStep[0m  [20/53], [94mLoss[0m : 2.50779
[1mStep[0m  [25/53], [94mLoss[0m : 2.43852
[1mStep[0m  [30/53], [94mLoss[0m : 2.37108
[1mStep[0m  [35/53], [94mLoss[0m : 2.26853
[1mStep[0m  [40/53], [94mLoss[0m : 2.33200
[1mStep[0m  [45/53], [94mLoss[0m : 2.43160
[1mStep[0m  [50/53], [94mLoss[0m : 2.24076

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51907
[1mStep[0m  [5/53], [94mLoss[0m : 2.60337
[1mStep[0m  [10/53], [94mLoss[0m : 2.49125
[1mStep[0m  [15/53], [94mLoss[0m : 2.45452
[1mStep[0m  [20/53], [94mLoss[0m : 2.45609
[1mStep[0m  [25/53], [94mLoss[0m : 2.41362
[1mStep[0m  [30/53], [94mLoss[0m : 2.38743
[1mStep[0m  [35/53], [94mLoss[0m : 2.37962
[1mStep[0m  [40/53], [94mLoss[0m : 2.75988
[1mStep[0m  [45/53], [94mLoss[0m : 2.53898
[1mStep[0m  [50/53], [94mLoss[0m : 2.31711

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49048
[1mStep[0m  [5/53], [94mLoss[0m : 2.44776
[1mStep[0m  [10/53], [94mLoss[0m : 2.46599
[1mStep[0m  [15/53], [94mLoss[0m : 2.61294
[1mStep[0m  [20/53], [94mLoss[0m : 2.62576
[1mStep[0m  [25/53], [94mLoss[0m : 2.42431
[1mStep[0m  [30/53], [94mLoss[0m : 2.29916
[1mStep[0m  [35/53], [94mLoss[0m : 2.54257
[1mStep[0m  [40/53], [94mLoss[0m : 2.56552
[1mStep[0m  [45/53], [94mLoss[0m : 2.44791
[1mStep[0m  [50/53], [94mLoss[0m : 2.47332

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35609
[1mStep[0m  [5/53], [94mLoss[0m : 2.43768
[1mStep[0m  [10/53], [94mLoss[0m : 2.41636
[1mStep[0m  [15/53], [94mLoss[0m : 2.44208
[1mStep[0m  [20/53], [94mLoss[0m : 2.40730
[1mStep[0m  [25/53], [94mLoss[0m : 2.47409
[1mStep[0m  [30/53], [94mLoss[0m : 2.25946
[1mStep[0m  [35/53], [94mLoss[0m : 2.53289
[1mStep[0m  [40/53], [94mLoss[0m : 2.49167
[1mStep[0m  [45/53], [94mLoss[0m : 2.40337
[1mStep[0m  [50/53], [94mLoss[0m : 2.41282

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.397, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.382
====================================

Phase 1 - Evaluation MAE:  2.382296773103567
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.56090
[1mStep[0m  [5/53], [94mLoss[0m : 2.72066
[1mStep[0m  [10/53], [94mLoss[0m : 2.37913
[1mStep[0m  [15/53], [94mLoss[0m : 2.52499
[1mStep[0m  [20/53], [94mLoss[0m : 2.58713
[1mStep[0m  [25/53], [94mLoss[0m : 2.44204
[1mStep[0m  [30/53], [94mLoss[0m : 2.59805
[1mStep[0m  [35/53], [94mLoss[0m : 2.48124
[1mStep[0m  [40/53], [94mLoss[0m : 2.52785
[1mStep[0m  [45/53], [94mLoss[0m : 2.64034
[1mStep[0m  [50/53], [94mLoss[0m : 2.54718

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46806
[1mStep[0m  [5/53], [94mLoss[0m : 2.23175
[1mStep[0m  [10/53], [94mLoss[0m : 2.33932
[1mStep[0m  [15/53], [94mLoss[0m : 2.30129
[1mStep[0m  [20/53], [94mLoss[0m : 2.20046
[1mStep[0m  [25/53], [94mLoss[0m : 2.41944
[1mStep[0m  [30/53], [94mLoss[0m : 2.40380
[1mStep[0m  [35/53], [94mLoss[0m : 2.54843
[1mStep[0m  [40/53], [94mLoss[0m : 2.19247
[1mStep[0m  [45/53], [94mLoss[0m : 2.64770
[1mStep[0m  [50/53], [94mLoss[0m : 2.35402

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34049
[1mStep[0m  [5/53], [94mLoss[0m : 2.23162
[1mStep[0m  [10/53], [94mLoss[0m : 2.37594
[1mStep[0m  [15/53], [94mLoss[0m : 2.38242
[1mStep[0m  [20/53], [94mLoss[0m : 2.40044
[1mStep[0m  [25/53], [94mLoss[0m : 2.34526
[1mStep[0m  [30/53], [94mLoss[0m : 2.27861
[1mStep[0m  [35/53], [94mLoss[0m : 2.46137
[1mStep[0m  [40/53], [94mLoss[0m : 2.25919
[1mStep[0m  [45/53], [94mLoss[0m : 2.48788
[1mStep[0m  [50/53], [94mLoss[0m : 2.32985

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31865
[1mStep[0m  [5/53], [94mLoss[0m : 2.18114
[1mStep[0m  [10/53], [94mLoss[0m : 2.27107
[1mStep[0m  [15/53], [94mLoss[0m : 2.25867
[1mStep[0m  [20/53], [94mLoss[0m : 2.21319
[1mStep[0m  [25/53], [94mLoss[0m : 2.09908
[1mStep[0m  [30/53], [94mLoss[0m : 1.90580
[1mStep[0m  [35/53], [94mLoss[0m : 2.43073
[1mStep[0m  [40/53], [94mLoss[0m : 2.30610
[1mStep[0m  [45/53], [94mLoss[0m : 2.13250
[1mStep[0m  [50/53], [94mLoss[0m : 2.26504

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18972
[1mStep[0m  [5/53], [94mLoss[0m : 2.14381
[1mStep[0m  [10/53], [94mLoss[0m : 1.94508
[1mStep[0m  [15/53], [94mLoss[0m : 2.00311
[1mStep[0m  [20/53], [94mLoss[0m : 2.23397
[1mStep[0m  [25/53], [94mLoss[0m : 1.99018
[1mStep[0m  [30/53], [94mLoss[0m : 2.22704
[1mStep[0m  [35/53], [94mLoss[0m : 2.11088
[1mStep[0m  [40/53], [94mLoss[0m : 2.04504
[1mStep[0m  [45/53], [94mLoss[0m : 2.08034
[1mStep[0m  [50/53], [94mLoss[0m : 2.04673

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.94700
[1mStep[0m  [5/53], [94mLoss[0m : 2.18706
[1mStep[0m  [10/53], [94mLoss[0m : 2.13009
[1mStep[0m  [15/53], [94mLoss[0m : 2.02346
[1mStep[0m  [20/53], [94mLoss[0m : 2.03142
[1mStep[0m  [25/53], [94mLoss[0m : 1.86619
[1mStep[0m  [30/53], [94mLoss[0m : 2.11565
[1mStep[0m  [35/53], [94mLoss[0m : 2.16204
[1mStep[0m  [40/53], [94mLoss[0m : 2.04106
[1mStep[0m  [45/53], [94mLoss[0m : 2.28574
[1mStep[0m  [50/53], [94mLoss[0m : 2.16769

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.073, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.92631
[1mStep[0m  [5/53], [94mLoss[0m : 1.82919
[1mStep[0m  [10/53], [94mLoss[0m : 2.08302
[1mStep[0m  [15/53], [94mLoss[0m : 1.86330
[1mStep[0m  [20/53], [94mLoss[0m : 1.89015
[1mStep[0m  [25/53], [94mLoss[0m : 2.08751
[1mStep[0m  [30/53], [94mLoss[0m : 2.07418
[1mStep[0m  [35/53], [94mLoss[0m : 2.18533
[1mStep[0m  [40/53], [94mLoss[0m : 1.89304
[1mStep[0m  [45/53], [94mLoss[0m : 2.06176
[1mStep[0m  [50/53], [94mLoss[0m : 1.93112

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.74541
[1mStep[0m  [5/53], [94mLoss[0m : 1.96523
[1mStep[0m  [10/53], [94mLoss[0m : 1.88762
[1mStep[0m  [15/53], [94mLoss[0m : 2.06021
[1mStep[0m  [20/53], [94mLoss[0m : 2.02584
[1mStep[0m  [25/53], [94mLoss[0m : 2.03613
[1mStep[0m  [30/53], [94mLoss[0m : 1.78799
[1mStep[0m  [35/53], [94mLoss[0m : 2.12192
[1mStep[0m  [40/53], [94mLoss[0m : 1.90406
[1mStep[0m  [45/53], [94mLoss[0m : 2.01079
[1mStep[0m  [50/53], [94mLoss[0m : 2.09080

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.75713
[1mStep[0m  [5/53], [94mLoss[0m : 1.85462
[1mStep[0m  [10/53], [94mLoss[0m : 1.95018
[1mStep[0m  [15/53], [94mLoss[0m : 1.92799
[1mStep[0m  [20/53], [94mLoss[0m : 1.90431
[1mStep[0m  [25/53], [94mLoss[0m : 1.72305
[1mStep[0m  [30/53], [94mLoss[0m : 2.04978
[1mStep[0m  [35/53], [94mLoss[0m : 1.89374
[1mStep[0m  [40/53], [94mLoss[0m : 1.95851
[1mStep[0m  [45/53], [94mLoss[0m : 1.93803
[1mStep[0m  [50/53], [94mLoss[0m : 1.82191

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.98451
[1mStep[0m  [5/53], [94mLoss[0m : 1.77870
[1mStep[0m  [10/53], [94mLoss[0m : 1.76139
[1mStep[0m  [15/53], [94mLoss[0m : 1.84505
[1mStep[0m  [20/53], [94mLoss[0m : 1.73544
[1mStep[0m  [25/53], [94mLoss[0m : 1.75397
[1mStep[0m  [30/53], [94mLoss[0m : 1.64806
[1mStep[0m  [35/53], [94mLoss[0m : 1.92402
[1mStep[0m  [40/53], [94mLoss[0m : 1.98033
[1mStep[0m  [45/53], [94mLoss[0m : 1.78109
[1mStep[0m  [50/53], [94mLoss[0m : 1.88336

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.81145
[1mStep[0m  [5/53], [94mLoss[0m : 1.66045
[1mStep[0m  [10/53], [94mLoss[0m : 1.70096
[1mStep[0m  [15/53], [94mLoss[0m : 1.75034
[1mStep[0m  [20/53], [94mLoss[0m : 1.67427
[1mStep[0m  [25/53], [94mLoss[0m : 1.82341
[1mStep[0m  [30/53], [94mLoss[0m : 1.82669
[1mStep[0m  [35/53], [94mLoss[0m : 1.85330
[1mStep[0m  [40/53], [94mLoss[0m : 1.75636
[1mStep[0m  [45/53], [94mLoss[0m : 1.82571
[1mStep[0m  [50/53], [94mLoss[0m : 1.70859

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68811
[1mStep[0m  [5/53], [94mLoss[0m : 1.77618
[1mStep[0m  [10/53], [94mLoss[0m : 1.71118
[1mStep[0m  [15/53], [94mLoss[0m : 1.85302
[1mStep[0m  [20/53], [94mLoss[0m : 1.67888
[1mStep[0m  [25/53], [94mLoss[0m : 1.87564
[1mStep[0m  [30/53], [94mLoss[0m : 1.86356
[1mStep[0m  [35/53], [94mLoss[0m : 1.83487
[1mStep[0m  [40/53], [94mLoss[0m : 1.72334
[1mStep[0m  [45/53], [94mLoss[0m : 1.90718
[1mStep[0m  [50/53], [94mLoss[0m : 1.68175

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.65157
[1mStep[0m  [5/53], [94mLoss[0m : 1.67038
[1mStep[0m  [10/53], [94mLoss[0m : 1.61609
[1mStep[0m  [15/53], [94mLoss[0m : 1.70674
[1mStep[0m  [20/53], [94mLoss[0m : 1.71693
[1mStep[0m  [25/53], [94mLoss[0m : 1.75790
[1mStep[0m  [30/53], [94mLoss[0m : 1.68665
[1mStep[0m  [35/53], [94mLoss[0m : 1.46599
[1mStep[0m  [40/53], [94mLoss[0m : 1.82767
[1mStep[0m  [45/53], [94mLoss[0m : 1.65328
[1mStep[0m  [50/53], [94mLoss[0m : 1.82042

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63864
[1mStep[0m  [5/53], [94mLoss[0m : 1.55965
[1mStep[0m  [10/53], [94mLoss[0m : 1.68393
[1mStep[0m  [15/53], [94mLoss[0m : 1.45863
[1mStep[0m  [20/53], [94mLoss[0m : 1.64935
[1mStep[0m  [25/53], [94mLoss[0m : 1.83870
[1mStep[0m  [30/53], [94mLoss[0m : 1.63782
[1mStep[0m  [35/53], [94mLoss[0m : 1.67188
[1mStep[0m  [40/53], [94mLoss[0m : 1.78490
[1mStep[0m  [45/53], [94mLoss[0m : 1.56211
[1mStep[0m  [50/53], [94mLoss[0m : 1.70624

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63163
[1mStep[0m  [5/53], [94mLoss[0m : 1.46236
[1mStep[0m  [10/53], [94mLoss[0m : 1.81142
[1mStep[0m  [15/53], [94mLoss[0m : 1.82617
[1mStep[0m  [20/53], [94mLoss[0m : 1.69203
[1mStep[0m  [25/53], [94mLoss[0m : 1.64655
[1mStep[0m  [30/53], [94mLoss[0m : 1.69976
[1mStep[0m  [35/53], [94mLoss[0m : 1.60334
[1mStep[0m  [40/53], [94mLoss[0m : 1.61834
[1mStep[0m  [45/53], [94mLoss[0m : 1.71029
[1mStep[0m  [50/53], [94mLoss[0m : 1.81215

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.568, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.53640
[1mStep[0m  [5/53], [94mLoss[0m : 1.52745
[1mStep[0m  [10/53], [94mLoss[0m : 1.68974
[1mStep[0m  [15/53], [94mLoss[0m : 1.52117
[1mStep[0m  [20/53], [94mLoss[0m : 1.59140
[1mStep[0m  [25/53], [94mLoss[0m : 1.57181
[1mStep[0m  [30/53], [94mLoss[0m : 1.60154
[1mStep[0m  [35/53], [94mLoss[0m : 1.41736
[1mStep[0m  [40/53], [94mLoss[0m : 1.49153
[1mStep[0m  [45/53], [94mLoss[0m : 1.56893
[1mStep[0m  [50/53], [94mLoss[0m : 1.61318

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.583, [92mTest[0m: 2.489, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.61926
[1mStep[0m  [5/53], [94mLoss[0m : 1.61191
[1mStep[0m  [10/53], [94mLoss[0m : 1.47571
[1mStep[0m  [15/53], [94mLoss[0m : 1.64721
[1mStep[0m  [20/53], [94mLoss[0m : 1.49288
[1mStep[0m  [25/53], [94mLoss[0m : 1.56570
[1mStep[0m  [30/53], [94mLoss[0m : 1.61619
[1mStep[0m  [35/53], [94mLoss[0m : 1.69858
[1mStep[0m  [40/53], [94mLoss[0m : 1.57284
[1mStep[0m  [45/53], [94mLoss[0m : 1.65382
[1mStep[0m  [50/53], [94mLoss[0m : 1.55013

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.532, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.56959
[1mStep[0m  [5/53], [94mLoss[0m : 1.46040
[1mStep[0m  [10/53], [94mLoss[0m : 1.46486
[1mStep[0m  [15/53], [94mLoss[0m : 1.60953
[1mStep[0m  [20/53], [94mLoss[0m : 1.53777
[1mStep[0m  [25/53], [94mLoss[0m : 1.55683
[1mStep[0m  [30/53], [94mLoss[0m : 1.42786
[1mStep[0m  [35/53], [94mLoss[0m : 1.61935
[1mStep[0m  [40/53], [94mLoss[0m : 1.52734
[1mStep[0m  [45/53], [94mLoss[0m : 1.62435
[1mStep[0m  [50/53], [94mLoss[0m : 1.54000

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.532, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.37531
[1mStep[0m  [5/53], [94mLoss[0m : 1.48802
[1mStep[0m  [10/53], [94mLoss[0m : 1.38689
[1mStep[0m  [15/53], [94mLoss[0m : 1.65832
[1mStep[0m  [20/53], [94mLoss[0m : 1.40369
[1mStep[0m  [25/53], [94mLoss[0m : 1.36018
[1mStep[0m  [30/53], [94mLoss[0m : 1.42723
[1mStep[0m  [35/53], [94mLoss[0m : 1.42837
[1mStep[0m  [40/53], [94mLoss[0m : 1.54424
[1mStep[0m  [45/53], [94mLoss[0m : 1.61259
[1mStep[0m  [50/53], [94mLoss[0m : 1.61705

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.49595
[1mStep[0m  [5/53], [94mLoss[0m : 1.52452
[1mStep[0m  [10/53], [94mLoss[0m : 1.54355
[1mStep[0m  [15/53], [94mLoss[0m : 1.60713
[1mStep[0m  [20/53], [94mLoss[0m : 1.50470
[1mStep[0m  [25/53], [94mLoss[0m : 1.48405
[1mStep[0m  [30/53], [94mLoss[0m : 1.41895
[1mStep[0m  [35/53], [94mLoss[0m : 1.27540
[1mStep[0m  [40/53], [94mLoss[0m : 1.49396
[1mStep[0m  [45/53], [94mLoss[0m : 1.57046
[1mStep[0m  [50/53], [94mLoss[0m : 1.49383

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.577, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.58145
[1mStep[0m  [5/53], [94mLoss[0m : 1.30034
[1mStep[0m  [10/53], [94mLoss[0m : 1.51004
[1mStep[0m  [15/53], [94mLoss[0m : 1.52512
[1mStep[0m  [20/53], [94mLoss[0m : 1.43276
[1mStep[0m  [25/53], [94mLoss[0m : 1.42989
[1mStep[0m  [30/53], [94mLoss[0m : 1.53454
[1mStep[0m  [35/53], [94mLoss[0m : 1.41265
[1mStep[0m  [40/53], [94mLoss[0m : 1.30436
[1mStep[0m  [45/53], [94mLoss[0m : 1.52484
[1mStep[0m  [50/53], [94mLoss[0m : 1.51949

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.444, [92mTest[0m: 2.570, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.25547
[1mStep[0m  [5/53], [94mLoss[0m : 1.42037
[1mStep[0m  [10/53], [94mLoss[0m : 1.45896
[1mStep[0m  [15/53], [94mLoss[0m : 1.26490
[1mStep[0m  [20/53], [94mLoss[0m : 1.40520
[1mStep[0m  [25/53], [94mLoss[0m : 1.46015
[1mStep[0m  [30/53], [94mLoss[0m : 1.31678
[1mStep[0m  [35/53], [94mLoss[0m : 1.50110
[1mStep[0m  [40/53], [94mLoss[0m : 1.46533
[1mStep[0m  [45/53], [94mLoss[0m : 1.46602
[1mStep[0m  [50/53], [94mLoss[0m : 1.47389

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.408, [92mTest[0m: 2.618, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.38696
[1mStep[0m  [5/53], [94mLoss[0m : 1.27140
[1mStep[0m  [10/53], [94mLoss[0m : 1.31334
[1mStep[0m  [15/53], [94mLoss[0m : 1.39707
[1mStep[0m  [20/53], [94mLoss[0m : 1.47506
[1mStep[0m  [25/53], [94mLoss[0m : 1.37269
[1mStep[0m  [30/53], [94mLoss[0m : 1.26975
[1mStep[0m  [35/53], [94mLoss[0m : 1.51734
[1mStep[0m  [40/53], [94mLoss[0m : 1.36470
[1mStep[0m  [45/53], [94mLoss[0m : 1.30131
[1mStep[0m  [50/53], [94mLoss[0m : 1.38428

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.396, [92mTest[0m: 2.499, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.28266
[1mStep[0m  [5/53], [94mLoss[0m : 1.28360
[1mStep[0m  [10/53], [94mLoss[0m : 1.19519
[1mStep[0m  [15/53], [94mLoss[0m : 1.29557
[1mStep[0m  [20/53], [94mLoss[0m : 1.31362
[1mStep[0m  [25/53], [94mLoss[0m : 1.43174
[1mStep[0m  [30/53], [94mLoss[0m : 1.27853
[1mStep[0m  [35/53], [94mLoss[0m : 1.43462
[1mStep[0m  [40/53], [94mLoss[0m : 1.52206
[1mStep[0m  [45/53], [94mLoss[0m : 1.38738
[1mStep[0m  [50/53], [94mLoss[0m : 1.46135

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.384, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.549
====================================

Phase 2 - Evaluation MAE:  2.5490192358310404
MAE score P1        2.382297
MAE score P2        2.549019
loss                1.384303
learning_rate           0.01
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 11.07633
[1mStep[0m  [2/26], [94mLoss[0m : 10.26465
[1mStep[0m  [4/26], [94mLoss[0m : 10.28877
[1mStep[0m  [6/26], [94mLoss[0m : 10.21782
[1mStep[0m  [8/26], [94mLoss[0m : 9.77149
[1mStep[0m  [10/26], [94mLoss[0m : 9.27204
[1mStep[0m  [12/26], [94mLoss[0m : 8.94769
[1mStep[0m  [14/26], [94mLoss[0m : 8.61467
[1mStep[0m  [16/26], [94mLoss[0m : 8.21311
[1mStep[0m  [18/26], [94mLoss[0m : 8.01994
[1mStep[0m  [20/26], [94mLoss[0m : 7.68661
[1mStep[0m  [22/26], [94mLoss[0m : 7.57861
[1mStep[0m  [24/26], [94mLoss[0m : 6.80090

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.945, [92mTest[0m: 10.833, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.36858
[1mStep[0m  [2/26], [94mLoss[0m : 6.21171
[1mStep[0m  [4/26], [94mLoss[0m : 5.84565
[1mStep[0m  [6/26], [94mLoss[0m : 5.80690
[1mStep[0m  [8/26], [94mLoss[0m : 5.16337
[1mStep[0m  [10/26], [94mLoss[0m : 4.79082
[1mStep[0m  [12/26], [94mLoss[0m : 4.51041
[1mStep[0m  [14/26], [94mLoss[0m : 4.24522
[1mStep[0m  [16/26], [94mLoss[0m : 4.12181
[1mStep[0m  [18/26], [94mLoss[0m : 4.00496
[1mStep[0m  [20/26], [94mLoss[0m : 3.64650
[1mStep[0m  [22/26], [94mLoss[0m : 3.45415
[1mStep[0m  [24/26], [94mLoss[0m : 3.60197

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.671, [92mTest[0m: 8.822, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.52276
[1mStep[0m  [2/26], [94mLoss[0m : 3.25383
[1mStep[0m  [4/26], [94mLoss[0m : 3.12873
[1mStep[0m  [6/26], [94mLoss[0m : 3.42075
[1mStep[0m  [8/26], [94mLoss[0m : 3.21764
[1mStep[0m  [10/26], [94mLoss[0m : 2.82224
[1mStep[0m  [12/26], [94mLoss[0m : 2.99688
[1mStep[0m  [14/26], [94mLoss[0m : 3.02577
[1mStep[0m  [16/26], [94mLoss[0m : 3.01574
[1mStep[0m  [18/26], [94mLoss[0m : 2.89356
[1mStep[0m  [20/26], [94mLoss[0m : 3.10649
[1mStep[0m  [22/26], [94mLoss[0m : 2.89112
[1mStep[0m  [24/26], [94mLoss[0m : 3.14084

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.115, [92mTest[0m: 5.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.92905
[1mStep[0m  [2/26], [94mLoss[0m : 3.04860
[1mStep[0m  [4/26], [94mLoss[0m : 2.93567
[1mStep[0m  [6/26], [94mLoss[0m : 3.09150
[1mStep[0m  [8/26], [94mLoss[0m : 3.10642
[1mStep[0m  [10/26], [94mLoss[0m : 3.04897
[1mStep[0m  [12/26], [94mLoss[0m : 3.03682
[1mStep[0m  [14/26], [94mLoss[0m : 3.00810
[1mStep[0m  [16/26], [94mLoss[0m : 2.79968
[1mStep[0m  [18/26], [94mLoss[0m : 3.03123
[1mStep[0m  [20/26], [94mLoss[0m : 2.92829
[1mStep[0m  [22/26], [94mLoss[0m : 2.92879
[1mStep[0m  [24/26], [94mLoss[0m : 2.91988

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.998, [92mTest[0m: 3.591, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.85403
[1mStep[0m  [2/26], [94mLoss[0m : 3.02155
[1mStep[0m  [4/26], [94mLoss[0m : 2.98774
[1mStep[0m  [6/26], [94mLoss[0m : 2.79774
[1mStep[0m  [8/26], [94mLoss[0m : 3.05071
[1mStep[0m  [10/26], [94mLoss[0m : 2.84170
[1mStep[0m  [12/26], [94mLoss[0m : 2.97419
[1mStep[0m  [14/26], [94mLoss[0m : 2.80950
[1mStep[0m  [16/26], [94mLoss[0m : 2.92651
[1mStep[0m  [18/26], [94mLoss[0m : 2.79394
[1mStep[0m  [20/26], [94mLoss[0m : 2.91245
[1mStep[0m  [22/26], [94mLoss[0m : 2.98067
[1mStep[0m  [24/26], [94mLoss[0m : 2.77951

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.925, [92mTest[0m: 3.133, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79075
[1mStep[0m  [2/26], [94mLoss[0m : 2.96924
[1mStep[0m  [4/26], [94mLoss[0m : 2.89654
[1mStep[0m  [6/26], [94mLoss[0m : 3.05363
[1mStep[0m  [8/26], [94mLoss[0m : 2.81209
[1mStep[0m  [10/26], [94mLoss[0m : 2.95569
[1mStep[0m  [12/26], [94mLoss[0m : 3.16817
[1mStep[0m  [14/26], [94mLoss[0m : 2.83723
[1mStep[0m  [16/26], [94mLoss[0m : 2.89214
[1mStep[0m  [18/26], [94mLoss[0m : 2.81502
[1mStep[0m  [20/26], [94mLoss[0m : 2.94031
[1mStep[0m  [22/26], [94mLoss[0m : 2.83934
[1mStep[0m  [24/26], [94mLoss[0m : 2.95053

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.920, [92mTest[0m: 3.079, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.93409
[1mStep[0m  [2/26], [94mLoss[0m : 2.81741
[1mStep[0m  [4/26], [94mLoss[0m : 2.73190
[1mStep[0m  [6/26], [94mLoss[0m : 2.80089
[1mStep[0m  [8/26], [94mLoss[0m : 2.83839
[1mStep[0m  [10/26], [94mLoss[0m : 2.89233
[1mStep[0m  [12/26], [94mLoss[0m : 2.88208
[1mStep[0m  [14/26], [94mLoss[0m : 2.98174
[1mStep[0m  [16/26], [94mLoss[0m : 2.83739
[1mStep[0m  [18/26], [94mLoss[0m : 2.74062
[1mStep[0m  [20/26], [94mLoss[0m : 2.75110
[1mStep[0m  [22/26], [94mLoss[0m : 2.83430
[1mStep[0m  [24/26], [94mLoss[0m : 2.85098

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.874, [92mTest[0m: 2.782, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78743
[1mStep[0m  [2/26], [94mLoss[0m : 2.74854
[1mStep[0m  [4/26], [94mLoss[0m : 2.81121
[1mStep[0m  [6/26], [94mLoss[0m : 2.93482
[1mStep[0m  [8/26], [94mLoss[0m : 2.79093
[1mStep[0m  [10/26], [94mLoss[0m : 2.83737
[1mStep[0m  [12/26], [94mLoss[0m : 2.83487
[1mStep[0m  [14/26], [94mLoss[0m : 2.82715
[1mStep[0m  [16/26], [94mLoss[0m : 2.72748
[1mStep[0m  [18/26], [94mLoss[0m : 3.01374
[1mStep[0m  [20/26], [94mLoss[0m : 2.92082
[1mStep[0m  [22/26], [94mLoss[0m : 2.86050
[1mStep[0m  [24/26], [94mLoss[0m : 2.80612

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.826, [92mTest[0m: 2.774, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.84687
[1mStep[0m  [2/26], [94mLoss[0m : 3.03279
[1mStep[0m  [4/26], [94mLoss[0m : 2.67042
[1mStep[0m  [6/26], [94mLoss[0m : 2.67894
[1mStep[0m  [8/26], [94mLoss[0m : 2.86653
[1mStep[0m  [10/26], [94mLoss[0m : 2.99722
[1mStep[0m  [12/26], [94mLoss[0m : 3.08956
[1mStep[0m  [14/26], [94mLoss[0m : 2.75428
[1mStep[0m  [16/26], [94mLoss[0m : 2.82791
[1mStep[0m  [18/26], [94mLoss[0m : 2.84410
[1mStep[0m  [20/26], [94mLoss[0m : 2.96004
[1mStep[0m  [22/26], [94mLoss[0m : 2.66760
[1mStep[0m  [24/26], [94mLoss[0m : 2.86966

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.856, [92mTest[0m: 2.722, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68809
[1mStep[0m  [2/26], [94mLoss[0m : 2.74179
[1mStep[0m  [4/26], [94mLoss[0m : 2.88317
[1mStep[0m  [6/26], [94mLoss[0m : 2.77812
[1mStep[0m  [8/26], [94mLoss[0m : 2.87714
[1mStep[0m  [10/26], [94mLoss[0m : 2.81867
[1mStep[0m  [12/26], [94mLoss[0m : 2.91814
[1mStep[0m  [14/26], [94mLoss[0m : 2.74511
[1mStep[0m  [16/26], [94mLoss[0m : 2.68169
[1mStep[0m  [18/26], [94mLoss[0m : 2.89145
[1mStep[0m  [20/26], [94mLoss[0m : 3.16368
[1mStep[0m  [22/26], [94mLoss[0m : 2.79807
[1mStep[0m  [24/26], [94mLoss[0m : 2.73964

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.650, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.94061
[1mStep[0m  [2/26], [94mLoss[0m : 2.88700
[1mStep[0m  [4/26], [94mLoss[0m : 2.80372
[1mStep[0m  [6/26], [94mLoss[0m : 2.80897
[1mStep[0m  [8/26], [94mLoss[0m : 2.78903
[1mStep[0m  [10/26], [94mLoss[0m : 2.88287
[1mStep[0m  [12/26], [94mLoss[0m : 2.94828
[1mStep[0m  [14/26], [94mLoss[0m : 2.74753
[1mStep[0m  [16/26], [94mLoss[0m : 2.86614
[1mStep[0m  [18/26], [94mLoss[0m : 2.75221
[1mStep[0m  [20/26], [94mLoss[0m : 2.58384
[1mStep[0m  [22/26], [94mLoss[0m : 2.72484
[1mStep[0m  [24/26], [94mLoss[0m : 2.71655

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.808, [92mTest[0m: 2.663, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74526
[1mStep[0m  [2/26], [94mLoss[0m : 2.71374
[1mStep[0m  [4/26], [94mLoss[0m : 2.76547
[1mStep[0m  [6/26], [94mLoss[0m : 2.87754
[1mStep[0m  [8/26], [94mLoss[0m : 2.75401
[1mStep[0m  [10/26], [94mLoss[0m : 2.88051
[1mStep[0m  [12/26], [94mLoss[0m : 2.81599
[1mStep[0m  [14/26], [94mLoss[0m : 2.63931
[1mStep[0m  [16/26], [94mLoss[0m : 2.75010
[1mStep[0m  [18/26], [94mLoss[0m : 2.90172
[1mStep[0m  [20/26], [94mLoss[0m : 2.80291
[1mStep[0m  [22/26], [94mLoss[0m : 2.69610
[1mStep[0m  [24/26], [94mLoss[0m : 2.78792

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.782, [92mTest[0m: 2.601, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75914
[1mStep[0m  [2/26], [94mLoss[0m : 2.66957
[1mStep[0m  [4/26], [94mLoss[0m : 2.71500
[1mStep[0m  [6/26], [94mLoss[0m : 2.76326
[1mStep[0m  [8/26], [94mLoss[0m : 2.85797
[1mStep[0m  [10/26], [94mLoss[0m : 2.71664
[1mStep[0m  [12/26], [94mLoss[0m : 2.83504
[1mStep[0m  [14/26], [94mLoss[0m : 2.65991
[1mStep[0m  [16/26], [94mLoss[0m : 2.82712
[1mStep[0m  [18/26], [94mLoss[0m : 2.80440
[1mStep[0m  [20/26], [94mLoss[0m : 2.87783
[1mStep[0m  [22/26], [94mLoss[0m : 2.67403
[1mStep[0m  [24/26], [94mLoss[0m : 2.75989

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.778, [92mTest[0m: 2.559, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.82291
[1mStep[0m  [2/26], [94mLoss[0m : 2.75063
[1mStep[0m  [4/26], [94mLoss[0m : 2.83556
[1mStep[0m  [6/26], [94mLoss[0m : 2.80134
[1mStep[0m  [8/26], [94mLoss[0m : 2.78600
[1mStep[0m  [10/26], [94mLoss[0m : 2.62470
[1mStep[0m  [12/26], [94mLoss[0m : 2.95690
[1mStep[0m  [14/26], [94mLoss[0m : 2.72634
[1mStep[0m  [16/26], [94mLoss[0m : 2.80751
[1mStep[0m  [18/26], [94mLoss[0m : 2.81779
[1mStep[0m  [20/26], [94mLoss[0m : 2.67575
[1mStep[0m  [22/26], [94mLoss[0m : 2.96021
[1mStep[0m  [24/26], [94mLoss[0m : 2.76187

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.804, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72904
[1mStep[0m  [2/26], [94mLoss[0m : 2.96868
[1mStep[0m  [4/26], [94mLoss[0m : 2.71740
[1mStep[0m  [6/26], [94mLoss[0m : 2.77051
[1mStep[0m  [8/26], [94mLoss[0m : 2.63767
[1mStep[0m  [10/26], [94mLoss[0m : 2.66215
[1mStep[0m  [12/26], [94mLoss[0m : 2.86818
[1mStep[0m  [14/26], [94mLoss[0m : 2.68352
[1mStep[0m  [16/26], [94mLoss[0m : 2.78402
[1mStep[0m  [18/26], [94mLoss[0m : 2.80034
[1mStep[0m  [20/26], [94mLoss[0m : 2.72465
[1mStep[0m  [22/26], [94mLoss[0m : 2.79657
[1mStep[0m  [24/26], [94mLoss[0m : 2.66759

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.562, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74609
[1mStep[0m  [2/26], [94mLoss[0m : 2.69641
[1mStep[0m  [4/26], [94mLoss[0m : 2.76622
[1mStep[0m  [6/26], [94mLoss[0m : 2.83022
[1mStep[0m  [8/26], [94mLoss[0m : 2.60257
[1mStep[0m  [10/26], [94mLoss[0m : 2.59695
[1mStep[0m  [12/26], [94mLoss[0m : 2.76606
[1mStep[0m  [14/26], [94mLoss[0m : 2.67657
[1mStep[0m  [16/26], [94mLoss[0m : 2.78245
[1mStep[0m  [18/26], [94mLoss[0m : 2.61798
[1mStep[0m  [20/26], [94mLoss[0m : 2.65815
[1mStep[0m  [22/26], [94mLoss[0m : 2.82357
[1mStep[0m  [24/26], [94mLoss[0m : 2.69504

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.747, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78323
[1mStep[0m  [2/26], [94mLoss[0m : 2.64620
[1mStep[0m  [4/26], [94mLoss[0m : 2.50500
[1mStep[0m  [6/26], [94mLoss[0m : 2.69609
[1mStep[0m  [8/26], [94mLoss[0m : 2.65672
[1mStep[0m  [10/26], [94mLoss[0m : 2.78765
[1mStep[0m  [12/26], [94mLoss[0m : 2.87744
[1mStep[0m  [14/26], [94mLoss[0m : 2.59892
[1mStep[0m  [16/26], [94mLoss[0m : 2.76417
[1mStep[0m  [18/26], [94mLoss[0m : 2.74238
[1mStep[0m  [20/26], [94mLoss[0m : 2.68783
[1mStep[0m  [22/26], [94mLoss[0m : 2.57008
[1mStep[0m  [24/26], [94mLoss[0m : 2.75426

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70885
[1mStep[0m  [2/26], [94mLoss[0m : 2.69888
[1mStep[0m  [4/26], [94mLoss[0m : 2.49596
[1mStep[0m  [6/26], [94mLoss[0m : 2.78934
[1mStep[0m  [8/26], [94mLoss[0m : 2.86849
[1mStep[0m  [10/26], [94mLoss[0m : 2.67133
[1mStep[0m  [12/26], [94mLoss[0m : 2.64222
[1mStep[0m  [14/26], [94mLoss[0m : 2.79032
[1mStep[0m  [16/26], [94mLoss[0m : 2.73118
[1mStep[0m  [18/26], [94mLoss[0m : 2.71335
[1mStep[0m  [20/26], [94mLoss[0m : 2.85269
[1mStep[0m  [22/26], [94mLoss[0m : 2.81523
[1mStep[0m  [24/26], [94mLoss[0m : 2.71693

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.90253
[1mStep[0m  [2/26], [94mLoss[0m : 2.72172
[1mStep[0m  [4/26], [94mLoss[0m : 2.62814
[1mStep[0m  [6/26], [94mLoss[0m : 2.71060
[1mStep[0m  [8/26], [94mLoss[0m : 2.74725
[1mStep[0m  [10/26], [94mLoss[0m : 2.86400
[1mStep[0m  [12/26], [94mLoss[0m : 2.61352
[1mStep[0m  [14/26], [94mLoss[0m : 2.85206
[1mStep[0m  [16/26], [94mLoss[0m : 2.69008
[1mStep[0m  [18/26], [94mLoss[0m : 2.70319
[1mStep[0m  [20/26], [94mLoss[0m : 2.72971
[1mStep[0m  [22/26], [94mLoss[0m : 2.48943
[1mStep[0m  [24/26], [94mLoss[0m : 2.73142

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67855
[1mStep[0m  [2/26], [94mLoss[0m : 2.74328
[1mStep[0m  [4/26], [94mLoss[0m : 2.81482
[1mStep[0m  [6/26], [94mLoss[0m : 2.60349
[1mStep[0m  [8/26], [94mLoss[0m : 2.70471
[1mStep[0m  [10/26], [94mLoss[0m : 2.82309
[1mStep[0m  [12/26], [94mLoss[0m : 2.72934
[1mStep[0m  [14/26], [94mLoss[0m : 2.75068
[1mStep[0m  [16/26], [94mLoss[0m : 2.71527
[1mStep[0m  [18/26], [94mLoss[0m : 2.59782
[1mStep[0m  [20/26], [94mLoss[0m : 2.69107
[1mStep[0m  [22/26], [94mLoss[0m : 2.76207
[1mStep[0m  [24/26], [94mLoss[0m : 2.80143

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.730, [92mTest[0m: 2.461, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69477
[1mStep[0m  [2/26], [94mLoss[0m : 2.65451
[1mStep[0m  [4/26], [94mLoss[0m : 2.74683
[1mStep[0m  [6/26], [94mLoss[0m : 2.81959
[1mStep[0m  [8/26], [94mLoss[0m : 2.56056
[1mStep[0m  [10/26], [94mLoss[0m : 2.81293
[1mStep[0m  [12/26], [94mLoss[0m : 2.75852
[1mStep[0m  [14/26], [94mLoss[0m : 2.72617
[1mStep[0m  [16/26], [94mLoss[0m : 2.67090
[1mStep[0m  [18/26], [94mLoss[0m : 2.78784
[1mStep[0m  [20/26], [94mLoss[0m : 2.65998
[1mStep[0m  [22/26], [94mLoss[0m : 2.77308
[1mStep[0m  [24/26], [94mLoss[0m : 2.67849

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.715, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70311
[1mStep[0m  [2/26], [94mLoss[0m : 2.59879
[1mStep[0m  [4/26], [94mLoss[0m : 2.57929
[1mStep[0m  [6/26], [94mLoss[0m : 2.63522
[1mStep[0m  [8/26], [94mLoss[0m : 2.74661
[1mStep[0m  [10/26], [94mLoss[0m : 2.64922
[1mStep[0m  [12/26], [94mLoss[0m : 2.66628
[1mStep[0m  [14/26], [94mLoss[0m : 2.63426
[1mStep[0m  [16/26], [94mLoss[0m : 2.76395
[1mStep[0m  [18/26], [94mLoss[0m : 2.73193
[1mStep[0m  [20/26], [94mLoss[0m : 2.79039
[1mStep[0m  [22/26], [94mLoss[0m : 2.51516
[1mStep[0m  [24/26], [94mLoss[0m : 2.67600

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.688, [92mTest[0m: 2.444, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79785
[1mStep[0m  [2/26], [94mLoss[0m : 2.64994
[1mStep[0m  [4/26], [94mLoss[0m : 2.62462
[1mStep[0m  [6/26], [94mLoss[0m : 2.69077
[1mStep[0m  [8/26], [94mLoss[0m : 2.73359
[1mStep[0m  [10/26], [94mLoss[0m : 2.66665
[1mStep[0m  [12/26], [94mLoss[0m : 2.68032
[1mStep[0m  [14/26], [94mLoss[0m : 2.72102
[1mStep[0m  [16/26], [94mLoss[0m : 2.60597
[1mStep[0m  [18/26], [94mLoss[0m : 2.81450
[1mStep[0m  [20/26], [94mLoss[0m : 2.55150
[1mStep[0m  [22/26], [94mLoss[0m : 2.66026
[1mStep[0m  [24/26], [94mLoss[0m : 2.81693

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78076
[1mStep[0m  [2/26], [94mLoss[0m : 2.69649
[1mStep[0m  [4/26], [94mLoss[0m : 2.61859
[1mStep[0m  [6/26], [94mLoss[0m : 2.63947
[1mStep[0m  [8/26], [94mLoss[0m : 2.57355
[1mStep[0m  [10/26], [94mLoss[0m : 2.69728
[1mStep[0m  [12/26], [94mLoss[0m : 2.85046
[1mStep[0m  [14/26], [94mLoss[0m : 2.81458
[1mStep[0m  [16/26], [94mLoss[0m : 2.88745
[1mStep[0m  [18/26], [94mLoss[0m : 2.75224
[1mStep[0m  [20/26], [94mLoss[0m : 2.66590
[1mStep[0m  [22/26], [94mLoss[0m : 2.51683
[1mStep[0m  [24/26], [94mLoss[0m : 2.59070

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62418
[1mStep[0m  [2/26], [94mLoss[0m : 2.64415
[1mStep[0m  [4/26], [94mLoss[0m : 2.66731
[1mStep[0m  [6/26], [94mLoss[0m : 2.81263
[1mStep[0m  [8/26], [94mLoss[0m : 2.91870
[1mStep[0m  [10/26], [94mLoss[0m : 2.70122
[1mStep[0m  [12/26], [94mLoss[0m : 2.73814
[1mStep[0m  [14/26], [94mLoss[0m : 2.66824
[1mStep[0m  [16/26], [94mLoss[0m : 2.66063
[1mStep[0m  [18/26], [94mLoss[0m : 2.69340
[1mStep[0m  [20/26], [94mLoss[0m : 2.78665
[1mStep[0m  [22/26], [94mLoss[0m : 2.74062
[1mStep[0m  [24/26], [94mLoss[0m : 2.77010

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65841
[1mStep[0m  [2/26], [94mLoss[0m : 2.50832
[1mStep[0m  [4/26], [94mLoss[0m : 2.73379
[1mStep[0m  [6/26], [94mLoss[0m : 2.55776
[1mStep[0m  [8/26], [94mLoss[0m : 2.80075
[1mStep[0m  [10/26], [94mLoss[0m : 2.51325
[1mStep[0m  [12/26], [94mLoss[0m : 2.73102
[1mStep[0m  [14/26], [94mLoss[0m : 2.55673
[1mStep[0m  [16/26], [94mLoss[0m : 2.77349
[1mStep[0m  [18/26], [94mLoss[0m : 2.67497
[1mStep[0m  [20/26], [94mLoss[0m : 2.66564
[1mStep[0m  [22/26], [94mLoss[0m : 2.62079
[1mStep[0m  [24/26], [94mLoss[0m : 2.79481

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.458, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78549
[1mStep[0m  [2/26], [94mLoss[0m : 2.65969
[1mStep[0m  [4/26], [94mLoss[0m : 2.66745
[1mStep[0m  [6/26], [94mLoss[0m : 2.71395
[1mStep[0m  [8/26], [94mLoss[0m : 2.57074
[1mStep[0m  [10/26], [94mLoss[0m : 2.71981
[1mStep[0m  [12/26], [94mLoss[0m : 2.71842
[1mStep[0m  [14/26], [94mLoss[0m : 2.68001
[1mStep[0m  [16/26], [94mLoss[0m : 2.67247
[1mStep[0m  [18/26], [94mLoss[0m : 2.64766
[1mStep[0m  [20/26], [94mLoss[0m : 2.64422
[1mStep[0m  [22/26], [94mLoss[0m : 2.53033
[1mStep[0m  [24/26], [94mLoss[0m : 2.64173

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.457, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69825
[1mStep[0m  [2/26], [94mLoss[0m : 2.80429
[1mStep[0m  [4/26], [94mLoss[0m : 2.65899
[1mStep[0m  [6/26], [94mLoss[0m : 2.56349
[1mStep[0m  [8/26], [94mLoss[0m : 2.76204
[1mStep[0m  [10/26], [94mLoss[0m : 2.49347
[1mStep[0m  [12/26], [94mLoss[0m : 2.56424
[1mStep[0m  [14/26], [94mLoss[0m : 2.50031
[1mStep[0m  [16/26], [94mLoss[0m : 2.82521
[1mStep[0m  [18/26], [94mLoss[0m : 2.72864
[1mStep[0m  [20/26], [94mLoss[0m : 2.63023
[1mStep[0m  [22/26], [94mLoss[0m : 2.72652
[1mStep[0m  [24/26], [94mLoss[0m : 2.68239

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63246
[1mStep[0m  [2/26], [94mLoss[0m : 2.54414
[1mStep[0m  [4/26], [94mLoss[0m : 2.67784
[1mStep[0m  [6/26], [94mLoss[0m : 2.84067
[1mStep[0m  [8/26], [94mLoss[0m : 2.64503
[1mStep[0m  [10/26], [94mLoss[0m : 2.85653
[1mStep[0m  [12/26], [94mLoss[0m : 2.73730
[1mStep[0m  [14/26], [94mLoss[0m : 2.49873
[1mStep[0m  [16/26], [94mLoss[0m : 2.77340
[1mStep[0m  [18/26], [94mLoss[0m : 2.75051
[1mStep[0m  [20/26], [94mLoss[0m : 2.56229
[1mStep[0m  [22/26], [94mLoss[0m : 2.50194
[1mStep[0m  [24/26], [94mLoss[0m : 2.71982

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.425, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66843
[1mStep[0m  [2/26], [94mLoss[0m : 2.61257
[1mStep[0m  [4/26], [94mLoss[0m : 2.66408
[1mStep[0m  [6/26], [94mLoss[0m : 2.68237
[1mStep[0m  [8/26], [94mLoss[0m : 2.75261
[1mStep[0m  [10/26], [94mLoss[0m : 2.72274
[1mStep[0m  [12/26], [94mLoss[0m : 2.74898
[1mStep[0m  [14/26], [94mLoss[0m : 2.62798
[1mStep[0m  [16/26], [94mLoss[0m : 2.59413
[1mStep[0m  [18/26], [94mLoss[0m : 2.83922
[1mStep[0m  [20/26], [94mLoss[0m : 2.66532
[1mStep[0m  [22/26], [94mLoss[0m : 2.53680
[1mStep[0m  [24/26], [94mLoss[0m : 2.57879

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.425, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.422
====================================

Phase 1 - Evaluation MAE:  2.422047871809739
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 2.67895
[1mStep[0m  [2/26], [94mLoss[0m : 2.66291
[1mStep[0m  [4/26], [94mLoss[0m : 2.78036
[1mStep[0m  [6/26], [94mLoss[0m : 2.68808
[1mStep[0m  [8/26], [94mLoss[0m : 2.67367
[1mStep[0m  [10/26], [94mLoss[0m : 2.76689
[1mStep[0m  [12/26], [94mLoss[0m : 2.53028
[1mStep[0m  [14/26], [94mLoss[0m : 2.64173
[1mStep[0m  [16/26], [94mLoss[0m : 2.67951
[1mStep[0m  [18/26], [94mLoss[0m : 2.61145
[1mStep[0m  [20/26], [94mLoss[0m : 2.61882
[1mStep[0m  [22/26], [94mLoss[0m : 2.65553
[1mStep[0m  [24/26], [94mLoss[0m : 2.73419

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.86533
[1mStep[0m  [2/26], [94mLoss[0m : 2.78531
[1mStep[0m  [4/26], [94mLoss[0m : 2.58551
[1mStep[0m  [6/26], [94mLoss[0m : 2.53601
[1mStep[0m  [8/26], [94mLoss[0m : 2.74143
[1mStep[0m  [10/26], [94mLoss[0m : 2.74031
[1mStep[0m  [12/26], [94mLoss[0m : 2.63836
[1mStep[0m  [14/26], [94mLoss[0m : 2.77260
[1mStep[0m  [16/26], [94mLoss[0m : 2.77421
[1mStep[0m  [18/26], [94mLoss[0m : 2.69762
[1mStep[0m  [20/26], [94mLoss[0m : 2.68307
[1mStep[0m  [22/26], [94mLoss[0m : 2.65527
[1mStep[0m  [24/26], [94mLoss[0m : 2.74193

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63351
[1mStep[0m  [2/26], [94mLoss[0m : 2.82494
[1mStep[0m  [4/26], [94mLoss[0m : 2.65885
[1mStep[0m  [6/26], [94mLoss[0m : 2.74256
[1mStep[0m  [8/26], [94mLoss[0m : 2.72834
[1mStep[0m  [10/26], [94mLoss[0m : 2.67009
[1mStep[0m  [12/26], [94mLoss[0m : 2.64574
[1mStep[0m  [14/26], [94mLoss[0m : 2.64231
[1mStep[0m  [16/26], [94mLoss[0m : 2.67051
[1mStep[0m  [18/26], [94mLoss[0m : 2.67204
[1mStep[0m  [20/26], [94mLoss[0m : 2.77958
[1mStep[0m  [22/26], [94mLoss[0m : 2.76197
[1mStep[0m  [24/26], [94mLoss[0m : 2.70211

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.621, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70378
[1mStep[0m  [2/26], [94mLoss[0m : 2.53720
[1mStep[0m  [4/26], [94mLoss[0m : 2.65264
[1mStep[0m  [6/26], [94mLoss[0m : 2.76428
[1mStep[0m  [8/26], [94mLoss[0m : 2.65862
[1mStep[0m  [10/26], [94mLoss[0m : 2.58858
[1mStep[0m  [12/26], [94mLoss[0m : 2.63418
[1mStep[0m  [14/26], [94mLoss[0m : 2.63557
[1mStep[0m  [16/26], [94mLoss[0m : 2.81305
[1mStep[0m  [18/26], [94mLoss[0m : 2.71703
[1mStep[0m  [20/26], [94mLoss[0m : 2.79218
[1mStep[0m  [22/26], [94mLoss[0m : 2.63377
[1mStep[0m  [24/26], [94mLoss[0m : 2.66448

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.641, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54464
[1mStep[0m  [2/26], [94mLoss[0m : 2.56752
[1mStep[0m  [4/26], [94mLoss[0m : 2.64337
[1mStep[0m  [6/26], [94mLoss[0m : 2.66562
[1mStep[0m  [8/26], [94mLoss[0m : 2.59768
[1mStep[0m  [10/26], [94mLoss[0m : 2.70039
[1mStep[0m  [12/26], [94mLoss[0m : 2.66082
[1mStep[0m  [14/26], [94mLoss[0m : 2.61013
[1mStep[0m  [16/26], [94mLoss[0m : 2.55063
[1mStep[0m  [18/26], [94mLoss[0m : 2.68402
[1mStep[0m  [20/26], [94mLoss[0m : 2.52101
[1mStep[0m  [22/26], [94mLoss[0m : 2.53041
[1mStep[0m  [24/26], [94mLoss[0m : 2.62057

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.689, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71654
[1mStep[0m  [2/26], [94mLoss[0m : 2.60332
[1mStep[0m  [4/26], [94mLoss[0m : 2.64336
[1mStep[0m  [6/26], [94mLoss[0m : 2.60738
[1mStep[0m  [8/26], [94mLoss[0m : 2.70614
[1mStep[0m  [10/26], [94mLoss[0m : 2.65581
[1mStep[0m  [12/26], [94mLoss[0m : 2.69687
[1mStep[0m  [14/26], [94mLoss[0m : 2.45931
[1mStep[0m  [16/26], [94mLoss[0m : 2.47099
[1mStep[0m  [18/26], [94mLoss[0m : 2.58535
[1mStep[0m  [20/26], [94mLoss[0m : 2.60453
[1mStep[0m  [22/26], [94mLoss[0m : 2.65123
[1mStep[0m  [24/26], [94mLoss[0m : 2.50051

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.684, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62976
[1mStep[0m  [2/26], [94mLoss[0m : 2.75127
[1mStep[0m  [4/26], [94mLoss[0m : 2.65241
[1mStep[0m  [6/26], [94mLoss[0m : 2.57203
[1mStep[0m  [8/26], [94mLoss[0m : 2.35599
[1mStep[0m  [10/26], [94mLoss[0m : 2.63972
[1mStep[0m  [12/26], [94mLoss[0m : 2.62102
[1mStep[0m  [14/26], [94mLoss[0m : 2.58860
[1mStep[0m  [16/26], [94mLoss[0m : 2.59773
[1mStep[0m  [18/26], [94mLoss[0m : 2.56644
[1mStep[0m  [20/26], [94mLoss[0m : 2.46919
[1mStep[0m  [22/26], [94mLoss[0m : 2.45467
[1mStep[0m  [24/26], [94mLoss[0m : 2.49680

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.688, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54566
[1mStep[0m  [2/26], [94mLoss[0m : 2.50554
[1mStep[0m  [4/26], [94mLoss[0m : 2.50799
[1mStep[0m  [6/26], [94mLoss[0m : 2.43467
[1mStep[0m  [8/26], [94mLoss[0m : 2.54205
[1mStep[0m  [10/26], [94mLoss[0m : 2.66694
[1mStep[0m  [12/26], [94mLoss[0m : 2.80323
[1mStep[0m  [14/26], [94mLoss[0m : 2.61544
[1mStep[0m  [16/26], [94mLoss[0m : 2.37294
[1mStep[0m  [18/26], [94mLoss[0m : 2.54494
[1mStep[0m  [20/26], [94mLoss[0m : 2.50567
[1mStep[0m  [22/26], [94mLoss[0m : 2.79787
[1mStep[0m  [24/26], [94mLoss[0m : 2.57368

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.621, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50072
[1mStep[0m  [2/26], [94mLoss[0m : 2.54843
[1mStep[0m  [4/26], [94mLoss[0m : 2.53112
[1mStep[0m  [6/26], [94mLoss[0m : 2.55401
[1mStep[0m  [8/26], [94mLoss[0m : 2.59791
[1mStep[0m  [10/26], [94mLoss[0m : 2.68554
[1mStep[0m  [12/26], [94mLoss[0m : 2.59612
[1mStep[0m  [14/26], [94mLoss[0m : 2.34127
[1mStep[0m  [16/26], [94mLoss[0m : 2.52864
[1mStep[0m  [18/26], [94mLoss[0m : 2.49430
[1mStep[0m  [20/26], [94mLoss[0m : 2.53287
[1mStep[0m  [22/26], [94mLoss[0m : 2.52515
[1mStep[0m  [24/26], [94mLoss[0m : 2.45582

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.629, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42899
[1mStep[0m  [2/26], [94mLoss[0m : 2.50412
[1mStep[0m  [4/26], [94mLoss[0m : 2.41534
[1mStep[0m  [6/26], [94mLoss[0m : 2.46200
[1mStep[0m  [8/26], [94mLoss[0m : 2.43252
[1mStep[0m  [10/26], [94mLoss[0m : 2.39538
[1mStep[0m  [12/26], [94mLoss[0m : 2.38826
[1mStep[0m  [14/26], [94mLoss[0m : 2.46335
[1mStep[0m  [16/26], [94mLoss[0m : 2.46499
[1mStep[0m  [18/26], [94mLoss[0m : 2.44302
[1mStep[0m  [20/26], [94mLoss[0m : 2.59146
[1mStep[0m  [22/26], [94mLoss[0m : 2.47137
[1mStep[0m  [24/26], [94mLoss[0m : 2.50124

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.567, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47874
[1mStep[0m  [2/26], [94mLoss[0m : 2.43409
[1mStep[0m  [4/26], [94mLoss[0m : 2.62110
[1mStep[0m  [6/26], [94mLoss[0m : 2.39647
[1mStep[0m  [8/26], [94mLoss[0m : 2.44259
[1mStep[0m  [10/26], [94mLoss[0m : 2.47800
[1mStep[0m  [12/26], [94mLoss[0m : 2.34669
[1mStep[0m  [14/26], [94mLoss[0m : 2.44661
[1mStep[0m  [16/26], [94mLoss[0m : 2.45870
[1mStep[0m  [18/26], [94mLoss[0m : 2.51356
[1mStep[0m  [20/26], [94mLoss[0m : 2.43599
[1mStep[0m  [22/26], [94mLoss[0m : 2.31996
[1mStep[0m  [24/26], [94mLoss[0m : 2.50159

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.532, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42786
[1mStep[0m  [2/26], [94mLoss[0m : 2.22802
[1mStep[0m  [4/26], [94mLoss[0m : 2.31762
[1mStep[0m  [6/26], [94mLoss[0m : 2.46232
[1mStep[0m  [8/26], [94mLoss[0m : 2.58738
[1mStep[0m  [10/26], [94mLoss[0m : 2.58208
[1mStep[0m  [12/26], [94mLoss[0m : 2.53842
[1mStep[0m  [14/26], [94mLoss[0m : 2.46227
[1mStep[0m  [16/26], [94mLoss[0m : 2.43009
[1mStep[0m  [18/26], [94mLoss[0m : 2.46052
[1mStep[0m  [20/26], [94mLoss[0m : 2.48152
[1mStep[0m  [22/26], [94mLoss[0m : 2.63246
[1mStep[0m  [24/26], [94mLoss[0m : 2.47592

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.595, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45570
[1mStep[0m  [2/26], [94mLoss[0m : 2.36987
[1mStep[0m  [4/26], [94mLoss[0m : 2.31213
[1mStep[0m  [6/26], [94mLoss[0m : 2.50595
[1mStep[0m  [8/26], [94mLoss[0m : 2.35899
[1mStep[0m  [10/26], [94mLoss[0m : 2.54468
[1mStep[0m  [12/26], [94mLoss[0m : 2.41361
[1mStep[0m  [14/26], [94mLoss[0m : 2.50071
[1mStep[0m  [16/26], [94mLoss[0m : 2.45729
[1mStep[0m  [18/26], [94mLoss[0m : 2.29220
[1mStep[0m  [20/26], [94mLoss[0m : 2.48927
[1mStep[0m  [22/26], [94mLoss[0m : 2.22174
[1mStep[0m  [24/26], [94mLoss[0m : 2.33326

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.513, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35145
[1mStep[0m  [2/26], [94mLoss[0m : 2.52140
[1mStep[0m  [4/26], [94mLoss[0m : 2.39973
[1mStep[0m  [6/26], [94mLoss[0m : 2.46664
[1mStep[0m  [8/26], [94mLoss[0m : 2.45723
[1mStep[0m  [10/26], [94mLoss[0m : 2.28748
[1mStep[0m  [12/26], [94mLoss[0m : 2.52764
[1mStep[0m  [14/26], [94mLoss[0m : 2.34176
[1mStep[0m  [16/26], [94mLoss[0m : 2.46884
[1mStep[0m  [18/26], [94mLoss[0m : 2.53325
[1mStep[0m  [20/26], [94mLoss[0m : 2.32791
[1mStep[0m  [22/26], [94mLoss[0m : 2.45143
[1mStep[0m  [24/26], [94mLoss[0m : 2.47626

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26981
[1mStep[0m  [2/26], [94mLoss[0m : 2.24380
[1mStep[0m  [4/26], [94mLoss[0m : 2.31472
[1mStep[0m  [6/26], [94mLoss[0m : 2.27121
[1mStep[0m  [8/26], [94mLoss[0m : 2.43784
[1mStep[0m  [10/26], [94mLoss[0m : 2.36357
[1mStep[0m  [12/26], [94mLoss[0m : 2.55314
[1mStep[0m  [14/26], [94mLoss[0m : 2.52413
[1mStep[0m  [16/26], [94mLoss[0m : 2.47691
[1mStep[0m  [18/26], [94mLoss[0m : 2.23688
[1mStep[0m  [20/26], [94mLoss[0m : 2.38585
[1mStep[0m  [22/26], [94mLoss[0m : 2.37507
[1mStep[0m  [24/26], [94mLoss[0m : 2.42727

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31797
[1mStep[0m  [2/26], [94mLoss[0m : 2.25818
[1mStep[0m  [4/26], [94mLoss[0m : 2.45995
[1mStep[0m  [6/26], [94mLoss[0m : 2.38898
[1mStep[0m  [8/26], [94mLoss[0m : 2.35402
[1mStep[0m  [10/26], [94mLoss[0m : 2.32451
[1mStep[0m  [12/26], [94mLoss[0m : 2.37378
[1mStep[0m  [14/26], [94mLoss[0m : 2.31014
[1mStep[0m  [16/26], [94mLoss[0m : 2.40643
[1mStep[0m  [18/26], [94mLoss[0m : 2.32248
[1mStep[0m  [20/26], [94mLoss[0m : 2.20424
[1mStep[0m  [22/26], [94mLoss[0m : 2.42025
[1mStep[0m  [24/26], [94mLoss[0m : 2.18024

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30623
[1mStep[0m  [2/26], [94mLoss[0m : 2.22228
[1mStep[0m  [4/26], [94mLoss[0m : 2.25588
[1mStep[0m  [6/26], [94mLoss[0m : 2.26317
[1mStep[0m  [8/26], [94mLoss[0m : 2.31643
[1mStep[0m  [10/26], [94mLoss[0m : 2.38334
[1mStep[0m  [12/26], [94mLoss[0m : 2.28662
[1mStep[0m  [14/26], [94mLoss[0m : 2.27326
[1mStep[0m  [16/26], [94mLoss[0m : 2.19659
[1mStep[0m  [18/26], [94mLoss[0m : 2.32299
[1mStep[0m  [20/26], [94mLoss[0m : 2.43096
[1mStep[0m  [22/26], [94mLoss[0m : 2.39374
[1mStep[0m  [24/26], [94mLoss[0m : 2.19634

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27043
[1mStep[0m  [2/26], [94mLoss[0m : 2.40095
[1mStep[0m  [4/26], [94mLoss[0m : 2.29321
[1mStep[0m  [6/26], [94mLoss[0m : 2.16319
[1mStep[0m  [8/26], [94mLoss[0m : 2.25131
[1mStep[0m  [10/26], [94mLoss[0m : 2.37050
[1mStep[0m  [12/26], [94mLoss[0m : 2.38047
[1mStep[0m  [14/26], [94mLoss[0m : 2.23615
[1mStep[0m  [16/26], [94mLoss[0m : 2.36907
[1mStep[0m  [18/26], [94mLoss[0m : 2.33878
[1mStep[0m  [20/26], [94mLoss[0m : 2.24875
[1mStep[0m  [22/26], [94mLoss[0m : 2.29529
[1mStep[0m  [24/26], [94mLoss[0m : 2.07105

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24631
[1mStep[0m  [2/26], [94mLoss[0m : 2.38205
[1mStep[0m  [4/26], [94mLoss[0m : 2.18692
[1mStep[0m  [6/26], [94mLoss[0m : 2.19782
[1mStep[0m  [8/26], [94mLoss[0m : 2.12791
[1mStep[0m  [10/26], [94mLoss[0m : 2.22971
[1mStep[0m  [12/26], [94mLoss[0m : 2.10575
[1mStep[0m  [14/26], [94mLoss[0m : 2.25247
[1mStep[0m  [16/26], [94mLoss[0m : 2.22397
[1mStep[0m  [18/26], [94mLoss[0m : 2.20880
[1mStep[0m  [20/26], [94mLoss[0m : 2.40544
[1mStep[0m  [22/26], [94mLoss[0m : 2.24158
[1mStep[0m  [24/26], [94mLoss[0m : 2.28174

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.570, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27621
[1mStep[0m  [2/26], [94mLoss[0m : 2.30369
[1mStep[0m  [4/26], [94mLoss[0m : 2.21940
[1mStep[0m  [6/26], [94mLoss[0m : 2.32701
[1mStep[0m  [8/26], [94mLoss[0m : 2.27036
[1mStep[0m  [10/26], [94mLoss[0m : 2.15504
[1mStep[0m  [12/26], [94mLoss[0m : 2.25370
[1mStep[0m  [14/26], [94mLoss[0m : 2.13901
[1mStep[0m  [16/26], [94mLoss[0m : 2.20061
[1mStep[0m  [18/26], [94mLoss[0m : 2.09383
[1mStep[0m  [20/26], [94mLoss[0m : 2.18338
[1mStep[0m  [22/26], [94mLoss[0m : 2.15543
[1mStep[0m  [24/26], [94mLoss[0m : 2.11137

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.514, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26654
[1mStep[0m  [2/26], [94mLoss[0m : 2.32282
[1mStep[0m  [4/26], [94mLoss[0m : 2.23019
[1mStep[0m  [6/26], [94mLoss[0m : 2.24084
[1mStep[0m  [8/26], [94mLoss[0m : 2.21374
[1mStep[0m  [10/26], [94mLoss[0m : 2.11110
[1mStep[0m  [12/26], [94mLoss[0m : 2.17619
[1mStep[0m  [14/26], [94mLoss[0m : 2.22438
[1mStep[0m  [16/26], [94mLoss[0m : 2.18438
[1mStep[0m  [18/26], [94mLoss[0m : 2.33902
[1mStep[0m  [20/26], [94mLoss[0m : 2.11706
[1mStep[0m  [22/26], [94mLoss[0m : 2.15718
[1mStep[0m  [24/26], [94mLoss[0m : 2.19455

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07432
[1mStep[0m  [2/26], [94mLoss[0m : 2.08085
[1mStep[0m  [4/26], [94mLoss[0m : 2.14130
[1mStep[0m  [6/26], [94mLoss[0m : 2.04439
[1mStep[0m  [8/26], [94mLoss[0m : 2.13354
[1mStep[0m  [10/26], [94mLoss[0m : 2.13347
[1mStep[0m  [12/26], [94mLoss[0m : 2.07478
[1mStep[0m  [14/26], [94mLoss[0m : 2.45358
[1mStep[0m  [16/26], [94mLoss[0m : 2.13081
[1mStep[0m  [18/26], [94mLoss[0m : 2.14481
[1mStep[0m  [20/26], [94mLoss[0m : 2.12358
[1mStep[0m  [22/26], [94mLoss[0m : 2.17101
[1mStep[0m  [24/26], [94mLoss[0m : 2.06797

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.419, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07042
[1mStep[0m  [2/26], [94mLoss[0m : 2.09610
[1mStep[0m  [4/26], [94mLoss[0m : 2.14645
[1mStep[0m  [6/26], [94mLoss[0m : 2.17717
[1mStep[0m  [8/26], [94mLoss[0m : 2.07357
[1mStep[0m  [10/26], [94mLoss[0m : 2.17829
[1mStep[0m  [12/26], [94mLoss[0m : 2.06942
[1mStep[0m  [14/26], [94mLoss[0m : 2.10691
[1mStep[0m  [16/26], [94mLoss[0m : 2.21312
[1mStep[0m  [18/26], [94mLoss[0m : 2.12985
[1mStep[0m  [20/26], [94mLoss[0m : 2.34741
[1mStep[0m  [22/26], [94mLoss[0m : 2.07439
[1mStep[0m  [24/26], [94mLoss[0m : 2.24196

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.11361
[1mStep[0m  [2/26], [94mLoss[0m : 2.22932
[1mStep[0m  [4/26], [94mLoss[0m : 2.03807
[1mStep[0m  [6/26], [94mLoss[0m : 2.04678
[1mStep[0m  [8/26], [94mLoss[0m : 2.08418
[1mStep[0m  [10/26], [94mLoss[0m : 2.22729
[1mStep[0m  [12/26], [94mLoss[0m : 2.06290
[1mStep[0m  [14/26], [94mLoss[0m : 2.13168
[1mStep[0m  [16/26], [94mLoss[0m : 2.08680
[1mStep[0m  [18/26], [94mLoss[0m : 2.18049
[1mStep[0m  [20/26], [94mLoss[0m : 2.15342
[1mStep[0m  [22/26], [94mLoss[0m : 2.06002
[1mStep[0m  [24/26], [94mLoss[0m : 2.12877

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.05212
[1mStep[0m  [2/26], [94mLoss[0m : 2.01159
[1mStep[0m  [4/26], [94mLoss[0m : 1.99728
[1mStep[0m  [6/26], [94mLoss[0m : 1.89365
[1mStep[0m  [8/26], [94mLoss[0m : 1.98463
[1mStep[0m  [10/26], [94mLoss[0m : 2.07979
[1mStep[0m  [12/26], [94mLoss[0m : 2.21752
[1mStep[0m  [14/26], [94mLoss[0m : 2.03558
[1mStep[0m  [16/26], [94mLoss[0m : 2.23083
[1mStep[0m  [18/26], [94mLoss[0m : 2.15215
[1mStep[0m  [20/26], [94mLoss[0m : 2.12170
[1mStep[0m  [22/26], [94mLoss[0m : 2.18121
[1mStep[0m  [24/26], [94mLoss[0m : 2.07922

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.04975
[1mStep[0m  [2/26], [94mLoss[0m : 1.98038
[1mStep[0m  [4/26], [94mLoss[0m : 2.09905
[1mStep[0m  [6/26], [94mLoss[0m : 2.13256
[1mStep[0m  [8/26], [94mLoss[0m : 2.11089
[1mStep[0m  [10/26], [94mLoss[0m : 2.11647
[1mStep[0m  [12/26], [94mLoss[0m : 1.86934
[1mStep[0m  [14/26], [94mLoss[0m : 2.00947
[1mStep[0m  [16/26], [94mLoss[0m : 2.04865
[1mStep[0m  [18/26], [94mLoss[0m : 2.09384
[1mStep[0m  [20/26], [94mLoss[0m : 2.04377
[1mStep[0m  [22/26], [94mLoss[0m : 2.07956
[1mStep[0m  [24/26], [94mLoss[0m : 2.04312

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.440, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93962
[1mStep[0m  [2/26], [94mLoss[0m : 1.98465
[1mStep[0m  [4/26], [94mLoss[0m : 2.12577
[1mStep[0m  [6/26], [94mLoss[0m : 2.16374
[1mStep[0m  [8/26], [94mLoss[0m : 2.19751
[1mStep[0m  [10/26], [94mLoss[0m : 2.04534
[1mStep[0m  [12/26], [94mLoss[0m : 2.05340
[1mStep[0m  [14/26], [94mLoss[0m : 2.01405
[1mStep[0m  [16/26], [94mLoss[0m : 2.11974
[1mStep[0m  [18/26], [94mLoss[0m : 2.12831
[1mStep[0m  [20/26], [94mLoss[0m : 1.92048
[1mStep[0m  [22/26], [94mLoss[0m : 2.07424
[1mStep[0m  [24/26], [94mLoss[0m : 2.13920

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.01113
[1mStep[0m  [2/26], [94mLoss[0m : 2.02316
[1mStep[0m  [4/26], [94mLoss[0m : 2.03692
[1mStep[0m  [6/26], [94mLoss[0m : 2.07348
[1mStep[0m  [8/26], [94mLoss[0m : 1.98244
[1mStep[0m  [10/26], [94mLoss[0m : 1.97731
[1mStep[0m  [12/26], [94mLoss[0m : 1.96224
[1mStep[0m  [14/26], [94mLoss[0m : 2.09268
[1mStep[0m  [16/26], [94mLoss[0m : 1.96382
[1mStep[0m  [18/26], [94mLoss[0m : 2.04748
[1mStep[0m  [20/26], [94mLoss[0m : 1.91363
[1mStep[0m  [22/26], [94mLoss[0m : 2.01017
[1mStep[0m  [24/26], [94mLoss[0m : 1.97987

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93398
[1mStep[0m  [2/26], [94mLoss[0m : 1.93610
[1mStep[0m  [4/26], [94mLoss[0m : 1.98650
[1mStep[0m  [6/26], [94mLoss[0m : 1.91977
[1mStep[0m  [8/26], [94mLoss[0m : 2.00054
[1mStep[0m  [10/26], [94mLoss[0m : 2.14528
[1mStep[0m  [12/26], [94mLoss[0m : 1.89953
[1mStep[0m  [14/26], [94mLoss[0m : 2.06664
[1mStep[0m  [16/26], [94mLoss[0m : 1.89624
[1mStep[0m  [18/26], [94mLoss[0m : 2.03294
[1mStep[0m  [20/26], [94mLoss[0m : 2.12899
[1mStep[0m  [22/26], [94mLoss[0m : 2.16031
[1mStep[0m  [24/26], [94mLoss[0m : 2.12870

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.92353
[1mStep[0m  [2/26], [94mLoss[0m : 1.95971
[1mStep[0m  [4/26], [94mLoss[0m : 1.92786
[1mStep[0m  [6/26], [94mLoss[0m : 1.96620
[1mStep[0m  [8/26], [94mLoss[0m : 1.82795
[1mStep[0m  [10/26], [94mLoss[0m : 1.92062
[1mStep[0m  [12/26], [94mLoss[0m : 2.12663
[1mStep[0m  [14/26], [94mLoss[0m : 2.01285
[1mStep[0m  [16/26], [94mLoss[0m : 1.86914
[1mStep[0m  [18/26], [94mLoss[0m : 2.04194
[1mStep[0m  [20/26], [94mLoss[0m : 1.94822
[1mStep[0m  [22/26], [94mLoss[0m : 1.99970
[1mStep[0m  [24/26], [94mLoss[0m : 1.98274

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.549, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 2 - Evaluation MAE:  2.4920560946831336
MAE score P1       2.422048
MAE score P2       2.492056
loss               1.965229
learning_rate          0.01
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.34970
[1mStep[0m  [2/26], [94mLoss[0m : 9.93173
[1mStep[0m  [4/26], [94mLoss[0m : 9.76140
[1mStep[0m  [6/26], [94mLoss[0m : 9.01632
[1mStep[0m  [8/26], [94mLoss[0m : 9.07986
[1mStep[0m  [10/26], [94mLoss[0m : 8.75077
[1mStep[0m  [12/26], [94mLoss[0m : 8.61296
[1mStep[0m  [14/26], [94mLoss[0m : 8.00580
[1mStep[0m  [16/26], [94mLoss[0m : 7.87255
[1mStep[0m  [18/26], [94mLoss[0m : 7.47346
[1mStep[0m  [20/26], [94mLoss[0m : 7.25192
[1mStep[0m  [22/26], [94mLoss[0m : 6.93563
[1mStep[0m  [24/26], [94mLoss[0m : 6.61075

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.392, [92mTest[0m: 10.272, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.28759
[1mStep[0m  [2/26], [94mLoss[0m : 5.85118
[1mStep[0m  [4/26], [94mLoss[0m : 5.72722
[1mStep[0m  [6/26], [94mLoss[0m : 5.43503
[1mStep[0m  [8/26], [94mLoss[0m : 5.02120
[1mStep[0m  [10/26], [94mLoss[0m : 5.02636
[1mStep[0m  [12/26], [94mLoss[0m : 4.77414
[1mStep[0m  [14/26], [94mLoss[0m : 4.50682
[1mStep[0m  [16/26], [94mLoss[0m : 4.34061
[1mStep[0m  [18/26], [94mLoss[0m : 4.47658
[1mStep[0m  [20/26], [94mLoss[0m : 4.08741
[1mStep[0m  [22/26], [94mLoss[0m : 3.85193
[1mStep[0m  [24/26], [94mLoss[0m : 3.80740

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.812, [92mTest[0m: 6.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.71180
[1mStep[0m  [2/26], [94mLoss[0m : 3.60606
[1mStep[0m  [4/26], [94mLoss[0m : 3.32724
[1mStep[0m  [6/26], [94mLoss[0m : 3.42720
[1mStep[0m  [8/26], [94mLoss[0m : 3.20522
[1mStep[0m  [10/26], [94mLoss[0m : 3.18867
[1mStep[0m  [12/26], [94mLoss[0m : 3.11932
[1mStep[0m  [14/26], [94mLoss[0m : 3.00324
[1mStep[0m  [16/26], [94mLoss[0m : 3.10815
[1mStep[0m  [18/26], [94mLoss[0m : 3.07319
[1mStep[0m  [20/26], [94mLoss[0m : 2.90202
[1mStep[0m  [22/26], [94mLoss[0m : 2.78118
[1mStep[0m  [24/26], [94mLoss[0m : 2.93966

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.177, [92mTest[0m: 3.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.82217
[1mStep[0m  [2/26], [94mLoss[0m : 2.77855
[1mStep[0m  [4/26], [94mLoss[0m : 2.74932
[1mStep[0m  [6/26], [94mLoss[0m : 2.89211
[1mStep[0m  [8/26], [94mLoss[0m : 3.07796
[1mStep[0m  [10/26], [94mLoss[0m : 2.66302
[1mStep[0m  [12/26], [94mLoss[0m : 2.75607
[1mStep[0m  [14/26], [94mLoss[0m : 2.70921
[1mStep[0m  [16/26], [94mLoss[0m : 2.72231
[1mStep[0m  [18/26], [94mLoss[0m : 2.57963
[1mStep[0m  [20/26], [94mLoss[0m : 2.86003
[1mStep[0m  [22/26], [94mLoss[0m : 2.70388
[1mStep[0m  [24/26], [94mLoss[0m : 2.64183

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.744, [92mTest[0m: 2.672, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.81205
[1mStep[0m  [2/26], [94mLoss[0m : 2.71505
[1mStep[0m  [4/26], [94mLoss[0m : 2.86871
[1mStep[0m  [6/26], [94mLoss[0m : 2.76377
[1mStep[0m  [8/26], [94mLoss[0m : 2.61263
[1mStep[0m  [10/26], [94mLoss[0m : 2.80424
[1mStep[0m  [12/26], [94mLoss[0m : 2.69046
[1mStep[0m  [14/26], [94mLoss[0m : 2.65469
[1mStep[0m  [16/26], [94mLoss[0m : 2.66715
[1mStep[0m  [18/26], [94mLoss[0m : 2.78644
[1mStep[0m  [20/26], [94mLoss[0m : 2.73910
[1mStep[0m  [22/26], [94mLoss[0m : 2.64121
[1mStep[0m  [24/26], [94mLoss[0m : 2.49076

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.510, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56658
[1mStep[0m  [2/26], [94mLoss[0m : 2.58825
[1mStep[0m  [4/26], [94mLoss[0m : 2.50457
[1mStep[0m  [6/26], [94mLoss[0m : 2.71545
[1mStep[0m  [8/26], [94mLoss[0m : 2.57700
[1mStep[0m  [10/26], [94mLoss[0m : 2.50332
[1mStep[0m  [12/26], [94mLoss[0m : 2.67624
[1mStep[0m  [14/26], [94mLoss[0m : 2.49907
[1mStep[0m  [16/26], [94mLoss[0m : 2.60612
[1mStep[0m  [18/26], [94mLoss[0m : 2.69287
[1mStep[0m  [20/26], [94mLoss[0m : 2.61710
[1mStep[0m  [22/26], [94mLoss[0m : 2.72712
[1mStep[0m  [24/26], [94mLoss[0m : 2.64085

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70759
[1mStep[0m  [2/26], [94mLoss[0m : 2.54013
[1mStep[0m  [4/26], [94mLoss[0m : 2.55703
[1mStep[0m  [6/26], [94mLoss[0m : 2.50512
[1mStep[0m  [8/26], [94mLoss[0m : 2.59698
[1mStep[0m  [10/26], [94mLoss[0m : 2.55455
[1mStep[0m  [12/26], [94mLoss[0m : 2.47357
[1mStep[0m  [14/26], [94mLoss[0m : 2.69532
[1mStep[0m  [16/26], [94mLoss[0m : 2.72514
[1mStep[0m  [18/26], [94mLoss[0m : 2.45926
[1mStep[0m  [20/26], [94mLoss[0m : 2.59662
[1mStep[0m  [22/26], [94mLoss[0m : 2.85533
[1mStep[0m  [24/26], [94mLoss[0m : 2.50575

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64015
[1mStep[0m  [2/26], [94mLoss[0m : 2.56570
[1mStep[0m  [4/26], [94mLoss[0m : 2.57912
[1mStep[0m  [6/26], [94mLoss[0m : 2.58509
[1mStep[0m  [8/26], [94mLoss[0m : 2.62570
[1mStep[0m  [10/26], [94mLoss[0m : 2.71406
[1mStep[0m  [12/26], [94mLoss[0m : 2.53497
[1mStep[0m  [14/26], [94mLoss[0m : 2.70385
[1mStep[0m  [16/26], [94mLoss[0m : 2.47955
[1mStep[0m  [18/26], [94mLoss[0m : 2.49544
[1mStep[0m  [20/26], [94mLoss[0m : 2.59183
[1mStep[0m  [22/26], [94mLoss[0m : 2.54321
[1mStep[0m  [24/26], [94mLoss[0m : 2.62956

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44697
[1mStep[0m  [2/26], [94mLoss[0m : 2.64082
[1mStep[0m  [4/26], [94mLoss[0m : 2.72749
[1mStep[0m  [6/26], [94mLoss[0m : 2.55317
[1mStep[0m  [8/26], [94mLoss[0m : 2.79233
[1mStep[0m  [10/26], [94mLoss[0m : 2.67568
[1mStep[0m  [12/26], [94mLoss[0m : 2.61725
[1mStep[0m  [14/26], [94mLoss[0m : 2.62447
[1mStep[0m  [16/26], [94mLoss[0m : 2.63281
[1mStep[0m  [18/26], [94mLoss[0m : 2.53092
[1mStep[0m  [20/26], [94mLoss[0m : 2.68355
[1mStep[0m  [22/26], [94mLoss[0m : 2.59919
[1mStep[0m  [24/26], [94mLoss[0m : 2.65655

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70692
[1mStep[0m  [2/26], [94mLoss[0m : 2.53961
[1mStep[0m  [4/26], [94mLoss[0m : 2.54227
[1mStep[0m  [6/26], [94mLoss[0m : 2.58559
[1mStep[0m  [8/26], [94mLoss[0m : 2.52136
[1mStep[0m  [10/26], [94mLoss[0m : 2.53329
[1mStep[0m  [12/26], [94mLoss[0m : 2.63770
[1mStep[0m  [14/26], [94mLoss[0m : 2.62160
[1mStep[0m  [16/26], [94mLoss[0m : 2.57567
[1mStep[0m  [18/26], [94mLoss[0m : 2.44872
[1mStep[0m  [20/26], [94mLoss[0m : 2.72694
[1mStep[0m  [22/26], [94mLoss[0m : 2.43600
[1mStep[0m  [24/26], [94mLoss[0m : 2.72005

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50632
[1mStep[0m  [2/26], [94mLoss[0m : 2.74756
[1mStep[0m  [4/26], [94mLoss[0m : 2.52298
[1mStep[0m  [6/26], [94mLoss[0m : 2.60699
[1mStep[0m  [8/26], [94mLoss[0m : 2.52891
[1mStep[0m  [10/26], [94mLoss[0m : 2.54401
[1mStep[0m  [12/26], [94mLoss[0m : 2.67033
[1mStep[0m  [14/26], [94mLoss[0m : 2.61925
[1mStep[0m  [16/26], [94mLoss[0m : 2.52189
[1mStep[0m  [18/26], [94mLoss[0m : 2.60347
[1mStep[0m  [20/26], [94mLoss[0m : 2.52508
[1mStep[0m  [22/26], [94mLoss[0m : 2.64969
[1mStep[0m  [24/26], [94mLoss[0m : 2.47727

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48212
[1mStep[0m  [2/26], [94mLoss[0m : 2.64095
[1mStep[0m  [4/26], [94mLoss[0m : 2.47562
[1mStep[0m  [6/26], [94mLoss[0m : 2.65185
[1mStep[0m  [8/26], [94mLoss[0m : 2.62910
[1mStep[0m  [10/26], [94mLoss[0m : 2.50434
[1mStep[0m  [12/26], [94mLoss[0m : 2.61160
[1mStep[0m  [14/26], [94mLoss[0m : 2.44246
[1mStep[0m  [16/26], [94mLoss[0m : 2.65256
[1mStep[0m  [18/26], [94mLoss[0m : 2.62950
[1mStep[0m  [20/26], [94mLoss[0m : 2.77373
[1mStep[0m  [22/26], [94mLoss[0m : 2.47331
[1mStep[0m  [24/26], [94mLoss[0m : 2.55019

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64965
[1mStep[0m  [2/26], [94mLoss[0m : 2.52902
[1mStep[0m  [4/26], [94mLoss[0m : 2.42376
[1mStep[0m  [6/26], [94mLoss[0m : 2.66481
[1mStep[0m  [8/26], [94mLoss[0m : 2.42583
[1mStep[0m  [10/26], [94mLoss[0m : 2.62912
[1mStep[0m  [12/26], [94mLoss[0m : 2.70315
[1mStep[0m  [14/26], [94mLoss[0m : 2.52874
[1mStep[0m  [16/26], [94mLoss[0m : 2.58619
[1mStep[0m  [18/26], [94mLoss[0m : 2.68384
[1mStep[0m  [20/26], [94mLoss[0m : 2.60549
[1mStep[0m  [22/26], [94mLoss[0m : 2.52147
[1mStep[0m  [24/26], [94mLoss[0m : 2.46150

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57473
[1mStep[0m  [2/26], [94mLoss[0m : 2.52555
[1mStep[0m  [4/26], [94mLoss[0m : 2.47570
[1mStep[0m  [6/26], [94mLoss[0m : 2.69684
[1mStep[0m  [8/26], [94mLoss[0m : 2.65916
[1mStep[0m  [10/26], [94mLoss[0m : 2.80939
[1mStep[0m  [12/26], [94mLoss[0m : 2.58788
[1mStep[0m  [14/26], [94mLoss[0m : 2.47097
[1mStep[0m  [16/26], [94mLoss[0m : 2.51157
[1mStep[0m  [18/26], [94mLoss[0m : 2.60172
[1mStep[0m  [20/26], [94mLoss[0m : 2.53473
[1mStep[0m  [22/26], [94mLoss[0m : 2.44961
[1mStep[0m  [24/26], [94mLoss[0m : 2.61940

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52712
[1mStep[0m  [2/26], [94mLoss[0m : 2.62092
[1mStep[0m  [4/26], [94mLoss[0m : 2.50235
[1mStep[0m  [6/26], [94mLoss[0m : 2.69775
[1mStep[0m  [8/26], [94mLoss[0m : 2.33119
[1mStep[0m  [10/26], [94mLoss[0m : 2.48285
[1mStep[0m  [12/26], [94mLoss[0m : 2.36703
[1mStep[0m  [14/26], [94mLoss[0m : 2.73902
[1mStep[0m  [16/26], [94mLoss[0m : 2.67239
[1mStep[0m  [18/26], [94mLoss[0m : 2.52141
[1mStep[0m  [20/26], [94mLoss[0m : 2.67693
[1mStep[0m  [22/26], [94mLoss[0m : 2.66391
[1mStep[0m  [24/26], [94mLoss[0m : 2.46889

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54612
[1mStep[0m  [2/26], [94mLoss[0m : 2.64785
[1mStep[0m  [4/26], [94mLoss[0m : 2.51311
[1mStep[0m  [6/26], [94mLoss[0m : 2.45747
[1mStep[0m  [8/26], [94mLoss[0m : 2.70675
[1mStep[0m  [10/26], [94mLoss[0m : 2.49881
[1mStep[0m  [12/26], [94mLoss[0m : 2.62507
[1mStep[0m  [14/26], [94mLoss[0m : 2.58295
[1mStep[0m  [16/26], [94mLoss[0m : 2.50445
[1mStep[0m  [18/26], [94mLoss[0m : 2.75688
[1mStep[0m  [20/26], [94mLoss[0m : 2.40851
[1mStep[0m  [22/26], [94mLoss[0m : 2.49900
[1mStep[0m  [24/26], [94mLoss[0m : 2.51424

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40938
[1mStep[0m  [2/26], [94mLoss[0m : 2.52818
[1mStep[0m  [4/26], [94mLoss[0m : 2.54923
[1mStep[0m  [6/26], [94mLoss[0m : 2.72206
[1mStep[0m  [8/26], [94mLoss[0m : 2.57983
[1mStep[0m  [10/26], [94mLoss[0m : 2.41447
[1mStep[0m  [12/26], [94mLoss[0m : 2.62909
[1mStep[0m  [14/26], [94mLoss[0m : 2.49847
[1mStep[0m  [16/26], [94mLoss[0m : 2.67975
[1mStep[0m  [18/26], [94mLoss[0m : 2.62384
[1mStep[0m  [20/26], [94mLoss[0m : 2.49030
[1mStep[0m  [22/26], [94mLoss[0m : 2.67341
[1mStep[0m  [24/26], [94mLoss[0m : 2.57659

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62845
[1mStep[0m  [2/26], [94mLoss[0m : 2.47029
[1mStep[0m  [4/26], [94mLoss[0m : 2.68948
[1mStep[0m  [6/26], [94mLoss[0m : 2.64084
[1mStep[0m  [8/26], [94mLoss[0m : 2.58264
[1mStep[0m  [10/26], [94mLoss[0m : 2.75314
[1mStep[0m  [12/26], [94mLoss[0m : 2.61327
[1mStep[0m  [14/26], [94mLoss[0m : 2.45377
[1mStep[0m  [16/26], [94mLoss[0m : 2.52759
[1mStep[0m  [18/26], [94mLoss[0m : 2.39599
[1mStep[0m  [20/26], [94mLoss[0m : 2.65791
[1mStep[0m  [22/26], [94mLoss[0m : 2.53622
[1mStep[0m  [24/26], [94mLoss[0m : 2.61519

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57671
[1mStep[0m  [2/26], [94mLoss[0m : 2.47193
[1mStep[0m  [4/26], [94mLoss[0m : 2.55387
[1mStep[0m  [6/26], [94mLoss[0m : 2.52479
[1mStep[0m  [8/26], [94mLoss[0m : 2.54863
[1mStep[0m  [10/26], [94mLoss[0m : 2.49348
[1mStep[0m  [12/26], [94mLoss[0m : 2.51112
[1mStep[0m  [14/26], [94mLoss[0m : 2.63198
[1mStep[0m  [16/26], [94mLoss[0m : 2.46345
[1mStep[0m  [18/26], [94mLoss[0m : 2.79142
[1mStep[0m  [20/26], [94mLoss[0m : 2.65581
[1mStep[0m  [22/26], [94mLoss[0m : 2.53343
[1mStep[0m  [24/26], [94mLoss[0m : 2.48780

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54244
[1mStep[0m  [2/26], [94mLoss[0m : 2.53692
[1mStep[0m  [4/26], [94mLoss[0m : 2.58948
[1mStep[0m  [6/26], [94mLoss[0m : 2.60414
[1mStep[0m  [8/26], [94mLoss[0m : 2.50298
[1mStep[0m  [10/26], [94mLoss[0m : 2.44058
[1mStep[0m  [12/26], [94mLoss[0m : 2.66541
[1mStep[0m  [14/26], [94mLoss[0m : 2.51613
[1mStep[0m  [16/26], [94mLoss[0m : 2.59667
[1mStep[0m  [18/26], [94mLoss[0m : 2.54158
[1mStep[0m  [20/26], [94mLoss[0m : 2.68737
[1mStep[0m  [22/26], [94mLoss[0m : 2.49860
[1mStep[0m  [24/26], [94mLoss[0m : 2.47907

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63769
[1mStep[0m  [2/26], [94mLoss[0m : 2.69655
[1mStep[0m  [4/26], [94mLoss[0m : 2.71354
[1mStep[0m  [6/26], [94mLoss[0m : 2.52633
[1mStep[0m  [8/26], [94mLoss[0m : 2.56209
[1mStep[0m  [10/26], [94mLoss[0m : 2.53913
[1mStep[0m  [12/26], [94mLoss[0m : 2.58343
[1mStep[0m  [14/26], [94mLoss[0m : 2.53538
[1mStep[0m  [16/26], [94mLoss[0m : 2.61947
[1mStep[0m  [18/26], [94mLoss[0m : 2.50464
[1mStep[0m  [20/26], [94mLoss[0m : 2.68477
[1mStep[0m  [22/26], [94mLoss[0m : 2.61717
[1mStep[0m  [24/26], [94mLoss[0m : 2.55973

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.433, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68523
[1mStep[0m  [2/26], [94mLoss[0m : 2.58987
[1mStep[0m  [4/26], [94mLoss[0m : 2.52369
[1mStep[0m  [6/26], [94mLoss[0m : 2.40876
[1mStep[0m  [8/26], [94mLoss[0m : 2.62720
[1mStep[0m  [10/26], [94mLoss[0m : 2.47370
[1mStep[0m  [12/26], [94mLoss[0m : 2.66374
[1mStep[0m  [14/26], [94mLoss[0m : 2.62770
[1mStep[0m  [16/26], [94mLoss[0m : 2.60760
[1mStep[0m  [18/26], [94mLoss[0m : 2.60072
[1mStep[0m  [20/26], [94mLoss[0m : 2.46915
[1mStep[0m  [22/26], [94mLoss[0m : 2.59918
[1mStep[0m  [24/26], [94mLoss[0m : 2.48413

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.445, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51149
[1mStep[0m  [2/26], [94mLoss[0m : 2.49449
[1mStep[0m  [4/26], [94mLoss[0m : 2.67423
[1mStep[0m  [6/26], [94mLoss[0m : 2.56306
[1mStep[0m  [8/26], [94mLoss[0m : 2.58640
[1mStep[0m  [10/26], [94mLoss[0m : 2.73889
[1mStep[0m  [12/26], [94mLoss[0m : 2.46549
[1mStep[0m  [14/26], [94mLoss[0m : 2.66850
[1mStep[0m  [16/26], [94mLoss[0m : 2.54454
[1mStep[0m  [18/26], [94mLoss[0m : 2.63553
[1mStep[0m  [20/26], [94mLoss[0m : 2.58021
[1mStep[0m  [22/26], [94mLoss[0m : 2.60290
[1mStep[0m  [24/26], [94mLoss[0m : 2.62875

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60681
[1mStep[0m  [2/26], [94mLoss[0m : 2.45849
[1mStep[0m  [4/26], [94mLoss[0m : 2.62259
[1mStep[0m  [6/26], [94mLoss[0m : 2.59970
[1mStep[0m  [8/26], [94mLoss[0m : 2.56575
[1mStep[0m  [10/26], [94mLoss[0m : 2.51703
[1mStep[0m  [12/26], [94mLoss[0m : 2.51059
[1mStep[0m  [14/26], [94mLoss[0m : 2.57201
[1mStep[0m  [16/26], [94mLoss[0m : 2.70106
[1mStep[0m  [18/26], [94mLoss[0m : 2.80840
[1mStep[0m  [20/26], [94mLoss[0m : 2.67974
[1mStep[0m  [22/26], [94mLoss[0m : 2.59363
[1mStep[0m  [24/26], [94mLoss[0m : 2.72964

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.434, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51247
[1mStep[0m  [2/26], [94mLoss[0m : 2.57857
[1mStep[0m  [4/26], [94mLoss[0m : 2.58298
[1mStep[0m  [6/26], [94mLoss[0m : 2.48628
[1mStep[0m  [8/26], [94mLoss[0m : 2.39456
[1mStep[0m  [10/26], [94mLoss[0m : 2.63458
[1mStep[0m  [12/26], [94mLoss[0m : 2.65476
[1mStep[0m  [14/26], [94mLoss[0m : 2.49147
[1mStep[0m  [16/26], [94mLoss[0m : 2.66231
[1mStep[0m  [18/26], [94mLoss[0m : 2.52787
[1mStep[0m  [20/26], [94mLoss[0m : 2.65850
[1mStep[0m  [22/26], [94mLoss[0m : 2.44617
[1mStep[0m  [24/26], [94mLoss[0m : 2.45135

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.436, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64444
[1mStep[0m  [2/26], [94mLoss[0m : 2.63167
[1mStep[0m  [4/26], [94mLoss[0m : 2.53913
[1mStep[0m  [6/26], [94mLoss[0m : 2.64091
[1mStep[0m  [8/26], [94mLoss[0m : 2.55713
[1mStep[0m  [10/26], [94mLoss[0m : 2.43985
[1mStep[0m  [12/26], [94mLoss[0m : 2.56102
[1mStep[0m  [14/26], [94mLoss[0m : 2.55992
[1mStep[0m  [16/26], [94mLoss[0m : 2.55099
[1mStep[0m  [18/26], [94mLoss[0m : 2.46649
[1mStep[0m  [20/26], [94mLoss[0m : 2.63038
[1mStep[0m  [22/26], [94mLoss[0m : 2.54015
[1mStep[0m  [24/26], [94mLoss[0m : 2.61429

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.436, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40487
[1mStep[0m  [2/26], [94mLoss[0m : 2.46976
[1mStep[0m  [4/26], [94mLoss[0m : 2.51287
[1mStep[0m  [6/26], [94mLoss[0m : 2.55411
[1mStep[0m  [8/26], [94mLoss[0m : 2.45828
[1mStep[0m  [10/26], [94mLoss[0m : 2.48430
[1mStep[0m  [12/26], [94mLoss[0m : 2.54338
[1mStep[0m  [14/26], [94mLoss[0m : 2.53898
[1mStep[0m  [16/26], [94mLoss[0m : 2.58087
[1mStep[0m  [18/26], [94mLoss[0m : 2.63149
[1mStep[0m  [20/26], [94mLoss[0m : 2.56763
[1mStep[0m  [22/26], [94mLoss[0m : 2.62598
[1mStep[0m  [24/26], [94mLoss[0m : 2.61436

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61309
[1mStep[0m  [2/26], [94mLoss[0m : 2.55164
[1mStep[0m  [4/26], [94mLoss[0m : 2.57650
[1mStep[0m  [6/26], [94mLoss[0m : 2.58714
[1mStep[0m  [8/26], [94mLoss[0m : 2.55840
[1mStep[0m  [10/26], [94mLoss[0m : 2.45288
[1mStep[0m  [12/26], [94mLoss[0m : 2.46864
[1mStep[0m  [14/26], [94mLoss[0m : 2.56111
[1mStep[0m  [16/26], [94mLoss[0m : 2.53126
[1mStep[0m  [18/26], [94mLoss[0m : 2.59288
[1mStep[0m  [20/26], [94mLoss[0m : 2.71287
[1mStep[0m  [22/26], [94mLoss[0m : 2.63280
[1mStep[0m  [24/26], [94mLoss[0m : 2.39868

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.441, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51961
[1mStep[0m  [2/26], [94mLoss[0m : 2.61785
[1mStep[0m  [4/26], [94mLoss[0m : 2.63162
[1mStep[0m  [6/26], [94mLoss[0m : 2.49876
[1mStep[0m  [8/26], [94mLoss[0m : 2.68267
[1mStep[0m  [10/26], [94mLoss[0m : 2.47107
[1mStep[0m  [12/26], [94mLoss[0m : 2.56221
[1mStep[0m  [14/26], [94mLoss[0m : 2.51976
[1mStep[0m  [16/26], [94mLoss[0m : 2.56904
[1mStep[0m  [18/26], [94mLoss[0m : 2.66821
[1mStep[0m  [20/26], [94mLoss[0m : 2.40298
[1mStep[0m  [22/26], [94mLoss[0m : 2.47941
[1mStep[0m  [24/26], [94mLoss[0m : 2.49238

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.432, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58476
[1mStep[0m  [2/26], [94mLoss[0m : 2.54588
[1mStep[0m  [4/26], [94mLoss[0m : 2.55916
[1mStep[0m  [6/26], [94mLoss[0m : 2.47747
[1mStep[0m  [8/26], [94mLoss[0m : 2.49577
[1mStep[0m  [10/26], [94mLoss[0m : 2.56779
[1mStep[0m  [12/26], [94mLoss[0m : 2.48976
[1mStep[0m  [14/26], [94mLoss[0m : 2.64999
[1mStep[0m  [16/26], [94mLoss[0m : 2.52658
[1mStep[0m  [18/26], [94mLoss[0m : 2.44922
[1mStep[0m  [20/26], [94mLoss[0m : 2.58673
[1mStep[0m  [22/26], [94mLoss[0m : 2.40795
[1mStep[0m  [24/26], [94mLoss[0m : 2.55602

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.435, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.430
====================================

Phase 1 - Evaluation MAE:  2.4301265203035793
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 2.64935
[1mStep[0m  [2/26], [94mLoss[0m : 2.62755
[1mStep[0m  [4/26], [94mLoss[0m : 2.44836
[1mStep[0m  [6/26], [94mLoss[0m : 2.53013
[1mStep[0m  [8/26], [94mLoss[0m : 2.58133
[1mStep[0m  [10/26], [94mLoss[0m : 2.64250
[1mStep[0m  [12/26], [94mLoss[0m : 2.75233
[1mStep[0m  [14/26], [94mLoss[0m : 2.60575
[1mStep[0m  [16/26], [94mLoss[0m : 2.72949
[1mStep[0m  [18/26], [94mLoss[0m : 2.43163
[1mStep[0m  [20/26], [94mLoss[0m : 2.51611
[1mStep[0m  [22/26], [94mLoss[0m : 2.46212
[1mStep[0m  [24/26], [94mLoss[0m : 2.50375

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57923
[1mStep[0m  [2/26], [94mLoss[0m : 2.74157
[1mStep[0m  [4/26], [94mLoss[0m : 2.49729
[1mStep[0m  [6/26], [94mLoss[0m : 2.41289
[1mStep[0m  [8/26], [94mLoss[0m : 2.57123
[1mStep[0m  [10/26], [94mLoss[0m : 2.55164
[1mStep[0m  [12/26], [94mLoss[0m : 2.54762
[1mStep[0m  [14/26], [94mLoss[0m : 2.50189
[1mStep[0m  [16/26], [94mLoss[0m : 2.60474
[1mStep[0m  [18/26], [94mLoss[0m : 2.46546
[1mStep[0m  [20/26], [94mLoss[0m : 2.70009
[1mStep[0m  [22/26], [94mLoss[0m : 2.59617
[1mStep[0m  [24/26], [94mLoss[0m : 2.54978

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53830
[1mStep[0m  [2/26], [94mLoss[0m : 2.53810
[1mStep[0m  [4/26], [94mLoss[0m : 2.57386
[1mStep[0m  [6/26], [94mLoss[0m : 2.56022
[1mStep[0m  [8/26], [94mLoss[0m : 2.67502
[1mStep[0m  [10/26], [94mLoss[0m : 2.57814
[1mStep[0m  [12/26], [94mLoss[0m : 2.52975
[1mStep[0m  [14/26], [94mLoss[0m : 2.68453
[1mStep[0m  [16/26], [94mLoss[0m : 2.51836
[1mStep[0m  [18/26], [94mLoss[0m : 2.60153
[1mStep[0m  [20/26], [94mLoss[0m : 2.54164
[1mStep[0m  [22/26], [94mLoss[0m : 2.61690
[1mStep[0m  [24/26], [94mLoss[0m : 2.52861

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64041
[1mStep[0m  [2/26], [94mLoss[0m : 2.47999
[1mStep[0m  [4/26], [94mLoss[0m : 2.44267
[1mStep[0m  [6/26], [94mLoss[0m : 2.51173
[1mStep[0m  [8/26], [94mLoss[0m : 2.50071
[1mStep[0m  [10/26], [94mLoss[0m : 2.57368
[1mStep[0m  [12/26], [94mLoss[0m : 2.45862
[1mStep[0m  [14/26], [94mLoss[0m : 2.40698
[1mStep[0m  [16/26], [94mLoss[0m : 2.51882
[1mStep[0m  [18/26], [94mLoss[0m : 2.58649
[1mStep[0m  [20/26], [94mLoss[0m : 2.57377
[1mStep[0m  [22/26], [94mLoss[0m : 2.47755
[1mStep[0m  [24/26], [94mLoss[0m : 2.52313

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51310
[1mStep[0m  [2/26], [94mLoss[0m : 2.46475
[1mStep[0m  [4/26], [94mLoss[0m : 2.46712
[1mStep[0m  [6/26], [94mLoss[0m : 2.57546
[1mStep[0m  [8/26], [94mLoss[0m : 2.54002
[1mStep[0m  [10/26], [94mLoss[0m : 2.48930
[1mStep[0m  [12/26], [94mLoss[0m : 2.33867
[1mStep[0m  [14/26], [94mLoss[0m : 2.36424
[1mStep[0m  [16/26], [94mLoss[0m : 2.56114
[1mStep[0m  [18/26], [94mLoss[0m : 2.41709
[1mStep[0m  [20/26], [94mLoss[0m : 2.62171
[1mStep[0m  [22/26], [94mLoss[0m : 2.52749
[1mStep[0m  [24/26], [94mLoss[0m : 2.41454

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59689
[1mStep[0m  [2/26], [94mLoss[0m : 2.50121
[1mStep[0m  [4/26], [94mLoss[0m : 2.36873
[1mStep[0m  [6/26], [94mLoss[0m : 2.43912
[1mStep[0m  [8/26], [94mLoss[0m : 2.30076
[1mStep[0m  [10/26], [94mLoss[0m : 2.55751
[1mStep[0m  [12/26], [94mLoss[0m : 2.55739
[1mStep[0m  [14/26], [94mLoss[0m : 2.58503
[1mStep[0m  [16/26], [94mLoss[0m : 2.44349
[1mStep[0m  [18/26], [94mLoss[0m : 2.59449
[1mStep[0m  [20/26], [94mLoss[0m : 2.48208
[1mStep[0m  [22/26], [94mLoss[0m : 2.37703
[1mStep[0m  [24/26], [94mLoss[0m : 2.55049

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.546, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52860
[1mStep[0m  [2/26], [94mLoss[0m : 2.54991
[1mStep[0m  [4/26], [94mLoss[0m : 2.43295
[1mStep[0m  [6/26], [94mLoss[0m : 2.56197
[1mStep[0m  [8/26], [94mLoss[0m : 2.52886
[1mStep[0m  [10/26], [94mLoss[0m : 2.49488
[1mStep[0m  [12/26], [94mLoss[0m : 2.60086
[1mStep[0m  [14/26], [94mLoss[0m : 2.44193
[1mStep[0m  [16/26], [94mLoss[0m : 2.63140
[1mStep[0m  [18/26], [94mLoss[0m : 2.30666
[1mStep[0m  [20/26], [94mLoss[0m : 2.26163
[1mStep[0m  [22/26], [94mLoss[0m : 2.67322
[1mStep[0m  [24/26], [94mLoss[0m : 2.52114

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.584, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38352
[1mStep[0m  [2/26], [94mLoss[0m : 2.60965
[1mStep[0m  [4/26], [94mLoss[0m : 2.27409
[1mStep[0m  [6/26], [94mLoss[0m : 2.58554
[1mStep[0m  [8/26], [94mLoss[0m : 2.40809
[1mStep[0m  [10/26], [94mLoss[0m : 2.47608
[1mStep[0m  [12/26], [94mLoss[0m : 2.52968
[1mStep[0m  [14/26], [94mLoss[0m : 2.44669
[1mStep[0m  [16/26], [94mLoss[0m : 2.53738
[1mStep[0m  [18/26], [94mLoss[0m : 2.70967
[1mStep[0m  [20/26], [94mLoss[0m : 2.47653
[1mStep[0m  [22/26], [94mLoss[0m : 2.50430
[1mStep[0m  [24/26], [94mLoss[0m : 2.44556

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.643, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40943
[1mStep[0m  [2/26], [94mLoss[0m : 2.50467
[1mStep[0m  [4/26], [94mLoss[0m : 2.53533
[1mStep[0m  [6/26], [94mLoss[0m : 2.47800
[1mStep[0m  [8/26], [94mLoss[0m : 2.50742
[1mStep[0m  [10/26], [94mLoss[0m : 2.32189
[1mStep[0m  [12/26], [94mLoss[0m : 2.44010
[1mStep[0m  [14/26], [94mLoss[0m : 2.45592
[1mStep[0m  [16/26], [94mLoss[0m : 2.58244
[1mStep[0m  [18/26], [94mLoss[0m : 2.42175
[1mStep[0m  [20/26], [94mLoss[0m : 2.40692
[1mStep[0m  [22/26], [94mLoss[0m : 2.54637
[1mStep[0m  [24/26], [94mLoss[0m : 2.51260

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.605, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61979
[1mStep[0m  [2/26], [94mLoss[0m : 2.29729
[1mStep[0m  [4/26], [94mLoss[0m : 2.57062
[1mStep[0m  [6/26], [94mLoss[0m : 2.58216
[1mStep[0m  [8/26], [94mLoss[0m : 2.59241
[1mStep[0m  [10/26], [94mLoss[0m : 2.43515
[1mStep[0m  [12/26], [94mLoss[0m : 2.41925
[1mStep[0m  [14/26], [94mLoss[0m : 2.30071
[1mStep[0m  [16/26], [94mLoss[0m : 2.41276
[1mStep[0m  [18/26], [94mLoss[0m : 2.42448
[1mStep[0m  [20/26], [94mLoss[0m : 2.38318
[1mStep[0m  [22/26], [94mLoss[0m : 2.38802
[1mStep[0m  [24/26], [94mLoss[0m : 2.40203

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.593, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35490
[1mStep[0m  [2/26], [94mLoss[0m : 2.41872
[1mStep[0m  [4/26], [94mLoss[0m : 2.40032
[1mStep[0m  [6/26], [94mLoss[0m : 2.44706
[1mStep[0m  [8/26], [94mLoss[0m : 2.43976
[1mStep[0m  [10/26], [94mLoss[0m : 2.31332
[1mStep[0m  [12/26], [94mLoss[0m : 2.41982
[1mStep[0m  [14/26], [94mLoss[0m : 2.34990
[1mStep[0m  [16/26], [94mLoss[0m : 2.55906
[1mStep[0m  [18/26], [94mLoss[0m : 2.40628
[1mStep[0m  [20/26], [94mLoss[0m : 2.46167
[1mStep[0m  [22/26], [94mLoss[0m : 2.45778
[1mStep[0m  [24/26], [94mLoss[0m : 2.46253

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.645, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41667
[1mStep[0m  [2/26], [94mLoss[0m : 2.35980
[1mStep[0m  [4/26], [94mLoss[0m : 2.35550
[1mStep[0m  [6/26], [94mLoss[0m : 2.70590
[1mStep[0m  [8/26], [94mLoss[0m : 2.66329
[1mStep[0m  [10/26], [94mLoss[0m : 2.47927
[1mStep[0m  [12/26], [94mLoss[0m : 2.40020
[1mStep[0m  [14/26], [94mLoss[0m : 2.46605
[1mStep[0m  [16/26], [94mLoss[0m : 2.30443
[1mStep[0m  [18/26], [94mLoss[0m : 2.50493
[1mStep[0m  [20/26], [94mLoss[0m : 2.39963
[1mStep[0m  [22/26], [94mLoss[0m : 2.37912
[1mStep[0m  [24/26], [94mLoss[0m : 2.47200

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.651, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48353
[1mStep[0m  [2/26], [94mLoss[0m : 2.34785
[1mStep[0m  [4/26], [94mLoss[0m : 2.37158
[1mStep[0m  [6/26], [94mLoss[0m : 2.49959
[1mStep[0m  [8/26], [94mLoss[0m : 2.42457
[1mStep[0m  [10/26], [94mLoss[0m : 2.43952
[1mStep[0m  [12/26], [94mLoss[0m : 2.33874
[1mStep[0m  [14/26], [94mLoss[0m : 2.41720
[1mStep[0m  [16/26], [94mLoss[0m : 2.36718
[1mStep[0m  [18/26], [94mLoss[0m : 2.52410
[1mStep[0m  [20/26], [94mLoss[0m : 2.52089
[1mStep[0m  [22/26], [94mLoss[0m : 2.38913
[1mStep[0m  [24/26], [94mLoss[0m : 2.44729

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.718, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30330
[1mStep[0m  [2/26], [94mLoss[0m : 2.35478
[1mStep[0m  [4/26], [94mLoss[0m : 2.40257
[1mStep[0m  [6/26], [94mLoss[0m : 2.23773
[1mStep[0m  [8/26], [94mLoss[0m : 2.37882
[1mStep[0m  [10/26], [94mLoss[0m : 2.40040
[1mStep[0m  [12/26], [94mLoss[0m : 2.45418
[1mStep[0m  [14/26], [94mLoss[0m : 2.29962
[1mStep[0m  [16/26], [94mLoss[0m : 2.31716
[1mStep[0m  [18/26], [94mLoss[0m : 2.49726
[1mStep[0m  [20/26], [94mLoss[0m : 2.33209
[1mStep[0m  [22/26], [94mLoss[0m : 2.51367
[1mStep[0m  [24/26], [94mLoss[0m : 2.40419

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.680, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41178
[1mStep[0m  [2/26], [94mLoss[0m : 2.24609
[1mStep[0m  [4/26], [94mLoss[0m : 2.41708
[1mStep[0m  [6/26], [94mLoss[0m : 2.36524
[1mStep[0m  [8/26], [94mLoss[0m : 2.41369
[1mStep[0m  [10/26], [94mLoss[0m : 2.36539
[1mStep[0m  [12/26], [94mLoss[0m : 2.30071
[1mStep[0m  [14/26], [94mLoss[0m : 2.38780
[1mStep[0m  [16/26], [94mLoss[0m : 2.59927
[1mStep[0m  [18/26], [94mLoss[0m : 2.31944
[1mStep[0m  [20/26], [94mLoss[0m : 2.33647
[1mStep[0m  [22/26], [94mLoss[0m : 2.45515
[1mStep[0m  [24/26], [94mLoss[0m : 2.51091

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.665, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35913
[1mStep[0m  [2/26], [94mLoss[0m : 2.49320
[1mStep[0m  [4/26], [94mLoss[0m : 2.28583
[1mStep[0m  [6/26], [94mLoss[0m : 2.31353
[1mStep[0m  [8/26], [94mLoss[0m : 2.45917
[1mStep[0m  [10/26], [94mLoss[0m : 2.35254
[1mStep[0m  [12/26], [94mLoss[0m : 2.44422
[1mStep[0m  [14/26], [94mLoss[0m : 2.48728
[1mStep[0m  [16/26], [94mLoss[0m : 2.30271
[1mStep[0m  [18/26], [94mLoss[0m : 2.33625
[1mStep[0m  [20/26], [94mLoss[0m : 2.39383
[1mStep[0m  [22/26], [94mLoss[0m : 2.32295
[1mStep[0m  [24/26], [94mLoss[0m : 2.35642

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.721, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49821
[1mStep[0m  [2/26], [94mLoss[0m : 2.37062
[1mStep[0m  [4/26], [94mLoss[0m : 2.26042
[1mStep[0m  [6/26], [94mLoss[0m : 2.30936
[1mStep[0m  [8/26], [94mLoss[0m : 2.24642
[1mStep[0m  [10/26], [94mLoss[0m : 2.35598
[1mStep[0m  [12/26], [94mLoss[0m : 2.33582
[1mStep[0m  [14/26], [94mLoss[0m : 2.27699
[1mStep[0m  [16/26], [94mLoss[0m : 2.39742
[1mStep[0m  [18/26], [94mLoss[0m : 2.35246
[1mStep[0m  [20/26], [94mLoss[0m : 2.29921
[1mStep[0m  [22/26], [94mLoss[0m : 2.29054
[1mStep[0m  [24/26], [94mLoss[0m : 2.36853

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.697, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34490
[1mStep[0m  [2/26], [94mLoss[0m : 2.41786
[1mStep[0m  [4/26], [94mLoss[0m : 2.36105
[1mStep[0m  [6/26], [94mLoss[0m : 2.38455
[1mStep[0m  [8/26], [94mLoss[0m : 2.34996
[1mStep[0m  [10/26], [94mLoss[0m : 2.35710
[1mStep[0m  [12/26], [94mLoss[0m : 2.10999
[1mStep[0m  [14/26], [94mLoss[0m : 2.26356
[1mStep[0m  [16/26], [94mLoss[0m : 2.27224
[1mStep[0m  [18/26], [94mLoss[0m : 2.32520
[1mStep[0m  [20/26], [94mLoss[0m : 2.39020
[1mStep[0m  [22/26], [94mLoss[0m : 2.31878
[1mStep[0m  [24/26], [94mLoss[0m : 2.35278

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.703, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24189
[1mStep[0m  [2/26], [94mLoss[0m : 2.31370
[1mStep[0m  [4/26], [94mLoss[0m : 2.39418
[1mStep[0m  [6/26], [94mLoss[0m : 2.27229
[1mStep[0m  [8/26], [94mLoss[0m : 2.19140
[1mStep[0m  [10/26], [94mLoss[0m : 2.17621
[1mStep[0m  [12/26], [94mLoss[0m : 2.23914
[1mStep[0m  [14/26], [94mLoss[0m : 2.19998
[1mStep[0m  [16/26], [94mLoss[0m : 2.29629
[1mStep[0m  [18/26], [94mLoss[0m : 2.49947
[1mStep[0m  [20/26], [94mLoss[0m : 2.18648
[1mStep[0m  [22/26], [94mLoss[0m : 2.31898
[1mStep[0m  [24/26], [94mLoss[0m : 2.39598

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.694, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19373
[1mStep[0m  [2/26], [94mLoss[0m : 2.25141
[1mStep[0m  [4/26], [94mLoss[0m : 2.22727
[1mStep[0m  [6/26], [94mLoss[0m : 2.27236
[1mStep[0m  [8/26], [94mLoss[0m : 2.23056
[1mStep[0m  [10/26], [94mLoss[0m : 2.26409
[1mStep[0m  [12/26], [94mLoss[0m : 2.34835
[1mStep[0m  [14/26], [94mLoss[0m : 2.22310
[1mStep[0m  [16/26], [94mLoss[0m : 2.25156
[1mStep[0m  [18/26], [94mLoss[0m : 2.41114
[1mStep[0m  [20/26], [94mLoss[0m : 2.27921
[1mStep[0m  [22/26], [94mLoss[0m : 2.32594
[1mStep[0m  [24/26], [94mLoss[0m : 2.12917

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.779, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22130
[1mStep[0m  [2/26], [94mLoss[0m : 2.28878
[1mStep[0m  [4/26], [94mLoss[0m : 2.17314
[1mStep[0m  [6/26], [94mLoss[0m : 2.24939
[1mStep[0m  [8/26], [94mLoss[0m : 2.22193
[1mStep[0m  [10/26], [94mLoss[0m : 2.30802
[1mStep[0m  [12/26], [94mLoss[0m : 2.35802
[1mStep[0m  [14/26], [94mLoss[0m : 2.26903
[1mStep[0m  [16/26], [94mLoss[0m : 2.26287
[1mStep[0m  [18/26], [94mLoss[0m : 2.26596
[1mStep[0m  [20/26], [94mLoss[0m : 2.20699
[1mStep[0m  [22/26], [94mLoss[0m : 2.27826
[1mStep[0m  [24/26], [94mLoss[0m : 2.34309

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.718, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24954
[1mStep[0m  [2/26], [94mLoss[0m : 2.19769
[1mStep[0m  [4/26], [94mLoss[0m : 2.29948
[1mStep[0m  [6/26], [94mLoss[0m : 2.18963
[1mStep[0m  [8/26], [94mLoss[0m : 2.14646
[1mStep[0m  [10/26], [94mLoss[0m : 2.18930
[1mStep[0m  [12/26], [94mLoss[0m : 2.13999
[1mStep[0m  [14/26], [94mLoss[0m : 2.25181
[1mStep[0m  [16/26], [94mLoss[0m : 2.41197
[1mStep[0m  [18/26], [94mLoss[0m : 2.23909
[1mStep[0m  [20/26], [94mLoss[0m : 2.37830
[1mStep[0m  [22/26], [94mLoss[0m : 2.19739
[1mStep[0m  [24/26], [94mLoss[0m : 2.08974

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.582, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33771
[1mStep[0m  [2/26], [94mLoss[0m : 2.15003
[1mStep[0m  [4/26], [94mLoss[0m : 2.25057
[1mStep[0m  [6/26], [94mLoss[0m : 2.32889
[1mStep[0m  [8/26], [94mLoss[0m : 2.31253
[1mStep[0m  [10/26], [94mLoss[0m : 2.18951
[1mStep[0m  [12/26], [94mLoss[0m : 2.12928
[1mStep[0m  [14/26], [94mLoss[0m : 2.15781
[1mStep[0m  [16/26], [94mLoss[0m : 2.25579
[1mStep[0m  [18/26], [94mLoss[0m : 2.26818
[1mStep[0m  [20/26], [94mLoss[0m : 2.33052
[1mStep[0m  [22/26], [94mLoss[0m : 2.20195
[1mStep[0m  [24/26], [94mLoss[0m : 2.18779

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.629, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17752
[1mStep[0m  [2/26], [94mLoss[0m : 2.25428
[1mStep[0m  [4/26], [94mLoss[0m : 2.20128
[1mStep[0m  [6/26], [94mLoss[0m : 2.25052
[1mStep[0m  [8/26], [94mLoss[0m : 2.07313
[1mStep[0m  [10/26], [94mLoss[0m : 2.21186
[1mStep[0m  [12/26], [94mLoss[0m : 2.01265
[1mStep[0m  [14/26], [94mLoss[0m : 2.24745
[1mStep[0m  [16/26], [94mLoss[0m : 2.28792
[1mStep[0m  [18/26], [94mLoss[0m : 2.14963
[1mStep[0m  [20/26], [94mLoss[0m : 2.02980
[1mStep[0m  [22/26], [94mLoss[0m : 2.28275
[1mStep[0m  [24/26], [94mLoss[0m : 2.24468

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.617, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15175
[1mStep[0m  [2/26], [94mLoss[0m : 2.18274
[1mStep[0m  [4/26], [94mLoss[0m : 2.20526
[1mStep[0m  [6/26], [94mLoss[0m : 2.08142
[1mStep[0m  [8/26], [94mLoss[0m : 2.30520
[1mStep[0m  [10/26], [94mLoss[0m : 2.11899
[1mStep[0m  [12/26], [94mLoss[0m : 2.12413
[1mStep[0m  [14/26], [94mLoss[0m : 2.10922
[1mStep[0m  [16/26], [94mLoss[0m : 2.12615
[1mStep[0m  [18/26], [94mLoss[0m : 2.05569
[1mStep[0m  [20/26], [94mLoss[0m : 2.09047
[1mStep[0m  [22/26], [94mLoss[0m : 2.20209
[1mStep[0m  [24/26], [94mLoss[0m : 2.22914

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.590, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09376
[1mStep[0m  [2/26], [94mLoss[0m : 2.12360
[1mStep[0m  [4/26], [94mLoss[0m : 2.14421
[1mStep[0m  [6/26], [94mLoss[0m : 2.16266
[1mStep[0m  [8/26], [94mLoss[0m : 2.19834
[1mStep[0m  [10/26], [94mLoss[0m : 2.21810
[1mStep[0m  [12/26], [94mLoss[0m : 2.15352
[1mStep[0m  [14/26], [94mLoss[0m : 2.04674
[1mStep[0m  [16/26], [94mLoss[0m : 2.20869
[1mStep[0m  [18/26], [94mLoss[0m : 2.11604
[1mStep[0m  [20/26], [94mLoss[0m : 2.10055
[1mStep[0m  [22/26], [94mLoss[0m : 2.24262
[1mStep[0m  [24/26], [94mLoss[0m : 2.28174

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.157, [92mTest[0m: 2.625, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27816
[1mStep[0m  [2/26], [94mLoss[0m : 2.16800
[1mStep[0m  [4/26], [94mLoss[0m : 2.12116
[1mStep[0m  [6/26], [94mLoss[0m : 2.29226
[1mStep[0m  [8/26], [94mLoss[0m : 2.12105
[1mStep[0m  [10/26], [94mLoss[0m : 2.16696
[1mStep[0m  [12/26], [94mLoss[0m : 2.03771
[1mStep[0m  [14/26], [94mLoss[0m : 2.31552
[1mStep[0m  [16/26], [94mLoss[0m : 2.20657
[1mStep[0m  [18/26], [94mLoss[0m : 2.11091
[1mStep[0m  [20/26], [94mLoss[0m : 2.22290
[1mStep[0m  [22/26], [94mLoss[0m : 2.25137
[1mStep[0m  [24/26], [94mLoss[0m : 2.02688

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.626, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.18366
[1mStep[0m  [2/26], [94mLoss[0m : 2.10841
[1mStep[0m  [4/26], [94mLoss[0m : 2.23694
[1mStep[0m  [6/26], [94mLoss[0m : 2.05223
[1mStep[0m  [8/26], [94mLoss[0m : 2.11118
[1mStep[0m  [10/26], [94mLoss[0m : 2.15176
[1mStep[0m  [12/26], [94mLoss[0m : 2.10894
[1mStep[0m  [14/26], [94mLoss[0m : 2.04693
[1mStep[0m  [16/26], [94mLoss[0m : 2.28756
[1mStep[0m  [18/26], [94mLoss[0m : 2.09983
[1mStep[0m  [20/26], [94mLoss[0m : 2.12908
[1mStep[0m  [22/26], [94mLoss[0m : 2.05027
[1mStep[0m  [24/26], [94mLoss[0m : 2.10504

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.493, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96123
[1mStep[0m  [2/26], [94mLoss[0m : 2.18910
[1mStep[0m  [4/26], [94mLoss[0m : 2.23722
[1mStep[0m  [6/26], [94mLoss[0m : 2.04901
[1mStep[0m  [8/26], [94mLoss[0m : 2.06517
[1mStep[0m  [10/26], [94mLoss[0m : 2.15799
[1mStep[0m  [12/26], [94mLoss[0m : 2.20570
[1mStep[0m  [14/26], [94mLoss[0m : 2.09208
[1mStep[0m  [16/26], [94mLoss[0m : 2.17531
[1mStep[0m  [18/26], [94mLoss[0m : 2.13159
[1mStep[0m  [20/26], [94mLoss[0m : 2.00788
[1mStep[0m  [22/26], [94mLoss[0m : 2.14884
[1mStep[0m  [24/26], [94mLoss[0m : 2.09826

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.647, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97242
[1mStep[0m  [2/26], [94mLoss[0m : 2.11289
[1mStep[0m  [4/26], [94mLoss[0m : 2.10113
[1mStep[0m  [6/26], [94mLoss[0m : 2.03070
[1mStep[0m  [8/26], [94mLoss[0m : 1.92844
[1mStep[0m  [10/26], [94mLoss[0m : 1.96695
[1mStep[0m  [12/26], [94mLoss[0m : 2.08134
[1mStep[0m  [14/26], [94mLoss[0m : 2.02122
[1mStep[0m  [16/26], [94mLoss[0m : 2.17705
[1mStep[0m  [18/26], [94mLoss[0m : 2.06912
[1mStep[0m  [20/26], [94mLoss[0m : 2.12705
[1mStep[0m  [22/26], [94mLoss[0m : 2.09016
[1mStep[0m  [24/26], [94mLoss[0m : 2.10686

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.556, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.514
====================================

Phase 2 - Evaluation MAE:  2.513879189124474
MAE score P1       2.430127
MAE score P2       2.513879
loss               2.068791
learning_rate          0.01
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay         0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 11.11012
[1mStep[0m  [2/26], [94mLoss[0m : 10.77698
[1mStep[0m  [4/26], [94mLoss[0m : 10.64892
[1mStep[0m  [6/26], [94mLoss[0m : 10.62773
[1mStep[0m  [8/26], [94mLoss[0m : 10.65508
[1mStep[0m  [10/26], [94mLoss[0m : 10.39268
[1mStep[0m  [12/26], [94mLoss[0m : 10.33096
[1mStep[0m  [14/26], [94mLoss[0m : 9.84685
[1mStep[0m  [16/26], [94mLoss[0m : 9.69208
[1mStep[0m  [18/26], [94mLoss[0m : 9.74604
[1mStep[0m  [20/26], [94mLoss[0m : 9.43922
[1mStep[0m  [22/26], [94mLoss[0m : 8.80054
[1mStep[0m  [24/26], [94mLoss[0m : 9.02298

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.998, [92mTest[0m: 10.749, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.63363
[1mStep[0m  [2/26], [94mLoss[0m : 8.25085
[1mStep[0m  [4/26], [94mLoss[0m : 7.95814
[1mStep[0m  [6/26], [94mLoss[0m : 7.40012
[1mStep[0m  [8/26], [94mLoss[0m : 7.24912
[1mStep[0m  [10/26], [94mLoss[0m : 6.71191
[1mStep[0m  [12/26], [94mLoss[0m : 6.13952
[1mStep[0m  [14/26], [94mLoss[0m : 6.05450
[1mStep[0m  [16/26], [94mLoss[0m : 5.45982
[1mStep[0m  [18/26], [94mLoss[0m : 5.20950
[1mStep[0m  [20/26], [94mLoss[0m : 4.99531
[1mStep[0m  [22/26], [94mLoss[0m : 4.75325
[1mStep[0m  [24/26], [94mLoss[0m : 4.20648

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.315, [92mTest[0m: 8.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.68126
[1mStep[0m  [2/26], [94mLoss[0m : 3.48999
[1mStep[0m  [4/26], [94mLoss[0m : 3.16028
[1mStep[0m  [6/26], [94mLoss[0m : 3.00144
[1mStep[0m  [8/26], [94mLoss[0m : 2.74666
[1mStep[0m  [10/26], [94mLoss[0m : 2.68872
[1mStep[0m  [12/26], [94mLoss[0m : 2.66797
[1mStep[0m  [14/26], [94mLoss[0m : 2.58911
[1mStep[0m  [16/26], [94mLoss[0m : 2.61757
[1mStep[0m  [18/26], [94mLoss[0m : 2.71338
[1mStep[0m  [20/26], [94mLoss[0m : 2.54039
[1mStep[0m  [22/26], [94mLoss[0m : 2.71802
[1mStep[0m  [24/26], [94mLoss[0m : 2.63288

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.898, [92mTest[0m: 3.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59546
[1mStep[0m  [2/26], [94mLoss[0m : 2.59804
[1mStep[0m  [4/26], [94mLoss[0m : 2.68041
[1mStep[0m  [6/26], [94mLoss[0m : 2.66820
[1mStep[0m  [8/26], [94mLoss[0m : 2.59936
[1mStep[0m  [10/26], [94mLoss[0m : 2.59037
[1mStep[0m  [12/26], [94mLoss[0m : 2.63605
[1mStep[0m  [14/26], [94mLoss[0m : 2.59900
[1mStep[0m  [16/26], [94mLoss[0m : 2.48312
[1mStep[0m  [18/26], [94mLoss[0m : 2.44355
[1mStep[0m  [20/26], [94mLoss[0m : 2.55358
[1mStep[0m  [22/26], [94mLoss[0m : 2.45068
[1mStep[0m  [24/26], [94mLoss[0m : 2.66374

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.527, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43638
[1mStep[0m  [2/26], [94mLoss[0m : 2.48522
[1mStep[0m  [4/26], [94mLoss[0m : 2.56979
[1mStep[0m  [6/26], [94mLoss[0m : 2.34821
[1mStep[0m  [8/26], [94mLoss[0m : 2.51857
[1mStep[0m  [10/26], [94mLoss[0m : 2.48539
[1mStep[0m  [12/26], [94mLoss[0m : 2.55461
[1mStep[0m  [14/26], [94mLoss[0m : 2.40886
[1mStep[0m  [16/26], [94mLoss[0m : 2.44551
[1mStep[0m  [18/26], [94mLoss[0m : 2.62567
[1mStep[0m  [20/26], [94mLoss[0m : 2.67209
[1mStep[0m  [22/26], [94mLoss[0m : 2.56172
[1mStep[0m  [24/26], [94mLoss[0m : 2.56197

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45570
[1mStep[0m  [2/26], [94mLoss[0m : 2.46747
[1mStep[0m  [4/26], [94mLoss[0m : 2.48617
[1mStep[0m  [6/26], [94mLoss[0m : 2.64874
[1mStep[0m  [8/26], [94mLoss[0m : 2.46716
[1mStep[0m  [10/26], [94mLoss[0m : 2.49443
[1mStep[0m  [12/26], [94mLoss[0m : 2.52294
[1mStep[0m  [14/26], [94mLoss[0m : 2.44491
[1mStep[0m  [16/26], [94mLoss[0m : 2.56191
[1mStep[0m  [18/26], [94mLoss[0m : 2.50809
[1mStep[0m  [20/26], [94mLoss[0m : 2.42246
[1mStep[0m  [22/26], [94mLoss[0m : 2.51638
[1mStep[0m  [24/26], [94mLoss[0m : 2.50065

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41624
[1mStep[0m  [2/26], [94mLoss[0m : 2.48251
[1mStep[0m  [4/26], [94mLoss[0m : 2.54858
[1mStep[0m  [6/26], [94mLoss[0m : 2.44673
[1mStep[0m  [8/26], [94mLoss[0m : 2.55739
[1mStep[0m  [10/26], [94mLoss[0m : 2.39042
[1mStep[0m  [12/26], [94mLoss[0m : 2.46355
[1mStep[0m  [14/26], [94mLoss[0m : 2.49334
[1mStep[0m  [16/26], [94mLoss[0m : 2.55426
[1mStep[0m  [18/26], [94mLoss[0m : 2.49002
[1mStep[0m  [20/26], [94mLoss[0m : 2.51756
[1mStep[0m  [22/26], [94mLoss[0m : 2.60167
[1mStep[0m  [24/26], [94mLoss[0m : 2.44605

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49093
[1mStep[0m  [2/26], [94mLoss[0m : 2.33899
[1mStep[0m  [4/26], [94mLoss[0m : 2.51568
[1mStep[0m  [6/26], [94mLoss[0m : 2.47280
[1mStep[0m  [8/26], [94mLoss[0m : 2.41765
[1mStep[0m  [10/26], [94mLoss[0m : 2.49444
[1mStep[0m  [12/26], [94mLoss[0m : 2.49579
[1mStep[0m  [14/26], [94mLoss[0m : 2.48593
[1mStep[0m  [16/26], [94mLoss[0m : 2.37702
[1mStep[0m  [18/26], [94mLoss[0m : 2.52905
[1mStep[0m  [20/26], [94mLoss[0m : 2.58789
[1mStep[0m  [22/26], [94mLoss[0m : 2.40821
[1mStep[0m  [24/26], [94mLoss[0m : 2.44118

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37418
[1mStep[0m  [2/26], [94mLoss[0m : 2.34854
[1mStep[0m  [4/26], [94mLoss[0m : 2.39678
[1mStep[0m  [6/26], [94mLoss[0m : 2.38932
[1mStep[0m  [8/26], [94mLoss[0m : 2.48373
[1mStep[0m  [10/26], [94mLoss[0m : 2.47708
[1mStep[0m  [12/26], [94mLoss[0m : 2.38479
[1mStep[0m  [14/26], [94mLoss[0m : 2.40181
[1mStep[0m  [16/26], [94mLoss[0m : 2.49283
[1mStep[0m  [18/26], [94mLoss[0m : 2.51061
[1mStep[0m  [20/26], [94mLoss[0m : 2.40339
[1mStep[0m  [22/26], [94mLoss[0m : 2.45023
[1mStep[0m  [24/26], [94mLoss[0m : 2.49942

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40144
[1mStep[0m  [2/26], [94mLoss[0m : 2.64664
[1mStep[0m  [4/26], [94mLoss[0m : 2.43538
[1mStep[0m  [6/26], [94mLoss[0m : 2.49183
[1mStep[0m  [8/26], [94mLoss[0m : 2.56958
[1mStep[0m  [10/26], [94mLoss[0m : 2.54447
[1mStep[0m  [12/26], [94mLoss[0m : 2.43319
[1mStep[0m  [14/26], [94mLoss[0m : 2.56437
[1mStep[0m  [16/26], [94mLoss[0m : 2.55041
[1mStep[0m  [18/26], [94mLoss[0m : 2.33563
[1mStep[0m  [20/26], [94mLoss[0m : 2.45020
[1mStep[0m  [22/26], [94mLoss[0m : 2.33679
[1mStep[0m  [24/26], [94mLoss[0m : 2.51245

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47968
[1mStep[0m  [2/26], [94mLoss[0m : 2.37197
[1mStep[0m  [4/26], [94mLoss[0m : 2.45819
[1mStep[0m  [6/26], [94mLoss[0m : 2.42212
[1mStep[0m  [8/26], [94mLoss[0m : 2.49888
[1mStep[0m  [10/26], [94mLoss[0m : 2.55393
[1mStep[0m  [12/26], [94mLoss[0m : 2.33276
[1mStep[0m  [14/26], [94mLoss[0m : 2.52301
[1mStep[0m  [16/26], [94mLoss[0m : 2.33505
[1mStep[0m  [18/26], [94mLoss[0m : 2.35739
[1mStep[0m  [20/26], [94mLoss[0m : 2.44698
[1mStep[0m  [22/26], [94mLoss[0m : 2.24585
[1mStep[0m  [24/26], [94mLoss[0m : 2.46621

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34808
[1mStep[0m  [2/26], [94mLoss[0m : 2.42206
[1mStep[0m  [4/26], [94mLoss[0m : 2.39696
[1mStep[0m  [6/26], [94mLoss[0m : 2.36749
[1mStep[0m  [8/26], [94mLoss[0m : 2.35648
[1mStep[0m  [10/26], [94mLoss[0m : 2.49548
[1mStep[0m  [12/26], [94mLoss[0m : 2.37735
[1mStep[0m  [14/26], [94mLoss[0m : 2.39416
[1mStep[0m  [16/26], [94mLoss[0m : 2.38072
[1mStep[0m  [18/26], [94mLoss[0m : 2.39276
[1mStep[0m  [20/26], [94mLoss[0m : 2.47505
[1mStep[0m  [22/26], [94mLoss[0m : 2.53064
[1mStep[0m  [24/26], [94mLoss[0m : 2.50135

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47232
[1mStep[0m  [2/26], [94mLoss[0m : 2.45919
[1mStep[0m  [4/26], [94mLoss[0m : 2.34872
[1mStep[0m  [6/26], [94mLoss[0m : 2.55109
[1mStep[0m  [8/26], [94mLoss[0m : 2.41900
[1mStep[0m  [10/26], [94mLoss[0m : 2.52037
[1mStep[0m  [12/26], [94mLoss[0m : 2.38293
[1mStep[0m  [14/26], [94mLoss[0m : 2.47823
[1mStep[0m  [16/26], [94mLoss[0m : 2.41658
[1mStep[0m  [18/26], [94mLoss[0m : 2.39085
[1mStep[0m  [20/26], [94mLoss[0m : 2.52667
[1mStep[0m  [22/26], [94mLoss[0m : 2.33118
[1mStep[0m  [24/26], [94mLoss[0m : 2.50406

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39769
[1mStep[0m  [2/26], [94mLoss[0m : 2.25848
[1mStep[0m  [4/26], [94mLoss[0m : 2.57391
[1mStep[0m  [6/26], [94mLoss[0m : 2.54957
[1mStep[0m  [8/26], [94mLoss[0m : 2.38132
[1mStep[0m  [10/26], [94mLoss[0m : 2.42678
[1mStep[0m  [12/26], [94mLoss[0m : 2.47611
[1mStep[0m  [14/26], [94mLoss[0m : 2.44178
[1mStep[0m  [16/26], [94mLoss[0m : 2.56174
[1mStep[0m  [18/26], [94mLoss[0m : 2.33957
[1mStep[0m  [20/26], [94mLoss[0m : 2.34210
[1mStep[0m  [22/26], [94mLoss[0m : 2.43258
[1mStep[0m  [24/26], [94mLoss[0m : 2.56010

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45921
[1mStep[0m  [2/26], [94mLoss[0m : 2.44824
[1mStep[0m  [4/26], [94mLoss[0m : 2.38477
[1mStep[0m  [6/26], [94mLoss[0m : 2.37878
[1mStep[0m  [8/26], [94mLoss[0m : 2.35441
[1mStep[0m  [10/26], [94mLoss[0m : 2.43470
[1mStep[0m  [12/26], [94mLoss[0m : 2.31903
[1mStep[0m  [14/26], [94mLoss[0m : 2.63238
[1mStep[0m  [16/26], [94mLoss[0m : 2.49241
[1mStep[0m  [18/26], [94mLoss[0m : 2.37063
[1mStep[0m  [20/26], [94mLoss[0m : 2.46748
[1mStep[0m  [22/26], [94mLoss[0m : 2.37864
[1mStep[0m  [24/26], [94mLoss[0m : 2.56052

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45440
[1mStep[0m  [2/26], [94mLoss[0m : 2.38207
[1mStep[0m  [4/26], [94mLoss[0m : 2.32590
[1mStep[0m  [6/26], [94mLoss[0m : 2.51553
[1mStep[0m  [8/26], [94mLoss[0m : 2.37102
[1mStep[0m  [10/26], [94mLoss[0m : 2.25776
[1mStep[0m  [12/26], [94mLoss[0m : 2.35373
[1mStep[0m  [14/26], [94mLoss[0m : 2.54625
[1mStep[0m  [16/26], [94mLoss[0m : 2.42143
[1mStep[0m  [18/26], [94mLoss[0m : 2.32598
[1mStep[0m  [20/26], [94mLoss[0m : 2.44560
[1mStep[0m  [22/26], [94mLoss[0m : 2.42830
[1mStep[0m  [24/26], [94mLoss[0m : 2.48920

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34277
[1mStep[0m  [2/26], [94mLoss[0m : 2.40930
[1mStep[0m  [4/26], [94mLoss[0m : 2.46973
[1mStep[0m  [6/26], [94mLoss[0m : 2.44874
[1mStep[0m  [8/26], [94mLoss[0m : 2.51324
[1mStep[0m  [10/26], [94mLoss[0m : 2.27181
[1mStep[0m  [12/26], [94mLoss[0m : 2.34017
[1mStep[0m  [14/26], [94mLoss[0m : 2.41402
[1mStep[0m  [16/26], [94mLoss[0m : 2.34393
[1mStep[0m  [18/26], [94mLoss[0m : 2.27756
[1mStep[0m  [20/26], [94mLoss[0m : 2.41726
[1mStep[0m  [22/26], [94mLoss[0m : 2.37738
[1mStep[0m  [24/26], [94mLoss[0m : 2.45489

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19949
[1mStep[0m  [2/26], [94mLoss[0m : 2.50507
[1mStep[0m  [4/26], [94mLoss[0m : 2.47083
[1mStep[0m  [6/26], [94mLoss[0m : 2.34250
[1mStep[0m  [8/26], [94mLoss[0m : 2.19754
[1mStep[0m  [10/26], [94mLoss[0m : 2.38510
[1mStep[0m  [12/26], [94mLoss[0m : 2.51125
[1mStep[0m  [14/26], [94mLoss[0m : 2.41458
[1mStep[0m  [16/26], [94mLoss[0m : 2.46960
[1mStep[0m  [18/26], [94mLoss[0m : 2.33408
[1mStep[0m  [20/26], [94mLoss[0m : 2.45963
[1mStep[0m  [22/26], [94mLoss[0m : 2.39699
[1mStep[0m  [24/26], [94mLoss[0m : 2.39952

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38416
[1mStep[0m  [2/26], [94mLoss[0m : 2.37153
[1mStep[0m  [4/26], [94mLoss[0m : 2.37227
[1mStep[0m  [6/26], [94mLoss[0m : 2.35827
[1mStep[0m  [8/26], [94mLoss[0m : 2.29894
[1mStep[0m  [10/26], [94mLoss[0m : 2.34527
[1mStep[0m  [12/26], [94mLoss[0m : 2.36614
[1mStep[0m  [14/26], [94mLoss[0m : 2.50224
[1mStep[0m  [16/26], [94mLoss[0m : 2.39433
[1mStep[0m  [18/26], [94mLoss[0m : 2.36631
[1mStep[0m  [20/26], [94mLoss[0m : 2.44778
[1mStep[0m  [22/26], [94mLoss[0m : 2.37828
[1mStep[0m  [24/26], [94mLoss[0m : 2.34767

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48738
[1mStep[0m  [2/26], [94mLoss[0m : 2.25374
[1mStep[0m  [4/26], [94mLoss[0m : 2.29965
[1mStep[0m  [6/26], [94mLoss[0m : 2.36781
[1mStep[0m  [8/26], [94mLoss[0m : 2.39313
[1mStep[0m  [10/26], [94mLoss[0m : 2.48639
[1mStep[0m  [12/26], [94mLoss[0m : 2.35056
[1mStep[0m  [14/26], [94mLoss[0m : 2.39979
[1mStep[0m  [16/26], [94mLoss[0m : 2.37185
[1mStep[0m  [18/26], [94mLoss[0m : 2.56194
[1mStep[0m  [20/26], [94mLoss[0m : 2.47710
[1mStep[0m  [22/26], [94mLoss[0m : 2.41131
[1mStep[0m  [24/26], [94mLoss[0m : 2.40767

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.386, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31231
[1mStep[0m  [2/26], [94mLoss[0m : 2.33451
[1mStep[0m  [4/26], [94mLoss[0m : 2.36750
[1mStep[0m  [6/26], [94mLoss[0m : 2.22996
[1mStep[0m  [8/26], [94mLoss[0m : 2.42930
[1mStep[0m  [10/26], [94mLoss[0m : 2.44701
[1mStep[0m  [12/26], [94mLoss[0m : 2.37701
[1mStep[0m  [14/26], [94mLoss[0m : 2.30086
[1mStep[0m  [16/26], [94mLoss[0m : 2.41633
[1mStep[0m  [18/26], [94mLoss[0m : 2.43412
[1mStep[0m  [20/26], [94mLoss[0m : 2.34892
[1mStep[0m  [22/26], [94mLoss[0m : 2.36128
[1mStep[0m  [24/26], [94mLoss[0m : 2.37451

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.372, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48110
[1mStep[0m  [2/26], [94mLoss[0m : 2.30094
[1mStep[0m  [4/26], [94mLoss[0m : 2.19546
[1mStep[0m  [6/26], [94mLoss[0m : 2.27510
[1mStep[0m  [8/26], [94mLoss[0m : 2.47824
[1mStep[0m  [10/26], [94mLoss[0m : 2.55149
[1mStep[0m  [12/26], [94mLoss[0m : 2.40407
[1mStep[0m  [14/26], [94mLoss[0m : 2.26079
[1mStep[0m  [16/26], [94mLoss[0m : 2.38598
[1mStep[0m  [18/26], [94mLoss[0m : 2.38725
[1mStep[0m  [20/26], [94mLoss[0m : 2.37244
[1mStep[0m  [22/26], [94mLoss[0m : 2.39271
[1mStep[0m  [24/26], [94mLoss[0m : 2.44771

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30564
[1mStep[0m  [2/26], [94mLoss[0m : 2.43376
[1mStep[0m  [4/26], [94mLoss[0m : 2.38455
[1mStep[0m  [6/26], [94mLoss[0m : 2.40350
[1mStep[0m  [8/26], [94mLoss[0m : 2.44832
[1mStep[0m  [10/26], [94mLoss[0m : 2.25953
[1mStep[0m  [12/26], [94mLoss[0m : 2.47205
[1mStep[0m  [14/26], [94mLoss[0m : 2.48257
[1mStep[0m  [16/26], [94mLoss[0m : 2.42128
[1mStep[0m  [18/26], [94mLoss[0m : 2.23811
[1mStep[0m  [20/26], [94mLoss[0m : 2.49069
[1mStep[0m  [22/26], [94mLoss[0m : 2.37971
[1mStep[0m  [24/26], [94mLoss[0m : 2.34227

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40372
[1mStep[0m  [2/26], [94mLoss[0m : 2.37789
[1mStep[0m  [4/26], [94mLoss[0m : 2.33850
[1mStep[0m  [6/26], [94mLoss[0m : 2.39604
[1mStep[0m  [8/26], [94mLoss[0m : 2.37151
[1mStep[0m  [10/26], [94mLoss[0m : 2.30289
[1mStep[0m  [12/26], [94mLoss[0m : 2.34254
[1mStep[0m  [14/26], [94mLoss[0m : 2.45282
[1mStep[0m  [16/26], [94mLoss[0m : 2.43578
[1mStep[0m  [18/26], [94mLoss[0m : 2.54807
[1mStep[0m  [20/26], [94mLoss[0m : 2.16792
[1mStep[0m  [22/26], [94mLoss[0m : 2.32927
[1mStep[0m  [24/26], [94mLoss[0m : 2.50675

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42930
[1mStep[0m  [2/26], [94mLoss[0m : 2.42215
[1mStep[0m  [4/26], [94mLoss[0m : 2.33473
[1mStep[0m  [6/26], [94mLoss[0m : 2.21566
[1mStep[0m  [8/26], [94mLoss[0m : 2.23245
[1mStep[0m  [10/26], [94mLoss[0m : 2.42124
[1mStep[0m  [12/26], [94mLoss[0m : 2.44546
[1mStep[0m  [14/26], [94mLoss[0m : 2.41554
[1mStep[0m  [16/26], [94mLoss[0m : 2.36493
[1mStep[0m  [18/26], [94mLoss[0m : 2.37872
[1mStep[0m  [20/26], [94mLoss[0m : 2.37502
[1mStep[0m  [22/26], [94mLoss[0m : 2.34431
[1mStep[0m  [24/26], [94mLoss[0m : 2.25481

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40714
[1mStep[0m  [2/26], [94mLoss[0m : 2.31592
[1mStep[0m  [4/26], [94mLoss[0m : 2.39083
[1mStep[0m  [6/26], [94mLoss[0m : 2.49013
[1mStep[0m  [8/26], [94mLoss[0m : 2.50294
[1mStep[0m  [10/26], [94mLoss[0m : 2.42330
[1mStep[0m  [12/26], [94mLoss[0m : 2.38006
[1mStep[0m  [14/26], [94mLoss[0m : 2.40737
[1mStep[0m  [16/26], [94mLoss[0m : 2.38270
[1mStep[0m  [18/26], [94mLoss[0m : 2.28846
[1mStep[0m  [20/26], [94mLoss[0m : 2.27789
[1mStep[0m  [22/26], [94mLoss[0m : 2.44102
[1mStep[0m  [24/26], [94mLoss[0m : 2.29584

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43283
[1mStep[0m  [2/26], [94mLoss[0m : 2.42399
[1mStep[0m  [4/26], [94mLoss[0m : 2.40580
[1mStep[0m  [6/26], [94mLoss[0m : 2.50609
[1mStep[0m  [8/26], [94mLoss[0m : 2.33812
[1mStep[0m  [10/26], [94mLoss[0m : 2.40419
[1mStep[0m  [12/26], [94mLoss[0m : 2.28424
[1mStep[0m  [14/26], [94mLoss[0m : 2.45754
[1mStep[0m  [16/26], [94mLoss[0m : 2.32716
[1mStep[0m  [18/26], [94mLoss[0m : 2.43143
[1mStep[0m  [20/26], [94mLoss[0m : 2.39943
[1mStep[0m  [22/26], [94mLoss[0m : 2.32413
[1mStep[0m  [24/26], [94mLoss[0m : 2.51209

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25262
[1mStep[0m  [2/26], [94mLoss[0m : 2.26295
[1mStep[0m  [4/26], [94mLoss[0m : 2.21382
[1mStep[0m  [6/26], [94mLoss[0m : 2.36658
[1mStep[0m  [8/26], [94mLoss[0m : 2.36766
[1mStep[0m  [10/26], [94mLoss[0m : 2.26273
[1mStep[0m  [12/26], [94mLoss[0m : 2.31343
[1mStep[0m  [14/26], [94mLoss[0m : 2.33031
[1mStep[0m  [16/26], [94mLoss[0m : 2.19468
[1mStep[0m  [18/26], [94mLoss[0m : 2.44954
[1mStep[0m  [20/26], [94mLoss[0m : 2.49747
[1mStep[0m  [22/26], [94mLoss[0m : 2.29683
[1mStep[0m  [24/26], [94mLoss[0m : 2.22849

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37179
[1mStep[0m  [2/26], [94mLoss[0m : 2.25428
[1mStep[0m  [4/26], [94mLoss[0m : 2.53292
[1mStep[0m  [6/26], [94mLoss[0m : 2.26103
[1mStep[0m  [8/26], [94mLoss[0m : 2.40728
[1mStep[0m  [10/26], [94mLoss[0m : 2.44104
[1mStep[0m  [12/26], [94mLoss[0m : 2.35655
[1mStep[0m  [14/26], [94mLoss[0m : 2.37096
[1mStep[0m  [16/26], [94mLoss[0m : 2.28397
[1mStep[0m  [18/26], [94mLoss[0m : 2.19124
[1mStep[0m  [20/26], [94mLoss[0m : 2.44043
[1mStep[0m  [22/26], [94mLoss[0m : 2.48102
[1mStep[0m  [24/26], [94mLoss[0m : 2.33302

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25460
[1mStep[0m  [2/26], [94mLoss[0m : 2.39335
[1mStep[0m  [4/26], [94mLoss[0m : 2.42954
[1mStep[0m  [6/26], [94mLoss[0m : 2.42434
[1mStep[0m  [8/26], [94mLoss[0m : 2.24490
[1mStep[0m  [10/26], [94mLoss[0m : 2.16976
[1mStep[0m  [12/26], [94mLoss[0m : 2.51537
[1mStep[0m  [14/26], [94mLoss[0m : 2.31192
[1mStep[0m  [16/26], [94mLoss[0m : 2.23735
[1mStep[0m  [18/26], [94mLoss[0m : 2.29863
[1mStep[0m  [20/26], [94mLoss[0m : 2.52729
[1mStep[0m  [22/26], [94mLoss[0m : 2.36017
[1mStep[0m  [24/26], [94mLoss[0m : 2.18472

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.346
====================================

Phase 1 - Evaluation MAE:  2.3461170196533203
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.48614
[1mStep[0m  [2/26], [94mLoss[0m : 2.33094
[1mStep[0m  [4/26], [94mLoss[0m : 2.51486
[1mStep[0m  [6/26], [94mLoss[0m : 2.37211
[1mStep[0m  [8/26], [94mLoss[0m : 2.56933
[1mStep[0m  [10/26], [94mLoss[0m : 2.37284
[1mStep[0m  [12/26], [94mLoss[0m : 2.39856
[1mStep[0m  [14/26], [94mLoss[0m : 2.33934
[1mStep[0m  [16/26], [94mLoss[0m : 2.46316
[1mStep[0m  [18/26], [94mLoss[0m : 2.43893
[1mStep[0m  [20/26], [94mLoss[0m : 2.36052
[1mStep[0m  [22/26], [94mLoss[0m : 2.50963
[1mStep[0m  [24/26], [94mLoss[0m : 2.41909

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22061
[1mStep[0m  [2/26], [94mLoss[0m : 2.23668
[1mStep[0m  [4/26], [94mLoss[0m : 2.29726
[1mStep[0m  [6/26], [94mLoss[0m : 2.32655
[1mStep[0m  [8/26], [94mLoss[0m : 2.36507
[1mStep[0m  [10/26], [94mLoss[0m : 2.37951
[1mStep[0m  [12/26], [94mLoss[0m : 2.38169
[1mStep[0m  [14/26], [94mLoss[0m : 2.44555
[1mStep[0m  [16/26], [94mLoss[0m : 2.22398
[1mStep[0m  [18/26], [94mLoss[0m : 2.32120
[1mStep[0m  [20/26], [94mLoss[0m : 2.30476
[1mStep[0m  [22/26], [94mLoss[0m : 2.34954
[1mStep[0m  [24/26], [94mLoss[0m : 2.28925

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23707
[1mStep[0m  [2/26], [94mLoss[0m : 2.26401
[1mStep[0m  [4/26], [94mLoss[0m : 2.33503
[1mStep[0m  [6/26], [94mLoss[0m : 2.31647
[1mStep[0m  [8/26], [94mLoss[0m : 2.17076
[1mStep[0m  [10/26], [94mLoss[0m : 2.27025
[1mStep[0m  [12/26], [94mLoss[0m : 2.14038
[1mStep[0m  [14/26], [94mLoss[0m : 2.22280
[1mStep[0m  [16/26], [94mLoss[0m : 2.32593
[1mStep[0m  [18/26], [94mLoss[0m : 2.30834
[1mStep[0m  [20/26], [94mLoss[0m : 2.19888
[1mStep[0m  [22/26], [94mLoss[0m : 2.18922
[1mStep[0m  [24/26], [94mLoss[0m : 2.23260

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.238, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09780
[1mStep[0m  [2/26], [94mLoss[0m : 2.19472
[1mStep[0m  [4/26], [94mLoss[0m : 2.13304
[1mStep[0m  [6/26], [94mLoss[0m : 2.18522
[1mStep[0m  [8/26], [94mLoss[0m : 2.14332
[1mStep[0m  [10/26], [94mLoss[0m : 2.00198
[1mStep[0m  [12/26], [94mLoss[0m : 2.09307
[1mStep[0m  [14/26], [94mLoss[0m : 2.13173
[1mStep[0m  [16/26], [94mLoss[0m : 2.19477
[1mStep[0m  [18/26], [94mLoss[0m : 2.26180
[1mStep[0m  [20/26], [94mLoss[0m : 2.18408
[1mStep[0m  [22/26], [94mLoss[0m : 2.19387
[1mStep[0m  [24/26], [94mLoss[0m : 2.17994

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.06659
[1mStep[0m  [2/26], [94mLoss[0m : 2.12617
[1mStep[0m  [4/26], [94mLoss[0m : 2.13518
[1mStep[0m  [6/26], [94mLoss[0m : 1.96614
[1mStep[0m  [8/26], [94mLoss[0m : 2.10026
[1mStep[0m  [10/26], [94mLoss[0m : 2.04354
[1mStep[0m  [12/26], [94mLoss[0m : 2.19314
[1mStep[0m  [14/26], [94mLoss[0m : 2.06565
[1mStep[0m  [16/26], [94mLoss[0m : 1.95737
[1mStep[0m  [18/26], [94mLoss[0m : 2.16189
[1mStep[0m  [20/26], [94mLoss[0m : 2.16025
[1mStep[0m  [22/26], [94mLoss[0m : 2.15120
[1mStep[0m  [24/26], [94mLoss[0m : 2.12910

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07260
[1mStep[0m  [2/26], [94mLoss[0m : 2.00968
[1mStep[0m  [4/26], [94mLoss[0m : 2.10482
[1mStep[0m  [6/26], [94mLoss[0m : 1.93202
[1mStep[0m  [8/26], [94mLoss[0m : 1.99082
[1mStep[0m  [10/26], [94mLoss[0m : 2.08771
[1mStep[0m  [12/26], [94mLoss[0m : 1.97353
[1mStep[0m  [14/26], [94mLoss[0m : 2.05638
[1mStep[0m  [16/26], [94mLoss[0m : 1.92086
[1mStep[0m  [18/26], [94mLoss[0m : 2.02122
[1mStep[0m  [20/26], [94mLoss[0m : 2.13991
[1mStep[0m  [22/26], [94mLoss[0m : 1.94169
[1mStep[0m  [24/26], [94mLoss[0m : 2.02070

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88247
[1mStep[0m  [2/26], [94mLoss[0m : 1.97547
[1mStep[0m  [4/26], [94mLoss[0m : 1.93195
[1mStep[0m  [6/26], [94mLoss[0m : 1.90838
[1mStep[0m  [8/26], [94mLoss[0m : 1.96330
[1mStep[0m  [10/26], [94mLoss[0m : 1.94900
[1mStep[0m  [12/26], [94mLoss[0m : 1.78224
[1mStep[0m  [14/26], [94mLoss[0m : 2.01727
[1mStep[0m  [16/26], [94mLoss[0m : 1.82790
[1mStep[0m  [18/26], [94mLoss[0m : 2.04963
[1mStep[0m  [20/26], [94mLoss[0m : 2.02477
[1mStep[0m  [22/26], [94mLoss[0m : 1.87357
[1mStep[0m  [24/26], [94mLoss[0m : 1.98754

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.934, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.77542
[1mStep[0m  [2/26], [94mLoss[0m : 1.91731
[1mStep[0m  [4/26], [94mLoss[0m : 1.91382
[1mStep[0m  [6/26], [94mLoss[0m : 1.92069
[1mStep[0m  [8/26], [94mLoss[0m : 1.90734
[1mStep[0m  [10/26], [94mLoss[0m : 1.84929
[1mStep[0m  [12/26], [94mLoss[0m : 1.77198
[1mStep[0m  [14/26], [94mLoss[0m : 1.88714
[1mStep[0m  [16/26], [94mLoss[0m : 1.78488
[1mStep[0m  [18/26], [94mLoss[0m : 1.77410
[1mStep[0m  [20/26], [94mLoss[0m : 1.84637
[1mStep[0m  [22/26], [94mLoss[0m : 1.84416
[1mStep[0m  [24/26], [94mLoss[0m : 1.96954

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.875, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76438
[1mStep[0m  [2/26], [94mLoss[0m : 1.80689
[1mStep[0m  [4/26], [94mLoss[0m : 1.82335
[1mStep[0m  [6/26], [94mLoss[0m : 1.71174
[1mStep[0m  [8/26], [94mLoss[0m : 1.79702
[1mStep[0m  [10/26], [94mLoss[0m : 1.78195
[1mStep[0m  [12/26], [94mLoss[0m : 1.81747
[1mStep[0m  [14/26], [94mLoss[0m : 1.82158
[1mStep[0m  [16/26], [94mLoss[0m : 1.83883
[1mStep[0m  [18/26], [94mLoss[0m : 1.78480
[1mStep[0m  [20/26], [94mLoss[0m : 1.85979
[1mStep[0m  [22/26], [94mLoss[0m : 1.96312
[1mStep[0m  [24/26], [94mLoss[0m : 1.90480

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.83800
[1mStep[0m  [2/26], [94mLoss[0m : 1.72683
[1mStep[0m  [4/26], [94mLoss[0m : 1.73509
[1mStep[0m  [6/26], [94mLoss[0m : 1.78671
[1mStep[0m  [8/26], [94mLoss[0m : 1.79593
[1mStep[0m  [10/26], [94mLoss[0m : 1.77027
[1mStep[0m  [12/26], [94mLoss[0m : 1.75621
[1mStep[0m  [14/26], [94mLoss[0m : 1.79548
[1mStep[0m  [16/26], [94mLoss[0m : 1.75717
[1mStep[0m  [18/26], [94mLoss[0m : 1.85374
[1mStep[0m  [20/26], [94mLoss[0m : 1.82193
[1mStep[0m  [22/26], [94mLoss[0m : 1.75149
[1mStep[0m  [24/26], [94mLoss[0m : 1.94706

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.777, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82486
[1mStep[0m  [2/26], [94mLoss[0m : 1.70325
[1mStep[0m  [4/26], [94mLoss[0m : 1.68637
[1mStep[0m  [6/26], [94mLoss[0m : 1.59002
[1mStep[0m  [8/26], [94mLoss[0m : 1.70433
[1mStep[0m  [10/26], [94mLoss[0m : 1.87983
[1mStep[0m  [12/26], [94mLoss[0m : 1.77798
[1mStep[0m  [14/26], [94mLoss[0m : 1.75762
[1mStep[0m  [16/26], [94mLoss[0m : 1.98864
[1mStep[0m  [18/26], [94mLoss[0m : 1.84758
[1mStep[0m  [20/26], [94mLoss[0m : 1.80485
[1mStep[0m  [22/26], [94mLoss[0m : 1.77520
[1mStep[0m  [24/26], [94mLoss[0m : 1.78827

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84301
[1mStep[0m  [2/26], [94mLoss[0m : 1.66149
[1mStep[0m  [4/26], [94mLoss[0m : 1.73773
[1mStep[0m  [6/26], [94mLoss[0m : 1.78152
[1mStep[0m  [8/26], [94mLoss[0m : 1.66307
[1mStep[0m  [10/26], [94mLoss[0m : 1.63429
[1mStep[0m  [12/26], [94mLoss[0m : 1.69531
[1mStep[0m  [14/26], [94mLoss[0m : 1.61614
[1mStep[0m  [16/26], [94mLoss[0m : 1.63351
[1mStep[0m  [18/26], [94mLoss[0m : 1.73842
[1mStep[0m  [20/26], [94mLoss[0m : 1.86806
[1mStep[0m  [22/26], [94mLoss[0m : 1.71226
[1mStep[0m  [24/26], [94mLoss[0m : 1.73433

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54497
[1mStep[0m  [2/26], [94mLoss[0m : 1.75493
[1mStep[0m  [4/26], [94mLoss[0m : 1.43057
[1mStep[0m  [6/26], [94mLoss[0m : 1.55325
[1mStep[0m  [8/26], [94mLoss[0m : 1.58669
[1mStep[0m  [10/26], [94mLoss[0m : 1.72082
[1mStep[0m  [12/26], [94mLoss[0m : 1.59403
[1mStep[0m  [14/26], [94mLoss[0m : 1.63846
[1mStep[0m  [16/26], [94mLoss[0m : 1.72483
[1mStep[0m  [18/26], [94mLoss[0m : 1.62779
[1mStep[0m  [20/26], [94mLoss[0m : 1.76305
[1mStep[0m  [22/26], [94mLoss[0m : 1.64387
[1mStep[0m  [24/26], [94mLoss[0m : 1.66950

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.69069
[1mStep[0m  [2/26], [94mLoss[0m : 1.59895
[1mStep[0m  [4/26], [94mLoss[0m : 1.53183
[1mStep[0m  [6/26], [94mLoss[0m : 1.57206
[1mStep[0m  [8/26], [94mLoss[0m : 1.58540
[1mStep[0m  [10/26], [94mLoss[0m : 1.57820
[1mStep[0m  [12/26], [94mLoss[0m : 1.52839
[1mStep[0m  [14/26], [94mLoss[0m : 1.56285
[1mStep[0m  [16/26], [94mLoss[0m : 1.67126
[1mStep[0m  [18/26], [94mLoss[0m : 1.54304
[1mStep[0m  [20/26], [94mLoss[0m : 1.65841
[1mStep[0m  [22/26], [94mLoss[0m : 1.67802
[1mStep[0m  [24/26], [94mLoss[0m : 1.69823

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54383
[1mStep[0m  [2/26], [94mLoss[0m : 1.54719
[1mStep[0m  [4/26], [94mLoss[0m : 1.55566
[1mStep[0m  [6/26], [94mLoss[0m : 1.56837
[1mStep[0m  [8/26], [94mLoss[0m : 1.64535
[1mStep[0m  [10/26], [94mLoss[0m : 1.53062
[1mStep[0m  [12/26], [94mLoss[0m : 1.46517
[1mStep[0m  [14/26], [94mLoss[0m : 1.57000
[1mStep[0m  [16/26], [94mLoss[0m : 1.53449
[1mStep[0m  [18/26], [94mLoss[0m : 1.45053
[1mStep[0m  [20/26], [94mLoss[0m : 1.59392
[1mStep[0m  [22/26], [94mLoss[0m : 1.61437
[1mStep[0m  [24/26], [94mLoss[0m : 1.53326

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.565, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.56381
[1mStep[0m  [2/26], [94mLoss[0m : 1.47414
[1mStep[0m  [4/26], [94mLoss[0m : 1.49272
[1mStep[0m  [6/26], [94mLoss[0m : 1.45661
[1mStep[0m  [8/26], [94mLoss[0m : 1.50063
[1mStep[0m  [10/26], [94mLoss[0m : 1.44708
[1mStep[0m  [12/26], [94mLoss[0m : 1.65991
[1mStep[0m  [14/26], [94mLoss[0m : 1.64572
[1mStep[0m  [16/26], [94mLoss[0m : 1.60316
[1mStep[0m  [18/26], [94mLoss[0m : 1.66078
[1mStep[0m  [20/26], [94mLoss[0m : 1.50647
[1mStep[0m  [22/26], [94mLoss[0m : 1.54000
[1mStep[0m  [24/26], [94mLoss[0m : 1.55937

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.43430
[1mStep[0m  [2/26], [94mLoss[0m : 1.52296
[1mStep[0m  [4/26], [94mLoss[0m : 1.49677
[1mStep[0m  [6/26], [94mLoss[0m : 1.58546
[1mStep[0m  [8/26], [94mLoss[0m : 1.43114
[1mStep[0m  [10/26], [94mLoss[0m : 1.55622
[1mStep[0m  [12/26], [94mLoss[0m : 1.60226
[1mStep[0m  [14/26], [94mLoss[0m : 1.56424
[1mStep[0m  [16/26], [94mLoss[0m : 1.49349
[1mStep[0m  [18/26], [94mLoss[0m : 1.55667
[1mStep[0m  [20/26], [94mLoss[0m : 1.56671
[1mStep[0m  [22/26], [94mLoss[0m : 1.62787
[1mStep[0m  [24/26], [94mLoss[0m : 1.43254

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.46808
[1mStep[0m  [2/26], [94mLoss[0m : 1.43666
[1mStep[0m  [4/26], [94mLoss[0m : 1.56162
[1mStep[0m  [6/26], [94mLoss[0m : 1.41433
[1mStep[0m  [8/26], [94mLoss[0m : 1.48545
[1mStep[0m  [10/26], [94mLoss[0m : 1.50540
[1mStep[0m  [12/26], [94mLoss[0m : 1.51775
[1mStep[0m  [14/26], [94mLoss[0m : 1.44656
[1mStep[0m  [16/26], [94mLoss[0m : 1.55158
[1mStep[0m  [18/26], [94mLoss[0m : 1.54978
[1mStep[0m  [20/26], [94mLoss[0m : 1.41291
[1mStep[0m  [22/26], [94mLoss[0m : 1.52955
[1mStep[0m  [24/26], [94mLoss[0m : 1.59516

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.43104
[1mStep[0m  [2/26], [94mLoss[0m : 1.51965
[1mStep[0m  [4/26], [94mLoss[0m : 1.39073
[1mStep[0m  [6/26], [94mLoss[0m : 1.43893
[1mStep[0m  [8/26], [94mLoss[0m : 1.34899
[1mStep[0m  [10/26], [94mLoss[0m : 1.47183
[1mStep[0m  [12/26], [94mLoss[0m : 1.37151
[1mStep[0m  [14/26], [94mLoss[0m : 1.42648
[1mStep[0m  [16/26], [94mLoss[0m : 1.48302
[1mStep[0m  [18/26], [94mLoss[0m : 1.41318
[1mStep[0m  [20/26], [94mLoss[0m : 1.48143
[1mStep[0m  [22/26], [94mLoss[0m : 1.49918
[1mStep[0m  [24/26], [94mLoss[0m : 1.43381

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.450, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.42603
[1mStep[0m  [2/26], [94mLoss[0m : 1.41069
[1mStep[0m  [4/26], [94mLoss[0m : 1.48357
[1mStep[0m  [6/26], [94mLoss[0m : 1.36713
[1mStep[0m  [8/26], [94mLoss[0m : 1.46994
[1mStep[0m  [10/26], [94mLoss[0m : 1.54838
[1mStep[0m  [12/26], [94mLoss[0m : 1.43948
[1mStep[0m  [14/26], [94mLoss[0m : 1.44890
[1mStep[0m  [16/26], [94mLoss[0m : 1.42734
[1mStep[0m  [18/26], [94mLoss[0m : 1.38757
[1mStep[0m  [20/26], [94mLoss[0m : 1.51331
[1mStep[0m  [22/26], [94mLoss[0m : 1.48425
[1mStep[0m  [24/26], [94mLoss[0m : 1.39330

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.502, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.42333
[1mStep[0m  [2/26], [94mLoss[0m : 1.33889
[1mStep[0m  [4/26], [94mLoss[0m : 1.38644
[1mStep[0m  [6/26], [94mLoss[0m : 1.35724
[1mStep[0m  [8/26], [94mLoss[0m : 1.39368
[1mStep[0m  [10/26], [94mLoss[0m : 1.38355
[1mStep[0m  [12/26], [94mLoss[0m : 1.42597
[1mStep[0m  [14/26], [94mLoss[0m : 1.28338
[1mStep[0m  [16/26], [94mLoss[0m : 1.43249
[1mStep[0m  [18/26], [94mLoss[0m : 1.40054
[1mStep[0m  [20/26], [94mLoss[0m : 1.44197
[1mStep[0m  [22/26], [94mLoss[0m : 1.49483
[1mStep[0m  [24/26], [94mLoss[0m : 1.41879

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.404, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.33698
[1mStep[0m  [2/26], [94mLoss[0m : 1.38259
[1mStep[0m  [4/26], [94mLoss[0m : 1.34097
[1mStep[0m  [6/26], [94mLoss[0m : 1.32179
[1mStep[0m  [8/26], [94mLoss[0m : 1.40271
[1mStep[0m  [10/26], [94mLoss[0m : 1.41050
[1mStep[0m  [12/26], [94mLoss[0m : 1.34033
[1mStep[0m  [14/26], [94mLoss[0m : 1.39412
[1mStep[0m  [16/26], [94mLoss[0m : 1.35761
[1mStep[0m  [18/26], [94mLoss[0m : 1.41255
[1mStep[0m  [20/26], [94mLoss[0m : 1.41760
[1mStep[0m  [22/26], [94mLoss[0m : 1.40392
[1mStep[0m  [24/26], [94mLoss[0m : 1.35427

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.374, [92mTest[0m: 2.468, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.31211
[1mStep[0m  [2/26], [94mLoss[0m : 1.39327
[1mStep[0m  [4/26], [94mLoss[0m : 1.24259
[1mStep[0m  [6/26], [94mLoss[0m : 1.43545
[1mStep[0m  [8/26], [94mLoss[0m : 1.43262
[1mStep[0m  [10/26], [94mLoss[0m : 1.26216
[1mStep[0m  [12/26], [94mLoss[0m : 1.32447
[1mStep[0m  [14/26], [94mLoss[0m : 1.33975
[1mStep[0m  [16/26], [94mLoss[0m : 1.31605
[1mStep[0m  [18/26], [94mLoss[0m : 1.30625
[1mStep[0m  [20/26], [94mLoss[0m : 1.34227
[1mStep[0m  [22/26], [94mLoss[0m : 1.32604
[1mStep[0m  [24/26], [94mLoss[0m : 1.48554

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.355, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.493
====================================

Phase 2 - Evaluation MAE:  2.493414732126089
MAE score P1       2.346117
MAE score P2       2.493415
loss               1.355403
learning_rate          0.01
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay          0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.59193
[1mStep[0m  [2/26], [94mLoss[0m : 9.85763
[1mStep[0m  [4/26], [94mLoss[0m : 9.11711
[1mStep[0m  [6/26], [94mLoss[0m : 8.80407
[1mStep[0m  [8/26], [94mLoss[0m : 8.05949
[1mStep[0m  [10/26], [94mLoss[0m : 7.83447
[1mStep[0m  [12/26], [94mLoss[0m : 7.01425
[1mStep[0m  [14/26], [94mLoss[0m : 6.32269
[1mStep[0m  [16/26], [94mLoss[0m : 5.70665
[1mStep[0m  [18/26], [94mLoss[0m : 5.36509
[1mStep[0m  [20/26], [94mLoss[0m : 4.47210
[1mStep[0m  [22/26], [94mLoss[0m : 4.41667
[1mStep[0m  [24/26], [94mLoss[0m : 4.26029

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.934, [92mTest[0m: 10.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.81075
[1mStep[0m  [2/26], [94mLoss[0m : 3.38725
[1mStep[0m  [4/26], [94mLoss[0m : 3.12994
[1mStep[0m  [6/26], [94mLoss[0m : 3.09284
[1mStep[0m  [8/26], [94mLoss[0m : 2.86463
[1mStep[0m  [10/26], [94mLoss[0m : 3.12124
[1mStep[0m  [12/26], [94mLoss[0m : 2.77224
[1mStep[0m  [14/26], [94mLoss[0m : 2.98704
[1mStep[0m  [16/26], [94mLoss[0m : 2.67482
[1mStep[0m  [18/26], [94mLoss[0m : 2.72072
[1mStep[0m  [20/26], [94mLoss[0m : 2.69975
[1mStep[0m  [22/26], [94mLoss[0m : 2.65377
[1mStep[0m  [24/26], [94mLoss[0m : 2.61436

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.963, [92mTest[0m: 3.698, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70040
[1mStep[0m  [2/26], [94mLoss[0m : 2.73305
[1mStep[0m  [4/26], [94mLoss[0m : 2.43287
[1mStep[0m  [6/26], [94mLoss[0m : 2.51450
[1mStep[0m  [8/26], [94mLoss[0m : 2.56168
[1mStep[0m  [10/26], [94mLoss[0m : 2.51579
[1mStep[0m  [12/26], [94mLoss[0m : 2.58054
[1mStep[0m  [14/26], [94mLoss[0m : 2.59287
[1mStep[0m  [16/26], [94mLoss[0m : 2.74109
[1mStep[0m  [18/26], [94mLoss[0m : 2.53572
[1mStep[0m  [20/26], [94mLoss[0m : 2.66981
[1mStep[0m  [22/26], [94mLoss[0m : 2.53757
[1mStep[0m  [24/26], [94mLoss[0m : 2.59232

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69671
[1mStep[0m  [2/26], [94mLoss[0m : 2.50497
[1mStep[0m  [4/26], [94mLoss[0m : 2.55857
[1mStep[0m  [6/26], [94mLoss[0m : 2.52167
[1mStep[0m  [8/26], [94mLoss[0m : 2.53264
[1mStep[0m  [10/26], [94mLoss[0m : 2.36850
[1mStep[0m  [12/26], [94mLoss[0m : 2.55494
[1mStep[0m  [14/26], [94mLoss[0m : 2.61121
[1mStep[0m  [16/26], [94mLoss[0m : 2.59324
[1mStep[0m  [18/26], [94mLoss[0m : 2.52015
[1mStep[0m  [20/26], [94mLoss[0m : 2.52891
[1mStep[0m  [22/26], [94mLoss[0m : 2.49630
[1mStep[0m  [24/26], [94mLoss[0m : 2.52565

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48359
[1mStep[0m  [2/26], [94mLoss[0m : 2.39741
[1mStep[0m  [4/26], [94mLoss[0m : 2.39659
[1mStep[0m  [6/26], [94mLoss[0m : 2.45134
[1mStep[0m  [8/26], [94mLoss[0m : 2.49901
[1mStep[0m  [10/26], [94mLoss[0m : 2.59619
[1mStep[0m  [12/26], [94mLoss[0m : 2.65344
[1mStep[0m  [14/26], [94mLoss[0m : 2.46513
[1mStep[0m  [16/26], [94mLoss[0m : 2.59500
[1mStep[0m  [18/26], [94mLoss[0m : 2.58790
[1mStep[0m  [20/26], [94mLoss[0m : 2.57806
[1mStep[0m  [22/26], [94mLoss[0m : 2.58130
[1mStep[0m  [24/26], [94mLoss[0m : 2.51462

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46702
[1mStep[0m  [2/26], [94mLoss[0m : 2.42548
[1mStep[0m  [4/26], [94mLoss[0m : 2.47776
[1mStep[0m  [6/26], [94mLoss[0m : 2.50180
[1mStep[0m  [8/26], [94mLoss[0m : 2.72474
[1mStep[0m  [10/26], [94mLoss[0m : 2.41516
[1mStep[0m  [12/26], [94mLoss[0m : 2.43416
[1mStep[0m  [14/26], [94mLoss[0m : 2.54673
[1mStep[0m  [16/26], [94mLoss[0m : 2.51503
[1mStep[0m  [18/26], [94mLoss[0m : 2.48263
[1mStep[0m  [20/26], [94mLoss[0m : 2.49864
[1mStep[0m  [22/26], [94mLoss[0m : 2.36477
[1mStep[0m  [24/26], [94mLoss[0m : 2.59812

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50249
[1mStep[0m  [2/26], [94mLoss[0m : 2.49284
[1mStep[0m  [4/26], [94mLoss[0m : 2.54599
[1mStep[0m  [6/26], [94mLoss[0m : 2.59667
[1mStep[0m  [8/26], [94mLoss[0m : 2.66431
[1mStep[0m  [10/26], [94mLoss[0m : 2.55472
[1mStep[0m  [12/26], [94mLoss[0m : 2.42578
[1mStep[0m  [14/26], [94mLoss[0m : 2.49153
[1mStep[0m  [16/26], [94mLoss[0m : 2.40285
[1mStep[0m  [18/26], [94mLoss[0m : 2.33929
[1mStep[0m  [20/26], [94mLoss[0m : 2.55939
[1mStep[0m  [22/26], [94mLoss[0m : 2.43618
[1mStep[0m  [24/26], [94mLoss[0m : 2.49618

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46385
[1mStep[0m  [2/26], [94mLoss[0m : 2.50606
[1mStep[0m  [4/26], [94mLoss[0m : 2.54956
[1mStep[0m  [6/26], [94mLoss[0m : 2.60508
[1mStep[0m  [8/26], [94mLoss[0m : 2.52091
[1mStep[0m  [10/26], [94mLoss[0m : 2.45157
[1mStep[0m  [12/26], [94mLoss[0m : 2.50000
[1mStep[0m  [14/26], [94mLoss[0m : 2.66500
[1mStep[0m  [16/26], [94mLoss[0m : 2.46186
[1mStep[0m  [18/26], [94mLoss[0m : 2.37366
[1mStep[0m  [20/26], [94mLoss[0m : 2.64062
[1mStep[0m  [22/26], [94mLoss[0m : 2.60680
[1mStep[0m  [24/26], [94mLoss[0m : 2.58717

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35367
[1mStep[0m  [2/26], [94mLoss[0m : 2.64475
[1mStep[0m  [4/26], [94mLoss[0m : 2.54490
[1mStep[0m  [6/26], [94mLoss[0m : 2.60774
[1mStep[0m  [8/26], [94mLoss[0m : 2.46601
[1mStep[0m  [10/26], [94mLoss[0m : 2.36585
[1mStep[0m  [12/26], [94mLoss[0m : 2.38628
[1mStep[0m  [14/26], [94mLoss[0m : 2.53041
[1mStep[0m  [16/26], [94mLoss[0m : 2.40693
[1mStep[0m  [18/26], [94mLoss[0m : 2.57305
[1mStep[0m  [20/26], [94mLoss[0m : 2.60036
[1mStep[0m  [22/26], [94mLoss[0m : 2.53329
[1mStep[0m  [24/26], [94mLoss[0m : 2.43727

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42141
[1mStep[0m  [2/26], [94mLoss[0m : 2.57762
[1mStep[0m  [4/26], [94mLoss[0m : 2.45950
[1mStep[0m  [6/26], [94mLoss[0m : 2.39550
[1mStep[0m  [8/26], [94mLoss[0m : 2.35420
[1mStep[0m  [10/26], [94mLoss[0m : 2.63825
[1mStep[0m  [12/26], [94mLoss[0m : 2.44795
[1mStep[0m  [14/26], [94mLoss[0m : 2.53089
[1mStep[0m  [16/26], [94mLoss[0m : 2.41111
[1mStep[0m  [18/26], [94mLoss[0m : 2.46049
[1mStep[0m  [20/26], [94mLoss[0m : 2.47067
[1mStep[0m  [22/26], [94mLoss[0m : 2.64515
[1mStep[0m  [24/26], [94mLoss[0m : 2.53364

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38946
[1mStep[0m  [2/26], [94mLoss[0m : 2.58429
[1mStep[0m  [4/26], [94mLoss[0m : 2.55042
[1mStep[0m  [6/26], [94mLoss[0m : 2.52155
[1mStep[0m  [8/26], [94mLoss[0m : 2.30282
[1mStep[0m  [10/26], [94mLoss[0m : 2.54159
[1mStep[0m  [12/26], [94mLoss[0m : 2.36977
[1mStep[0m  [14/26], [94mLoss[0m : 2.60518
[1mStep[0m  [16/26], [94mLoss[0m : 2.62061
[1mStep[0m  [18/26], [94mLoss[0m : 2.45742
[1mStep[0m  [20/26], [94mLoss[0m : 2.42042
[1mStep[0m  [22/26], [94mLoss[0m : 2.48819
[1mStep[0m  [24/26], [94mLoss[0m : 2.43231

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61175
[1mStep[0m  [2/26], [94mLoss[0m : 2.44523
[1mStep[0m  [4/26], [94mLoss[0m : 2.46120
[1mStep[0m  [6/26], [94mLoss[0m : 2.44182
[1mStep[0m  [8/26], [94mLoss[0m : 2.45149
[1mStep[0m  [10/26], [94mLoss[0m : 2.64127
[1mStep[0m  [12/26], [94mLoss[0m : 2.45245
[1mStep[0m  [14/26], [94mLoss[0m : 2.42234
[1mStep[0m  [16/26], [94mLoss[0m : 2.38893
[1mStep[0m  [18/26], [94mLoss[0m : 2.35343
[1mStep[0m  [20/26], [94mLoss[0m : 2.52832
[1mStep[0m  [22/26], [94mLoss[0m : 2.53123
[1mStep[0m  [24/26], [94mLoss[0m : 2.53720

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44925
[1mStep[0m  [2/26], [94mLoss[0m : 2.55367
[1mStep[0m  [4/26], [94mLoss[0m : 2.35363
[1mStep[0m  [6/26], [94mLoss[0m : 2.49736
[1mStep[0m  [8/26], [94mLoss[0m : 2.50022
[1mStep[0m  [10/26], [94mLoss[0m : 2.44124
[1mStep[0m  [12/26], [94mLoss[0m : 2.59357
[1mStep[0m  [14/26], [94mLoss[0m : 2.38998
[1mStep[0m  [16/26], [94mLoss[0m : 2.34983
[1mStep[0m  [18/26], [94mLoss[0m : 2.55690
[1mStep[0m  [20/26], [94mLoss[0m : 2.57973
[1mStep[0m  [22/26], [94mLoss[0m : 2.50788
[1mStep[0m  [24/26], [94mLoss[0m : 2.54696

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55747
[1mStep[0m  [2/26], [94mLoss[0m : 2.46765
[1mStep[0m  [4/26], [94mLoss[0m : 2.62570
[1mStep[0m  [6/26], [94mLoss[0m : 2.36478
[1mStep[0m  [8/26], [94mLoss[0m : 2.41453
[1mStep[0m  [10/26], [94mLoss[0m : 2.59010
[1mStep[0m  [12/26], [94mLoss[0m : 2.43124
[1mStep[0m  [14/26], [94mLoss[0m : 2.49124
[1mStep[0m  [16/26], [94mLoss[0m : 2.52383
[1mStep[0m  [18/26], [94mLoss[0m : 2.37911
[1mStep[0m  [20/26], [94mLoss[0m : 2.52285
[1mStep[0m  [22/26], [94mLoss[0m : 2.35359
[1mStep[0m  [24/26], [94mLoss[0m : 2.47730

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27606
[1mStep[0m  [2/26], [94mLoss[0m : 2.49675
[1mStep[0m  [4/26], [94mLoss[0m : 2.56522
[1mStep[0m  [6/26], [94mLoss[0m : 2.49074
[1mStep[0m  [8/26], [94mLoss[0m : 2.43437
[1mStep[0m  [10/26], [94mLoss[0m : 2.58467
[1mStep[0m  [12/26], [94mLoss[0m : 2.40689
[1mStep[0m  [14/26], [94mLoss[0m : 2.67554
[1mStep[0m  [16/26], [94mLoss[0m : 2.50642
[1mStep[0m  [18/26], [94mLoss[0m : 2.44102
[1mStep[0m  [20/26], [94mLoss[0m : 2.54563
[1mStep[0m  [22/26], [94mLoss[0m : 2.42494
[1mStep[0m  [24/26], [94mLoss[0m : 2.54152

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32651
[1mStep[0m  [2/26], [94mLoss[0m : 2.58241
[1mStep[0m  [4/26], [94mLoss[0m : 2.40417
[1mStep[0m  [6/26], [94mLoss[0m : 2.59430
[1mStep[0m  [8/26], [94mLoss[0m : 2.46643
[1mStep[0m  [10/26], [94mLoss[0m : 2.45566
[1mStep[0m  [12/26], [94mLoss[0m : 2.49262
[1mStep[0m  [14/26], [94mLoss[0m : 2.44580
[1mStep[0m  [16/26], [94mLoss[0m : 2.49546
[1mStep[0m  [18/26], [94mLoss[0m : 2.46919
[1mStep[0m  [20/26], [94mLoss[0m : 2.52118
[1mStep[0m  [22/26], [94mLoss[0m : 2.42076
[1mStep[0m  [24/26], [94mLoss[0m : 2.54902

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68471
[1mStep[0m  [2/26], [94mLoss[0m : 2.49839
[1mStep[0m  [4/26], [94mLoss[0m : 2.62981
[1mStep[0m  [6/26], [94mLoss[0m : 2.37857
[1mStep[0m  [8/26], [94mLoss[0m : 2.52024
[1mStep[0m  [10/26], [94mLoss[0m : 2.46863
[1mStep[0m  [12/26], [94mLoss[0m : 2.41802
[1mStep[0m  [14/26], [94mLoss[0m : 2.49642
[1mStep[0m  [16/26], [94mLoss[0m : 2.57666
[1mStep[0m  [18/26], [94mLoss[0m : 2.43979
[1mStep[0m  [20/26], [94mLoss[0m : 2.41370
[1mStep[0m  [22/26], [94mLoss[0m : 2.43536
[1mStep[0m  [24/26], [94mLoss[0m : 2.40674

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51089
[1mStep[0m  [2/26], [94mLoss[0m : 2.37107
[1mStep[0m  [4/26], [94mLoss[0m : 2.38461
[1mStep[0m  [6/26], [94mLoss[0m : 2.50361
[1mStep[0m  [8/26], [94mLoss[0m : 2.36933
[1mStep[0m  [10/26], [94mLoss[0m : 2.30672
[1mStep[0m  [12/26], [94mLoss[0m : 2.53310
[1mStep[0m  [14/26], [94mLoss[0m : 2.61747
[1mStep[0m  [16/26], [94mLoss[0m : 2.38874
[1mStep[0m  [18/26], [94mLoss[0m : 2.53888
[1mStep[0m  [20/26], [94mLoss[0m : 2.65727
[1mStep[0m  [22/26], [94mLoss[0m : 2.49227
[1mStep[0m  [24/26], [94mLoss[0m : 2.40555

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46924
[1mStep[0m  [2/26], [94mLoss[0m : 2.40392
[1mStep[0m  [4/26], [94mLoss[0m : 2.47879
[1mStep[0m  [6/26], [94mLoss[0m : 2.48770
[1mStep[0m  [8/26], [94mLoss[0m : 2.52010
[1mStep[0m  [10/26], [94mLoss[0m : 2.58150
[1mStep[0m  [12/26], [94mLoss[0m : 2.48590
[1mStep[0m  [14/26], [94mLoss[0m : 2.46158
[1mStep[0m  [16/26], [94mLoss[0m : 2.45151
[1mStep[0m  [18/26], [94mLoss[0m : 2.40460
[1mStep[0m  [20/26], [94mLoss[0m : 2.52745
[1mStep[0m  [22/26], [94mLoss[0m : 2.65089
[1mStep[0m  [24/26], [94mLoss[0m : 2.43414

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31455
[1mStep[0m  [2/26], [94mLoss[0m : 2.51192
[1mStep[0m  [4/26], [94mLoss[0m : 2.51702
[1mStep[0m  [6/26], [94mLoss[0m : 2.47298
[1mStep[0m  [8/26], [94mLoss[0m : 2.41218
[1mStep[0m  [10/26], [94mLoss[0m : 2.58174
[1mStep[0m  [12/26], [94mLoss[0m : 2.51600
[1mStep[0m  [14/26], [94mLoss[0m : 2.29238
[1mStep[0m  [16/26], [94mLoss[0m : 2.61413
[1mStep[0m  [18/26], [94mLoss[0m : 2.36646
[1mStep[0m  [20/26], [94mLoss[0m : 2.40130
[1mStep[0m  [22/26], [94mLoss[0m : 2.48621
[1mStep[0m  [24/26], [94mLoss[0m : 2.50803

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.424, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57348
[1mStep[0m  [2/26], [94mLoss[0m : 2.40853
[1mStep[0m  [4/26], [94mLoss[0m : 2.46002
[1mStep[0m  [6/26], [94mLoss[0m : 2.52524
[1mStep[0m  [8/26], [94mLoss[0m : 2.48405
[1mStep[0m  [10/26], [94mLoss[0m : 2.36626
[1mStep[0m  [12/26], [94mLoss[0m : 2.46632
[1mStep[0m  [14/26], [94mLoss[0m : 2.53856
[1mStep[0m  [16/26], [94mLoss[0m : 2.45400
[1mStep[0m  [18/26], [94mLoss[0m : 2.54697
[1mStep[0m  [20/26], [94mLoss[0m : 2.45982
[1mStep[0m  [22/26], [94mLoss[0m : 2.57721
[1mStep[0m  [24/26], [94mLoss[0m : 2.44734

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.419, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60354
[1mStep[0m  [2/26], [94mLoss[0m : 2.68109
[1mStep[0m  [4/26], [94mLoss[0m : 2.38455
[1mStep[0m  [6/26], [94mLoss[0m : 2.66845
[1mStep[0m  [8/26], [94mLoss[0m : 2.55180
[1mStep[0m  [10/26], [94mLoss[0m : 2.47434
[1mStep[0m  [12/26], [94mLoss[0m : 2.46939
[1mStep[0m  [14/26], [94mLoss[0m : 2.36747
[1mStep[0m  [16/26], [94mLoss[0m : 2.47215
[1mStep[0m  [18/26], [94mLoss[0m : 2.34107
[1mStep[0m  [20/26], [94mLoss[0m : 2.41671
[1mStep[0m  [22/26], [94mLoss[0m : 2.49191
[1mStep[0m  [24/26], [94mLoss[0m : 2.38244

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.422, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44306
[1mStep[0m  [2/26], [94mLoss[0m : 2.59562
[1mStep[0m  [4/26], [94mLoss[0m : 2.43779
[1mStep[0m  [6/26], [94mLoss[0m : 2.43129
[1mStep[0m  [8/26], [94mLoss[0m : 2.38401
[1mStep[0m  [10/26], [94mLoss[0m : 2.73983
[1mStep[0m  [12/26], [94mLoss[0m : 2.45534
[1mStep[0m  [14/26], [94mLoss[0m : 2.37411
[1mStep[0m  [16/26], [94mLoss[0m : 2.56284
[1mStep[0m  [18/26], [94mLoss[0m : 2.61166
[1mStep[0m  [20/26], [94mLoss[0m : 2.48926
[1mStep[0m  [22/26], [94mLoss[0m : 2.33064
[1mStep[0m  [24/26], [94mLoss[0m : 2.47113

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.416, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44125
[1mStep[0m  [2/26], [94mLoss[0m : 2.44731
[1mStep[0m  [4/26], [94mLoss[0m : 2.58927
[1mStep[0m  [6/26], [94mLoss[0m : 2.48782
[1mStep[0m  [8/26], [94mLoss[0m : 2.51939
[1mStep[0m  [10/26], [94mLoss[0m : 2.43367
[1mStep[0m  [12/26], [94mLoss[0m : 2.49085
[1mStep[0m  [14/26], [94mLoss[0m : 2.45212
[1mStep[0m  [16/26], [94mLoss[0m : 2.56048
[1mStep[0m  [18/26], [94mLoss[0m : 2.43685
[1mStep[0m  [20/26], [94mLoss[0m : 2.41217
[1mStep[0m  [22/26], [94mLoss[0m : 2.59213
[1mStep[0m  [24/26], [94mLoss[0m : 2.58207

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55119
[1mStep[0m  [2/26], [94mLoss[0m : 2.55840
[1mStep[0m  [4/26], [94mLoss[0m : 2.41157
[1mStep[0m  [6/26], [94mLoss[0m : 2.53929
[1mStep[0m  [8/26], [94mLoss[0m : 2.37824
[1mStep[0m  [10/26], [94mLoss[0m : 2.39862
[1mStep[0m  [12/26], [94mLoss[0m : 2.42620
[1mStep[0m  [14/26], [94mLoss[0m : 2.50361
[1mStep[0m  [16/26], [94mLoss[0m : 2.48838
[1mStep[0m  [18/26], [94mLoss[0m : 2.57380
[1mStep[0m  [20/26], [94mLoss[0m : 2.59189
[1mStep[0m  [22/26], [94mLoss[0m : 2.33093
[1mStep[0m  [24/26], [94mLoss[0m : 2.40712

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.419, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60782
[1mStep[0m  [2/26], [94mLoss[0m : 2.44823
[1mStep[0m  [4/26], [94mLoss[0m : 2.40566
[1mStep[0m  [6/26], [94mLoss[0m : 2.45647
[1mStep[0m  [8/26], [94mLoss[0m : 2.48173
[1mStep[0m  [10/26], [94mLoss[0m : 2.49595
[1mStep[0m  [12/26], [94mLoss[0m : 2.65750
[1mStep[0m  [14/26], [94mLoss[0m : 2.36186
[1mStep[0m  [16/26], [94mLoss[0m : 2.38676
[1mStep[0m  [18/26], [94mLoss[0m : 2.48893
[1mStep[0m  [20/26], [94mLoss[0m : 2.46354
[1mStep[0m  [22/26], [94mLoss[0m : 2.56358
[1mStep[0m  [24/26], [94mLoss[0m : 2.55287

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57155
[1mStep[0m  [2/26], [94mLoss[0m : 2.49682
[1mStep[0m  [4/26], [94mLoss[0m : 2.40562
[1mStep[0m  [6/26], [94mLoss[0m : 2.35356
[1mStep[0m  [8/26], [94mLoss[0m : 2.40621
[1mStep[0m  [10/26], [94mLoss[0m : 2.41311
[1mStep[0m  [12/26], [94mLoss[0m : 2.49256
[1mStep[0m  [14/26], [94mLoss[0m : 2.66520
[1mStep[0m  [16/26], [94mLoss[0m : 2.47551
[1mStep[0m  [18/26], [94mLoss[0m : 2.39297
[1mStep[0m  [20/26], [94mLoss[0m : 2.49377
[1mStep[0m  [22/26], [94mLoss[0m : 2.30268
[1mStep[0m  [24/26], [94mLoss[0m : 2.46201

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.412, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27700
[1mStep[0m  [2/26], [94mLoss[0m : 2.48281
[1mStep[0m  [4/26], [94mLoss[0m : 2.51027
[1mStep[0m  [6/26], [94mLoss[0m : 2.72113
[1mStep[0m  [8/26], [94mLoss[0m : 2.43363
[1mStep[0m  [10/26], [94mLoss[0m : 2.40352
[1mStep[0m  [12/26], [94mLoss[0m : 2.40724
[1mStep[0m  [14/26], [94mLoss[0m : 2.39484
[1mStep[0m  [16/26], [94mLoss[0m : 2.39591
[1mStep[0m  [18/26], [94mLoss[0m : 2.43233
[1mStep[0m  [20/26], [94mLoss[0m : 2.44238
[1mStep[0m  [22/26], [94mLoss[0m : 2.45625
[1mStep[0m  [24/26], [94mLoss[0m : 2.52282

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.412, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49472
[1mStep[0m  [2/26], [94mLoss[0m : 2.54446
[1mStep[0m  [4/26], [94mLoss[0m : 2.49436
[1mStep[0m  [6/26], [94mLoss[0m : 2.42795
[1mStep[0m  [8/26], [94mLoss[0m : 2.46642
[1mStep[0m  [10/26], [94mLoss[0m : 2.45479
[1mStep[0m  [12/26], [94mLoss[0m : 2.52035
[1mStep[0m  [14/26], [94mLoss[0m : 2.41202
[1mStep[0m  [16/26], [94mLoss[0m : 2.68581
[1mStep[0m  [18/26], [94mLoss[0m : 2.47343
[1mStep[0m  [20/26], [94mLoss[0m : 2.49520
[1mStep[0m  [22/26], [94mLoss[0m : 2.53144
[1mStep[0m  [24/26], [94mLoss[0m : 2.36669

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.417, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52556
[1mStep[0m  [2/26], [94mLoss[0m : 2.46263
[1mStep[0m  [4/26], [94mLoss[0m : 2.50903
[1mStep[0m  [6/26], [94mLoss[0m : 2.36873
[1mStep[0m  [8/26], [94mLoss[0m : 2.49181
[1mStep[0m  [10/26], [94mLoss[0m : 2.38579
[1mStep[0m  [12/26], [94mLoss[0m : 2.53603
[1mStep[0m  [14/26], [94mLoss[0m : 2.49670
[1mStep[0m  [16/26], [94mLoss[0m : 2.49433
[1mStep[0m  [18/26], [94mLoss[0m : 2.67210
[1mStep[0m  [20/26], [94mLoss[0m : 2.35052
[1mStep[0m  [22/26], [94mLoss[0m : 2.47711
[1mStep[0m  [24/26], [94mLoss[0m : 2.50755

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.410, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.400
====================================

Phase 1 - Evaluation MAE:  2.399866525943463
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 2.54039
[1mStep[0m  [2/26], [94mLoss[0m : 2.49113
[1mStep[0m  [4/26], [94mLoss[0m : 2.55430
[1mStep[0m  [6/26], [94mLoss[0m : 2.49990
[1mStep[0m  [8/26], [94mLoss[0m : 2.41813
[1mStep[0m  [10/26], [94mLoss[0m : 2.52865
[1mStep[0m  [12/26], [94mLoss[0m : 2.54680
[1mStep[0m  [14/26], [94mLoss[0m : 2.38065
[1mStep[0m  [16/26], [94mLoss[0m : 2.55345
[1mStep[0m  [18/26], [94mLoss[0m : 2.49946
[1mStep[0m  [20/26], [94mLoss[0m : 2.37029
[1mStep[0m  [22/26], [94mLoss[0m : 2.63060
[1mStep[0m  [24/26], [94mLoss[0m : 2.44563

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44906
[1mStep[0m  [2/26], [94mLoss[0m : 2.44682
[1mStep[0m  [4/26], [94mLoss[0m : 2.46533
[1mStep[0m  [6/26], [94mLoss[0m : 2.40342
[1mStep[0m  [8/26], [94mLoss[0m : 2.47158
[1mStep[0m  [10/26], [94mLoss[0m : 2.46243
[1mStep[0m  [12/26], [94mLoss[0m : 2.54116
[1mStep[0m  [14/26], [94mLoss[0m : 2.31976
[1mStep[0m  [16/26], [94mLoss[0m : 2.42358
[1mStep[0m  [18/26], [94mLoss[0m : 2.46362
[1mStep[0m  [20/26], [94mLoss[0m : 2.54876
[1mStep[0m  [22/26], [94mLoss[0m : 2.52076
[1mStep[0m  [24/26], [94mLoss[0m : 2.54837

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39237
[1mStep[0m  [2/26], [94mLoss[0m : 2.42882
[1mStep[0m  [4/26], [94mLoss[0m : 2.53470
[1mStep[0m  [6/26], [94mLoss[0m : 2.41701
[1mStep[0m  [8/26], [94mLoss[0m : 2.48257
[1mStep[0m  [10/26], [94mLoss[0m : 2.49862
[1mStep[0m  [12/26], [94mLoss[0m : 2.50317
[1mStep[0m  [14/26], [94mLoss[0m : 2.42452
[1mStep[0m  [16/26], [94mLoss[0m : 2.29273
[1mStep[0m  [18/26], [94mLoss[0m : 2.48259
[1mStep[0m  [20/26], [94mLoss[0m : 2.39280
[1mStep[0m  [22/26], [94mLoss[0m : 2.44415
[1mStep[0m  [24/26], [94mLoss[0m : 2.43552

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53818
[1mStep[0m  [2/26], [94mLoss[0m : 2.39777
[1mStep[0m  [4/26], [94mLoss[0m : 2.57459
[1mStep[0m  [6/26], [94mLoss[0m : 2.36120
[1mStep[0m  [8/26], [94mLoss[0m : 2.39191
[1mStep[0m  [10/26], [94mLoss[0m : 2.45101
[1mStep[0m  [12/26], [94mLoss[0m : 2.25225
[1mStep[0m  [14/26], [94mLoss[0m : 2.24060
[1mStep[0m  [16/26], [94mLoss[0m : 2.62754
[1mStep[0m  [18/26], [94mLoss[0m : 2.42000
[1mStep[0m  [20/26], [94mLoss[0m : 2.43979
[1mStep[0m  [22/26], [94mLoss[0m : 2.46454
[1mStep[0m  [24/26], [94mLoss[0m : 2.30590

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38879
[1mStep[0m  [2/26], [94mLoss[0m : 2.34980
[1mStep[0m  [4/26], [94mLoss[0m : 2.40714
[1mStep[0m  [6/26], [94mLoss[0m : 2.38154
[1mStep[0m  [8/26], [94mLoss[0m : 2.28894
[1mStep[0m  [10/26], [94mLoss[0m : 2.50146
[1mStep[0m  [12/26], [94mLoss[0m : 2.33038
[1mStep[0m  [14/26], [94mLoss[0m : 2.45465
[1mStep[0m  [16/26], [94mLoss[0m : 2.50162
[1mStep[0m  [18/26], [94mLoss[0m : 2.44385
[1mStep[0m  [20/26], [94mLoss[0m : 2.41556
[1mStep[0m  [22/26], [94mLoss[0m : 2.45638
[1mStep[0m  [24/26], [94mLoss[0m : 2.52273

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40478
[1mStep[0m  [2/26], [94mLoss[0m : 2.49312
[1mStep[0m  [4/26], [94mLoss[0m : 2.38920
[1mStep[0m  [6/26], [94mLoss[0m : 2.31538
[1mStep[0m  [8/26], [94mLoss[0m : 2.30406
[1mStep[0m  [10/26], [94mLoss[0m : 2.27560
[1mStep[0m  [12/26], [94mLoss[0m : 2.51313
[1mStep[0m  [14/26], [94mLoss[0m : 2.43520
[1mStep[0m  [16/26], [94mLoss[0m : 2.41934
[1mStep[0m  [18/26], [94mLoss[0m : 2.38001
[1mStep[0m  [20/26], [94mLoss[0m : 2.45272
[1mStep[0m  [22/26], [94mLoss[0m : 2.35351
[1mStep[0m  [24/26], [94mLoss[0m : 2.39620

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43186
[1mStep[0m  [2/26], [94mLoss[0m : 2.29989
[1mStep[0m  [4/26], [94mLoss[0m : 2.37445
[1mStep[0m  [6/26], [94mLoss[0m : 2.39802
[1mStep[0m  [8/26], [94mLoss[0m : 2.30740
[1mStep[0m  [10/26], [94mLoss[0m : 2.43274
[1mStep[0m  [12/26], [94mLoss[0m : 2.42731
[1mStep[0m  [14/26], [94mLoss[0m : 2.36984
[1mStep[0m  [16/26], [94mLoss[0m : 2.34495
[1mStep[0m  [18/26], [94mLoss[0m : 2.45575
[1mStep[0m  [20/26], [94mLoss[0m : 2.39590
[1mStep[0m  [22/26], [94mLoss[0m : 2.39107
[1mStep[0m  [24/26], [94mLoss[0m : 2.23422

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30340
[1mStep[0m  [2/26], [94mLoss[0m : 2.38436
[1mStep[0m  [4/26], [94mLoss[0m : 2.34958
[1mStep[0m  [6/26], [94mLoss[0m : 2.35425
[1mStep[0m  [8/26], [94mLoss[0m : 2.34944
[1mStep[0m  [10/26], [94mLoss[0m : 2.33423
[1mStep[0m  [12/26], [94mLoss[0m : 2.27113
[1mStep[0m  [14/26], [94mLoss[0m : 2.29033
[1mStep[0m  [16/26], [94mLoss[0m : 2.26473
[1mStep[0m  [18/26], [94mLoss[0m : 2.26947
[1mStep[0m  [20/26], [94mLoss[0m : 2.48328
[1mStep[0m  [22/26], [94mLoss[0m : 2.35964
[1mStep[0m  [24/26], [94mLoss[0m : 2.39117

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29832
[1mStep[0m  [2/26], [94mLoss[0m : 2.20455
[1mStep[0m  [4/26], [94mLoss[0m : 2.35740
[1mStep[0m  [6/26], [94mLoss[0m : 2.21970
[1mStep[0m  [8/26], [94mLoss[0m : 2.29118
[1mStep[0m  [10/26], [94mLoss[0m : 2.54173
[1mStep[0m  [12/26], [94mLoss[0m : 2.21878
[1mStep[0m  [14/26], [94mLoss[0m : 2.24228
[1mStep[0m  [16/26], [94mLoss[0m : 2.36970
[1mStep[0m  [18/26], [94mLoss[0m : 2.32994
[1mStep[0m  [20/26], [94mLoss[0m : 2.41104
[1mStep[0m  [22/26], [94mLoss[0m : 2.29082
[1mStep[0m  [24/26], [94mLoss[0m : 2.27772

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32660
[1mStep[0m  [2/26], [94mLoss[0m : 2.27400
[1mStep[0m  [4/26], [94mLoss[0m : 2.32773
[1mStep[0m  [6/26], [94mLoss[0m : 2.22237
[1mStep[0m  [8/26], [94mLoss[0m : 2.33725
[1mStep[0m  [10/26], [94mLoss[0m : 2.29767
[1mStep[0m  [12/26], [94mLoss[0m : 2.33640
[1mStep[0m  [14/26], [94mLoss[0m : 2.23607
[1mStep[0m  [16/26], [94mLoss[0m : 2.32819
[1mStep[0m  [18/26], [94mLoss[0m : 2.48962
[1mStep[0m  [20/26], [94mLoss[0m : 2.39467
[1mStep[0m  [22/26], [94mLoss[0m : 2.32867
[1mStep[0m  [24/26], [94mLoss[0m : 2.35194

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27038
[1mStep[0m  [2/26], [94mLoss[0m : 2.41683
[1mStep[0m  [4/26], [94mLoss[0m : 2.15515
[1mStep[0m  [6/26], [94mLoss[0m : 2.32651
[1mStep[0m  [8/26], [94mLoss[0m : 2.26001
[1mStep[0m  [10/26], [94mLoss[0m : 2.37039
[1mStep[0m  [12/26], [94mLoss[0m : 2.18669
[1mStep[0m  [14/26], [94mLoss[0m : 2.30488
[1mStep[0m  [16/26], [94mLoss[0m : 2.30016
[1mStep[0m  [18/26], [94mLoss[0m : 2.36690
[1mStep[0m  [20/26], [94mLoss[0m : 2.20773
[1mStep[0m  [22/26], [94mLoss[0m : 2.28839
[1mStep[0m  [24/26], [94mLoss[0m : 2.23631

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.06472
[1mStep[0m  [2/26], [94mLoss[0m : 2.28079
[1mStep[0m  [4/26], [94mLoss[0m : 2.35681
[1mStep[0m  [6/26], [94mLoss[0m : 2.21886
[1mStep[0m  [8/26], [94mLoss[0m : 2.21710
[1mStep[0m  [10/26], [94mLoss[0m : 2.22594
[1mStep[0m  [12/26], [94mLoss[0m : 2.26580
[1mStep[0m  [14/26], [94mLoss[0m : 2.33243
[1mStep[0m  [16/26], [94mLoss[0m : 2.12359
[1mStep[0m  [18/26], [94mLoss[0m : 2.30938
[1mStep[0m  [20/26], [94mLoss[0m : 2.11860
[1mStep[0m  [22/26], [94mLoss[0m : 2.23573
[1mStep[0m  [24/26], [94mLoss[0m : 2.41131

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19029
[1mStep[0m  [2/26], [94mLoss[0m : 2.16881
[1mStep[0m  [4/26], [94mLoss[0m : 2.28642
[1mStep[0m  [6/26], [94mLoss[0m : 2.40893
[1mStep[0m  [8/26], [94mLoss[0m : 2.13685
[1mStep[0m  [10/26], [94mLoss[0m : 2.24916
[1mStep[0m  [12/26], [94mLoss[0m : 2.32527
[1mStep[0m  [14/26], [94mLoss[0m : 2.21624
[1mStep[0m  [16/26], [94mLoss[0m : 2.25211
[1mStep[0m  [18/26], [94mLoss[0m : 2.28570
[1mStep[0m  [20/26], [94mLoss[0m : 2.31581
[1mStep[0m  [22/26], [94mLoss[0m : 2.12316
[1mStep[0m  [24/26], [94mLoss[0m : 2.22884

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26927
[1mStep[0m  [2/26], [94mLoss[0m : 2.22560
[1mStep[0m  [4/26], [94mLoss[0m : 2.35734
[1mStep[0m  [6/26], [94mLoss[0m : 2.05207
[1mStep[0m  [8/26], [94mLoss[0m : 2.26643
[1mStep[0m  [10/26], [94mLoss[0m : 2.33675
[1mStep[0m  [12/26], [94mLoss[0m : 2.11872
[1mStep[0m  [14/26], [94mLoss[0m : 2.07960
[1mStep[0m  [16/26], [94mLoss[0m : 2.36580
[1mStep[0m  [18/26], [94mLoss[0m : 2.34456
[1mStep[0m  [20/26], [94mLoss[0m : 2.30834
[1mStep[0m  [22/26], [94mLoss[0m : 2.17964
[1mStep[0m  [24/26], [94mLoss[0m : 2.19171

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20734
[1mStep[0m  [2/26], [94mLoss[0m : 2.16679
[1mStep[0m  [4/26], [94mLoss[0m : 2.20876
[1mStep[0m  [6/26], [94mLoss[0m : 2.19064
[1mStep[0m  [8/26], [94mLoss[0m : 2.19594
[1mStep[0m  [10/26], [94mLoss[0m : 2.05401
[1mStep[0m  [12/26], [94mLoss[0m : 2.14308
[1mStep[0m  [14/26], [94mLoss[0m : 2.24865
[1mStep[0m  [16/26], [94mLoss[0m : 2.29409
[1mStep[0m  [18/26], [94mLoss[0m : 2.29922
[1mStep[0m  [20/26], [94mLoss[0m : 2.28403
[1mStep[0m  [22/26], [94mLoss[0m : 2.20784
[1mStep[0m  [24/26], [94mLoss[0m : 2.15339

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20495
[1mStep[0m  [2/26], [94mLoss[0m : 2.19299
[1mStep[0m  [4/26], [94mLoss[0m : 2.20981
[1mStep[0m  [6/26], [94mLoss[0m : 2.12659
[1mStep[0m  [8/26], [94mLoss[0m : 2.16027
[1mStep[0m  [10/26], [94mLoss[0m : 2.14385
[1mStep[0m  [12/26], [94mLoss[0m : 2.04217
[1mStep[0m  [14/26], [94mLoss[0m : 2.29859
[1mStep[0m  [16/26], [94mLoss[0m : 2.11180
[1mStep[0m  [18/26], [94mLoss[0m : 2.33825
[1mStep[0m  [20/26], [94mLoss[0m : 2.12559
[1mStep[0m  [22/26], [94mLoss[0m : 2.14858
[1mStep[0m  [24/26], [94mLoss[0m : 2.17737

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08651
[1mStep[0m  [2/26], [94mLoss[0m : 2.32283
[1mStep[0m  [4/26], [94mLoss[0m : 2.13763
[1mStep[0m  [6/26], [94mLoss[0m : 2.11071
[1mStep[0m  [8/26], [94mLoss[0m : 2.08223
[1mStep[0m  [10/26], [94mLoss[0m : 2.17387
[1mStep[0m  [12/26], [94mLoss[0m : 1.99357
[1mStep[0m  [14/26], [94mLoss[0m : 2.20532
[1mStep[0m  [16/26], [94mLoss[0m : 2.15425
[1mStep[0m  [18/26], [94mLoss[0m : 2.14915
[1mStep[0m  [20/26], [94mLoss[0m : 2.17966
[1mStep[0m  [22/26], [94mLoss[0m : 2.12680
[1mStep[0m  [24/26], [94mLoss[0m : 2.13701

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30607
[1mStep[0m  [2/26], [94mLoss[0m : 2.04118
[1mStep[0m  [4/26], [94mLoss[0m : 2.16729
[1mStep[0m  [6/26], [94mLoss[0m : 2.18433
[1mStep[0m  [8/26], [94mLoss[0m : 2.16746
[1mStep[0m  [10/26], [94mLoss[0m : 2.20430
[1mStep[0m  [12/26], [94mLoss[0m : 1.94364
[1mStep[0m  [14/26], [94mLoss[0m : 2.16643
[1mStep[0m  [16/26], [94mLoss[0m : 2.11636
[1mStep[0m  [18/26], [94mLoss[0m : 2.14787
[1mStep[0m  [20/26], [94mLoss[0m : 2.14601
[1mStep[0m  [22/26], [94mLoss[0m : 2.11944
[1mStep[0m  [24/26], [94mLoss[0m : 2.15735

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.546, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.12927
[1mStep[0m  [2/26], [94mLoss[0m : 1.99757
[1mStep[0m  [4/26], [94mLoss[0m : 2.15684
[1mStep[0m  [6/26], [94mLoss[0m : 2.12079
[1mStep[0m  [8/26], [94mLoss[0m : 2.11056
[1mStep[0m  [10/26], [94mLoss[0m : 2.15467
[1mStep[0m  [12/26], [94mLoss[0m : 2.10095
[1mStep[0m  [14/26], [94mLoss[0m : 2.02824
[1mStep[0m  [16/26], [94mLoss[0m : 2.05013
[1mStep[0m  [18/26], [94mLoss[0m : 2.14286
[1mStep[0m  [20/26], [94mLoss[0m : 2.14834
[1mStep[0m  [22/26], [94mLoss[0m : 2.21039
[1mStep[0m  [24/26], [94mLoss[0m : 2.16949

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.12771
[1mStep[0m  [2/26], [94mLoss[0m : 2.00059
[1mStep[0m  [4/26], [94mLoss[0m : 2.00550
[1mStep[0m  [6/26], [94mLoss[0m : 2.08602
[1mStep[0m  [8/26], [94mLoss[0m : 2.16921
[1mStep[0m  [10/26], [94mLoss[0m : 2.06535
[1mStep[0m  [12/26], [94mLoss[0m : 2.25172
[1mStep[0m  [14/26], [94mLoss[0m : 2.05964
[1mStep[0m  [16/26], [94mLoss[0m : 2.10781
[1mStep[0m  [18/26], [94mLoss[0m : 2.01518
[1mStep[0m  [20/26], [94mLoss[0m : 2.13637
[1mStep[0m  [22/26], [94mLoss[0m : 2.23721
[1mStep[0m  [24/26], [94mLoss[0m : 2.01223

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.075, [92mTest[0m: 2.448, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03969
[1mStep[0m  [2/26], [94mLoss[0m : 2.07286
[1mStep[0m  [4/26], [94mLoss[0m : 1.87975
[1mStep[0m  [6/26], [94mLoss[0m : 2.15202
[1mStep[0m  [8/26], [94mLoss[0m : 2.02588
[1mStep[0m  [10/26], [94mLoss[0m : 2.13894
[1mStep[0m  [12/26], [94mLoss[0m : 2.07035
[1mStep[0m  [14/26], [94mLoss[0m : 2.04204
[1mStep[0m  [16/26], [94mLoss[0m : 2.05687
[1mStep[0m  [18/26], [94mLoss[0m : 2.21945
[1mStep[0m  [20/26], [94mLoss[0m : 1.92425
[1mStep[0m  [22/26], [94mLoss[0m : 2.08583
[1mStep[0m  [24/26], [94mLoss[0m : 1.92254

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.480, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03416
[1mStep[0m  [2/26], [94mLoss[0m : 2.01815
[1mStep[0m  [4/26], [94mLoss[0m : 2.01145
[1mStep[0m  [6/26], [94mLoss[0m : 2.19355
[1mStep[0m  [8/26], [94mLoss[0m : 2.13851
[1mStep[0m  [10/26], [94mLoss[0m : 2.07030
[1mStep[0m  [12/26], [94mLoss[0m : 1.94062
[1mStep[0m  [14/26], [94mLoss[0m : 2.05301
[1mStep[0m  [16/26], [94mLoss[0m : 1.93348
[1mStep[0m  [18/26], [94mLoss[0m : 2.03345
[1mStep[0m  [20/26], [94mLoss[0m : 1.88082
[1mStep[0m  [22/26], [94mLoss[0m : 1.97699
[1mStep[0m  [24/26], [94mLoss[0m : 2.00049

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.89266
[1mStep[0m  [2/26], [94mLoss[0m : 2.03769
[1mStep[0m  [4/26], [94mLoss[0m : 2.04929
[1mStep[0m  [6/26], [94mLoss[0m : 2.00984
[1mStep[0m  [8/26], [94mLoss[0m : 1.95891
[1mStep[0m  [10/26], [94mLoss[0m : 2.01751
[1mStep[0m  [12/26], [94mLoss[0m : 2.02105
[1mStep[0m  [14/26], [94mLoss[0m : 1.86422
[1mStep[0m  [16/26], [94mLoss[0m : 1.99224
[1mStep[0m  [18/26], [94mLoss[0m : 1.94908
[1mStep[0m  [20/26], [94mLoss[0m : 1.89138
[1mStep[0m  [22/26], [94mLoss[0m : 2.05330
[1mStep[0m  [24/26], [94mLoss[0m : 2.12106

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.412, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03507
[1mStep[0m  [2/26], [94mLoss[0m : 1.86403
[1mStep[0m  [4/26], [94mLoss[0m : 2.00760
[1mStep[0m  [6/26], [94mLoss[0m : 1.86114
[1mStep[0m  [8/26], [94mLoss[0m : 1.99097
[1mStep[0m  [10/26], [94mLoss[0m : 1.97096
[1mStep[0m  [12/26], [94mLoss[0m : 2.01800
[1mStep[0m  [14/26], [94mLoss[0m : 1.94226
[1mStep[0m  [16/26], [94mLoss[0m : 1.92139
[1mStep[0m  [18/26], [94mLoss[0m : 2.08029
[1mStep[0m  [20/26], [94mLoss[0m : 2.11383
[1mStep[0m  [22/26], [94mLoss[0m : 2.17871
[1mStep[0m  [24/26], [94mLoss[0m : 1.92472

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93394
[1mStep[0m  [2/26], [94mLoss[0m : 2.04054
[1mStep[0m  [4/26], [94mLoss[0m : 1.88727
[1mStep[0m  [6/26], [94mLoss[0m : 2.01011
[1mStep[0m  [8/26], [94mLoss[0m : 1.99700
[1mStep[0m  [10/26], [94mLoss[0m : 1.78870
[1mStep[0m  [12/26], [94mLoss[0m : 1.98058
[1mStep[0m  [14/26], [94mLoss[0m : 2.03196
[1mStep[0m  [16/26], [94mLoss[0m : 2.01162
[1mStep[0m  [18/26], [94mLoss[0m : 1.90656
[1mStep[0m  [20/26], [94mLoss[0m : 2.05886
[1mStep[0m  [22/26], [94mLoss[0m : 1.98172
[1mStep[0m  [24/26], [94mLoss[0m : 1.90855

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.953, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.69411
[1mStep[0m  [2/26], [94mLoss[0m : 1.79779
[1mStep[0m  [4/26], [94mLoss[0m : 1.93150
[1mStep[0m  [6/26], [94mLoss[0m : 1.92063
[1mStep[0m  [8/26], [94mLoss[0m : 1.94297
[1mStep[0m  [10/26], [94mLoss[0m : 1.98776
[1mStep[0m  [12/26], [94mLoss[0m : 1.88803
[1mStep[0m  [14/26], [94mLoss[0m : 1.82901
[1mStep[0m  [16/26], [94mLoss[0m : 2.02135
[1mStep[0m  [18/26], [94mLoss[0m : 1.94637
[1mStep[0m  [20/26], [94mLoss[0m : 2.06322
[1mStep[0m  [22/26], [94mLoss[0m : 1.95117
[1mStep[0m  [24/26], [94mLoss[0m : 2.01908

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.87490
[1mStep[0m  [2/26], [94mLoss[0m : 1.87604
[1mStep[0m  [4/26], [94mLoss[0m : 1.92279
[1mStep[0m  [6/26], [94mLoss[0m : 1.86600
[1mStep[0m  [8/26], [94mLoss[0m : 1.73479
[1mStep[0m  [10/26], [94mLoss[0m : 1.90981
[1mStep[0m  [12/26], [94mLoss[0m : 2.02353
[1mStep[0m  [14/26], [94mLoss[0m : 1.87887
[1mStep[0m  [16/26], [94mLoss[0m : 2.01361
[1mStep[0m  [18/26], [94mLoss[0m : 1.80098
[1mStep[0m  [20/26], [94mLoss[0m : 1.87327
[1mStep[0m  [22/26], [94mLoss[0m : 1.88577
[1mStep[0m  [24/26], [94mLoss[0m : 2.02598

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.428, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81404
[1mStep[0m  [2/26], [94mLoss[0m : 1.84082
[1mStep[0m  [4/26], [94mLoss[0m : 1.85648
[1mStep[0m  [6/26], [94mLoss[0m : 1.86500
[1mStep[0m  [8/26], [94mLoss[0m : 1.89865
[1mStep[0m  [10/26], [94mLoss[0m : 1.96759
[1mStep[0m  [12/26], [94mLoss[0m : 1.93452
[1mStep[0m  [14/26], [94mLoss[0m : 2.03195
[1mStep[0m  [16/26], [94mLoss[0m : 1.94639
[1mStep[0m  [18/26], [94mLoss[0m : 1.95027
[1mStep[0m  [20/26], [94mLoss[0m : 1.87469
[1mStep[0m  [22/26], [94mLoss[0m : 1.85563
[1mStep[0m  [24/26], [94mLoss[0m : 1.85362

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.85288
[1mStep[0m  [2/26], [94mLoss[0m : 1.83251
[1mStep[0m  [4/26], [94mLoss[0m : 1.89677
[1mStep[0m  [6/26], [94mLoss[0m : 1.70801
[1mStep[0m  [8/26], [94mLoss[0m : 1.82737
[1mStep[0m  [10/26], [94mLoss[0m : 1.81819
[1mStep[0m  [12/26], [94mLoss[0m : 1.91170
[1mStep[0m  [14/26], [94mLoss[0m : 1.78426
[1mStep[0m  [16/26], [94mLoss[0m : 1.75937
[1mStep[0m  [18/26], [94mLoss[0m : 1.86839
[1mStep[0m  [20/26], [94mLoss[0m : 1.77765
[1mStep[0m  [22/26], [94mLoss[0m : 1.85645
[1mStep[0m  [24/26], [94mLoss[0m : 1.89420

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.83555
[1mStep[0m  [2/26], [94mLoss[0m : 1.71993
[1mStep[0m  [4/26], [94mLoss[0m : 1.92344
[1mStep[0m  [6/26], [94mLoss[0m : 1.82311
[1mStep[0m  [8/26], [94mLoss[0m : 1.82278
[1mStep[0m  [10/26], [94mLoss[0m : 1.79826
[1mStep[0m  [12/26], [94mLoss[0m : 1.80928
[1mStep[0m  [14/26], [94mLoss[0m : 1.68193
[1mStep[0m  [16/26], [94mLoss[0m : 1.79336
[1mStep[0m  [18/26], [94mLoss[0m : 1.80724
[1mStep[0m  [20/26], [94mLoss[0m : 1.93578
[1mStep[0m  [22/26], [94mLoss[0m : 1.73843
[1mStep[0m  [24/26], [94mLoss[0m : 1.92035

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.479
====================================

Phase 2 - Evaluation MAE:  2.4791575211745043
MAE score P1      2.399867
MAE score P2      2.479158
loss              1.818476
learning_rate         0.01
batch_size             512
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 11.00400
[1mStep[0m  [2/26], [94mLoss[0m : 10.79240
[1mStep[0m  [4/26], [94mLoss[0m : 10.73118
[1mStep[0m  [6/26], [94mLoss[0m : 10.59738
[1mStep[0m  [8/26], [94mLoss[0m : 10.52386
[1mStep[0m  [10/26], [94mLoss[0m : 10.13142
[1mStep[0m  [12/26], [94mLoss[0m : 10.20622
[1mStep[0m  [14/26], [94mLoss[0m : 9.58196
[1mStep[0m  [16/26], [94mLoss[0m : 9.53696
[1mStep[0m  [18/26], [94mLoss[0m : 9.10339
[1mStep[0m  [20/26], [94mLoss[0m : 8.70509
[1mStep[0m  [22/26], [94mLoss[0m : 8.36520
[1mStep[0m  [24/26], [94mLoss[0m : 7.76217

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.743, [92mTest[0m: 10.977, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.72104
[1mStep[0m  [2/26], [94mLoss[0m : 7.57155
[1mStep[0m  [4/26], [94mLoss[0m : 7.37969
[1mStep[0m  [6/26], [94mLoss[0m : 6.42663
[1mStep[0m  [8/26], [94mLoss[0m : 6.64196
[1mStep[0m  [10/26], [94mLoss[0m : 6.28889
[1mStep[0m  [12/26], [94mLoss[0m : 6.35934
[1mStep[0m  [14/26], [94mLoss[0m : 5.85733
[1mStep[0m  [16/26], [94mLoss[0m : 5.89446
[1mStep[0m  [18/26], [94mLoss[0m : 5.30840
[1mStep[0m  [20/26], [94mLoss[0m : 5.27379
[1mStep[0m  [22/26], [94mLoss[0m : 4.58197
[1mStep[0m  [24/26], [94mLoss[0m : 4.65770

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.039, [92mTest[0m: 7.939, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.17826
[1mStep[0m  [2/26], [94mLoss[0m : 3.78982
[1mStep[0m  [4/26], [94mLoss[0m : 3.82476
[1mStep[0m  [6/26], [94mLoss[0m : 3.45589
[1mStep[0m  [8/26], [94mLoss[0m : 3.23406
[1mStep[0m  [10/26], [94mLoss[0m : 3.06616
[1mStep[0m  [12/26], [94mLoss[0m : 2.79410
[1mStep[0m  [14/26], [94mLoss[0m : 2.72646
[1mStep[0m  [16/26], [94mLoss[0m : 2.78338
[1mStep[0m  [18/26], [94mLoss[0m : 2.61557
[1mStep[0m  [20/26], [94mLoss[0m : 2.60271
[1mStep[0m  [22/26], [94mLoss[0m : 2.64058
[1mStep[0m  [24/26], [94mLoss[0m : 2.78541

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.068, [92mTest[0m: 3.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.80539
[1mStep[0m  [2/26], [94mLoss[0m : 2.55971
[1mStep[0m  [4/26], [94mLoss[0m : 2.68358
[1mStep[0m  [6/26], [94mLoss[0m : 2.43614
[1mStep[0m  [8/26], [94mLoss[0m : 2.41904
[1mStep[0m  [10/26], [94mLoss[0m : 2.67429
[1mStep[0m  [12/26], [94mLoss[0m : 2.57473
[1mStep[0m  [14/26], [94mLoss[0m : 2.61803
[1mStep[0m  [16/26], [94mLoss[0m : 2.53965
[1mStep[0m  [18/26], [94mLoss[0m : 2.61680
[1mStep[0m  [20/26], [94mLoss[0m : 2.67218
[1mStep[0m  [22/26], [94mLoss[0m : 2.26402
[1mStep[0m  [24/26], [94mLoss[0m : 2.49193

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.699, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46274
[1mStep[0m  [2/26], [94mLoss[0m : 2.50041
[1mStep[0m  [4/26], [94mLoss[0m : 2.59577
[1mStep[0m  [6/26], [94mLoss[0m : 2.57650
[1mStep[0m  [8/26], [94mLoss[0m : 2.42088
[1mStep[0m  [10/26], [94mLoss[0m : 2.43501
[1mStep[0m  [12/26], [94mLoss[0m : 2.72039
[1mStep[0m  [14/26], [94mLoss[0m : 2.54618
[1mStep[0m  [16/26], [94mLoss[0m : 2.53309
[1mStep[0m  [18/26], [94mLoss[0m : 2.58854
[1mStep[0m  [20/26], [94mLoss[0m : 2.51162
[1mStep[0m  [22/26], [94mLoss[0m : 2.61310
[1mStep[0m  [24/26], [94mLoss[0m : 2.67147

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42071
[1mStep[0m  [2/26], [94mLoss[0m : 2.41717
[1mStep[0m  [4/26], [94mLoss[0m : 2.59799
[1mStep[0m  [6/26], [94mLoss[0m : 2.62993
[1mStep[0m  [8/26], [94mLoss[0m : 2.52099
[1mStep[0m  [10/26], [94mLoss[0m : 2.55335
[1mStep[0m  [12/26], [94mLoss[0m : 2.41028
[1mStep[0m  [14/26], [94mLoss[0m : 2.38962
[1mStep[0m  [16/26], [94mLoss[0m : 2.70757
[1mStep[0m  [18/26], [94mLoss[0m : 2.56402
[1mStep[0m  [20/26], [94mLoss[0m : 2.45176
[1mStep[0m  [22/26], [94mLoss[0m : 2.62059
[1mStep[0m  [24/26], [94mLoss[0m : 2.37108

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49479
[1mStep[0m  [2/26], [94mLoss[0m : 2.41953
[1mStep[0m  [4/26], [94mLoss[0m : 2.41454
[1mStep[0m  [6/26], [94mLoss[0m : 2.52310
[1mStep[0m  [8/26], [94mLoss[0m : 2.43995
[1mStep[0m  [10/26], [94mLoss[0m : 2.56445
[1mStep[0m  [12/26], [94mLoss[0m : 2.61641
[1mStep[0m  [14/26], [94mLoss[0m : 2.40310
[1mStep[0m  [16/26], [94mLoss[0m : 2.60934
[1mStep[0m  [18/26], [94mLoss[0m : 2.42373
[1mStep[0m  [20/26], [94mLoss[0m : 2.40742
[1mStep[0m  [22/26], [94mLoss[0m : 2.65187
[1mStep[0m  [24/26], [94mLoss[0m : 2.49784

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49486
[1mStep[0m  [2/26], [94mLoss[0m : 2.44306
[1mStep[0m  [4/26], [94mLoss[0m : 2.53356
[1mStep[0m  [6/26], [94mLoss[0m : 2.49510
[1mStep[0m  [8/26], [94mLoss[0m : 2.43713
[1mStep[0m  [10/26], [94mLoss[0m : 2.39620
[1mStep[0m  [12/26], [94mLoss[0m : 2.40658
[1mStep[0m  [14/26], [94mLoss[0m : 2.50117
[1mStep[0m  [16/26], [94mLoss[0m : 2.44128
[1mStep[0m  [18/26], [94mLoss[0m : 2.37769
[1mStep[0m  [20/26], [94mLoss[0m : 2.59906
[1mStep[0m  [22/26], [94mLoss[0m : 2.42122
[1mStep[0m  [24/26], [94mLoss[0m : 2.50197

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41832
[1mStep[0m  [2/26], [94mLoss[0m : 2.51689
[1mStep[0m  [4/26], [94mLoss[0m : 2.33221
[1mStep[0m  [6/26], [94mLoss[0m : 2.33127
[1mStep[0m  [8/26], [94mLoss[0m : 2.35845
[1mStep[0m  [10/26], [94mLoss[0m : 2.56382
[1mStep[0m  [12/26], [94mLoss[0m : 2.46680
[1mStep[0m  [14/26], [94mLoss[0m : 2.37184
[1mStep[0m  [16/26], [94mLoss[0m : 2.38496
[1mStep[0m  [18/26], [94mLoss[0m : 2.57378
[1mStep[0m  [20/26], [94mLoss[0m : 2.42033
[1mStep[0m  [22/26], [94mLoss[0m : 2.42755
[1mStep[0m  [24/26], [94mLoss[0m : 2.44143

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46624
[1mStep[0m  [2/26], [94mLoss[0m : 2.55344
[1mStep[0m  [4/26], [94mLoss[0m : 2.59880
[1mStep[0m  [6/26], [94mLoss[0m : 2.26769
[1mStep[0m  [8/26], [94mLoss[0m : 2.38288
[1mStep[0m  [10/26], [94mLoss[0m : 2.55252
[1mStep[0m  [12/26], [94mLoss[0m : 2.35733
[1mStep[0m  [14/26], [94mLoss[0m : 2.48288
[1mStep[0m  [16/26], [94mLoss[0m : 2.49281
[1mStep[0m  [18/26], [94mLoss[0m : 2.36295
[1mStep[0m  [20/26], [94mLoss[0m : 2.39364
[1mStep[0m  [22/26], [94mLoss[0m : 2.32377
[1mStep[0m  [24/26], [94mLoss[0m : 2.44599

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46311
[1mStep[0m  [2/26], [94mLoss[0m : 2.27900
[1mStep[0m  [4/26], [94mLoss[0m : 2.41922
[1mStep[0m  [6/26], [94mLoss[0m : 2.40698
[1mStep[0m  [8/26], [94mLoss[0m : 2.58003
[1mStep[0m  [10/26], [94mLoss[0m : 2.34663
[1mStep[0m  [12/26], [94mLoss[0m : 2.26837
[1mStep[0m  [14/26], [94mLoss[0m : 2.46931
[1mStep[0m  [16/26], [94mLoss[0m : 2.41244
[1mStep[0m  [18/26], [94mLoss[0m : 2.41866
[1mStep[0m  [20/26], [94mLoss[0m : 2.38134
[1mStep[0m  [22/26], [94mLoss[0m : 2.40806
[1mStep[0m  [24/26], [94mLoss[0m : 2.47110

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35026
[1mStep[0m  [2/26], [94mLoss[0m : 2.30027
[1mStep[0m  [4/26], [94mLoss[0m : 2.32468
[1mStep[0m  [6/26], [94mLoss[0m : 2.46949
[1mStep[0m  [8/26], [94mLoss[0m : 2.45582
[1mStep[0m  [10/26], [94mLoss[0m : 2.31848
[1mStep[0m  [12/26], [94mLoss[0m : 2.44060
[1mStep[0m  [14/26], [94mLoss[0m : 2.34658
[1mStep[0m  [16/26], [94mLoss[0m : 2.37680
[1mStep[0m  [18/26], [94mLoss[0m : 2.42787
[1mStep[0m  [20/26], [94mLoss[0m : 2.33467
[1mStep[0m  [22/26], [94mLoss[0m : 2.46706
[1mStep[0m  [24/26], [94mLoss[0m : 2.44386

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37718
[1mStep[0m  [2/26], [94mLoss[0m : 2.55695
[1mStep[0m  [4/26], [94mLoss[0m : 2.40790
[1mStep[0m  [6/26], [94mLoss[0m : 2.31638
[1mStep[0m  [8/26], [94mLoss[0m : 2.35872
[1mStep[0m  [10/26], [94mLoss[0m : 2.37565
[1mStep[0m  [12/26], [94mLoss[0m : 2.55602
[1mStep[0m  [14/26], [94mLoss[0m : 2.27622
[1mStep[0m  [16/26], [94mLoss[0m : 2.53164
[1mStep[0m  [18/26], [94mLoss[0m : 2.34918
[1mStep[0m  [20/26], [94mLoss[0m : 2.36496
[1mStep[0m  [22/26], [94mLoss[0m : 2.47972
[1mStep[0m  [24/26], [94mLoss[0m : 2.46285

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37571
[1mStep[0m  [2/26], [94mLoss[0m : 2.40051
[1mStep[0m  [4/26], [94mLoss[0m : 2.38534
[1mStep[0m  [6/26], [94mLoss[0m : 2.29216
[1mStep[0m  [8/26], [94mLoss[0m : 2.43619
[1mStep[0m  [10/26], [94mLoss[0m : 2.36934
[1mStep[0m  [12/26], [94mLoss[0m : 2.43401
[1mStep[0m  [14/26], [94mLoss[0m : 2.48280
[1mStep[0m  [16/26], [94mLoss[0m : 2.48561
[1mStep[0m  [18/26], [94mLoss[0m : 2.38399
[1mStep[0m  [20/26], [94mLoss[0m : 2.44015
[1mStep[0m  [22/26], [94mLoss[0m : 2.29472
[1mStep[0m  [24/26], [94mLoss[0m : 2.52301

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42854
[1mStep[0m  [2/26], [94mLoss[0m : 2.28539
[1mStep[0m  [4/26], [94mLoss[0m : 2.37404
[1mStep[0m  [6/26], [94mLoss[0m : 2.50663
[1mStep[0m  [8/26], [94mLoss[0m : 2.50200
[1mStep[0m  [10/26], [94mLoss[0m : 2.31057
[1mStep[0m  [12/26], [94mLoss[0m : 2.35015
[1mStep[0m  [14/26], [94mLoss[0m : 2.35789
[1mStep[0m  [16/26], [94mLoss[0m : 2.24998
[1mStep[0m  [18/26], [94mLoss[0m : 2.45726
[1mStep[0m  [20/26], [94mLoss[0m : 2.35274
[1mStep[0m  [22/26], [94mLoss[0m : 2.37266
[1mStep[0m  [24/26], [94mLoss[0m : 2.42969

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35073
[1mStep[0m  [2/26], [94mLoss[0m : 2.31782
[1mStep[0m  [4/26], [94mLoss[0m : 2.42672
[1mStep[0m  [6/26], [94mLoss[0m : 2.53303
[1mStep[0m  [8/26], [94mLoss[0m : 2.38367
[1mStep[0m  [10/26], [94mLoss[0m : 2.32348
[1mStep[0m  [12/26], [94mLoss[0m : 2.32800
[1mStep[0m  [14/26], [94mLoss[0m : 2.48277
[1mStep[0m  [16/26], [94mLoss[0m : 2.44407
[1mStep[0m  [18/26], [94mLoss[0m : 2.45954
[1mStep[0m  [20/26], [94mLoss[0m : 2.39939
[1mStep[0m  [22/26], [94mLoss[0m : 2.44969
[1mStep[0m  [24/26], [94mLoss[0m : 2.46058

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46484
[1mStep[0m  [2/26], [94mLoss[0m : 2.38086
[1mStep[0m  [4/26], [94mLoss[0m : 2.45323
[1mStep[0m  [6/26], [94mLoss[0m : 2.38006
[1mStep[0m  [8/26], [94mLoss[0m : 2.32650
[1mStep[0m  [10/26], [94mLoss[0m : 2.29412
[1mStep[0m  [12/26], [94mLoss[0m : 2.26529
[1mStep[0m  [14/26], [94mLoss[0m : 2.35657
[1mStep[0m  [16/26], [94mLoss[0m : 2.32453
[1mStep[0m  [18/26], [94mLoss[0m : 2.44070
[1mStep[0m  [20/26], [94mLoss[0m : 2.28993
[1mStep[0m  [22/26], [94mLoss[0m : 2.31961
[1mStep[0m  [24/26], [94mLoss[0m : 2.40183

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41702
[1mStep[0m  [2/26], [94mLoss[0m : 2.39204
[1mStep[0m  [4/26], [94mLoss[0m : 2.46479
[1mStep[0m  [6/26], [94mLoss[0m : 2.45266
[1mStep[0m  [8/26], [94mLoss[0m : 2.31174
[1mStep[0m  [10/26], [94mLoss[0m : 2.41268
[1mStep[0m  [12/26], [94mLoss[0m : 2.46035
[1mStep[0m  [14/26], [94mLoss[0m : 2.48757
[1mStep[0m  [16/26], [94mLoss[0m : 2.43313
[1mStep[0m  [18/26], [94mLoss[0m : 2.32851
[1mStep[0m  [20/26], [94mLoss[0m : 2.35727
[1mStep[0m  [22/26], [94mLoss[0m : 2.38695
[1mStep[0m  [24/26], [94mLoss[0m : 2.27205

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42753
[1mStep[0m  [2/26], [94mLoss[0m : 2.36829
[1mStep[0m  [4/26], [94mLoss[0m : 2.40081
[1mStep[0m  [6/26], [94mLoss[0m : 2.34185
[1mStep[0m  [8/26], [94mLoss[0m : 2.44510
[1mStep[0m  [10/26], [94mLoss[0m : 2.46006
[1mStep[0m  [12/26], [94mLoss[0m : 2.34451
[1mStep[0m  [14/26], [94mLoss[0m : 2.34538
[1mStep[0m  [16/26], [94mLoss[0m : 2.50257
[1mStep[0m  [18/26], [94mLoss[0m : 2.54766
[1mStep[0m  [20/26], [94mLoss[0m : 2.45497
[1mStep[0m  [22/26], [94mLoss[0m : 2.51638
[1mStep[0m  [24/26], [94mLoss[0m : 2.28427

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29381
[1mStep[0m  [2/26], [94mLoss[0m : 2.32335
[1mStep[0m  [4/26], [94mLoss[0m : 2.29793
[1mStep[0m  [6/26], [94mLoss[0m : 2.42411
[1mStep[0m  [8/26], [94mLoss[0m : 2.47885
[1mStep[0m  [10/26], [94mLoss[0m : 2.41553
[1mStep[0m  [12/26], [94mLoss[0m : 2.30499
[1mStep[0m  [14/26], [94mLoss[0m : 2.41131
[1mStep[0m  [16/26], [94mLoss[0m : 2.33475
[1mStep[0m  [18/26], [94mLoss[0m : 2.47782
[1mStep[0m  [20/26], [94mLoss[0m : 2.28224
[1mStep[0m  [22/26], [94mLoss[0m : 2.53432
[1mStep[0m  [24/26], [94mLoss[0m : 2.35279

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.392, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37574
[1mStep[0m  [2/26], [94mLoss[0m : 2.34030
[1mStep[0m  [4/26], [94mLoss[0m : 2.34313
[1mStep[0m  [6/26], [94mLoss[0m : 2.40307
[1mStep[0m  [8/26], [94mLoss[0m : 2.41609
[1mStep[0m  [10/26], [94mLoss[0m : 2.50911
[1mStep[0m  [12/26], [94mLoss[0m : 2.28107
[1mStep[0m  [14/26], [94mLoss[0m : 2.39016
[1mStep[0m  [16/26], [94mLoss[0m : 2.47955
[1mStep[0m  [18/26], [94mLoss[0m : 2.45433
[1mStep[0m  [20/26], [94mLoss[0m : 2.47224
[1mStep[0m  [22/26], [94mLoss[0m : 2.31428
[1mStep[0m  [24/26], [94mLoss[0m : 2.41608

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21140
[1mStep[0m  [2/26], [94mLoss[0m : 2.44051
[1mStep[0m  [4/26], [94mLoss[0m : 2.36489
[1mStep[0m  [6/26], [94mLoss[0m : 2.35755
[1mStep[0m  [8/26], [94mLoss[0m : 2.61416
[1mStep[0m  [10/26], [94mLoss[0m : 2.48812
[1mStep[0m  [12/26], [94mLoss[0m : 2.35758
[1mStep[0m  [14/26], [94mLoss[0m : 2.32470
[1mStep[0m  [16/26], [94mLoss[0m : 2.44046
[1mStep[0m  [18/26], [94mLoss[0m : 2.39213
[1mStep[0m  [20/26], [94mLoss[0m : 2.28385
[1mStep[0m  [22/26], [94mLoss[0m : 2.34237
[1mStep[0m  [24/26], [94mLoss[0m : 2.28800

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.376, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40826
[1mStep[0m  [2/26], [94mLoss[0m : 2.29263
[1mStep[0m  [4/26], [94mLoss[0m : 2.25349
[1mStep[0m  [6/26], [94mLoss[0m : 2.34619
[1mStep[0m  [8/26], [94mLoss[0m : 2.37298
[1mStep[0m  [10/26], [94mLoss[0m : 2.37534
[1mStep[0m  [12/26], [94mLoss[0m : 2.27268
[1mStep[0m  [14/26], [94mLoss[0m : 2.24256
[1mStep[0m  [16/26], [94mLoss[0m : 2.26278
[1mStep[0m  [18/26], [94mLoss[0m : 2.45675
[1mStep[0m  [20/26], [94mLoss[0m : 2.35033
[1mStep[0m  [22/26], [94mLoss[0m : 2.29045
[1mStep[0m  [24/26], [94mLoss[0m : 2.41251

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42034
[1mStep[0m  [2/26], [94mLoss[0m : 2.38495
[1mStep[0m  [4/26], [94mLoss[0m : 2.32180
[1mStep[0m  [6/26], [94mLoss[0m : 2.33670
[1mStep[0m  [8/26], [94mLoss[0m : 2.39775
[1mStep[0m  [10/26], [94mLoss[0m : 2.33434
[1mStep[0m  [12/26], [94mLoss[0m : 2.41289
[1mStep[0m  [14/26], [94mLoss[0m : 2.53810
[1mStep[0m  [16/26], [94mLoss[0m : 2.38101
[1mStep[0m  [18/26], [94mLoss[0m : 2.18320
[1mStep[0m  [20/26], [94mLoss[0m : 2.29655
[1mStep[0m  [22/26], [94mLoss[0m : 2.27205
[1mStep[0m  [24/26], [94mLoss[0m : 2.19618

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.363, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21809
[1mStep[0m  [2/26], [94mLoss[0m : 2.36299
[1mStep[0m  [4/26], [94mLoss[0m : 2.49980
[1mStep[0m  [6/26], [94mLoss[0m : 2.35932
[1mStep[0m  [8/26], [94mLoss[0m : 2.50486
[1mStep[0m  [10/26], [94mLoss[0m : 2.35047
[1mStep[0m  [12/26], [94mLoss[0m : 2.33986
[1mStep[0m  [14/26], [94mLoss[0m : 2.41976
[1mStep[0m  [16/26], [94mLoss[0m : 2.38547
[1mStep[0m  [18/26], [94mLoss[0m : 2.43928
[1mStep[0m  [20/26], [94mLoss[0m : 2.47341
[1mStep[0m  [22/26], [94mLoss[0m : 2.39982
[1mStep[0m  [24/26], [94mLoss[0m : 2.35506

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27294
[1mStep[0m  [2/26], [94mLoss[0m : 2.38050
[1mStep[0m  [4/26], [94mLoss[0m : 2.32967
[1mStep[0m  [6/26], [94mLoss[0m : 2.66592
[1mStep[0m  [8/26], [94mLoss[0m : 2.34054
[1mStep[0m  [10/26], [94mLoss[0m : 2.39074
[1mStep[0m  [12/26], [94mLoss[0m : 2.35137
[1mStep[0m  [14/26], [94mLoss[0m : 2.32375
[1mStep[0m  [16/26], [94mLoss[0m : 2.29030
[1mStep[0m  [18/26], [94mLoss[0m : 2.44979
[1mStep[0m  [20/26], [94mLoss[0m : 2.30102
[1mStep[0m  [22/26], [94mLoss[0m : 2.40701
[1mStep[0m  [24/26], [94mLoss[0m : 2.31883

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43449
[1mStep[0m  [2/26], [94mLoss[0m : 2.42219
[1mStep[0m  [4/26], [94mLoss[0m : 2.41861
[1mStep[0m  [6/26], [94mLoss[0m : 2.20871
[1mStep[0m  [8/26], [94mLoss[0m : 2.22462
[1mStep[0m  [10/26], [94mLoss[0m : 2.36993
[1mStep[0m  [12/26], [94mLoss[0m : 2.29636
[1mStep[0m  [14/26], [94mLoss[0m : 2.41803
[1mStep[0m  [16/26], [94mLoss[0m : 2.31456
[1mStep[0m  [18/26], [94mLoss[0m : 2.54936
[1mStep[0m  [20/26], [94mLoss[0m : 2.40199
[1mStep[0m  [22/26], [94mLoss[0m : 2.32538
[1mStep[0m  [24/26], [94mLoss[0m : 2.17662

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.365, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25200
[1mStep[0m  [2/26], [94mLoss[0m : 2.26440
[1mStep[0m  [4/26], [94mLoss[0m : 2.34262
[1mStep[0m  [6/26], [94mLoss[0m : 2.46807
[1mStep[0m  [8/26], [94mLoss[0m : 2.18167
[1mStep[0m  [10/26], [94mLoss[0m : 2.40419
[1mStep[0m  [12/26], [94mLoss[0m : 2.30782
[1mStep[0m  [14/26], [94mLoss[0m : 2.39919
[1mStep[0m  [16/26], [94mLoss[0m : 2.40817
[1mStep[0m  [18/26], [94mLoss[0m : 2.36401
[1mStep[0m  [20/26], [94mLoss[0m : 2.51424
[1mStep[0m  [22/26], [94mLoss[0m : 2.26624
[1mStep[0m  [24/26], [94mLoss[0m : 2.30666

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.374, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40187
[1mStep[0m  [2/26], [94mLoss[0m : 2.36753
[1mStep[0m  [4/26], [94mLoss[0m : 2.18214
[1mStep[0m  [6/26], [94mLoss[0m : 2.17645
[1mStep[0m  [8/26], [94mLoss[0m : 2.25269
[1mStep[0m  [10/26], [94mLoss[0m : 2.48464
[1mStep[0m  [12/26], [94mLoss[0m : 2.30886
[1mStep[0m  [14/26], [94mLoss[0m : 2.30345
[1mStep[0m  [16/26], [94mLoss[0m : 2.30942
[1mStep[0m  [18/26], [94mLoss[0m : 2.35944
[1mStep[0m  [20/26], [94mLoss[0m : 2.37976
[1mStep[0m  [22/26], [94mLoss[0m : 2.42917
[1mStep[0m  [24/26], [94mLoss[0m : 2.33017

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41853
[1mStep[0m  [2/26], [94mLoss[0m : 2.44623
[1mStep[0m  [4/26], [94mLoss[0m : 2.29193
[1mStep[0m  [6/26], [94mLoss[0m : 2.27646
[1mStep[0m  [8/26], [94mLoss[0m : 2.37836
[1mStep[0m  [10/26], [94mLoss[0m : 2.36208
[1mStep[0m  [12/26], [94mLoss[0m : 2.37809
[1mStep[0m  [14/26], [94mLoss[0m : 2.34130
[1mStep[0m  [16/26], [94mLoss[0m : 2.47249
[1mStep[0m  [18/26], [94mLoss[0m : 2.22421
[1mStep[0m  [20/26], [94mLoss[0m : 2.34818
[1mStep[0m  [22/26], [94mLoss[0m : 2.39425
[1mStep[0m  [24/26], [94mLoss[0m : 2.25475

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.356
====================================

Phase 1 - Evaluation MAE:  2.356321774996244
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 2.24820
[1mStep[0m  [2/26], [94mLoss[0m : 2.24332
[1mStep[0m  [4/26], [94mLoss[0m : 2.18881
[1mStep[0m  [6/26], [94mLoss[0m : 2.38451
[1mStep[0m  [8/26], [94mLoss[0m : 2.41621
[1mStep[0m  [10/26], [94mLoss[0m : 2.61424
[1mStep[0m  [12/26], [94mLoss[0m : 2.37408
[1mStep[0m  [14/26], [94mLoss[0m : 2.44128
[1mStep[0m  [16/26], [94mLoss[0m : 2.56115
[1mStep[0m  [18/26], [94mLoss[0m : 2.32594
[1mStep[0m  [20/26], [94mLoss[0m : 2.37555
[1mStep[0m  [22/26], [94mLoss[0m : 2.49210
[1mStep[0m  [24/26], [94mLoss[0m : 2.34643

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33388
[1mStep[0m  [2/26], [94mLoss[0m : 2.39598
[1mStep[0m  [4/26], [94mLoss[0m : 2.35140
[1mStep[0m  [6/26], [94mLoss[0m : 2.23214
[1mStep[0m  [8/26], [94mLoss[0m : 2.17408
[1mStep[0m  [10/26], [94mLoss[0m : 2.29310
[1mStep[0m  [12/26], [94mLoss[0m : 2.18592
[1mStep[0m  [14/26], [94mLoss[0m : 2.32798
[1mStep[0m  [16/26], [94mLoss[0m : 2.25457
[1mStep[0m  [18/26], [94mLoss[0m : 2.56975
[1mStep[0m  [20/26], [94mLoss[0m : 2.31059
[1mStep[0m  [22/26], [94mLoss[0m : 2.36328
[1mStep[0m  [24/26], [94mLoss[0m : 2.23348

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23861
[1mStep[0m  [2/26], [94mLoss[0m : 2.27711
[1mStep[0m  [4/26], [94mLoss[0m : 2.22546
[1mStep[0m  [6/26], [94mLoss[0m : 2.10438
[1mStep[0m  [8/26], [94mLoss[0m : 2.11952
[1mStep[0m  [10/26], [94mLoss[0m : 2.21307
[1mStep[0m  [12/26], [94mLoss[0m : 2.22938
[1mStep[0m  [14/26], [94mLoss[0m : 2.27657
[1mStep[0m  [16/26], [94mLoss[0m : 2.06645
[1mStep[0m  [18/26], [94mLoss[0m : 2.28212
[1mStep[0m  [20/26], [94mLoss[0m : 2.32655
[1mStep[0m  [22/26], [94mLoss[0m : 2.11102
[1mStep[0m  [24/26], [94mLoss[0m : 2.17186

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15383
[1mStep[0m  [2/26], [94mLoss[0m : 2.03994
[1mStep[0m  [4/26], [94mLoss[0m : 2.20913
[1mStep[0m  [6/26], [94mLoss[0m : 2.10857
[1mStep[0m  [8/26], [94mLoss[0m : 2.31867
[1mStep[0m  [10/26], [94mLoss[0m : 2.09236
[1mStep[0m  [12/26], [94mLoss[0m : 2.11985
[1mStep[0m  [14/26], [94mLoss[0m : 2.03268
[1mStep[0m  [16/26], [94mLoss[0m : 2.03926
[1mStep[0m  [18/26], [94mLoss[0m : 2.08750
[1mStep[0m  [20/26], [94mLoss[0m : 2.16865
[1mStep[0m  [22/26], [94mLoss[0m : 2.26317
[1mStep[0m  [24/26], [94mLoss[0m : 2.18721

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.04083
[1mStep[0m  [2/26], [94mLoss[0m : 2.15513
[1mStep[0m  [4/26], [94mLoss[0m : 1.96817
[1mStep[0m  [6/26], [94mLoss[0m : 2.05588
[1mStep[0m  [8/26], [94mLoss[0m : 2.11954
[1mStep[0m  [10/26], [94mLoss[0m : 2.13396
[1mStep[0m  [12/26], [94mLoss[0m : 1.91100
[1mStep[0m  [14/26], [94mLoss[0m : 2.02729
[1mStep[0m  [16/26], [94mLoss[0m : 2.02447
[1mStep[0m  [18/26], [94mLoss[0m : 2.10790
[1mStep[0m  [20/26], [94mLoss[0m : 2.03216
[1mStep[0m  [22/26], [94mLoss[0m : 2.20018
[1mStep[0m  [24/26], [94mLoss[0m : 2.07605

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.92570
[1mStep[0m  [2/26], [94mLoss[0m : 1.96016
[1mStep[0m  [4/26], [94mLoss[0m : 1.91467
[1mStep[0m  [6/26], [94mLoss[0m : 2.00427
[1mStep[0m  [8/26], [94mLoss[0m : 2.12954
[1mStep[0m  [10/26], [94mLoss[0m : 1.91488
[1mStep[0m  [12/26], [94mLoss[0m : 1.92188
[1mStep[0m  [14/26], [94mLoss[0m : 2.14103
[1mStep[0m  [16/26], [94mLoss[0m : 1.99712
[1mStep[0m  [18/26], [94mLoss[0m : 2.02977
[1mStep[0m  [20/26], [94mLoss[0m : 1.93273
[1mStep[0m  [22/26], [94mLoss[0m : 1.95823
[1mStep[0m  [24/26], [94mLoss[0m : 2.06916

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.79842
[1mStep[0m  [2/26], [94mLoss[0m : 2.03615
[1mStep[0m  [4/26], [94mLoss[0m : 1.90956
[1mStep[0m  [6/26], [94mLoss[0m : 1.94400
[1mStep[0m  [8/26], [94mLoss[0m : 1.91825
[1mStep[0m  [10/26], [94mLoss[0m : 1.88574
[1mStep[0m  [12/26], [94mLoss[0m : 1.93229
[1mStep[0m  [14/26], [94mLoss[0m : 1.97801
[1mStep[0m  [16/26], [94mLoss[0m : 2.04856
[1mStep[0m  [18/26], [94mLoss[0m : 2.08100
[1mStep[0m  [20/26], [94mLoss[0m : 1.88467
[1mStep[0m  [22/26], [94mLoss[0m : 1.99073
[1mStep[0m  [24/26], [94mLoss[0m : 1.93580

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93052
[1mStep[0m  [2/26], [94mLoss[0m : 1.80232
[1mStep[0m  [4/26], [94mLoss[0m : 1.80534
[1mStep[0m  [6/26], [94mLoss[0m : 1.96864
[1mStep[0m  [8/26], [94mLoss[0m : 1.82681
[1mStep[0m  [10/26], [94mLoss[0m : 1.96734
[1mStep[0m  [12/26], [94mLoss[0m : 1.94720
[1mStep[0m  [14/26], [94mLoss[0m : 1.63942
[1mStep[0m  [16/26], [94mLoss[0m : 1.83514
[1mStep[0m  [18/26], [94mLoss[0m : 1.92029
[1mStep[0m  [20/26], [94mLoss[0m : 1.88080
[1mStep[0m  [22/26], [94mLoss[0m : 1.94502
[1mStep[0m  [24/26], [94mLoss[0m : 1.85009

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.77905
[1mStep[0m  [2/26], [94mLoss[0m : 1.76907
[1mStep[0m  [4/26], [94mLoss[0m : 1.86216
[1mStep[0m  [6/26], [94mLoss[0m : 1.81943
[1mStep[0m  [8/26], [94mLoss[0m : 1.87447
[1mStep[0m  [10/26], [94mLoss[0m : 1.70364
[1mStep[0m  [12/26], [94mLoss[0m : 1.81295
[1mStep[0m  [14/26], [94mLoss[0m : 1.84090
[1mStep[0m  [16/26], [94mLoss[0m : 1.79073
[1mStep[0m  [18/26], [94mLoss[0m : 1.68206
[1mStep[0m  [20/26], [94mLoss[0m : 1.87551
[1mStep[0m  [22/26], [94mLoss[0m : 1.82083
[1mStep[0m  [24/26], [94mLoss[0m : 1.99140

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76471
[1mStep[0m  [2/26], [94mLoss[0m : 1.71620
[1mStep[0m  [4/26], [94mLoss[0m : 1.70391
[1mStep[0m  [6/26], [94mLoss[0m : 1.78506
[1mStep[0m  [8/26], [94mLoss[0m : 1.73357
[1mStep[0m  [10/26], [94mLoss[0m : 1.58182
[1mStep[0m  [12/26], [94mLoss[0m : 1.69113
[1mStep[0m  [14/26], [94mLoss[0m : 1.71044
[1mStep[0m  [16/26], [94mLoss[0m : 1.84806
[1mStep[0m  [18/26], [94mLoss[0m : 1.76794
[1mStep[0m  [20/26], [94mLoss[0m : 1.72371
[1mStep[0m  [22/26], [94mLoss[0m : 1.70758
[1mStep[0m  [24/26], [94mLoss[0m : 1.83437

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73612
[1mStep[0m  [2/26], [94mLoss[0m : 1.76195
[1mStep[0m  [4/26], [94mLoss[0m : 1.59581
[1mStep[0m  [6/26], [94mLoss[0m : 1.66368
[1mStep[0m  [8/26], [94mLoss[0m : 1.76296
[1mStep[0m  [10/26], [94mLoss[0m : 1.71296
[1mStep[0m  [12/26], [94mLoss[0m : 1.74217
[1mStep[0m  [14/26], [94mLoss[0m : 1.65130
[1mStep[0m  [16/26], [94mLoss[0m : 1.71473
[1mStep[0m  [18/26], [94mLoss[0m : 1.65062
[1mStep[0m  [20/26], [94mLoss[0m : 1.69260
[1mStep[0m  [22/26], [94mLoss[0m : 1.71857
[1mStep[0m  [24/26], [94mLoss[0m : 1.78839

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.64267
[1mStep[0m  [2/26], [94mLoss[0m : 1.64814
[1mStep[0m  [4/26], [94mLoss[0m : 1.51665
[1mStep[0m  [6/26], [94mLoss[0m : 1.57648
[1mStep[0m  [8/26], [94mLoss[0m : 1.69823
[1mStep[0m  [10/26], [94mLoss[0m : 1.58090
[1mStep[0m  [12/26], [94mLoss[0m : 1.67446
[1mStep[0m  [14/26], [94mLoss[0m : 1.62456
[1mStep[0m  [16/26], [94mLoss[0m : 1.66599
[1mStep[0m  [18/26], [94mLoss[0m : 1.65999
[1mStep[0m  [20/26], [94mLoss[0m : 1.69956
[1mStep[0m  [22/26], [94mLoss[0m : 1.84492
[1mStep[0m  [24/26], [94mLoss[0m : 1.63009

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.47835
[1mStep[0m  [2/26], [94mLoss[0m : 1.47663
[1mStep[0m  [4/26], [94mLoss[0m : 1.56497
[1mStep[0m  [6/26], [94mLoss[0m : 1.63656
[1mStep[0m  [8/26], [94mLoss[0m : 1.54842
[1mStep[0m  [10/26], [94mLoss[0m : 1.61061
[1mStep[0m  [12/26], [94mLoss[0m : 1.72970
[1mStep[0m  [14/26], [94mLoss[0m : 1.61386
[1mStep[0m  [16/26], [94mLoss[0m : 1.68193
[1mStep[0m  [18/26], [94mLoss[0m : 1.53725
[1mStep[0m  [20/26], [94mLoss[0m : 1.64883
[1mStep[0m  [22/26], [94mLoss[0m : 1.57554
[1mStep[0m  [24/26], [94mLoss[0m : 1.61823

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.58746
[1mStep[0m  [2/26], [94mLoss[0m : 1.46636
[1mStep[0m  [4/26], [94mLoss[0m : 1.50326
[1mStep[0m  [6/26], [94mLoss[0m : 1.41641
[1mStep[0m  [8/26], [94mLoss[0m : 1.63146
[1mStep[0m  [10/26], [94mLoss[0m : 1.57022
[1mStep[0m  [12/26], [94mLoss[0m : 1.45529
[1mStep[0m  [14/26], [94mLoss[0m : 1.69032
[1mStep[0m  [16/26], [94mLoss[0m : 1.66259
[1mStep[0m  [18/26], [94mLoss[0m : 1.64196
[1mStep[0m  [20/26], [94mLoss[0m : 1.61690
[1mStep[0m  [22/26], [94mLoss[0m : 1.48785
[1mStep[0m  [24/26], [94mLoss[0m : 1.61375

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.569, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.59096
[1mStep[0m  [2/26], [94mLoss[0m : 1.44105
[1mStep[0m  [4/26], [94mLoss[0m : 1.48970
[1mStep[0m  [6/26], [94mLoss[0m : 1.49194
[1mStep[0m  [8/26], [94mLoss[0m : 1.55239
[1mStep[0m  [10/26], [94mLoss[0m : 1.55975
[1mStep[0m  [12/26], [94mLoss[0m : 1.44056
[1mStep[0m  [14/26], [94mLoss[0m : 1.57853
[1mStep[0m  [16/26], [94mLoss[0m : 1.49946
[1mStep[0m  [18/26], [94mLoss[0m : 1.48953
[1mStep[0m  [20/26], [94mLoss[0m : 1.43083
[1mStep[0m  [22/26], [94mLoss[0m : 1.57936
[1mStep[0m  [24/26], [94mLoss[0m : 1.52418

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.519, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.53945
[1mStep[0m  [2/26], [94mLoss[0m : 1.41599
[1mStep[0m  [4/26], [94mLoss[0m : 1.55246
[1mStep[0m  [6/26], [94mLoss[0m : 1.52453
[1mStep[0m  [8/26], [94mLoss[0m : 1.52576
[1mStep[0m  [10/26], [94mLoss[0m : 1.50935
[1mStep[0m  [12/26], [94mLoss[0m : 1.52344
[1mStep[0m  [14/26], [94mLoss[0m : 1.52550
[1mStep[0m  [16/26], [94mLoss[0m : 1.53212
[1mStep[0m  [18/26], [94mLoss[0m : 1.50200
[1mStep[0m  [20/26], [94mLoss[0m : 1.47465
[1mStep[0m  [22/26], [94mLoss[0m : 1.41360
[1mStep[0m  [24/26], [94mLoss[0m : 1.41222

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.509, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.44860
[1mStep[0m  [2/26], [94mLoss[0m : 1.30881
[1mStep[0m  [4/26], [94mLoss[0m : 1.48272
[1mStep[0m  [6/26], [94mLoss[0m : 1.43787
[1mStep[0m  [8/26], [94mLoss[0m : 1.54012
[1mStep[0m  [10/26], [94mLoss[0m : 1.37540
[1mStep[0m  [12/26], [94mLoss[0m : 1.50584
[1mStep[0m  [14/26], [94mLoss[0m : 1.49132
[1mStep[0m  [16/26], [94mLoss[0m : 1.39486
[1mStep[0m  [18/26], [94mLoss[0m : 1.51405
[1mStep[0m  [20/26], [94mLoss[0m : 1.44245
[1mStep[0m  [22/26], [94mLoss[0m : 1.50834
[1mStep[0m  [24/26], [94mLoss[0m : 1.51430

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.615, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.40764
[1mStep[0m  [2/26], [94mLoss[0m : 1.36834
[1mStep[0m  [4/26], [94mLoss[0m : 1.56572
[1mStep[0m  [6/26], [94mLoss[0m : 1.44410
[1mStep[0m  [8/26], [94mLoss[0m : 1.48290
[1mStep[0m  [10/26], [94mLoss[0m : 1.48787
[1mStep[0m  [12/26], [94mLoss[0m : 1.50069
[1mStep[0m  [14/26], [94mLoss[0m : 1.47473
[1mStep[0m  [16/26], [94mLoss[0m : 1.54362
[1mStep[0m  [18/26], [94mLoss[0m : 1.41016
[1mStep[0m  [20/26], [94mLoss[0m : 1.37962
[1mStep[0m  [22/26], [94mLoss[0m : 1.45798
[1mStep[0m  [24/26], [94mLoss[0m : 1.34463

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.31669
[1mStep[0m  [2/26], [94mLoss[0m : 1.38685
[1mStep[0m  [4/26], [94mLoss[0m : 1.29829
[1mStep[0m  [6/26], [94mLoss[0m : 1.39888
[1mStep[0m  [8/26], [94mLoss[0m : 1.39452
[1mStep[0m  [10/26], [94mLoss[0m : 1.50215
[1mStep[0m  [12/26], [94mLoss[0m : 1.31003
[1mStep[0m  [14/26], [94mLoss[0m : 1.48820
[1mStep[0m  [16/26], [94mLoss[0m : 1.38845
[1mStep[0m  [18/26], [94mLoss[0m : 1.42022
[1mStep[0m  [20/26], [94mLoss[0m : 1.45257
[1mStep[0m  [22/26], [94mLoss[0m : 1.42377
[1mStep[0m  [24/26], [94mLoss[0m : 1.49965

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.391, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.31887
[1mStep[0m  [2/26], [94mLoss[0m : 1.37068
[1mStep[0m  [4/26], [94mLoss[0m : 1.33171
[1mStep[0m  [6/26], [94mLoss[0m : 1.25408
[1mStep[0m  [8/26], [94mLoss[0m : 1.38815
[1mStep[0m  [10/26], [94mLoss[0m : 1.31019
[1mStep[0m  [12/26], [94mLoss[0m : 1.38402
[1mStep[0m  [14/26], [94mLoss[0m : 1.35416
[1mStep[0m  [16/26], [94mLoss[0m : 1.31940
[1mStep[0m  [18/26], [94mLoss[0m : 1.41999
[1mStep[0m  [20/26], [94mLoss[0m : 1.29695
[1mStep[0m  [22/26], [94mLoss[0m : 1.35841
[1mStep[0m  [24/26], [94mLoss[0m : 1.31195

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.359, [92mTest[0m: 2.479, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 19 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.444
====================================

Phase 2 - Evaluation MAE:  2.443701303922213
MAE score P1      2.356322
MAE score P2      2.443701
loss              1.358827
learning_rate         0.01
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 5, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.59571
[1mStep[0m  [5/53], [94mLoss[0m : 10.68887
[1mStep[0m  [10/53], [94mLoss[0m : 10.45320
[1mStep[0m  [15/53], [94mLoss[0m : 9.98875
[1mStep[0m  [20/53], [94mLoss[0m : 8.92361
[1mStep[0m  [25/53], [94mLoss[0m : 8.58076
[1mStep[0m  [30/53], [94mLoss[0m : 8.23593
[1mStep[0m  [35/53], [94mLoss[0m : 6.98321
[1mStep[0m  [40/53], [94mLoss[0m : 6.34407
[1mStep[0m  [45/53], [94mLoss[0m : 5.54608
[1mStep[0m  [50/53], [94mLoss[0m : 4.35997

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.218, [92mTest[0m: 10.802, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.79745
[1mStep[0m  [5/53], [94mLoss[0m : 3.24229
[1mStep[0m  [10/53], [94mLoss[0m : 3.07334
[1mStep[0m  [15/53], [94mLoss[0m : 2.67654
[1mStep[0m  [20/53], [94mLoss[0m : 2.95505
[1mStep[0m  [25/53], [94mLoss[0m : 2.96421
[1mStep[0m  [30/53], [94mLoss[0m : 2.94960
[1mStep[0m  [35/53], [94mLoss[0m : 2.80622
[1mStep[0m  [40/53], [94mLoss[0m : 2.66707
[1mStep[0m  [45/53], [94mLoss[0m : 2.60315
[1mStep[0m  [50/53], [94mLoss[0m : 2.85059

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.917, [92mTest[0m: 3.536, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64008
[1mStep[0m  [5/53], [94mLoss[0m : 2.65892
[1mStep[0m  [10/53], [94mLoss[0m : 2.77771
[1mStep[0m  [15/53], [94mLoss[0m : 2.63390
[1mStep[0m  [20/53], [94mLoss[0m : 2.71132
[1mStep[0m  [25/53], [94mLoss[0m : 2.72550
[1mStep[0m  [30/53], [94mLoss[0m : 2.49917
[1mStep[0m  [35/53], [94mLoss[0m : 2.53106
[1mStep[0m  [40/53], [94mLoss[0m : 2.52214
[1mStep[0m  [45/53], [94mLoss[0m : 2.47234
[1mStep[0m  [50/53], [94mLoss[0m : 2.49700

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.81160
[1mStep[0m  [5/53], [94mLoss[0m : 2.45907
[1mStep[0m  [10/53], [94mLoss[0m : 2.55155
[1mStep[0m  [15/53], [94mLoss[0m : 2.39782
[1mStep[0m  [20/53], [94mLoss[0m : 2.64870
[1mStep[0m  [25/53], [94mLoss[0m : 2.40410
[1mStep[0m  [30/53], [94mLoss[0m : 2.41420
[1mStep[0m  [35/53], [94mLoss[0m : 2.78454
[1mStep[0m  [40/53], [94mLoss[0m : 2.73587
[1mStep[0m  [45/53], [94mLoss[0m : 2.64239
[1mStep[0m  [50/53], [94mLoss[0m : 2.63331

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44790
[1mStep[0m  [5/53], [94mLoss[0m : 2.61768
[1mStep[0m  [10/53], [94mLoss[0m : 2.53739
[1mStep[0m  [15/53], [94mLoss[0m : 2.52734
[1mStep[0m  [20/53], [94mLoss[0m : 2.51045
[1mStep[0m  [25/53], [94mLoss[0m : 2.76510
[1mStep[0m  [30/53], [94mLoss[0m : 2.67623
[1mStep[0m  [35/53], [94mLoss[0m : 2.39742
[1mStep[0m  [40/53], [94mLoss[0m : 2.59523
[1mStep[0m  [45/53], [94mLoss[0m : 2.52011
[1mStep[0m  [50/53], [94mLoss[0m : 2.46213

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31536
[1mStep[0m  [5/53], [94mLoss[0m : 2.49696
[1mStep[0m  [10/53], [94mLoss[0m : 2.50743
[1mStep[0m  [15/53], [94mLoss[0m : 2.46876
[1mStep[0m  [20/53], [94mLoss[0m : 2.47099
[1mStep[0m  [25/53], [94mLoss[0m : 2.46239
[1mStep[0m  [30/53], [94mLoss[0m : 2.48316
[1mStep[0m  [35/53], [94mLoss[0m : 2.56076
[1mStep[0m  [40/53], [94mLoss[0m : 2.50224
[1mStep[0m  [45/53], [94mLoss[0m : 2.24335
[1mStep[0m  [50/53], [94mLoss[0m : 2.45151

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57553
[1mStep[0m  [5/53], [94mLoss[0m : 2.29110
[1mStep[0m  [10/53], [94mLoss[0m : 2.60772
[1mStep[0m  [15/53], [94mLoss[0m : 2.77824
[1mStep[0m  [20/53], [94mLoss[0m : 2.15209
[1mStep[0m  [25/53], [94mLoss[0m : 2.56539
[1mStep[0m  [30/53], [94mLoss[0m : 2.49670
[1mStep[0m  [35/53], [94mLoss[0m : 2.44737
[1mStep[0m  [40/53], [94mLoss[0m : 2.46500
[1mStep[0m  [45/53], [94mLoss[0m : 2.46425
[1mStep[0m  [50/53], [94mLoss[0m : 2.55133

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48976
[1mStep[0m  [5/53], [94mLoss[0m : 2.40908
[1mStep[0m  [10/53], [94mLoss[0m : 2.61366
[1mStep[0m  [15/53], [94mLoss[0m : 2.32146
[1mStep[0m  [20/53], [94mLoss[0m : 2.37570
[1mStep[0m  [25/53], [94mLoss[0m : 2.68948
[1mStep[0m  [30/53], [94mLoss[0m : 2.36904
[1mStep[0m  [35/53], [94mLoss[0m : 2.49353
[1mStep[0m  [40/53], [94mLoss[0m : 2.60597
[1mStep[0m  [45/53], [94mLoss[0m : 2.40987
[1mStep[0m  [50/53], [94mLoss[0m : 2.40668

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32107
[1mStep[0m  [5/53], [94mLoss[0m : 2.44428
[1mStep[0m  [10/53], [94mLoss[0m : 2.61981
[1mStep[0m  [15/53], [94mLoss[0m : 2.23628
[1mStep[0m  [20/53], [94mLoss[0m : 2.49701
[1mStep[0m  [25/53], [94mLoss[0m : 2.36212
[1mStep[0m  [30/53], [94mLoss[0m : 2.43623
[1mStep[0m  [35/53], [94mLoss[0m : 2.52283
[1mStep[0m  [40/53], [94mLoss[0m : 2.31167
[1mStep[0m  [45/53], [94mLoss[0m : 2.53215
[1mStep[0m  [50/53], [94mLoss[0m : 2.39925

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39969
[1mStep[0m  [5/53], [94mLoss[0m : 2.53264
[1mStep[0m  [10/53], [94mLoss[0m : 2.48142
[1mStep[0m  [15/53], [94mLoss[0m : 2.27562
[1mStep[0m  [20/53], [94mLoss[0m : 2.38001
[1mStep[0m  [25/53], [94mLoss[0m : 2.31508
[1mStep[0m  [30/53], [94mLoss[0m : 2.55822
[1mStep[0m  [35/53], [94mLoss[0m : 2.55799
[1mStep[0m  [40/53], [94mLoss[0m : 2.54476
[1mStep[0m  [45/53], [94mLoss[0m : 2.41541
[1mStep[0m  [50/53], [94mLoss[0m : 2.44657

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52904
[1mStep[0m  [5/53], [94mLoss[0m : 2.51273
[1mStep[0m  [10/53], [94mLoss[0m : 2.52497
[1mStep[0m  [15/53], [94mLoss[0m : 2.57324
[1mStep[0m  [20/53], [94mLoss[0m : 2.57221
[1mStep[0m  [25/53], [94mLoss[0m : 2.38741
[1mStep[0m  [30/53], [94mLoss[0m : 2.38802
[1mStep[0m  [35/53], [94mLoss[0m : 2.51798
[1mStep[0m  [40/53], [94mLoss[0m : 2.50298
[1mStep[0m  [45/53], [94mLoss[0m : 2.45152
[1mStep[0m  [50/53], [94mLoss[0m : 2.53165

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37067
[1mStep[0m  [5/53], [94mLoss[0m : 2.43049
[1mStep[0m  [10/53], [94mLoss[0m : 2.48655
[1mStep[0m  [15/53], [94mLoss[0m : 2.44083
[1mStep[0m  [20/53], [94mLoss[0m : 2.47798
[1mStep[0m  [25/53], [94mLoss[0m : 2.32400
[1mStep[0m  [30/53], [94mLoss[0m : 2.30902
[1mStep[0m  [35/53], [94mLoss[0m : 2.38138
[1mStep[0m  [40/53], [94mLoss[0m : 2.44499
[1mStep[0m  [45/53], [94mLoss[0m : 2.41263
[1mStep[0m  [50/53], [94mLoss[0m : 2.39805

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49117
[1mStep[0m  [5/53], [94mLoss[0m : 2.52423
[1mStep[0m  [10/53], [94mLoss[0m : 2.40238
[1mStep[0m  [15/53], [94mLoss[0m : 2.42200
[1mStep[0m  [20/53], [94mLoss[0m : 2.35169
[1mStep[0m  [25/53], [94mLoss[0m : 2.48853
[1mStep[0m  [30/53], [94mLoss[0m : 2.34706
[1mStep[0m  [35/53], [94mLoss[0m : 2.42573
[1mStep[0m  [40/53], [94mLoss[0m : 2.37911
[1mStep[0m  [45/53], [94mLoss[0m : 2.29387
[1mStep[0m  [50/53], [94mLoss[0m : 2.44087

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45608
[1mStep[0m  [5/53], [94mLoss[0m : 2.37735
[1mStep[0m  [10/53], [94mLoss[0m : 2.31088
[1mStep[0m  [15/53], [94mLoss[0m : 2.55946
[1mStep[0m  [20/53], [94mLoss[0m : 2.27027
[1mStep[0m  [25/53], [94mLoss[0m : 2.49939
[1mStep[0m  [30/53], [94mLoss[0m : 2.40668
[1mStep[0m  [35/53], [94mLoss[0m : 2.40542
[1mStep[0m  [40/53], [94mLoss[0m : 2.44625
[1mStep[0m  [45/53], [94mLoss[0m : 2.41685
[1mStep[0m  [50/53], [94mLoss[0m : 2.43727

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58234
[1mStep[0m  [5/53], [94mLoss[0m : 2.27882
[1mStep[0m  [10/53], [94mLoss[0m : 2.44093
[1mStep[0m  [15/53], [94mLoss[0m : 2.33598
[1mStep[0m  [20/53], [94mLoss[0m : 2.23667
[1mStep[0m  [25/53], [94mLoss[0m : 2.41732
[1mStep[0m  [30/53], [94mLoss[0m : 2.38228
[1mStep[0m  [35/53], [94mLoss[0m : 2.33104
[1mStep[0m  [40/53], [94mLoss[0m : 2.35985
[1mStep[0m  [45/53], [94mLoss[0m : 2.27295
[1mStep[0m  [50/53], [94mLoss[0m : 2.45641

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41419
[1mStep[0m  [5/53], [94mLoss[0m : 2.36124
[1mStep[0m  [10/53], [94mLoss[0m : 2.43843
[1mStep[0m  [15/53], [94mLoss[0m : 2.44425
[1mStep[0m  [20/53], [94mLoss[0m : 2.54553
[1mStep[0m  [25/53], [94mLoss[0m : 2.37982
[1mStep[0m  [30/53], [94mLoss[0m : 2.48378
[1mStep[0m  [35/53], [94mLoss[0m : 2.23772
[1mStep[0m  [40/53], [94mLoss[0m : 2.50760
[1mStep[0m  [45/53], [94mLoss[0m : 2.55153
[1mStep[0m  [50/53], [94mLoss[0m : 2.68585

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37597
[1mStep[0m  [5/53], [94mLoss[0m : 2.19024
[1mStep[0m  [10/53], [94mLoss[0m : 2.54064
[1mStep[0m  [15/53], [94mLoss[0m : 2.36709
[1mStep[0m  [20/53], [94mLoss[0m : 2.41332
[1mStep[0m  [25/53], [94mLoss[0m : 2.56450
[1mStep[0m  [30/53], [94mLoss[0m : 2.46619
[1mStep[0m  [35/53], [94mLoss[0m : 2.43134
[1mStep[0m  [40/53], [94mLoss[0m : 2.30178
[1mStep[0m  [45/53], [94mLoss[0m : 2.37467
[1mStep[0m  [50/53], [94mLoss[0m : 2.43667

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.23352
[1mStep[0m  [5/53], [94mLoss[0m : 2.27130
[1mStep[0m  [10/53], [94mLoss[0m : 2.44855
[1mStep[0m  [15/53], [94mLoss[0m : 2.53135
[1mStep[0m  [20/53], [94mLoss[0m : 2.30913
[1mStep[0m  [25/53], [94mLoss[0m : 2.52445
[1mStep[0m  [30/53], [94mLoss[0m : 2.27639
[1mStep[0m  [35/53], [94mLoss[0m : 2.27305
[1mStep[0m  [40/53], [94mLoss[0m : 2.48939
[1mStep[0m  [45/53], [94mLoss[0m : 2.33177
[1mStep[0m  [50/53], [94mLoss[0m : 2.46255

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44601
[1mStep[0m  [5/53], [94mLoss[0m : 2.45156
[1mStep[0m  [10/53], [94mLoss[0m : 2.32423
[1mStep[0m  [15/53], [94mLoss[0m : 2.38644
[1mStep[0m  [20/53], [94mLoss[0m : 2.44793
[1mStep[0m  [25/53], [94mLoss[0m : 2.13576
[1mStep[0m  [30/53], [94mLoss[0m : 2.36348
[1mStep[0m  [35/53], [94mLoss[0m : 2.47130
[1mStep[0m  [40/53], [94mLoss[0m : 2.43241
[1mStep[0m  [45/53], [94mLoss[0m : 2.42042
[1mStep[0m  [50/53], [94mLoss[0m : 2.28679

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26979
[1mStep[0m  [5/53], [94mLoss[0m : 2.57996
[1mStep[0m  [10/53], [94mLoss[0m : 2.28345
[1mStep[0m  [15/53], [94mLoss[0m : 2.56221
[1mStep[0m  [20/53], [94mLoss[0m : 2.30642
[1mStep[0m  [25/53], [94mLoss[0m : 2.43635
[1mStep[0m  [30/53], [94mLoss[0m : 2.30795
[1mStep[0m  [35/53], [94mLoss[0m : 2.45777
[1mStep[0m  [40/53], [94mLoss[0m : 2.53049
[1mStep[0m  [45/53], [94mLoss[0m : 2.40915
[1mStep[0m  [50/53], [94mLoss[0m : 2.18732

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40735
[1mStep[0m  [5/53], [94mLoss[0m : 2.43948
[1mStep[0m  [10/53], [94mLoss[0m : 2.39899
[1mStep[0m  [15/53], [94mLoss[0m : 2.14342
[1mStep[0m  [20/53], [94mLoss[0m : 2.17776
[1mStep[0m  [25/53], [94mLoss[0m : 2.56595
[1mStep[0m  [30/53], [94mLoss[0m : 2.26865
[1mStep[0m  [35/53], [94mLoss[0m : 2.30798
[1mStep[0m  [40/53], [94mLoss[0m : 2.38500
[1mStep[0m  [45/53], [94mLoss[0m : 2.17719
[1mStep[0m  [50/53], [94mLoss[0m : 2.23893

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52339
[1mStep[0m  [5/53], [94mLoss[0m : 2.22417
[1mStep[0m  [10/53], [94mLoss[0m : 2.30771
[1mStep[0m  [15/53], [94mLoss[0m : 2.36555
[1mStep[0m  [20/53], [94mLoss[0m : 2.50527
[1mStep[0m  [25/53], [94mLoss[0m : 2.31407
[1mStep[0m  [30/53], [94mLoss[0m : 2.39298
[1mStep[0m  [35/53], [94mLoss[0m : 2.57038
[1mStep[0m  [40/53], [94mLoss[0m : 2.29635
[1mStep[0m  [45/53], [94mLoss[0m : 2.37862
[1mStep[0m  [50/53], [94mLoss[0m : 2.25486

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32197
[1mStep[0m  [5/53], [94mLoss[0m : 2.23321
[1mStep[0m  [10/53], [94mLoss[0m : 2.46269
[1mStep[0m  [15/53], [94mLoss[0m : 2.21759
[1mStep[0m  [20/53], [94mLoss[0m : 2.37617
[1mStep[0m  [25/53], [94mLoss[0m : 2.53937
[1mStep[0m  [30/53], [94mLoss[0m : 2.30857
[1mStep[0m  [35/53], [94mLoss[0m : 2.26218
[1mStep[0m  [40/53], [94mLoss[0m : 2.31663
[1mStep[0m  [45/53], [94mLoss[0m : 2.43448
[1mStep[0m  [50/53], [94mLoss[0m : 2.55540

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.379, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41853
[1mStep[0m  [5/53], [94mLoss[0m : 2.24024
[1mStep[0m  [10/53], [94mLoss[0m : 2.47989
[1mStep[0m  [15/53], [94mLoss[0m : 2.44955
[1mStep[0m  [20/53], [94mLoss[0m : 2.28710
[1mStep[0m  [25/53], [94mLoss[0m : 2.34446
[1mStep[0m  [30/53], [94mLoss[0m : 2.56100
[1mStep[0m  [35/53], [94mLoss[0m : 2.66700
[1mStep[0m  [40/53], [94mLoss[0m : 2.14163
[1mStep[0m  [45/53], [94mLoss[0m : 2.32830
[1mStep[0m  [50/53], [94mLoss[0m : 2.50970

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.400, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31485
[1mStep[0m  [5/53], [94mLoss[0m : 2.46740
[1mStep[0m  [10/53], [94mLoss[0m : 2.25702
[1mStep[0m  [15/53], [94mLoss[0m : 2.39780
[1mStep[0m  [20/53], [94mLoss[0m : 2.29642
[1mStep[0m  [25/53], [94mLoss[0m : 2.37747
[1mStep[0m  [30/53], [94mLoss[0m : 2.52243
[1mStep[0m  [35/53], [94mLoss[0m : 2.32435
[1mStep[0m  [40/53], [94mLoss[0m : 2.39967
[1mStep[0m  [45/53], [94mLoss[0m : 2.24662
[1mStep[0m  [50/53], [94mLoss[0m : 2.23997

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.388, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25755
[1mStep[0m  [5/53], [94mLoss[0m : 2.54330
[1mStep[0m  [10/53], [94mLoss[0m : 2.21702
[1mStep[0m  [15/53], [94mLoss[0m : 2.32074
[1mStep[0m  [20/53], [94mLoss[0m : 2.58593
[1mStep[0m  [25/53], [94mLoss[0m : 2.30063
[1mStep[0m  [30/53], [94mLoss[0m : 2.59275
[1mStep[0m  [35/53], [94mLoss[0m : 2.07314
[1mStep[0m  [40/53], [94mLoss[0m : 2.38287
[1mStep[0m  [45/53], [94mLoss[0m : 2.23416
[1mStep[0m  [50/53], [94mLoss[0m : 2.33403

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53671
[1mStep[0m  [5/53], [94mLoss[0m : 2.18647
[1mStep[0m  [10/53], [94mLoss[0m : 2.57926
[1mStep[0m  [15/53], [94mLoss[0m : 2.45639
[1mStep[0m  [20/53], [94mLoss[0m : 2.32956
[1mStep[0m  [25/53], [94mLoss[0m : 2.48092
[1mStep[0m  [30/53], [94mLoss[0m : 2.39951
[1mStep[0m  [35/53], [94mLoss[0m : 2.33650
[1mStep[0m  [40/53], [94mLoss[0m : 2.32880
[1mStep[0m  [45/53], [94mLoss[0m : 2.45711
[1mStep[0m  [50/53], [94mLoss[0m : 2.52805

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33607
[1mStep[0m  [5/53], [94mLoss[0m : 2.49401
[1mStep[0m  [10/53], [94mLoss[0m : 2.27797
[1mStep[0m  [15/53], [94mLoss[0m : 2.35054
[1mStep[0m  [20/53], [94mLoss[0m : 2.59399
[1mStep[0m  [25/53], [94mLoss[0m : 2.17089
[1mStep[0m  [30/53], [94mLoss[0m : 2.53144
[1mStep[0m  [35/53], [94mLoss[0m : 2.56630
[1mStep[0m  [40/53], [94mLoss[0m : 2.38157
[1mStep[0m  [45/53], [94mLoss[0m : 2.59008
[1mStep[0m  [50/53], [94mLoss[0m : 2.20819

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61137
[1mStep[0m  [5/53], [94mLoss[0m : 2.29903
[1mStep[0m  [10/53], [94mLoss[0m : 2.50565
[1mStep[0m  [15/53], [94mLoss[0m : 2.40164
[1mStep[0m  [20/53], [94mLoss[0m : 2.33874
[1mStep[0m  [25/53], [94mLoss[0m : 2.32087
[1mStep[0m  [30/53], [94mLoss[0m : 2.33063
[1mStep[0m  [35/53], [94mLoss[0m : 2.26085
[1mStep[0m  [40/53], [94mLoss[0m : 2.53602
[1mStep[0m  [45/53], [94mLoss[0m : 2.45943
[1mStep[0m  [50/53], [94mLoss[0m : 2.38441

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.360, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40573
[1mStep[0m  [5/53], [94mLoss[0m : 2.55571
[1mStep[0m  [10/53], [94mLoss[0m : 2.40830
[1mStep[0m  [15/53], [94mLoss[0m : 2.40276
[1mStep[0m  [20/53], [94mLoss[0m : 2.34597
[1mStep[0m  [25/53], [94mLoss[0m : 2.45741
[1mStep[0m  [30/53], [94mLoss[0m : 2.30809
[1mStep[0m  [35/53], [94mLoss[0m : 2.33189
[1mStep[0m  [40/53], [94mLoss[0m : 2.31579
[1mStep[0m  [45/53], [94mLoss[0m : 2.43708
[1mStep[0m  [50/53], [94mLoss[0m : 2.36882

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.384, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.373
====================================

Phase 1 - Evaluation MAE:  2.3727515935897827
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.41298
[1mStep[0m  [5/53], [94mLoss[0m : 2.47393
[1mStep[0m  [10/53], [94mLoss[0m : 2.59348
[1mStep[0m  [15/53], [94mLoss[0m : 2.41530
[1mStep[0m  [20/53], [94mLoss[0m : 2.44705
[1mStep[0m  [25/53], [94mLoss[0m : 2.51833
[1mStep[0m  [30/53], [94mLoss[0m : 2.37673
[1mStep[0m  [35/53], [94mLoss[0m : 2.56084
[1mStep[0m  [40/53], [94mLoss[0m : 2.52301
[1mStep[0m  [45/53], [94mLoss[0m : 2.51205
[1mStep[0m  [50/53], [94mLoss[0m : 2.49939

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25617
[1mStep[0m  [5/53], [94mLoss[0m : 2.24423
[1mStep[0m  [10/53], [94mLoss[0m : 2.20235
[1mStep[0m  [15/53], [94mLoss[0m : 2.25373
[1mStep[0m  [20/53], [94mLoss[0m : 2.33947
[1mStep[0m  [25/53], [94mLoss[0m : 2.20531
[1mStep[0m  [30/53], [94mLoss[0m : 2.25716
[1mStep[0m  [35/53], [94mLoss[0m : 2.18685
[1mStep[0m  [40/53], [94mLoss[0m : 2.33383
[1mStep[0m  [45/53], [94mLoss[0m : 2.38716
[1mStep[0m  [50/53], [94mLoss[0m : 2.32599

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08405
[1mStep[0m  [5/53], [94mLoss[0m : 2.21549
[1mStep[0m  [10/53], [94mLoss[0m : 2.06730
[1mStep[0m  [15/53], [94mLoss[0m : 2.11966
[1mStep[0m  [20/53], [94mLoss[0m : 2.19354
[1mStep[0m  [25/53], [94mLoss[0m : 2.43444
[1mStep[0m  [30/53], [94mLoss[0m : 2.25686
[1mStep[0m  [35/53], [94mLoss[0m : 2.37348
[1mStep[0m  [40/53], [94mLoss[0m : 2.36226
[1mStep[0m  [45/53], [94mLoss[0m : 1.99819
[1mStep[0m  [50/53], [94mLoss[0m : 2.34786

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21207
[1mStep[0m  [5/53], [94mLoss[0m : 2.15878
[1mStep[0m  [10/53], [94mLoss[0m : 2.05427
[1mStep[0m  [15/53], [94mLoss[0m : 2.16903
[1mStep[0m  [20/53], [94mLoss[0m : 2.14453
[1mStep[0m  [25/53], [94mLoss[0m : 2.19978
[1mStep[0m  [30/53], [94mLoss[0m : 2.33002
[1mStep[0m  [35/53], [94mLoss[0m : 2.27043
[1mStep[0m  [40/53], [94mLoss[0m : 2.25079
[1mStep[0m  [45/53], [94mLoss[0m : 2.08707
[1mStep[0m  [50/53], [94mLoss[0m : 2.13547

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93545
[1mStep[0m  [5/53], [94mLoss[0m : 2.00688
[1mStep[0m  [10/53], [94mLoss[0m : 2.02860
[1mStep[0m  [15/53], [94mLoss[0m : 1.91844
[1mStep[0m  [20/53], [94mLoss[0m : 2.21440
[1mStep[0m  [25/53], [94mLoss[0m : 1.96082
[1mStep[0m  [30/53], [94mLoss[0m : 2.06980
[1mStep[0m  [35/53], [94mLoss[0m : 2.18723
[1mStep[0m  [40/53], [94mLoss[0m : 1.99511
[1mStep[0m  [45/53], [94mLoss[0m : 2.01100
[1mStep[0m  [50/53], [94mLoss[0m : 1.99641

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.96591
[1mStep[0m  [5/53], [94mLoss[0m : 1.85815
[1mStep[0m  [10/53], [94mLoss[0m : 2.07613
[1mStep[0m  [15/53], [94mLoss[0m : 1.93848
[1mStep[0m  [20/53], [94mLoss[0m : 1.99917
[1mStep[0m  [25/53], [94mLoss[0m : 2.14423
[1mStep[0m  [30/53], [94mLoss[0m : 1.93896
[1mStep[0m  [35/53], [94mLoss[0m : 2.21645
[1mStep[0m  [40/53], [94mLoss[0m : 2.06870
[1mStep[0m  [45/53], [94mLoss[0m : 1.98049
[1mStep[0m  [50/53], [94mLoss[0m : 2.05963

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.03690
[1mStep[0m  [5/53], [94mLoss[0m : 2.02254
[1mStep[0m  [10/53], [94mLoss[0m : 1.85452
[1mStep[0m  [15/53], [94mLoss[0m : 1.86166
[1mStep[0m  [20/53], [94mLoss[0m : 1.96217
[1mStep[0m  [25/53], [94mLoss[0m : 2.19324
[1mStep[0m  [30/53], [94mLoss[0m : 1.81823
[1mStep[0m  [35/53], [94mLoss[0m : 1.95953
[1mStep[0m  [40/53], [94mLoss[0m : 2.01093
[1mStep[0m  [45/53], [94mLoss[0m : 2.11741
[1mStep[0m  [50/53], [94mLoss[0m : 1.92785

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.954, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80646
[1mStep[0m  [5/53], [94mLoss[0m : 2.01395
[1mStep[0m  [10/53], [94mLoss[0m : 1.83075
[1mStep[0m  [15/53], [94mLoss[0m : 1.58639
[1mStep[0m  [20/53], [94mLoss[0m : 2.03557
[1mStep[0m  [25/53], [94mLoss[0m : 1.88088
[1mStep[0m  [30/53], [94mLoss[0m : 1.74282
[1mStep[0m  [35/53], [94mLoss[0m : 1.85703
[1mStep[0m  [40/53], [94mLoss[0m : 1.81045
[1mStep[0m  [45/53], [94mLoss[0m : 2.04978
[1mStep[0m  [50/53], [94mLoss[0m : 1.84941

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66817
[1mStep[0m  [5/53], [94mLoss[0m : 1.82380
[1mStep[0m  [10/53], [94mLoss[0m : 1.78689
[1mStep[0m  [15/53], [94mLoss[0m : 1.69856
[1mStep[0m  [20/53], [94mLoss[0m : 1.96934
[1mStep[0m  [25/53], [94mLoss[0m : 1.85502
[1mStep[0m  [30/53], [94mLoss[0m : 1.81432
[1mStep[0m  [35/53], [94mLoss[0m : 1.75876
[1mStep[0m  [40/53], [94mLoss[0m : 1.94454
[1mStep[0m  [45/53], [94mLoss[0m : 1.91300
[1mStep[0m  [50/53], [94mLoss[0m : 1.84110

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79610
[1mStep[0m  [5/53], [94mLoss[0m : 1.61428
[1mStep[0m  [10/53], [94mLoss[0m : 1.67279
[1mStep[0m  [15/53], [94mLoss[0m : 1.85122
[1mStep[0m  [20/53], [94mLoss[0m : 1.76914
[1mStep[0m  [25/53], [94mLoss[0m : 1.84550
[1mStep[0m  [30/53], [94mLoss[0m : 1.86836
[1mStep[0m  [35/53], [94mLoss[0m : 1.66669
[1mStep[0m  [40/53], [94mLoss[0m : 2.00186
[1mStep[0m  [45/53], [94mLoss[0m : 1.91444
[1mStep[0m  [50/53], [94mLoss[0m : 2.03717

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.70270
[1mStep[0m  [5/53], [94mLoss[0m : 1.74436
[1mStep[0m  [10/53], [94mLoss[0m : 1.71104
[1mStep[0m  [15/53], [94mLoss[0m : 1.79047
[1mStep[0m  [20/53], [94mLoss[0m : 1.53124
[1mStep[0m  [25/53], [94mLoss[0m : 1.84056
[1mStep[0m  [30/53], [94mLoss[0m : 1.78215
[1mStep[0m  [35/53], [94mLoss[0m : 1.74425
[1mStep[0m  [40/53], [94mLoss[0m : 1.79375
[1mStep[0m  [45/53], [94mLoss[0m : 1.92527
[1mStep[0m  [50/53], [94mLoss[0m : 1.90653

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73638
[1mStep[0m  [5/53], [94mLoss[0m : 1.66687
[1mStep[0m  [10/53], [94mLoss[0m : 1.58772
[1mStep[0m  [15/53], [94mLoss[0m : 1.77279
[1mStep[0m  [20/53], [94mLoss[0m : 1.65963
[1mStep[0m  [25/53], [94mLoss[0m : 1.58486
[1mStep[0m  [30/53], [94mLoss[0m : 1.85837
[1mStep[0m  [35/53], [94mLoss[0m : 1.68409
[1mStep[0m  [40/53], [94mLoss[0m : 1.66535
[1mStep[0m  [45/53], [94mLoss[0m : 1.77817
[1mStep[0m  [50/53], [94mLoss[0m : 1.83751

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.55986
[1mStep[0m  [5/53], [94mLoss[0m : 1.53821
[1mStep[0m  [10/53], [94mLoss[0m : 1.51079
[1mStep[0m  [15/53], [94mLoss[0m : 1.71264
[1mStep[0m  [20/53], [94mLoss[0m : 1.61511
[1mStep[0m  [25/53], [94mLoss[0m : 1.60834
[1mStep[0m  [30/53], [94mLoss[0m : 1.65919
[1mStep[0m  [35/53], [94mLoss[0m : 1.65445
[1mStep[0m  [40/53], [94mLoss[0m : 1.93310
[1mStep[0m  [45/53], [94mLoss[0m : 1.89314
[1mStep[0m  [50/53], [94mLoss[0m : 1.80392

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64617
[1mStep[0m  [5/53], [94mLoss[0m : 1.50819
[1mStep[0m  [10/53], [94mLoss[0m : 1.47558
[1mStep[0m  [15/53], [94mLoss[0m : 1.58200
[1mStep[0m  [20/53], [94mLoss[0m : 1.52675
[1mStep[0m  [25/53], [94mLoss[0m : 1.54443
[1mStep[0m  [30/53], [94mLoss[0m : 1.54639
[1mStep[0m  [35/53], [94mLoss[0m : 1.73712
[1mStep[0m  [40/53], [94mLoss[0m : 1.59253
[1mStep[0m  [45/53], [94mLoss[0m : 1.59726
[1mStep[0m  [50/53], [94mLoss[0m : 1.74269

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.60021
[1mStep[0m  [5/53], [94mLoss[0m : 1.54301
[1mStep[0m  [10/53], [94mLoss[0m : 1.64203
[1mStep[0m  [15/53], [94mLoss[0m : 1.47969
[1mStep[0m  [20/53], [94mLoss[0m : 1.72308
[1mStep[0m  [25/53], [94mLoss[0m : 1.60134
[1mStep[0m  [30/53], [94mLoss[0m : 1.71439
[1mStep[0m  [35/53], [94mLoss[0m : 1.54643
[1mStep[0m  [40/53], [94mLoss[0m : 1.52966
[1mStep[0m  [45/53], [94mLoss[0m : 1.67547
[1mStep[0m  [50/53], [94mLoss[0m : 1.61808

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.60477
[1mStep[0m  [5/53], [94mLoss[0m : 1.40581
[1mStep[0m  [10/53], [94mLoss[0m : 1.42419
[1mStep[0m  [15/53], [94mLoss[0m : 1.49179
[1mStep[0m  [20/53], [94mLoss[0m : 1.48906
[1mStep[0m  [25/53], [94mLoss[0m : 1.49363
[1mStep[0m  [30/53], [94mLoss[0m : 1.55477
[1mStep[0m  [35/53], [94mLoss[0m : 1.68544
[1mStep[0m  [40/53], [94mLoss[0m : 1.70615
[1mStep[0m  [45/53], [94mLoss[0m : 1.66058
[1mStep[0m  [50/53], [94mLoss[0m : 1.81470

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.545, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43681
[1mStep[0m  [5/53], [94mLoss[0m : 1.67795
[1mStep[0m  [10/53], [94mLoss[0m : 1.51220
[1mStep[0m  [15/53], [94mLoss[0m : 1.45927
[1mStep[0m  [20/53], [94mLoss[0m : 1.37652
[1mStep[0m  [25/53], [94mLoss[0m : 1.47072
[1mStep[0m  [30/53], [94mLoss[0m : 1.49978
[1mStep[0m  [35/53], [94mLoss[0m : 1.51247
[1mStep[0m  [40/53], [94mLoss[0m : 1.65056
[1mStep[0m  [45/53], [94mLoss[0m : 1.56313
[1mStep[0m  [50/53], [94mLoss[0m : 1.53635

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.534, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46115
[1mStep[0m  [5/53], [94mLoss[0m : 1.37402
[1mStep[0m  [10/53], [94mLoss[0m : 1.52617
[1mStep[0m  [15/53], [94mLoss[0m : 1.48891
[1mStep[0m  [20/53], [94mLoss[0m : 1.43541
[1mStep[0m  [25/53], [94mLoss[0m : 1.61287
[1mStep[0m  [30/53], [94mLoss[0m : 1.50281
[1mStep[0m  [35/53], [94mLoss[0m : 1.47362
[1mStep[0m  [40/53], [94mLoss[0m : 1.51661
[1mStep[0m  [45/53], [94mLoss[0m : 1.56063
[1mStep[0m  [50/53], [94mLoss[0m : 1.50999

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.33535
[1mStep[0m  [5/53], [94mLoss[0m : 1.37193
[1mStep[0m  [10/53], [94mLoss[0m : 1.40520
[1mStep[0m  [15/53], [94mLoss[0m : 1.44308
[1mStep[0m  [20/53], [94mLoss[0m : 1.61558
[1mStep[0m  [25/53], [94mLoss[0m : 1.53408
[1mStep[0m  [30/53], [94mLoss[0m : 1.50897
[1mStep[0m  [35/53], [94mLoss[0m : 1.39848
[1mStep[0m  [40/53], [94mLoss[0m : 1.44216
[1mStep[0m  [45/53], [94mLoss[0m : 1.70820
[1mStep[0m  [50/53], [94mLoss[0m : 1.60664

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.474, [92mTest[0m: 2.562, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.48585
[1mStep[0m  [5/53], [94mLoss[0m : 1.46976
[1mStep[0m  [10/53], [94mLoss[0m : 1.55320
[1mStep[0m  [15/53], [94mLoss[0m : 1.43767
[1mStep[0m  [20/53], [94mLoss[0m : 1.44864
[1mStep[0m  [25/53], [94mLoss[0m : 1.46008
[1mStep[0m  [30/53], [94mLoss[0m : 1.49128
[1mStep[0m  [35/53], [94mLoss[0m : 1.42280
[1mStep[0m  [40/53], [94mLoss[0m : 1.63594
[1mStep[0m  [45/53], [94mLoss[0m : 1.57349
[1mStep[0m  [50/53], [94mLoss[0m : 1.48306

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.462, [92mTest[0m: 2.573, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.32942
[1mStep[0m  [5/53], [94mLoss[0m : 1.31794
[1mStep[0m  [10/53], [94mLoss[0m : 1.46392
[1mStep[0m  [15/53], [94mLoss[0m : 1.39575
[1mStep[0m  [20/53], [94mLoss[0m : 1.36893
[1mStep[0m  [25/53], [94mLoss[0m : 1.45135
[1mStep[0m  [30/53], [94mLoss[0m : 1.36304
[1mStep[0m  [35/53], [94mLoss[0m : 1.53326
[1mStep[0m  [40/53], [94mLoss[0m : 1.31322
[1mStep[0m  [45/53], [94mLoss[0m : 1.46030
[1mStep[0m  [50/53], [94mLoss[0m : 1.49224

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.399, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.33831
[1mStep[0m  [5/53], [94mLoss[0m : 1.35080
[1mStep[0m  [10/53], [94mLoss[0m : 1.29520
[1mStep[0m  [15/53], [94mLoss[0m : 1.20870
[1mStep[0m  [20/53], [94mLoss[0m : 1.29800
[1mStep[0m  [25/53], [94mLoss[0m : 1.20605
[1mStep[0m  [30/53], [94mLoss[0m : 1.34727
[1mStep[0m  [35/53], [94mLoss[0m : 1.42971
[1mStep[0m  [40/53], [94mLoss[0m : 1.42905
[1mStep[0m  [45/53], [94mLoss[0m : 1.52274
[1mStep[0m  [50/53], [94mLoss[0m : 1.43259

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.365, [92mTest[0m: 2.532, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.558
====================================

Phase 2 - Evaluation MAE:  2.5583738730503964
MAE score P1        2.372752
MAE score P2        2.558374
loss                1.364796
learning_rate           0.01
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.9
weight_decay          0.0001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.85078
[1mStep[0m  [2/26], [94mLoss[0m : 10.05577
[1mStep[0m  [4/26], [94mLoss[0m : 8.53151
[1mStep[0m  [6/26], [94mLoss[0m : 6.55361
[1mStep[0m  [8/26], [94mLoss[0m : 4.08764
[1mStep[0m  [10/26], [94mLoss[0m : 2.99957
[1mStep[0m  [12/26], [94mLoss[0m : 3.47094
[1mStep[0m  [14/26], [94mLoss[0m : 4.24414
[1mStep[0m  [16/26], [94mLoss[0m : 4.44948
[1mStep[0m  [18/26], [94mLoss[0m : 4.42920
[1mStep[0m  [20/26], [94mLoss[0m : 3.69680
[1mStep[0m  [22/26], [94mLoss[0m : 2.83550
[1mStep[0m  [24/26], [94mLoss[0m : 2.67144

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.098, [92mTest[0m: 10.968, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.80007
[1mStep[0m  [2/26], [94mLoss[0m : 2.92963
[1mStep[0m  [4/26], [94mLoss[0m : 2.91966
[1mStep[0m  [6/26], [94mLoss[0m : 3.02226
[1mStep[0m  [8/26], [94mLoss[0m : 2.83153
[1mStep[0m  [10/26], [94mLoss[0m : 2.60907
[1mStep[0m  [12/26], [94mLoss[0m : 2.74973
[1mStep[0m  [14/26], [94mLoss[0m : 2.71956
[1mStep[0m  [16/26], [94mLoss[0m : 2.76200
[1mStep[0m  [18/26], [94mLoss[0m : 2.64825
[1mStep[0m  [20/26], [94mLoss[0m : 2.57913
[1mStep[0m  [22/26], [94mLoss[0m : 2.56693
[1mStep[0m  [24/26], [94mLoss[0m : 2.53346

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.579, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59417
[1mStep[0m  [2/26], [94mLoss[0m : 2.49754
[1mStep[0m  [4/26], [94mLoss[0m : 2.64170
[1mStep[0m  [6/26], [94mLoss[0m : 2.53924
[1mStep[0m  [8/26], [94mLoss[0m : 2.48711
[1mStep[0m  [10/26], [94mLoss[0m : 2.56745
[1mStep[0m  [12/26], [94mLoss[0m : 2.55141
[1mStep[0m  [14/26], [94mLoss[0m : 2.65804
[1mStep[0m  [16/26], [94mLoss[0m : 2.56532
[1mStep[0m  [18/26], [94mLoss[0m : 2.58343
[1mStep[0m  [20/26], [94mLoss[0m : 2.62869
[1mStep[0m  [22/26], [94mLoss[0m : 2.59863
[1mStep[0m  [24/26], [94mLoss[0m : 2.54324

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63375
[1mStep[0m  [2/26], [94mLoss[0m : 2.53141
[1mStep[0m  [4/26], [94mLoss[0m : 2.51316
[1mStep[0m  [6/26], [94mLoss[0m : 2.65535
[1mStep[0m  [8/26], [94mLoss[0m : 2.51395
[1mStep[0m  [10/26], [94mLoss[0m : 2.46856
[1mStep[0m  [12/26], [94mLoss[0m : 2.55878
[1mStep[0m  [14/26], [94mLoss[0m : 2.54046
[1mStep[0m  [16/26], [94mLoss[0m : 2.50741
[1mStep[0m  [18/26], [94mLoss[0m : 2.54353
[1mStep[0m  [20/26], [94mLoss[0m : 2.48440
[1mStep[0m  [22/26], [94mLoss[0m : 2.55150
[1mStep[0m  [24/26], [94mLoss[0m : 2.53924

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68354
[1mStep[0m  [2/26], [94mLoss[0m : 2.38956
[1mStep[0m  [4/26], [94mLoss[0m : 2.67372
[1mStep[0m  [6/26], [94mLoss[0m : 2.58274
[1mStep[0m  [8/26], [94mLoss[0m : 2.62259
[1mStep[0m  [10/26], [94mLoss[0m : 2.54925
[1mStep[0m  [12/26], [94mLoss[0m : 2.53302
[1mStep[0m  [14/26], [94mLoss[0m : 2.69510
[1mStep[0m  [16/26], [94mLoss[0m : 2.55796
[1mStep[0m  [18/26], [94mLoss[0m : 2.61551
[1mStep[0m  [20/26], [94mLoss[0m : 2.68683
[1mStep[0m  [22/26], [94mLoss[0m : 2.47881
[1mStep[0m  [24/26], [94mLoss[0m : 2.44195

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54881
[1mStep[0m  [2/26], [94mLoss[0m : 2.61239
[1mStep[0m  [4/26], [94mLoss[0m : 2.61933
[1mStep[0m  [6/26], [94mLoss[0m : 2.49241
[1mStep[0m  [8/26], [94mLoss[0m : 2.52642
[1mStep[0m  [10/26], [94mLoss[0m : 2.67801
[1mStep[0m  [12/26], [94mLoss[0m : 2.63214
[1mStep[0m  [14/26], [94mLoss[0m : 2.45966
[1mStep[0m  [16/26], [94mLoss[0m : 2.57649
[1mStep[0m  [18/26], [94mLoss[0m : 2.52306
[1mStep[0m  [20/26], [94mLoss[0m : 2.73584
[1mStep[0m  [22/26], [94mLoss[0m : 2.44542
[1mStep[0m  [24/26], [94mLoss[0m : 2.51833

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57918
[1mStep[0m  [2/26], [94mLoss[0m : 2.55449
[1mStep[0m  [4/26], [94mLoss[0m : 2.60459
[1mStep[0m  [6/26], [94mLoss[0m : 2.52583
[1mStep[0m  [8/26], [94mLoss[0m : 2.53219
[1mStep[0m  [10/26], [94mLoss[0m : 2.59367
[1mStep[0m  [12/26], [94mLoss[0m : 2.46809
[1mStep[0m  [14/26], [94mLoss[0m : 2.57254
[1mStep[0m  [16/26], [94mLoss[0m : 2.53475
[1mStep[0m  [18/26], [94mLoss[0m : 2.62712
[1mStep[0m  [20/26], [94mLoss[0m : 2.68594
[1mStep[0m  [22/26], [94mLoss[0m : 2.57008
[1mStep[0m  [24/26], [94mLoss[0m : 2.48391

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55455
[1mStep[0m  [2/26], [94mLoss[0m : 2.59924
[1mStep[0m  [4/26], [94mLoss[0m : 2.58323
[1mStep[0m  [6/26], [94mLoss[0m : 2.43544
[1mStep[0m  [8/26], [94mLoss[0m : 2.56308
[1mStep[0m  [10/26], [94mLoss[0m : 2.26317
[1mStep[0m  [12/26], [94mLoss[0m : 2.46694
[1mStep[0m  [14/26], [94mLoss[0m : 2.66396
[1mStep[0m  [16/26], [94mLoss[0m : 2.65823
[1mStep[0m  [18/26], [94mLoss[0m : 2.44043
[1mStep[0m  [20/26], [94mLoss[0m : 2.66681
[1mStep[0m  [22/26], [94mLoss[0m : 2.49247
[1mStep[0m  [24/26], [94mLoss[0m : 2.57747

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67101
[1mStep[0m  [2/26], [94mLoss[0m : 2.54700
[1mStep[0m  [4/26], [94mLoss[0m : 2.60065
[1mStep[0m  [6/26], [94mLoss[0m : 2.51216
[1mStep[0m  [8/26], [94mLoss[0m : 2.47909
[1mStep[0m  [10/26], [94mLoss[0m : 2.48137
[1mStep[0m  [12/26], [94mLoss[0m : 2.58821
[1mStep[0m  [14/26], [94mLoss[0m : 2.64499
[1mStep[0m  [16/26], [94mLoss[0m : 2.42653
[1mStep[0m  [18/26], [94mLoss[0m : 2.39962
[1mStep[0m  [20/26], [94mLoss[0m : 2.46393
[1mStep[0m  [22/26], [94mLoss[0m : 2.47111
[1mStep[0m  [24/26], [94mLoss[0m : 2.38085

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43127
[1mStep[0m  [2/26], [94mLoss[0m : 2.60231
[1mStep[0m  [4/26], [94mLoss[0m : 2.62812
[1mStep[0m  [6/26], [94mLoss[0m : 2.49803
[1mStep[0m  [8/26], [94mLoss[0m : 2.40395
[1mStep[0m  [10/26], [94mLoss[0m : 2.59919
[1mStep[0m  [12/26], [94mLoss[0m : 2.59898
[1mStep[0m  [14/26], [94mLoss[0m : 2.41102
[1mStep[0m  [16/26], [94mLoss[0m : 2.58677
[1mStep[0m  [18/26], [94mLoss[0m : 2.51853
[1mStep[0m  [20/26], [94mLoss[0m : 2.56418
[1mStep[0m  [22/26], [94mLoss[0m : 2.66454
[1mStep[0m  [24/26], [94mLoss[0m : 2.35178

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51498
[1mStep[0m  [2/26], [94mLoss[0m : 2.57780
[1mStep[0m  [4/26], [94mLoss[0m : 2.59628
[1mStep[0m  [6/26], [94mLoss[0m : 2.57069
[1mStep[0m  [8/26], [94mLoss[0m : 2.58254
[1mStep[0m  [10/26], [94mLoss[0m : 2.48532
[1mStep[0m  [12/26], [94mLoss[0m : 2.49279
[1mStep[0m  [14/26], [94mLoss[0m : 2.53666
[1mStep[0m  [16/26], [94mLoss[0m : 2.51932
[1mStep[0m  [18/26], [94mLoss[0m : 2.53494
[1mStep[0m  [20/26], [94mLoss[0m : 2.51113
[1mStep[0m  [22/26], [94mLoss[0m : 2.44078
[1mStep[0m  [24/26], [94mLoss[0m : 2.61942

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45801
[1mStep[0m  [2/26], [94mLoss[0m : 2.43704
[1mStep[0m  [4/26], [94mLoss[0m : 2.62179
[1mStep[0m  [6/26], [94mLoss[0m : 2.46642
[1mStep[0m  [8/26], [94mLoss[0m : 2.43773
[1mStep[0m  [10/26], [94mLoss[0m : 2.58686
[1mStep[0m  [12/26], [94mLoss[0m : 2.43102
[1mStep[0m  [14/26], [94mLoss[0m : 2.58655
[1mStep[0m  [16/26], [94mLoss[0m : 2.47767
[1mStep[0m  [18/26], [94mLoss[0m : 2.45196
[1mStep[0m  [20/26], [94mLoss[0m : 2.56920
[1mStep[0m  [22/26], [94mLoss[0m : 2.59422
[1mStep[0m  [24/26], [94mLoss[0m : 2.42932

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61794
[1mStep[0m  [2/26], [94mLoss[0m : 2.50940
[1mStep[0m  [4/26], [94mLoss[0m : 2.43868
[1mStep[0m  [6/26], [94mLoss[0m : 2.45892
[1mStep[0m  [8/26], [94mLoss[0m : 2.57545
[1mStep[0m  [10/26], [94mLoss[0m : 2.50462
[1mStep[0m  [12/26], [94mLoss[0m : 2.52265
[1mStep[0m  [14/26], [94mLoss[0m : 2.54195
[1mStep[0m  [16/26], [94mLoss[0m : 2.53021
[1mStep[0m  [18/26], [94mLoss[0m : 2.65420
[1mStep[0m  [20/26], [94mLoss[0m : 2.54998
[1mStep[0m  [22/26], [94mLoss[0m : 2.51155
[1mStep[0m  [24/26], [94mLoss[0m : 2.53145

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34221
[1mStep[0m  [2/26], [94mLoss[0m : 2.53078
[1mStep[0m  [4/26], [94mLoss[0m : 2.48035
[1mStep[0m  [6/26], [94mLoss[0m : 2.39192
[1mStep[0m  [8/26], [94mLoss[0m : 2.32442
[1mStep[0m  [10/26], [94mLoss[0m : 2.57112
[1mStep[0m  [12/26], [94mLoss[0m : 2.56934
[1mStep[0m  [14/26], [94mLoss[0m : 2.61245
[1mStep[0m  [16/26], [94mLoss[0m : 2.63392
[1mStep[0m  [18/26], [94mLoss[0m : 2.64413
[1mStep[0m  [20/26], [94mLoss[0m : 2.60678
[1mStep[0m  [22/26], [94mLoss[0m : 2.66749
[1mStep[0m  [24/26], [94mLoss[0m : 2.62267

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60688
[1mStep[0m  [2/26], [94mLoss[0m : 2.56089
[1mStep[0m  [4/26], [94mLoss[0m : 2.49210
[1mStep[0m  [6/26], [94mLoss[0m : 2.51040
[1mStep[0m  [8/26], [94mLoss[0m : 2.37149
[1mStep[0m  [10/26], [94mLoss[0m : 2.51049
[1mStep[0m  [12/26], [94mLoss[0m : 2.48543
[1mStep[0m  [14/26], [94mLoss[0m : 2.50182
[1mStep[0m  [16/26], [94mLoss[0m : 2.76364
[1mStep[0m  [18/26], [94mLoss[0m : 2.59193
[1mStep[0m  [20/26], [94mLoss[0m : 2.41388
[1mStep[0m  [22/26], [94mLoss[0m : 2.50290
[1mStep[0m  [24/26], [94mLoss[0m : 2.47563

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49127
[1mStep[0m  [2/26], [94mLoss[0m : 2.70224
[1mStep[0m  [4/26], [94mLoss[0m : 2.55040
[1mStep[0m  [6/26], [94mLoss[0m : 2.52788
[1mStep[0m  [8/26], [94mLoss[0m : 2.37466
[1mStep[0m  [10/26], [94mLoss[0m : 2.50636
[1mStep[0m  [12/26], [94mLoss[0m : 2.37857
[1mStep[0m  [14/26], [94mLoss[0m : 2.47186
[1mStep[0m  [16/26], [94mLoss[0m : 2.72351
[1mStep[0m  [18/26], [94mLoss[0m : 2.59723
[1mStep[0m  [20/26], [94mLoss[0m : 2.47267
[1mStep[0m  [22/26], [94mLoss[0m : 2.43208
[1mStep[0m  [24/26], [94mLoss[0m : 2.44045

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57164
[1mStep[0m  [2/26], [94mLoss[0m : 2.57169
[1mStep[0m  [4/26], [94mLoss[0m : 2.59973
[1mStep[0m  [6/26], [94mLoss[0m : 2.55954
[1mStep[0m  [8/26], [94mLoss[0m : 2.62057
[1mStep[0m  [10/26], [94mLoss[0m : 2.60360
[1mStep[0m  [12/26], [94mLoss[0m : 2.30619
[1mStep[0m  [14/26], [94mLoss[0m : 2.46899
[1mStep[0m  [16/26], [94mLoss[0m : 2.57492
[1mStep[0m  [18/26], [94mLoss[0m : 2.46251
[1mStep[0m  [20/26], [94mLoss[0m : 2.37497
[1mStep[0m  [22/26], [94mLoss[0m : 2.50249
[1mStep[0m  [24/26], [94mLoss[0m : 2.51689

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49887
[1mStep[0m  [2/26], [94mLoss[0m : 2.63126
[1mStep[0m  [4/26], [94mLoss[0m : 2.47893
[1mStep[0m  [6/26], [94mLoss[0m : 2.63789
[1mStep[0m  [8/26], [94mLoss[0m : 2.51255
[1mStep[0m  [10/26], [94mLoss[0m : 2.59011
[1mStep[0m  [12/26], [94mLoss[0m : 2.53776
[1mStep[0m  [14/26], [94mLoss[0m : 2.46049
[1mStep[0m  [16/26], [94mLoss[0m : 2.41794
[1mStep[0m  [18/26], [94mLoss[0m : 2.50029
[1mStep[0m  [20/26], [94mLoss[0m : 2.51577
[1mStep[0m  [22/26], [94mLoss[0m : 2.50758
[1mStep[0m  [24/26], [94mLoss[0m : 2.60470

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53834
[1mStep[0m  [2/26], [94mLoss[0m : 2.58908
[1mStep[0m  [4/26], [94mLoss[0m : 2.61713
[1mStep[0m  [6/26], [94mLoss[0m : 2.63626
[1mStep[0m  [8/26], [94mLoss[0m : 2.47848
[1mStep[0m  [10/26], [94mLoss[0m : 2.55533
[1mStep[0m  [12/26], [94mLoss[0m : 2.53017
[1mStep[0m  [14/26], [94mLoss[0m : 2.43070
[1mStep[0m  [16/26], [94mLoss[0m : 2.56551
[1mStep[0m  [18/26], [94mLoss[0m : 2.64073
[1mStep[0m  [20/26], [94mLoss[0m : 2.49999
[1mStep[0m  [22/26], [94mLoss[0m : 2.56144
[1mStep[0m  [24/26], [94mLoss[0m : 2.54114

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46104
[1mStep[0m  [2/26], [94mLoss[0m : 2.56412
[1mStep[0m  [4/26], [94mLoss[0m : 2.55046
[1mStep[0m  [6/26], [94mLoss[0m : 2.74530
[1mStep[0m  [8/26], [94mLoss[0m : 2.52126
[1mStep[0m  [10/26], [94mLoss[0m : 2.65660
[1mStep[0m  [12/26], [94mLoss[0m : 2.57927
[1mStep[0m  [14/26], [94mLoss[0m : 2.50201
[1mStep[0m  [16/26], [94mLoss[0m : 2.45453
[1mStep[0m  [18/26], [94mLoss[0m : 2.43948
[1mStep[0m  [20/26], [94mLoss[0m : 2.60189
[1mStep[0m  [22/26], [94mLoss[0m : 2.59021
[1mStep[0m  [24/26], [94mLoss[0m : 2.52629

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.410, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53746
[1mStep[0m  [2/26], [94mLoss[0m : 2.48963
[1mStep[0m  [4/26], [94mLoss[0m : 2.46006
[1mStep[0m  [6/26], [94mLoss[0m : 2.48943
[1mStep[0m  [8/26], [94mLoss[0m : 2.45745
[1mStep[0m  [10/26], [94mLoss[0m : 2.47515
[1mStep[0m  [12/26], [94mLoss[0m : 2.42441
[1mStep[0m  [14/26], [94mLoss[0m : 2.53839
[1mStep[0m  [16/26], [94mLoss[0m : 2.56187
[1mStep[0m  [18/26], [94mLoss[0m : 2.47576
[1mStep[0m  [20/26], [94mLoss[0m : 2.43485
[1mStep[0m  [22/26], [94mLoss[0m : 2.54230
[1mStep[0m  [24/26], [94mLoss[0m : 2.44562

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.404, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50667
[1mStep[0m  [2/26], [94mLoss[0m : 2.54872
[1mStep[0m  [4/26], [94mLoss[0m : 2.57140
[1mStep[0m  [6/26], [94mLoss[0m : 2.59327
[1mStep[0m  [8/26], [94mLoss[0m : 2.39622
[1mStep[0m  [10/26], [94mLoss[0m : 2.56105
[1mStep[0m  [12/26], [94mLoss[0m : 2.43433
[1mStep[0m  [14/26], [94mLoss[0m : 2.40779
[1mStep[0m  [16/26], [94mLoss[0m : 2.41169
[1mStep[0m  [18/26], [94mLoss[0m : 2.53735
[1mStep[0m  [20/26], [94mLoss[0m : 2.46614
[1mStep[0m  [22/26], [94mLoss[0m : 2.48427
[1mStep[0m  [24/26], [94mLoss[0m : 2.42011

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44270
[1mStep[0m  [2/26], [94mLoss[0m : 2.51255
[1mStep[0m  [4/26], [94mLoss[0m : 2.51086
[1mStep[0m  [6/26], [94mLoss[0m : 2.47374
[1mStep[0m  [8/26], [94mLoss[0m : 2.58854
[1mStep[0m  [10/26], [94mLoss[0m : 2.49105
[1mStep[0m  [12/26], [94mLoss[0m : 2.38647
[1mStep[0m  [14/26], [94mLoss[0m : 2.55263
[1mStep[0m  [16/26], [94mLoss[0m : 2.45629
[1mStep[0m  [18/26], [94mLoss[0m : 2.45219
[1mStep[0m  [20/26], [94mLoss[0m : 2.52778
[1mStep[0m  [22/26], [94mLoss[0m : 2.39133
[1mStep[0m  [24/26], [94mLoss[0m : 2.47438

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59857
[1mStep[0m  [2/26], [94mLoss[0m : 2.43204
[1mStep[0m  [4/26], [94mLoss[0m : 2.55580
[1mStep[0m  [6/26], [94mLoss[0m : 2.65841
[1mStep[0m  [8/26], [94mLoss[0m : 2.66229
[1mStep[0m  [10/26], [94mLoss[0m : 2.62365
[1mStep[0m  [12/26], [94mLoss[0m : 2.24355
[1mStep[0m  [14/26], [94mLoss[0m : 2.51765
[1mStep[0m  [16/26], [94mLoss[0m : 2.40398
[1mStep[0m  [18/26], [94mLoss[0m : 2.59456
[1mStep[0m  [20/26], [94mLoss[0m : 2.54096
[1mStep[0m  [22/26], [94mLoss[0m : 2.49385
[1mStep[0m  [24/26], [94mLoss[0m : 2.54466

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57246
[1mStep[0m  [2/26], [94mLoss[0m : 2.52909
[1mStep[0m  [4/26], [94mLoss[0m : 2.42534
[1mStep[0m  [6/26], [94mLoss[0m : 2.62074
[1mStep[0m  [8/26], [94mLoss[0m : 2.40335
[1mStep[0m  [10/26], [94mLoss[0m : 2.44135
[1mStep[0m  [12/26], [94mLoss[0m : 2.61128
[1mStep[0m  [14/26], [94mLoss[0m : 2.47330
[1mStep[0m  [16/26], [94mLoss[0m : 2.40703
[1mStep[0m  [18/26], [94mLoss[0m : 2.39033
[1mStep[0m  [20/26], [94mLoss[0m : 2.52545
[1mStep[0m  [22/26], [94mLoss[0m : 2.46370
[1mStep[0m  [24/26], [94mLoss[0m : 2.51189

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46431
[1mStep[0m  [2/26], [94mLoss[0m : 2.69046
[1mStep[0m  [4/26], [94mLoss[0m : 2.63864
[1mStep[0m  [6/26], [94mLoss[0m : 2.36482
[1mStep[0m  [8/26], [94mLoss[0m : 2.36156
[1mStep[0m  [10/26], [94mLoss[0m : 2.35453
[1mStep[0m  [12/26], [94mLoss[0m : 2.50215
[1mStep[0m  [14/26], [94mLoss[0m : 2.66242
[1mStep[0m  [16/26], [94mLoss[0m : 2.54605
[1mStep[0m  [18/26], [94mLoss[0m : 2.60516
[1mStep[0m  [20/26], [94mLoss[0m : 2.52918
[1mStep[0m  [22/26], [94mLoss[0m : 2.54049
[1mStep[0m  [24/26], [94mLoss[0m : 2.45858

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.406, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42738
[1mStep[0m  [2/26], [94mLoss[0m : 2.50562
[1mStep[0m  [4/26], [94mLoss[0m : 2.52365
[1mStep[0m  [6/26], [94mLoss[0m : 2.56487
[1mStep[0m  [8/26], [94mLoss[0m : 2.54669
[1mStep[0m  [10/26], [94mLoss[0m : 2.41336
[1mStep[0m  [12/26], [94mLoss[0m : 2.47461
[1mStep[0m  [14/26], [94mLoss[0m : 2.54872
[1mStep[0m  [16/26], [94mLoss[0m : 2.37831
[1mStep[0m  [18/26], [94mLoss[0m : 2.33527
[1mStep[0m  [20/26], [94mLoss[0m : 2.60419
[1mStep[0m  [22/26], [94mLoss[0m : 2.53566
[1mStep[0m  [24/26], [94mLoss[0m : 2.48914

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.400, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59346
[1mStep[0m  [2/26], [94mLoss[0m : 2.66194
[1mStep[0m  [4/26], [94mLoss[0m : 2.61044
[1mStep[0m  [6/26], [94mLoss[0m : 2.38161
[1mStep[0m  [8/26], [94mLoss[0m : 2.38774
[1mStep[0m  [10/26], [94mLoss[0m : 2.49415
[1mStep[0m  [12/26], [94mLoss[0m : 2.51526
[1mStep[0m  [14/26], [94mLoss[0m : 2.53916
[1mStep[0m  [16/26], [94mLoss[0m : 2.38223
[1mStep[0m  [18/26], [94mLoss[0m : 2.57612
[1mStep[0m  [20/26], [94mLoss[0m : 2.61887
[1mStep[0m  [22/26], [94mLoss[0m : 2.69107
[1mStep[0m  [24/26], [94mLoss[0m : 2.37122

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55911
[1mStep[0m  [2/26], [94mLoss[0m : 2.49904
[1mStep[0m  [4/26], [94mLoss[0m : 2.48870
[1mStep[0m  [6/26], [94mLoss[0m : 2.49629
[1mStep[0m  [8/26], [94mLoss[0m : 2.45969
[1mStep[0m  [10/26], [94mLoss[0m : 2.51679
[1mStep[0m  [12/26], [94mLoss[0m : 2.53773
[1mStep[0m  [14/26], [94mLoss[0m : 2.42066
[1mStep[0m  [16/26], [94mLoss[0m : 2.55919
[1mStep[0m  [18/26], [94mLoss[0m : 2.51975
[1mStep[0m  [20/26], [94mLoss[0m : 2.46394
[1mStep[0m  [22/26], [94mLoss[0m : 2.50400
[1mStep[0m  [24/26], [94mLoss[0m : 2.52015

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.400, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30309
[1mStep[0m  [2/26], [94mLoss[0m : 2.27802
[1mStep[0m  [4/26], [94mLoss[0m : 2.52462
[1mStep[0m  [6/26], [94mLoss[0m : 2.61279
[1mStep[0m  [8/26], [94mLoss[0m : 2.69115
[1mStep[0m  [10/26], [94mLoss[0m : 2.66727
[1mStep[0m  [12/26], [94mLoss[0m : 2.54061
[1mStep[0m  [14/26], [94mLoss[0m : 2.54092
[1mStep[0m  [16/26], [94mLoss[0m : 2.56729
[1mStep[0m  [18/26], [94mLoss[0m : 2.64031
[1mStep[0m  [20/26], [94mLoss[0m : 2.44495
[1mStep[0m  [22/26], [94mLoss[0m : 2.44812
[1mStep[0m  [24/26], [94mLoss[0m : 2.59304

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.406, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.403
====================================

Phase 1 - Evaluation MAE:  2.4027829537024865
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.37101
[1mStep[0m  [2/26], [94mLoss[0m : 2.68091
[1mStep[0m  [4/26], [94mLoss[0m : 2.39130
[1mStep[0m  [6/26], [94mLoss[0m : 2.52718
[1mStep[0m  [8/26], [94mLoss[0m : 2.36778
[1mStep[0m  [10/26], [94mLoss[0m : 2.69208
[1mStep[0m  [12/26], [94mLoss[0m : 2.61745
[1mStep[0m  [14/26], [94mLoss[0m : 2.45893
[1mStep[0m  [16/26], [94mLoss[0m : 2.30612
[1mStep[0m  [18/26], [94mLoss[0m : 2.49832
[1mStep[0m  [20/26], [94mLoss[0m : 2.59749
[1mStep[0m  [22/26], [94mLoss[0m : 2.70239
[1mStep[0m  [24/26], [94mLoss[0m : 2.62300

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35962
[1mStep[0m  [2/26], [94mLoss[0m : 2.40104
[1mStep[0m  [4/26], [94mLoss[0m : 2.47004
[1mStep[0m  [6/26], [94mLoss[0m : 2.53764
[1mStep[0m  [8/26], [94mLoss[0m : 2.49427
[1mStep[0m  [10/26], [94mLoss[0m : 2.52232
[1mStep[0m  [12/26], [94mLoss[0m : 2.45363
[1mStep[0m  [14/26], [94mLoss[0m : 2.38562
[1mStep[0m  [16/26], [94mLoss[0m : 2.40035
[1mStep[0m  [18/26], [94mLoss[0m : 2.37571
[1mStep[0m  [20/26], [94mLoss[0m : 2.66036
[1mStep[0m  [22/26], [94mLoss[0m : 2.65544
[1mStep[0m  [24/26], [94mLoss[0m : 2.65238

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.686, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46960
[1mStep[0m  [2/26], [94mLoss[0m : 2.41375
[1mStep[0m  [4/26], [94mLoss[0m : 2.41662
[1mStep[0m  [6/26], [94mLoss[0m : 2.53096
[1mStep[0m  [8/26], [94mLoss[0m : 2.38954
[1mStep[0m  [10/26], [94mLoss[0m : 2.31680
[1mStep[0m  [12/26], [94mLoss[0m : 2.27986
[1mStep[0m  [14/26], [94mLoss[0m : 2.46983
[1mStep[0m  [16/26], [94mLoss[0m : 2.41794
[1mStep[0m  [18/26], [94mLoss[0m : 2.33439
[1mStep[0m  [20/26], [94mLoss[0m : 2.50837
[1mStep[0m  [22/26], [94mLoss[0m : 2.34070
[1mStep[0m  [24/26], [94mLoss[0m : 2.40355

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.909, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51303
[1mStep[0m  [2/26], [94mLoss[0m : 2.40775
[1mStep[0m  [4/26], [94mLoss[0m : 2.37794
[1mStep[0m  [6/26], [94mLoss[0m : 2.19661
[1mStep[0m  [8/26], [94mLoss[0m : 2.45606
[1mStep[0m  [10/26], [94mLoss[0m : 2.28457
[1mStep[0m  [12/26], [94mLoss[0m : 2.44414
[1mStep[0m  [14/26], [94mLoss[0m : 2.40443
[1mStep[0m  [16/26], [94mLoss[0m : 2.29722
[1mStep[0m  [18/26], [94mLoss[0m : 2.34657
[1mStep[0m  [20/26], [94mLoss[0m : 2.32319
[1mStep[0m  [22/26], [94mLoss[0m : 2.31485
[1mStep[0m  [24/26], [94mLoss[0m : 2.23899

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.911, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26940
[1mStep[0m  [2/26], [94mLoss[0m : 2.29190
[1mStep[0m  [4/26], [94mLoss[0m : 2.31782
[1mStep[0m  [6/26], [94mLoss[0m : 2.33093
[1mStep[0m  [8/26], [94mLoss[0m : 2.27378
[1mStep[0m  [10/26], [94mLoss[0m : 2.26906
[1mStep[0m  [12/26], [94mLoss[0m : 2.21746
[1mStep[0m  [14/26], [94mLoss[0m : 2.21589
[1mStep[0m  [16/26], [94mLoss[0m : 2.37482
[1mStep[0m  [18/26], [94mLoss[0m : 2.24415
[1mStep[0m  [20/26], [94mLoss[0m : 2.33027
[1mStep[0m  [22/26], [94mLoss[0m : 2.34521
[1mStep[0m  [24/26], [94mLoss[0m : 2.24230

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.289, [92mTest[0m: 3.027, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.10540
[1mStep[0m  [2/26], [94mLoss[0m : 2.17748
[1mStep[0m  [4/26], [94mLoss[0m : 2.49311
[1mStep[0m  [6/26], [94mLoss[0m : 2.11555
[1mStep[0m  [8/26], [94mLoss[0m : 2.19954
[1mStep[0m  [10/26], [94mLoss[0m : 2.16014
[1mStep[0m  [12/26], [94mLoss[0m : 2.17873
[1mStep[0m  [14/26], [94mLoss[0m : 2.28327
[1mStep[0m  [16/26], [94mLoss[0m : 2.31889
[1mStep[0m  [18/26], [94mLoss[0m : 2.25214
[1mStep[0m  [20/26], [94mLoss[0m : 2.23230
[1mStep[0m  [22/26], [94mLoss[0m : 2.44228
[1mStep[0m  [24/26], [94mLoss[0m : 2.21459

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.654, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15161
[1mStep[0m  [2/26], [94mLoss[0m : 2.16767
[1mStep[0m  [4/26], [94mLoss[0m : 2.17771
[1mStep[0m  [6/26], [94mLoss[0m : 2.16197
[1mStep[0m  [8/26], [94mLoss[0m : 2.12778
[1mStep[0m  [10/26], [94mLoss[0m : 2.18629
[1mStep[0m  [12/26], [94mLoss[0m : 2.18705
[1mStep[0m  [14/26], [94mLoss[0m : 2.12193
[1mStep[0m  [16/26], [94mLoss[0m : 2.26751
[1mStep[0m  [18/26], [94mLoss[0m : 2.37965
[1mStep[0m  [20/26], [94mLoss[0m : 2.23948
[1mStep[0m  [22/26], [94mLoss[0m : 2.16501
[1mStep[0m  [24/26], [94mLoss[0m : 2.34119

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.203, [92mTest[0m: 2.771, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.10288
[1mStep[0m  [2/26], [94mLoss[0m : 2.23589
[1mStep[0m  [4/26], [94mLoss[0m : 2.16979
[1mStep[0m  [6/26], [94mLoss[0m : 2.16684
[1mStep[0m  [8/26], [94mLoss[0m : 2.10838
[1mStep[0m  [10/26], [94mLoss[0m : 2.21440
[1mStep[0m  [12/26], [94mLoss[0m : 2.00821
[1mStep[0m  [14/26], [94mLoss[0m : 2.02028
[1mStep[0m  [16/26], [94mLoss[0m : 2.23309
[1mStep[0m  [18/26], [94mLoss[0m : 2.23636
[1mStep[0m  [20/26], [94mLoss[0m : 2.17633
[1mStep[0m  [22/26], [94mLoss[0m : 2.14727
[1mStep[0m  [24/26], [94mLoss[0m : 2.14962

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.04125
[1mStep[0m  [2/26], [94mLoss[0m : 2.16445
[1mStep[0m  [4/26], [94mLoss[0m : 2.11898
[1mStep[0m  [6/26], [94mLoss[0m : 2.23181
[1mStep[0m  [8/26], [94mLoss[0m : 2.09035
[1mStep[0m  [10/26], [94mLoss[0m : 2.21366
[1mStep[0m  [12/26], [94mLoss[0m : 2.07205
[1mStep[0m  [14/26], [94mLoss[0m : 2.10715
[1mStep[0m  [16/26], [94mLoss[0m : 2.26365
[1mStep[0m  [18/26], [94mLoss[0m : 2.07750
[1mStep[0m  [20/26], [94mLoss[0m : 2.16695
[1mStep[0m  [22/26], [94mLoss[0m : 2.08147
[1mStep[0m  [24/26], [94mLoss[0m : 2.15498

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.545, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98704
[1mStep[0m  [2/26], [94mLoss[0m : 2.03187
[1mStep[0m  [4/26], [94mLoss[0m : 2.08708
[1mStep[0m  [6/26], [94mLoss[0m : 2.08514
[1mStep[0m  [8/26], [94mLoss[0m : 2.06942
[1mStep[0m  [10/26], [94mLoss[0m : 2.01886
[1mStep[0m  [12/26], [94mLoss[0m : 2.11673
[1mStep[0m  [14/26], [94mLoss[0m : 2.15172
[1mStep[0m  [16/26], [94mLoss[0m : 2.06932
[1mStep[0m  [18/26], [94mLoss[0m : 2.10370
[1mStep[0m  [20/26], [94mLoss[0m : 2.23076
[1mStep[0m  [22/26], [94mLoss[0m : 2.11038
[1mStep[0m  [24/26], [94mLoss[0m : 2.26962

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09024
[1mStep[0m  [2/26], [94mLoss[0m : 2.05562
[1mStep[0m  [4/26], [94mLoss[0m : 1.98136
[1mStep[0m  [6/26], [94mLoss[0m : 1.89976
[1mStep[0m  [8/26], [94mLoss[0m : 2.04381
[1mStep[0m  [10/26], [94mLoss[0m : 2.02658
[1mStep[0m  [12/26], [94mLoss[0m : 2.05770
[1mStep[0m  [14/26], [94mLoss[0m : 1.97161
[1mStep[0m  [16/26], [94mLoss[0m : 2.08533
[1mStep[0m  [18/26], [94mLoss[0m : 2.08808
[1mStep[0m  [20/26], [94mLoss[0m : 1.95616
[1mStep[0m  [22/26], [94mLoss[0m : 2.01820
[1mStep[0m  [24/26], [94mLoss[0m : 2.11134

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26379
[1mStep[0m  [2/26], [94mLoss[0m : 2.09294
[1mStep[0m  [4/26], [94mLoss[0m : 2.05276
[1mStep[0m  [6/26], [94mLoss[0m : 1.90317
[1mStep[0m  [8/26], [94mLoss[0m : 2.05819
[1mStep[0m  [10/26], [94mLoss[0m : 1.99819
[1mStep[0m  [12/26], [94mLoss[0m : 2.05286
[1mStep[0m  [14/26], [94mLoss[0m : 2.06086
[1mStep[0m  [16/26], [94mLoss[0m : 1.91230
[1mStep[0m  [18/26], [94mLoss[0m : 2.22541
[1mStep[0m  [20/26], [94mLoss[0m : 1.94639
[1mStep[0m  [22/26], [94mLoss[0m : 1.98912
[1mStep[0m  [24/26], [94mLoss[0m : 2.00046

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.78466
[1mStep[0m  [2/26], [94mLoss[0m : 1.94629
[1mStep[0m  [4/26], [94mLoss[0m : 2.03567
[1mStep[0m  [6/26], [94mLoss[0m : 1.90395
[1mStep[0m  [8/26], [94mLoss[0m : 1.90784
[1mStep[0m  [10/26], [94mLoss[0m : 1.91529
[1mStep[0m  [12/26], [94mLoss[0m : 2.10278
[1mStep[0m  [14/26], [94mLoss[0m : 1.98602
[1mStep[0m  [16/26], [94mLoss[0m : 1.96054
[1mStep[0m  [18/26], [94mLoss[0m : 1.96244
[1mStep[0m  [20/26], [94mLoss[0m : 2.01457
[1mStep[0m  [22/26], [94mLoss[0m : 2.12102
[1mStep[0m  [24/26], [94mLoss[0m : 2.04653

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.89432
[1mStep[0m  [2/26], [94mLoss[0m : 2.04585
[1mStep[0m  [4/26], [94mLoss[0m : 2.03164
[1mStep[0m  [6/26], [94mLoss[0m : 1.87357
[1mStep[0m  [8/26], [94mLoss[0m : 1.89146
[1mStep[0m  [10/26], [94mLoss[0m : 1.90610
[1mStep[0m  [12/26], [94mLoss[0m : 1.93289
[1mStep[0m  [14/26], [94mLoss[0m : 2.01149
[1mStep[0m  [16/26], [94mLoss[0m : 2.08497
[1mStep[0m  [18/26], [94mLoss[0m : 1.90947
[1mStep[0m  [20/26], [94mLoss[0m : 1.89120
[1mStep[0m  [22/26], [94mLoss[0m : 1.83860
[1mStep[0m  [24/26], [94mLoss[0m : 1.98287

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.99471
[1mStep[0m  [2/26], [94mLoss[0m : 1.91623
[1mStep[0m  [4/26], [94mLoss[0m : 1.85908
[1mStep[0m  [6/26], [94mLoss[0m : 1.93085
[1mStep[0m  [8/26], [94mLoss[0m : 1.73210
[1mStep[0m  [10/26], [94mLoss[0m : 1.97992
[1mStep[0m  [12/26], [94mLoss[0m : 2.04557
[1mStep[0m  [14/26], [94mLoss[0m : 1.79281
[1mStep[0m  [16/26], [94mLoss[0m : 1.95599
[1mStep[0m  [18/26], [94mLoss[0m : 1.84441
[1mStep[0m  [20/26], [94mLoss[0m : 2.09731
[1mStep[0m  [22/26], [94mLoss[0m : 2.11650
[1mStep[0m  [24/26], [94mLoss[0m : 1.85796

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88618
[1mStep[0m  [2/26], [94mLoss[0m : 1.89006
[1mStep[0m  [4/26], [94mLoss[0m : 1.94093
[1mStep[0m  [6/26], [94mLoss[0m : 1.82110
[1mStep[0m  [8/26], [94mLoss[0m : 1.85153
[1mStep[0m  [10/26], [94mLoss[0m : 1.94834
[1mStep[0m  [12/26], [94mLoss[0m : 1.92011
[1mStep[0m  [14/26], [94mLoss[0m : 1.98909
[1mStep[0m  [16/26], [94mLoss[0m : 1.94330
[1mStep[0m  [18/26], [94mLoss[0m : 1.83383
[1mStep[0m  [20/26], [94mLoss[0m : 1.93869
[1mStep[0m  [22/26], [94mLoss[0m : 2.03688
[1mStep[0m  [24/26], [94mLoss[0m : 2.02266

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.90176
[1mStep[0m  [2/26], [94mLoss[0m : 1.84522
[1mStep[0m  [4/26], [94mLoss[0m : 1.98691
[1mStep[0m  [6/26], [94mLoss[0m : 1.82575
[1mStep[0m  [8/26], [94mLoss[0m : 1.83322
[1mStep[0m  [10/26], [94mLoss[0m : 1.84129
[1mStep[0m  [12/26], [94mLoss[0m : 1.82950
[1mStep[0m  [14/26], [94mLoss[0m : 2.05184
[1mStep[0m  [16/26], [94mLoss[0m : 2.05054
[1mStep[0m  [18/26], [94mLoss[0m : 1.90395
[1mStep[0m  [20/26], [94mLoss[0m : 1.89160
[1mStep[0m  [22/26], [94mLoss[0m : 1.99863
[1mStep[0m  [24/26], [94mLoss[0m : 1.89947

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.83281
[1mStep[0m  [2/26], [94mLoss[0m : 1.90970
[1mStep[0m  [4/26], [94mLoss[0m : 1.79326
[1mStep[0m  [6/26], [94mLoss[0m : 2.04528
[1mStep[0m  [8/26], [94mLoss[0m : 1.91533
[1mStep[0m  [10/26], [94mLoss[0m : 1.86787
[1mStep[0m  [12/26], [94mLoss[0m : 1.78668
[1mStep[0m  [14/26], [94mLoss[0m : 1.86904
[1mStep[0m  [16/26], [94mLoss[0m : 1.94019
[1mStep[0m  [18/26], [94mLoss[0m : 1.89507
[1mStep[0m  [20/26], [94mLoss[0m : 1.97637
[1mStep[0m  [22/26], [94mLoss[0m : 1.97987
[1mStep[0m  [24/26], [94mLoss[0m : 1.80670

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.906, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88976
[1mStep[0m  [2/26], [94mLoss[0m : 1.79145
[1mStep[0m  [4/26], [94mLoss[0m : 1.86710
[1mStep[0m  [6/26], [94mLoss[0m : 1.77735
[1mStep[0m  [8/26], [94mLoss[0m : 1.89474
[1mStep[0m  [10/26], [94mLoss[0m : 1.84635
[1mStep[0m  [12/26], [94mLoss[0m : 1.90173
[1mStep[0m  [14/26], [94mLoss[0m : 1.96108
[1mStep[0m  [16/26], [94mLoss[0m : 1.97647
[1mStep[0m  [18/26], [94mLoss[0m : 2.00607
[1mStep[0m  [20/26], [94mLoss[0m : 1.92257
[1mStep[0m  [22/26], [94mLoss[0m : 1.89241
[1mStep[0m  [24/26], [94mLoss[0m : 1.88547

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.72641
[1mStep[0m  [2/26], [94mLoss[0m : 1.88302
[1mStep[0m  [4/26], [94mLoss[0m : 1.78508
[1mStep[0m  [6/26], [94mLoss[0m : 1.84628
[1mStep[0m  [8/26], [94mLoss[0m : 1.82772
[1mStep[0m  [10/26], [94mLoss[0m : 1.96060
[1mStep[0m  [12/26], [94mLoss[0m : 1.95540
[1mStep[0m  [14/26], [94mLoss[0m : 1.85646
[1mStep[0m  [16/26], [94mLoss[0m : 1.97684
[1mStep[0m  [18/26], [94mLoss[0m : 1.94302
[1mStep[0m  [20/26], [94mLoss[0m : 1.90247
[1mStep[0m  [22/26], [94mLoss[0m : 2.02584
[1mStep[0m  [24/26], [94mLoss[0m : 1.82173

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.80691
[1mStep[0m  [2/26], [94mLoss[0m : 1.89486
[1mStep[0m  [4/26], [94mLoss[0m : 1.88296
[1mStep[0m  [6/26], [94mLoss[0m : 1.85094
[1mStep[0m  [8/26], [94mLoss[0m : 1.81960
[1mStep[0m  [10/26], [94mLoss[0m : 1.87311
[1mStep[0m  [12/26], [94mLoss[0m : 1.82047
[1mStep[0m  [14/26], [94mLoss[0m : 1.95402
[1mStep[0m  [16/26], [94mLoss[0m : 1.77626
[1mStep[0m  [18/26], [94mLoss[0m : 1.93894
[1mStep[0m  [20/26], [94mLoss[0m : 1.76185
[1mStep[0m  [22/26], [94mLoss[0m : 1.87002
[1mStep[0m  [24/26], [94mLoss[0m : 1.93762

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.469, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.91479
[1mStep[0m  [2/26], [94mLoss[0m : 1.84666
[1mStep[0m  [4/26], [94mLoss[0m : 1.89145
[1mStep[0m  [6/26], [94mLoss[0m : 1.78136
[1mStep[0m  [8/26], [94mLoss[0m : 1.61775
[1mStep[0m  [10/26], [94mLoss[0m : 1.93808
[1mStep[0m  [12/26], [94mLoss[0m : 1.85938
[1mStep[0m  [14/26], [94mLoss[0m : 1.95381
[1mStep[0m  [16/26], [94mLoss[0m : 1.87380
[1mStep[0m  [18/26], [94mLoss[0m : 1.84684
[1mStep[0m  [20/26], [94mLoss[0m : 1.86103
[1mStep[0m  [22/26], [94mLoss[0m : 1.88174
[1mStep[0m  [24/26], [94mLoss[0m : 1.78043

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.434, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.75417
[1mStep[0m  [2/26], [94mLoss[0m : 1.79837
[1mStep[0m  [4/26], [94mLoss[0m : 1.92025
[1mStep[0m  [6/26], [94mLoss[0m : 1.77321
[1mStep[0m  [8/26], [94mLoss[0m : 1.84429
[1mStep[0m  [10/26], [94mLoss[0m : 1.72932
[1mStep[0m  [12/26], [94mLoss[0m : 1.84672
[1mStep[0m  [14/26], [94mLoss[0m : 1.84194
[1mStep[0m  [16/26], [94mLoss[0m : 1.81068
[1mStep[0m  [18/26], [94mLoss[0m : 1.80621
[1mStep[0m  [20/26], [94mLoss[0m : 1.86810
[1mStep[0m  [22/26], [94mLoss[0m : 1.83047
[1mStep[0m  [24/26], [94mLoss[0m : 1.86042

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.436, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76066
[1mStep[0m  [2/26], [94mLoss[0m : 1.67932
[1mStep[0m  [4/26], [94mLoss[0m : 1.70880
[1mStep[0m  [6/26], [94mLoss[0m : 1.87709
[1mStep[0m  [8/26], [94mLoss[0m : 1.91199
[1mStep[0m  [10/26], [94mLoss[0m : 1.80554
[1mStep[0m  [12/26], [94mLoss[0m : 1.84863
[1mStep[0m  [14/26], [94mLoss[0m : 1.99415
[1mStep[0m  [16/26], [94mLoss[0m : 1.78556
[1mStep[0m  [18/26], [94mLoss[0m : 1.84131
[1mStep[0m  [20/26], [94mLoss[0m : 1.90027
[1mStep[0m  [22/26], [94mLoss[0m : 1.72936
[1mStep[0m  [24/26], [94mLoss[0m : 1.92056

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.90923
[1mStep[0m  [2/26], [94mLoss[0m : 1.85167
[1mStep[0m  [4/26], [94mLoss[0m : 1.80157
[1mStep[0m  [6/26], [94mLoss[0m : 1.86476
[1mStep[0m  [8/26], [94mLoss[0m : 1.77544
[1mStep[0m  [10/26], [94mLoss[0m : 1.94993
[1mStep[0m  [12/26], [94mLoss[0m : 1.77457
[1mStep[0m  [14/26], [94mLoss[0m : 1.77256
[1mStep[0m  [16/26], [94mLoss[0m : 1.91181
[1mStep[0m  [18/26], [94mLoss[0m : 1.87585
[1mStep[0m  [20/26], [94mLoss[0m : 1.86698
[1mStep[0m  [22/26], [94mLoss[0m : 1.84514
[1mStep[0m  [24/26], [94mLoss[0m : 1.83844

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76642
[1mStep[0m  [2/26], [94mLoss[0m : 1.81595
[1mStep[0m  [4/26], [94mLoss[0m : 1.83628
[1mStep[0m  [6/26], [94mLoss[0m : 1.93226
[1mStep[0m  [8/26], [94mLoss[0m : 1.91424
[1mStep[0m  [10/26], [94mLoss[0m : 1.89107
[1mStep[0m  [12/26], [94mLoss[0m : 1.79371
[1mStep[0m  [14/26], [94mLoss[0m : 1.72732
[1mStep[0m  [16/26], [94mLoss[0m : 1.77407
[1mStep[0m  [18/26], [94mLoss[0m : 1.76104
[1mStep[0m  [20/26], [94mLoss[0m : 1.88351
[1mStep[0m  [22/26], [94mLoss[0m : 1.78277
[1mStep[0m  [24/26], [94mLoss[0m : 1.80230

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81092
[1mStep[0m  [2/26], [94mLoss[0m : 1.78830
[1mStep[0m  [4/26], [94mLoss[0m : 1.76475
[1mStep[0m  [6/26], [94mLoss[0m : 1.78797
[1mStep[0m  [8/26], [94mLoss[0m : 1.75513
[1mStep[0m  [10/26], [94mLoss[0m : 1.65692
[1mStep[0m  [12/26], [94mLoss[0m : 1.75665
[1mStep[0m  [14/26], [94mLoss[0m : 1.72111
[1mStep[0m  [16/26], [94mLoss[0m : 1.75697
[1mStep[0m  [18/26], [94mLoss[0m : 1.92436
[1mStep[0m  [20/26], [94mLoss[0m : 1.90180
[1mStep[0m  [22/26], [94mLoss[0m : 1.85468
[1mStep[0m  [24/26], [94mLoss[0m : 1.73159

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.451, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.79667
[1mStep[0m  [2/26], [94mLoss[0m : 1.72953
[1mStep[0m  [4/26], [94mLoss[0m : 1.71630
[1mStep[0m  [6/26], [94mLoss[0m : 1.62605
[1mStep[0m  [8/26], [94mLoss[0m : 1.81947
[1mStep[0m  [10/26], [94mLoss[0m : 1.81388
[1mStep[0m  [12/26], [94mLoss[0m : 1.85533
[1mStep[0m  [14/26], [94mLoss[0m : 1.83646
[1mStep[0m  [16/26], [94mLoss[0m : 1.73007
[1mStep[0m  [18/26], [94mLoss[0m : 1.81876
[1mStep[0m  [20/26], [94mLoss[0m : 1.78195
[1mStep[0m  [22/26], [94mLoss[0m : 1.78530
[1mStep[0m  [24/26], [94mLoss[0m : 1.81905

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.777, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.67398
[1mStep[0m  [2/26], [94mLoss[0m : 1.76406
[1mStep[0m  [4/26], [94mLoss[0m : 1.75377
[1mStep[0m  [6/26], [94mLoss[0m : 1.70787
[1mStep[0m  [8/26], [94mLoss[0m : 1.72865
[1mStep[0m  [10/26], [94mLoss[0m : 1.78492
[1mStep[0m  [12/26], [94mLoss[0m : 1.76913
[1mStep[0m  [14/26], [94mLoss[0m : 1.76575
[1mStep[0m  [16/26], [94mLoss[0m : 1.90610
[1mStep[0m  [18/26], [94mLoss[0m : 1.85371
[1mStep[0m  [20/26], [94mLoss[0m : 1.76524
[1mStep[0m  [22/26], [94mLoss[0m : 1.80639
[1mStep[0m  [24/26], [94mLoss[0m : 1.73196

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.449, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.67097
[1mStep[0m  [2/26], [94mLoss[0m : 1.76085
[1mStep[0m  [4/26], [94mLoss[0m : 1.66355
[1mStep[0m  [6/26], [94mLoss[0m : 1.84043
[1mStep[0m  [8/26], [94mLoss[0m : 1.73426
[1mStep[0m  [10/26], [94mLoss[0m : 1.77329
[1mStep[0m  [12/26], [94mLoss[0m : 1.78182
[1mStep[0m  [14/26], [94mLoss[0m : 1.76454
[1mStep[0m  [16/26], [94mLoss[0m : 1.71949
[1mStep[0m  [18/26], [94mLoss[0m : 1.78237
[1mStep[0m  [20/26], [94mLoss[0m : 1.78831
[1mStep[0m  [22/26], [94mLoss[0m : 1.67454
[1mStep[0m  [24/26], [94mLoss[0m : 1.79388

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.423, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.548
====================================

Phase 2 - Evaluation MAE:  2.5482269617227407
MAE score P1        2.402783
MAE score P2        2.548227
loss                1.766924
learning_rate           0.01
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.9
weight_decay            0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.10324
[1mStep[0m  [5/53], [94mLoss[0m : 7.09525
[1mStep[0m  [10/53], [94mLoss[0m : 2.68717
[1mStep[0m  [15/53], [94mLoss[0m : 4.18160
[1mStep[0m  [20/53], [94mLoss[0m : 2.88022
[1mStep[0m  [25/53], [94mLoss[0m : 2.67117
[1mStep[0m  [30/53], [94mLoss[0m : 2.88981
[1mStep[0m  [35/53], [94mLoss[0m : 2.41707
[1mStep[0m  [40/53], [94mLoss[0m : 2.59924
[1mStep[0m  [45/53], [94mLoss[0m : 2.41042
[1mStep[0m  [50/53], [94mLoss[0m : 2.53836

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.647, [92mTest[0m: 10.285, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47061
[1mStep[0m  [5/53], [94mLoss[0m : 2.23983
[1mStep[0m  [10/53], [94mLoss[0m : 2.38560
[1mStep[0m  [15/53], [94mLoss[0m : 2.33877
[1mStep[0m  [20/53], [94mLoss[0m : 2.32795
[1mStep[0m  [25/53], [94mLoss[0m : 2.56995
[1mStep[0m  [30/53], [94mLoss[0m : 2.50464
[1mStep[0m  [35/53], [94mLoss[0m : 2.46350
[1mStep[0m  [40/53], [94mLoss[0m : 2.51593
[1mStep[0m  [45/53], [94mLoss[0m : 2.30058
[1mStep[0m  [50/53], [94mLoss[0m : 2.57100

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48585
[1mStep[0m  [5/53], [94mLoss[0m : 2.54313
[1mStep[0m  [10/53], [94mLoss[0m : 2.38178
[1mStep[0m  [15/53], [94mLoss[0m : 2.43855
[1mStep[0m  [20/53], [94mLoss[0m : 2.57923
[1mStep[0m  [25/53], [94mLoss[0m : 2.51224
[1mStep[0m  [30/53], [94mLoss[0m : 2.55089
[1mStep[0m  [35/53], [94mLoss[0m : 2.39147
[1mStep[0m  [40/53], [94mLoss[0m : 2.42181
[1mStep[0m  [45/53], [94mLoss[0m : 2.42730
[1mStep[0m  [50/53], [94mLoss[0m : 2.12677

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37181
[1mStep[0m  [5/53], [94mLoss[0m : 2.40063
[1mStep[0m  [10/53], [94mLoss[0m : 2.34339
[1mStep[0m  [15/53], [94mLoss[0m : 2.73047
[1mStep[0m  [20/53], [94mLoss[0m : 2.35632
[1mStep[0m  [25/53], [94mLoss[0m : 2.40360
[1mStep[0m  [30/53], [94mLoss[0m : 2.35607
[1mStep[0m  [35/53], [94mLoss[0m : 2.30136
[1mStep[0m  [40/53], [94mLoss[0m : 2.45536
[1mStep[0m  [45/53], [94mLoss[0m : 2.64262
[1mStep[0m  [50/53], [94mLoss[0m : 2.32003

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55552
[1mStep[0m  [5/53], [94mLoss[0m : 2.35384
[1mStep[0m  [10/53], [94mLoss[0m : 2.44292
[1mStep[0m  [15/53], [94mLoss[0m : 2.34585
[1mStep[0m  [20/53], [94mLoss[0m : 2.49922
[1mStep[0m  [25/53], [94mLoss[0m : 2.44901
[1mStep[0m  [30/53], [94mLoss[0m : 2.41601
[1mStep[0m  [35/53], [94mLoss[0m : 2.19850
[1mStep[0m  [40/53], [94mLoss[0m : 2.39343
[1mStep[0m  [45/53], [94mLoss[0m : 2.34044
[1mStep[0m  [50/53], [94mLoss[0m : 2.25667

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45770
[1mStep[0m  [5/53], [94mLoss[0m : 2.48323
[1mStep[0m  [10/53], [94mLoss[0m : 2.22551
[1mStep[0m  [15/53], [94mLoss[0m : 2.61018
[1mStep[0m  [20/53], [94mLoss[0m : 2.37653
[1mStep[0m  [25/53], [94mLoss[0m : 2.46227
[1mStep[0m  [30/53], [94mLoss[0m : 2.32428
[1mStep[0m  [35/53], [94mLoss[0m : 2.43255
[1mStep[0m  [40/53], [94mLoss[0m : 2.59216
[1mStep[0m  [45/53], [94mLoss[0m : 2.57509
[1mStep[0m  [50/53], [94mLoss[0m : 2.29722

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29985
[1mStep[0m  [5/53], [94mLoss[0m : 2.45544
[1mStep[0m  [10/53], [94mLoss[0m : 2.23699
[1mStep[0m  [15/53], [94mLoss[0m : 2.32554
[1mStep[0m  [20/53], [94mLoss[0m : 2.51133
[1mStep[0m  [25/53], [94mLoss[0m : 2.85415
[1mStep[0m  [30/53], [94mLoss[0m : 2.44345
[1mStep[0m  [35/53], [94mLoss[0m : 2.33299
[1mStep[0m  [40/53], [94mLoss[0m : 2.46044
[1mStep[0m  [45/53], [94mLoss[0m : 2.47361
[1mStep[0m  [50/53], [94mLoss[0m : 2.41245

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46479
[1mStep[0m  [5/53], [94mLoss[0m : 2.32337
[1mStep[0m  [10/53], [94mLoss[0m : 2.43236
[1mStep[0m  [15/53], [94mLoss[0m : 2.35380
[1mStep[0m  [20/53], [94mLoss[0m : 2.36924
[1mStep[0m  [25/53], [94mLoss[0m : 2.59651
[1mStep[0m  [30/53], [94mLoss[0m : 2.32203
[1mStep[0m  [35/53], [94mLoss[0m : 2.63506
[1mStep[0m  [40/53], [94mLoss[0m : 2.44170
[1mStep[0m  [45/53], [94mLoss[0m : 2.59872
[1mStep[0m  [50/53], [94mLoss[0m : 2.37009

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20977
[1mStep[0m  [5/53], [94mLoss[0m : 2.56500
[1mStep[0m  [10/53], [94mLoss[0m : 2.41400
[1mStep[0m  [15/53], [94mLoss[0m : 2.34973
[1mStep[0m  [20/53], [94mLoss[0m : 2.62001
[1mStep[0m  [25/53], [94mLoss[0m : 2.28776
[1mStep[0m  [30/53], [94mLoss[0m : 2.46479
[1mStep[0m  [35/53], [94mLoss[0m : 2.47602
[1mStep[0m  [40/53], [94mLoss[0m : 2.46435
[1mStep[0m  [45/53], [94mLoss[0m : 2.39619
[1mStep[0m  [50/53], [94mLoss[0m : 2.33729

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25304
[1mStep[0m  [5/53], [94mLoss[0m : 2.49267
[1mStep[0m  [10/53], [94mLoss[0m : 2.40973
[1mStep[0m  [15/53], [94mLoss[0m : 2.42579
[1mStep[0m  [20/53], [94mLoss[0m : 2.44459
[1mStep[0m  [25/53], [94mLoss[0m : 2.59805
[1mStep[0m  [30/53], [94mLoss[0m : 2.51994
[1mStep[0m  [35/53], [94mLoss[0m : 2.41209
[1mStep[0m  [40/53], [94mLoss[0m : 2.22551
[1mStep[0m  [45/53], [94mLoss[0m : 2.49478
[1mStep[0m  [50/53], [94mLoss[0m : 2.32156

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53785
[1mStep[0m  [5/53], [94mLoss[0m : 2.15848
[1mStep[0m  [10/53], [94mLoss[0m : 2.77678
[1mStep[0m  [15/53], [94mLoss[0m : 2.44598
[1mStep[0m  [20/53], [94mLoss[0m : 2.48769
[1mStep[0m  [25/53], [94mLoss[0m : 2.35728
[1mStep[0m  [30/53], [94mLoss[0m : 2.31937
[1mStep[0m  [35/53], [94mLoss[0m : 2.31025
[1mStep[0m  [40/53], [94mLoss[0m : 2.31595
[1mStep[0m  [45/53], [94mLoss[0m : 2.37154
[1mStep[0m  [50/53], [94mLoss[0m : 2.43593

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39118
[1mStep[0m  [5/53], [94mLoss[0m : 2.29522
[1mStep[0m  [10/53], [94mLoss[0m : 2.63029
[1mStep[0m  [15/53], [94mLoss[0m : 2.45826
[1mStep[0m  [20/53], [94mLoss[0m : 2.32162
[1mStep[0m  [25/53], [94mLoss[0m : 2.42710
[1mStep[0m  [30/53], [94mLoss[0m : 2.39404
[1mStep[0m  [35/53], [94mLoss[0m : 2.34547
[1mStep[0m  [40/53], [94mLoss[0m : 2.44486
[1mStep[0m  [45/53], [94mLoss[0m : 2.37835
[1mStep[0m  [50/53], [94mLoss[0m : 2.47618

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34302
[1mStep[0m  [5/53], [94mLoss[0m : 2.27340
[1mStep[0m  [10/53], [94mLoss[0m : 2.18554
[1mStep[0m  [15/53], [94mLoss[0m : 2.37868
[1mStep[0m  [20/53], [94mLoss[0m : 2.56731
[1mStep[0m  [25/53], [94mLoss[0m : 2.53499
[1mStep[0m  [30/53], [94mLoss[0m : 2.51436
[1mStep[0m  [35/53], [94mLoss[0m : 2.44910
[1mStep[0m  [40/53], [94mLoss[0m : 2.44883
[1mStep[0m  [45/53], [94mLoss[0m : 2.45916
[1mStep[0m  [50/53], [94mLoss[0m : 2.45306

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59037
[1mStep[0m  [5/53], [94mLoss[0m : 2.68235
[1mStep[0m  [10/53], [94mLoss[0m : 2.44208
[1mStep[0m  [15/53], [94mLoss[0m : 2.75301
[1mStep[0m  [20/53], [94mLoss[0m : 2.25193
[1mStep[0m  [25/53], [94mLoss[0m : 2.38468
[1mStep[0m  [30/53], [94mLoss[0m : 2.34704
[1mStep[0m  [35/53], [94mLoss[0m : 2.29364
[1mStep[0m  [40/53], [94mLoss[0m : 2.43889
[1mStep[0m  [45/53], [94mLoss[0m : 2.45027
[1mStep[0m  [50/53], [94mLoss[0m : 2.43277

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50031
[1mStep[0m  [5/53], [94mLoss[0m : 2.24009
[1mStep[0m  [10/53], [94mLoss[0m : 2.36767
[1mStep[0m  [15/53], [94mLoss[0m : 2.49040
[1mStep[0m  [20/53], [94mLoss[0m : 2.36135
[1mStep[0m  [25/53], [94mLoss[0m : 2.31122
[1mStep[0m  [30/53], [94mLoss[0m : 2.41756
[1mStep[0m  [35/53], [94mLoss[0m : 2.53156
[1mStep[0m  [40/53], [94mLoss[0m : 2.33386
[1mStep[0m  [45/53], [94mLoss[0m : 2.26384
[1mStep[0m  [50/53], [94mLoss[0m : 2.57838

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60253
[1mStep[0m  [5/53], [94mLoss[0m : 2.25434
[1mStep[0m  [10/53], [94mLoss[0m : 2.48181
[1mStep[0m  [15/53], [94mLoss[0m : 2.56553
[1mStep[0m  [20/53], [94mLoss[0m : 2.23757
[1mStep[0m  [25/53], [94mLoss[0m : 2.32437
[1mStep[0m  [30/53], [94mLoss[0m : 2.44761
[1mStep[0m  [35/53], [94mLoss[0m : 2.33522
[1mStep[0m  [40/53], [94mLoss[0m : 2.43494
[1mStep[0m  [45/53], [94mLoss[0m : 2.45411
[1mStep[0m  [50/53], [94mLoss[0m : 2.36122

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48971
[1mStep[0m  [5/53], [94mLoss[0m : 2.45903
[1mStep[0m  [10/53], [94mLoss[0m : 2.34723
[1mStep[0m  [15/53], [94mLoss[0m : 2.55268
[1mStep[0m  [20/53], [94mLoss[0m : 2.37927
[1mStep[0m  [25/53], [94mLoss[0m : 2.48923
[1mStep[0m  [30/53], [94mLoss[0m : 2.56823
[1mStep[0m  [35/53], [94mLoss[0m : 2.53520
[1mStep[0m  [40/53], [94mLoss[0m : 2.36643
[1mStep[0m  [45/53], [94mLoss[0m : 2.48496
[1mStep[0m  [50/53], [94mLoss[0m : 2.33580

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40545
[1mStep[0m  [5/53], [94mLoss[0m : 2.51109
[1mStep[0m  [10/53], [94mLoss[0m : 2.44027
[1mStep[0m  [15/53], [94mLoss[0m : 2.39502
[1mStep[0m  [20/53], [94mLoss[0m : 2.40673
[1mStep[0m  [25/53], [94mLoss[0m : 2.28873
[1mStep[0m  [30/53], [94mLoss[0m : 2.19491
[1mStep[0m  [35/53], [94mLoss[0m : 2.49797
[1mStep[0m  [40/53], [94mLoss[0m : 2.47939
[1mStep[0m  [45/53], [94mLoss[0m : 2.38759
[1mStep[0m  [50/53], [94mLoss[0m : 2.39995

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42750
[1mStep[0m  [5/53], [94mLoss[0m : 2.58203
[1mStep[0m  [10/53], [94mLoss[0m : 2.43207
[1mStep[0m  [15/53], [94mLoss[0m : 2.36838
[1mStep[0m  [20/53], [94mLoss[0m : 2.40493
[1mStep[0m  [25/53], [94mLoss[0m : 2.44587
[1mStep[0m  [30/53], [94mLoss[0m : 2.55943
[1mStep[0m  [35/53], [94mLoss[0m : 2.40487
[1mStep[0m  [40/53], [94mLoss[0m : 2.58005
[1mStep[0m  [45/53], [94mLoss[0m : 2.29083
[1mStep[0m  [50/53], [94mLoss[0m : 2.36513

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34793
[1mStep[0m  [5/53], [94mLoss[0m : 2.52520
[1mStep[0m  [10/53], [94mLoss[0m : 2.29529
[1mStep[0m  [15/53], [94mLoss[0m : 2.33416
[1mStep[0m  [20/53], [94mLoss[0m : 2.33167
[1mStep[0m  [25/53], [94mLoss[0m : 2.47432
[1mStep[0m  [30/53], [94mLoss[0m : 2.54135
[1mStep[0m  [35/53], [94mLoss[0m : 2.12093
[1mStep[0m  [40/53], [94mLoss[0m : 2.23433
[1mStep[0m  [45/53], [94mLoss[0m : 2.36611
[1mStep[0m  [50/53], [94mLoss[0m : 2.35421

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59327
[1mStep[0m  [5/53], [94mLoss[0m : 2.45168
[1mStep[0m  [10/53], [94mLoss[0m : 2.55544
[1mStep[0m  [15/53], [94mLoss[0m : 2.42727
[1mStep[0m  [20/53], [94mLoss[0m : 2.31543
[1mStep[0m  [25/53], [94mLoss[0m : 2.45166
[1mStep[0m  [30/53], [94mLoss[0m : 2.42883
[1mStep[0m  [35/53], [94mLoss[0m : 2.36195
[1mStep[0m  [40/53], [94mLoss[0m : 2.50982
[1mStep[0m  [45/53], [94mLoss[0m : 2.27339
[1mStep[0m  [50/53], [94mLoss[0m : 2.54387

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25465
[1mStep[0m  [5/53], [94mLoss[0m : 2.38793
[1mStep[0m  [10/53], [94mLoss[0m : 2.42843
[1mStep[0m  [15/53], [94mLoss[0m : 2.28507
[1mStep[0m  [20/53], [94mLoss[0m : 2.38485
[1mStep[0m  [25/53], [94mLoss[0m : 2.56011
[1mStep[0m  [30/53], [94mLoss[0m : 2.45515
[1mStep[0m  [35/53], [94mLoss[0m : 2.22350
[1mStep[0m  [40/53], [94mLoss[0m : 2.31144
[1mStep[0m  [45/53], [94mLoss[0m : 2.41460
[1mStep[0m  [50/53], [94mLoss[0m : 2.35433

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.397, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25789
[1mStep[0m  [5/53], [94mLoss[0m : 2.29619
[1mStep[0m  [10/53], [94mLoss[0m : 2.40400
[1mStep[0m  [15/53], [94mLoss[0m : 2.48227
[1mStep[0m  [20/53], [94mLoss[0m : 2.52468
[1mStep[0m  [25/53], [94mLoss[0m : 2.44128
[1mStep[0m  [30/53], [94mLoss[0m : 2.48432
[1mStep[0m  [35/53], [94mLoss[0m : 2.42526
[1mStep[0m  [40/53], [94mLoss[0m : 2.63132
[1mStep[0m  [45/53], [94mLoss[0m : 2.49657
[1mStep[0m  [50/53], [94mLoss[0m : 2.35203

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43321
[1mStep[0m  [5/53], [94mLoss[0m : 2.29632
[1mStep[0m  [10/53], [94mLoss[0m : 2.31297
[1mStep[0m  [15/53], [94mLoss[0m : 2.53137
[1mStep[0m  [20/53], [94mLoss[0m : 2.21929
[1mStep[0m  [25/53], [94mLoss[0m : 2.45092
[1mStep[0m  [30/53], [94mLoss[0m : 2.45498
[1mStep[0m  [35/53], [94mLoss[0m : 2.37247
[1mStep[0m  [40/53], [94mLoss[0m : 2.36503
[1mStep[0m  [45/53], [94mLoss[0m : 2.36514
[1mStep[0m  [50/53], [94mLoss[0m : 2.43067

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.395, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30629
[1mStep[0m  [5/53], [94mLoss[0m : 2.34412
[1mStep[0m  [10/53], [94mLoss[0m : 2.29370
[1mStep[0m  [15/53], [94mLoss[0m : 2.26608
[1mStep[0m  [20/53], [94mLoss[0m : 2.50695
[1mStep[0m  [25/53], [94mLoss[0m : 2.41039
[1mStep[0m  [30/53], [94mLoss[0m : 2.46729
[1mStep[0m  [35/53], [94mLoss[0m : 2.37444
[1mStep[0m  [40/53], [94mLoss[0m : 2.46021
[1mStep[0m  [45/53], [94mLoss[0m : 2.52269
[1mStep[0m  [50/53], [94mLoss[0m : 2.42430

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30373
[1mStep[0m  [5/53], [94mLoss[0m : 2.26317
[1mStep[0m  [10/53], [94mLoss[0m : 2.31507
[1mStep[0m  [15/53], [94mLoss[0m : 2.35484
[1mStep[0m  [20/53], [94mLoss[0m : 2.31693
[1mStep[0m  [25/53], [94mLoss[0m : 2.47844
[1mStep[0m  [30/53], [94mLoss[0m : 2.50864
[1mStep[0m  [35/53], [94mLoss[0m : 2.58867
[1mStep[0m  [40/53], [94mLoss[0m : 2.53833
[1mStep[0m  [45/53], [94mLoss[0m : 2.32829
[1mStep[0m  [50/53], [94mLoss[0m : 2.38712

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.392, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42498
[1mStep[0m  [5/53], [94mLoss[0m : 2.44490
[1mStep[0m  [10/53], [94mLoss[0m : 2.48570
[1mStep[0m  [15/53], [94mLoss[0m : 2.37714
[1mStep[0m  [20/53], [94mLoss[0m : 2.38411
[1mStep[0m  [25/53], [94mLoss[0m : 2.14119
[1mStep[0m  [30/53], [94mLoss[0m : 2.28414
[1mStep[0m  [35/53], [94mLoss[0m : 2.40698
[1mStep[0m  [40/53], [94mLoss[0m : 2.52100
[1mStep[0m  [45/53], [94mLoss[0m : 2.57903
[1mStep[0m  [50/53], [94mLoss[0m : 2.35613

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.373, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51076
[1mStep[0m  [5/53], [94mLoss[0m : 2.42173
[1mStep[0m  [10/53], [94mLoss[0m : 2.27376
[1mStep[0m  [15/53], [94mLoss[0m : 2.35607
[1mStep[0m  [20/53], [94mLoss[0m : 2.47114
[1mStep[0m  [25/53], [94mLoss[0m : 2.67241
[1mStep[0m  [30/53], [94mLoss[0m : 2.23653
[1mStep[0m  [35/53], [94mLoss[0m : 2.50313
[1mStep[0m  [40/53], [94mLoss[0m : 2.48150
[1mStep[0m  [45/53], [94mLoss[0m : 2.23775
[1mStep[0m  [50/53], [94mLoss[0m : 2.38735

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40506
[1mStep[0m  [5/53], [94mLoss[0m : 2.54816
[1mStep[0m  [10/53], [94mLoss[0m : 2.31258
[1mStep[0m  [15/53], [94mLoss[0m : 2.35713
[1mStep[0m  [20/53], [94mLoss[0m : 2.29602
[1mStep[0m  [25/53], [94mLoss[0m : 2.48413
[1mStep[0m  [30/53], [94mLoss[0m : 2.43082
[1mStep[0m  [35/53], [94mLoss[0m : 2.49274
[1mStep[0m  [40/53], [94mLoss[0m : 2.52532
[1mStep[0m  [45/53], [94mLoss[0m : 2.65171
[1mStep[0m  [50/53], [94mLoss[0m : 2.44446

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42200
[1mStep[0m  [5/53], [94mLoss[0m : 2.32645
[1mStep[0m  [10/53], [94mLoss[0m : 2.39179
[1mStep[0m  [15/53], [94mLoss[0m : 2.33779
[1mStep[0m  [20/53], [94mLoss[0m : 2.38392
[1mStep[0m  [25/53], [94mLoss[0m : 2.41109
[1mStep[0m  [30/53], [94mLoss[0m : 2.34924
[1mStep[0m  [35/53], [94mLoss[0m : 2.18555
[1mStep[0m  [40/53], [94mLoss[0m : 2.37880
[1mStep[0m  [45/53], [94mLoss[0m : 2.50519
[1mStep[0m  [50/53], [94mLoss[0m : 2.30929

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.402, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.378
====================================

Phase 1 - Evaluation MAE:  2.37847494162046
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.36849
[1mStep[0m  [5/53], [94mLoss[0m : 2.46702
[1mStep[0m  [10/53], [94mLoss[0m : 2.63480
[1mStep[0m  [15/53], [94mLoss[0m : 2.43982
[1mStep[0m  [20/53], [94mLoss[0m : 2.51313
[1mStep[0m  [25/53], [94mLoss[0m : 2.46175
[1mStep[0m  [30/53], [94mLoss[0m : 2.54138
[1mStep[0m  [35/53], [94mLoss[0m : 2.47784
[1mStep[0m  [40/53], [94mLoss[0m : 2.32934
[1mStep[0m  [45/53], [94mLoss[0m : 2.38511
[1mStep[0m  [50/53], [94mLoss[0m : 2.33667

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64437
[1mStep[0m  [5/53], [94mLoss[0m : 2.24121
[1mStep[0m  [10/53], [94mLoss[0m : 2.21213
[1mStep[0m  [15/53], [94mLoss[0m : 2.21712
[1mStep[0m  [20/53], [94mLoss[0m : 2.36802
[1mStep[0m  [25/53], [94mLoss[0m : 2.51750
[1mStep[0m  [30/53], [94mLoss[0m : 2.23958
[1mStep[0m  [35/53], [94mLoss[0m : 2.27101
[1mStep[0m  [40/53], [94mLoss[0m : 2.31284
[1mStep[0m  [45/53], [94mLoss[0m : 2.41708
[1mStep[0m  [50/53], [94mLoss[0m : 2.31750

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20322
[1mStep[0m  [5/53], [94mLoss[0m : 2.19547
[1mStep[0m  [10/53], [94mLoss[0m : 2.53480
[1mStep[0m  [15/53], [94mLoss[0m : 2.28854
[1mStep[0m  [20/53], [94mLoss[0m : 2.18981
[1mStep[0m  [25/53], [94mLoss[0m : 2.29050
[1mStep[0m  [30/53], [94mLoss[0m : 2.25622
[1mStep[0m  [35/53], [94mLoss[0m : 2.16243
[1mStep[0m  [40/53], [94mLoss[0m : 2.14236
[1mStep[0m  [45/53], [94mLoss[0m : 2.12565
[1mStep[0m  [50/53], [94mLoss[0m : 2.28677

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.02595
[1mStep[0m  [5/53], [94mLoss[0m : 2.28273
[1mStep[0m  [10/53], [94mLoss[0m : 2.02186
[1mStep[0m  [15/53], [94mLoss[0m : 1.97784
[1mStep[0m  [20/53], [94mLoss[0m : 2.08703
[1mStep[0m  [25/53], [94mLoss[0m : 2.06658
[1mStep[0m  [30/53], [94mLoss[0m : 1.96571
[1mStep[0m  [35/53], [94mLoss[0m : 2.00053
[1mStep[0m  [40/53], [94mLoss[0m : 2.17075
[1mStep[0m  [45/53], [94mLoss[0m : 2.29012
[1mStep[0m  [50/53], [94mLoss[0m : 2.48561

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08111
[1mStep[0m  [5/53], [94mLoss[0m : 2.08259
[1mStep[0m  [10/53], [94mLoss[0m : 1.93001
[1mStep[0m  [15/53], [94mLoss[0m : 1.99806
[1mStep[0m  [20/53], [94mLoss[0m : 2.14978
[1mStep[0m  [25/53], [94mLoss[0m : 2.26665
[1mStep[0m  [30/53], [94mLoss[0m : 2.01529
[1mStep[0m  [35/53], [94mLoss[0m : 2.09706
[1mStep[0m  [40/53], [94mLoss[0m : 2.08408
[1mStep[0m  [45/53], [94mLoss[0m : 2.16797
[1mStep[0m  [50/53], [94mLoss[0m : 1.84156

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84314
[1mStep[0m  [5/53], [94mLoss[0m : 1.95654
[1mStep[0m  [10/53], [94mLoss[0m : 1.83954
[1mStep[0m  [15/53], [94mLoss[0m : 2.14407
[1mStep[0m  [20/53], [94mLoss[0m : 1.91732
[1mStep[0m  [25/53], [94mLoss[0m : 1.92414
[1mStep[0m  [30/53], [94mLoss[0m : 1.99931
[1mStep[0m  [35/53], [94mLoss[0m : 2.04059
[1mStep[0m  [40/53], [94mLoss[0m : 1.88279
[1mStep[0m  [45/53], [94mLoss[0m : 2.14480
[1mStep[0m  [50/53], [94mLoss[0m : 2.04011

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71285
[1mStep[0m  [5/53], [94mLoss[0m : 1.83402
[1mStep[0m  [10/53], [94mLoss[0m : 1.95086
[1mStep[0m  [15/53], [94mLoss[0m : 1.78462
[1mStep[0m  [20/53], [94mLoss[0m : 1.95905
[1mStep[0m  [25/53], [94mLoss[0m : 1.99051
[1mStep[0m  [30/53], [94mLoss[0m : 1.96784
[1mStep[0m  [35/53], [94mLoss[0m : 1.97596
[1mStep[0m  [40/53], [94mLoss[0m : 1.92663
[1mStep[0m  [45/53], [94mLoss[0m : 2.04241
[1mStep[0m  [50/53], [94mLoss[0m : 1.88939

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86675
[1mStep[0m  [5/53], [94mLoss[0m : 1.75160
[1mStep[0m  [10/53], [94mLoss[0m : 1.79592
[1mStep[0m  [15/53], [94mLoss[0m : 1.83531
[1mStep[0m  [20/53], [94mLoss[0m : 2.07226
[1mStep[0m  [25/53], [94mLoss[0m : 1.75295
[1mStep[0m  [30/53], [94mLoss[0m : 1.85618
[1mStep[0m  [35/53], [94mLoss[0m : 1.94724
[1mStep[0m  [40/53], [94mLoss[0m : 1.89829
[1mStep[0m  [45/53], [94mLoss[0m : 1.80362
[1mStep[0m  [50/53], [94mLoss[0m : 1.90439

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72607
[1mStep[0m  [5/53], [94mLoss[0m : 1.90120
[1mStep[0m  [10/53], [94mLoss[0m : 1.68209
[1mStep[0m  [15/53], [94mLoss[0m : 1.80043
[1mStep[0m  [20/53], [94mLoss[0m : 1.69832
[1mStep[0m  [25/53], [94mLoss[0m : 1.78633
[1mStep[0m  [30/53], [94mLoss[0m : 1.64498
[1mStep[0m  [35/53], [94mLoss[0m : 1.80870
[1mStep[0m  [40/53], [94mLoss[0m : 1.96046
[1mStep[0m  [45/53], [94mLoss[0m : 2.04284
[1mStep[0m  [50/53], [94mLoss[0m : 1.77084

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.85683
[1mStep[0m  [5/53], [94mLoss[0m : 1.58736
[1mStep[0m  [10/53], [94mLoss[0m : 1.60089
[1mStep[0m  [15/53], [94mLoss[0m : 1.78845
[1mStep[0m  [20/53], [94mLoss[0m : 1.91859
[1mStep[0m  [25/53], [94mLoss[0m : 1.69836
[1mStep[0m  [30/53], [94mLoss[0m : 1.81840
[1mStep[0m  [35/53], [94mLoss[0m : 1.89852
[1mStep[0m  [40/53], [94mLoss[0m : 2.04139
[1mStep[0m  [45/53], [94mLoss[0m : 1.94473
[1mStep[0m  [50/53], [94mLoss[0m : 2.09559

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.56930
[1mStep[0m  [5/53], [94mLoss[0m : 1.67258
[1mStep[0m  [10/53], [94mLoss[0m : 1.54383
[1mStep[0m  [15/53], [94mLoss[0m : 1.62386
[1mStep[0m  [20/53], [94mLoss[0m : 1.67125
[1mStep[0m  [25/53], [94mLoss[0m : 1.49919
[1mStep[0m  [30/53], [94mLoss[0m : 1.60237
[1mStep[0m  [35/53], [94mLoss[0m : 1.81961
[1mStep[0m  [40/53], [94mLoss[0m : 1.73539
[1mStep[0m  [45/53], [94mLoss[0m : 1.74996
[1mStep[0m  [50/53], [94mLoss[0m : 1.89915

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.44267
[1mStep[0m  [5/53], [94mLoss[0m : 1.55087
[1mStep[0m  [10/53], [94mLoss[0m : 1.47342
[1mStep[0m  [15/53], [94mLoss[0m : 1.80728
[1mStep[0m  [20/53], [94mLoss[0m : 1.70446
[1mStep[0m  [25/53], [94mLoss[0m : 1.61730
[1mStep[0m  [30/53], [94mLoss[0m : 1.68681
[1mStep[0m  [35/53], [94mLoss[0m : 1.62384
[1mStep[0m  [40/53], [94mLoss[0m : 1.61284
[1mStep[0m  [45/53], [94mLoss[0m : 1.66991
[1mStep[0m  [50/53], [94mLoss[0m : 1.88174

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.49658
[1mStep[0m  [5/53], [94mLoss[0m : 1.63414
[1mStep[0m  [10/53], [94mLoss[0m : 1.64482
[1mStep[0m  [15/53], [94mLoss[0m : 1.56185
[1mStep[0m  [20/53], [94mLoss[0m : 1.63168
[1mStep[0m  [25/53], [94mLoss[0m : 1.79277
[1mStep[0m  [30/53], [94mLoss[0m : 1.41556
[1mStep[0m  [35/53], [94mLoss[0m : 1.55112
[1mStep[0m  [40/53], [94mLoss[0m : 1.61985
[1mStep[0m  [45/53], [94mLoss[0m : 1.69425
[1mStep[0m  [50/53], [94mLoss[0m : 1.67848

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.41423
[1mStep[0m  [5/53], [94mLoss[0m : 1.56195
[1mStep[0m  [10/53], [94mLoss[0m : 1.47182
[1mStep[0m  [15/53], [94mLoss[0m : 1.56177
[1mStep[0m  [20/53], [94mLoss[0m : 1.81173
[1mStep[0m  [25/53], [94mLoss[0m : 1.54370
[1mStep[0m  [30/53], [94mLoss[0m : 1.63943
[1mStep[0m  [35/53], [94mLoss[0m : 1.55810
[1mStep[0m  [40/53], [94mLoss[0m : 1.73213
[1mStep[0m  [45/53], [94mLoss[0m : 1.58196
[1mStep[0m  [50/53], [94mLoss[0m : 1.70650

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.606, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64944
[1mStep[0m  [5/53], [94mLoss[0m : 1.38189
[1mStep[0m  [10/53], [94mLoss[0m : 1.62955
[1mStep[0m  [15/53], [94mLoss[0m : 1.70994
[1mStep[0m  [20/53], [94mLoss[0m : 1.56331
[1mStep[0m  [25/53], [94mLoss[0m : 1.61084
[1mStep[0m  [30/53], [94mLoss[0m : 1.56062
[1mStep[0m  [35/53], [94mLoss[0m : 1.52096
[1mStep[0m  [40/53], [94mLoss[0m : 1.61734
[1mStep[0m  [45/53], [94mLoss[0m : 1.60535
[1mStep[0m  [50/53], [94mLoss[0m : 1.69537

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.571, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.47141
[1mStep[0m  [5/53], [94mLoss[0m : 1.41992
[1mStep[0m  [10/53], [94mLoss[0m : 1.46278
[1mStep[0m  [15/53], [94mLoss[0m : 1.68711
[1mStep[0m  [20/53], [94mLoss[0m : 1.48871
[1mStep[0m  [25/53], [94mLoss[0m : 1.38728
[1mStep[0m  [30/53], [94mLoss[0m : 1.47562
[1mStep[0m  [35/53], [94mLoss[0m : 1.59847
[1mStep[0m  [40/53], [94mLoss[0m : 1.46270
[1mStep[0m  [45/53], [94mLoss[0m : 1.62878
[1mStep[0m  [50/53], [94mLoss[0m : 1.44811

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.45433
[1mStep[0m  [5/53], [94mLoss[0m : 1.37822
[1mStep[0m  [10/53], [94mLoss[0m : 1.47889
[1mStep[0m  [15/53], [94mLoss[0m : 1.70792
[1mStep[0m  [20/53], [94mLoss[0m : 1.51081
[1mStep[0m  [25/53], [94mLoss[0m : 1.47486
[1mStep[0m  [30/53], [94mLoss[0m : 1.50184
[1mStep[0m  [35/53], [94mLoss[0m : 1.38165
[1mStep[0m  [40/53], [94mLoss[0m : 1.48170
[1mStep[0m  [45/53], [94mLoss[0m : 1.43595
[1mStep[0m  [50/53], [94mLoss[0m : 1.77851

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54917
[1mStep[0m  [5/53], [94mLoss[0m : 1.62592
[1mStep[0m  [10/53], [94mLoss[0m : 1.44826
[1mStep[0m  [15/53], [94mLoss[0m : 1.38885
[1mStep[0m  [20/53], [94mLoss[0m : 1.25030
[1mStep[0m  [25/53], [94mLoss[0m : 1.32640
[1mStep[0m  [30/53], [94mLoss[0m : 1.46882
[1mStep[0m  [35/53], [94mLoss[0m : 1.42981
[1mStep[0m  [40/53], [94mLoss[0m : 1.47168
[1mStep[0m  [45/53], [94mLoss[0m : 1.41486
[1mStep[0m  [50/53], [94mLoss[0m : 1.42571

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.458, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46469
[1mStep[0m  [5/53], [94mLoss[0m : 1.35247
[1mStep[0m  [10/53], [94mLoss[0m : 1.34221
[1mStep[0m  [15/53], [94mLoss[0m : 1.43234
[1mStep[0m  [20/53], [94mLoss[0m : 1.50921
[1mStep[0m  [25/53], [94mLoss[0m : 1.45975
[1mStep[0m  [30/53], [94mLoss[0m : 1.32203
[1mStep[0m  [35/53], [94mLoss[0m : 1.54681
[1mStep[0m  [40/53], [94mLoss[0m : 1.37216
[1mStep[0m  [45/53], [94mLoss[0m : 1.51018
[1mStep[0m  [50/53], [94mLoss[0m : 1.51549

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.29874
[1mStep[0m  [5/53], [94mLoss[0m : 1.43260
[1mStep[0m  [10/53], [94mLoss[0m : 1.30497
[1mStep[0m  [15/53], [94mLoss[0m : 1.36814
[1mStep[0m  [20/53], [94mLoss[0m : 1.49326
[1mStep[0m  [25/53], [94mLoss[0m : 1.32302
[1mStep[0m  [30/53], [94mLoss[0m : 1.51577
[1mStep[0m  [35/53], [94mLoss[0m : 1.41262
[1mStep[0m  [40/53], [94mLoss[0m : 1.52139
[1mStep[0m  [45/53], [94mLoss[0m : 1.58590
[1mStep[0m  [50/53], [94mLoss[0m : 1.35844

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.412, [92mTest[0m: 2.570, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.41993
[1mStep[0m  [5/53], [94mLoss[0m : 1.20439
[1mStep[0m  [10/53], [94mLoss[0m : 1.25494
[1mStep[0m  [15/53], [94mLoss[0m : 1.40551
[1mStep[0m  [20/53], [94mLoss[0m : 1.28173
[1mStep[0m  [25/53], [94mLoss[0m : 1.30727
[1mStep[0m  [30/53], [94mLoss[0m : 1.40140
[1mStep[0m  [35/53], [94mLoss[0m : 1.34167
[1mStep[0m  [40/53], [94mLoss[0m : 1.52748
[1mStep[0m  [45/53], [94mLoss[0m : 1.41716
[1mStep[0m  [50/53], [94mLoss[0m : 1.38816

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.368, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.468
====================================

Phase 2 - Evaluation MAE:  2.46834566959968
MAE score P1      2.378475
MAE score P2      2.468346
loss              1.368479
learning_rate         0.01
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.92348
[1mStep[0m  [5/53], [94mLoss[0m : 10.62775
[1mStep[0m  [10/53], [94mLoss[0m : 10.42617
[1mStep[0m  [15/53], [94mLoss[0m : 9.65975
[1mStep[0m  [20/53], [94mLoss[0m : 8.99239
[1mStep[0m  [25/53], [94mLoss[0m : 8.62089
[1mStep[0m  [30/53], [94mLoss[0m : 8.34529
[1mStep[0m  [35/53], [94mLoss[0m : 6.86780
[1mStep[0m  [40/53], [94mLoss[0m : 6.07203
[1mStep[0m  [45/53], [94mLoss[0m : 5.09089
[1mStep[0m  [50/53], [94mLoss[0m : 4.10683

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.181, [92mTest[0m: 10.995, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.51214
[1mStep[0m  [5/53], [94mLoss[0m : 2.71834
[1mStep[0m  [10/53], [94mLoss[0m : 2.68269
[1mStep[0m  [15/53], [94mLoss[0m : 2.43046
[1mStep[0m  [20/53], [94mLoss[0m : 2.77603
[1mStep[0m  [25/53], [94mLoss[0m : 2.91795
[1mStep[0m  [30/53], [94mLoss[0m : 2.54692
[1mStep[0m  [35/53], [94mLoss[0m : 2.83260
[1mStep[0m  [40/53], [94mLoss[0m : 2.71748
[1mStep[0m  [45/53], [94mLoss[0m : 2.53312
[1mStep[0m  [50/53], [94mLoss[0m : 2.73319

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.769, [92mTest[0m: 3.190, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.85337
[1mStep[0m  [5/53], [94mLoss[0m : 2.49121
[1mStep[0m  [10/53], [94mLoss[0m : 2.74141
[1mStep[0m  [15/53], [94mLoss[0m : 2.81469
[1mStep[0m  [20/53], [94mLoss[0m : 2.73456
[1mStep[0m  [25/53], [94mLoss[0m : 2.55025
[1mStep[0m  [30/53], [94mLoss[0m : 2.70025
[1mStep[0m  [35/53], [94mLoss[0m : 2.55193
[1mStep[0m  [40/53], [94mLoss[0m : 2.65582
[1mStep[0m  [45/53], [94mLoss[0m : 2.61029
[1mStep[0m  [50/53], [94mLoss[0m : 2.64768

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61654
[1mStep[0m  [5/53], [94mLoss[0m : 2.67960
[1mStep[0m  [10/53], [94mLoss[0m : 2.60623
[1mStep[0m  [15/53], [94mLoss[0m : 2.52588
[1mStep[0m  [20/53], [94mLoss[0m : 2.42763
[1mStep[0m  [25/53], [94mLoss[0m : 2.43727
[1mStep[0m  [30/53], [94mLoss[0m : 2.33111
[1mStep[0m  [35/53], [94mLoss[0m : 2.62616
[1mStep[0m  [40/53], [94mLoss[0m : 2.71384
[1mStep[0m  [45/53], [94mLoss[0m : 2.60602
[1mStep[0m  [50/53], [94mLoss[0m : 2.59451

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47154
[1mStep[0m  [5/53], [94mLoss[0m : 2.46656
[1mStep[0m  [10/53], [94mLoss[0m : 2.72367
[1mStep[0m  [15/53], [94mLoss[0m : 2.34174
[1mStep[0m  [20/53], [94mLoss[0m : 2.32970
[1mStep[0m  [25/53], [94mLoss[0m : 2.22915
[1mStep[0m  [30/53], [94mLoss[0m : 2.47522
[1mStep[0m  [35/53], [94mLoss[0m : 2.47807
[1mStep[0m  [40/53], [94mLoss[0m : 2.69842
[1mStep[0m  [45/53], [94mLoss[0m : 2.18557
[1mStep[0m  [50/53], [94mLoss[0m : 2.62807

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55882
[1mStep[0m  [5/53], [94mLoss[0m : 2.32508
[1mStep[0m  [10/53], [94mLoss[0m : 2.35750
[1mStep[0m  [15/53], [94mLoss[0m : 2.60272
[1mStep[0m  [20/53], [94mLoss[0m : 2.70538
[1mStep[0m  [25/53], [94mLoss[0m : 2.69503
[1mStep[0m  [30/53], [94mLoss[0m : 2.62045
[1mStep[0m  [35/53], [94mLoss[0m : 2.61541
[1mStep[0m  [40/53], [94mLoss[0m : 2.54677
[1mStep[0m  [45/53], [94mLoss[0m : 2.65429
[1mStep[0m  [50/53], [94mLoss[0m : 2.54488

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71475
[1mStep[0m  [5/53], [94mLoss[0m : 2.15948
[1mStep[0m  [10/53], [94mLoss[0m : 2.60266
[1mStep[0m  [15/53], [94mLoss[0m : 2.43121
[1mStep[0m  [20/53], [94mLoss[0m : 2.50429
[1mStep[0m  [25/53], [94mLoss[0m : 2.40499
[1mStep[0m  [30/53], [94mLoss[0m : 2.64509
[1mStep[0m  [35/53], [94mLoss[0m : 2.46620
[1mStep[0m  [40/53], [94mLoss[0m : 2.57449
[1mStep[0m  [45/53], [94mLoss[0m : 2.57248
[1mStep[0m  [50/53], [94mLoss[0m : 2.36533

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29161
[1mStep[0m  [5/53], [94mLoss[0m : 2.58969
[1mStep[0m  [10/53], [94mLoss[0m : 2.44293
[1mStep[0m  [15/53], [94mLoss[0m : 2.58557
[1mStep[0m  [20/53], [94mLoss[0m : 2.50129
[1mStep[0m  [25/53], [94mLoss[0m : 2.36292
[1mStep[0m  [30/53], [94mLoss[0m : 2.42128
[1mStep[0m  [35/53], [94mLoss[0m : 2.35149
[1mStep[0m  [40/53], [94mLoss[0m : 2.32454
[1mStep[0m  [45/53], [94mLoss[0m : 2.74394
[1mStep[0m  [50/53], [94mLoss[0m : 2.26910

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35915
[1mStep[0m  [5/53], [94mLoss[0m : 2.41774
[1mStep[0m  [10/53], [94mLoss[0m : 2.65995
[1mStep[0m  [15/53], [94mLoss[0m : 2.40083
[1mStep[0m  [20/53], [94mLoss[0m : 2.47009
[1mStep[0m  [25/53], [94mLoss[0m : 2.52830
[1mStep[0m  [30/53], [94mLoss[0m : 2.50385
[1mStep[0m  [35/53], [94mLoss[0m : 2.61487
[1mStep[0m  [40/53], [94mLoss[0m : 2.61696
[1mStep[0m  [45/53], [94mLoss[0m : 2.24132
[1mStep[0m  [50/53], [94mLoss[0m : 2.30493

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31313
[1mStep[0m  [5/53], [94mLoss[0m : 2.25778
[1mStep[0m  [10/53], [94mLoss[0m : 2.58683
[1mStep[0m  [15/53], [94mLoss[0m : 2.73909
[1mStep[0m  [20/53], [94mLoss[0m : 2.47402
[1mStep[0m  [25/53], [94mLoss[0m : 2.41879
[1mStep[0m  [30/53], [94mLoss[0m : 2.54167
[1mStep[0m  [35/53], [94mLoss[0m : 2.23450
[1mStep[0m  [40/53], [94mLoss[0m : 2.54632
[1mStep[0m  [45/53], [94mLoss[0m : 2.28173
[1mStep[0m  [50/53], [94mLoss[0m : 2.38333

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44643
[1mStep[0m  [5/53], [94mLoss[0m : 2.45567
[1mStep[0m  [10/53], [94mLoss[0m : 2.44326
[1mStep[0m  [15/53], [94mLoss[0m : 2.58876
[1mStep[0m  [20/53], [94mLoss[0m : 2.42088
[1mStep[0m  [25/53], [94mLoss[0m : 2.24732
[1mStep[0m  [30/53], [94mLoss[0m : 2.72118
[1mStep[0m  [35/53], [94mLoss[0m : 2.62475
[1mStep[0m  [40/53], [94mLoss[0m : 2.24457
[1mStep[0m  [45/53], [94mLoss[0m : 2.48546
[1mStep[0m  [50/53], [94mLoss[0m : 2.44755

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35686
[1mStep[0m  [5/53], [94mLoss[0m : 2.35730
[1mStep[0m  [10/53], [94mLoss[0m : 2.41609
[1mStep[0m  [15/53], [94mLoss[0m : 2.32039
[1mStep[0m  [20/53], [94mLoss[0m : 2.59211
[1mStep[0m  [25/53], [94mLoss[0m : 2.14758
[1mStep[0m  [30/53], [94mLoss[0m : 2.32340
[1mStep[0m  [35/53], [94mLoss[0m : 2.21994
[1mStep[0m  [40/53], [94mLoss[0m : 2.40543
[1mStep[0m  [45/53], [94mLoss[0m : 2.31340
[1mStep[0m  [50/53], [94mLoss[0m : 2.56223

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44553
[1mStep[0m  [5/53], [94mLoss[0m : 2.38019
[1mStep[0m  [10/53], [94mLoss[0m : 2.61411
[1mStep[0m  [15/53], [94mLoss[0m : 2.42780
[1mStep[0m  [20/53], [94mLoss[0m : 2.29853
[1mStep[0m  [25/53], [94mLoss[0m : 2.21353
[1mStep[0m  [30/53], [94mLoss[0m : 2.57860
[1mStep[0m  [35/53], [94mLoss[0m : 2.50501
[1mStep[0m  [40/53], [94mLoss[0m : 2.52663
[1mStep[0m  [45/53], [94mLoss[0m : 2.47046
[1mStep[0m  [50/53], [94mLoss[0m : 2.14964

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33917
[1mStep[0m  [5/53], [94mLoss[0m : 2.09176
[1mStep[0m  [10/53], [94mLoss[0m : 2.51956
[1mStep[0m  [15/53], [94mLoss[0m : 2.46665
[1mStep[0m  [20/53], [94mLoss[0m : 2.44365
[1mStep[0m  [25/53], [94mLoss[0m : 2.51163
[1mStep[0m  [30/53], [94mLoss[0m : 2.55948
[1mStep[0m  [35/53], [94mLoss[0m : 2.38910
[1mStep[0m  [40/53], [94mLoss[0m : 2.42461
[1mStep[0m  [45/53], [94mLoss[0m : 2.55002
[1mStep[0m  [50/53], [94mLoss[0m : 2.49877

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.23803
[1mStep[0m  [5/53], [94mLoss[0m : 2.55329
[1mStep[0m  [10/53], [94mLoss[0m : 2.59073
[1mStep[0m  [15/53], [94mLoss[0m : 2.50111
[1mStep[0m  [20/53], [94mLoss[0m : 2.28877
[1mStep[0m  [25/53], [94mLoss[0m : 2.27507
[1mStep[0m  [30/53], [94mLoss[0m : 2.17636
[1mStep[0m  [35/53], [94mLoss[0m : 2.62107
[1mStep[0m  [40/53], [94mLoss[0m : 2.47904
[1mStep[0m  [45/53], [94mLoss[0m : 2.56790
[1mStep[0m  [50/53], [94mLoss[0m : 2.37981

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52573
[1mStep[0m  [5/53], [94mLoss[0m : 2.34141
[1mStep[0m  [10/53], [94mLoss[0m : 2.35237
[1mStep[0m  [15/53], [94mLoss[0m : 2.36866
[1mStep[0m  [20/53], [94mLoss[0m : 2.36313
[1mStep[0m  [25/53], [94mLoss[0m : 2.48428
[1mStep[0m  [30/53], [94mLoss[0m : 2.41874
[1mStep[0m  [35/53], [94mLoss[0m : 2.42428
[1mStep[0m  [40/53], [94mLoss[0m : 2.37121
[1mStep[0m  [45/53], [94mLoss[0m : 2.34443
[1mStep[0m  [50/53], [94mLoss[0m : 2.51413

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32710
[1mStep[0m  [5/53], [94mLoss[0m : 2.26006
[1mStep[0m  [10/53], [94mLoss[0m : 2.27638
[1mStep[0m  [15/53], [94mLoss[0m : 2.37236
[1mStep[0m  [20/53], [94mLoss[0m : 2.30365
[1mStep[0m  [25/53], [94mLoss[0m : 2.35458
[1mStep[0m  [30/53], [94mLoss[0m : 2.20859
[1mStep[0m  [35/53], [94mLoss[0m : 2.43026
[1mStep[0m  [40/53], [94mLoss[0m : 2.29949
[1mStep[0m  [45/53], [94mLoss[0m : 2.51668
[1mStep[0m  [50/53], [94mLoss[0m : 2.46988

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21323
[1mStep[0m  [5/53], [94mLoss[0m : 2.26312
[1mStep[0m  [10/53], [94mLoss[0m : 2.30305
[1mStep[0m  [15/53], [94mLoss[0m : 2.63569
[1mStep[0m  [20/53], [94mLoss[0m : 2.33150
[1mStep[0m  [25/53], [94mLoss[0m : 2.27620
[1mStep[0m  [30/53], [94mLoss[0m : 2.15139
[1mStep[0m  [35/53], [94mLoss[0m : 2.52686
[1mStep[0m  [40/53], [94mLoss[0m : 2.57337
[1mStep[0m  [45/53], [94mLoss[0m : 2.44254
[1mStep[0m  [50/53], [94mLoss[0m : 2.50056

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29055
[1mStep[0m  [5/53], [94mLoss[0m : 2.40748
[1mStep[0m  [10/53], [94mLoss[0m : 2.22838
[1mStep[0m  [15/53], [94mLoss[0m : 2.52441
[1mStep[0m  [20/53], [94mLoss[0m : 2.12958
[1mStep[0m  [25/53], [94mLoss[0m : 2.53143
[1mStep[0m  [30/53], [94mLoss[0m : 2.29499
[1mStep[0m  [35/53], [94mLoss[0m : 2.41220
[1mStep[0m  [40/53], [94mLoss[0m : 2.61789
[1mStep[0m  [45/53], [94mLoss[0m : 2.34182
[1mStep[0m  [50/53], [94mLoss[0m : 2.31706

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24772
[1mStep[0m  [5/53], [94mLoss[0m : 2.41927
[1mStep[0m  [10/53], [94mLoss[0m : 2.47737
[1mStep[0m  [15/53], [94mLoss[0m : 2.37275
[1mStep[0m  [20/53], [94mLoss[0m : 2.28642
[1mStep[0m  [25/53], [94mLoss[0m : 2.37901
[1mStep[0m  [30/53], [94mLoss[0m : 2.54102
[1mStep[0m  [35/53], [94mLoss[0m : 2.57572
[1mStep[0m  [40/53], [94mLoss[0m : 2.22896
[1mStep[0m  [45/53], [94mLoss[0m : 2.45457
[1mStep[0m  [50/53], [94mLoss[0m : 2.37778

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27297
[1mStep[0m  [5/53], [94mLoss[0m : 2.45569
[1mStep[0m  [10/53], [94mLoss[0m : 2.26669
[1mStep[0m  [15/53], [94mLoss[0m : 2.32710
[1mStep[0m  [20/53], [94mLoss[0m : 2.37192
[1mStep[0m  [25/53], [94mLoss[0m : 2.40104
[1mStep[0m  [30/53], [94mLoss[0m : 2.52434
[1mStep[0m  [35/53], [94mLoss[0m : 2.40981
[1mStep[0m  [40/53], [94mLoss[0m : 2.40347
[1mStep[0m  [45/53], [94mLoss[0m : 2.50633
[1mStep[0m  [50/53], [94mLoss[0m : 2.51340

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.388, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30695
[1mStep[0m  [5/53], [94mLoss[0m : 2.54516
[1mStep[0m  [10/53], [94mLoss[0m : 2.48224
[1mStep[0m  [15/53], [94mLoss[0m : 2.64056
[1mStep[0m  [20/53], [94mLoss[0m : 2.58863
[1mStep[0m  [25/53], [94mLoss[0m : 2.33678
[1mStep[0m  [30/53], [94mLoss[0m : 2.45012
[1mStep[0m  [35/53], [94mLoss[0m : 2.25384
[1mStep[0m  [40/53], [94mLoss[0m : 2.50517
[1mStep[0m  [45/53], [94mLoss[0m : 2.53966
[1mStep[0m  [50/53], [94mLoss[0m : 2.45044

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32522
[1mStep[0m  [5/53], [94mLoss[0m : 2.32829
[1mStep[0m  [10/53], [94mLoss[0m : 2.46445
[1mStep[0m  [15/53], [94mLoss[0m : 2.45390
[1mStep[0m  [20/53], [94mLoss[0m : 2.25668
[1mStep[0m  [25/53], [94mLoss[0m : 2.31706
[1mStep[0m  [30/53], [94mLoss[0m : 2.48222
[1mStep[0m  [35/53], [94mLoss[0m : 2.39469
[1mStep[0m  [40/53], [94mLoss[0m : 2.58379
[1mStep[0m  [45/53], [94mLoss[0m : 2.51698
[1mStep[0m  [50/53], [94mLoss[0m : 2.46094

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.365, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41027
[1mStep[0m  [5/53], [94mLoss[0m : 2.20780
[1mStep[0m  [10/53], [94mLoss[0m : 2.34727
[1mStep[0m  [15/53], [94mLoss[0m : 2.62498
[1mStep[0m  [20/53], [94mLoss[0m : 2.33440
[1mStep[0m  [25/53], [94mLoss[0m : 2.28223
[1mStep[0m  [30/53], [94mLoss[0m : 2.29973
[1mStep[0m  [35/53], [94mLoss[0m : 2.22231
[1mStep[0m  [40/53], [94mLoss[0m : 2.33713
[1mStep[0m  [45/53], [94mLoss[0m : 2.48005
[1mStep[0m  [50/53], [94mLoss[0m : 2.42119

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54384
[1mStep[0m  [5/53], [94mLoss[0m : 2.30094
[1mStep[0m  [10/53], [94mLoss[0m : 2.47215
[1mStep[0m  [15/53], [94mLoss[0m : 2.46991
[1mStep[0m  [20/53], [94mLoss[0m : 2.36624
[1mStep[0m  [25/53], [94mLoss[0m : 2.32765
[1mStep[0m  [30/53], [94mLoss[0m : 2.52193
[1mStep[0m  [35/53], [94mLoss[0m : 2.22877
[1mStep[0m  [40/53], [94mLoss[0m : 2.51340
[1mStep[0m  [45/53], [94mLoss[0m : 2.31598
[1mStep[0m  [50/53], [94mLoss[0m : 2.25543

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.364, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36614
[1mStep[0m  [5/53], [94mLoss[0m : 2.22305
[1mStep[0m  [10/53], [94mLoss[0m : 2.35644
[1mStep[0m  [15/53], [94mLoss[0m : 2.03492
[1mStep[0m  [20/53], [94mLoss[0m : 2.33375
[1mStep[0m  [25/53], [94mLoss[0m : 2.22702
[1mStep[0m  [30/53], [94mLoss[0m : 2.63807
[1mStep[0m  [35/53], [94mLoss[0m : 2.53830
[1mStep[0m  [40/53], [94mLoss[0m : 2.36797
[1mStep[0m  [45/53], [94mLoss[0m : 2.53070
[1mStep[0m  [50/53], [94mLoss[0m : 2.61111

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28367
[1mStep[0m  [5/53], [94mLoss[0m : 2.33006
[1mStep[0m  [10/53], [94mLoss[0m : 2.49652
[1mStep[0m  [15/53], [94mLoss[0m : 2.33255
[1mStep[0m  [20/53], [94mLoss[0m : 2.35119
[1mStep[0m  [25/53], [94mLoss[0m : 2.38481
[1mStep[0m  [30/53], [94mLoss[0m : 2.18480
[1mStep[0m  [35/53], [94mLoss[0m : 2.29069
[1mStep[0m  [40/53], [94mLoss[0m : 2.24843
[1mStep[0m  [45/53], [94mLoss[0m : 2.27224
[1mStep[0m  [50/53], [94mLoss[0m : 2.36153

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.363, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21403
[1mStep[0m  [5/53], [94mLoss[0m : 2.40958
[1mStep[0m  [10/53], [94mLoss[0m : 2.39839
[1mStep[0m  [15/53], [94mLoss[0m : 2.35918
[1mStep[0m  [20/53], [94mLoss[0m : 2.36575
[1mStep[0m  [25/53], [94mLoss[0m : 2.48018
[1mStep[0m  [30/53], [94mLoss[0m : 2.27260
[1mStep[0m  [35/53], [94mLoss[0m : 2.46015
[1mStep[0m  [40/53], [94mLoss[0m : 2.44988
[1mStep[0m  [45/53], [94mLoss[0m : 2.41972
[1mStep[0m  [50/53], [94mLoss[0m : 2.25029

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.365, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28205
[1mStep[0m  [5/53], [94mLoss[0m : 2.31422
[1mStep[0m  [10/53], [94mLoss[0m : 2.23685
[1mStep[0m  [15/53], [94mLoss[0m : 2.48136
[1mStep[0m  [20/53], [94mLoss[0m : 2.40748
[1mStep[0m  [25/53], [94mLoss[0m : 2.26858
[1mStep[0m  [30/53], [94mLoss[0m : 2.35146
[1mStep[0m  [35/53], [94mLoss[0m : 2.60534
[1mStep[0m  [40/53], [94mLoss[0m : 2.14072
[1mStep[0m  [45/53], [94mLoss[0m : 2.53917
[1mStep[0m  [50/53], [94mLoss[0m : 2.36627

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52892
[1mStep[0m  [5/53], [94mLoss[0m : 2.45290
[1mStep[0m  [10/53], [94mLoss[0m : 2.49510
[1mStep[0m  [15/53], [94mLoss[0m : 2.39782
[1mStep[0m  [20/53], [94mLoss[0m : 2.35504
[1mStep[0m  [25/53], [94mLoss[0m : 2.43475
[1mStep[0m  [30/53], [94mLoss[0m : 2.30699
[1mStep[0m  [35/53], [94mLoss[0m : 2.45706
[1mStep[0m  [40/53], [94mLoss[0m : 2.13615
[1mStep[0m  [45/53], [94mLoss[0m : 2.43040
[1mStep[0m  [50/53], [94mLoss[0m : 2.37277

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.370, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.362
====================================

Phase 1 - Evaluation MAE:  2.3622836149655857
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.38180
[1mStep[0m  [5/53], [94mLoss[0m : 2.55081
[1mStep[0m  [10/53], [94mLoss[0m : 2.25806
[1mStep[0m  [15/53], [94mLoss[0m : 2.45654
[1mStep[0m  [20/53], [94mLoss[0m : 2.53741
[1mStep[0m  [25/53], [94mLoss[0m : 2.50514
[1mStep[0m  [30/53], [94mLoss[0m : 2.47573
[1mStep[0m  [35/53], [94mLoss[0m : 2.27734
[1mStep[0m  [40/53], [94mLoss[0m : 2.34820
[1mStep[0m  [45/53], [94mLoss[0m : 2.51261
[1mStep[0m  [50/53], [94mLoss[0m : 2.44957

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43121
[1mStep[0m  [5/53], [94mLoss[0m : 2.50239
[1mStep[0m  [10/53], [94mLoss[0m : 2.30654
[1mStep[0m  [15/53], [94mLoss[0m : 2.33613
[1mStep[0m  [20/53], [94mLoss[0m : 2.20847
[1mStep[0m  [25/53], [94mLoss[0m : 2.26215
[1mStep[0m  [30/53], [94mLoss[0m : 2.14215
[1mStep[0m  [35/53], [94mLoss[0m : 2.26001
[1mStep[0m  [40/53], [94mLoss[0m : 2.58743
[1mStep[0m  [45/53], [94mLoss[0m : 2.42671
[1mStep[0m  [50/53], [94mLoss[0m : 2.49066

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.07874
[1mStep[0m  [5/53], [94mLoss[0m : 2.26296
[1mStep[0m  [10/53], [94mLoss[0m : 2.10559
[1mStep[0m  [15/53], [94mLoss[0m : 2.31684
[1mStep[0m  [20/53], [94mLoss[0m : 2.28998
[1mStep[0m  [25/53], [94mLoss[0m : 2.19732
[1mStep[0m  [30/53], [94mLoss[0m : 2.12812
[1mStep[0m  [35/53], [94mLoss[0m : 2.13235
[1mStep[0m  [40/53], [94mLoss[0m : 2.32543
[1mStep[0m  [45/53], [94mLoss[0m : 1.99877
[1mStep[0m  [50/53], [94mLoss[0m : 2.50219

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34391
[1mStep[0m  [5/53], [94mLoss[0m : 2.10183
[1mStep[0m  [10/53], [94mLoss[0m : 1.92121
[1mStep[0m  [15/53], [94mLoss[0m : 1.96534
[1mStep[0m  [20/53], [94mLoss[0m : 2.44697
[1mStep[0m  [25/53], [94mLoss[0m : 2.01276
[1mStep[0m  [30/53], [94mLoss[0m : 2.17982
[1mStep[0m  [35/53], [94mLoss[0m : 1.88088
[1mStep[0m  [40/53], [94mLoss[0m : 2.19362
[1mStep[0m  [45/53], [94mLoss[0m : 2.19973
[1mStep[0m  [50/53], [94mLoss[0m : 2.02974

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19977
[1mStep[0m  [5/53], [94mLoss[0m : 2.28544
[1mStep[0m  [10/53], [94mLoss[0m : 1.99682
[1mStep[0m  [15/53], [94mLoss[0m : 2.09466
[1mStep[0m  [20/53], [94mLoss[0m : 1.94978
[1mStep[0m  [25/53], [94mLoss[0m : 2.10645
[1mStep[0m  [30/53], [94mLoss[0m : 2.08462
[1mStep[0m  [35/53], [94mLoss[0m : 2.15865
[1mStep[0m  [40/53], [94mLoss[0m : 2.03631
[1mStep[0m  [45/53], [94mLoss[0m : 2.29742
[1mStep[0m  [50/53], [94mLoss[0m : 2.06147

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08699
[1mStep[0m  [5/53], [94mLoss[0m : 1.96516
[1mStep[0m  [10/53], [94mLoss[0m : 1.88114
[1mStep[0m  [15/53], [94mLoss[0m : 1.93502
[1mStep[0m  [20/53], [94mLoss[0m : 1.92018
[1mStep[0m  [25/53], [94mLoss[0m : 1.94413
[1mStep[0m  [30/53], [94mLoss[0m : 2.07295
[1mStep[0m  [35/53], [94mLoss[0m : 1.95833
[1mStep[0m  [40/53], [94mLoss[0m : 2.06128
[1mStep[0m  [45/53], [94mLoss[0m : 2.22575
[1mStep[0m  [50/53], [94mLoss[0m : 2.14220

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83615
[1mStep[0m  [5/53], [94mLoss[0m : 1.79027
[1mStep[0m  [10/53], [94mLoss[0m : 1.82321
[1mStep[0m  [15/53], [94mLoss[0m : 1.88473
[1mStep[0m  [20/53], [94mLoss[0m : 1.82454
[1mStep[0m  [25/53], [94mLoss[0m : 1.75885
[1mStep[0m  [30/53], [94mLoss[0m : 1.95870
[1mStep[0m  [35/53], [94mLoss[0m : 1.80184
[1mStep[0m  [40/53], [94mLoss[0m : 1.95619
[1mStep[0m  [45/53], [94mLoss[0m : 2.08501
[1mStep[0m  [50/53], [94mLoss[0m : 1.85615

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.01104
[1mStep[0m  [5/53], [94mLoss[0m : 1.98125
[1mStep[0m  [10/53], [94mLoss[0m : 1.90443
[1mStep[0m  [15/53], [94mLoss[0m : 1.89520
[1mStep[0m  [20/53], [94mLoss[0m : 1.96730
[1mStep[0m  [25/53], [94mLoss[0m : 1.67423
[1mStep[0m  [30/53], [94mLoss[0m : 1.85934
[1mStep[0m  [35/53], [94mLoss[0m : 1.83570
[1mStep[0m  [40/53], [94mLoss[0m : 1.99299
[1mStep[0m  [45/53], [94mLoss[0m : 1.93100
[1mStep[0m  [50/53], [94mLoss[0m : 1.94154

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72025
[1mStep[0m  [5/53], [94mLoss[0m : 1.68870
[1mStep[0m  [10/53], [94mLoss[0m : 1.94885
[1mStep[0m  [15/53], [94mLoss[0m : 1.82163
[1mStep[0m  [20/53], [94mLoss[0m : 1.85346
[1mStep[0m  [25/53], [94mLoss[0m : 1.98101
[1mStep[0m  [30/53], [94mLoss[0m : 1.73614
[1mStep[0m  [35/53], [94mLoss[0m : 1.89677
[1mStep[0m  [40/53], [94mLoss[0m : 2.09745
[1mStep[0m  [45/53], [94mLoss[0m : 1.67355
[1mStep[0m  [50/53], [94mLoss[0m : 1.91745

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66061
[1mStep[0m  [5/53], [94mLoss[0m : 1.76399
[1mStep[0m  [10/53], [94mLoss[0m : 1.69245
[1mStep[0m  [15/53], [94mLoss[0m : 1.65595
[1mStep[0m  [20/53], [94mLoss[0m : 1.80623
[1mStep[0m  [25/53], [94mLoss[0m : 1.68019
[1mStep[0m  [30/53], [94mLoss[0m : 1.73237
[1mStep[0m  [35/53], [94mLoss[0m : 1.75304
[1mStep[0m  [40/53], [94mLoss[0m : 1.97051
[1mStep[0m  [45/53], [94mLoss[0m : 1.98825
[1mStep[0m  [50/53], [94mLoss[0m : 1.73137

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.820, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72652
[1mStep[0m  [5/53], [94mLoss[0m : 1.84696
[1mStep[0m  [10/53], [94mLoss[0m : 1.89019
[1mStep[0m  [15/53], [94mLoss[0m : 1.58232
[1mStep[0m  [20/53], [94mLoss[0m : 1.80636
[1mStep[0m  [25/53], [94mLoss[0m : 1.62030
[1mStep[0m  [30/53], [94mLoss[0m : 1.78348
[1mStep[0m  [35/53], [94mLoss[0m : 1.76340
[1mStep[0m  [40/53], [94mLoss[0m : 1.77699
[1mStep[0m  [45/53], [94mLoss[0m : 1.76598
[1mStep[0m  [50/53], [94mLoss[0m : 1.95809

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.62963
[1mStep[0m  [5/53], [94mLoss[0m : 1.67756
[1mStep[0m  [10/53], [94mLoss[0m : 1.57331
[1mStep[0m  [15/53], [94mLoss[0m : 1.74676
[1mStep[0m  [20/53], [94mLoss[0m : 1.81178
[1mStep[0m  [25/53], [94mLoss[0m : 1.80944
[1mStep[0m  [30/53], [94mLoss[0m : 1.77103
[1mStep[0m  [35/53], [94mLoss[0m : 1.69361
[1mStep[0m  [40/53], [94mLoss[0m : 1.86446
[1mStep[0m  [45/53], [94mLoss[0m : 1.65184
[1mStep[0m  [50/53], [94mLoss[0m : 1.79353

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54855
[1mStep[0m  [5/53], [94mLoss[0m : 1.54148
[1mStep[0m  [10/53], [94mLoss[0m : 1.54655
[1mStep[0m  [15/53], [94mLoss[0m : 1.59698
[1mStep[0m  [20/53], [94mLoss[0m : 1.65132
[1mStep[0m  [25/53], [94mLoss[0m : 1.55519
[1mStep[0m  [30/53], [94mLoss[0m : 1.53643
[1mStep[0m  [35/53], [94mLoss[0m : 1.62595
[1mStep[0m  [40/53], [94mLoss[0m : 1.72701
[1mStep[0m  [45/53], [94mLoss[0m : 1.97104
[1mStep[0m  [50/53], [94mLoss[0m : 1.69240

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.518, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.55944
[1mStep[0m  [5/53], [94mLoss[0m : 1.46435
[1mStep[0m  [10/53], [94mLoss[0m : 1.54232
[1mStep[0m  [15/53], [94mLoss[0m : 1.53117
[1mStep[0m  [20/53], [94mLoss[0m : 1.36778
[1mStep[0m  [25/53], [94mLoss[0m : 1.45085
[1mStep[0m  [30/53], [94mLoss[0m : 1.70394
[1mStep[0m  [35/53], [94mLoss[0m : 1.44512
[1mStep[0m  [40/53], [94mLoss[0m : 1.75545
[1mStep[0m  [45/53], [94mLoss[0m : 1.70405
[1mStep[0m  [50/53], [94mLoss[0m : 1.71043

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66562
[1mStep[0m  [5/53], [94mLoss[0m : 1.65086
[1mStep[0m  [10/53], [94mLoss[0m : 1.64276
[1mStep[0m  [15/53], [94mLoss[0m : 1.52777
[1mStep[0m  [20/53], [94mLoss[0m : 1.75064
[1mStep[0m  [25/53], [94mLoss[0m : 1.78098
[1mStep[0m  [30/53], [94mLoss[0m : 1.46108
[1mStep[0m  [35/53], [94mLoss[0m : 1.69697
[1mStep[0m  [40/53], [94mLoss[0m : 1.61485
[1mStep[0m  [45/53], [94mLoss[0m : 1.63493
[1mStep[0m  [50/53], [94mLoss[0m : 1.67801

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.58468
[1mStep[0m  [5/53], [94mLoss[0m : 1.53324
[1mStep[0m  [10/53], [94mLoss[0m : 1.31545
[1mStep[0m  [15/53], [94mLoss[0m : 1.59585
[1mStep[0m  [20/53], [94mLoss[0m : 1.46123
[1mStep[0m  [25/53], [94mLoss[0m : 1.73251
[1mStep[0m  [30/53], [94mLoss[0m : 1.53867
[1mStep[0m  [35/53], [94mLoss[0m : 1.73306
[1mStep[0m  [40/53], [94mLoss[0m : 1.63631
[1mStep[0m  [45/53], [94mLoss[0m : 1.64051
[1mStep[0m  [50/53], [94mLoss[0m : 1.52647

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.52050
[1mStep[0m  [5/53], [94mLoss[0m : 1.50319
[1mStep[0m  [10/53], [94mLoss[0m : 1.35079
[1mStep[0m  [15/53], [94mLoss[0m : 1.54269
[1mStep[0m  [20/53], [94mLoss[0m : 1.65329
[1mStep[0m  [25/53], [94mLoss[0m : 1.52978
[1mStep[0m  [30/53], [94mLoss[0m : 1.46093
[1mStep[0m  [35/53], [94mLoss[0m : 1.49026
[1mStep[0m  [40/53], [94mLoss[0m : 1.65074
[1mStep[0m  [45/53], [94mLoss[0m : 1.62292
[1mStep[0m  [50/53], [94mLoss[0m : 1.62470

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.553, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.50328
[1mStep[0m  [5/53], [94mLoss[0m : 1.54698
[1mStep[0m  [10/53], [94mLoss[0m : 1.57945
[1mStep[0m  [15/53], [94mLoss[0m : 1.45083
[1mStep[0m  [20/53], [94mLoss[0m : 1.45775
[1mStep[0m  [25/53], [94mLoss[0m : 1.40396
[1mStep[0m  [30/53], [94mLoss[0m : 1.61834
[1mStep[0m  [35/53], [94mLoss[0m : 1.56454
[1mStep[0m  [40/53], [94mLoss[0m : 1.65660
[1mStep[0m  [45/53], [94mLoss[0m : 1.42647
[1mStep[0m  [50/53], [94mLoss[0m : 1.48097

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.529, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46912
[1mStep[0m  [5/53], [94mLoss[0m : 1.46094
[1mStep[0m  [10/53], [94mLoss[0m : 1.53282
[1mStep[0m  [15/53], [94mLoss[0m : 1.39945
[1mStep[0m  [20/53], [94mLoss[0m : 1.40702
[1mStep[0m  [25/53], [94mLoss[0m : 1.53841
[1mStep[0m  [30/53], [94mLoss[0m : 1.63096
[1mStep[0m  [35/53], [94mLoss[0m : 1.41765
[1mStep[0m  [40/53], [94mLoss[0m : 1.59497
[1mStep[0m  [45/53], [94mLoss[0m : 1.31161
[1mStep[0m  [50/53], [94mLoss[0m : 1.46944

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.41762
[1mStep[0m  [5/53], [94mLoss[0m : 1.55208
[1mStep[0m  [10/53], [94mLoss[0m : 1.42423
[1mStep[0m  [15/53], [94mLoss[0m : 1.43439
[1mStep[0m  [20/53], [94mLoss[0m : 1.44478
[1mStep[0m  [25/53], [94mLoss[0m : 1.54357
[1mStep[0m  [30/53], [94mLoss[0m : 1.49664
[1mStep[0m  [35/53], [94mLoss[0m : 1.47725
[1mStep[0m  [40/53], [94mLoss[0m : 1.32775
[1mStep[0m  [45/53], [94mLoss[0m : 1.65043
[1mStep[0m  [50/53], [94mLoss[0m : 1.44928

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.481, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.24460
[1mStep[0m  [5/53], [94mLoss[0m : 1.35249
[1mStep[0m  [10/53], [94mLoss[0m : 1.31055
[1mStep[0m  [15/53], [94mLoss[0m : 1.40758
[1mStep[0m  [20/53], [94mLoss[0m : 1.44064
[1mStep[0m  [25/53], [94mLoss[0m : 1.46290
[1mStep[0m  [30/53], [94mLoss[0m : 1.39243
[1mStep[0m  [35/53], [94mLoss[0m : 1.39404
[1mStep[0m  [40/53], [94mLoss[0m : 1.53385
[1mStep[0m  [45/53], [94mLoss[0m : 1.51356
[1mStep[0m  [50/53], [94mLoss[0m : 1.43585

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.431, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.34662
[1mStep[0m  [5/53], [94mLoss[0m : 1.35470
[1mStep[0m  [10/53], [94mLoss[0m : 1.33355
[1mStep[0m  [15/53], [94mLoss[0m : 1.42150
[1mStep[0m  [20/53], [94mLoss[0m : 1.35637
[1mStep[0m  [25/53], [94mLoss[0m : 1.47529
[1mStep[0m  [30/53], [94mLoss[0m : 1.46873
[1mStep[0m  [35/53], [94mLoss[0m : 1.33460
[1mStep[0m  [40/53], [94mLoss[0m : 1.58810
[1mStep[0m  [45/53], [94mLoss[0m : 1.44644
[1mStep[0m  [50/53], [94mLoss[0m : 1.26735

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.387, [92mTest[0m: 2.468, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.31410
[1mStep[0m  [5/53], [94mLoss[0m : 1.42356
[1mStep[0m  [10/53], [94mLoss[0m : 1.63260
[1mStep[0m  [15/53], [94mLoss[0m : 1.27949
[1mStep[0m  [20/53], [94mLoss[0m : 1.32075
[1mStep[0m  [25/53], [94mLoss[0m : 1.29180
[1mStep[0m  [30/53], [94mLoss[0m : 1.49113
[1mStep[0m  [35/53], [94mLoss[0m : 1.50067
[1mStep[0m  [40/53], [94mLoss[0m : 1.43830
[1mStep[0m  [45/53], [94mLoss[0m : 1.32789
[1mStep[0m  [50/53], [94mLoss[0m : 1.54695

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.392, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.36584
[1mStep[0m  [5/53], [94mLoss[0m : 1.30428
[1mStep[0m  [10/53], [94mLoss[0m : 1.31879
[1mStep[0m  [15/53], [94mLoss[0m : 1.36826
[1mStep[0m  [20/53], [94mLoss[0m : 1.37038
[1mStep[0m  [25/53], [94mLoss[0m : 1.47044
[1mStep[0m  [30/53], [94mLoss[0m : 1.34435
[1mStep[0m  [35/53], [94mLoss[0m : 1.37572
[1mStep[0m  [40/53], [94mLoss[0m : 1.44990
[1mStep[0m  [45/53], [94mLoss[0m : 1.44370
[1mStep[0m  [50/53], [94mLoss[0m : 1.28656

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.368, [92mTest[0m: 2.466, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.532
====================================

Phase 2 - Evaluation MAE:  2.532439323572012
MAE score P1       2.362284
MAE score P2       2.532439
loss               1.367888
learning_rate          0.01
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay         0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.91903
[1mStep[0m  [5/53], [94mLoss[0m : 9.19133
[1mStep[0m  [10/53], [94mLoss[0m : 8.35055
[1mStep[0m  [15/53], [94mLoss[0m : 6.89198
[1mStep[0m  [20/53], [94mLoss[0m : 5.52795
[1mStep[0m  [25/53], [94mLoss[0m : 4.75315
[1mStep[0m  [30/53], [94mLoss[0m : 3.52996
[1mStep[0m  [35/53], [94mLoss[0m : 3.24730
[1mStep[0m  [40/53], [94mLoss[0m : 3.19495
[1mStep[0m  [45/53], [94mLoss[0m : 2.66659
[1mStep[0m  [50/53], [94mLoss[0m : 2.78563

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.294, [92mTest[0m: 10.656, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50715
[1mStep[0m  [5/53], [94mLoss[0m : 2.62985
[1mStep[0m  [10/53], [94mLoss[0m : 2.40501
[1mStep[0m  [15/53], [94mLoss[0m : 2.54318
[1mStep[0m  [20/53], [94mLoss[0m : 2.50954
[1mStep[0m  [25/53], [94mLoss[0m : 2.59807
[1mStep[0m  [30/53], [94mLoss[0m : 2.65229
[1mStep[0m  [35/53], [94mLoss[0m : 2.37455
[1mStep[0m  [40/53], [94mLoss[0m : 2.57486
[1mStep[0m  [45/53], [94mLoss[0m : 2.49137
[1mStep[0m  [50/53], [94mLoss[0m : 2.37274

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72647
[1mStep[0m  [5/53], [94mLoss[0m : 2.48606
[1mStep[0m  [10/53], [94mLoss[0m : 2.45256
[1mStep[0m  [15/53], [94mLoss[0m : 2.37159
[1mStep[0m  [20/53], [94mLoss[0m : 2.53494
[1mStep[0m  [25/53], [94mLoss[0m : 2.33050
[1mStep[0m  [30/53], [94mLoss[0m : 2.62273
[1mStep[0m  [35/53], [94mLoss[0m : 2.54850
[1mStep[0m  [40/53], [94mLoss[0m : 2.66275
[1mStep[0m  [45/53], [94mLoss[0m : 2.57283
[1mStep[0m  [50/53], [94mLoss[0m : 2.61386

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46755
[1mStep[0m  [5/53], [94mLoss[0m : 2.38737
[1mStep[0m  [10/53], [94mLoss[0m : 2.62189
[1mStep[0m  [15/53], [94mLoss[0m : 2.69287
[1mStep[0m  [20/53], [94mLoss[0m : 2.59390
[1mStep[0m  [25/53], [94mLoss[0m : 2.58861
[1mStep[0m  [30/53], [94mLoss[0m : 2.46193
[1mStep[0m  [35/53], [94mLoss[0m : 2.44450
[1mStep[0m  [40/53], [94mLoss[0m : 2.49750
[1mStep[0m  [45/53], [94mLoss[0m : 2.59800
[1mStep[0m  [50/53], [94mLoss[0m : 2.58371

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62305
[1mStep[0m  [5/53], [94mLoss[0m : 2.50714
[1mStep[0m  [10/53], [94mLoss[0m : 2.63484
[1mStep[0m  [15/53], [94mLoss[0m : 2.50967
[1mStep[0m  [20/53], [94mLoss[0m : 2.56603
[1mStep[0m  [25/53], [94mLoss[0m : 2.36586
[1mStep[0m  [30/53], [94mLoss[0m : 2.10174
[1mStep[0m  [35/53], [94mLoss[0m : 2.28280
[1mStep[0m  [40/53], [94mLoss[0m : 2.65897
[1mStep[0m  [45/53], [94mLoss[0m : 2.76930
[1mStep[0m  [50/53], [94mLoss[0m : 2.33035

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39656
[1mStep[0m  [5/53], [94mLoss[0m : 2.42886
[1mStep[0m  [10/53], [94mLoss[0m : 2.60039
[1mStep[0m  [15/53], [94mLoss[0m : 2.32035
[1mStep[0m  [20/53], [94mLoss[0m : 2.56312
[1mStep[0m  [25/53], [94mLoss[0m : 2.50484
[1mStep[0m  [30/53], [94mLoss[0m : 2.20866
[1mStep[0m  [35/53], [94mLoss[0m : 2.35989
[1mStep[0m  [40/53], [94mLoss[0m : 2.64081
[1mStep[0m  [45/53], [94mLoss[0m : 2.66373
[1mStep[0m  [50/53], [94mLoss[0m : 2.61851

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61020
[1mStep[0m  [5/53], [94mLoss[0m : 2.42199
[1mStep[0m  [10/53], [94mLoss[0m : 2.33202
[1mStep[0m  [15/53], [94mLoss[0m : 2.47117
[1mStep[0m  [20/53], [94mLoss[0m : 2.56296
[1mStep[0m  [25/53], [94mLoss[0m : 2.58580
[1mStep[0m  [30/53], [94mLoss[0m : 2.50904
[1mStep[0m  [35/53], [94mLoss[0m : 2.61378
[1mStep[0m  [40/53], [94mLoss[0m : 2.42317
[1mStep[0m  [45/53], [94mLoss[0m : 2.44557
[1mStep[0m  [50/53], [94mLoss[0m : 2.49098

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54582
[1mStep[0m  [5/53], [94mLoss[0m : 2.63677
[1mStep[0m  [10/53], [94mLoss[0m : 2.58462
[1mStep[0m  [15/53], [94mLoss[0m : 2.62832
[1mStep[0m  [20/53], [94mLoss[0m : 2.49015
[1mStep[0m  [25/53], [94mLoss[0m : 2.55946
[1mStep[0m  [30/53], [94mLoss[0m : 2.55540
[1mStep[0m  [35/53], [94mLoss[0m : 2.60681
[1mStep[0m  [40/53], [94mLoss[0m : 2.60358
[1mStep[0m  [45/53], [94mLoss[0m : 2.58075
[1mStep[0m  [50/53], [94mLoss[0m : 2.55367

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41682
[1mStep[0m  [5/53], [94mLoss[0m : 2.48783
[1mStep[0m  [10/53], [94mLoss[0m : 2.31928
[1mStep[0m  [15/53], [94mLoss[0m : 2.59359
[1mStep[0m  [20/53], [94mLoss[0m : 2.54051
[1mStep[0m  [25/53], [94mLoss[0m : 2.56918
[1mStep[0m  [30/53], [94mLoss[0m : 2.52639
[1mStep[0m  [35/53], [94mLoss[0m : 2.61715
[1mStep[0m  [40/53], [94mLoss[0m : 2.46595
[1mStep[0m  [45/53], [94mLoss[0m : 2.65048
[1mStep[0m  [50/53], [94mLoss[0m : 2.50693

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40374
[1mStep[0m  [5/53], [94mLoss[0m : 2.50645
[1mStep[0m  [10/53], [94mLoss[0m : 2.52184
[1mStep[0m  [15/53], [94mLoss[0m : 2.40319
[1mStep[0m  [20/53], [94mLoss[0m : 2.32413
[1mStep[0m  [25/53], [94mLoss[0m : 2.50364
[1mStep[0m  [30/53], [94mLoss[0m : 2.57734
[1mStep[0m  [35/53], [94mLoss[0m : 2.43617
[1mStep[0m  [40/53], [94mLoss[0m : 2.33815
[1mStep[0m  [45/53], [94mLoss[0m : 2.85252
[1mStep[0m  [50/53], [94mLoss[0m : 2.55780

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26375
[1mStep[0m  [5/53], [94mLoss[0m : 2.61721
[1mStep[0m  [10/53], [94mLoss[0m : 2.51544
[1mStep[0m  [15/53], [94mLoss[0m : 2.25336
[1mStep[0m  [20/53], [94mLoss[0m : 2.38172
[1mStep[0m  [25/53], [94mLoss[0m : 2.68247
[1mStep[0m  [30/53], [94mLoss[0m : 2.42508
[1mStep[0m  [35/53], [94mLoss[0m : 2.41564
[1mStep[0m  [40/53], [94mLoss[0m : 2.25116
[1mStep[0m  [45/53], [94mLoss[0m : 2.91116
[1mStep[0m  [50/53], [94mLoss[0m : 2.53000

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50849
[1mStep[0m  [5/53], [94mLoss[0m : 2.63665
[1mStep[0m  [10/53], [94mLoss[0m : 2.51085
[1mStep[0m  [15/53], [94mLoss[0m : 2.47428
[1mStep[0m  [20/53], [94mLoss[0m : 2.47959
[1mStep[0m  [25/53], [94mLoss[0m : 2.35716
[1mStep[0m  [30/53], [94mLoss[0m : 2.41113
[1mStep[0m  [35/53], [94mLoss[0m : 2.46931
[1mStep[0m  [40/53], [94mLoss[0m : 2.49492
[1mStep[0m  [45/53], [94mLoss[0m : 2.47173
[1mStep[0m  [50/53], [94mLoss[0m : 2.23374

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49937
[1mStep[0m  [5/53], [94mLoss[0m : 2.64605
[1mStep[0m  [10/53], [94mLoss[0m : 2.38783
[1mStep[0m  [15/53], [94mLoss[0m : 2.52963
[1mStep[0m  [20/53], [94mLoss[0m : 2.50875
[1mStep[0m  [25/53], [94mLoss[0m : 2.54344
[1mStep[0m  [30/53], [94mLoss[0m : 2.49714
[1mStep[0m  [35/53], [94mLoss[0m : 2.50642
[1mStep[0m  [40/53], [94mLoss[0m : 2.57036
[1mStep[0m  [45/53], [94mLoss[0m : 2.38055
[1mStep[0m  [50/53], [94mLoss[0m : 2.34177

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51814
[1mStep[0m  [5/53], [94mLoss[0m : 2.39265
[1mStep[0m  [10/53], [94mLoss[0m : 2.74029
[1mStep[0m  [15/53], [94mLoss[0m : 2.60250
[1mStep[0m  [20/53], [94mLoss[0m : 2.58906
[1mStep[0m  [25/53], [94mLoss[0m : 2.45837
[1mStep[0m  [30/53], [94mLoss[0m : 2.53786
[1mStep[0m  [35/53], [94mLoss[0m : 2.41277
[1mStep[0m  [40/53], [94mLoss[0m : 2.48786
[1mStep[0m  [45/53], [94mLoss[0m : 2.76228
[1mStep[0m  [50/53], [94mLoss[0m : 2.48834

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45685
[1mStep[0m  [5/53], [94mLoss[0m : 2.62112
[1mStep[0m  [10/53], [94mLoss[0m : 2.62283
[1mStep[0m  [15/53], [94mLoss[0m : 2.45568
[1mStep[0m  [20/53], [94mLoss[0m : 2.40707
[1mStep[0m  [25/53], [94mLoss[0m : 2.39636
[1mStep[0m  [30/53], [94mLoss[0m : 2.59365
[1mStep[0m  [35/53], [94mLoss[0m : 2.51132
[1mStep[0m  [40/53], [94mLoss[0m : 2.75496
[1mStep[0m  [45/53], [94mLoss[0m : 2.62786
[1mStep[0m  [50/53], [94mLoss[0m : 2.50860

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62256
[1mStep[0m  [5/53], [94mLoss[0m : 2.36614
[1mStep[0m  [10/53], [94mLoss[0m : 2.51968
[1mStep[0m  [15/53], [94mLoss[0m : 2.56516
[1mStep[0m  [20/53], [94mLoss[0m : 2.54380
[1mStep[0m  [25/53], [94mLoss[0m : 2.36110
[1mStep[0m  [30/53], [94mLoss[0m : 2.55492
[1mStep[0m  [35/53], [94mLoss[0m : 2.33313
[1mStep[0m  [40/53], [94mLoss[0m : 2.52592
[1mStep[0m  [45/53], [94mLoss[0m : 2.70314
[1mStep[0m  [50/53], [94mLoss[0m : 2.56678

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56605
[1mStep[0m  [5/53], [94mLoss[0m : 2.36523
[1mStep[0m  [10/53], [94mLoss[0m : 2.43469
[1mStep[0m  [15/53], [94mLoss[0m : 2.39849
[1mStep[0m  [20/53], [94mLoss[0m : 2.46238
[1mStep[0m  [25/53], [94mLoss[0m : 2.45324
[1mStep[0m  [30/53], [94mLoss[0m : 2.32831
[1mStep[0m  [35/53], [94mLoss[0m : 2.32733
[1mStep[0m  [40/53], [94mLoss[0m : 2.49951
[1mStep[0m  [45/53], [94mLoss[0m : 2.73655
[1mStep[0m  [50/53], [94mLoss[0m : 2.60248

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60160
[1mStep[0m  [5/53], [94mLoss[0m : 2.61109
[1mStep[0m  [10/53], [94mLoss[0m : 2.68013
[1mStep[0m  [15/53], [94mLoss[0m : 2.62796
[1mStep[0m  [20/53], [94mLoss[0m : 2.55822
[1mStep[0m  [25/53], [94mLoss[0m : 2.36514
[1mStep[0m  [30/53], [94mLoss[0m : 2.47045
[1mStep[0m  [35/53], [94mLoss[0m : 2.63190
[1mStep[0m  [40/53], [94mLoss[0m : 2.50487
[1mStep[0m  [45/53], [94mLoss[0m : 2.70275
[1mStep[0m  [50/53], [94mLoss[0m : 2.64843

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41369
[1mStep[0m  [5/53], [94mLoss[0m : 2.29715
[1mStep[0m  [10/53], [94mLoss[0m : 2.31960
[1mStep[0m  [15/53], [94mLoss[0m : 2.41003
[1mStep[0m  [20/53], [94mLoss[0m : 2.45252
[1mStep[0m  [25/53], [94mLoss[0m : 2.45111
[1mStep[0m  [30/53], [94mLoss[0m : 2.44983
[1mStep[0m  [35/53], [94mLoss[0m : 2.46569
[1mStep[0m  [40/53], [94mLoss[0m : 2.63475
[1mStep[0m  [45/53], [94mLoss[0m : 2.37154
[1mStep[0m  [50/53], [94mLoss[0m : 2.48579

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51771
[1mStep[0m  [5/53], [94mLoss[0m : 2.61069
[1mStep[0m  [10/53], [94mLoss[0m : 2.49272
[1mStep[0m  [15/53], [94mLoss[0m : 2.54665
[1mStep[0m  [20/53], [94mLoss[0m : 2.43744
[1mStep[0m  [25/53], [94mLoss[0m : 2.35977
[1mStep[0m  [30/53], [94mLoss[0m : 2.48918
[1mStep[0m  [35/53], [94mLoss[0m : 2.49710
[1mStep[0m  [40/53], [94mLoss[0m : 2.66882
[1mStep[0m  [45/53], [94mLoss[0m : 2.60365
[1mStep[0m  [50/53], [94mLoss[0m : 2.29393

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52653
[1mStep[0m  [5/53], [94mLoss[0m : 2.41810
[1mStep[0m  [10/53], [94mLoss[0m : 2.65699
[1mStep[0m  [15/53], [94mLoss[0m : 2.57785
[1mStep[0m  [20/53], [94mLoss[0m : 2.60295
[1mStep[0m  [25/53], [94mLoss[0m : 2.40677
[1mStep[0m  [30/53], [94mLoss[0m : 2.31939
[1mStep[0m  [35/53], [94mLoss[0m : 2.42722
[1mStep[0m  [40/53], [94mLoss[0m : 2.35455
[1mStep[0m  [45/53], [94mLoss[0m : 2.47032
[1mStep[0m  [50/53], [94mLoss[0m : 2.45115

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.409, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50907
[1mStep[0m  [5/53], [94mLoss[0m : 2.15091
[1mStep[0m  [10/53], [94mLoss[0m : 2.35522
[1mStep[0m  [15/53], [94mLoss[0m : 2.59810
[1mStep[0m  [20/53], [94mLoss[0m : 2.64194
[1mStep[0m  [25/53], [94mLoss[0m : 2.54822
[1mStep[0m  [30/53], [94mLoss[0m : 2.39150
[1mStep[0m  [35/53], [94mLoss[0m : 2.42221
[1mStep[0m  [40/53], [94mLoss[0m : 2.43139
[1mStep[0m  [45/53], [94mLoss[0m : 2.69523
[1mStep[0m  [50/53], [94mLoss[0m : 2.53739

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.79198
[1mStep[0m  [5/53], [94mLoss[0m : 2.31696
[1mStep[0m  [10/53], [94mLoss[0m : 2.55768
[1mStep[0m  [15/53], [94mLoss[0m : 2.48177
[1mStep[0m  [20/53], [94mLoss[0m : 2.54413
[1mStep[0m  [25/53], [94mLoss[0m : 2.73715
[1mStep[0m  [30/53], [94mLoss[0m : 2.67482
[1mStep[0m  [35/53], [94mLoss[0m : 2.59497
[1mStep[0m  [40/53], [94mLoss[0m : 2.33318
[1mStep[0m  [45/53], [94mLoss[0m : 2.72210
[1mStep[0m  [50/53], [94mLoss[0m : 2.36243

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42514
[1mStep[0m  [5/53], [94mLoss[0m : 2.46094
[1mStep[0m  [10/53], [94mLoss[0m : 2.39692
[1mStep[0m  [15/53], [94mLoss[0m : 2.28542
[1mStep[0m  [20/53], [94mLoss[0m : 2.51570
[1mStep[0m  [25/53], [94mLoss[0m : 2.53222
[1mStep[0m  [30/53], [94mLoss[0m : 2.41243
[1mStep[0m  [35/53], [94mLoss[0m : 2.60342
[1mStep[0m  [40/53], [94mLoss[0m : 2.61995
[1mStep[0m  [45/53], [94mLoss[0m : 2.43744
[1mStep[0m  [50/53], [94mLoss[0m : 2.56774

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53966
[1mStep[0m  [5/53], [94mLoss[0m : 2.22780
[1mStep[0m  [10/53], [94mLoss[0m : 2.35881
[1mStep[0m  [15/53], [94mLoss[0m : 2.44932
[1mStep[0m  [20/53], [94mLoss[0m : 2.57120
[1mStep[0m  [25/53], [94mLoss[0m : 2.33996
[1mStep[0m  [30/53], [94mLoss[0m : 2.62410
[1mStep[0m  [35/53], [94mLoss[0m : 2.57537
[1mStep[0m  [40/53], [94mLoss[0m : 2.53229
[1mStep[0m  [45/53], [94mLoss[0m : 2.55615
[1mStep[0m  [50/53], [94mLoss[0m : 2.67721

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.400, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62023
[1mStep[0m  [5/53], [94mLoss[0m : 2.46904
[1mStep[0m  [10/53], [94mLoss[0m : 2.45432
[1mStep[0m  [15/53], [94mLoss[0m : 2.40250
[1mStep[0m  [20/53], [94mLoss[0m : 2.22385
[1mStep[0m  [25/53], [94mLoss[0m : 2.50044
[1mStep[0m  [30/53], [94mLoss[0m : 2.53614
[1mStep[0m  [35/53], [94mLoss[0m : 2.35881
[1mStep[0m  [40/53], [94mLoss[0m : 2.59807
[1mStep[0m  [45/53], [94mLoss[0m : 2.39785
[1mStep[0m  [50/53], [94mLoss[0m : 2.52428

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38648
[1mStep[0m  [5/53], [94mLoss[0m : 2.46285
[1mStep[0m  [10/53], [94mLoss[0m : 2.45214
[1mStep[0m  [15/53], [94mLoss[0m : 2.63623
[1mStep[0m  [20/53], [94mLoss[0m : 2.55198
[1mStep[0m  [25/53], [94mLoss[0m : 2.28864
[1mStep[0m  [30/53], [94mLoss[0m : 2.37665
[1mStep[0m  [35/53], [94mLoss[0m : 2.48034
[1mStep[0m  [40/53], [94mLoss[0m : 2.15854
[1mStep[0m  [45/53], [94mLoss[0m : 2.57858
[1mStep[0m  [50/53], [94mLoss[0m : 2.54200

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61171
[1mStep[0m  [5/53], [94mLoss[0m : 2.43238
[1mStep[0m  [10/53], [94mLoss[0m : 2.28364
[1mStep[0m  [15/53], [94mLoss[0m : 2.54549
[1mStep[0m  [20/53], [94mLoss[0m : 2.50531
[1mStep[0m  [25/53], [94mLoss[0m : 2.62835
[1mStep[0m  [30/53], [94mLoss[0m : 2.50930
[1mStep[0m  [35/53], [94mLoss[0m : 2.48095
[1mStep[0m  [40/53], [94mLoss[0m : 2.50946
[1mStep[0m  [45/53], [94mLoss[0m : 2.52558
[1mStep[0m  [50/53], [94mLoss[0m : 2.49774

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49044
[1mStep[0m  [5/53], [94mLoss[0m : 2.50089
[1mStep[0m  [10/53], [94mLoss[0m : 2.30575
[1mStep[0m  [15/53], [94mLoss[0m : 2.31449
[1mStep[0m  [20/53], [94mLoss[0m : 2.38809
[1mStep[0m  [25/53], [94mLoss[0m : 2.44379
[1mStep[0m  [30/53], [94mLoss[0m : 2.43460
[1mStep[0m  [35/53], [94mLoss[0m : 2.15305
[1mStep[0m  [40/53], [94mLoss[0m : 2.35784
[1mStep[0m  [45/53], [94mLoss[0m : 2.58101
[1mStep[0m  [50/53], [94mLoss[0m : 2.57706

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.398, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56810
[1mStep[0m  [5/53], [94mLoss[0m : 2.44132
[1mStep[0m  [10/53], [94mLoss[0m : 2.63106
[1mStep[0m  [15/53], [94mLoss[0m : 2.35325
[1mStep[0m  [20/53], [94mLoss[0m : 2.30615
[1mStep[0m  [25/53], [94mLoss[0m : 2.41141
[1mStep[0m  [30/53], [94mLoss[0m : 2.42883
[1mStep[0m  [35/53], [94mLoss[0m : 2.63614
[1mStep[0m  [40/53], [94mLoss[0m : 2.73486
[1mStep[0m  [45/53], [94mLoss[0m : 2.30769
[1mStep[0m  [50/53], [94mLoss[0m : 2.45040

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.404, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.406
====================================

Phase 1 - Evaluation MAE:  2.40612426170936
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.43703
[1mStep[0m  [5/53], [94mLoss[0m : 2.45395
[1mStep[0m  [10/53], [94mLoss[0m : 2.40914
[1mStep[0m  [15/53], [94mLoss[0m : 2.59196
[1mStep[0m  [20/53], [94mLoss[0m : 2.46103
[1mStep[0m  [25/53], [94mLoss[0m : 2.51306
[1mStep[0m  [30/53], [94mLoss[0m : 2.41083
[1mStep[0m  [35/53], [94mLoss[0m : 2.62000
[1mStep[0m  [40/53], [94mLoss[0m : 2.39831
[1mStep[0m  [45/53], [94mLoss[0m : 2.41387
[1mStep[0m  [50/53], [94mLoss[0m : 2.67053

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36978
[1mStep[0m  [5/53], [94mLoss[0m : 2.52720
[1mStep[0m  [10/53], [94mLoss[0m : 2.51127
[1mStep[0m  [15/53], [94mLoss[0m : 2.18973
[1mStep[0m  [20/53], [94mLoss[0m : 2.63497
[1mStep[0m  [25/53], [94mLoss[0m : 2.58427
[1mStep[0m  [30/53], [94mLoss[0m : 2.42720
[1mStep[0m  [35/53], [94mLoss[0m : 2.56052
[1mStep[0m  [40/53], [94mLoss[0m : 2.39076
[1mStep[0m  [45/53], [94mLoss[0m : 2.48104
[1mStep[0m  [50/53], [94mLoss[0m : 2.64414

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.539, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28099
[1mStep[0m  [5/53], [94mLoss[0m : 2.50742
[1mStep[0m  [10/53], [94mLoss[0m : 2.37831
[1mStep[0m  [15/53], [94mLoss[0m : 2.39775
[1mStep[0m  [20/53], [94mLoss[0m : 2.61764
[1mStep[0m  [25/53], [94mLoss[0m : 2.24987
[1mStep[0m  [30/53], [94mLoss[0m : 2.57780
[1mStep[0m  [35/53], [94mLoss[0m : 2.24088
[1mStep[0m  [40/53], [94mLoss[0m : 2.48573
[1mStep[0m  [45/53], [94mLoss[0m : 2.37422
[1mStep[0m  [50/53], [94mLoss[0m : 2.37800

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.543, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35919
[1mStep[0m  [5/53], [94mLoss[0m : 2.46310
[1mStep[0m  [10/53], [94mLoss[0m : 2.31352
[1mStep[0m  [15/53], [94mLoss[0m : 2.38156
[1mStep[0m  [20/53], [94mLoss[0m : 2.27955
[1mStep[0m  [25/53], [94mLoss[0m : 2.23092
[1mStep[0m  [30/53], [94mLoss[0m : 2.25912
[1mStep[0m  [35/53], [94mLoss[0m : 2.16292
[1mStep[0m  [40/53], [94mLoss[0m : 2.38706
[1mStep[0m  [45/53], [94mLoss[0m : 2.29430
[1mStep[0m  [50/53], [94mLoss[0m : 2.53099

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.567, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22711
[1mStep[0m  [5/53], [94mLoss[0m : 2.28803
[1mStep[0m  [10/53], [94mLoss[0m : 2.14252
[1mStep[0m  [15/53], [94mLoss[0m : 2.24093
[1mStep[0m  [20/53], [94mLoss[0m : 2.12615
[1mStep[0m  [25/53], [94mLoss[0m : 2.46587
[1mStep[0m  [30/53], [94mLoss[0m : 2.48660
[1mStep[0m  [35/53], [94mLoss[0m : 2.12931
[1mStep[0m  [40/53], [94mLoss[0m : 2.46231
[1mStep[0m  [45/53], [94mLoss[0m : 2.33262
[1mStep[0m  [50/53], [94mLoss[0m : 2.40643

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.563, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24014
[1mStep[0m  [5/53], [94mLoss[0m : 2.33770
[1mStep[0m  [10/53], [94mLoss[0m : 2.38338
[1mStep[0m  [15/53], [94mLoss[0m : 2.31301
[1mStep[0m  [20/53], [94mLoss[0m : 2.21987
[1mStep[0m  [25/53], [94mLoss[0m : 2.29613
[1mStep[0m  [30/53], [94mLoss[0m : 2.14782
[1mStep[0m  [35/53], [94mLoss[0m : 2.41647
[1mStep[0m  [40/53], [94mLoss[0m : 2.24595
[1mStep[0m  [45/53], [94mLoss[0m : 2.07539
[1mStep[0m  [50/53], [94mLoss[0m : 2.22025

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.574, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42290
[1mStep[0m  [5/53], [94mLoss[0m : 2.11606
[1mStep[0m  [10/53], [94mLoss[0m : 2.35453
[1mStep[0m  [15/53], [94mLoss[0m : 2.16251
[1mStep[0m  [20/53], [94mLoss[0m : 2.19547
[1mStep[0m  [25/53], [94mLoss[0m : 2.31625
[1mStep[0m  [30/53], [94mLoss[0m : 2.19497
[1mStep[0m  [35/53], [94mLoss[0m : 2.24425
[1mStep[0m  [40/53], [94mLoss[0m : 2.19580
[1mStep[0m  [45/53], [94mLoss[0m : 2.19716
[1mStep[0m  [50/53], [94mLoss[0m : 2.21603

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.197, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30321
[1mStep[0m  [5/53], [94mLoss[0m : 2.01054
[1mStep[0m  [10/53], [94mLoss[0m : 2.02617
[1mStep[0m  [15/53], [94mLoss[0m : 2.07509
[1mStep[0m  [20/53], [94mLoss[0m : 2.08462
[1mStep[0m  [25/53], [94mLoss[0m : 2.07292
[1mStep[0m  [30/53], [94mLoss[0m : 2.22422
[1mStep[0m  [35/53], [94mLoss[0m : 2.08165
[1mStep[0m  [40/53], [94mLoss[0m : 2.20044
[1mStep[0m  [45/53], [94mLoss[0m : 2.06208
[1mStep[0m  [50/53], [94mLoss[0m : 2.26031

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.96109
[1mStep[0m  [5/53], [94mLoss[0m : 2.01306
[1mStep[0m  [10/53], [94mLoss[0m : 2.12032
[1mStep[0m  [15/53], [94mLoss[0m : 2.00900
[1mStep[0m  [20/53], [94mLoss[0m : 2.16171
[1mStep[0m  [25/53], [94mLoss[0m : 1.99261
[1mStep[0m  [30/53], [94mLoss[0m : 2.25118
[1mStep[0m  [35/53], [94mLoss[0m : 2.29244
[1mStep[0m  [40/53], [94mLoss[0m : 2.04848
[1mStep[0m  [45/53], [94mLoss[0m : 2.22614
[1mStep[0m  [50/53], [94mLoss[0m : 2.10086

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86727
[1mStep[0m  [5/53], [94mLoss[0m : 2.05611
[1mStep[0m  [10/53], [94mLoss[0m : 1.92006
[1mStep[0m  [15/53], [94mLoss[0m : 2.17470
[1mStep[0m  [20/53], [94mLoss[0m : 2.03174
[1mStep[0m  [25/53], [94mLoss[0m : 2.20308
[1mStep[0m  [30/53], [94mLoss[0m : 2.00684
[1mStep[0m  [35/53], [94mLoss[0m : 2.09278
[1mStep[0m  [40/53], [94mLoss[0m : 1.88953
[1mStep[0m  [45/53], [94mLoss[0m : 2.02244
[1mStep[0m  [50/53], [94mLoss[0m : 2.14781

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79609
[1mStep[0m  [5/53], [94mLoss[0m : 1.90040
[1mStep[0m  [10/53], [94mLoss[0m : 1.86758
[1mStep[0m  [15/53], [94mLoss[0m : 2.00212
[1mStep[0m  [20/53], [94mLoss[0m : 2.00285
[1mStep[0m  [25/53], [94mLoss[0m : 1.94819
[1mStep[0m  [30/53], [94mLoss[0m : 1.88298
[1mStep[0m  [35/53], [94mLoss[0m : 1.93302
[1mStep[0m  [40/53], [94mLoss[0m : 1.95887
[1mStep[0m  [45/53], [94mLoss[0m : 1.97317
[1mStep[0m  [50/53], [94mLoss[0m : 2.33515

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83712
[1mStep[0m  [5/53], [94mLoss[0m : 1.90103
[1mStep[0m  [10/53], [94mLoss[0m : 2.20492
[1mStep[0m  [15/53], [94mLoss[0m : 1.88502
[1mStep[0m  [20/53], [94mLoss[0m : 1.94584
[1mStep[0m  [25/53], [94mLoss[0m : 2.04617
[1mStep[0m  [30/53], [94mLoss[0m : 1.90061
[1mStep[0m  [35/53], [94mLoss[0m : 1.95153
[1mStep[0m  [40/53], [94mLoss[0m : 1.79366
[1mStep[0m  [45/53], [94mLoss[0m : 2.00935
[1mStep[0m  [50/53], [94mLoss[0m : 2.07286

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.954, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.60402
[1mStep[0m  [5/53], [94mLoss[0m : 1.86101
[1mStep[0m  [10/53], [94mLoss[0m : 2.03138
[1mStep[0m  [15/53], [94mLoss[0m : 1.89437
[1mStep[0m  [20/53], [94mLoss[0m : 1.86333
[1mStep[0m  [25/53], [94mLoss[0m : 1.94533
[1mStep[0m  [30/53], [94mLoss[0m : 1.95000
[1mStep[0m  [35/53], [94mLoss[0m : 1.86796
[1mStep[0m  [40/53], [94mLoss[0m : 2.01995
[1mStep[0m  [45/53], [94mLoss[0m : 1.88361
[1mStep[0m  [50/53], [94mLoss[0m : 1.97482

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.95967
[1mStep[0m  [5/53], [94mLoss[0m : 2.01403
[1mStep[0m  [10/53], [94mLoss[0m : 1.79340
[1mStep[0m  [15/53], [94mLoss[0m : 1.62740
[1mStep[0m  [20/53], [94mLoss[0m : 2.16523
[1mStep[0m  [25/53], [94mLoss[0m : 1.83403
[1mStep[0m  [30/53], [94mLoss[0m : 1.99681
[1mStep[0m  [35/53], [94mLoss[0m : 1.92576
[1mStep[0m  [40/53], [94mLoss[0m : 2.18779
[1mStep[0m  [45/53], [94mLoss[0m : 1.88919
[1mStep[0m  [50/53], [94mLoss[0m : 1.79322

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83545
[1mStep[0m  [5/53], [94mLoss[0m : 1.77237
[1mStep[0m  [10/53], [94mLoss[0m : 1.75994
[1mStep[0m  [15/53], [94mLoss[0m : 1.92748
[1mStep[0m  [20/53], [94mLoss[0m : 1.72055
[1mStep[0m  [25/53], [94mLoss[0m : 1.81367
[1mStep[0m  [30/53], [94mLoss[0m : 1.84405
[1mStep[0m  [35/53], [94mLoss[0m : 1.96374
[1mStep[0m  [40/53], [94mLoss[0m : 2.04412
[1mStep[0m  [45/53], [94mLoss[0m : 2.03288
[1mStep[0m  [50/53], [94mLoss[0m : 1.73125

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.70972
[1mStep[0m  [5/53], [94mLoss[0m : 1.69043
[1mStep[0m  [10/53], [94mLoss[0m : 1.82017
[1mStep[0m  [15/53], [94mLoss[0m : 1.96009
[1mStep[0m  [20/53], [94mLoss[0m : 1.85885
[1mStep[0m  [25/53], [94mLoss[0m : 1.98491
[1mStep[0m  [30/53], [94mLoss[0m : 1.75957
[1mStep[0m  [35/53], [94mLoss[0m : 1.77020
[1mStep[0m  [40/53], [94mLoss[0m : 1.62344
[1mStep[0m  [45/53], [94mLoss[0m : 1.72278
[1mStep[0m  [50/53], [94mLoss[0m : 1.77509

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.57911
[1mStep[0m  [5/53], [94mLoss[0m : 1.80661
[1mStep[0m  [10/53], [94mLoss[0m : 1.66378
[1mStep[0m  [15/53], [94mLoss[0m : 1.70431
[1mStep[0m  [20/53], [94mLoss[0m : 1.79243
[1mStep[0m  [25/53], [94mLoss[0m : 1.95268
[1mStep[0m  [30/53], [94mLoss[0m : 1.70444
[1mStep[0m  [35/53], [94mLoss[0m : 1.72576
[1mStep[0m  [40/53], [94mLoss[0m : 1.79986
[1mStep[0m  [45/53], [94mLoss[0m : 1.66786
[1mStep[0m  [50/53], [94mLoss[0m : 1.80729

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.546, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71344
[1mStep[0m  [5/53], [94mLoss[0m : 1.58131
[1mStep[0m  [10/53], [94mLoss[0m : 1.78348
[1mStep[0m  [15/53], [94mLoss[0m : 1.71971
[1mStep[0m  [20/53], [94mLoss[0m : 1.88566
[1mStep[0m  [25/53], [94mLoss[0m : 1.74065
[1mStep[0m  [30/53], [94mLoss[0m : 1.67393
[1mStep[0m  [35/53], [94mLoss[0m : 1.66974
[1mStep[0m  [40/53], [94mLoss[0m : 1.84780
[1mStep[0m  [45/53], [94mLoss[0m : 1.70786
[1mStep[0m  [50/53], [94mLoss[0m : 1.74734

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.746, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64605
[1mStep[0m  [5/53], [94mLoss[0m : 1.77716
[1mStep[0m  [10/53], [94mLoss[0m : 1.90906
[1mStep[0m  [15/53], [94mLoss[0m : 1.61688
[1mStep[0m  [20/53], [94mLoss[0m : 1.60458
[1mStep[0m  [25/53], [94mLoss[0m : 1.61193
[1mStep[0m  [30/53], [94mLoss[0m : 1.59837
[1mStep[0m  [35/53], [94mLoss[0m : 1.59728
[1mStep[0m  [40/53], [94mLoss[0m : 1.69258
[1mStep[0m  [45/53], [94mLoss[0m : 1.71111
[1mStep[0m  [50/53], [94mLoss[0m : 1.81375

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68188
[1mStep[0m  [5/53], [94mLoss[0m : 1.83576
[1mStep[0m  [10/53], [94mLoss[0m : 1.77752
[1mStep[0m  [15/53], [94mLoss[0m : 1.50149
[1mStep[0m  [20/53], [94mLoss[0m : 1.77285
[1mStep[0m  [25/53], [94mLoss[0m : 1.67822
[1mStep[0m  [30/53], [94mLoss[0m : 1.68727
[1mStep[0m  [35/53], [94mLoss[0m : 1.68764
[1mStep[0m  [40/53], [94mLoss[0m : 1.70359
[1mStep[0m  [45/53], [94mLoss[0m : 1.68208
[1mStep[0m  [50/53], [94mLoss[0m : 1.54327

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.672, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.57520
[1mStep[0m  [5/53], [94mLoss[0m : 1.53969
[1mStep[0m  [10/53], [94mLoss[0m : 1.65282
[1mStep[0m  [15/53], [94mLoss[0m : 1.87742
[1mStep[0m  [20/53], [94mLoss[0m : 1.71054
[1mStep[0m  [25/53], [94mLoss[0m : 1.66214
[1mStep[0m  [30/53], [94mLoss[0m : 1.62592
[1mStep[0m  [35/53], [94mLoss[0m : 1.69512
[1mStep[0m  [40/53], [94mLoss[0m : 1.66810
[1mStep[0m  [45/53], [94mLoss[0m : 1.65351
[1mStep[0m  [50/53], [94mLoss[0m : 1.66118

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.607, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64767
[1mStep[0m  [5/53], [94mLoss[0m : 1.48747
[1mStep[0m  [10/53], [94mLoss[0m : 1.53890
[1mStep[0m  [15/53], [94mLoss[0m : 1.75054
[1mStep[0m  [20/53], [94mLoss[0m : 1.56699
[1mStep[0m  [25/53], [94mLoss[0m : 1.60773
[1mStep[0m  [30/53], [94mLoss[0m : 1.44101
[1mStep[0m  [35/53], [94mLoss[0m : 1.63554
[1mStep[0m  [40/53], [94mLoss[0m : 1.68822
[1mStep[0m  [45/53], [94mLoss[0m : 1.65974
[1mStep[0m  [50/53], [94mLoss[0m : 1.66627

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.498, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.55658
[1mStep[0m  [5/53], [94mLoss[0m : 1.61376
[1mStep[0m  [10/53], [94mLoss[0m : 1.54343
[1mStep[0m  [15/53], [94mLoss[0m : 1.52490
[1mStep[0m  [20/53], [94mLoss[0m : 1.55288
[1mStep[0m  [25/53], [94mLoss[0m : 1.63425
[1mStep[0m  [30/53], [94mLoss[0m : 1.46711
[1mStep[0m  [35/53], [94mLoss[0m : 1.68234
[1mStep[0m  [40/53], [94mLoss[0m : 1.45013
[1mStep[0m  [45/53], [94mLoss[0m : 1.65601
[1mStep[0m  [50/53], [94mLoss[0m : 1.46429

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.497, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.74743
[1mStep[0m  [5/53], [94mLoss[0m : 1.55057
[1mStep[0m  [10/53], [94mLoss[0m : 1.60921
[1mStep[0m  [15/53], [94mLoss[0m : 1.59280
[1mStep[0m  [20/53], [94mLoss[0m : 1.60274
[1mStep[0m  [25/53], [94mLoss[0m : 1.61655
[1mStep[0m  [30/53], [94mLoss[0m : 1.58766
[1mStep[0m  [35/53], [94mLoss[0m : 1.66257
[1mStep[0m  [40/53], [94mLoss[0m : 1.50049
[1mStep[0m  [45/53], [94mLoss[0m : 1.65480
[1mStep[0m  [50/53], [94mLoss[0m : 1.50159

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46677
[1mStep[0m  [5/53], [94mLoss[0m : 1.43352
[1mStep[0m  [10/53], [94mLoss[0m : 1.66938
[1mStep[0m  [15/53], [94mLoss[0m : 1.55615
[1mStep[0m  [20/53], [94mLoss[0m : 1.32414
[1mStep[0m  [25/53], [94mLoss[0m : 1.52482
[1mStep[0m  [30/53], [94mLoss[0m : 1.62249
[1mStep[0m  [35/53], [94mLoss[0m : 1.48748
[1mStep[0m  [40/53], [94mLoss[0m : 1.63078
[1mStep[0m  [45/53], [94mLoss[0m : 1.71676
[1mStep[0m  [50/53], [94mLoss[0m : 1.57086

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.530, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.37638
[1mStep[0m  [5/53], [94mLoss[0m : 1.56302
[1mStep[0m  [10/53], [94mLoss[0m : 1.30520
[1mStep[0m  [15/53], [94mLoss[0m : 1.63168
[1mStep[0m  [20/53], [94mLoss[0m : 1.49932
[1mStep[0m  [25/53], [94mLoss[0m : 1.58698
[1mStep[0m  [30/53], [94mLoss[0m : 1.45077
[1mStep[0m  [35/53], [94mLoss[0m : 1.50453
[1mStep[0m  [40/53], [94mLoss[0m : 1.44776
[1mStep[0m  [45/53], [94mLoss[0m : 1.50096
[1mStep[0m  [50/53], [94mLoss[0m : 1.57065

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.592, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.39760
[1mStep[0m  [5/53], [94mLoss[0m : 1.64042
[1mStep[0m  [10/53], [94mLoss[0m : 1.44203
[1mStep[0m  [15/53], [94mLoss[0m : 1.52946
[1mStep[0m  [20/53], [94mLoss[0m : 1.50337
[1mStep[0m  [25/53], [94mLoss[0m : 1.59387
[1mStep[0m  [30/53], [94mLoss[0m : 1.44680
[1mStep[0m  [35/53], [94mLoss[0m : 1.50491
[1mStep[0m  [40/53], [94mLoss[0m : 1.52516
[1mStep[0m  [45/53], [94mLoss[0m : 1.56698
[1mStep[0m  [50/53], [94mLoss[0m : 1.50195

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.516, [92mTest[0m: 2.525, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43405
[1mStep[0m  [5/53], [94mLoss[0m : 1.49460
[1mStep[0m  [10/53], [94mLoss[0m : 1.46823
[1mStep[0m  [15/53], [94mLoss[0m : 1.56961
[1mStep[0m  [20/53], [94mLoss[0m : 1.40511
[1mStep[0m  [25/53], [94mLoss[0m : 1.60430
[1mStep[0m  [30/53], [94mLoss[0m : 1.43930
[1mStep[0m  [35/53], [94mLoss[0m : 1.64688
[1mStep[0m  [40/53], [94mLoss[0m : 1.60316
[1mStep[0m  [45/53], [94mLoss[0m : 1.43005
[1mStep[0m  [50/53], [94mLoss[0m : 1.37638

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.565, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54444
[1mStep[0m  [5/53], [94mLoss[0m : 1.53278
[1mStep[0m  [10/53], [94mLoss[0m : 1.47948
[1mStep[0m  [15/53], [94mLoss[0m : 1.33880
[1mStep[0m  [20/53], [94mLoss[0m : 1.51232
[1mStep[0m  [25/53], [94mLoss[0m : 1.37274
[1mStep[0m  [30/53], [94mLoss[0m : 1.63517
[1mStep[0m  [35/53], [94mLoss[0m : 1.52811
[1mStep[0m  [40/53], [94mLoss[0m : 1.35550
[1mStep[0m  [45/53], [94mLoss[0m : 1.52739
[1mStep[0m  [50/53], [94mLoss[0m : 1.55173

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.622, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.39618
[1mStep[0m  [5/53], [94mLoss[0m : 1.46249
[1mStep[0m  [10/53], [94mLoss[0m : 1.45700
[1mStep[0m  [15/53], [94mLoss[0m : 1.46084
[1mStep[0m  [20/53], [94mLoss[0m : 1.41670
[1mStep[0m  [25/53], [94mLoss[0m : 1.42000
[1mStep[0m  [30/53], [94mLoss[0m : 1.35772
[1mStep[0m  [35/53], [94mLoss[0m : 1.36204
[1mStep[0m  [40/53], [94mLoss[0m : 1.32868
[1mStep[0m  [45/53], [94mLoss[0m : 1.42305
[1mStep[0m  [50/53], [94mLoss[0m : 1.42476

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.445, [92mTest[0m: 2.648, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.572
====================================

Phase 2 - Evaluation MAE:  2.5716288089752197
MAE score P1       2.406124
MAE score P2       2.571629
loss               1.445445
learning_rate          0.01
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.95193
[1mStep[0m  [2/26], [94mLoss[0m : 10.89463
[1mStep[0m  [4/26], [94mLoss[0m : 10.87196
[1mStep[0m  [6/26], [94mLoss[0m : 10.95042
[1mStep[0m  [8/26], [94mLoss[0m : 10.38401
[1mStep[0m  [10/26], [94mLoss[0m : 10.46398
[1mStep[0m  [12/26], [94mLoss[0m : 9.96423
[1mStep[0m  [14/26], [94mLoss[0m : 9.92422
[1mStep[0m  [16/26], [94mLoss[0m : 9.40905
[1mStep[0m  [18/26], [94mLoss[0m : 9.03235
[1mStep[0m  [20/26], [94mLoss[0m : 8.99881
[1mStep[0m  [22/26], [94mLoss[0m : 8.64341
[1mStep[0m  [24/26], [94mLoss[0m : 8.11378

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.812, [92mTest[0m: 11.033, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.70188
[1mStep[0m  [2/26], [94mLoss[0m : 7.48031
[1mStep[0m  [4/26], [94mLoss[0m : 7.24121
[1mStep[0m  [6/26], [94mLoss[0m : 6.86933
[1mStep[0m  [8/26], [94mLoss[0m : 6.44682
[1mStep[0m  [10/26], [94mLoss[0m : 6.30577
[1mStep[0m  [12/26], [94mLoss[0m : 6.34423
[1mStep[0m  [14/26], [94mLoss[0m : 5.88437
[1mStep[0m  [16/26], [94mLoss[0m : 5.78425
[1mStep[0m  [18/26], [94mLoss[0m : 5.15734
[1mStep[0m  [20/26], [94mLoss[0m : 4.98423
[1mStep[0m  [22/26], [94mLoss[0m : 4.78341
[1mStep[0m  [24/26], [94mLoss[0m : 4.17498

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.072, [92mTest[0m: 8.031, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.48001
[1mStep[0m  [2/26], [94mLoss[0m : 4.04405
[1mStep[0m  [4/26], [94mLoss[0m : 3.51316
[1mStep[0m  [6/26], [94mLoss[0m : 3.28220
[1mStep[0m  [8/26], [94mLoss[0m : 3.29416
[1mStep[0m  [10/26], [94mLoss[0m : 2.93682
[1mStep[0m  [12/26], [94mLoss[0m : 3.05652
[1mStep[0m  [14/26], [94mLoss[0m : 2.75635
[1mStep[0m  [16/26], [94mLoss[0m : 2.69396
[1mStep[0m  [18/26], [94mLoss[0m : 2.73191
[1mStep[0m  [20/26], [94mLoss[0m : 2.58047
[1mStep[0m  [22/26], [94mLoss[0m : 2.59019
[1mStep[0m  [24/26], [94mLoss[0m : 2.62984

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.101, [92mTest[0m: 3.621, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58199
[1mStep[0m  [2/26], [94mLoss[0m : 2.64258
[1mStep[0m  [4/26], [94mLoss[0m : 2.53928
[1mStep[0m  [6/26], [94mLoss[0m : 2.64532
[1mStep[0m  [8/26], [94mLoss[0m : 2.62065
[1mStep[0m  [10/26], [94mLoss[0m : 2.69695
[1mStep[0m  [12/26], [94mLoss[0m : 2.64130
[1mStep[0m  [14/26], [94mLoss[0m : 2.42968
[1mStep[0m  [16/26], [94mLoss[0m : 2.62358
[1mStep[0m  [18/26], [94mLoss[0m : 2.66276
[1mStep[0m  [20/26], [94mLoss[0m : 2.62901
[1mStep[0m  [22/26], [94mLoss[0m : 2.43857
[1mStep[0m  [24/26], [94mLoss[0m : 2.63241

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.877, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55408
[1mStep[0m  [2/26], [94mLoss[0m : 2.58858
[1mStep[0m  [4/26], [94mLoss[0m : 2.65553
[1mStep[0m  [6/26], [94mLoss[0m : 2.55388
[1mStep[0m  [8/26], [94mLoss[0m : 2.39775
[1mStep[0m  [10/26], [94mLoss[0m : 2.53130
[1mStep[0m  [12/26], [94mLoss[0m : 2.68960
[1mStep[0m  [14/26], [94mLoss[0m : 2.54090
[1mStep[0m  [16/26], [94mLoss[0m : 2.58107
[1mStep[0m  [18/26], [94mLoss[0m : 2.48983
[1mStep[0m  [20/26], [94mLoss[0m : 2.55231
[1mStep[0m  [22/26], [94mLoss[0m : 2.54759
[1mStep[0m  [24/26], [94mLoss[0m : 2.55769

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61227
[1mStep[0m  [2/26], [94mLoss[0m : 2.71594
[1mStep[0m  [4/26], [94mLoss[0m : 2.55927
[1mStep[0m  [6/26], [94mLoss[0m : 2.58995
[1mStep[0m  [8/26], [94mLoss[0m : 2.57567
[1mStep[0m  [10/26], [94mLoss[0m : 2.42847
[1mStep[0m  [12/26], [94mLoss[0m : 2.49169
[1mStep[0m  [14/26], [94mLoss[0m : 2.60111
[1mStep[0m  [16/26], [94mLoss[0m : 2.51488
[1mStep[0m  [18/26], [94mLoss[0m : 2.50881
[1mStep[0m  [20/26], [94mLoss[0m : 2.48249
[1mStep[0m  [22/26], [94mLoss[0m : 2.52868
[1mStep[0m  [24/26], [94mLoss[0m : 2.55984

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49773
[1mStep[0m  [2/26], [94mLoss[0m : 2.62007
[1mStep[0m  [4/26], [94mLoss[0m : 2.48590
[1mStep[0m  [6/26], [94mLoss[0m : 2.54815
[1mStep[0m  [8/26], [94mLoss[0m : 2.45574
[1mStep[0m  [10/26], [94mLoss[0m : 2.62224
[1mStep[0m  [12/26], [94mLoss[0m : 2.44266
[1mStep[0m  [14/26], [94mLoss[0m : 2.46234
[1mStep[0m  [16/26], [94mLoss[0m : 2.42547
[1mStep[0m  [18/26], [94mLoss[0m : 2.51378
[1mStep[0m  [20/26], [94mLoss[0m : 2.50758
[1mStep[0m  [22/26], [94mLoss[0m : 2.39109
[1mStep[0m  [24/26], [94mLoss[0m : 2.42403

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30397
[1mStep[0m  [2/26], [94mLoss[0m : 2.47209
[1mStep[0m  [4/26], [94mLoss[0m : 2.57093
[1mStep[0m  [6/26], [94mLoss[0m : 2.55055
[1mStep[0m  [8/26], [94mLoss[0m : 2.39923
[1mStep[0m  [10/26], [94mLoss[0m : 2.43510
[1mStep[0m  [12/26], [94mLoss[0m : 2.57907
[1mStep[0m  [14/26], [94mLoss[0m : 2.57493
[1mStep[0m  [16/26], [94mLoss[0m : 2.55563
[1mStep[0m  [18/26], [94mLoss[0m : 2.57061
[1mStep[0m  [20/26], [94mLoss[0m : 2.49366
[1mStep[0m  [22/26], [94mLoss[0m : 2.41510
[1mStep[0m  [24/26], [94mLoss[0m : 2.51520

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49879
[1mStep[0m  [2/26], [94mLoss[0m : 2.48057
[1mStep[0m  [4/26], [94mLoss[0m : 2.57938
[1mStep[0m  [6/26], [94mLoss[0m : 2.58295
[1mStep[0m  [8/26], [94mLoss[0m : 2.54731
[1mStep[0m  [10/26], [94mLoss[0m : 2.52118
[1mStep[0m  [12/26], [94mLoss[0m : 2.47525
[1mStep[0m  [14/26], [94mLoss[0m : 2.34497
[1mStep[0m  [16/26], [94mLoss[0m : 2.50105
[1mStep[0m  [18/26], [94mLoss[0m : 2.43869
[1mStep[0m  [20/26], [94mLoss[0m : 2.45161
[1mStep[0m  [22/26], [94mLoss[0m : 2.35524
[1mStep[0m  [24/26], [94mLoss[0m : 2.62537

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57546
[1mStep[0m  [2/26], [94mLoss[0m : 2.65985
[1mStep[0m  [4/26], [94mLoss[0m : 2.39218
[1mStep[0m  [6/26], [94mLoss[0m : 2.41058
[1mStep[0m  [8/26], [94mLoss[0m : 2.37832
[1mStep[0m  [10/26], [94mLoss[0m : 2.31750
[1mStep[0m  [12/26], [94mLoss[0m : 2.54330
[1mStep[0m  [14/26], [94mLoss[0m : 2.37556
[1mStep[0m  [16/26], [94mLoss[0m : 2.56834
[1mStep[0m  [18/26], [94mLoss[0m : 2.60942
[1mStep[0m  [20/26], [94mLoss[0m : 2.38782
[1mStep[0m  [22/26], [94mLoss[0m : 2.48416
[1mStep[0m  [24/26], [94mLoss[0m : 2.44451

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41904
[1mStep[0m  [2/26], [94mLoss[0m : 2.37294
[1mStep[0m  [4/26], [94mLoss[0m : 2.41903
[1mStep[0m  [6/26], [94mLoss[0m : 2.54372
[1mStep[0m  [8/26], [94mLoss[0m : 2.33752
[1mStep[0m  [10/26], [94mLoss[0m : 2.62459
[1mStep[0m  [12/26], [94mLoss[0m : 2.49592
[1mStep[0m  [14/26], [94mLoss[0m : 2.48161
[1mStep[0m  [16/26], [94mLoss[0m : 2.31982
[1mStep[0m  [18/26], [94mLoss[0m : 2.51731
[1mStep[0m  [20/26], [94mLoss[0m : 2.56313
[1mStep[0m  [22/26], [94mLoss[0m : 2.59848
[1mStep[0m  [24/26], [94mLoss[0m : 2.34136

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39872
[1mStep[0m  [2/26], [94mLoss[0m : 2.36963
[1mStep[0m  [4/26], [94mLoss[0m : 2.49952
[1mStep[0m  [6/26], [94mLoss[0m : 2.49717
[1mStep[0m  [8/26], [94mLoss[0m : 2.50921
[1mStep[0m  [10/26], [94mLoss[0m : 2.55826
[1mStep[0m  [12/26], [94mLoss[0m : 2.40634
[1mStep[0m  [14/26], [94mLoss[0m : 2.56129
[1mStep[0m  [16/26], [94mLoss[0m : 2.50108
[1mStep[0m  [18/26], [94mLoss[0m : 2.55878
[1mStep[0m  [20/26], [94mLoss[0m : 2.34041
[1mStep[0m  [22/26], [94mLoss[0m : 2.53417
[1mStep[0m  [24/26], [94mLoss[0m : 2.45501

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52675
[1mStep[0m  [2/26], [94mLoss[0m : 2.42316
[1mStep[0m  [4/26], [94mLoss[0m : 2.40474
[1mStep[0m  [6/26], [94mLoss[0m : 2.45163
[1mStep[0m  [8/26], [94mLoss[0m : 2.42224
[1mStep[0m  [10/26], [94mLoss[0m : 2.49988
[1mStep[0m  [12/26], [94mLoss[0m : 2.35302
[1mStep[0m  [14/26], [94mLoss[0m : 2.42393
[1mStep[0m  [16/26], [94mLoss[0m : 2.41221
[1mStep[0m  [18/26], [94mLoss[0m : 2.42304
[1mStep[0m  [20/26], [94mLoss[0m : 2.33201
[1mStep[0m  [22/26], [94mLoss[0m : 2.53713
[1mStep[0m  [24/26], [94mLoss[0m : 2.50221

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59713
[1mStep[0m  [2/26], [94mLoss[0m : 2.39674
[1mStep[0m  [4/26], [94mLoss[0m : 2.33505
[1mStep[0m  [6/26], [94mLoss[0m : 2.34180
[1mStep[0m  [8/26], [94mLoss[0m : 2.45689
[1mStep[0m  [10/26], [94mLoss[0m : 2.27759
[1mStep[0m  [12/26], [94mLoss[0m : 2.42065
[1mStep[0m  [14/26], [94mLoss[0m : 2.36026
[1mStep[0m  [16/26], [94mLoss[0m : 2.31591
[1mStep[0m  [18/26], [94mLoss[0m : 2.47297
[1mStep[0m  [20/26], [94mLoss[0m : 2.36876
[1mStep[0m  [22/26], [94mLoss[0m : 2.55132
[1mStep[0m  [24/26], [94mLoss[0m : 2.34708

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28531
[1mStep[0m  [2/26], [94mLoss[0m : 2.52610
[1mStep[0m  [4/26], [94mLoss[0m : 2.32725
[1mStep[0m  [6/26], [94mLoss[0m : 2.38785
[1mStep[0m  [8/26], [94mLoss[0m : 2.45198
[1mStep[0m  [10/26], [94mLoss[0m : 2.36396
[1mStep[0m  [12/26], [94mLoss[0m : 2.42702
[1mStep[0m  [14/26], [94mLoss[0m : 2.57749
[1mStep[0m  [16/26], [94mLoss[0m : 2.32134
[1mStep[0m  [18/26], [94mLoss[0m : 2.34632
[1mStep[0m  [20/26], [94mLoss[0m : 2.33214
[1mStep[0m  [22/26], [94mLoss[0m : 2.42654
[1mStep[0m  [24/26], [94mLoss[0m : 2.47290

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42561
[1mStep[0m  [2/26], [94mLoss[0m : 2.43832
[1mStep[0m  [4/26], [94mLoss[0m : 2.42553
[1mStep[0m  [6/26], [94mLoss[0m : 2.43804
[1mStep[0m  [8/26], [94mLoss[0m : 2.39032
[1mStep[0m  [10/26], [94mLoss[0m : 2.41204
[1mStep[0m  [12/26], [94mLoss[0m : 2.26402
[1mStep[0m  [14/26], [94mLoss[0m : 2.58450
[1mStep[0m  [16/26], [94mLoss[0m : 2.50017
[1mStep[0m  [18/26], [94mLoss[0m : 2.43822
[1mStep[0m  [20/26], [94mLoss[0m : 2.40773
[1mStep[0m  [22/26], [94mLoss[0m : 2.58064
[1mStep[0m  [24/26], [94mLoss[0m : 2.42974

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46104
[1mStep[0m  [2/26], [94mLoss[0m : 2.42823
[1mStep[0m  [4/26], [94mLoss[0m : 2.42787
[1mStep[0m  [6/26], [94mLoss[0m : 2.52224
[1mStep[0m  [8/26], [94mLoss[0m : 2.36915
[1mStep[0m  [10/26], [94mLoss[0m : 2.59686
[1mStep[0m  [12/26], [94mLoss[0m : 2.40003
[1mStep[0m  [14/26], [94mLoss[0m : 2.29615
[1mStep[0m  [16/26], [94mLoss[0m : 2.27564
[1mStep[0m  [18/26], [94mLoss[0m : 2.48068
[1mStep[0m  [20/26], [94mLoss[0m : 2.36723
[1mStep[0m  [22/26], [94mLoss[0m : 2.38334
[1mStep[0m  [24/26], [94mLoss[0m : 2.53385

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28427
[1mStep[0m  [2/26], [94mLoss[0m : 2.25372
[1mStep[0m  [4/26], [94mLoss[0m : 2.49506
[1mStep[0m  [6/26], [94mLoss[0m : 2.44843
[1mStep[0m  [8/26], [94mLoss[0m : 2.33625
[1mStep[0m  [10/26], [94mLoss[0m : 2.55538
[1mStep[0m  [12/26], [94mLoss[0m : 2.33028
[1mStep[0m  [14/26], [94mLoss[0m : 2.28376
[1mStep[0m  [16/26], [94mLoss[0m : 2.55747
[1mStep[0m  [18/26], [94mLoss[0m : 2.53224
[1mStep[0m  [20/26], [94mLoss[0m : 2.31813
[1mStep[0m  [22/26], [94mLoss[0m : 2.26979
[1mStep[0m  [24/26], [94mLoss[0m : 2.40972

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34429
[1mStep[0m  [2/26], [94mLoss[0m : 2.50171
[1mStep[0m  [4/26], [94mLoss[0m : 2.29661
[1mStep[0m  [6/26], [94mLoss[0m : 2.26369
[1mStep[0m  [8/26], [94mLoss[0m : 2.20912
[1mStep[0m  [10/26], [94mLoss[0m : 2.39812
[1mStep[0m  [12/26], [94mLoss[0m : 2.52177
[1mStep[0m  [14/26], [94mLoss[0m : 2.21198
[1mStep[0m  [16/26], [94mLoss[0m : 2.39493
[1mStep[0m  [18/26], [94mLoss[0m : 2.58750
[1mStep[0m  [20/26], [94mLoss[0m : 2.54549
[1mStep[0m  [22/26], [94mLoss[0m : 2.50019
[1mStep[0m  [24/26], [94mLoss[0m : 2.29155

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36365
[1mStep[0m  [2/26], [94mLoss[0m : 2.33226
[1mStep[0m  [4/26], [94mLoss[0m : 2.42914
[1mStep[0m  [6/26], [94mLoss[0m : 2.34772
[1mStep[0m  [8/26], [94mLoss[0m : 2.46020
[1mStep[0m  [10/26], [94mLoss[0m : 2.44582
[1mStep[0m  [12/26], [94mLoss[0m : 2.48015
[1mStep[0m  [14/26], [94mLoss[0m : 2.44992
[1mStep[0m  [16/26], [94mLoss[0m : 2.22344
[1mStep[0m  [18/26], [94mLoss[0m : 2.43764
[1mStep[0m  [20/26], [94mLoss[0m : 2.35869
[1mStep[0m  [22/26], [94mLoss[0m : 2.29032
[1mStep[0m  [24/26], [94mLoss[0m : 2.43941

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.374, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41724
[1mStep[0m  [2/26], [94mLoss[0m : 2.38239
[1mStep[0m  [4/26], [94mLoss[0m : 2.42325
[1mStep[0m  [6/26], [94mLoss[0m : 2.40689
[1mStep[0m  [8/26], [94mLoss[0m : 2.40123
[1mStep[0m  [10/26], [94mLoss[0m : 2.42302
[1mStep[0m  [12/26], [94mLoss[0m : 2.33044
[1mStep[0m  [14/26], [94mLoss[0m : 2.51832
[1mStep[0m  [16/26], [94mLoss[0m : 2.35591
[1mStep[0m  [18/26], [94mLoss[0m : 2.41335
[1mStep[0m  [20/26], [94mLoss[0m : 2.40422
[1mStep[0m  [22/26], [94mLoss[0m : 2.40857
[1mStep[0m  [24/26], [94mLoss[0m : 2.29321

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.385, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48644
[1mStep[0m  [2/26], [94mLoss[0m : 2.35746
[1mStep[0m  [4/26], [94mLoss[0m : 2.44217
[1mStep[0m  [6/26], [94mLoss[0m : 2.41224
[1mStep[0m  [8/26], [94mLoss[0m : 2.40345
[1mStep[0m  [10/26], [94mLoss[0m : 2.28086
[1mStep[0m  [12/26], [94mLoss[0m : 2.39382
[1mStep[0m  [14/26], [94mLoss[0m : 2.45181
[1mStep[0m  [16/26], [94mLoss[0m : 2.33790
[1mStep[0m  [18/26], [94mLoss[0m : 2.50261
[1mStep[0m  [20/26], [94mLoss[0m : 2.42838
[1mStep[0m  [22/26], [94mLoss[0m : 2.47136
[1mStep[0m  [24/26], [94mLoss[0m : 2.48326

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.398, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43997
[1mStep[0m  [2/26], [94mLoss[0m : 2.48152
[1mStep[0m  [4/26], [94mLoss[0m : 2.41289
[1mStep[0m  [6/26], [94mLoss[0m : 2.44362
[1mStep[0m  [8/26], [94mLoss[0m : 2.35616
[1mStep[0m  [10/26], [94mLoss[0m : 2.35527
[1mStep[0m  [12/26], [94mLoss[0m : 2.29783
[1mStep[0m  [14/26], [94mLoss[0m : 2.43179
[1mStep[0m  [16/26], [94mLoss[0m : 2.37897
[1mStep[0m  [18/26], [94mLoss[0m : 2.40389
[1mStep[0m  [20/26], [94mLoss[0m : 2.33975
[1mStep[0m  [22/26], [94mLoss[0m : 2.34345
[1mStep[0m  [24/26], [94mLoss[0m : 2.34580

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.377, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28399
[1mStep[0m  [2/26], [94mLoss[0m : 2.26030
[1mStep[0m  [4/26], [94mLoss[0m : 2.35234
[1mStep[0m  [6/26], [94mLoss[0m : 2.37275
[1mStep[0m  [8/26], [94mLoss[0m : 2.31492
[1mStep[0m  [10/26], [94mLoss[0m : 2.48243
[1mStep[0m  [12/26], [94mLoss[0m : 2.38471
[1mStep[0m  [14/26], [94mLoss[0m : 2.37120
[1mStep[0m  [16/26], [94mLoss[0m : 2.48381
[1mStep[0m  [18/26], [94mLoss[0m : 2.53971
[1mStep[0m  [20/26], [94mLoss[0m : 2.46132
[1mStep[0m  [22/26], [94mLoss[0m : 2.30379
[1mStep[0m  [24/26], [94mLoss[0m : 2.52053

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.371, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34045
[1mStep[0m  [2/26], [94mLoss[0m : 2.43343
[1mStep[0m  [4/26], [94mLoss[0m : 2.43909
[1mStep[0m  [6/26], [94mLoss[0m : 2.28905
[1mStep[0m  [8/26], [94mLoss[0m : 2.36217
[1mStep[0m  [10/26], [94mLoss[0m : 2.22913
[1mStep[0m  [12/26], [94mLoss[0m : 2.39355
[1mStep[0m  [14/26], [94mLoss[0m : 2.37023
[1mStep[0m  [16/26], [94mLoss[0m : 2.45750
[1mStep[0m  [18/26], [94mLoss[0m : 2.48483
[1mStep[0m  [20/26], [94mLoss[0m : 2.36243
[1mStep[0m  [22/26], [94mLoss[0m : 2.48183
[1mStep[0m  [24/26], [94mLoss[0m : 2.38156

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30098
[1mStep[0m  [2/26], [94mLoss[0m : 2.35517
[1mStep[0m  [4/26], [94mLoss[0m : 2.33739
[1mStep[0m  [6/26], [94mLoss[0m : 2.46491
[1mStep[0m  [8/26], [94mLoss[0m : 2.40419
[1mStep[0m  [10/26], [94mLoss[0m : 2.22562
[1mStep[0m  [12/26], [94mLoss[0m : 2.44491
[1mStep[0m  [14/26], [94mLoss[0m : 2.28973
[1mStep[0m  [16/26], [94mLoss[0m : 2.23663
[1mStep[0m  [18/26], [94mLoss[0m : 2.53958
[1mStep[0m  [20/26], [94mLoss[0m : 2.42921
[1mStep[0m  [22/26], [94mLoss[0m : 2.26452
[1mStep[0m  [24/26], [94mLoss[0m : 2.35686

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37486
[1mStep[0m  [2/26], [94mLoss[0m : 2.38551
[1mStep[0m  [4/26], [94mLoss[0m : 2.49906
[1mStep[0m  [6/26], [94mLoss[0m : 2.37040
[1mStep[0m  [8/26], [94mLoss[0m : 2.38334
[1mStep[0m  [10/26], [94mLoss[0m : 2.51462
[1mStep[0m  [12/26], [94mLoss[0m : 2.32190
[1mStep[0m  [14/26], [94mLoss[0m : 2.33125
[1mStep[0m  [16/26], [94mLoss[0m : 2.35258
[1mStep[0m  [18/26], [94mLoss[0m : 2.48026
[1mStep[0m  [20/26], [94mLoss[0m : 2.36228
[1mStep[0m  [22/26], [94mLoss[0m : 2.39151
[1mStep[0m  [24/26], [94mLoss[0m : 2.51265

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34461
[1mStep[0m  [2/26], [94mLoss[0m : 2.51264
[1mStep[0m  [4/26], [94mLoss[0m : 2.36015
[1mStep[0m  [6/26], [94mLoss[0m : 2.24394
[1mStep[0m  [8/26], [94mLoss[0m : 2.42516
[1mStep[0m  [10/26], [94mLoss[0m : 2.42880
[1mStep[0m  [12/26], [94mLoss[0m : 2.37092
[1mStep[0m  [14/26], [94mLoss[0m : 2.36987
[1mStep[0m  [16/26], [94mLoss[0m : 2.40121
[1mStep[0m  [18/26], [94mLoss[0m : 2.26110
[1mStep[0m  [20/26], [94mLoss[0m : 2.45333
[1mStep[0m  [22/26], [94mLoss[0m : 2.28204
[1mStep[0m  [24/26], [94mLoss[0m : 2.39752

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.372, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43209
[1mStep[0m  [2/26], [94mLoss[0m : 2.39281
[1mStep[0m  [4/26], [94mLoss[0m : 2.39570
[1mStep[0m  [6/26], [94mLoss[0m : 2.34469
[1mStep[0m  [8/26], [94mLoss[0m : 2.39094
[1mStep[0m  [10/26], [94mLoss[0m : 2.29019
[1mStep[0m  [12/26], [94mLoss[0m : 2.40784
[1mStep[0m  [14/26], [94mLoss[0m : 2.46421
[1mStep[0m  [16/26], [94mLoss[0m : 2.44315
[1mStep[0m  [18/26], [94mLoss[0m : 2.34443
[1mStep[0m  [20/26], [94mLoss[0m : 2.29069
[1mStep[0m  [22/26], [94mLoss[0m : 2.47555
[1mStep[0m  [24/26], [94mLoss[0m : 2.48944

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.363, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42805
[1mStep[0m  [2/26], [94mLoss[0m : 2.30092
[1mStep[0m  [4/26], [94mLoss[0m : 2.41578
[1mStep[0m  [6/26], [94mLoss[0m : 2.33668
[1mStep[0m  [8/26], [94mLoss[0m : 2.36106
[1mStep[0m  [10/26], [94mLoss[0m : 2.49956
[1mStep[0m  [12/26], [94mLoss[0m : 2.44770
[1mStep[0m  [14/26], [94mLoss[0m : 2.27869
[1mStep[0m  [16/26], [94mLoss[0m : 2.41342
[1mStep[0m  [18/26], [94mLoss[0m : 2.34705
[1mStep[0m  [20/26], [94mLoss[0m : 2.31894
[1mStep[0m  [22/26], [94mLoss[0m : 2.49212
[1mStep[0m  [24/26], [94mLoss[0m : 2.37787

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.379, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.3930269938248854
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.32876
[1mStep[0m  [2/26], [94mLoss[0m : 2.29385
[1mStep[0m  [4/26], [94mLoss[0m : 2.33808
[1mStep[0m  [6/26], [94mLoss[0m : 2.32947
[1mStep[0m  [8/26], [94mLoss[0m : 2.46774
[1mStep[0m  [10/26], [94mLoss[0m : 2.52799
[1mStep[0m  [12/26], [94mLoss[0m : 2.40328
[1mStep[0m  [14/26], [94mLoss[0m : 2.46094
[1mStep[0m  [16/26], [94mLoss[0m : 2.58417
[1mStep[0m  [18/26], [94mLoss[0m : 2.41442
[1mStep[0m  [20/26], [94mLoss[0m : 2.64924
[1mStep[0m  [22/26], [94mLoss[0m : 2.41906
[1mStep[0m  [24/26], [94mLoss[0m : 2.49587

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37883
[1mStep[0m  [2/26], [94mLoss[0m : 2.42251
[1mStep[0m  [4/26], [94mLoss[0m : 2.23290
[1mStep[0m  [6/26], [94mLoss[0m : 2.27953
[1mStep[0m  [8/26], [94mLoss[0m : 2.32862
[1mStep[0m  [10/26], [94mLoss[0m : 2.28888
[1mStep[0m  [12/26], [94mLoss[0m : 2.27404
[1mStep[0m  [14/26], [94mLoss[0m : 2.33880
[1mStep[0m  [16/26], [94mLoss[0m : 2.37867
[1mStep[0m  [18/26], [94mLoss[0m : 2.27831
[1mStep[0m  [20/26], [94mLoss[0m : 2.29145
[1mStep[0m  [22/26], [94mLoss[0m : 2.39360
[1mStep[0m  [24/26], [94mLoss[0m : 2.23775

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22153
[1mStep[0m  [2/26], [94mLoss[0m : 2.35137
[1mStep[0m  [4/26], [94mLoss[0m : 2.29979
[1mStep[0m  [6/26], [94mLoss[0m : 2.21157
[1mStep[0m  [8/26], [94mLoss[0m : 2.26143
[1mStep[0m  [10/26], [94mLoss[0m : 2.23222
[1mStep[0m  [12/26], [94mLoss[0m : 2.10052
[1mStep[0m  [14/26], [94mLoss[0m : 2.21782
[1mStep[0m  [16/26], [94mLoss[0m : 2.17152
[1mStep[0m  [18/26], [94mLoss[0m : 2.22679
[1mStep[0m  [20/26], [94mLoss[0m : 2.21524
[1mStep[0m  [22/26], [94mLoss[0m : 2.21324
[1mStep[0m  [24/26], [94mLoss[0m : 2.26204

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31530
[1mStep[0m  [2/26], [94mLoss[0m : 2.11561
[1mStep[0m  [4/26], [94mLoss[0m : 2.20715
[1mStep[0m  [6/26], [94mLoss[0m : 2.15250
[1mStep[0m  [8/26], [94mLoss[0m : 2.16268
[1mStep[0m  [10/26], [94mLoss[0m : 2.10814
[1mStep[0m  [12/26], [94mLoss[0m : 2.11229
[1mStep[0m  [14/26], [94mLoss[0m : 2.16386
[1mStep[0m  [16/26], [94mLoss[0m : 2.13785
[1mStep[0m  [18/26], [94mLoss[0m : 2.31924
[1mStep[0m  [20/26], [94mLoss[0m : 2.07544
[1mStep[0m  [22/26], [94mLoss[0m : 2.17233
[1mStep[0m  [24/26], [94mLoss[0m : 2.07854

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.06059
[1mStep[0m  [2/26], [94mLoss[0m : 2.13900
[1mStep[0m  [4/26], [94mLoss[0m : 2.03213
[1mStep[0m  [6/26], [94mLoss[0m : 2.07939
[1mStep[0m  [8/26], [94mLoss[0m : 2.11793
[1mStep[0m  [10/26], [94mLoss[0m : 1.92614
[1mStep[0m  [12/26], [94mLoss[0m : 2.15264
[1mStep[0m  [14/26], [94mLoss[0m : 2.07189
[1mStep[0m  [16/26], [94mLoss[0m : 2.08883
[1mStep[0m  [18/26], [94mLoss[0m : 2.08903
[1mStep[0m  [20/26], [94mLoss[0m : 2.13436
[1mStep[0m  [22/26], [94mLoss[0m : 2.18240
[1mStep[0m  [24/26], [94mLoss[0m : 2.17402

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.91629
[1mStep[0m  [2/26], [94mLoss[0m : 1.91381
[1mStep[0m  [4/26], [94mLoss[0m : 2.05714
[1mStep[0m  [6/26], [94mLoss[0m : 2.02772
[1mStep[0m  [8/26], [94mLoss[0m : 2.02536
[1mStep[0m  [10/26], [94mLoss[0m : 1.99007
[1mStep[0m  [12/26], [94mLoss[0m : 1.95042
[1mStep[0m  [14/26], [94mLoss[0m : 2.05293
[1mStep[0m  [16/26], [94mLoss[0m : 2.05507
[1mStep[0m  [18/26], [94mLoss[0m : 1.96007
[1mStep[0m  [20/26], [94mLoss[0m : 2.02847
[1mStep[0m  [22/26], [94mLoss[0m : 2.15530
[1mStep[0m  [24/26], [94mLoss[0m : 2.21112

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.02285
[1mStep[0m  [2/26], [94mLoss[0m : 2.05170
[1mStep[0m  [4/26], [94mLoss[0m : 1.86665
[1mStep[0m  [6/26], [94mLoss[0m : 1.89947
[1mStep[0m  [8/26], [94mLoss[0m : 2.00654
[1mStep[0m  [10/26], [94mLoss[0m : 1.86493
[1mStep[0m  [12/26], [94mLoss[0m : 1.94293
[1mStep[0m  [14/26], [94mLoss[0m : 2.01473
[1mStep[0m  [16/26], [94mLoss[0m : 1.92169
[1mStep[0m  [18/26], [94mLoss[0m : 1.94880
[1mStep[0m  [20/26], [94mLoss[0m : 1.97837
[1mStep[0m  [22/26], [94mLoss[0m : 2.09022
[1mStep[0m  [24/26], [94mLoss[0m : 2.00665

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.87857
[1mStep[0m  [2/26], [94mLoss[0m : 1.71311
[1mStep[0m  [4/26], [94mLoss[0m : 1.85018
[1mStep[0m  [6/26], [94mLoss[0m : 1.84599
[1mStep[0m  [8/26], [94mLoss[0m : 1.73237
[1mStep[0m  [10/26], [94mLoss[0m : 1.75182
[1mStep[0m  [12/26], [94mLoss[0m : 1.83307
[1mStep[0m  [14/26], [94mLoss[0m : 1.90468
[1mStep[0m  [16/26], [94mLoss[0m : 1.80948
[1mStep[0m  [18/26], [94mLoss[0m : 1.82013
[1mStep[0m  [20/26], [94mLoss[0m : 1.84001
[1mStep[0m  [22/26], [94mLoss[0m : 2.10763
[1mStep[0m  [24/26], [94mLoss[0m : 2.01179

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.71643
[1mStep[0m  [2/26], [94mLoss[0m : 1.89092
[1mStep[0m  [4/26], [94mLoss[0m : 1.89886
[1mStep[0m  [6/26], [94mLoss[0m : 1.72601
[1mStep[0m  [8/26], [94mLoss[0m : 2.01278
[1mStep[0m  [10/26], [94mLoss[0m : 1.94973
[1mStep[0m  [12/26], [94mLoss[0m : 1.89436
[1mStep[0m  [14/26], [94mLoss[0m : 1.86237
[1mStep[0m  [16/26], [94mLoss[0m : 1.81199
[1mStep[0m  [18/26], [94mLoss[0m : 1.85943
[1mStep[0m  [20/26], [94mLoss[0m : 1.76369
[1mStep[0m  [22/26], [94mLoss[0m : 1.83475
[1mStep[0m  [24/26], [94mLoss[0m : 1.76610

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.828, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.69052
[1mStep[0m  [2/26], [94mLoss[0m : 1.82117
[1mStep[0m  [4/26], [94mLoss[0m : 1.70084
[1mStep[0m  [6/26], [94mLoss[0m : 1.71901
[1mStep[0m  [8/26], [94mLoss[0m : 1.64704
[1mStep[0m  [10/26], [94mLoss[0m : 1.74759
[1mStep[0m  [12/26], [94mLoss[0m : 1.78103
[1mStep[0m  [14/26], [94mLoss[0m : 1.90594
[1mStep[0m  [16/26], [94mLoss[0m : 1.77749
[1mStep[0m  [18/26], [94mLoss[0m : 1.79786
[1mStep[0m  [20/26], [94mLoss[0m : 1.82892
[1mStep[0m  [22/26], [94mLoss[0m : 1.83561
[1mStep[0m  [24/26], [94mLoss[0m : 1.85302

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.63001
[1mStep[0m  [2/26], [94mLoss[0m : 1.67724
[1mStep[0m  [4/26], [94mLoss[0m : 1.74241
[1mStep[0m  [6/26], [94mLoss[0m : 1.77800
[1mStep[0m  [8/26], [94mLoss[0m : 1.59703
[1mStep[0m  [10/26], [94mLoss[0m : 1.71040
[1mStep[0m  [12/26], [94mLoss[0m : 1.70234
[1mStep[0m  [14/26], [94mLoss[0m : 1.77036
[1mStep[0m  [16/26], [94mLoss[0m : 1.73373
[1mStep[0m  [18/26], [94mLoss[0m : 1.72164
[1mStep[0m  [20/26], [94mLoss[0m : 1.86832
[1mStep[0m  [22/26], [94mLoss[0m : 1.70841
[1mStep[0m  [24/26], [94mLoss[0m : 1.80191

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82464
[1mStep[0m  [2/26], [94mLoss[0m : 1.71047
[1mStep[0m  [4/26], [94mLoss[0m : 1.58188
[1mStep[0m  [6/26], [94mLoss[0m : 1.67865
[1mStep[0m  [8/26], [94mLoss[0m : 1.63475
[1mStep[0m  [10/26], [94mLoss[0m : 1.65219
[1mStep[0m  [12/26], [94mLoss[0m : 1.64278
[1mStep[0m  [14/26], [94mLoss[0m : 1.74325
[1mStep[0m  [16/26], [94mLoss[0m : 1.78125
[1mStep[0m  [18/26], [94mLoss[0m : 1.70685
[1mStep[0m  [20/26], [94mLoss[0m : 1.67874
[1mStep[0m  [22/26], [94mLoss[0m : 1.69041
[1mStep[0m  [24/26], [94mLoss[0m : 1.59290

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.682, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.53787
[1mStep[0m  [2/26], [94mLoss[0m : 1.48034
[1mStep[0m  [4/26], [94mLoss[0m : 1.70577
[1mStep[0m  [6/26], [94mLoss[0m : 1.64988
[1mStep[0m  [8/26], [94mLoss[0m : 1.64370
[1mStep[0m  [10/26], [94mLoss[0m : 1.70228
[1mStep[0m  [12/26], [94mLoss[0m : 1.76924
[1mStep[0m  [14/26], [94mLoss[0m : 1.64431
[1mStep[0m  [16/26], [94mLoss[0m : 1.65944
[1mStep[0m  [18/26], [94mLoss[0m : 1.74704
[1mStep[0m  [20/26], [94mLoss[0m : 1.74383
[1mStep[0m  [22/26], [94mLoss[0m : 1.79009
[1mStep[0m  [24/26], [94mLoss[0m : 1.70563

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.57557
[1mStep[0m  [2/26], [94mLoss[0m : 1.57473
[1mStep[0m  [4/26], [94mLoss[0m : 1.47938
[1mStep[0m  [6/26], [94mLoss[0m : 1.52094
[1mStep[0m  [8/26], [94mLoss[0m : 1.61862
[1mStep[0m  [10/26], [94mLoss[0m : 1.57378
[1mStep[0m  [12/26], [94mLoss[0m : 1.59313
[1mStep[0m  [14/26], [94mLoss[0m : 1.61873
[1mStep[0m  [16/26], [94mLoss[0m : 1.67766
[1mStep[0m  [18/26], [94mLoss[0m : 1.73384
[1mStep[0m  [20/26], [94mLoss[0m : 1.55217
[1mStep[0m  [22/26], [94mLoss[0m : 1.60912
[1mStep[0m  [24/26], [94mLoss[0m : 1.55410

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.63657
[1mStep[0m  [2/26], [94mLoss[0m : 1.49314
[1mStep[0m  [4/26], [94mLoss[0m : 1.50111
[1mStep[0m  [6/26], [94mLoss[0m : 1.63128
[1mStep[0m  [8/26], [94mLoss[0m : 1.58979
[1mStep[0m  [10/26], [94mLoss[0m : 1.60322
[1mStep[0m  [12/26], [94mLoss[0m : 1.56579
[1mStep[0m  [14/26], [94mLoss[0m : 1.64955
[1mStep[0m  [16/26], [94mLoss[0m : 1.53400
[1mStep[0m  [18/26], [94mLoss[0m : 1.52065
[1mStep[0m  [20/26], [94mLoss[0m : 1.60165
[1mStep[0m  [22/26], [94mLoss[0m : 1.69903
[1mStep[0m  [24/26], [94mLoss[0m : 1.50427

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54546
[1mStep[0m  [2/26], [94mLoss[0m : 1.50385
[1mStep[0m  [4/26], [94mLoss[0m : 1.40015
[1mStep[0m  [6/26], [94mLoss[0m : 1.50025
[1mStep[0m  [8/26], [94mLoss[0m : 1.49736
[1mStep[0m  [10/26], [94mLoss[0m : 1.51987
[1mStep[0m  [12/26], [94mLoss[0m : 1.51993
[1mStep[0m  [14/26], [94mLoss[0m : 1.51537
[1mStep[0m  [16/26], [94mLoss[0m : 1.59875
[1mStep[0m  [18/26], [94mLoss[0m : 1.61035
[1mStep[0m  [20/26], [94mLoss[0m : 1.49282
[1mStep[0m  [22/26], [94mLoss[0m : 1.45921
[1mStep[0m  [24/26], [94mLoss[0m : 1.68311

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.540, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52801
[1mStep[0m  [2/26], [94mLoss[0m : 1.48848
[1mStep[0m  [4/26], [94mLoss[0m : 1.52892
[1mStep[0m  [6/26], [94mLoss[0m : 1.48436
[1mStep[0m  [8/26], [94mLoss[0m : 1.46245
[1mStep[0m  [10/26], [94mLoss[0m : 1.43938
[1mStep[0m  [12/26], [94mLoss[0m : 1.57672
[1mStep[0m  [14/26], [94mLoss[0m : 1.46260
[1mStep[0m  [16/26], [94mLoss[0m : 1.51755
[1mStep[0m  [18/26], [94mLoss[0m : 1.50432
[1mStep[0m  [20/26], [94mLoss[0m : 1.45195
[1mStep[0m  [22/26], [94mLoss[0m : 1.58241
[1mStep[0m  [24/26], [94mLoss[0m : 1.55076

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49510
[1mStep[0m  [2/26], [94mLoss[0m : 1.64388
[1mStep[0m  [4/26], [94mLoss[0m : 1.37709
[1mStep[0m  [6/26], [94mLoss[0m : 1.49596
[1mStep[0m  [8/26], [94mLoss[0m : 1.44959
[1mStep[0m  [10/26], [94mLoss[0m : 1.44565
[1mStep[0m  [12/26], [94mLoss[0m : 1.59926
[1mStep[0m  [14/26], [94mLoss[0m : 1.52273
[1mStep[0m  [16/26], [94mLoss[0m : 1.52729
[1mStep[0m  [18/26], [94mLoss[0m : 1.49129
[1mStep[0m  [20/26], [94mLoss[0m : 1.37387
[1mStep[0m  [22/26], [94mLoss[0m : 1.47712
[1mStep[0m  [24/26], [94mLoss[0m : 1.50332

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.43287
[1mStep[0m  [2/26], [94mLoss[0m : 1.47662
[1mStep[0m  [4/26], [94mLoss[0m : 1.37837
[1mStep[0m  [6/26], [94mLoss[0m : 1.31478
[1mStep[0m  [8/26], [94mLoss[0m : 1.35173
[1mStep[0m  [10/26], [94mLoss[0m : 1.43996
[1mStep[0m  [12/26], [94mLoss[0m : 1.39068
[1mStep[0m  [14/26], [94mLoss[0m : 1.43936
[1mStep[0m  [16/26], [94mLoss[0m : 1.47123
[1mStep[0m  [18/26], [94mLoss[0m : 1.51057
[1mStep[0m  [20/26], [94mLoss[0m : 1.36485
[1mStep[0m  [22/26], [94mLoss[0m : 1.41482
[1mStep[0m  [24/26], [94mLoss[0m : 1.55529

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.433, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.35162
[1mStep[0m  [2/26], [94mLoss[0m : 1.39618
[1mStep[0m  [4/26], [94mLoss[0m : 1.43206
[1mStep[0m  [6/26], [94mLoss[0m : 1.43633
[1mStep[0m  [8/26], [94mLoss[0m : 1.40834
[1mStep[0m  [10/26], [94mLoss[0m : 1.38062
[1mStep[0m  [12/26], [94mLoss[0m : 1.36444
[1mStep[0m  [14/26], [94mLoss[0m : 1.41775
[1mStep[0m  [16/26], [94mLoss[0m : 1.40548
[1mStep[0m  [18/26], [94mLoss[0m : 1.47704
[1mStep[0m  [20/26], [94mLoss[0m : 1.27369
[1mStep[0m  [22/26], [94mLoss[0m : 1.48233
[1mStep[0m  [24/26], [94mLoss[0m : 1.37570

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.390, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.21795
[1mStep[0m  [2/26], [94mLoss[0m : 1.37082
[1mStep[0m  [4/26], [94mLoss[0m : 1.35110
[1mStep[0m  [6/26], [94mLoss[0m : 1.33379
[1mStep[0m  [8/26], [94mLoss[0m : 1.15266
[1mStep[0m  [10/26], [94mLoss[0m : 1.35109
[1mStep[0m  [12/26], [94mLoss[0m : 1.44918
[1mStep[0m  [14/26], [94mLoss[0m : 1.41870
[1mStep[0m  [16/26], [94mLoss[0m : 1.32711
[1mStep[0m  [18/26], [94mLoss[0m : 1.44713
[1mStep[0m  [20/26], [94mLoss[0m : 1.32185
[1mStep[0m  [22/26], [94mLoss[0m : 1.40583
[1mStep[0m  [24/26], [94mLoss[0m : 1.35698

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.356, [92mTest[0m: 2.544, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.508
====================================

Phase 2 - Evaluation MAE:  2.507576263867892
MAE score P1      2.393027
MAE score P2      2.507576
loss              1.355981
learning_rate         0.01
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 11.26219
[1mStep[0m  [2/26], [94mLoss[0m : 10.81225
[1mStep[0m  [4/26], [94mLoss[0m : 10.71931
[1mStep[0m  [6/26], [94mLoss[0m : 11.00360
[1mStep[0m  [8/26], [94mLoss[0m : 10.77262
[1mStep[0m  [10/26], [94mLoss[0m : 10.66299
[1mStep[0m  [12/26], [94mLoss[0m : 10.88387
[1mStep[0m  [14/26], [94mLoss[0m : 10.49836
[1mStep[0m  [16/26], [94mLoss[0m : 10.60702
[1mStep[0m  [18/26], [94mLoss[0m : 10.60020
[1mStep[0m  [20/26], [94mLoss[0m : 10.89034
[1mStep[0m  [22/26], [94mLoss[0m : 10.74463
[1mStep[0m  [24/26], [94mLoss[0m : 10.69290

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.748, [92mTest[0m: 10.919, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.51577
[1mStep[0m  [2/26], [94mLoss[0m : 10.57967
[1mStep[0m  [4/26], [94mLoss[0m : 10.34210
[1mStep[0m  [6/26], [94mLoss[0m : 10.41681
[1mStep[0m  [8/26], [94mLoss[0m : 10.43486
[1mStep[0m  [10/26], [94mLoss[0m : 10.08037
[1mStep[0m  [12/26], [94mLoss[0m : 10.29300
[1mStep[0m  [14/26], [94mLoss[0m : 10.41890
[1mStep[0m  [16/26], [94mLoss[0m : 10.39007
[1mStep[0m  [18/26], [94mLoss[0m : 10.27712
[1mStep[0m  [20/26], [94mLoss[0m : 10.08168
[1mStep[0m  [22/26], [94mLoss[0m : 10.09397
[1mStep[0m  [24/26], [94mLoss[0m : 10.18900

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.291, [92mTest[0m: 10.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.11321
[1mStep[0m  [2/26], [94mLoss[0m : 9.85648
[1mStep[0m  [4/26], [94mLoss[0m : 9.93411
[1mStep[0m  [6/26], [94mLoss[0m : 9.90613
[1mStep[0m  [8/26], [94mLoss[0m : 9.89320
[1mStep[0m  [10/26], [94mLoss[0m : 9.71947
[1mStep[0m  [12/26], [94mLoss[0m : 9.77974
[1mStep[0m  [14/26], [94mLoss[0m : 9.70491
[1mStep[0m  [16/26], [94mLoss[0m : 9.77838
[1mStep[0m  [18/26], [94mLoss[0m : 10.01329
[1mStep[0m  [20/26], [94mLoss[0m : 9.52671
[1mStep[0m  [22/26], [94mLoss[0m : 9.42272
[1mStep[0m  [24/26], [94mLoss[0m : 9.64216

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.793, [92mTest[0m: 9.915, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.69958
[1mStep[0m  [2/26], [94mLoss[0m : 9.45948
[1mStep[0m  [4/26], [94mLoss[0m : 9.32159
[1mStep[0m  [6/26], [94mLoss[0m : 9.73991
[1mStep[0m  [8/26], [94mLoss[0m : 9.31078
[1mStep[0m  [10/26], [94mLoss[0m : 9.47567
[1mStep[0m  [12/26], [94mLoss[0m : 9.22455
[1mStep[0m  [14/26], [94mLoss[0m : 9.19669
[1mStep[0m  [16/26], [94mLoss[0m : 8.79552
[1mStep[0m  [18/26], [94mLoss[0m : 8.78403
[1mStep[0m  [20/26], [94mLoss[0m : 9.13294
[1mStep[0m  [22/26], [94mLoss[0m : 8.86365
[1mStep[0m  [24/26], [94mLoss[0m : 9.13255

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.191, [92mTest[0m: 9.280, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.60408
[1mStep[0m  [2/26], [94mLoss[0m : 9.06284
[1mStep[0m  [4/26], [94mLoss[0m : 8.82637
[1mStep[0m  [6/26], [94mLoss[0m : 8.82464
[1mStep[0m  [8/26], [94mLoss[0m : 8.56202
[1mStep[0m  [10/26], [94mLoss[0m : 8.50077
[1mStep[0m  [12/26], [94mLoss[0m : 8.38941
[1mStep[0m  [14/26], [94mLoss[0m : 8.22558
[1mStep[0m  [16/26], [94mLoss[0m : 8.50371
[1mStep[0m  [18/26], [94mLoss[0m : 8.25883
[1mStep[0m  [20/26], [94mLoss[0m : 7.94519
[1mStep[0m  [22/26], [94mLoss[0m : 8.09136
[1mStep[0m  [24/26], [94mLoss[0m : 8.13467

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.465, [92mTest[0m: 8.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.93971
[1mStep[0m  [2/26], [94mLoss[0m : 7.85607
[1mStep[0m  [4/26], [94mLoss[0m : 7.85178
[1mStep[0m  [6/26], [94mLoss[0m : 7.71329
[1mStep[0m  [8/26], [94mLoss[0m : 7.49649
[1mStep[0m  [10/26], [94mLoss[0m : 7.71428
[1mStep[0m  [12/26], [94mLoss[0m : 7.58354
[1mStep[0m  [14/26], [94mLoss[0m : 7.57067
[1mStep[0m  [16/26], [94mLoss[0m : 7.22084
[1mStep[0m  [18/26], [94mLoss[0m : 7.45925
[1mStep[0m  [20/26], [94mLoss[0m : 7.52431
[1mStep[0m  [22/26], [94mLoss[0m : 7.60308
[1mStep[0m  [24/26], [94mLoss[0m : 7.46085

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.634, [92mTest[0m: 7.589, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.27704
[1mStep[0m  [2/26], [94mLoss[0m : 7.23760
[1mStep[0m  [4/26], [94mLoss[0m : 7.10407
[1mStep[0m  [6/26], [94mLoss[0m : 6.92049
[1mStep[0m  [8/26], [94mLoss[0m : 6.72954
[1mStep[0m  [10/26], [94mLoss[0m : 6.83760
[1mStep[0m  [12/26], [94mLoss[0m : 7.31691
[1mStep[0m  [14/26], [94mLoss[0m : 6.84529
[1mStep[0m  [16/26], [94mLoss[0m : 6.60982
[1mStep[0m  [18/26], [94mLoss[0m : 6.74134
[1mStep[0m  [20/26], [94mLoss[0m : 6.83686
[1mStep[0m  [22/26], [94mLoss[0m : 6.77690
[1mStep[0m  [24/26], [94mLoss[0m : 6.60682

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.883, [92mTest[0m: 6.682, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.42432
[1mStep[0m  [2/26], [94mLoss[0m : 6.38494
[1mStep[0m  [4/26], [94mLoss[0m : 6.49245
[1mStep[0m  [6/26], [94mLoss[0m : 6.39069
[1mStep[0m  [8/26], [94mLoss[0m : 6.38150
[1mStep[0m  [10/26], [94mLoss[0m : 6.22594
[1mStep[0m  [12/26], [94mLoss[0m : 6.64294
[1mStep[0m  [14/26], [94mLoss[0m : 6.21877
[1mStep[0m  [16/26], [94mLoss[0m : 6.19946
[1mStep[0m  [18/26], [94mLoss[0m : 5.99870
[1mStep[0m  [20/26], [94mLoss[0m : 5.75608
[1mStep[0m  [22/26], [94mLoss[0m : 6.22726
[1mStep[0m  [24/26], [94mLoss[0m : 6.16828

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.277, [92mTest[0m: 5.944, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.10870
[1mStep[0m  [2/26], [94mLoss[0m : 5.92902
[1mStep[0m  [4/26], [94mLoss[0m : 5.85135
[1mStep[0m  [6/26], [94mLoss[0m : 6.17517
[1mStep[0m  [8/26], [94mLoss[0m : 5.85095
[1mStep[0m  [10/26], [94mLoss[0m : 5.94044
[1mStep[0m  [12/26], [94mLoss[0m : 5.82783
[1mStep[0m  [14/26], [94mLoss[0m : 5.41260
[1mStep[0m  [16/26], [94mLoss[0m : 5.55860
[1mStep[0m  [18/26], [94mLoss[0m : 5.52896
[1mStep[0m  [20/26], [94mLoss[0m : 6.01127
[1mStep[0m  [22/26], [94mLoss[0m : 5.85723
[1mStep[0m  [24/26], [94mLoss[0m : 5.51068

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.755, [92mTest[0m: 5.220, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.25611
[1mStep[0m  [2/26], [94mLoss[0m : 5.30302
[1mStep[0m  [4/26], [94mLoss[0m : 5.22315
[1mStep[0m  [6/26], [94mLoss[0m : 5.45756
[1mStep[0m  [8/26], [94mLoss[0m : 5.38207
[1mStep[0m  [10/26], [94mLoss[0m : 5.36254
[1mStep[0m  [12/26], [94mLoss[0m : 5.55373
[1mStep[0m  [14/26], [94mLoss[0m : 5.31178
[1mStep[0m  [16/26], [94mLoss[0m : 5.29908
[1mStep[0m  [18/26], [94mLoss[0m : 4.94464
[1mStep[0m  [20/26], [94mLoss[0m : 4.97696
[1mStep[0m  [22/26], [94mLoss[0m : 5.09504
[1mStep[0m  [24/26], [94mLoss[0m : 5.13755

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.233, [92mTest[0m: 4.603, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.90383
[1mStep[0m  [2/26], [94mLoss[0m : 4.86126
[1mStep[0m  [4/26], [94mLoss[0m : 5.03602
[1mStep[0m  [6/26], [94mLoss[0m : 4.73698
[1mStep[0m  [8/26], [94mLoss[0m : 4.74789
[1mStep[0m  [10/26], [94mLoss[0m : 4.88036
[1mStep[0m  [12/26], [94mLoss[0m : 4.68593
[1mStep[0m  [14/26], [94mLoss[0m : 4.56281
[1mStep[0m  [16/26], [94mLoss[0m : 4.62038
[1mStep[0m  [18/26], [94mLoss[0m : 4.92415
[1mStep[0m  [20/26], [94mLoss[0m : 4.47319
[1mStep[0m  [22/26], [94mLoss[0m : 4.39664
[1mStep[0m  [24/26], [94mLoss[0m : 4.23996

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.648, [92mTest[0m: 4.112, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.46381
[1mStep[0m  [2/26], [94mLoss[0m : 4.15732
[1mStep[0m  [4/26], [94mLoss[0m : 4.57792
[1mStep[0m  [6/26], [94mLoss[0m : 4.25276
[1mStep[0m  [8/26], [94mLoss[0m : 4.23782
[1mStep[0m  [10/26], [94mLoss[0m : 4.24921
[1mStep[0m  [12/26], [94mLoss[0m : 3.85170
[1mStep[0m  [14/26], [94mLoss[0m : 4.05427
[1mStep[0m  [16/26], [94mLoss[0m : 4.13927
[1mStep[0m  [18/26], [94mLoss[0m : 4.00290
[1mStep[0m  [20/26], [94mLoss[0m : 3.90206
[1mStep[0m  [22/26], [94mLoss[0m : 3.82940
[1mStep[0m  [24/26], [94mLoss[0m : 4.05525

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.096, [92mTest[0m: 3.671, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.04717
[1mStep[0m  [2/26], [94mLoss[0m : 3.76597
[1mStep[0m  [4/26], [94mLoss[0m : 3.80990
[1mStep[0m  [6/26], [94mLoss[0m : 4.09893
[1mStep[0m  [8/26], [94mLoss[0m : 3.60928
[1mStep[0m  [10/26], [94mLoss[0m : 3.68066
[1mStep[0m  [12/26], [94mLoss[0m : 3.58720
[1mStep[0m  [14/26], [94mLoss[0m : 3.47958
[1mStep[0m  [16/26], [94mLoss[0m : 3.75728
[1mStep[0m  [18/26], [94mLoss[0m : 3.49607
[1mStep[0m  [20/26], [94mLoss[0m : 3.45756
[1mStep[0m  [22/26], [94mLoss[0m : 3.49358
[1mStep[0m  [24/26], [94mLoss[0m : 3.42950

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.628, [92mTest[0m: 3.267, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.42832
[1mStep[0m  [2/26], [94mLoss[0m : 3.24377
[1mStep[0m  [4/26], [94mLoss[0m : 3.37427
[1mStep[0m  [6/26], [94mLoss[0m : 3.35169
[1mStep[0m  [8/26], [94mLoss[0m : 3.10851
[1mStep[0m  [10/26], [94mLoss[0m : 3.38873
[1mStep[0m  [12/26], [94mLoss[0m : 3.16798
[1mStep[0m  [14/26], [94mLoss[0m : 3.29709
[1mStep[0m  [16/26], [94mLoss[0m : 3.03016
[1mStep[0m  [18/26], [94mLoss[0m : 3.28557
[1mStep[0m  [20/26], [94mLoss[0m : 3.24864
[1mStep[0m  [22/26], [94mLoss[0m : 3.13180
[1mStep[0m  [24/26], [94mLoss[0m : 3.13853

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.201, [92mTest[0m: 2.901, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.91654
[1mStep[0m  [2/26], [94mLoss[0m : 2.98234
[1mStep[0m  [4/26], [94mLoss[0m : 2.75612
[1mStep[0m  [6/26], [94mLoss[0m : 2.97316
[1mStep[0m  [8/26], [94mLoss[0m : 2.88998
[1mStep[0m  [10/26], [94mLoss[0m : 2.75766
[1mStep[0m  [12/26], [94mLoss[0m : 2.94086
[1mStep[0m  [14/26], [94mLoss[0m : 2.89458
[1mStep[0m  [16/26], [94mLoss[0m : 2.98300
[1mStep[0m  [18/26], [94mLoss[0m : 2.79510
[1mStep[0m  [20/26], [94mLoss[0m : 2.98384
[1mStep[0m  [22/26], [94mLoss[0m : 2.62910
[1mStep[0m  [24/26], [94mLoss[0m : 2.89675

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.905, [92mTest[0m: 2.683, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.95056
[1mStep[0m  [2/26], [94mLoss[0m : 2.72782
[1mStep[0m  [4/26], [94mLoss[0m : 2.83574
[1mStep[0m  [6/26], [94mLoss[0m : 2.88240
[1mStep[0m  [8/26], [94mLoss[0m : 2.80014
[1mStep[0m  [10/26], [94mLoss[0m : 2.68854
[1mStep[0m  [12/26], [94mLoss[0m : 2.80982
[1mStep[0m  [14/26], [94mLoss[0m : 2.69033
[1mStep[0m  [16/26], [94mLoss[0m : 2.82473
[1mStep[0m  [18/26], [94mLoss[0m : 2.64049
[1mStep[0m  [20/26], [94mLoss[0m : 2.71711
[1mStep[0m  [22/26], [94mLoss[0m : 2.78850
[1mStep[0m  [24/26], [94mLoss[0m : 2.79664

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.779, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.86968
[1mStep[0m  [2/26], [94mLoss[0m : 2.81607
[1mStep[0m  [4/26], [94mLoss[0m : 2.64163
[1mStep[0m  [6/26], [94mLoss[0m : 2.77588
[1mStep[0m  [8/26], [94mLoss[0m : 2.70635
[1mStep[0m  [10/26], [94mLoss[0m : 2.92472
[1mStep[0m  [12/26], [94mLoss[0m : 2.76619
[1mStep[0m  [14/26], [94mLoss[0m : 2.74791
[1mStep[0m  [16/26], [94mLoss[0m : 2.75907
[1mStep[0m  [18/26], [94mLoss[0m : 2.68428
[1mStep[0m  [20/26], [94mLoss[0m : 2.64691
[1mStep[0m  [22/26], [94mLoss[0m : 2.70592
[1mStep[0m  [24/26], [94mLoss[0m : 2.65530

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.717, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74784
[1mStep[0m  [2/26], [94mLoss[0m : 2.65923
[1mStep[0m  [4/26], [94mLoss[0m : 2.60518
[1mStep[0m  [6/26], [94mLoss[0m : 2.59519
[1mStep[0m  [8/26], [94mLoss[0m : 2.70350
[1mStep[0m  [10/26], [94mLoss[0m : 2.53167
[1mStep[0m  [12/26], [94mLoss[0m : 2.49281
[1mStep[0m  [14/26], [94mLoss[0m : 2.59554
[1mStep[0m  [16/26], [94mLoss[0m : 2.60244
[1mStep[0m  [18/26], [94mLoss[0m : 2.69127
[1mStep[0m  [20/26], [94mLoss[0m : 2.67106
[1mStep[0m  [22/26], [94mLoss[0m : 2.86440
[1mStep[0m  [24/26], [94mLoss[0m : 2.72132

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56980
[1mStep[0m  [2/26], [94mLoss[0m : 2.69028
[1mStep[0m  [4/26], [94mLoss[0m : 2.73887
[1mStep[0m  [6/26], [94mLoss[0m : 2.72001
[1mStep[0m  [8/26], [94mLoss[0m : 2.60612
[1mStep[0m  [10/26], [94mLoss[0m : 2.45537
[1mStep[0m  [12/26], [94mLoss[0m : 2.57331
[1mStep[0m  [14/26], [94mLoss[0m : 2.66197
[1mStep[0m  [16/26], [94mLoss[0m : 2.76484
[1mStep[0m  [18/26], [94mLoss[0m : 2.79508
[1mStep[0m  [20/26], [94mLoss[0m : 2.72290
[1mStep[0m  [22/26], [94mLoss[0m : 2.58889
[1mStep[0m  [24/26], [94mLoss[0m : 2.57754

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64436
[1mStep[0m  [2/26], [94mLoss[0m : 2.54039
[1mStep[0m  [4/26], [94mLoss[0m : 2.60339
[1mStep[0m  [6/26], [94mLoss[0m : 2.44240
[1mStep[0m  [8/26], [94mLoss[0m : 2.67680
[1mStep[0m  [10/26], [94mLoss[0m : 2.72109
[1mStep[0m  [12/26], [94mLoss[0m : 2.55000
[1mStep[0m  [14/26], [94mLoss[0m : 2.74120
[1mStep[0m  [16/26], [94mLoss[0m : 2.67353
[1mStep[0m  [18/26], [94mLoss[0m : 2.66351
[1mStep[0m  [20/26], [94mLoss[0m : 2.54940
[1mStep[0m  [22/26], [94mLoss[0m : 2.68246
[1mStep[0m  [24/26], [94mLoss[0m : 2.62261

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.430, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56468
[1mStep[0m  [2/26], [94mLoss[0m : 2.72560
[1mStep[0m  [4/26], [94mLoss[0m : 2.58690
[1mStep[0m  [6/26], [94mLoss[0m : 2.70787
[1mStep[0m  [8/26], [94mLoss[0m : 2.64496
[1mStep[0m  [10/26], [94mLoss[0m : 2.58495
[1mStep[0m  [12/26], [94mLoss[0m : 2.65995
[1mStep[0m  [14/26], [94mLoss[0m : 2.59635
[1mStep[0m  [16/26], [94mLoss[0m : 2.54746
[1mStep[0m  [18/26], [94mLoss[0m : 2.55660
[1mStep[0m  [20/26], [94mLoss[0m : 2.60269
[1mStep[0m  [22/26], [94mLoss[0m : 2.77646
[1mStep[0m  [24/26], [94mLoss[0m : 2.64477

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.434, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67230
[1mStep[0m  [2/26], [94mLoss[0m : 2.64410
[1mStep[0m  [4/26], [94mLoss[0m : 2.63276
[1mStep[0m  [6/26], [94mLoss[0m : 2.63968
[1mStep[0m  [8/26], [94mLoss[0m : 2.61801
[1mStep[0m  [10/26], [94mLoss[0m : 2.68119
[1mStep[0m  [12/26], [94mLoss[0m : 2.68815
[1mStep[0m  [14/26], [94mLoss[0m : 2.48301
[1mStep[0m  [16/26], [94mLoss[0m : 2.78217
[1mStep[0m  [18/26], [94mLoss[0m : 2.63076
[1mStep[0m  [20/26], [94mLoss[0m : 2.60089
[1mStep[0m  [22/26], [94mLoss[0m : 2.65763
[1mStep[0m  [24/26], [94mLoss[0m : 2.41319

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56155
[1mStep[0m  [2/26], [94mLoss[0m : 2.54213
[1mStep[0m  [4/26], [94mLoss[0m : 2.65737
[1mStep[0m  [6/26], [94mLoss[0m : 2.58703
[1mStep[0m  [8/26], [94mLoss[0m : 2.57237
[1mStep[0m  [10/26], [94mLoss[0m : 2.66034
[1mStep[0m  [12/26], [94mLoss[0m : 2.63133
[1mStep[0m  [14/26], [94mLoss[0m : 2.51975
[1mStep[0m  [16/26], [94mLoss[0m : 2.58810
[1mStep[0m  [18/26], [94mLoss[0m : 2.52945
[1mStep[0m  [20/26], [94mLoss[0m : 2.55709
[1mStep[0m  [22/26], [94mLoss[0m : 2.83232
[1mStep[0m  [24/26], [94mLoss[0m : 2.48655

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76239
[1mStep[0m  [2/26], [94mLoss[0m : 2.67831
[1mStep[0m  [4/26], [94mLoss[0m : 2.67079
[1mStep[0m  [6/26], [94mLoss[0m : 2.57727
[1mStep[0m  [8/26], [94mLoss[0m : 2.49406
[1mStep[0m  [10/26], [94mLoss[0m : 2.49316
[1mStep[0m  [12/26], [94mLoss[0m : 2.58802
[1mStep[0m  [14/26], [94mLoss[0m : 2.63199
[1mStep[0m  [16/26], [94mLoss[0m : 2.65276
[1mStep[0m  [18/26], [94mLoss[0m : 2.78604
[1mStep[0m  [20/26], [94mLoss[0m : 2.62529
[1mStep[0m  [22/26], [94mLoss[0m : 2.58230
[1mStep[0m  [24/26], [94mLoss[0m : 2.80041

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.413, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58807
[1mStep[0m  [2/26], [94mLoss[0m : 2.58368
[1mStep[0m  [4/26], [94mLoss[0m : 2.61572
[1mStep[0m  [6/26], [94mLoss[0m : 2.56367
[1mStep[0m  [8/26], [94mLoss[0m : 2.49354
[1mStep[0m  [10/26], [94mLoss[0m : 2.62715
[1mStep[0m  [12/26], [94mLoss[0m : 2.69349
[1mStep[0m  [14/26], [94mLoss[0m : 2.48982
[1mStep[0m  [16/26], [94mLoss[0m : 2.61147
[1mStep[0m  [18/26], [94mLoss[0m : 2.54175
[1mStep[0m  [20/26], [94mLoss[0m : 2.53068
[1mStep[0m  [22/26], [94mLoss[0m : 2.65752
[1mStep[0m  [24/26], [94mLoss[0m : 2.75057

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.416, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52925
[1mStep[0m  [2/26], [94mLoss[0m : 2.56668
[1mStep[0m  [4/26], [94mLoss[0m : 2.76460
[1mStep[0m  [6/26], [94mLoss[0m : 2.61408
[1mStep[0m  [8/26], [94mLoss[0m : 2.58246
[1mStep[0m  [10/26], [94mLoss[0m : 2.55350
[1mStep[0m  [12/26], [94mLoss[0m : 2.48929
[1mStep[0m  [14/26], [94mLoss[0m : 2.47675
[1mStep[0m  [16/26], [94mLoss[0m : 2.56361
[1mStep[0m  [18/26], [94mLoss[0m : 2.73559
[1mStep[0m  [20/26], [94mLoss[0m : 2.64189
[1mStep[0m  [22/26], [94mLoss[0m : 2.60934
[1mStep[0m  [24/26], [94mLoss[0m : 2.56402

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.407, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52189
[1mStep[0m  [2/26], [94mLoss[0m : 2.66892
[1mStep[0m  [4/26], [94mLoss[0m : 2.50122
[1mStep[0m  [6/26], [94mLoss[0m : 2.57245
[1mStep[0m  [8/26], [94mLoss[0m : 2.56705
[1mStep[0m  [10/26], [94mLoss[0m : 2.47888
[1mStep[0m  [12/26], [94mLoss[0m : 2.70955
[1mStep[0m  [14/26], [94mLoss[0m : 2.57007
[1mStep[0m  [16/26], [94mLoss[0m : 2.58233
[1mStep[0m  [18/26], [94mLoss[0m : 2.60374
[1mStep[0m  [20/26], [94mLoss[0m : 2.59844
[1mStep[0m  [22/26], [94mLoss[0m : 2.57081
[1mStep[0m  [24/26], [94mLoss[0m : 2.66089

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.406, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53893
[1mStep[0m  [2/26], [94mLoss[0m : 2.48584
[1mStep[0m  [4/26], [94mLoss[0m : 2.65042
[1mStep[0m  [6/26], [94mLoss[0m : 2.52813
[1mStep[0m  [8/26], [94mLoss[0m : 2.55337
[1mStep[0m  [10/26], [94mLoss[0m : 2.60707
[1mStep[0m  [12/26], [94mLoss[0m : 2.72406
[1mStep[0m  [14/26], [94mLoss[0m : 2.53842
[1mStep[0m  [16/26], [94mLoss[0m : 2.65593
[1mStep[0m  [18/26], [94mLoss[0m : 2.48712
[1mStep[0m  [20/26], [94mLoss[0m : 2.44047
[1mStep[0m  [22/26], [94mLoss[0m : 2.69540
[1mStep[0m  [24/26], [94mLoss[0m : 2.71616

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.409, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67876
[1mStep[0m  [2/26], [94mLoss[0m : 2.70382
[1mStep[0m  [4/26], [94mLoss[0m : 2.53446
[1mStep[0m  [6/26], [94mLoss[0m : 2.62698
[1mStep[0m  [8/26], [94mLoss[0m : 2.49996
[1mStep[0m  [10/26], [94mLoss[0m : 2.51321
[1mStep[0m  [12/26], [94mLoss[0m : 2.43932
[1mStep[0m  [14/26], [94mLoss[0m : 2.62660
[1mStep[0m  [16/26], [94mLoss[0m : 2.50673
[1mStep[0m  [18/26], [94mLoss[0m : 2.63899
[1mStep[0m  [20/26], [94mLoss[0m : 2.73776
[1mStep[0m  [22/26], [94mLoss[0m : 2.63096
[1mStep[0m  [24/26], [94mLoss[0m : 2.40524

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.400, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48845
[1mStep[0m  [2/26], [94mLoss[0m : 2.54008
[1mStep[0m  [4/26], [94mLoss[0m : 2.66378
[1mStep[0m  [6/26], [94mLoss[0m : 2.46243
[1mStep[0m  [8/26], [94mLoss[0m : 2.51181
[1mStep[0m  [10/26], [94mLoss[0m : 2.52720
[1mStep[0m  [12/26], [94mLoss[0m : 2.58521
[1mStep[0m  [14/26], [94mLoss[0m : 2.64127
[1mStep[0m  [16/26], [94mLoss[0m : 2.60811
[1mStep[0m  [18/26], [94mLoss[0m : 2.63859
[1mStep[0m  [20/26], [94mLoss[0m : 2.58084
[1mStep[0m  [22/26], [94mLoss[0m : 2.67404
[1mStep[0m  [24/26], [94mLoss[0m : 2.64060

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.404, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 1 - Evaluation MAE:  2.396690772129939
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.42725
[1mStep[0m  [2/26], [94mLoss[0m : 2.55967
[1mStep[0m  [4/26], [94mLoss[0m : 2.70325
[1mStep[0m  [6/26], [94mLoss[0m : 2.68261
[1mStep[0m  [8/26], [94mLoss[0m : 2.56252
[1mStep[0m  [10/26], [94mLoss[0m : 2.65789
[1mStep[0m  [12/26], [94mLoss[0m : 2.71500
[1mStep[0m  [14/26], [94mLoss[0m : 2.56167
[1mStep[0m  [16/26], [94mLoss[0m : 2.54335
[1mStep[0m  [18/26], [94mLoss[0m : 2.52083
[1mStep[0m  [20/26], [94mLoss[0m : 2.56768
[1mStep[0m  [22/26], [94mLoss[0m : 2.60047
[1mStep[0m  [24/26], [94mLoss[0m : 2.59376

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55271
[1mStep[0m  [2/26], [94mLoss[0m : 2.63420
[1mStep[0m  [4/26], [94mLoss[0m : 2.55631
[1mStep[0m  [6/26], [94mLoss[0m : 2.63595
[1mStep[0m  [8/26], [94mLoss[0m : 2.50259
[1mStep[0m  [10/26], [94mLoss[0m : 2.55714
[1mStep[0m  [12/26], [94mLoss[0m : 2.62288
[1mStep[0m  [14/26], [94mLoss[0m : 2.73379
[1mStep[0m  [16/26], [94mLoss[0m : 2.55143
[1mStep[0m  [18/26], [94mLoss[0m : 2.64635
[1mStep[0m  [20/26], [94mLoss[0m : 2.68104
[1mStep[0m  [22/26], [94mLoss[0m : 2.51443
[1mStep[0m  [24/26], [94mLoss[0m : 2.66263

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71586
[1mStep[0m  [2/26], [94mLoss[0m : 2.58524
[1mStep[0m  [4/26], [94mLoss[0m : 2.67106
[1mStep[0m  [6/26], [94mLoss[0m : 2.47537
[1mStep[0m  [8/26], [94mLoss[0m : 2.44549
[1mStep[0m  [10/26], [94mLoss[0m : 2.52492
[1mStep[0m  [12/26], [94mLoss[0m : 2.50681
[1mStep[0m  [14/26], [94mLoss[0m : 2.58815
[1mStep[0m  [16/26], [94mLoss[0m : 2.39351
[1mStep[0m  [18/26], [94mLoss[0m : 2.51128
[1mStep[0m  [20/26], [94mLoss[0m : 2.68915
[1mStep[0m  [22/26], [94mLoss[0m : 2.50126
[1mStep[0m  [24/26], [94mLoss[0m : 2.45011

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59125
[1mStep[0m  [2/26], [94mLoss[0m : 2.49414
[1mStep[0m  [4/26], [94mLoss[0m : 2.72409
[1mStep[0m  [6/26], [94mLoss[0m : 2.66104
[1mStep[0m  [8/26], [94mLoss[0m : 2.49580
[1mStep[0m  [10/26], [94mLoss[0m : 2.36203
[1mStep[0m  [12/26], [94mLoss[0m : 2.56965
[1mStep[0m  [14/26], [94mLoss[0m : 2.57371
[1mStep[0m  [16/26], [94mLoss[0m : 2.58891
[1mStep[0m  [18/26], [94mLoss[0m : 2.49596
[1mStep[0m  [20/26], [94mLoss[0m : 2.49332
[1mStep[0m  [22/26], [94mLoss[0m : 2.65444
[1mStep[0m  [24/26], [94mLoss[0m : 2.80486

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39500
[1mStep[0m  [2/26], [94mLoss[0m : 2.54236
[1mStep[0m  [4/26], [94mLoss[0m : 2.59681
[1mStep[0m  [6/26], [94mLoss[0m : 2.33682
[1mStep[0m  [8/26], [94mLoss[0m : 2.57674
[1mStep[0m  [10/26], [94mLoss[0m : 2.59919
[1mStep[0m  [12/26], [94mLoss[0m : 2.54668
[1mStep[0m  [14/26], [94mLoss[0m : 2.64661
[1mStep[0m  [16/26], [94mLoss[0m : 2.61747
[1mStep[0m  [18/26], [94mLoss[0m : 2.48152
[1mStep[0m  [20/26], [94mLoss[0m : 2.48103
[1mStep[0m  [22/26], [94mLoss[0m : 2.41824
[1mStep[0m  [24/26], [94mLoss[0m : 2.54408

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46046
[1mStep[0m  [2/26], [94mLoss[0m : 2.60009
[1mStep[0m  [4/26], [94mLoss[0m : 2.49607
[1mStep[0m  [6/26], [94mLoss[0m : 2.50993
[1mStep[0m  [8/26], [94mLoss[0m : 2.50094
[1mStep[0m  [10/26], [94mLoss[0m : 2.44198
[1mStep[0m  [12/26], [94mLoss[0m : 2.51317
[1mStep[0m  [14/26], [94mLoss[0m : 2.47635
[1mStep[0m  [16/26], [94mLoss[0m : 2.51506
[1mStep[0m  [18/26], [94mLoss[0m : 2.45040
[1mStep[0m  [20/26], [94mLoss[0m : 2.45536
[1mStep[0m  [22/26], [94mLoss[0m : 2.44217
[1mStep[0m  [24/26], [94mLoss[0m : 2.33207

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44294
[1mStep[0m  [2/26], [94mLoss[0m : 2.33182
[1mStep[0m  [4/26], [94mLoss[0m : 2.42746
[1mStep[0m  [6/26], [94mLoss[0m : 2.51564
[1mStep[0m  [8/26], [94mLoss[0m : 2.37018
[1mStep[0m  [10/26], [94mLoss[0m : 2.57605
[1mStep[0m  [12/26], [94mLoss[0m : 2.57044
[1mStep[0m  [14/26], [94mLoss[0m : 2.49499
[1mStep[0m  [16/26], [94mLoss[0m : 2.36189
[1mStep[0m  [18/26], [94mLoss[0m : 2.39383
[1mStep[0m  [20/26], [94mLoss[0m : 2.43838
[1mStep[0m  [22/26], [94mLoss[0m : 2.47757
[1mStep[0m  [24/26], [94mLoss[0m : 2.53707

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67806
[1mStep[0m  [2/26], [94mLoss[0m : 2.46624
[1mStep[0m  [4/26], [94mLoss[0m : 2.42277
[1mStep[0m  [6/26], [94mLoss[0m : 2.44603
[1mStep[0m  [8/26], [94mLoss[0m : 2.46179
[1mStep[0m  [10/26], [94mLoss[0m : 2.36285
[1mStep[0m  [12/26], [94mLoss[0m : 2.27592
[1mStep[0m  [14/26], [94mLoss[0m : 2.39373
[1mStep[0m  [16/26], [94mLoss[0m : 2.41414
[1mStep[0m  [18/26], [94mLoss[0m : 2.39404
[1mStep[0m  [20/26], [94mLoss[0m : 2.40728
[1mStep[0m  [22/26], [94mLoss[0m : 2.39253
[1mStep[0m  [24/26], [94mLoss[0m : 2.36769

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39211
[1mStep[0m  [2/26], [94mLoss[0m : 2.35484
[1mStep[0m  [4/26], [94mLoss[0m : 2.52046
[1mStep[0m  [6/26], [94mLoss[0m : 2.32874
[1mStep[0m  [8/26], [94mLoss[0m : 2.39981
[1mStep[0m  [10/26], [94mLoss[0m : 2.33635
[1mStep[0m  [12/26], [94mLoss[0m : 2.29485
[1mStep[0m  [14/26], [94mLoss[0m : 2.34596
[1mStep[0m  [16/26], [94mLoss[0m : 2.45125
[1mStep[0m  [18/26], [94mLoss[0m : 2.32839
[1mStep[0m  [20/26], [94mLoss[0m : 2.38637
[1mStep[0m  [22/26], [94mLoss[0m : 2.34507
[1mStep[0m  [24/26], [94mLoss[0m : 2.19817

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31297
[1mStep[0m  [2/26], [94mLoss[0m : 2.32960
[1mStep[0m  [4/26], [94mLoss[0m : 2.50866
[1mStep[0m  [6/26], [94mLoss[0m : 2.49342
[1mStep[0m  [8/26], [94mLoss[0m : 2.21935
[1mStep[0m  [10/26], [94mLoss[0m : 2.39216
[1mStep[0m  [12/26], [94mLoss[0m : 2.36615
[1mStep[0m  [14/26], [94mLoss[0m : 2.57933
[1mStep[0m  [16/26], [94mLoss[0m : 2.35368
[1mStep[0m  [18/26], [94mLoss[0m : 2.57644
[1mStep[0m  [20/26], [94mLoss[0m : 2.27106
[1mStep[0m  [22/26], [94mLoss[0m : 2.41819
[1mStep[0m  [24/26], [94mLoss[0m : 2.34552

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46783
[1mStep[0m  [2/26], [94mLoss[0m : 2.28200
[1mStep[0m  [4/26], [94mLoss[0m : 2.47817
[1mStep[0m  [6/26], [94mLoss[0m : 2.32070
[1mStep[0m  [8/26], [94mLoss[0m : 2.30835
[1mStep[0m  [10/26], [94mLoss[0m : 2.45820
[1mStep[0m  [12/26], [94mLoss[0m : 2.31068
[1mStep[0m  [14/26], [94mLoss[0m : 2.34245
[1mStep[0m  [16/26], [94mLoss[0m : 2.38319
[1mStep[0m  [18/26], [94mLoss[0m : 2.39190
[1mStep[0m  [20/26], [94mLoss[0m : 2.29551
[1mStep[0m  [22/26], [94mLoss[0m : 2.37726
[1mStep[0m  [24/26], [94mLoss[0m : 2.38521

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43431
[1mStep[0m  [2/26], [94mLoss[0m : 2.27997
[1mStep[0m  [4/26], [94mLoss[0m : 2.20837
[1mStep[0m  [6/26], [94mLoss[0m : 2.33704
[1mStep[0m  [8/26], [94mLoss[0m : 2.42793
[1mStep[0m  [10/26], [94mLoss[0m : 2.18041
[1mStep[0m  [12/26], [94mLoss[0m : 2.37212
[1mStep[0m  [14/26], [94mLoss[0m : 2.25824
[1mStep[0m  [16/26], [94mLoss[0m : 2.20371
[1mStep[0m  [18/26], [94mLoss[0m : 2.24970
[1mStep[0m  [20/26], [94mLoss[0m : 2.32541
[1mStep[0m  [22/26], [94mLoss[0m : 2.27516
[1mStep[0m  [24/26], [94mLoss[0m : 2.18014

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45264
[1mStep[0m  [2/26], [94mLoss[0m : 2.23372
[1mStep[0m  [4/26], [94mLoss[0m : 2.15027
[1mStep[0m  [6/26], [94mLoss[0m : 2.33501
[1mStep[0m  [8/26], [94mLoss[0m : 2.18523
[1mStep[0m  [10/26], [94mLoss[0m : 2.07404
[1mStep[0m  [12/26], [94mLoss[0m : 2.19356
[1mStep[0m  [14/26], [94mLoss[0m : 2.33277
[1mStep[0m  [16/26], [94mLoss[0m : 2.26174
[1mStep[0m  [18/26], [94mLoss[0m : 2.33084
[1mStep[0m  [20/26], [94mLoss[0m : 2.54067
[1mStep[0m  [22/26], [94mLoss[0m : 2.41802
[1mStep[0m  [24/26], [94mLoss[0m : 2.32898

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37250
[1mStep[0m  [2/26], [94mLoss[0m : 2.17749
[1mStep[0m  [4/26], [94mLoss[0m : 2.18570
[1mStep[0m  [6/26], [94mLoss[0m : 2.28225
[1mStep[0m  [8/26], [94mLoss[0m : 2.29947
[1mStep[0m  [10/26], [94mLoss[0m : 2.25698
[1mStep[0m  [12/26], [94mLoss[0m : 2.32056
[1mStep[0m  [14/26], [94mLoss[0m : 2.16942
[1mStep[0m  [16/26], [94mLoss[0m : 2.23197
[1mStep[0m  [18/26], [94mLoss[0m : 2.20794
[1mStep[0m  [20/26], [94mLoss[0m : 2.05335
[1mStep[0m  [22/26], [94mLoss[0m : 2.39557
[1mStep[0m  [24/26], [94mLoss[0m : 2.15789

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.14010
[1mStep[0m  [2/26], [94mLoss[0m : 2.27884
[1mStep[0m  [4/26], [94mLoss[0m : 2.24258
[1mStep[0m  [6/26], [94mLoss[0m : 2.24770
[1mStep[0m  [8/26], [94mLoss[0m : 2.19983
[1mStep[0m  [10/26], [94mLoss[0m : 2.29284
[1mStep[0m  [12/26], [94mLoss[0m : 2.16936
[1mStep[0m  [14/26], [94mLoss[0m : 2.06572
[1mStep[0m  [16/26], [94mLoss[0m : 2.23314
[1mStep[0m  [18/26], [94mLoss[0m : 2.21318
[1mStep[0m  [20/26], [94mLoss[0m : 2.19930
[1mStep[0m  [22/26], [94mLoss[0m : 2.25112
[1mStep[0m  [24/26], [94mLoss[0m : 2.28769

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09749
[1mStep[0m  [2/26], [94mLoss[0m : 2.47167
[1mStep[0m  [4/26], [94mLoss[0m : 2.12916
[1mStep[0m  [6/26], [94mLoss[0m : 2.05203
[1mStep[0m  [8/26], [94mLoss[0m : 2.24643
[1mStep[0m  [10/26], [94mLoss[0m : 2.17339
[1mStep[0m  [12/26], [94mLoss[0m : 2.24951
[1mStep[0m  [14/26], [94mLoss[0m : 2.19549
[1mStep[0m  [16/26], [94mLoss[0m : 2.15806
[1mStep[0m  [18/26], [94mLoss[0m : 2.12390
[1mStep[0m  [20/26], [94mLoss[0m : 2.15535
[1mStep[0m  [22/26], [94mLoss[0m : 2.27595
[1mStep[0m  [24/26], [94mLoss[0m : 2.21152

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.10074
[1mStep[0m  [2/26], [94mLoss[0m : 2.04561
[1mStep[0m  [4/26], [94mLoss[0m : 2.17418
[1mStep[0m  [6/26], [94mLoss[0m : 2.29189
[1mStep[0m  [8/26], [94mLoss[0m : 2.20068
[1mStep[0m  [10/26], [94mLoss[0m : 2.19449
[1mStep[0m  [12/26], [94mLoss[0m : 1.96031
[1mStep[0m  [14/26], [94mLoss[0m : 2.12619
[1mStep[0m  [16/26], [94mLoss[0m : 2.18931
[1mStep[0m  [18/26], [94mLoss[0m : 2.29306
[1mStep[0m  [20/26], [94mLoss[0m : 2.08964
[1mStep[0m  [22/26], [94mLoss[0m : 2.13704
[1mStep[0m  [24/26], [94mLoss[0m : 2.30308

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.174, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09506
[1mStep[0m  [2/26], [94mLoss[0m : 2.02112
[1mStep[0m  [4/26], [94mLoss[0m : 2.20529
[1mStep[0m  [6/26], [94mLoss[0m : 2.12539
[1mStep[0m  [8/26], [94mLoss[0m : 2.03608
[1mStep[0m  [10/26], [94mLoss[0m : 2.18338
[1mStep[0m  [12/26], [94mLoss[0m : 2.13015
[1mStep[0m  [14/26], [94mLoss[0m : 2.05090
[1mStep[0m  [16/26], [94mLoss[0m : 2.13909
[1mStep[0m  [18/26], [94mLoss[0m : 2.02447
[1mStep[0m  [20/26], [94mLoss[0m : 2.22458
[1mStep[0m  [22/26], [94mLoss[0m : 2.07520
[1mStep[0m  [24/26], [94mLoss[0m : 2.22203

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08764
[1mStep[0m  [2/26], [94mLoss[0m : 2.21623
[1mStep[0m  [4/26], [94mLoss[0m : 2.18982
[1mStep[0m  [6/26], [94mLoss[0m : 2.15265
[1mStep[0m  [8/26], [94mLoss[0m : 2.23117
[1mStep[0m  [10/26], [94mLoss[0m : 2.15960
[1mStep[0m  [12/26], [94mLoss[0m : 1.99034
[1mStep[0m  [14/26], [94mLoss[0m : 2.14922
[1mStep[0m  [16/26], [94mLoss[0m : 2.22700
[1mStep[0m  [18/26], [94mLoss[0m : 2.20176
[1mStep[0m  [20/26], [94mLoss[0m : 2.16902
[1mStep[0m  [22/26], [94mLoss[0m : 2.03213
[1mStep[0m  [24/26], [94mLoss[0m : 2.08359

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.04075
[1mStep[0m  [2/26], [94mLoss[0m : 2.20190
[1mStep[0m  [4/26], [94mLoss[0m : 2.07889
[1mStep[0m  [6/26], [94mLoss[0m : 2.03970
[1mStep[0m  [8/26], [94mLoss[0m : 1.95870
[1mStep[0m  [10/26], [94mLoss[0m : 2.16571
[1mStep[0m  [12/26], [94mLoss[0m : 2.13054
[1mStep[0m  [14/26], [94mLoss[0m : 1.99672
[1mStep[0m  [16/26], [94mLoss[0m : 2.07962
[1mStep[0m  [18/26], [94mLoss[0m : 2.07273
[1mStep[0m  [20/26], [94mLoss[0m : 2.06913
[1mStep[0m  [22/26], [94mLoss[0m : 2.06101
[1mStep[0m  [24/26], [94mLoss[0m : 2.02188

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.479, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03634
[1mStep[0m  [2/26], [94mLoss[0m : 1.99979
[1mStep[0m  [4/26], [94mLoss[0m : 2.01104
[1mStep[0m  [6/26], [94mLoss[0m : 2.00188
[1mStep[0m  [8/26], [94mLoss[0m : 2.16436
[1mStep[0m  [10/26], [94mLoss[0m : 1.94971
[1mStep[0m  [12/26], [94mLoss[0m : 2.04842
[1mStep[0m  [14/26], [94mLoss[0m : 2.10263
[1mStep[0m  [16/26], [94mLoss[0m : 2.16345
[1mStep[0m  [18/26], [94mLoss[0m : 2.07923
[1mStep[0m  [20/26], [94mLoss[0m : 1.91838
[1mStep[0m  [22/26], [94mLoss[0m : 2.03126
[1mStep[0m  [24/26], [94mLoss[0m : 2.03555

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82792
[1mStep[0m  [2/26], [94mLoss[0m : 2.05247
[1mStep[0m  [4/26], [94mLoss[0m : 1.95842
[1mStep[0m  [6/26], [94mLoss[0m : 2.01943
[1mStep[0m  [8/26], [94mLoss[0m : 2.13512
[1mStep[0m  [10/26], [94mLoss[0m : 2.06702
[1mStep[0m  [12/26], [94mLoss[0m : 2.03464
[1mStep[0m  [14/26], [94mLoss[0m : 2.01292
[1mStep[0m  [16/26], [94mLoss[0m : 2.00483
[1mStep[0m  [18/26], [94mLoss[0m : 1.85465
[1mStep[0m  [20/26], [94mLoss[0m : 2.04419
[1mStep[0m  [22/26], [94mLoss[0m : 2.07886
[1mStep[0m  [24/26], [94mLoss[0m : 2.10253

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.91486
[1mStep[0m  [2/26], [94mLoss[0m : 2.02511
[1mStep[0m  [4/26], [94mLoss[0m : 1.91930
[1mStep[0m  [6/26], [94mLoss[0m : 1.93595
[1mStep[0m  [8/26], [94mLoss[0m : 2.02937
[1mStep[0m  [10/26], [94mLoss[0m : 2.02946
[1mStep[0m  [12/26], [94mLoss[0m : 2.00452
[1mStep[0m  [14/26], [94mLoss[0m : 1.85841
[1mStep[0m  [16/26], [94mLoss[0m : 1.92216
[1mStep[0m  [18/26], [94mLoss[0m : 2.03496
[1mStep[0m  [20/26], [94mLoss[0m : 2.17444
[1mStep[0m  [22/26], [94mLoss[0m : 1.88129
[1mStep[0m  [24/26], [94mLoss[0m : 2.11631

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.399, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03612
[1mStep[0m  [2/26], [94mLoss[0m : 2.06356
[1mStep[0m  [4/26], [94mLoss[0m : 1.86671
[1mStep[0m  [6/26], [94mLoss[0m : 2.03863
[1mStep[0m  [8/26], [94mLoss[0m : 2.05911
[1mStep[0m  [10/26], [94mLoss[0m : 1.81398
[1mStep[0m  [12/26], [94mLoss[0m : 2.05266
[1mStep[0m  [14/26], [94mLoss[0m : 1.93410
[1mStep[0m  [16/26], [94mLoss[0m : 1.94876
[1mStep[0m  [18/26], [94mLoss[0m : 1.91648
[1mStep[0m  [20/26], [94mLoss[0m : 1.91166
[1mStep[0m  [22/26], [94mLoss[0m : 1.90634
[1mStep[0m  [24/26], [94mLoss[0m : 2.03933

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.86860
[1mStep[0m  [2/26], [94mLoss[0m : 1.97733
[1mStep[0m  [4/26], [94mLoss[0m : 1.92882
[1mStep[0m  [6/26], [94mLoss[0m : 1.85747
[1mStep[0m  [8/26], [94mLoss[0m : 1.93641
[1mStep[0m  [10/26], [94mLoss[0m : 1.85427
[1mStep[0m  [12/26], [94mLoss[0m : 1.92989
[1mStep[0m  [14/26], [94mLoss[0m : 1.91425
[1mStep[0m  [16/26], [94mLoss[0m : 1.97737
[1mStep[0m  [18/26], [94mLoss[0m : 1.82812
[1mStep[0m  [20/26], [94mLoss[0m : 1.97909
[1mStep[0m  [22/26], [94mLoss[0m : 2.10053
[1mStep[0m  [24/26], [94mLoss[0m : 2.07079

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.438, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.85346
[1mStep[0m  [2/26], [94mLoss[0m : 1.93002
[1mStep[0m  [4/26], [94mLoss[0m : 1.93234
[1mStep[0m  [6/26], [94mLoss[0m : 1.80025
[1mStep[0m  [8/26], [94mLoss[0m : 1.94903
[1mStep[0m  [10/26], [94mLoss[0m : 1.89060
[1mStep[0m  [12/26], [94mLoss[0m : 1.87086
[1mStep[0m  [14/26], [94mLoss[0m : 1.96507
[1mStep[0m  [16/26], [94mLoss[0m : 1.97572
[1mStep[0m  [18/26], [94mLoss[0m : 1.90090
[1mStep[0m  [20/26], [94mLoss[0m : 1.80046
[1mStep[0m  [22/26], [94mLoss[0m : 1.80399
[1mStep[0m  [24/26], [94mLoss[0m : 2.00627

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.429, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93984
[1mStep[0m  [2/26], [94mLoss[0m : 1.93234
[1mStep[0m  [4/26], [94mLoss[0m : 1.87701
[1mStep[0m  [6/26], [94mLoss[0m : 1.87227
[1mStep[0m  [8/26], [94mLoss[0m : 1.91773
[1mStep[0m  [10/26], [94mLoss[0m : 1.90273
[1mStep[0m  [12/26], [94mLoss[0m : 2.01909
[1mStep[0m  [14/26], [94mLoss[0m : 1.87214
[1mStep[0m  [16/26], [94mLoss[0m : 2.03357
[1mStep[0m  [18/26], [94mLoss[0m : 1.80611
[1mStep[0m  [20/26], [94mLoss[0m : 1.85434
[1mStep[0m  [22/26], [94mLoss[0m : 1.90778
[1mStep[0m  [24/26], [94mLoss[0m : 1.93206

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.906, [92mTest[0m: 2.399, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76950
[1mStep[0m  [2/26], [94mLoss[0m : 1.76995
[1mStep[0m  [4/26], [94mLoss[0m : 1.82268
[1mStep[0m  [6/26], [94mLoss[0m : 1.96076
[1mStep[0m  [8/26], [94mLoss[0m : 1.99110
[1mStep[0m  [10/26], [94mLoss[0m : 1.79612
[1mStep[0m  [12/26], [94mLoss[0m : 1.91199
[1mStep[0m  [14/26], [94mLoss[0m : 1.91162
[1mStep[0m  [16/26], [94mLoss[0m : 2.00371
[1mStep[0m  [18/26], [94mLoss[0m : 1.86022
[1mStep[0m  [20/26], [94mLoss[0m : 1.89979
[1mStep[0m  [22/26], [94mLoss[0m : 1.83267
[1mStep[0m  [24/26], [94mLoss[0m : 2.02001

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.407, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.75487
[1mStep[0m  [2/26], [94mLoss[0m : 1.83509
[1mStep[0m  [4/26], [94mLoss[0m : 2.00273
[1mStep[0m  [6/26], [94mLoss[0m : 1.92355
[1mStep[0m  [8/26], [94mLoss[0m : 1.80057
[1mStep[0m  [10/26], [94mLoss[0m : 1.91628
[1mStep[0m  [12/26], [94mLoss[0m : 1.90921
[1mStep[0m  [14/26], [94mLoss[0m : 1.77884
[1mStep[0m  [16/26], [94mLoss[0m : 1.92092
[1mStep[0m  [18/26], [94mLoss[0m : 1.82621
[1mStep[0m  [20/26], [94mLoss[0m : 1.92055
[1mStep[0m  [22/26], [94mLoss[0m : 1.92784
[1mStep[0m  [24/26], [94mLoss[0m : 1.91521

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.446, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88249
[1mStep[0m  [2/26], [94mLoss[0m : 1.80866
[1mStep[0m  [4/26], [94mLoss[0m : 1.89799
[1mStep[0m  [6/26], [94mLoss[0m : 1.78692
[1mStep[0m  [8/26], [94mLoss[0m : 1.89009
[1mStep[0m  [10/26], [94mLoss[0m : 2.01171
[1mStep[0m  [12/26], [94mLoss[0m : 2.00634
[1mStep[0m  [14/26], [94mLoss[0m : 1.90926
[1mStep[0m  [16/26], [94mLoss[0m : 1.84712
[1mStep[0m  [18/26], [94mLoss[0m : 2.02553
[1mStep[0m  [20/26], [94mLoss[0m : 1.90434
[1mStep[0m  [22/26], [94mLoss[0m : 1.90587
[1mStep[0m  [24/26], [94mLoss[0m : 1.88412

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.481
====================================

Phase 2 - Evaluation MAE:  2.4805085475628195
MAE score P1      2.396691
MAE score P2      2.480509
loss               1.86749
learning_rate         0.01
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 11.05496
[1mStep[0m  [2/26], [94mLoss[0m : 9.83087
[1mStep[0m  [4/26], [94mLoss[0m : 7.30365
[1mStep[0m  [6/26], [94mLoss[0m : 4.67831
[1mStep[0m  [8/26], [94mLoss[0m : 3.33935
[1mStep[0m  [10/26], [94mLoss[0m : 3.52205
[1mStep[0m  [12/26], [94mLoss[0m : 4.90177
[1mStep[0m  [14/26], [94mLoss[0m : 4.64803
[1mStep[0m  [16/26], [94mLoss[0m : 3.52867
[1mStep[0m  [18/26], [94mLoss[0m : 2.84856
[1mStep[0m  [20/26], [94mLoss[0m : 3.07177
[1mStep[0m  [22/26], [94mLoss[0m : 3.21060
[1mStep[0m  [24/26], [94mLoss[0m : 3.23162

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.838, [92mTest[0m: 10.812, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.99012
[1mStep[0m  [2/26], [94mLoss[0m : 2.75429
[1mStep[0m  [4/26], [94mLoss[0m : 2.46172
[1mStep[0m  [6/26], [94mLoss[0m : 2.75161
[1mStep[0m  [8/26], [94mLoss[0m : 2.81462
[1mStep[0m  [10/26], [94mLoss[0m : 2.93239
[1mStep[0m  [12/26], [94mLoss[0m : 2.66987
[1mStep[0m  [14/26], [94mLoss[0m : 2.72645
[1mStep[0m  [16/26], [94mLoss[0m : 2.64003
[1mStep[0m  [18/26], [94mLoss[0m : 2.72918
[1mStep[0m  [20/26], [94mLoss[0m : 2.53843
[1mStep[0m  [22/26], [94mLoss[0m : 2.58640
[1mStep[0m  [24/26], [94mLoss[0m : 2.67142

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.695, [92mTest[0m: 5.258, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67251
[1mStep[0m  [2/26], [94mLoss[0m : 2.60214
[1mStep[0m  [4/26], [94mLoss[0m : 2.48407
[1mStep[0m  [6/26], [94mLoss[0m : 2.44234
[1mStep[0m  [8/26], [94mLoss[0m : 2.61939
[1mStep[0m  [10/26], [94mLoss[0m : 2.55998
[1mStep[0m  [12/26], [94mLoss[0m : 2.50672
[1mStep[0m  [14/26], [94mLoss[0m : 2.53900
[1mStep[0m  [16/26], [94mLoss[0m : 2.64471
[1mStep[0m  [18/26], [94mLoss[0m : 2.54626
[1mStep[0m  [20/26], [94mLoss[0m : 2.65503
[1mStep[0m  [22/26], [94mLoss[0m : 2.46082
[1mStep[0m  [24/26], [94mLoss[0m : 2.55881

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.679, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47266
[1mStep[0m  [2/26], [94mLoss[0m : 2.56966
[1mStep[0m  [4/26], [94mLoss[0m : 2.53990
[1mStep[0m  [6/26], [94mLoss[0m : 2.61175
[1mStep[0m  [8/26], [94mLoss[0m : 2.42037
[1mStep[0m  [10/26], [94mLoss[0m : 2.59756
[1mStep[0m  [12/26], [94mLoss[0m : 2.47142
[1mStep[0m  [14/26], [94mLoss[0m : 2.38649
[1mStep[0m  [16/26], [94mLoss[0m : 2.52991
[1mStep[0m  [18/26], [94mLoss[0m : 2.50587
[1mStep[0m  [20/26], [94mLoss[0m : 2.53748
[1mStep[0m  [22/26], [94mLoss[0m : 2.58446
[1mStep[0m  [24/26], [94mLoss[0m : 2.34762

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.596, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38017
[1mStep[0m  [2/26], [94mLoss[0m : 2.61260
[1mStep[0m  [4/26], [94mLoss[0m : 2.49665
[1mStep[0m  [6/26], [94mLoss[0m : 2.51276
[1mStep[0m  [8/26], [94mLoss[0m : 2.42397
[1mStep[0m  [10/26], [94mLoss[0m : 2.52974
[1mStep[0m  [12/26], [94mLoss[0m : 2.53981
[1mStep[0m  [14/26], [94mLoss[0m : 2.45795
[1mStep[0m  [16/26], [94mLoss[0m : 2.59733
[1mStep[0m  [18/26], [94mLoss[0m : 2.53736
[1mStep[0m  [20/26], [94mLoss[0m : 2.56549
[1mStep[0m  [22/26], [94mLoss[0m : 2.48648
[1mStep[0m  [24/26], [94mLoss[0m : 2.55857

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58647
[1mStep[0m  [2/26], [94mLoss[0m : 2.29087
[1mStep[0m  [4/26], [94mLoss[0m : 2.51748
[1mStep[0m  [6/26], [94mLoss[0m : 2.49578
[1mStep[0m  [8/26], [94mLoss[0m : 2.48357
[1mStep[0m  [10/26], [94mLoss[0m : 2.35050
[1mStep[0m  [12/26], [94mLoss[0m : 2.42135
[1mStep[0m  [14/26], [94mLoss[0m : 2.43447
[1mStep[0m  [16/26], [94mLoss[0m : 2.61556
[1mStep[0m  [18/26], [94mLoss[0m : 2.36964
[1mStep[0m  [20/26], [94mLoss[0m : 2.37662
[1mStep[0m  [22/26], [94mLoss[0m : 2.37287
[1mStep[0m  [24/26], [94mLoss[0m : 2.48534

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51049
[1mStep[0m  [2/26], [94mLoss[0m : 2.28395
[1mStep[0m  [4/26], [94mLoss[0m : 2.61647
[1mStep[0m  [6/26], [94mLoss[0m : 2.46137
[1mStep[0m  [8/26], [94mLoss[0m : 2.57836
[1mStep[0m  [10/26], [94mLoss[0m : 2.50071
[1mStep[0m  [12/26], [94mLoss[0m : 2.49793
[1mStep[0m  [14/26], [94mLoss[0m : 2.33435
[1mStep[0m  [16/26], [94mLoss[0m : 2.41868
[1mStep[0m  [18/26], [94mLoss[0m : 2.53518
[1mStep[0m  [20/26], [94mLoss[0m : 2.62285
[1mStep[0m  [22/26], [94mLoss[0m : 2.59366
[1mStep[0m  [24/26], [94mLoss[0m : 2.42541

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.612, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58806
[1mStep[0m  [2/26], [94mLoss[0m : 2.32755
[1mStep[0m  [4/26], [94mLoss[0m : 2.58643
[1mStep[0m  [6/26], [94mLoss[0m : 2.38031
[1mStep[0m  [8/26], [94mLoss[0m : 2.39780
[1mStep[0m  [10/26], [94mLoss[0m : 2.54487
[1mStep[0m  [12/26], [94mLoss[0m : 2.38910
[1mStep[0m  [14/26], [94mLoss[0m : 2.43183
[1mStep[0m  [16/26], [94mLoss[0m : 2.51080
[1mStep[0m  [18/26], [94mLoss[0m : 2.42850
[1mStep[0m  [20/26], [94mLoss[0m : 2.45779
[1mStep[0m  [22/26], [94mLoss[0m : 2.40026
[1mStep[0m  [24/26], [94mLoss[0m : 2.45679

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48553
[1mStep[0m  [2/26], [94mLoss[0m : 2.46095
[1mStep[0m  [4/26], [94mLoss[0m : 2.42882
[1mStep[0m  [6/26], [94mLoss[0m : 2.38577
[1mStep[0m  [8/26], [94mLoss[0m : 2.40336
[1mStep[0m  [10/26], [94mLoss[0m : 2.37329
[1mStep[0m  [12/26], [94mLoss[0m : 2.41656
[1mStep[0m  [14/26], [94mLoss[0m : 2.67275
[1mStep[0m  [16/26], [94mLoss[0m : 2.54309
[1mStep[0m  [18/26], [94mLoss[0m : 2.27149
[1mStep[0m  [20/26], [94mLoss[0m : 2.52930
[1mStep[0m  [22/26], [94mLoss[0m : 2.37875
[1mStep[0m  [24/26], [94mLoss[0m : 2.57549

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48612
[1mStep[0m  [2/26], [94mLoss[0m : 2.47479
[1mStep[0m  [4/26], [94mLoss[0m : 2.45316
[1mStep[0m  [6/26], [94mLoss[0m : 2.56651
[1mStep[0m  [8/26], [94mLoss[0m : 2.33941
[1mStep[0m  [10/26], [94mLoss[0m : 2.44928
[1mStep[0m  [12/26], [94mLoss[0m : 2.40453
[1mStep[0m  [14/26], [94mLoss[0m : 2.52141
[1mStep[0m  [16/26], [94mLoss[0m : 2.38001
[1mStep[0m  [18/26], [94mLoss[0m : 2.40183
[1mStep[0m  [20/26], [94mLoss[0m : 2.49741
[1mStep[0m  [22/26], [94mLoss[0m : 2.57724
[1mStep[0m  [24/26], [94mLoss[0m : 2.31652

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31306
[1mStep[0m  [2/26], [94mLoss[0m : 2.43180
[1mStep[0m  [4/26], [94mLoss[0m : 2.55062
[1mStep[0m  [6/26], [94mLoss[0m : 2.32632
[1mStep[0m  [8/26], [94mLoss[0m : 2.55346
[1mStep[0m  [10/26], [94mLoss[0m : 2.38060
[1mStep[0m  [12/26], [94mLoss[0m : 2.57619
[1mStep[0m  [14/26], [94mLoss[0m : 2.37721
[1mStep[0m  [16/26], [94mLoss[0m : 2.30257
[1mStep[0m  [18/26], [94mLoss[0m : 2.34062
[1mStep[0m  [20/26], [94mLoss[0m : 2.62535
[1mStep[0m  [22/26], [94mLoss[0m : 2.45333
[1mStep[0m  [24/26], [94mLoss[0m : 2.55634

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56005
[1mStep[0m  [2/26], [94mLoss[0m : 2.49342
[1mStep[0m  [4/26], [94mLoss[0m : 2.40805
[1mStep[0m  [6/26], [94mLoss[0m : 2.36391
[1mStep[0m  [8/26], [94mLoss[0m : 2.19500
[1mStep[0m  [10/26], [94mLoss[0m : 2.30214
[1mStep[0m  [12/26], [94mLoss[0m : 2.45496
[1mStep[0m  [14/26], [94mLoss[0m : 2.37199
[1mStep[0m  [16/26], [94mLoss[0m : 2.37739
[1mStep[0m  [18/26], [94mLoss[0m : 2.35586
[1mStep[0m  [20/26], [94mLoss[0m : 2.36192
[1mStep[0m  [22/26], [94mLoss[0m : 2.45010
[1mStep[0m  [24/26], [94mLoss[0m : 2.41982

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25649
[1mStep[0m  [2/26], [94mLoss[0m : 2.45274
[1mStep[0m  [4/26], [94mLoss[0m : 2.24432
[1mStep[0m  [6/26], [94mLoss[0m : 2.38732
[1mStep[0m  [8/26], [94mLoss[0m : 2.38015
[1mStep[0m  [10/26], [94mLoss[0m : 2.48348
[1mStep[0m  [12/26], [94mLoss[0m : 2.55772
[1mStep[0m  [14/26], [94mLoss[0m : 2.38454
[1mStep[0m  [16/26], [94mLoss[0m : 2.32029
[1mStep[0m  [18/26], [94mLoss[0m : 2.45806
[1mStep[0m  [20/26], [94mLoss[0m : 2.50506
[1mStep[0m  [22/26], [94mLoss[0m : 2.44508
[1mStep[0m  [24/26], [94mLoss[0m : 2.46100

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51662
[1mStep[0m  [2/26], [94mLoss[0m : 2.50177
[1mStep[0m  [4/26], [94mLoss[0m : 2.46453
[1mStep[0m  [6/26], [94mLoss[0m : 2.32239
[1mStep[0m  [8/26], [94mLoss[0m : 2.31372
[1mStep[0m  [10/26], [94mLoss[0m : 2.53893
[1mStep[0m  [12/26], [94mLoss[0m : 2.52637
[1mStep[0m  [14/26], [94mLoss[0m : 2.51024
[1mStep[0m  [16/26], [94mLoss[0m : 2.30684
[1mStep[0m  [18/26], [94mLoss[0m : 2.42095
[1mStep[0m  [20/26], [94mLoss[0m : 2.35819
[1mStep[0m  [22/26], [94mLoss[0m : 2.24605
[1mStep[0m  [24/26], [94mLoss[0m : 2.40864

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37831
[1mStep[0m  [2/26], [94mLoss[0m : 2.48462
[1mStep[0m  [4/26], [94mLoss[0m : 2.27017
[1mStep[0m  [6/26], [94mLoss[0m : 2.42541
[1mStep[0m  [8/26], [94mLoss[0m : 2.46922
[1mStep[0m  [10/26], [94mLoss[0m : 2.32998
[1mStep[0m  [12/26], [94mLoss[0m : 2.66280
[1mStep[0m  [14/26], [94mLoss[0m : 2.44439
[1mStep[0m  [16/26], [94mLoss[0m : 2.34367
[1mStep[0m  [18/26], [94mLoss[0m : 2.49412
[1mStep[0m  [20/26], [94mLoss[0m : 2.46172
[1mStep[0m  [22/26], [94mLoss[0m : 2.20921
[1mStep[0m  [24/26], [94mLoss[0m : 2.34310

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25743
[1mStep[0m  [2/26], [94mLoss[0m : 2.42198
[1mStep[0m  [4/26], [94mLoss[0m : 2.26632
[1mStep[0m  [6/26], [94mLoss[0m : 2.27731
[1mStep[0m  [8/26], [94mLoss[0m : 2.51474
[1mStep[0m  [10/26], [94mLoss[0m : 2.31347
[1mStep[0m  [12/26], [94mLoss[0m : 2.45226
[1mStep[0m  [14/26], [94mLoss[0m : 2.35741
[1mStep[0m  [16/26], [94mLoss[0m : 2.30419
[1mStep[0m  [18/26], [94mLoss[0m : 2.34816
[1mStep[0m  [20/26], [94mLoss[0m : 2.35527
[1mStep[0m  [22/26], [94mLoss[0m : 2.39112
[1mStep[0m  [24/26], [94mLoss[0m : 2.58034

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49583
[1mStep[0m  [2/26], [94mLoss[0m : 2.32391
[1mStep[0m  [4/26], [94mLoss[0m : 2.33515
[1mStep[0m  [6/26], [94mLoss[0m : 2.31392
[1mStep[0m  [8/26], [94mLoss[0m : 2.46595
[1mStep[0m  [10/26], [94mLoss[0m : 2.46804
[1mStep[0m  [12/26], [94mLoss[0m : 2.55594
[1mStep[0m  [14/26], [94mLoss[0m : 2.38603
[1mStep[0m  [16/26], [94mLoss[0m : 2.29896
[1mStep[0m  [18/26], [94mLoss[0m : 2.50708
[1mStep[0m  [20/26], [94mLoss[0m : 2.52508
[1mStep[0m  [22/26], [94mLoss[0m : 2.43729
[1mStep[0m  [24/26], [94mLoss[0m : 2.32079

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32047
[1mStep[0m  [2/26], [94mLoss[0m : 2.23942
[1mStep[0m  [4/26], [94mLoss[0m : 2.34181
[1mStep[0m  [6/26], [94mLoss[0m : 2.38017
[1mStep[0m  [8/26], [94mLoss[0m : 2.40362
[1mStep[0m  [10/26], [94mLoss[0m : 2.40154
[1mStep[0m  [12/26], [94mLoss[0m : 2.43748
[1mStep[0m  [14/26], [94mLoss[0m : 2.42231
[1mStep[0m  [16/26], [94mLoss[0m : 2.33687
[1mStep[0m  [18/26], [94mLoss[0m : 2.37228
[1mStep[0m  [20/26], [94mLoss[0m : 2.22625
[1mStep[0m  [22/26], [94mLoss[0m : 2.42447
[1mStep[0m  [24/26], [94mLoss[0m : 2.40547

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32878
[1mStep[0m  [2/26], [94mLoss[0m : 2.43254
[1mStep[0m  [4/26], [94mLoss[0m : 2.31156
[1mStep[0m  [6/26], [94mLoss[0m : 2.21167
[1mStep[0m  [8/26], [94mLoss[0m : 2.33788
[1mStep[0m  [10/26], [94mLoss[0m : 2.40086
[1mStep[0m  [12/26], [94mLoss[0m : 2.26813
[1mStep[0m  [14/26], [94mLoss[0m : 2.42031
[1mStep[0m  [16/26], [94mLoss[0m : 2.27718
[1mStep[0m  [18/26], [94mLoss[0m : 2.31970
[1mStep[0m  [20/26], [94mLoss[0m : 2.27023
[1mStep[0m  [22/26], [94mLoss[0m : 2.47558
[1mStep[0m  [24/26], [94mLoss[0m : 2.47782

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49703
[1mStep[0m  [2/26], [94mLoss[0m : 2.38877
[1mStep[0m  [4/26], [94mLoss[0m : 2.26351
[1mStep[0m  [6/26], [94mLoss[0m : 2.28756
[1mStep[0m  [8/26], [94mLoss[0m : 2.48790
[1mStep[0m  [10/26], [94mLoss[0m : 2.37510
[1mStep[0m  [12/26], [94mLoss[0m : 2.43713
[1mStep[0m  [14/26], [94mLoss[0m : 2.52174
[1mStep[0m  [16/26], [94mLoss[0m : 2.36714
[1mStep[0m  [18/26], [94mLoss[0m : 2.41541
[1mStep[0m  [20/26], [94mLoss[0m : 2.37865
[1mStep[0m  [22/26], [94mLoss[0m : 2.31031
[1mStep[0m  [24/26], [94mLoss[0m : 2.37628

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.368, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34367
[1mStep[0m  [2/26], [94mLoss[0m : 2.28670
[1mStep[0m  [4/26], [94mLoss[0m : 2.45243
[1mStep[0m  [6/26], [94mLoss[0m : 2.35309
[1mStep[0m  [8/26], [94mLoss[0m : 2.41258
[1mStep[0m  [10/26], [94mLoss[0m : 2.33617
[1mStep[0m  [12/26], [94mLoss[0m : 2.27422
[1mStep[0m  [14/26], [94mLoss[0m : 2.20654
[1mStep[0m  [16/26], [94mLoss[0m : 2.49556
[1mStep[0m  [18/26], [94mLoss[0m : 2.41796
[1mStep[0m  [20/26], [94mLoss[0m : 2.34439
[1mStep[0m  [22/26], [94mLoss[0m : 2.36185
[1mStep[0m  [24/26], [94mLoss[0m : 2.37116

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.369, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29045
[1mStep[0m  [2/26], [94mLoss[0m : 2.31904
[1mStep[0m  [4/26], [94mLoss[0m : 2.48017
[1mStep[0m  [6/26], [94mLoss[0m : 2.46964
[1mStep[0m  [8/26], [94mLoss[0m : 2.24416
[1mStep[0m  [10/26], [94mLoss[0m : 2.25383
[1mStep[0m  [12/26], [94mLoss[0m : 2.45053
[1mStep[0m  [14/26], [94mLoss[0m : 2.42975
[1mStep[0m  [16/26], [94mLoss[0m : 2.41821
[1mStep[0m  [18/26], [94mLoss[0m : 2.33110
[1mStep[0m  [20/26], [94mLoss[0m : 2.18912
[1mStep[0m  [22/26], [94mLoss[0m : 2.41859
[1mStep[0m  [24/26], [94mLoss[0m : 2.47567

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63205
[1mStep[0m  [2/26], [94mLoss[0m : 2.32299
[1mStep[0m  [4/26], [94mLoss[0m : 2.22771
[1mStep[0m  [6/26], [94mLoss[0m : 2.30787
[1mStep[0m  [8/26], [94mLoss[0m : 2.44855
[1mStep[0m  [10/26], [94mLoss[0m : 2.29984
[1mStep[0m  [12/26], [94mLoss[0m : 2.26090
[1mStep[0m  [14/26], [94mLoss[0m : 2.36678
[1mStep[0m  [16/26], [94mLoss[0m : 2.56371
[1mStep[0m  [18/26], [94mLoss[0m : 2.32943
[1mStep[0m  [20/26], [94mLoss[0m : 2.33411
[1mStep[0m  [22/26], [94mLoss[0m : 2.42531
[1mStep[0m  [24/26], [94mLoss[0m : 2.35797

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54934
[1mStep[0m  [2/26], [94mLoss[0m : 2.25862
[1mStep[0m  [4/26], [94mLoss[0m : 2.48665
[1mStep[0m  [6/26], [94mLoss[0m : 2.42449
[1mStep[0m  [8/26], [94mLoss[0m : 2.25720
[1mStep[0m  [10/26], [94mLoss[0m : 2.55724
[1mStep[0m  [12/26], [94mLoss[0m : 2.29477
[1mStep[0m  [14/26], [94mLoss[0m : 2.32388
[1mStep[0m  [16/26], [94mLoss[0m : 2.34349
[1mStep[0m  [18/26], [94mLoss[0m : 2.27984
[1mStep[0m  [20/26], [94mLoss[0m : 2.54155
[1mStep[0m  [22/26], [94mLoss[0m : 2.24393
[1mStep[0m  [24/26], [94mLoss[0m : 2.33951

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45267
[1mStep[0m  [2/26], [94mLoss[0m : 2.41111
[1mStep[0m  [4/26], [94mLoss[0m : 2.26727
[1mStep[0m  [6/26], [94mLoss[0m : 2.29142
[1mStep[0m  [8/26], [94mLoss[0m : 2.36537
[1mStep[0m  [10/26], [94mLoss[0m : 2.35776
[1mStep[0m  [12/26], [94mLoss[0m : 2.44900
[1mStep[0m  [14/26], [94mLoss[0m : 2.40369
[1mStep[0m  [16/26], [94mLoss[0m : 2.34288
[1mStep[0m  [18/26], [94mLoss[0m : 2.46149
[1mStep[0m  [20/26], [94mLoss[0m : 2.26377
[1mStep[0m  [22/26], [94mLoss[0m : 2.30730
[1mStep[0m  [24/26], [94mLoss[0m : 2.31592

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27023
[1mStep[0m  [2/26], [94mLoss[0m : 2.25987
[1mStep[0m  [4/26], [94mLoss[0m : 2.49185
[1mStep[0m  [6/26], [94mLoss[0m : 2.29796
[1mStep[0m  [8/26], [94mLoss[0m : 2.31849
[1mStep[0m  [10/26], [94mLoss[0m : 2.44075
[1mStep[0m  [12/26], [94mLoss[0m : 2.34187
[1mStep[0m  [14/26], [94mLoss[0m : 2.39537
[1mStep[0m  [16/26], [94mLoss[0m : 2.18828
[1mStep[0m  [18/26], [94mLoss[0m : 2.30096
[1mStep[0m  [20/26], [94mLoss[0m : 2.26581
[1mStep[0m  [22/26], [94mLoss[0m : 2.31223
[1mStep[0m  [24/26], [94mLoss[0m : 2.28508

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35472
[1mStep[0m  [2/26], [94mLoss[0m : 2.32066
[1mStep[0m  [4/26], [94mLoss[0m : 2.30748
[1mStep[0m  [6/26], [94mLoss[0m : 2.42690
[1mStep[0m  [8/26], [94mLoss[0m : 2.35416
[1mStep[0m  [10/26], [94mLoss[0m : 2.35147
[1mStep[0m  [12/26], [94mLoss[0m : 2.24844
[1mStep[0m  [14/26], [94mLoss[0m : 2.33863
[1mStep[0m  [16/26], [94mLoss[0m : 2.25066
[1mStep[0m  [18/26], [94mLoss[0m : 2.33278
[1mStep[0m  [20/26], [94mLoss[0m : 2.34847
[1mStep[0m  [22/26], [94mLoss[0m : 2.20930
[1mStep[0m  [24/26], [94mLoss[0m : 2.39604

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22259
[1mStep[0m  [2/26], [94mLoss[0m : 2.31921
[1mStep[0m  [4/26], [94mLoss[0m : 2.28755
[1mStep[0m  [6/26], [94mLoss[0m : 2.42636
[1mStep[0m  [8/26], [94mLoss[0m : 2.25305
[1mStep[0m  [10/26], [94mLoss[0m : 2.20376
[1mStep[0m  [12/26], [94mLoss[0m : 2.32358
[1mStep[0m  [14/26], [94mLoss[0m : 2.43393
[1mStep[0m  [16/26], [94mLoss[0m : 2.24724
[1mStep[0m  [18/26], [94mLoss[0m : 2.36555
[1mStep[0m  [20/26], [94mLoss[0m : 2.28952
[1mStep[0m  [22/26], [94mLoss[0m : 2.25449
[1mStep[0m  [24/26], [94mLoss[0m : 2.39782

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23971
[1mStep[0m  [2/26], [94mLoss[0m : 2.28105
[1mStep[0m  [4/26], [94mLoss[0m : 2.26059
[1mStep[0m  [6/26], [94mLoss[0m : 2.35577
[1mStep[0m  [8/26], [94mLoss[0m : 2.16625
[1mStep[0m  [10/26], [94mLoss[0m : 2.32480
[1mStep[0m  [12/26], [94mLoss[0m : 2.26658
[1mStep[0m  [14/26], [94mLoss[0m : 2.32446
[1mStep[0m  [16/26], [94mLoss[0m : 2.31861
[1mStep[0m  [18/26], [94mLoss[0m : 2.38039
[1mStep[0m  [20/26], [94mLoss[0m : 2.35331
[1mStep[0m  [22/26], [94mLoss[0m : 2.33544
[1mStep[0m  [24/26], [94mLoss[0m : 2.40839

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25666
[1mStep[0m  [2/26], [94mLoss[0m : 2.23062
[1mStep[0m  [4/26], [94mLoss[0m : 2.17734
[1mStep[0m  [6/26], [94mLoss[0m : 2.35494
[1mStep[0m  [8/26], [94mLoss[0m : 2.20837
[1mStep[0m  [10/26], [94mLoss[0m : 2.35884
[1mStep[0m  [12/26], [94mLoss[0m : 2.23526
[1mStep[0m  [14/26], [94mLoss[0m : 2.35151
[1mStep[0m  [16/26], [94mLoss[0m : 2.42558
[1mStep[0m  [18/26], [94mLoss[0m : 2.46844
[1mStep[0m  [20/26], [94mLoss[0m : 2.56126
[1mStep[0m  [22/26], [94mLoss[0m : 2.33604
[1mStep[0m  [24/26], [94mLoss[0m : 2.39285

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.360, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.390
====================================

Phase 1 - Evaluation MAE:  2.3896398544311523
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.28124
[1mStep[0m  [2/26], [94mLoss[0m : 2.45275
[1mStep[0m  [4/26], [94mLoss[0m : 2.50285
[1mStep[0m  [6/26], [94mLoss[0m : 2.40289
[1mStep[0m  [8/26], [94mLoss[0m : 2.40887
[1mStep[0m  [10/26], [94mLoss[0m : 2.56588
[1mStep[0m  [12/26], [94mLoss[0m : 2.58035
[1mStep[0m  [14/26], [94mLoss[0m : 2.46227
[1mStep[0m  [16/26], [94mLoss[0m : 2.36822
[1mStep[0m  [18/26], [94mLoss[0m : 2.48239
[1mStep[0m  [20/26], [94mLoss[0m : 2.44004
[1mStep[0m  [22/26], [94mLoss[0m : 2.44575
[1mStep[0m  [24/26], [94mLoss[0m : 2.37387

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38474
[1mStep[0m  [2/26], [94mLoss[0m : 2.34123
[1mStep[0m  [4/26], [94mLoss[0m : 2.37594
[1mStep[0m  [6/26], [94mLoss[0m : 2.27778
[1mStep[0m  [8/26], [94mLoss[0m : 2.24320
[1mStep[0m  [10/26], [94mLoss[0m : 2.22950
[1mStep[0m  [12/26], [94mLoss[0m : 2.52026
[1mStep[0m  [14/26], [94mLoss[0m : 2.41469
[1mStep[0m  [16/26], [94mLoss[0m : 2.40892
[1mStep[0m  [18/26], [94mLoss[0m : 2.29564
[1mStep[0m  [20/26], [94mLoss[0m : 2.43178
[1mStep[0m  [22/26], [94mLoss[0m : 2.28658
[1mStep[0m  [24/26], [94mLoss[0m : 2.27981

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.351, [92mTest[0m: 11.553, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08116
[1mStep[0m  [2/26], [94mLoss[0m : 2.11427
[1mStep[0m  [4/26], [94mLoss[0m : 2.20368
[1mStep[0m  [6/26], [94mLoss[0m : 2.28395
[1mStep[0m  [8/26], [94mLoss[0m : 2.23049
[1mStep[0m  [10/26], [94mLoss[0m : 2.19176
[1mStep[0m  [12/26], [94mLoss[0m : 2.21818
[1mStep[0m  [14/26], [94mLoss[0m : 2.13261
[1mStep[0m  [16/26], [94mLoss[0m : 2.29686
[1mStep[0m  [18/26], [94mLoss[0m : 2.32818
[1mStep[0m  [20/26], [94mLoss[0m : 2.31951
[1mStep[0m  [22/26], [94mLoss[0m : 2.17970
[1mStep[0m  [24/26], [94mLoss[0m : 2.20752

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.240, [92mTest[0m: 5.540, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07296
[1mStep[0m  [2/26], [94mLoss[0m : 2.18429
[1mStep[0m  [4/26], [94mLoss[0m : 2.13776
[1mStep[0m  [6/26], [94mLoss[0m : 2.11625
[1mStep[0m  [8/26], [94mLoss[0m : 2.23921
[1mStep[0m  [10/26], [94mLoss[0m : 2.21513
[1mStep[0m  [12/26], [94mLoss[0m : 2.14390
[1mStep[0m  [14/26], [94mLoss[0m : 2.08895
[1mStep[0m  [16/26], [94mLoss[0m : 2.09824
[1mStep[0m  [18/26], [94mLoss[0m : 2.29540
[1mStep[0m  [20/26], [94mLoss[0m : 2.21736
[1mStep[0m  [22/26], [94mLoss[0m : 2.24555
[1mStep[0m  [24/26], [94mLoss[0m : 2.12213

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.162, [92mTest[0m: 3.579, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.10261
[1mStep[0m  [2/26], [94mLoss[0m : 2.09958
[1mStep[0m  [4/26], [94mLoss[0m : 2.12948
[1mStep[0m  [6/26], [94mLoss[0m : 2.09712
[1mStep[0m  [8/26], [94mLoss[0m : 2.18323
[1mStep[0m  [10/26], [94mLoss[0m : 2.31154
[1mStep[0m  [12/26], [94mLoss[0m : 2.14246
[1mStep[0m  [14/26], [94mLoss[0m : 2.07489
[1mStep[0m  [16/26], [94mLoss[0m : 2.21790
[1mStep[0m  [18/26], [94mLoss[0m : 2.17583
[1mStep[0m  [20/26], [94mLoss[0m : 2.00367
[1mStep[0m  [22/26], [94mLoss[0m : 2.09625
[1mStep[0m  [24/26], [94mLoss[0m : 2.18608

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.621, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03086
[1mStep[0m  [2/26], [94mLoss[0m : 1.98408
[1mStep[0m  [4/26], [94mLoss[0m : 2.00178
[1mStep[0m  [6/26], [94mLoss[0m : 1.93913
[1mStep[0m  [8/26], [94mLoss[0m : 1.98512
[1mStep[0m  [10/26], [94mLoss[0m : 1.97810
[1mStep[0m  [12/26], [94mLoss[0m : 1.86963
[1mStep[0m  [14/26], [94mLoss[0m : 2.01470
[1mStep[0m  [16/26], [94mLoss[0m : 1.93082
[1mStep[0m  [18/26], [94mLoss[0m : 2.06519
[1mStep[0m  [20/26], [94mLoss[0m : 1.95893
[1mStep[0m  [22/26], [94mLoss[0m : 2.13794
[1mStep[0m  [24/26], [94mLoss[0m : 2.17127

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96899
[1mStep[0m  [2/26], [94mLoss[0m : 1.90418
[1mStep[0m  [4/26], [94mLoss[0m : 1.86553
[1mStep[0m  [6/26], [94mLoss[0m : 2.06521
[1mStep[0m  [8/26], [94mLoss[0m : 1.98876
[1mStep[0m  [10/26], [94mLoss[0m : 1.88767
[1mStep[0m  [12/26], [94mLoss[0m : 1.85677
[1mStep[0m  [14/26], [94mLoss[0m : 1.98124
[1mStep[0m  [16/26], [94mLoss[0m : 2.04957
[1mStep[0m  [18/26], [94mLoss[0m : 1.90881
[1mStep[0m  [20/26], [94mLoss[0m : 1.90726
[1mStep[0m  [22/26], [94mLoss[0m : 1.83974
[1mStep[0m  [24/26], [94mLoss[0m : 2.04208

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.80350
[1mStep[0m  [2/26], [94mLoss[0m : 1.73148
[1mStep[0m  [4/26], [94mLoss[0m : 1.74803
[1mStep[0m  [6/26], [94mLoss[0m : 1.94536
[1mStep[0m  [8/26], [94mLoss[0m : 1.96889
[1mStep[0m  [10/26], [94mLoss[0m : 1.94954
[1mStep[0m  [12/26], [94mLoss[0m : 1.91944
[1mStep[0m  [14/26], [94mLoss[0m : 1.97564
[1mStep[0m  [16/26], [94mLoss[0m : 2.02210
[1mStep[0m  [18/26], [94mLoss[0m : 1.92068
[1mStep[0m  [20/26], [94mLoss[0m : 1.96491
[1mStep[0m  [22/26], [94mLoss[0m : 1.85809
[1mStep[0m  [24/26], [94mLoss[0m : 1.92926

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.887, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.85907
[1mStep[0m  [2/26], [94mLoss[0m : 1.98183
[1mStep[0m  [4/26], [94mLoss[0m : 1.91033
[1mStep[0m  [6/26], [94mLoss[0m : 1.82513
[1mStep[0m  [8/26], [94mLoss[0m : 1.87411
[1mStep[0m  [10/26], [94mLoss[0m : 1.84681
[1mStep[0m  [12/26], [94mLoss[0m : 1.79939
[1mStep[0m  [14/26], [94mLoss[0m : 1.95993
[1mStep[0m  [16/26], [94mLoss[0m : 1.79457
[1mStep[0m  [18/26], [94mLoss[0m : 1.97676
[1mStep[0m  [20/26], [94mLoss[0m : 2.07322
[1mStep[0m  [22/26], [94mLoss[0m : 1.91271
[1mStep[0m  [24/26], [94mLoss[0m : 1.89291

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.877, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.63186
[1mStep[0m  [2/26], [94mLoss[0m : 1.84355
[1mStep[0m  [4/26], [94mLoss[0m : 1.79677
[1mStep[0m  [6/26], [94mLoss[0m : 1.76996
[1mStep[0m  [8/26], [94mLoss[0m : 1.82344
[1mStep[0m  [10/26], [94mLoss[0m : 1.73684
[1mStep[0m  [12/26], [94mLoss[0m : 1.72123
[1mStep[0m  [14/26], [94mLoss[0m : 1.78927
[1mStep[0m  [16/26], [94mLoss[0m : 1.82975
[1mStep[0m  [18/26], [94mLoss[0m : 1.88075
[1mStep[0m  [20/26], [94mLoss[0m : 1.81447
[1mStep[0m  [22/26], [94mLoss[0m : 1.82951
[1mStep[0m  [24/26], [94mLoss[0m : 1.89679

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81037
[1mStep[0m  [2/26], [94mLoss[0m : 1.60532
[1mStep[0m  [4/26], [94mLoss[0m : 1.63760
[1mStep[0m  [6/26], [94mLoss[0m : 1.71368
[1mStep[0m  [8/26], [94mLoss[0m : 1.75988
[1mStep[0m  [10/26], [94mLoss[0m : 1.75775
[1mStep[0m  [12/26], [94mLoss[0m : 1.80004
[1mStep[0m  [14/26], [94mLoss[0m : 1.73250
[1mStep[0m  [16/26], [94mLoss[0m : 1.75055
[1mStep[0m  [18/26], [94mLoss[0m : 1.89830
[1mStep[0m  [20/26], [94mLoss[0m : 1.87895
[1mStep[0m  [22/26], [94mLoss[0m : 1.83266
[1mStep[0m  [24/26], [94mLoss[0m : 1.73745

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82262
[1mStep[0m  [2/26], [94mLoss[0m : 1.72490
[1mStep[0m  [4/26], [94mLoss[0m : 1.82261
[1mStep[0m  [6/26], [94mLoss[0m : 1.74712
[1mStep[0m  [8/26], [94mLoss[0m : 1.70367
[1mStep[0m  [10/26], [94mLoss[0m : 1.76983
[1mStep[0m  [12/26], [94mLoss[0m : 1.78158
[1mStep[0m  [14/26], [94mLoss[0m : 1.76081
[1mStep[0m  [16/26], [94mLoss[0m : 1.71424
[1mStep[0m  [18/26], [94mLoss[0m : 1.73618
[1mStep[0m  [20/26], [94mLoss[0m : 1.67901
[1mStep[0m  [22/26], [94mLoss[0m : 1.71044
[1mStep[0m  [24/26], [94mLoss[0m : 1.79136

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.71144
[1mStep[0m  [2/26], [94mLoss[0m : 1.65271
[1mStep[0m  [4/26], [94mLoss[0m : 1.79708
[1mStep[0m  [6/26], [94mLoss[0m : 1.82854
[1mStep[0m  [8/26], [94mLoss[0m : 1.68306
[1mStep[0m  [10/26], [94mLoss[0m : 1.67143
[1mStep[0m  [12/26], [94mLoss[0m : 1.61208
[1mStep[0m  [14/26], [94mLoss[0m : 1.76525
[1mStep[0m  [16/26], [94mLoss[0m : 1.68218
[1mStep[0m  [18/26], [94mLoss[0m : 1.70684
[1mStep[0m  [20/26], [94mLoss[0m : 1.76413
[1mStep[0m  [22/26], [94mLoss[0m : 1.70487
[1mStep[0m  [24/26], [94mLoss[0m : 1.81669

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.60565
[1mStep[0m  [2/26], [94mLoss[0m : 1.65585
[1mStep[0m  [4/26], [94mLoss[0m : 1.63249
[1mStep[0m  [6/26], [94mLoss[0m : 1.53861
[1mStep[0m  [8/26], [94mLoss[0m : 1.66672
[1mStep[0m  [10/26], [94mLoss[0m : 1.84290
[1mStep[0m  [12/26], [94mLoss[0m : 1.83362
[1mStep[0m  [14/26], [94mLoss[0m : 1.72408
[1mStep[0m  [16/26], [94mLoss[0m : 1.65154
[1mStep[0m  [18/26], [94mLoss[0m : 1.73974
[1mStep[0m  [20/26], [94mLoss[0m : 1.63577
[1mStep[0m  [22/26], [94mLoss[0m : 1.76028
[1mStep[0m  [24/26], [94mLoss[0m : 1.71460

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.670, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54376
[1mStep[0m  [2/26], [94mLoss[0m : 1.59034
[1mStep[0m  [4/26], [94mLoss[0m : 1.54761
[1mStep[0m  [6/26], [94mLoss[0m : 1.69641
[1mStep[0m  [8/26], [94mLoss[0m : 1.62222
[1mStep[0m  [10/26], [94mLoss[0m : 1.67039
[1mStep[0m  [12/26], [94mLoss[0m : 1.69228
[1mStep[0m  [14/26], [94mLoss[0m : 1.76940
[1mStep[0m  [16/26], [94mLoss[0m : 1.74740
[1mStep[0m  [18/26], [94mLoss[0m : 1.70196
[1mStep[0m  [20/26], [94mLoss[0m : 1.68344
[1mStep[0m  [22/26], [94mLoss[0m : 1.87283
[1mStep[0m  [24/26], [94mLoss[0m : 1.65824

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.667, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.59001
[1mStep[0m  [2/26], [94mLoss[0m : 1.53654
[1mStep[0m  [4/26], [94mLoss[0m : 1.61751
[1mStep[0m  [6/26], [94mLoss[0m : 1.54945
[1mStep[0m  [8/26], [94mLoss[0m : 1.47698
[1mStep[0m  [10/26], [94mLoss[0m : 1.70548
[1mStep[0m  [12/26], [94mLoss[0m : 1.61755
[1mStep[0m  [14/26], [94mLoss[0m : 1.60174
[1mStep[0m  [16/26], [94mLoss[0m : 1.71347
[1mStep[0m  [18/26], [94mLoss[0m : 1.64539
[1mStep[0m  [20/26], [94mLoss[0m : 1.64488
[1mStep[0m  [22/26], [94mLoss[0m : 1.69502
[1mStep[0m  [24/26], [94mLoss[0m : 1.56573

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.56553
[1mStep[0m  [2/26], [94mLoss[0m : 1.56461
[1mStep[0m  [4/26], [94mLoss[0m : 1.55073
[1mStep[0m  [6/26], [94mLoss[0m : 1.53577
[1mStep[0m  [8/26], [94mLoss[0m : 1.68702
[1mStep[0m  [10/26], [94mLoss[0m : 1.53304
[1mStep[0m  [12/26], [94mLoss[0m : 1.50246
[1mStep[0m  [14/26], [94mLoss[0m : 1.62490
[1mStep[0m  [16/26], [94mLoss[0m : 1.55089
[1mStep[0m  [18/26], [94mLoss[0m : 1.64535
[1mStep[0m  [20/26], [94mLoss[0m : 1.66142
[1mStep[0m  [22/26], [94mLoss[0m : 1.60681
[1mStep[0m  [24/26], [94mLoss[0m : 1.56219

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.53334
[1mStep[0m  [2/26], [94mLoss[0m : 1.48256
[1mStep[0m  [4/26], [94mLoss[0m : 1.42601
[1mStep[0m  [6/26], [94mLoss[0m : 1.49412
[1mStep[0m  [8/26], [94mLoss[0m : 1.57169
[1mStep[0m  [10/26], [94mLoss[0m : 1.62599
[1mStep[0m  [12/26], [94mLoss[0m : 1.52109
[1mStep[0m  [14/26], [94mLoss[0m : 1.68541
[1mStep[0m  [16/26], [94mLoss[0m : 1.59482
[1mStep[0m  [18/26], [94mLoss[0m : 1.59526
[1mStep[0m  [20/26], [94mLoss[0m : 1.55446
[1mStep[0m  [22/26], [94mLoss[0m : 1.54867
[1mStep[0m  [24/26], [94mLoss[0m : 1.64771

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52842
[1mStep[0m  [2/26], [94mLoss[0m : 1.51843
[1mStep[0m  [4/26], [94mLoss[0m : 1.50138
[1mStep[0m  [6/26], [94mLoss[0m : 1.56708
[1mStep[0m  [8/26], [94mLoss[0m : 1.52704
[1mStep[0m  [10/26], [94mLoss[0m : 1.57171
[1mStep[0m  [12/26], [94mLoss[0m : 1.57022
[1mStep[0m  [14/26], [94mLoss[0m : 1.47384
[1mStep[0m  [16/26], [94mLoss[0m : 1.52773
[1mStep[0m  [18/26], [94mLoss[0m : 1.70781
[1mStep[0m  [20/26], [94mLoss[0m : 1.65367
[1mStep[0m  [22/26], [94mLoss[0m : 1.54045
[1mStep[0m  [24/26], [94mLoss[0m : 1.64519

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.576, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.45944
[1mStep[0m  [2/26], [94mLoss[0m : 1.56539
[1mStep[0m  [4/26], [94mLoss[0m : 1.53607
[1mStep[0m  [6/26], [94mLoss[0m : 1.52157
[1mStep[0m  [8/26], [94mLoss[0m : 1.57545
[1mStep[0m  [10/26], [94mLoss[0m : 1.61459
[1mStep[0m  [12/26], [94mLoss[0m : 1.45933
[1mStep[0m  [14/26], [94mLoss[0m : 1.44482
[1mStep[0m  [16/26], [94mLoss[0m : 1.59504
[1mStep[0m  [18/26], [94mLoss[0m : 1.53623
[1mStep[0m  [20/26], [94mLoss[0m : 1.66786
[1mStep[0m  [22/26], [94mLoss[0m : 1.49944
[1mStep[0m  [24/26], [94mLoss[0m : 1.65228

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.475, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.48621
[1mStep[0m  [2/26], [94mLoss[0m : 1.49843
[1mStep[0m  [4/26], [94mLoss[0m : 1.50409
[1mStep[0m  [6/26], [94mLoss[0m : 1.52206
[1mStep[0m  [8/26], [94mLoss[0m : 1.51776
[1mStep[0m  [10/26], [94mLoss[0m : 1.50781
[1mStep[0m  [12/26], [94mLoss[0m : 1.49625
[1mStep[0m  [14/26], [94mLoss[0m : 1.51399
[1mStep[0m  [16/26], [94mLoss[0m : 1.42688
[1mStep[0m  [18/26], [94mLoss[0m : 1.61898
[1mStep[0m  [20/26], [94mLoss[0m : 1.60883
[1mStep[0m  [22/26], [94mLoss[0m : 1.54460
[1mStep[0m  [24/26], [94mLoss[0m : 1.54723

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.44903
[1mStep[0m  [2/26], [94mLoss[0m : 1.41392
[1mStep[0m  [4/26], [94mLoss[0m : 1.46858
[1mStep[0m  [6/26], [94mLoss[0m : 1.46428
[1mStep[0m  [8/26], [94mLoss[0m : 1.49490
[1mStep[0m  [10/26], [94mLoss[0m : 1.53320
[1mStep[0m  [12/26], [94mLoss[0m : 1.48153
[1mStep[0m  [14/26], [94mLoss[0m : 1.39396
[1mStep[0m  [16/26], [94mLoss[0m : 1.50915
[1mStep[0m  [18/26], [94mLoss[0m : 1.54039
[1mStep[0m  [20/26], [94mLoss[0m : 1.43873
[1mStep[0m  [22/26], [94mLoss[0m : 1.45585
[1mStep[0m  [24/26], [94mLoss[0m : 1.44602

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.460, [92mTest[0m: 2.503, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.467
====================================

Phase 2 - Evaluation MAE:  2.4671717423659105
MAE score P1       2.38964
MAE score P2      2.467172
loss              1.459852
learning_rate         0.01
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.79032
[1mStep[0m  [5/53], [94mLoss[0m : 9.30531
[1mStep[0m  [10/53], [94mLoss[0m : 4.97476
[1mStep[0m  [15/53], [94mLoss[0m : 3.51192
[1mStep[0m  [20/53], [94mLoss[0m : 3.86371
[1mStep[0m  [25/53], [94mLoss[0m : 3.26655
[1mStep[0m  [30/53], [94mLoss[0m : 2.78301
[1mStep[0m  [35/53], [94mLoss[0m : 2.98112
[1mStep[0m  [40/53], [94mLoss[0m : 2.66610
[1mStep[0m  [45/53], [94mLoss[0m : 2.65422
[1mStep[0m  [50/53], [94mLoss[0m : 2.61858

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.196, [92mTest[0m: 10.854, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63880
[1mStep[0m  [5/53], [94mLoss[0m : 2.52250
[1mStep[0m  [10/53], [94mLoss[0m : 2.69302
[1mStep[0m  [15/53], [94mLoss[0m : 2.60412
[1mStep[0m  [20/53], [94mLoss[0m : 2.50337
[1mStep[0m  [25/53], [94mLoss[0m : 2.51315
[1mStep[0m  [30/53], [94mLoss[0m : 2.61523
[1mStep[0m  [35/53], [94mLoss[0m : 2.83183
[1mStep[0m  [40/53], [94mLoss[0m : 2.64240
[1mStep[0m  [45/53], [94mLoss[0m : 2.55084
[1mStep[0m  [50/53], [94mLoss[0m : 2.53155

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.553, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64744
[1mStep[0m  [5/53], [94mLoss[0m : 2.56645
[1mStep[0m  [10/53], [94mLoss[0m : 2.75321
[1mStep[0m  [15/53], [94mLoss[0m : 2.59019
[1mStep[0m  [20/53], [94mLoss[0m : 2.47208
[1mStep[0m  [25/53], [94mLoss[0m : 2.60980
[1mStep[0m  [30/53], [94mLoss[0m : 2.51188
[1mStep[0m  [35/53], [94mLoss[0m : 2.43388
[1mStep[0m  [40/53], [94mLoss[0m : 2.44448
[1mStep[0m  [45/53], [94mLoss[0m : 2.76658
[1mStep[0m  [50/53], [94mLoss[0m : 2.49209

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57373
[1mStep[0m  [5/53], [94mLoss[0m : 2.33205
[1mStep[0m  [10/53], [94mLoss[0m : 2.69950
[1mStep[0m  [15/53], [94mLoss[0m : 2.31960
[1mStep[0m  [20/53], [94mLoss[0m : 2.39127
[1mStep[0m  [25/53], [94mLoss[0m : 2.44447
[1mStep[0m  [30/53], [94mLoss[0m : 2.53939
[1mStep[0m  [35/53], [94mLoss[0m : 2.38358
[1mStep[0m  [40/53], [94mLoss[0m : 2.51629
[1mStep[0m  [45/53], [94mLoss[0m : 2.47212
[1mStep[0m  [50/53], [94mLoss[0m : 2.52868

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72554
[1mStep[0m  [5/53], [94mLoss[0m : 2.46889
[1mStep[0m  [10/53], [94mLoss[0m : 2.52764
[1mStep[0m  [15/53], [94mLoss[0m : 2.45500
[1mStep[0m  [20/53], [94mLoss[0m : 2.54741
[1mStep[0m  [25/53], [94mLoss[0m : 2.47795
[1mStep[0m  [30/53], [94mLoss[0m : 2.57650
[1mStep[0m  [35/53], [94mLoss[0m : 2.52923
[1mStep[0m  [40/53], [94mLoss[0m : 2.57091
[1mStep[0m  [45/53], [94mLoss[0m : 2.55865
[1mStep[0m  [50/53], [94mLoss[0m : 2.59908

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67905
[1mStep[0m  [5/53], [94mLoss[0m : 2.48918
[1mStep[0m  [10/53], [94mLoss[0m : 2.46392
[1mStep[0m  [15/53], [94mLoss[0m : 2.48754
[1mStep[0m  [20/53], [94mLoss[0m : 2.82692
[1mStep[0m  [25/53], [94mLoss[0m : 2.60143
[1mStep[0m  [30/53], [94mLoss[0m : 2.53787
[1mStep[0m  [35/53], [94mLoss[0m : 2.43753
[1mStep[0m  [40/53], [94mLoss[0m : 2.29756
[1mStep[0m  [45/53], [94mLoss[0m : 2.35841
[1mStep[0m  [50/53], [94mLoss[0m : 2.48063

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33282
[1mStep[0m  [5/53], [94mLoss[0m : 2.48217
[1mStep[0m  [10/53], [94mLoss[0m : 2.43470
[1mStep[0m  [15/53], [94mLoss[0m : 2.31095
[1mStep[0m  [20/53], [94mLoss[0m : 2.57353
[1mStep[0m  [25/53], [94mLoss[0m : 2.46502
[1mStep[0m  [30/53], [94mLoss[0m : 2.52973
[1mStep[0m  [35/53], [94mLoss[0m : 2.50251
[1mStep[0m  [40/53], [94mLoss[0m : 2.43348
[1mStep[0m  [45/53], [94mLoss[0m : 2.52366
[1mStep[0m  [50/53], [94mLoss[0m : 2.17026

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71637
[1mStep[0m  [5/53], [94mLoss[0m : 2.62711
[1mStep[0m  [10/53], [94mLoss[0m : 2.55681
[1mStep[0m  [15/53], [94mLoss[0m : 2.59242
[1mStep[0m  [20/53], [94mLoss[0m : 2.63083
[1mStep[0m  [25/53], [94mLoss[0m : 2.53005
[1mStep[0m  [30/53], [94mLoss[0m : 2.54058
[1mStep[0m  [35/53], [94mLoss[0m : 2.56608
[1mStep[0m  [40/53], [94mLoss[0m : 2.47033
[1mStep[0m  [45/53], [94mLoss[0m : 2.42326
[1mStep[0m  [50/53], [94mLoss[0m : 2.56457

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56312
[1mStep[0m  [5/53], [94mLoss[0m : 2.47964
[1mStep[0m  [10/53], [94mLoss[0m : 2.47247
[1mStep[0m  [15/53], [94mLoss[0m : 2.70743
[1mStep[0m  [20/53], [94mLoss[0m : 2.56818
[1mStep[0m  [25/53], [94mLoss[0m : 2.50214
[1mStep[0m  [30/53], [94mLoss[0m : 2.35520
[1mStep[0m  [35/53], [94mLoss[0m : 2.39903
[1mStep[0m  [40/53], [94mLoss[0m : 2.44067
[1mStep[0m  [45/53], [94mLoss[0m : 2.50603
[1mStep[0m  [50/53], [94mLoss[0m : 2.50519

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35651
[1mStep[0m  [5/53], [94mLoss[0m : 2.67945
[1mStep[0m  [10/53], [94mLoss[0m : 2.31057
[1mStep[0m  [15/53], [94mLoss[0m : 2.36459
[1mStep[0m  [20/53], [94mLoss[0m : 2.51018
[1mStep[0m  [25/53], [94mLoss[0m : 2.48837
[1mStep[0m  [30/53], [94mLoss[0m : 2.46319
[1mStep[0m  [35/53], [94mLoss[0m : 2.42160
[1mStep[0m  [40/53], [94mLoss[0m : 2.58790
[1mStep[0m  [45/53], [94mLoss[0m : 2.42315
[1mStep[0m  [50/53], [94mLoss[0m : 2.53104

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45635
[1mStep[0m  [5/53], [94mLoss[0m : 2.40624
[1mStep[0m  [10/53], [94mLoss[0m : 2.33353
[1mStep[0m  [15/53], [94mLoss[0m : 2.56806
[1mStep[0m  [20/53], [94mLoss[0m : 2.42471
[1mStep[0m  [25/53], [94mLoss[0m : 2.62194
[1mStep[0m  [30/53], [94mLoss[0m : 2.47139
[1mStep[0m  [35/53], [94mLoss[0m : 2.47259
[1mStep[0m  [40/53], [94mLoss[0m : 2.51097
[1mStep[0m  [45/53], [94mLoss[0m : 2.43277
[1mStep[0m  [50/53], [94mLoss[0m : 2.19861

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45491
[1mStep[0m  [5/53], [94mLoss[0m : 2.57923
[1mStep[0m  [10/53], [94mLoss[0m : 2.31869
[1mStep[0m  [15/53], [94mLoss[0m : 2.54082
[1mStep[0m  [20/53], [94mLoss[0m : 2.47377
[1mStep[0m  [25/53], [94mLoss[0m : 2.23135
[1mStep[0m  [30/53], [94mLoss[0m : 2.36971
[1mStep[0m  [35/53], [94mLoss[0m : 2.38982
[1mStep[0m  [40/53], [94mLoss[0m : 2.36475
[1mStep[0m  [45/53], [94mLoss[0m : 2.39243
[1mStep[0m  [50/53], [94mLoss[0m : 2.52563

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62741
[1mStep[0m  [5/53], [94mLoss[0m : 2.27958
[1mStep[0m  [10/53], [94mLoss[0m : 2.37533
[1mStep[0m  [15/53], [94mLoss[0m : 2.26689
[1mStep[0m  [20/53], [94mLoss[0m : 2.42214
[1mStep[0m  [25/53], [94mLoss[0m : 2.52001
[1mStep[0m  [30/53], [94mLoss[0m : 2.35967
[1mStep[0m  [35/53], [94mLoss[0m : 2.50373
[1mStep[0m  [40/53], [94mLoss[0m : 2.38327
[1mStep[0m  [45/53], [94mLoss[0m : 2.55302
[1mStep[0m  [50/53], [94mLoss[0m : 2.19211

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27454
[1mStep[0m  [5/53], [94mLoss[0m : 2.44753
[1mStep[0m  [10/53], [94mLoss[0m : 2.19187
[1mStep[0m  [15/53], [94mLoss[0m : 2.46851
[1mStep[0m  [20/53], [94mLoss[0m : 2.39153
[1mStep[0m  [25/53], [94mLoss[0m : 2.38626
[1mStep[0m  [30/53], [94mLoss[0m : 2.30370
[1mStep[0m  [35/53], [94mLoss[0m : 2.61641
[1mStep[0m  [40/53], [94mLoss[0m : 2.25410
[1mStep[0m  [45/53], [94mLoss[0m : 2.46287
[1mStep[0m  [50/53], [94mLoss[0m : 2.39886

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49636
[1mStep[0m  [5/53], [94mLoss[0m : 2.56035
[1mStep[0m  [10/53], [94mLoss[0m : 2.18352
[1mStep[0m  [15/53], [94mLoss[0m : 2.33255
[1mStep[0m  [20/53], [94mLoss[0m : 2.33048
[1mStep[0m  [25/53], [94mLoss[0m : 2.16791
[1mStep[0m  [30/53], [94mLoss[0m : 2.64063
[1mStep[0m  [35/53], [94mLoss[0m : 2.17574
[1mStep[0m  [40/53], [94mLoss[0m : 2.34138
[1mStep[0m  [45/53], [94mLoss[0m : 2.41024
[1mStep[0m  [50/53], [94mLoss[0m : 2.44948

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42480
[1mStep[0m  [5/53], [94mLoss[0m : 2.51001
[1mStep[0m  [10/53], [94mLoss[0m : 2.29268
[1mStep[0m  [15/53], [94mLoss[0m : 2.17872
[1mStep[0m  [20/53], [94mLoss[0m : 2.24313
[1mStep[0m  [25/53], [94mLoss[0m : 2.40422
[1mStep[0m  [30/53], [94mLoss[0m : 2.32885
[1mStep[0m  [35/53], [94mLoss[0m : 2.33588
[1mStep[0m  [40/53], [94mLoss[0m : 2.36244
[1mStep[0m  [45/53], [94mLoss[0m : 2.23552
[1mStep[0m  [50/53], [94mLoss[0m : 2.32124

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43144
[1mStep[0m  [5/53], [94mLoss[0m : 2.39252
[1mStep[0m  [10/53], [94mLoss[0m : 2.44146
[1mStep[0m  [15/53], [94mLoss[0m : 2.23491
[1mStep[0m  [20/53], [94mLoss[0m : 2.22702
[1mStep[0m  [25/53], [94mLoss[0m : 2.28238
[1mStep[0m  [30/53], [94mLoss[0m : 2.27540
[1mStep[0m  [35/53], [94mLoss[0m : 2.43718
[1mStep[0m  [40/53], [94mLoss[0m : 2.44410
[1mStep[0m  [45/53], [94mLoss[0m : 2.25433
[1mStep[0m  [50/53], [94mLoss[0m : 2.25663

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31250
[1mStep[0m  [5/53], [94mLoss[0m : 2.33097
[1mStep[0m  [10/53], [94mLoss[0m : 2.38108
[1mStep[0m  [15/53], [94mLoss[0m : 2.46178
[1mStep[0m  [20/53], [94mLoss[0m : 2.38594
[1mStep[0m  [25/53], [94mLoss[0m : 2.38489
[1mStep[0m  [30/53], [94mLoss[0m : 2.34122
[1mStep[0m  [35/53], [94mLoss[0m : 2.41082
[1mStep[0m  [40/53], [94mLoss[0m : 2.20609
[1mStep[0m  [45/53], [94mLoss[0m : 2.17838
[1mStep[0m  [50/53], [94mLoss[0m : 2.55280

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40401
[1mStep[0m  [5/53], [94mLoss[0m : 2.51443
[1mStep[0m  [10/53], [94mLoss[0m : 2.36513
[1mStep[0m  [15/53], [94mLoss[0m : 2.32358
[1mStep[0m  [20/53], [94mLoss[0m : 2.50890
[1mStep[0m  [25/53], [94mLoss[0m : 2.39877
[1mStep[0m  [30/53], [94mLoss[0m : 2.13748
[1mStep[0m  [35/53], [94mLoss[0m : 2.32227
[1mStep[0m  [40/53], [94mLoss[0m : 2.33308
[1mStep[0m  [45/53], [94mLoss[0m : 2.39637
[1mStep[0m  [50/53], [94mLoss[0m : 2.25928

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46023
[1mStep[0m  [5/53], [94mLoss[0m : 2.34589
[1mStep[0m  [10/53], [94mLoss[0m : 2.20789
[1mStep[0m  [15/53], [94mLoss[0m : 2.22187
[1mStep[0m  [20/53], [94mLoss[0m : 2.31999
[1mStep[0m  [25/53], [94mLoss[0m : 2.56090
[1mStep[0m  [30/53], [94mLoss[0m : 2.29375
[1mStep[0m  [35/53], [94mLoss[0m : 2.31924
[1mStep[0m  [40/53], [94mLoss[0m : 2.22831
[1mStep[0m  [45/53], [94mLoss[0m : 2.44442
[1mStep[0m  [50/53], [94mLoss[0m : 2.20463

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31967
[1mStep[0m  [5/53], [94mLoss[0m : 2.34978
[1mStep[0m  [10/53], [94mLoss[0m : 2.29077
[1mStep[0m  [15/53], [94mLoss[0m : 2.43000
[1mStep[0m  [20/53], [94mLoss[0m : 2.41212
[1mStep[0m  [25/53], [94mLoss[0m : 2.20762
[1mStep[0m  [30/53], [94mLoss[0m : 2.44177
[1mStep[0m  [35/53], [94mLoss[0m : 2.27884
[1mStep[0m  [40/53], [94mLoss[0m : 2.32308
[1mStep[0m  [45/53], [94mLoss[0m : 2.31799
[1mStep[0m  [50/53], [94mLoss[0m : 2.17800

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40563
[1mStep[0m  [5/53], [94mLoss[0m : 2.12904
[1mStep[0m  [10/53], [94mLoss[0m : 2.19296
[1mStep[0m  [15/53], [94mLoss[0m : 2.15470
[1mStep[0m  [20/53], [94mLoss[0m : 2.29094
[1mStep[0m  [25/53], [94mLoss[0m : 2.48627
[1mStep[0m  [30/53], [94mLoss[0m : 2.48039
[1mStep[0m  [35/53], [94mLoss[0m : 2.08803
[1mStep[0m  [40/53], [94mLoss[0m : 2.64463
[1mStep[0m  [45/53], [94mLoss[0m : 2.42002
[1mStep[0m  [50/53], [94mLoss[0m : 2.48754

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31803
[1mStep[0m  [5/53], [94mLoss[0m : 2.42966
[1mStep[0m  [10/53], [94mLoss[0m : 2.35245
[1mStep[0m  [15/53], [94mLoss[0m : 2.25183
[1mStep[0m  [20/53], [94mLoss[0m : 2.31081
[1mStep[0m  [25/53], [94mLoss[0m : 2.42932
[1mStep[0m  [30/53], [94mLoss[0m : 2.31920
[1mStep[0m  [35/53], [94mLoss[0m : 2.43803
[1mStep[0m  [40/53], [94mLoss[0m : 2.25935
[1mStep[0m  [45/53], [94mLoss[0m : 2.42637
[1mStep[0m  [50/53], [94mLoss[0m : 2.15148

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29130
[1mStep[0m  [5/53], [94mLoss[0m : 2.37221
[1mStep[0m  [10/53], [94mLoss[0m : 2.44858
[1mStep[0m  [15/53], [94mLoss[0m : 2.32139
[1mStep[0m  [20/53], [94mLoss[0m : 2.63702
[1mStep[0m  [25/53], [94mLoss[0m : 2.31587
[1mStep[0m  [30/53], [94mLoss[0m : 2.42866
[1mStep[0m  [35/53], [94mLoss[0m : 2.35875
[1mStep[0m  [40/53], [94mLoss[0m : 2.20356
[1mStep[0m  [45/53], [94mLoss[0m : 2.56826
[1mStep[0m  [50/53], [94mLoss[0m : 2.27608

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49629
[1mStep[0m  [5/53], [94mLoss[0m : 2.18838
[1mStep[0m  [10/53], [94mLoss[0m : 2.20927
[1mStep[0m  [15/53], [94mLoss[0m : 2.27826
[1mStep[0m  [20/53], [94mLoss[0m : 2.27103
[1mStep[0m  [25/53], [94mLoss[0m : 2.27512
[1mStep[0m  [30/53], [94mLoss[0m : 2.51819
[1mStep[0m  [35/53], [94mLoss[0m : 2.43290
[1mStep[0m  [40/53], [94mLoss[0m : 2.27399
[1mStep[0m  [45/53], [94mLoss[0m : 2.30708
[1mStep[0m  [50/53], [94mLoss[0m : 2.28750

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39925
[1mStep[0m  [5/53], [94mLoss[0m : 2.18289
[1mStep[0m  [10/53], [94mLoss[0m : 2.34301
[1mStep[0m  [15/53], [94mLoss[0m : 2.36919
[1mStep[0m  [20/53], [94mLoss[0m : 2.30325
[1mStep[0m  [25/53], [94mLoss[0m : 2.18393
[1mStep[0m  [30/53], [94mLoss[0m : 2.35708
[1mStep[0m  [35/53], [94mLoss[0m : 2.44612
[1mStep[0m  [40/53], [94mLoss[0m : 2.17788
[1mStep[0m  [45/53], [94mLoss[0m : 2.35036
[1mStep[0m  [50/53], [94mLoss[0m : 2.36091

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20836
[1mStep[0m  [5/53], [94mLoss[0m : 2.29702
[1mStep[0m  [10/53], [94mLoss[0m : 2.37393
[1mStep[0m  [15/53], [94mLoss[0m : 2.29872
[1mStep[0m  [20/53], [94mLoss[0m : 2.11764
[1mStep[0m  [25/53], [94mLoss[0m : 2.31415
[1mStep[0m  [30/53], [94mLoss[0m : 2.17798
[1mStep[0m  [35/53], [94mLoss[0m : 2.36417
[1mStep[0m  [40/53], [94mLoss[0m : 2.32505
[1mStep[0m  [45/53], [94mLoss[0m : 2.36692
[1mStep[0m  [50/53], [94mLoss[0m : 2.38676

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32523
[1mStep[0m  [5/53], [94mLoss[0m : 2.33464
[1mStep[0m  [10/53], [94mLoss[0m : 2.32915
[1mStep[0m  [15/53], [94mLoss[0m : 2.22639
[1mStep[0m  [20/53], [94mLoss[0m : 2.07675
[1mStep[0m  [25/53], [94mLoss[0m : 2.39464
[1mStep[0m  [30/53], [94mLoss[0m : 2.35460
[1mStep[0m  [35/53], [94mLoss[0m : 2.26042
[1mStep[0m  [40/53], [94mLoss[0m : 2.34266
[1mStep[0m  [45/53], [94mLoss[0m : 2.23885
[1mStep[0m  [50/53], [94mLoss[0m : 2.32456

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22678
[1mStep[0m  [5/53], [94mLoss[0m : 2.31755
[1mStep[0m  [10/53], [94mLoss[0m : 2.53186
[1mStep[0m  [15/53], [94mLoss[0m : 2.36122
[1mStep[0m  [20/53], [94mLoss[0m : 2.55326
[1mStep[0m  [25/53], [94mLoss[0m : 2.18057
[1mStep[0m  [30/53], [94mLoss[0m : 2.25357
[1mStep[0m  [35/53], [94mLoss[0m : 2.19542
[1mStep[0m  [40/53], [94mLoss[0m : 1.97301
[1mStep[0m  [45/53], [94mLoss[0m : 2.21992
[1mStep[0m  [50/53], [94mLoss[0m : 2.17157

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34932
[1mStep[0m  [5/53], [94mLoss[0m : 2.22184
[1mStep[0m  [10/53], [94mLoss[0m : 2.29427
[1mStep[0m  [15/53], [94mLoss[0m : 2.39503
[1mStep[0m  [20/53], [94mLoss[0m : 2.30736
[1mStep[0m  [25/53], [94mLoss[0m : 2.10056
[1mStep[0m  [30/53], [94mLoss[0m : 2.35386
[1mStep[0m  [35/53], [94mLoss[0m : 2.31552
[1mStep[0m  [40/53], [94mLoss[0m : 2.34301
[1mStep[0m  [45/53], [94mLoss[0m : 2.22755
[1mStep[0m  [50/53], [94mLoss[0m : 2.23479

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.312
====================================

Phase 1 - Evaluation MAE:  2.3120728180958676
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.33113
[1mStep[0m  [5/53], [94mLoss[0m : 2.38941
[1mStep[0m  [10/53], [94mLoss[0m : 2.23734
[1mStep[0m  [15/53], [94mLoss[0m : 2.47115
[1mStep[0m  [20/53], [94mLoss[0m : 2.53702
[1mStep[0m  [25/53], [94mLoss[0m : 2.40821
[1mStep[0m  [30/53], [94mLoss[0m : 2.30030
[1mStep[0m  [35/53], [94mLoss[0m : 2.46248
[1mStep[0m  [40/53], [94mLoss[0m : 2.39267
[1mStep[0m  [45/53], [94mLoss[0m : 2.50276
[1mStep[0m  [50/53], [94mLoss[0m : 2.28596

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.314, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.69383
[1mStep[0m  [5/53], [94mLoss[0m : 2.40916
[1mStep[0m  [10/53], [94mLoss[0m : 2.57356
[1mStep[0m  [15/53], [94mLoss[0m : 2.31557
[1mStep[0m  [20/53], [94mLoss[0m : 2.33306
[1mStep[0m  [25/53], [94mLoss[0m : 2.20949
[1mStep[0m  [30/53], [94mLoss[0m : 2.54245
[1mStep[0m  [35/53], [94mLoss[0m : 2.38629
[1mStep[0m  [40/53], [94mLoss[0m : 2.48190
[1mStep[0m  [45/53], [94mLoss[0m : 2.48704
[1mStep[0m  [50/53], [94mLoss[0m : 2.43110

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28731
[1mStep[0m  [5/53], [94mLoss[0m : 2.09388
[1mStep[0m  [10/53], [94mLoss[0m : 2.08360
[1mStep[0m  [15/53], [94mLoss[0m : 2.29399
[1mStep[0m  [20/53], [94mLoss[0m : 2.22886
[1mStep[0m  [25/53], [94mLoss[0m : 2.10926
[1mStep[0m  [30/53], [94mLoss[0m : 2.07200
[1mStep[0m  [35/53], [94mLoss[0m : 2.07102
[1mStep[0m  [40/53], [94mLoss[0m : 2.27583
[1mStep[0m  [45/53], [94mLoss[0m : 2.15391
[1mStep[0m  [50/53], [94mLoss[0m : 2.50187

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.00735
[1mStep[0m  [5/53], [94mLoss[0m : 2.20204
[1mStep[0m  [10/53], [94mLoss[0m : 2.17569
[1mStep[0m  [15/53], [94mLoss[0m : 2.21229
[1mStep[0m  [20/53], [94mLoss[0m : 2.13167
[1mStep[0m  [25/53], [94mLoss[0m : 2.29567
[1mStep[0m  [30/53], [94mLoss[0m : 2.22683
[1mStep[0m  [35/53], [94mLoss[0m : 2.21595
[1mStep[0m  [40/53], [94mLoss[0m : 2.38770
[1mStep[0m  [45/53], [94mLoss[0m : 2.23566
[1mStep[0m  [50/53], [94mLoss[0m : 2.22526

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.14135
[1mStep[0m  [5/53], [94mLoss[0m : 2.11827
[1mStep[0m  [10/53], [94mLoss[0m : 2.04804
[1mStep[0m  [15/53], [94mLoss[0m : 2.17441
[1mStep[0m  [20/53], [94mLoss[0m : 2.11816
[1mStep[0m  [25/53], [94mLoss[0m : 1.76890
[1mStep[0m  [30/53], [94mLoss[0m : 2.12291
[1mStep[0m  [35/53], [94mLoss[0m : 2.12486
[1mStep[0m  [40/53], [94mLoss[0m : 1.94795
[1mStep[0m  [45/53], [94mLoss[0m : 2.16776
[1mStep[0m  [50/53], [94mLoss[0m : 1.99071

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.77149
[1mStep[0m  [5/53], [94mLoss[0m : 1.95958
[1mStep[0m  [10/53], [94mLoss[0m : 1.89579
[1mStep[0m  [15/53], [94mLoss[0m : 2.15307
[1mStep[0m  [20/53], [94mLoss[0m : 2.00345
[1mStep[0m  [25/53], [94mLoss[0m : 1.94005
[1mStep[0m  [30/53], [94mLoss[0m : 2.05595
[1mStep[0m  [35/53], [94mLoss[0m : 2.07044
[1mStep[0m  [40/53], [94mLoss[0m : 2.16374
[1mStep[0m  [45/53], [94mLoss[0m : 2.12776
[1mStep[0m  [50/53], [94mLoss[0m : 2.01004

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78220
[1mStep[0m  [5/53], [94mLoss[0m : 2.18745
[1mStep[0m  [10/53], [94mLoss[0m : 1.84497
[1mStep[0m  [15/53], [94mLoss[0m : 1.85356
[1mStep[0m  [20/53], [94mLoss[0m : 2.15830
[1mStep[0m  [25/53], [94mLoss[0m : 1.77624
[1mStep[0m  [30/53], [94mLoss[0m : 1.95386
[1mStep[0m  [35/53], [94mLoss[0m : 1.97234
[1mStep[0m  [40/53], [94mLoss[0m : 1.92446
[1mStep[0m  [45/53], [94mLoss[0m : 2.08462
[1mStep[0m  [50/53], [94mLoss[0m : 1.97135

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.85797
[1mStep[0m  [5/53], [94mLoss[0m : 1.84698
[1mStep[0m  [10/53], [94mLoss[0m : 1.82074
[1mStep[0m  [15/53], [94mLoss[0m : 1.62579
[1mStep[0m  [20/53], [94mLoss[0m : 1.88424
[1mStep[0m  [25/53], [94mLoss[0m : 1.88523
[1mStep[0m  [30/53], [94mLoss[0m : 2.01576
[1mStep[0m  [35/53], [94mLoss[0m : 2.01859
[1mStep[0m  [40/53], [94mLoss[0m : 1.94618
[1mStep[0m  [45/53], [94mLoss[0m : 2.09802
[1mStep[0m  [50/53], [94mLoss[0m : 1.90300

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.77075
[1mStep[0m  [5/53], [94mLoss[0m : 1.89486
[1mStep[0m  [10/53], [94mLoss[0m : 1.79908
[1mStep[0m  [15/53], [94mLoss[0m : 1.88196
[1mStep[0m  [20/53], [94mLoss[0m : 1.77316
[1mStep[0m  [25/53], [94mLoss[0m : 1.83403
[1mStep[0m  [30/53], [94mLoss[0m : 1.74464
[1mStep[0m  [35/53], [94mLoss[0m : 2.11989
[1mStep[0m  [40/53], [94mLoss[0m : 1.88429
[1mStep[0m  [45/53], [94mLoss[0m : 1.81599
[1mStep[0m  [50/53], [94mLoss[0m : 2.18714

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.88247
[1mStep[0m  [5/53], [94mLoss[0m : 1.82460
[1mStep[0m  [10/53], [94mLoss[0m : 1.74533
[1mStep[0m  [15/53], [94mLoss[0m : 1.71295
[1mStep[0m  [20/53], [94mLoss[0m : 1.79078
[1mStep[0m  [25/53], [94mLoss[0m : 1.94830
[1mStep[0m  [30/53], [94mLoss[0m : 1.87063
[1mStep[0m  [35/53], [94mLoss[0m : 1.77929
[1mStep[0m  [40/53], [94mLoss[0m : 1.85079
[1mStep[0m  [45/53], [94mLoss[0m : 1.91837
[1mStep[0m  [50/53], [94mLoss[0m : 1.96351

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.74277
[1mStep[0m  [5/53], [94mLoss[0m : 1.75992
[1mStep[0m  [10/53], [94mLoss[0m : 1.66387
[1mStep[0m  [15/53], [94mLoss[0m : 1.57773
[1mStep[0m  [20/53], [94mLoss[0m : 1.79123
[1mStep[0m  [25/53], [94mLoss[0m : 1.71876
[1mStep[0m  [30/53], [94mLoss[0m : 1.70822
[1mStep[0m  [35/53], [94mLoss[0m : 1.84941
[1mStep[0m  [40/53], [94mLoss[0m : 1.77643
[1mStep[0m  [45/53], [94mLoss[0m : 1.69281
[1mStep[0m  [50/53], [94mLoss[0m : 1.69326

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80990
[1mStep[0m  [5/53], [94mLoss[0m : 1.76762
[1mStep[0m  [10/53], [94mLoss[0m : 1.65006
[1mStep[0m  [15/53], [94mLoss[0m : 1.63545
[1mStep[0m  [20/53], [94mLoss[0m : 1.62691
[1mStep[0m  [25/53], [94mLoss[0m : 1.75257
[1mStep[0m  [30/53], [94mLoss[0m : 1.77784
[1mStep[0m  [35/53], [94mLoss[0m : 1.69284
[1mStep[0m  [40/53], [94mLoss[0m : 1.76563
[1mStep[0m  [45/53], [94mLoss[0m : 1.87017
[1mStep[0m  [50/53], [94mLoss[0m : 1.85995

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.65888
[1mStep[0m  [5/53], [94mLoss[0m : 1.73949
[1mStep[0m  [10/53], [94mLoss[0m : 1.69574
[1mStep[0m  [15/53], [94mLoss[0m : 1.63443
[1mStep[0m  [20/53], [94mLoss[0m : 1.71092
[1mStep[0m  [25/53], [94mLoss[0m : 1.67061
[1mStep[0m  [30/53], [94mLoss[0m : 1.78289
[1mStep[0m  [35/53], [94mLoss[0m : 1.53837
[1mStep[0m  [40/53], [94mLoss[0m : 1.76953
[1mStep[0m  [45/53], [94mLoss[0m : 1.89278
[1mStep[0m  [50/53], [94mLoss[0m : 1.72432

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.53027
[1mStep[0m  [5/53], [94mLoss[0m : 1.72083
[1mStep[0m  [10/53], [94mLoss[0m : 1.57007
[1mStep[0m  [15/53], [94mLoss[0m : 1.53094
[1mStep[0m  [20/53], [94mLoss[0m : 1.60967
[1mStep[0m  [25/53], [94mLoss[0m : 1.70285
[1mStep[0m  [30/53], [94mLoss[0m : 1.65169
[1mStep[0m  [35/53], [94mLoss[0m : 1.76325
[1mStep[0m  [40/53], [94mLoss[0m : 1.61222
[1mStep[0m  [45/53], [94mLoss[0m : 1.69437
[1mStep[0m  [50/53], [94mLoss[0m : 1.71567

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54733
[1mStep[0m  [5/53], [94mLoss[0m : 1.47700
[1mStep[0m  [10/53], [94mLoss[0m : 1.46275
[1mStep[0m  [15/53], [94mLoss[0m : 1.66992
[1mStep[0m  [20/53], [94mLoss[0m : 1.52889
[1mStep[0m  [25/53], [94mLoss[0m : 1.58968
[1mStep[0m  [30/53], [94mLoss[0m : 1.52053
[1mStep[0m  [35/53], [94mLoss[0m : 1.56138
[1mStep[0m  [40/53], [94mLoss[0m : 1.59157
[1mStep[0m  [45/53], [94mLoss[0m : 1.59374
[1mStep[0m  [50/53], [94mLoss[0m : 1.61641

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.59853
[1mStep[0m  [5/53], [94mLoss[0m : 1.35480
[1mStep[0m  [10/53], [94mLoss[0m : 1.70772
[1mStep[0m  [15/53], [94mLoss[0m : 1.62392
[1mStep[0m  [20/53], [94mLoss[0m : 1.63521
[1mStep[0m  [25/53], [94mLoss[0m : 1.73870
[1mStep[0m  [30/53], [94mLoss[0m : 1.55415
[1mStep[0m  [35/53], [94mLoss[0m : 1.56338
[1mStep[0m  [40/53], [94mLoss[0m : 1.69084
[1mStep[0m  [45/53], [94mLoss[0m : 1.41875
[1mStep[0m  [50/53], [94mLoss[0m : 1.65780

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.528, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.62096
[1mStep[0m  [5/53], [94mLoss[0m : 1.47310
[1mStep[0m  [10/53], [94mLoss[0m : 1.52432
[1mStep[0m  [15/53], [94mLoss[0m : 1.48985
[1mStep[0m  [20/53], [94mLoss[0m : 1.61494
[1mStep[0m  [25/53], [94mLoss[0m : 1.69819
[1mStep[0m  [30/53], [94mLoss[0m : 1.50181
[1mStep[0m  [35/53], [94mLoss[0m : 1.74816
[1mStep[0m  [40/53], [94mLoss[0m : 1.61800
[1mStep[0m  [45/53], [94mLoss[0m : 1.46425
[1mStep[0m  [50/53], [94mLoss[0m : 1.77115

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.522, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.41011
[1mStep[0m  [5/53], [94mLoss[0m : 1.54549
[1mStep[0m  [10/53], [94mLoss[0m : 1.44634
[1mStep[0m  [15/53], [94mLoss[0m : 1.48339
[1mStep[0m  [20/53], [94mLoss[0m : 1.51692
[1mStep[0m  [25/53], [94mLoss[0m : 1.86004
[1mStep[0m  [30/53], [94mLoss[0m : 1.77859
[1mStep[0m  [35/53], [94mLoss[0m : 1.57152
[1mStep[0m  [40/53], [94mLoss[0m : 1.41069
[1mStep[0m  [45/53], [94mLoss[0m : 1.58099
[1mStep[0m  [50/53], [94mLoss[0m : 1.53136

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.50384
[1mStep[0m  [5/53], [94mLoss[0m : 1.50882
[1mStep[0m  [10/53], [94mLoss[0m : 1.39760
[1mStep[0m  [15/53], [94mLoss[0m : 1.58217
[1mStep[0m  [20/53], [94mLoss[0m : 1.34271
[1mStep[0m  [25/53], [94mLoss[0m : 1.62184
[1mStep[0m  [30/53], [94mLoss[0m : 1.69655
[1mStep[0m  [35/53], [94mLoss[0m : 1.37552
[1mStep[0m  [40/53], [94mLoss[0m : 1.47191
[1mStep[0m  [45/53], [94mLoss[0m : 1.42739
[1mStep[0m  [50/53], [94mLoss[0m : 1.65926

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.38541
[1mStep[0m  [5/53], [94mLoss[0m : 1.48350
[1mStep[0m  [10/53], [94mLoss[0m : 1.38133
[1mStep[0m  [15/53], [94mLoss[0m : 1.50445
[1mStep[0m  [20/53], [94mLoss[0m : 1.42925
[1mStep[0m  [25/53], [94mLoss[0m : 1.44211
[1mStep[0m  [30/53], [94mLoss[0m : 1.46915
[1mStep[0m  [35/53], [94mLoss[0m : 1.47682
[1mStep[0m  [40/53], [94mLoss[0m : 1.46693
[1mStep[0m  [45/53], [94mLoss[0m : 1.54565
[1mStep[0m  [50/53], [94mLoss[0m : 1.37622

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.472, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.26908
[1mStep[0m  [5/53], [94mLoss[0m : 1.45769
[1mStep[0m  [10/53], [94mLoss[0m : 1.46757
[1mStep[0m  [15/53], [94mLoss[0m : 1.43295
[1mStep[0m  [20/53], [94mLoss[0m : 1.52290
[1mStep[0m  [25/53], [94mLoss[0m : 1.40065
[1mStep[0m  [30/53], [94mLoss[0m : 1.31994
[1mStep[0m  [35/53], [94mLoss[0m : 1.55521
[1mStep[0m  [40/53], [94mLoss[0m : 1.42798
[1mStep[0m  [45/53], [94mLoss[0m : 1.33939
[1mStep[0m  [50/53], [94mLoss[0m : 1.39472

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.424, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.32215
[1mStep[0m  [5/53], [94mLoss[0m : 1.24978
[1mStep[0m  [10/53], [94mLoss[0m : 1.37818
[1mStep[0m  [15/53], [94mLoss[0m : 1.44317
[1mStep[0m  [20/53], [94mLoss[0m : 1.31697
[1mStep[0m  [25/53], [94mLoss[0m : 1.35077
[1mStep[0m  [30/53], [94mLoss[0m : 1.36661
[1mStep[0m  [35/53], [94mLoss[0m : 1.50148
[1mStep[0m  [40/53], [94mLoss[0m : 1.26751
[1mStep[0m  [45/53], [94mLoss[0m : 1.26372
[1mStep[0m  [50/53], [94mLoss[0m : 1.30934

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.369, [92mTest[0m: 2.547, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.30903
[1mStep[0m  [5/53], [94mLoss[0m : 1.24032
[1mStep[0m  [10/53], [94mLoss[0m : 1.31335
[1mStep[0m  [15/53], [94mLoss[0m : 1.27476
[1mStep[0m  [20/53], [94mLoss[0m : 1.43256
[1mStep[0m  [25/53], [94mLoss[0m : 1.35288
[1mStep[0m  [30/53], [94mLoss[0m : 1.27331
[1mStep[0m  [35/53], [94mLoss[0m : 1.34847
[1mStep[0m  [40/53], [94mLoss[0m : 1.35268
[1mStep[0m  [45/53], [94mLoss[0m : 1.41970
[1mStep[0m  [50/53], [94mLoss[0m : 1.32603

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.332, [92mTest[0m: 2.503, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.37810
[1mStep[0m  [5/53], [94mLoss[0m : 1.31146
[1mStep[0m  [10/53], [94mLoss[0m : 1.27927
[1mStep[0m  [15/53], [94mLoss[0m : 1.31668
[1mStep[0m  [20/53], [94mLoss[0m : 1.22647
[1mStep[0m  [25/53], [94mLoss[0m : 1.21410
[1mStep[0m  [30/53], [94mLoss[0m : 1.30795
[1mStep[0m  [35/53], [94mLoss[0m : 1.38869
[1mStep[0m  [40/53], [94mLoss[0m : 1.37306
[1mStep[0m  [45/53], [94mLoss[0m : 1.28466
[1mStep[0m  [50/53], [94mLoss[0m : 1.33221

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.335, [92mTest[0m: 2.551, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.504
====================================

Phase 2 - Evaluation MAE:  2.5037988057503333
MAE score P1        2.312073
MAE score P2        2.503799
loss                1.331802
learning_rate           0.01
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 10.86960
[1mStep[0m  [5/53], [94mLoss[0m : 10.98034
[1mStep[0m  [10/53], [94mLoss[0m : 10.66710
[1mStep[0m  [15/53], [94mLoss[0m : 10.66237
[1mStep[0m  [20/53], [94mLoss[0m : 10.71620
[1mStep[0m  [25/53], [94mLoss[0m : 10.33225
[1mStep[0m  [30/53], [94mLoss[0m : 10.40979
[1mStep[0m  [35/53], [94mLoss[0m : 10.36885
[1mStep[0m  [40/53], [94mLoss[0m : 10.12345
[1mStep[0m  [45/53], [94mLoss[0m : 10.24822
[1mStep[0m  [50/53], [94mLoss[0m : 10.29453

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.580, [92mTest[0m: 11.045, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.70221
[1mStep[0m  [5/53], [94mLoss[0m : 10.14084
[1mStep[0m  [10/53], [94mLoss[0m : 10.07668
[1mStep[0m  [15/53], [94mLoss[0m : 10.18508
[1mStep[0m  [20/53], [94mLoss[0m : 9.90289
[1mStep[0m  [25/53], [94mLoss[0m : 9.82268
[1mStep[0m  [30/53], [94mLoss[0m : 9.86143
[1mStep[0m  [35/53], [94mLoss[0m : 9.87584
[1mStep[0m  [40/53], [94mLoss[0m : 9.45139
[1mStep[0m  [45/53], [94mLoss[0m : 9.64759
[1mStep[0m  [50/53], [94mLoss[0m : 9.67513

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.841, [92mTest[0m: 10.115, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.35331
[1mStep[0m  [5/53], [94mLoss[0m : 9.22036
[1mStep[0m  [10/53], [94mLoss[0m : 9.18090
[1mStep[0m  [15/53], [94mLoss[0m : 9.38388
[1mStep[0m  [20/53], [94mLoss[0m : 9.13898
[1mStep[0m  [25/53], [94mLoss[0m : 8.72867
[1mStep[0m  [30/53], [94mLoss[0m : 8.93913
[1mStep[0m  [35/53], [94mLoss[0m : 9.04039
[1mStep[0m  [40/53], [94mLoss[0m : 8.64596
[1mStep[0m  [45/53], [94mLoss[0m : 8.84407
[1mStep[0m  [50/53], [94mLoss[0m : 8.67304

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.969, [92mTest[0m: 9.218, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.77689
[1mStep[0m  [5/53], [94mLoss[0m : 8.13450
[1mStep[0m  [10/53], [94mLoss[0m : 8.13993
[1mStep[0m  [15/53], [94mLoss[0m : 8.26159
[1mStep[0m  [20/53], [94mLoss[0m : 7.75733
[1mStep[0m  [25/53], [94mLoss[0m : 7.29970
[1mStep[0m  [30/53], [94mLoss[0m : 7.69653
[1mStep[0m  [35/53], [94mLoss[0m : 7.83562
[1mStep[0m  [40/53], [94mLoss[0m : 7.19043
[1mStep[0m  [45/53], [94mLoss[0m : 7.37442
[1mStep[0m  [50/53], [94mLoss[0m : 6.97520

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.798, [92mTest[0m: 8.054, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.12320
[1mStep[0m  [5/53], [94mLoss[0m : 7.02403
[1mStep[0m  [10/53], [94mLoss[0m : 6.81672
[1mStep[0m  [15/53], [94mLoss[0m : 6.55235
[1mStep[0m  [20/53], [94mLoss[0m : 6.42475
[1mStep[0m  [25/53], [94mLoss[0m : 6.18417
[1mStep[0m  [30/53], [94mLoss[0m : 6.48032
[1mStep[0m  [35/53], [94mLoss[0m : 6.07817
[1mStep[0m  [40/53], [94mLoss[0m : 5.83459
[1mStep[0m  [45/53], [94mLoss[0m : 5.86941
[1mStep[0m  [50/53], [94mLoss[0m : 5.92072

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.348, [92mTest[0m: 6.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.82514
[1mStep[0m  [5/53], [94mLoss[0m : 5.47934
[1mStep[0m  [10/53], [94mLoss[0m : 5.45433
[1mStep[0m  [15/53], [94mLoss[0m : 5.08039
[1mStep[0m  [20/53], [94mLoss[0m : 5.45897
[1mStep[0m  [25/53], [94mLoss[0m : 5.01917
[1mStep[0m  [30/53], [94mLoss[0m : 4.73230
[1mStep[0m  [35/53], [94mLoss[0m : 4.84662
[1mStep[0m  [40/53], [94mLoss[0m : 4.82457
[1mStep[0m  [45/53], [94mLoss[0m : 4.53252
[1mStep[0m  [50/53], [94mLoss[0m : 4.27249

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.986, [92mTest[0m: 4.825, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.34376
[1mStep[0m  [5/53], [94mLoss[0m : 3.98664
[1mStep[0m  [10/53], [94mLoss[0m : 4.07441
[1mStep[0m  [15/53], [94mLoss[0m : 4.28970
[1mStep[0m  [20/53], [94mLoss[0m : 3.53936
[1mStep[0m  [25/53], [94mLoss[0m : 3.76807
[1mStep[0m  [30/53], [94mLoss[0m : 3.43284
[1mStep[0m  [35/53], [94mLoss[0m : 3.57553
[1mStep[0m  [40/53], [94mLoss[0m : 3.50937
[1mStep[0m  [45/53], [94mLoss[0m : 3.56106
[1mStep[0m  [50/53], [94mLoss[0m : 3.28251

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.811, [92mTest[0m: 3.557, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.99843
[1mStep[0m  [5/53], [94mLoss[0m : 2.93883
[1mStep[0m  [10/53], [94mLoss[0m : 3.39957
[1mStep[0m  [15/53], [94mLoss[0m : 3.05157
[1mStep[0m  [20/53], [94mLoss[0m : 3.10566
[1mStep[0m  [25/53], [94mLoss[0m : 3.46129
[1mStep[0m  [30/53], [94mLoss[0m : 3.11816
[1mStep[0m  [35/53], [94mLoss[0m : 3.11501
[1mStep[0m  [40/53], [94mLoss[0m : 3.08408
[1mStep[0m  [45/53], [94mLoss[0m : 3.09043
[1mStep[0m  [50/53], [94mLoss[0m : 3.05509

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.107, [92mTest[0m: 2.862, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.07110
[1mStep[0m  [5/53], [94mLoss[0m : 2.85443
[1mStep[0m  [10/53], [94mLoss[0m : 2.55272
[1mStep[0m  [15/53], [94mLoss[0m : 2.86059
[1mStep[0m  [20/53], [94mLoss[0m : 2.89699
[1mStep[0m  [25/53], [94mLoss[0m : 2.87798
[1mStep[0m  [30/53], [94mLoss[0m : 3.03205
[1mStep[0m  [35/53], [94mLoss[0m : 3.00617
[1mStep[0m  [40/53], [94mLoss[0m : 2.60649
[1mStep[0m  [45/53], [94mLoss[0m : 2.99128
[1mStep[0m  [50/53], [94mLoss[0m : 2.73867

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.905, [92mTest[0m: 2.561, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.82432
[1mStep[0m  [5/53], [94mLoss[0m : 2.86802
[1mStep[0m  [10/53], [94mLoss[0m : 2.72751
[1mStep[0m  [15/53], [94mLoss[0m : 3.21497
[1mStep[0m  [20/53], [94mLoss[0m : 2.81832
[1mStep[0m  [25/53], [94mLoss[0m : 2.96225
[1mStep[0m  [30/53], [94mLoss[0m : 2.94787
[1mStep[0m  [35/53], [94mLoss[0m : 2.85380
[1mStep[0m  [40/53], [94mLoss[0m : 2.70262
[1mStep[0m  [45/53], [94mLoss[0m : 2.70926
[1mStep[0m  [50/53], [94mLoss[0m : 2.65544

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.828, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.99871
[1mStep[0m  [5/53], [94mLoss[0m : 2.73334
[1mStep[0m  [10/53], [94mLoss[0m : 2.69574
[1mStep[0m  [15/53], [94mLoss[0m : 2.83663
[1mStep[0m  [20/53], [94mLoss[0m : 2.80171
[1mStep[0m  [25/53], [94mLoss[0m : 3.01782
[1mStep[0m  [30/53], [94mLoss[0m : 2.67351
[1mStep[0m  [35/53], [94mLoss[0m : 2.81646
[1mStep[0m  [40/53], [94mLoss[0m : 2.81951
[1mStep[0m  [45/53], [94mLoss[0m : 2.98274
[1mStep[0m  [50/53], [94mLoss[0m : 2.76183

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.798, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71705
[1mStep[0m  [5/53], [94mLoss[0m : 2.64620
[1mStep[0m  [10/53], [94mLoss[0m : 2.81745
[1mStep[0m  [15/53], [94mLoss[0m : 2.70622
[1mStep[0m  [20/53], [94mLoss[0m : 2.91032
[1mStep[0m  [25/53], [94mLoss[0m : 2.70635
[1mStep[0m  [30/53], [94mLoss[0m : 2.85371
[1mStep[0m  [35/53], [94mLoss[0m : 2.89812
[1mStep[0m  [40/53], [94mLoss[0m : 2.84299
[1mStep[0m  [45/53], [94mLoss[0m : 2.79901
[1mStep[0m  [50/53], [94mLoss[0m : 2.83094

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.756, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63640
[1mStep[0m  [5/53], [94mLoss[0m : 2.89983
[1mStep[0m  [10/53], [94mLoss[0m : 2.82477
[1mStep[0m  [15/53], [94mLoss[0m : 2.78365
[1mStep[0m  [20/53], [94mLoss[0m : 2.80074
[1mStep[0m  [25/53], [94mLoss[0m : 2.64263
[1mStep[0m  [30/53], [94mLoss[0m : 2.81804
[1mStep[0m  [35/53], [94mLoss[0m : 2.77078
[1mStep[0m  [40/53], [94mLoss[0m : 2.80560
[1mStep[0m  [45/53], [94mLoss[0m : 2.52665
[1mStep[0m  [50/53], [94mLoss[0m : 2.70017

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.745, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70980
[1mStep[0m  [5/53], [94mLoss[0m : 2.88345
[1mStep[0m  [10/53], [94mLoss[0m : 2.62839
[1mStep[0m  [15/53], [94mLoss[0m : 2.76573
[1mStep[0m  [20/53], [94mLoss[0m : 2.93839
[1mStep[0m  [25/53], [94mLoss[0m : 2.81364
[1mStep[0m  [30/53], [94mLoss[0m : 2.68231
[1mStep[0m  [35/53], [94mLoss[0m : 2.65313
[1mStep[0m  [40/53], [94mLoss[0m : 2.82308
[1mStep[0m  [45/53], [94mLoss[0m : 2.73233
[1mStep[0m  [50/53], [94mLoss[0m : 2.72185

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54079
[1mStep[0m  [5/53], [94mLoss[0m : 2.59974
[1mStep[0m  [10/53], [94mLoss[0m : 2.82502
[1mStep[0m  [15/53], [94mLoss[0m : 2.81548
[1mStep[0m  [20/53], [94mLoss[0m : 2.60730
[1mStep[0m  [25/53], [94mLoss[0m : 2.71355
[1mStep[0m  [30/53], [94mLoss[0m : 2.79848
[1mStep[0m  [35/53], [94mLoss[0m : 2.88391
[1mStep[0m  [40/53], [94mLoss[0m : 2.68254
[1mStep[0m  [45/53], [94mLoss[0m : 2.67398
[1mStep[0m  [50/53], [94mLoss[0m : 3.12434

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.91776
[1mStep[0m  [5/53], [94mLoss[0m : 2.61374
[1mStep[0m  [10/53], [94mLoss[0m : 2.77964
[1mStep[0m  [15/53], [94mLoss[0m : 2.59914
[1mStep[0m  [20/53], [94mLoss[0m : 2.78962
[1mStep[0m  [25/53], [94mLoss[0m : 2.84764
[1mStep[0m  [30/53], [94mLoss[0m : 2.58187
[1mStep[0m  [35/53], [94mLoss[0m : 2.73397
[1mStep[0m  [40/53], [94mLoss[0m : 2.45238
[1mStep[0m  [45/53], [94mLoss[0m : 2.80018
[1mStep[0m  [50/53], [94mLoss[0m : 2.51181

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66041
[1mStep[0m  [5/53], [94mLoss[0m : 2.68738
[1mStep[0m  [10/53], [94mLoss[0m : 2.52061
[1mStep[0m  [15/53], [94mLoss[0m : 2.62529
[1mStep[0m  [20/53], [94mLoss[0m : 2.69014
[1mStep[0m  [25/53], [94mLoss[0m : 2.49737
[1mStep[0m  [30/53], [94mLoss[0m : 2.62201
[1mStep[0m  [35/53], [94mLoss[0m : 2.76780
[1mStep[0m  [40/53], [94mLoss[0m : 2.67412
[1mStep[0m  [45/53], [94mLoss[0m : 2.67693
[1mStep[0m  [50/53], [94mLoss[0m : 2.55715

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51709
[1mStep[0m  [5/53], [94mLoss[0m : 2.75822
[1mStep[0m  [10/53], [94mLoss[0m : 2.93561
[1mStep[0m  [15/53], [94mLoss[0m : 2.54396
[1mStep[0m  [20/53], [94mLoss[0m : 2.56554
[1mStep[0m  [25/53], [94mLoss[0m : 2.54023
[1mStep[0m  [30/53], [94mLoss[0m : 2.87103
[1mStep[0m  [35/53], [94mLoss[0m : 2.67920
[1mStep[0m  [40/53], [94mLoss[0m : 2.72450
[1mStep[0m  [45/53], [94mLoss[0m : 2.84564
[1mStep[0m  [50/53], [94mLoss[0m : 2.54409

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.704, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58798
[1mStep[0m  [5/53], [94mLoss[0m : 2.74610
[1mStep[0m  [10/53], [94mLoss[0m : 2.76818
[1mStep[0m  [15/53], [94mLoss[0m : 2.55292
[1mStep[0m  [20/53], [94mLoss[0m : 2.61529
[1mStep[0m  [25/53], [94mLoss[0m : 2.59178
[1mStep[0m  [30/53], [94mLoss[0m : 2.55269
[1mStep[0m  [35/53], [94mLoss[0m : 2.80081
[1mStep[0m  [40/53], [94mLoss[0m : 2.61839
[1mStep[0m  [45/53], [94mLoss[0m : 2.75092
[1mStep[0m  [50/53], [94mLoss[0m : 2.78032

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71237
[1mStep[0m  [5/53], [94mLoss[0m : 2.82906
[1mStep[0m  [10/53], [94mLoss[0m : 2.57770
[1mStep[0m  [15/53], [94mLoss[0m : 2.68952
[1mStep[0m  [20/53], [94mLoss[0m : 2.63847
[1mStep[0m  [25/53], [94mLoss[0m : 2.51456
[1mStep[0m  [30/53], [94mLoss[0m : 2.99684
[1mStep[0m  [35/53], [94mLoss[0m : 2.63898
[1mStep[0m  [40/53], [94mLoss[0m : 2.73523
[1mStep[0m  [45/53], [94mLoss[0m : 2.58622
[1mStep[0m  [50/53], [94mLoss[0m : 2.97591

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.399, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51900
[1mStep[0m  [5/53], [94mLoss[0m : 2.57947
[1mStep[0m  [10/53], [94mLoss[0m : 2.42219
[1mStep[0m  [15/53], [94mLoss[0m : 2.50858
[1mStep[0m  [20/53], [94mLoss[0m : 2.74214
[1mStep[0m  [25/53], [94mLoss[0m : 2.69583
[1mStep[0m  [30/53], [94mLoss[0m : 2.77090
[1mStep[0m  [35/53], [94mLoss[0m : 2.57272
[1mStep[0m  [40/53], [94mLoss[0m : 2.50585
[1mStep[0m  [45/53], [94mLoss[0m : 2.71300
[1mStep[0m  [50/53], [94mLoss[0m : 2.49712

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67184
[1mStep[0m  [5/53], [94mLoss[0m : 2.43841
[1mStep[0m  [10/53], [94mLoss[0m : 2.45069
[1mStep[0m  [15/53], [94mLoss[0m : 2.68960
[1mStep[0m  [20/53], [94mLoss[0m : 2.56451
[1mStep[0m  [25/53], [94mLoss[0m : 2.74217
[1mStep[0m  [30/53], [94mLoss[0m : 2.51368
[1mStep[0m  [35/53], [94mLoss[0m : 2.83216
[1mStep[0m  [40/53], [94mLoss[0m : 2.50997
[1mStep[0m  [45/53], [94mLoss[0m : 2.41740
[1mStep[0m  [50/53], [94mLoss[0m : 2.55778

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47216
[1mStep[0m  [5/53], [94mLoss[0m : 2.65708
[1mStep[0m  [10/53], [94mLoss[0m : 2.56768
[1mStep[0m  [15/53], [94mLoss[0m : 2.70052
[1mStep[0m  [20/53], [94mLoss[0m : 2.61256
[1mStep[0m  [25/53], [94mLoss[0m : 2.67476
[1mStep[0m  [30/53], [94mLoss[0m : 2.54721
[1mStep[0m  [35/53], [94mLoss[0m : 2.59212
[1mStep[0m  [40/53], [94mLoss[0m : 2.52095
[1mStep[0m  [45/53], [94mLoss[0m : 2.58333
[1mStep[0m  [50/53], [94mLoss[0m : 2.78953

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.80005
[1mStep[0m  [5/53], [94mLoss[0m : 2.53112
[1mStep[0m  [10/53], [94mLoss[0m : 2.71924
[1mStep[0m  [15/53], [94mLoss[0m : 2.44162
[1mStep[0m  [20/53], [94mLoss[0m : 2.67749
[1mStep[0m  [25/53], [94mLoss[0m : 2.53138
[1mStep[0m  [30/53], [94mLoss[0m : 2.62258
[1mStep[0m  [35/53], [94mLoss[0m : 2.73817
[1mStep[0m  [40/53], [94mLoss[0m : 2.60967
[1mStep[0m  [45/53], [94mLoss[0m : 2.60769
[1mStep[0m  [50/53], [94mLoss[0m : 2.50192

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51790
[1mStep[0m  [5/53], [94mLoss[0m : 2.67520
[1mStep[0m  [10/53], [94mLoss[0m : 2.63417
[1mStep[0m  [15/53], [94mLoss[0m : 2.53039
[1mStep[0m  [20/53], [94mLoss[0m : 2.77740
[1mStep[0m  [25/53], [94mLoss[0m : 2.63665
[1mStep[0m  [30/53], [94mLoss[0m : 2.48440
[1mStep[0m  [35/53], [94mLoss[0m : 2.62013
[1mStep[0m  [40/53], [94mLoss[0m : 2.63975
[1mStep[0m  [45/53], [94mLoss[0m : 2.71309
[1mStep[0m  [50/53], [94mLoss[0m : 2.54659

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51437
[1mStep[0m  [5/53], [94mLoss[0m : 2.72505
[1mStep[0m  [10/53], [94mLoss[0m : 2.61597
[1mStep[0m  [15/53], [94mLoss[0m : 2.66003
[1mStep[0m  [20/53], [94mLoss[0m : 2.57827
[1mStep[0m  [25/53], [94mLoss[0m : 2.67952
[1mStep[0m  [30/53], [94mLoss[0m : 2.70114
[1mStep[0m  [35/53], [94mLoss[0m : 2.84336
[1mStep[0m  [40/53], [94mLoss[0m : 2.71699
[1mStep[0m  [45/53], [94mLoss[0m : 2.69262
[1mStep[0m  [50/53], [94mLoss[0m : 2.77352

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.397, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56101
[1mStep[0m  [5/53], [94mLoss[0m : 2.54465
[1mStep[0m  [10/53], [94mLoss[0m : 2.75701
[1mStep[0m  [15/53], [94mLoss[0m : 2.80487
[1mStep[0m  [20/53], [94mLoss[0m : 2.58718
[1mStep[0m  [25/53], [94mLoss[0m : 2.62888
[1mStep[0m  [30/53], [94mLoss[0m : 2.67522
[1mStep[0m  [35/53], [94mLoss[0m : 2.91166
[1mStep[0m  [40/53], [94mLoss[0m : 2.80385
[1mStep[0m  [45/53], [94mLoss[0m : 2.75451
[1mStep[0m  [50/53], [94mLoss[0m : 2.62187

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37610
[1mStep[0m  [5/53], [94mLoss[0m : 2.50556
[1mStep[0m  [10/53], [94mLoss[0m : 2.44977
[1mStep[0m  [15/53], [94mLoss[0m : 2.66350
[1mStep[0m  [20/53], [94mLoss[0m : 2.78740
[1mStep[0m  [25/53], [94mLoss[0m : 2.68179
[1mStep[0m  [30/53], [94mLoss[0m : 2.85187
[1mStep[0m  [35/53], [94mLoss[0m : 2.42717
[1mStep[0m  [40/53], [94mLoss[0m : 2.75797
[1mStep[0m  [45/53], [94mLoss[0m : 2.79389
[1mStep[0m  [50/53], [94mLoss[0m : 2.62558

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.374, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.74063
[1mStep[0m  [5/53], [94mLoss[0m : 2.53845
[1mStep[0m  [10/53], [94mLoss[0m : 2.75875
[1mStep[0m  [15/53], [94mLoss[0m : 2.52013
[1mStep[0m  [20/53], [94mLoss[0m : 2.61895
[1mStep[0m  [25/53], [94mLoss[0m : 2.84181
[1mStep[0m  [30/53], [94mLoss[0m : 2.59510
[1mStep[0m  [35/53], [94mLoss[0m : 2.76942
[1mStep[0m  [40/53], [94mLoss[0m : 2.75254
[1mStep[0m  [45/53], [94mLoss[0m : 2.52361
[1mStep[0m  [50/53], [94mLoss[0m : 2.62691

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.376, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47804
[1mStep[0m  [5/53], [94mLoss[0m : 2.58858
[1mStep[0m  [10/53], [94mLoss[0m : 2.76815
[1mStep[0m  [15/53], [94mLoss[0m : 2.58577
[1mStep[0m  [20/53], [94mLoss[0m : 2.64255
[1mStep[0m  [25/53], [94mLoss[0m : 2.40383
[1mStep[0m  [30/53], [94mLoss[0m : 2.69375
[1mStep[0m  [35/53], [94mLoss[0m : 2.83460
[1mStep[0m  [40/53], [94mLoss[0m : 2.78507
[1mStep[0m  [45/53], [94mLoss[0m : 2.45078
[1mStep[0m  [50/53], [94mLoss[0m : 2.69994

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.383
====================================

Phase 1 - Evaluation MAE:  2.383010369080764
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.48064
[1mStep[0m  [5/53], [94mLoss[0m : 2.78692
[1mStep[0m  [10/53], [94mLoss[0m : 2.54003
[1mStep[0m  [15/53], [94mLoss[0m : 2.64174
[1mStep[0m  [20/53], [94mLoss[0m : 2.39004
[1mStep[0m  [25/53], [94mLoss[0m : 2.92343
[1mStep[0m  [30/53], [94mLoss[0m : 2.47371
[1mStep[0m  [35/53], [94mLoss[0m : 2.58913
[1mStep[0m  [40/53], [94mLoss[0m : 2.54942
[1mStep[0m  [45/53], [94mLoss[0m : 2.46070
[1mStep[0m  [50/53], [94mLoss[0m : 2.55127

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71925
[1mStep[0m  [5/53], [94mLoss[0m : 2.73263
[1mStep[0m  [10/53], [94mLoss[0m : 2.70864
[1mStep[0m  [15/53], [94mLoss[0m : 2.71139
[1mStep[0m  [20/53], [94mLoss[0m : 2.55154
[1mStep[0m  [25/53], [94mLoss[0m : 2.57888
[1mStep[0m  [30/53], [94mLoss[0m : 2.46480
[1mStep[0m  [35/53], [94mLoss[0m : 2.57399
[1mStep[0m  [40/53], [94mLoss[0m : 2.47676
[1mStep[0m  [45/53], [94mLoss[0m : 2.78096
[1mStep[0m  [50/53], [94mLoss[0m : 2.62224

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53228
[1mStep[0m  [5/53], [94mLoss[0m : 2.66077
[1mStep[0m  [10/53], [94mLoss[0m : 2.49453
[1mStep[0m  [15/53], [94mLoss[0m : 2.54060
[1mStep[0m  [20/53], [94mLoss[0m : 2.28929
[1mStep[0m  [25/53], [94mLoss[0m : 2.72218
[1mStep[0m  [30/53], [94mLoss[0m : 2.85351
[1mStep[0m  [35/53], [94mLoss[0m : 2.70591
[1mStep[0m  [40/53], [94mLoss[0m : 2.58228
[1mStep[0m  [45/53], [94mLoss[0m : 2.67440
[1mStep[0m  [50/53], [94mLoss[0m : 2.42126

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27333
[1mStep[0m  [5/53], [94mLoss[0m : 2.49241
[1mStep[0m  [10/53], [94mLoss[0m : 2.52418
[1mStep[0m  [15/53], [94mLoss[0m : 2.70804
[1mStep[0m  [20/53], [94mLoss[0m : 2.45009
[1mStep[0m  [25/53], [94mLoss[0m : 2.55016
[1mStep[0m  [30/53], [94mLoss[0m : 2.41965
[1mStep[0m  [35/53], [94mLoss[0m : 2.48391
[1mStep[0m  [40/53], [94mLoss[0m : 2.37812
[1mStep[0m  [45/53], [94mLoss[0m : 2.75668
[1mStep[0m  [50/53], [94mLoss[0m : 2.62267

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53141
[1mStep[0m  [5/53], [94mLoss[0m : 2.39813
[1mStep[0m  [10/53], [94mLoss[0m : 2.55358
[1mStep[0m  [15/53], [94mLoss[0m : 2.59046
[1mStep[0m  [20/53], [94mLoss[0m : 2.54793
[1mStep[0m  [25/53], [94mLoss[0m : 2.39159
[1mStep[0m  [30/53], [94mLoss[0m : 2.54180
[1mStep[0m  [35/53], [94mLoss[0m : 2.32329
[1mStep[0m  [40/53], [94mLoss[0m : 2.65820
[1mStep[0m  [45/53], [94mLoss[0m : 2.47117
[1mStep[0m  [50/53], [94mLoss[0m : 2.59786

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64950
[1mStep[0m  [5/53], [94mLoss[0m : 2.27124
[1mStep[0m  [10/53], [94mLoss[0m : 2.58923
[1mStep[0m  [15/53], [94mLoss[0m : 2.35381
[1mStep[0m  [20/53], [94mLoss[0m : 2.45894
[1mStep[0m  [25/53], [94mLoss[0m : 2.75998
[1mStep[0m  [30/53], [94mLoss[0m : 2.50584
[1mStep[0m  [35/53], [94mLoss[0m : 2.20345
[1mStep[0m  [40/53], [94mLoss[0m : 2.58936
[1mStep[0m  [45/53], [94mLoss[0m : 2.46594
[1mStep[0m  [50/53], [94mLoss[0m : 2.58434

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50990
[1mStep[0m  [5/53], [94mLoss[0m : 2.29215
[1mStep[0m  [10/53], [94mLoss[0m : 2.54222
[1mStep[0m  [15/53], [94mLoss[0m : 2.35930
[1mStep[0m  [20/53], [94mLoss[0m : 2.31506
[1mStep[0m  [25/53], [94mLoss[0m : 2.31238
[1mStep[0m  [30/53], [94mLoss[0m : 2.66359
[1mStep[0m  [35/53], [94mLoss[0m : 2.40600
[1mStep[0m  [40/53], [94mLoss[0m : 2.42820
[1mStep[0m  [45/53], [94mLoss[0m : 2.27017
[1mStep[0m  [50/53], [94mLoss[0m : 2.33054

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24138
[1mStep[0m  [5/53], [94mLoss[0m : 2.60025
[1mStep[0m  [10/53], [94mLoss[0m : 2.56771
[1mStep[0m  [15/53], [94mLoss[0m : 2.35694
[1mStep[0m  [20/53], [94mLoss[0m : 2.46978
[1mStep[0m  [25/53], [94mLoss[0m : 2.49845
[1mStep[0m  [30/53], [94mLoss[0m : 2.35563
[1mStep[0m  [35/53], [94mLoss[0m : 2.33291
[1mStep[0m  [40/53], [94mLoss[0m : 2.41670
[1mStep[0m  [45/53], [94mLoss[0m : 2.54359
[1mStep[0m  [50/53], [94mLoss[0m : 2.62461

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.23736
[1mStep[0m  [5/53], [94mLoss[0m : 2.21366
[1mStep[0m  [10/53], [94mLoss[0m : 2.33695
[1mStep[0m  [15/53], [94mLoss[0m : 2.43365
[1mStep[0m  [20/53], [94mLoss[0m : 2.15311
[1mStep[0m  [25/53], [94mLoss[0m : 2.59161
[1mStep[0m  [30/53], [94mLoss[0m : 2.23560
[1mStep[0m  [35/53], [94mLoss[0m : 2.37311
[1mStep[0m  [40/53], [94mLoss[0m : 2.44394
[1mStep[0m  [45/53], [94mLoss[0m : 2.56341
[1mStep[0m  [50/53], [94mLoss[0m : 2.34860

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21002
[1mStep[0m  [5/53], [94mLoss[0m : 2.33087
[1mStep[0m  [10/53], [94mLoss[0m : 2.11135
[1mStep[0m  [15/53], [94mLoss[0m : 2.26020
[1mStep[0m  [20/53], [94mLoss[0m : 2.31984
[1mStep[0m  [25/53], [94mLoss[0m : 2.46355
[1mStep[0m  [30/53], [94mLoss[0m : 2.34937
[1mStep[0m  [35/53], [94mLoss[0m : 2.38675
[1mStep[0m  [40/53], [94mLoss[0m : 2.36082
[1mStep[0m  [45/53], [94mLoss[0m : 2.30892
[1mStep[0m  [50/53], [94mLoss[0m : 2.21542

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.12597
[1mStep[0m  [5/53], [94mLoss[0m : 2.36127
[1mStep[0m  [10/53], [94mLoss[0m : 2.30484
[1mStep[0m  [15/53], [94mLoss[0m : 2.15544
[1mStep[0m  [20/53], [94mLoss[0m : 2.20560
[1mStep[0m  [25/53], [94mLoss[0m : 2.13419
[1mStep[0m  [30/53], [94mLoss[0m : 2.22388
[1mStep[0m  [35/53], [94mLoss[0m : 2.14363
[1mStep[0m  [40/53], [94mLoss[0m : 2.22075
[1mStep[0m  [45/53], [94mLoss[0m : 2.31164
[1mStep[0m  [50/53], [94mLoss[0m : 2.18711

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22169
[1mStep[0m  [5/53], [94mLoss[0m : 1.98463
[1mStep[0m  [10/53], [94mLoss[0m : 2.38897
[1mStep[0m  [15/53], [94mLoss[0m : 2.32818
[1mStep[0m  [20/53], [94mLoss[0m : 2.22820
[1mStep[0m  [25/53], [94mLoss[0m : 2.14749
[1mStep[0m  [30/53], [94mLoss[0m : 2.37516
[1mStep[0m  [35/53], [94mLoss[0m : 2.09837
[1mStep[0m  [40/53], [94mLoss[0m : 2.07573
[1mStep[0m  [45/53], [94mLoss[0m : 2.12481
[1mStep[0m  [50/53], [94mLoss[0m : 2.50535

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.238, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.16155
[1mStep[0m  [5/53], [94mLoss[0m : 2.34315
[1mStep[0m  [10/53], [94mLoss[0m : 2.12653
[1mStep[0m  [15/53], [94mLoss[0m : 2.16718
[1mStep[0m  [20/53], [94mLoss[0m : 2.28811
[1mStep[0m  [25/53], [94mLoss[0m : 2.09107
[1mStep[0m  [30/53], [94mLoss[0m : 2.15751
[1mStep[0m  [35/53], [94mLoss[0m : 2.26279
[1mStep[0m  [40/53], [94mLoss[0m : 2.30990
[1mStep[0m  [45/53], [94mLoss[0m : 2.58825
[1mStep[0m  [50/53], [94mLoss[0m : 2.25356

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22529
[1mStep[0m  [5/53], [94mLoss[0m : 2.01141
[1mStep[0m  [10/53], [94mLoss[0m : 2.14595
[1mStep[0m  [15/53], [94mLoss[0m : 2.21613
[1mStep[0m  [20/53], [94mLoss[0m : 2.20321
[1mStep[0m  [25/53], [94mLoss[0m : 2.17578
[1mStep[0m  [30/53], [94mLoss[0m : 2.20457
[1mStep[0m  [35/53], [94mLoss[0m : 2.06919
[1mStep[0m  [40/53], [94mLoss[0m : 2.06229
[1mStep[0m  [45/53], [94mLoss[0m : 2.29988
[1mStep[0m  [50/53], [94mLoss[0m : 2.19812

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.11054
[1mStep[0m  [5/53], [94mLoss[0m : 2.01465
[1mStep[0m  [10/53], [94mLoss[0m : 2.13615
[1mStep[0m  [15/53], [94mLoss[0m : 2.07767
[1mStep[0m  [20/53], [94mLoss[0m : 2.24523
[1mStep[0m  [25/53], [94mLoss[0m : 2.35550
[1mStep[0m  [30/53], [94mLoss[0m : 1.99050
[1mStep[0m  [35/53], [94mLoss[0m : 2.25162
[1mStep[0m  [40/53], [94mLoss[0m : 2.10942
[1mStep[0m  [45/53], [94mLoss[0m : 2.08121
[1mStep[0m  [50/53], [94mLoss[0m : 2.22721

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.12738
[1mStep[0m  [5/53], [94mLoss[0m : 2.19421
[1mStep[0m  [10/53], [94mLoss[0m : 2.18016
[1mStep[0m  [15/53], [94mLoss[0m : 2.18060
[1mStep[0m  [20/53], [94mLoss[0m : 2.09208
[1mStep[0m  [25/53], [94mLoss[0m : 1.97904
[1mStep[0m  [30/53], [94mLoss[0m : 2.17870
[1mStep[0m  [35/53], [94mLoss[0m : 2.16900
[1mStep[0m  [40/53], [94mLoss[0m : 2.09126
[1mStep[0m  [45/53], [94mLoss[0m : 2.22407
[1mStep[0m  [50/53], [94mLoss[0m : 1.94971

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17497
[1mStep[0m  [5/53], [94mLoss[0m : 2.15999
[1mStep[0m  [10/53], [94mLoss[0m : 2.08204
[1mStep[0m  [15/53], [94mLoss[0m : 2.09530
[1mStep[0m  [20/53], [94mLoss[0m : 1.98330
[1mStep[0m  [25/53], [94mLoss[0m : 2.18245
[1mStep[0m  [30/53], [94mLoss[0m : 2.27095
[1mStep[0m  [35/53], [94mLoss[0m : 2.17449
[1mStep[0m  [40/53], [94mLoss[0m : 2.02952
[1mStep[0m  [45/53], [94mLoss[0m : 1.98507
[1mStep[0m  [50/53], [94mLoss[0m : 2.06366

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.96148
[1mStep[0m  [5/53], [94mLoss[0m : 2.13563
[1mStep[0m  [10/53], [94mLoss[0m : 1.90774
[1mStep[0m  [15/53], [94mLoss[0m : 1.89701
[1mStep[0m  [20/53], [94mLoss[0m : 2.28684
[1mStep[0m  [25/53], [94mLoss[0m : 2.03580
[1mStep[0m  [30/53], [94mLoss[0m : 2.16217
[1mStep[0m  [35/53], [94mLoss[0m : 2.04423
[1mStep[0m  [40/53], [94mLoss[0m : 1.98975
[1mStep[0m  [45/53], [94mLoss[0m : 2.19324
[1mStep[0m  [50/53], [94mLoss[0m : 2.23511

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.11854
[1mStep[0m  [5/53], [94mLoss[0m : 2.15011
[1mStep[0m  [10/53], [94mLoss[0m : 1.90239
[1mStep[0m  [15/53], [94mLoss[0m : 1.95187
[1mStep[0m  [20/53], [94mLoss[0m : 1.99991
[1mStep[0m  [25/53], [94mLoss[0m : 2.20723
[1mStep[0m  [30/53], [94mLoss[0m : 1.98250
[1mStep[0m  [35/53], [94mLoss[0m : 2.22697
[1mStep[0m  [40/53], [94mLoss[0m : 2.07044
[1mStep[0m  [45/53], [94mLoss[0m : 1.98519
[1mStep[0m  [50/53], [94mLoss[0m : 2.01743

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.017, [92mTest[0m: 2.500, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.13212
[1mStep[0m  [5/53], [94mLoss[0m : 1.95184
[1mStep[0m  [10/53], [94mLoss[0m : 1.86227
[1mStep[0m  [15/53], [94mLoss[0m : 1.99454
[1mStep[0m  [20/53], [94mLoss[0m : 2.22886
[1mStep[0m  [25/53], [94mLoss[0m : 1.93677
[1mStep[0m  [30/53], [94mLoss[0m : 2.02925
[1mStep[0m  [35/53], [94mLoss[0m : 1.93939
[1mStep[0m  [40/53], [94mLoss[0m : 2.00993
[1mStep[0m  [45/53], [94mLoss[0m : 1.96359
[1mStep[0m  [50/53], [94mLoss[0m : 1.87668

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.92232
[1mStep[0m  [5/53], [94mLoss[0m : 1.95256
[1mStep[0m  [10/53], [94mLoss[0m : 1.95190
[1mStep[0m  [15/53], [94mLoss[0m : 2.05731
[1mStep[0m  [20/53], [94mLoss[0m : 1.90640
[1mStep[0m  [25/53], [94mLoss[0m : 2.06403
[1mStep[0m  [30/53], [94mLoss[0m : 1.92531
[1mStep[0m  [35/53], [94mLoss[0m : 1.91639
[1mStep[0m  [40/53], [94mLoss[0m : 1.99828
[1mStep[0m  [45/53], [94mLoss[0m : 1.86149
[1mStep[0m  [50/53], [94mLoss[0m : 2.12803

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.432, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80505
[1mStep[0m  [5/53], [94mLoss[0m : 1.81208
[1mStep[0m  [10/53], [94mLoss[0m : 1.84240
[1mStep[0m  [15/53], [94mLoss[0m : 1.95730
[1mStep[0m  [20/53], [94mLoss[0m : 1.91325
[1mStep[0m  [25/53], [94mLoss[0m : 1.75052
[1mStep[0m  [30/53], [94mLoss[0m : 1.89214
[1mStep[0m  [35/53], [94mLoss[0m : 2.11246
[1mStep[0m  [40/53], [94mLoss[0m : 2.00743
[1mStep[0m  [45/53], [94mLoss[0m : 1.87247
[1mStep[0m  [50/53], [94mLoss[0m : 1.75347

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.90344
[1mStep[0m  [5/53], [94mLoss[0m : 2.00408
[1mStep[0m  [10/53], [94mLoss[0m : 2.03558
[1mStep[0m  [15/53], [94mLoss[0m : 1.72217
[1mStep[0m  [20/53], [94mLoss[0m : 1.76048
[1mStep[0m  [25/53], [94mLoss[0m : 2.15693
[1mStep[0m  [30/53], [94mLoss[0m : 1.86107
[1mStep[0m  [35/53], [94mLoss[0m : 1.78763
[1mStep[0m  [40/53], [94mLoss[0m : 1.93011
[1mStep[0m  [45/53], [94mLoss[0m : 1.98784
[1mStep[0m  [50/53], [94mLoss[0m : 1.97301

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.75337
[1mStep[0m  [5/53], [94mLoss[0m : 1.90680
[1mStep[0m  [10/53], [94mLoss[0m : 1.99005
[1mStep[0m  [15/53], [94mLoss[0m : 2.03018
[1mStep[0m  [20/53], [94mLoss[0m : 1.77244
[1mStep[0m  [25/53], [94mLoss[0m : 1.79541
[1mStep[0m  [30/53], [94mLoss[0m : 1.88270
[1mStep[0m  [35/53], [94mLoss[0m : 1.93652
[1mStep[0m  [40/53], [94mLoss[0m : 1.87781
[1mStep[0m  [45/53], [94mLoss[0m : 1.84267
[1mStep[0m  [50/53], [94mLoss[0m : 1.99841

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.460, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.77248
[1mStep[0m  [5/53], [94mLoss[0m : 1.80893
[1mStep[0m  [10/53], [94mLoss[0m : 1.81885
[1mStep[0m  [15/53], [94mLoss[0m : 2.03634
[1mStep[0m  [20/53], [94mLoss[0m : 1.82898
[1mStep[0m  [25/53], [94mLoss[0m : 1.84296
[1mStep[0m  [30/53], [94mLoss[0m : 1.90562
[1mStep[0m  [35/53], [94mLoss[0m : 1.69912
[1mStep[0m  [40/53], [94mLoss[0m : 1.93624
[1mStep[0m  [45/53], [94mLoss[0m : 1.88118
[1mStep[0m  [50/53], [94mLoss[0m : 1.77873

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.450, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.77582
[1mStep[0m  [5/53], [94mLoss[0m : 1.65158
[1mStep[0m  [10/53], [94mLoss[0m : 1.76041
[1mStep[0m  [15/53], [94mLoss[0m : 1.66144
[1mStep[0m  [20/53], [94mLoss[0m : 1.95785
[1mStep[0m  [25/53], [94mLoss[0m : 1.75780
[1mStep[0m  [30/53], [94mLoss[0m : 1.99285
[1mStep[0m  [35/53], [94mLoss[0m : 1.78505
[1mStep[0m  [40/53], [94mLoss[0m : 1.88382
[1mStep[0m  [45/53], [94mLoss[0m : 2.06107
[1mStep[0m  [50/53], [94mLoss[0m : 2.01432

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.498, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86438
[1mStep[0m  [5/53], [94mLoss[0m : 1.89225
[1mStep[0m  [10/53], [94mLoss[0m : 1.93878
[1mStep[0m  [15/53], [94mLoss[0m : 1.91081
[1mStep[0m  [20/53], [94mLoss[0m : 1.81954
[1mStep[0m  [25/53], [94mLoss[0m : 1.70739
[1mStep[0m  [30/53], [94mLoss[0m : 1.91985
[1mStep[0m  [35/53], [94mLoss[0m : 1.84409
[1mStep[0m  [40/53], [94mLoss[0m : 1.87596
[1mStep[0m  [45/53], [94mLoss[0m : 1.83503
[1mStep[0m  [50/53], [94mLoss[0m : 1.86383

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.423, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86457
