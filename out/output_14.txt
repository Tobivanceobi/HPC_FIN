no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  14
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.02863
[1mStep[0m  [4/42], [94mLoss[0m : 7.33039
[1mStep[0m  [8/42], [94mLoss[0m : 3.08011
[1mStep[0m  [12/42], [94mLoss[0m : 2.73262
[1mStep[0m  [16/42], [94mLoss[0m : 2.70693
[1mStep[0m  [20/42], [94mLoss[0m : 2.62685
[1mStep[0m  [24/42], [94mLoss[0m : 2.28856
[1mStep[0m  [28/42], [94mLoss[0m : 2.71797
[1mStep[0m  [32/42], [94mLoss[0m : 2.42739
[1mStep[0m  [36/42], [94mLoss[0m : 2.48933
[1mStep[0m  [40/42], [94mLoss[0m : 2.46821

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.579, [92mTest[0m: 10.939, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68927
[1mStep[0m  [4/42], [94mLoss[0m : 2.53531
[1mStep[0m  [8/42], [94mLoss[0m : 2.46886
[1mStep[0m  [12/42], [94mLoss[0m : 2.47027
[1mStep[0m  [16/42], [94mLoss[0m : 2.50029
[1mStep[0m  [20/42], [94mLoss[0m : 2.53932
[1mStep[0m  [24/42], [94mLoss[0m : 2.67050
[1mStep[0m  [28/42], [94mLoss[0m : 2.66381
[1mStep[0m  [32/42], [94mLoss[0m : 2.38920
[1mStep[0m  [36/42], [94mLoss[0m : 2.35674
[1mStep[0m  [40/42], [94mLoss[0m : 2.48375

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46145
[1mStep[0m  [4/42], [94mLoss[0m : 2.54535
[1mStep[0m  [8/42], [94mLoss[0m : 2.26367
[1mStep[0m  [12/42], [94mLoss[0m : 2.55192
[1mStep[0m  [16/42], [94mLoss[0m : 2.47781
[1mStep[0m  [20/42], [94mLoss[0m : 2.55859
[1mStep[0m  [24/42], [94mLoss[0m : 2.56781
[1mStep[0m  [28/42], [94mLoss[0m : 2.52639
[1mStep[0m  [32/42], [94mLoss[0m : 2.35754
[1mStep[0m  [36/42], [94mLoss[0m : 2.40086
[1mStep[0m  [40/42], [94mLoss[0m : 2.58092

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56573
[1mStep[0m  [4/42], [94mLoss[0m : 2.51327
[1mStep[0m  [8/42], [94mLoss[0m : 2.53733
[1mStep[0m  [12/42], [94mLoss[0m : 2.44764
[1mStep[0m  [16/42], [94mLoss[0m : 2.49145
[1mStep[0m  [20/42], [94mLoss[0m : 2.44246
[1mStep[0m  [24/42], [94mLoss[0m : 2.91071
[1mStep[0m  [28/42], [94mLoss[0m : 2.37358
[1mStep[0m  [32/42], [94mLoss[0m : 2.85440
[1mStep[0m  [36/42], [94mLoss[0m : 2.45984
[1mStep[0m  [40/42], [94mLoss[0m : 2.53241

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45204
[1mStep[0m  [4/42], [94mLoss[0m : 2.34754
[1mStep[0m  [8/42], [94mLoss[0m : 2.46561
[1mStep[0m  [12/42], [94mLoss[0m : 2.43211
[1mStep[0m  [16/42], [94mLoss[0m : 2.28716
[1mStep[0m  [20/42], [94mLoss[0m : 2.41836
[1mStep[0m  [24/42], [94mLoss[0m : 2.31306
[1mStep[0m  [28/42], [94mLoss[0m : 2.47254
[1mStep[0m  [32/42], [94mLoss[0m : 2.56175
[1mStep[0m  [36/42], [94mLoss[0m : 2.62760
[1mStep[0m  [40/42], [94mLoss[0m : 2.37653

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21398
[1mStep[0m  [4/42], [94mLoss[0m : 2.37436
[1mStep[0m  [8/42], [94mLoss[0m : 2.50464
[1mStep[0m  [12/42], [94mLoss[0m : 2.41150
[1mStep[0m  [16/42], [94mLoss[0m : 2.43031
[1mStep[0m  [20/42], [94mLoss[0m : 2.49993
[1mStep[0m  [24/42], [94mLoss[0m : 2.33004
[1mStep[0m  [28/42], [94mLoss[0m : 2.47857
[1mStep[0m  [32/42], [94mLoss[0m : 2.40478
[1mStep[0m  [36/42], [94mLoss[0m : 2.42108
[1mStep[0m  [40/42], [94mLoss[0m : 2.48733

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20247
[1mStep[0m  [4/42], [94mLoss[0m : 2.43977
[1mStep[0m  [8/42], [94mLoss[0m : 2.52803
[1mStep[0m  [12/42], [94mLoss[0m : 2.32548
[1mStep[0m  [16/42], [94mLoss[0m : 2.48735
[1mStep[0m  [20/42], [94mLoss[0m : 2.27461
[1mStep[0m  [24/42], [94mLoss[0m : 2.49889
[1mStep[0m  [28/42], [94mLoss[0m : 2.49892
[1mStep[0m  [32/42], [94mLoss[0m : 2.35302
[1mStep[0m  [36/42], [94mLoss[0m : 2.36708
[1mStep[0m  [40/42], [94mLoss[0m : 2.46053

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42841
[1mStep[0m  [4/42], [94mLoss[0m : 2.51417
[1mStep[0m  [8/42], [94mLoss[0m : 2.40088
[1mStep[0m  [12/42], [94mLoss[0m : 2.33013
[1mStep[0m  [16/42], [94mLoss[0m : 2.45433
[1mStep[0m  [20/42], [94mLoss[0m : 2.51061
[1mStep[0m  [24/42], [94mLoss[0m : 2.39357
[1mStep[0m  [28/42], [94mLoss[0m : 2.51097
[1mStep[0m  [32/42], [94mLoss[0m : 2.48599
[1mStep[0m  [36/42], [94mLoss[0m : 2.35655
[1mStep[0m  [40/42], [94mLoss[0m : 2.44322

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49864
[1mStep[0m  [4/42], [94mLoss[0m : 2.62662
[1mStep[0m  [8/42], [94mLoss[0m : 2.32007
[1mStep[0m  [12/42], [94mLoss[0m : 2.45198
[1mStep[0m  [16/42], [94mLoss[0m : 2.41809
[1mStep[0m  [20/42], [94mLoss[0m : 2.63826
[1mStep[0m  [24/42], [94mLoss[0m : 2.26095
[1mStep[0m  [28/42], [94mLoss[0m : 2.37397
[1mStep[0m  [32/42], [94mLoss[0m : 2.50047
[1mStep[0m  [36/42], [94mLoss[0m : 2.53166
[1mStep[0m  [40/42], [94mLoss[0m : 2.55942

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54221
[1mStep[0m  [4/42], [94mLoss[0m : 2.48136
[1mStep[0m  [8/42], [94mLoss[0m : 2.51501
[1mStep[0m  [12/42], [94mLoss[0m : 2.54396
[1mStep[0m  [16/42], [94mLoss[0m : 2.46585
[1mStep[0m  [20/42], [94mLoss[0m : 2.40733
[1mStep[0m  [24/42], [94mLoss[0m : 2.50013
[1mStep[0m  [28/42], [94mLoss[0m : 2.27034
[1mStep[0m  [32/42], [94mLoss[0m : 2.33258
[1mStep[0m  [36/42], [94mLoss[0m : 2.30877
[1mStep[0m  [40/42], [94mLoss[0m : 2.52070

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52558
[1mStep[0m  [4/42], [94mLoss[0m : 2.20507
[1mStep[0m  [8/42], [94mLoss[0m : 2.37918
[1mStep[0m  [12/42], [94mLoss[0m : 2.32847
[1mStep[0m  [16/42], [94mLoss[0m : 2.30465
[1mStep[0m  [20/42], [94mLoss[0m : 2.40433
[1mStep[0m  [24/42], [94mLoss[0m : 2.29396
[1mStep[0m  [28/42], [94mLoss[0m : 2.26190
[1mStep[0m  [32/42], [94mLoss[0m : 2.36189
[1mStep[0m  [36/42], [94mLoss[0m : 2.68108
[1mStep[0m  [40/42], [94mLoss[0m : 2.49130

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32254
[1mStep[0m  [4/42], [94mLoss[0m : 2.48160
[1mStep[0m  [8/42], [94mLoss[0m : 2.37205
[1mStep[0m  [12/42], [94mLoss[0m : 2.16748
[1mStep[0m  [16/42], [94mLoss[0m : 2.63135
[1mStep[0m  [20/42], [94mLoss[0m : 2.39392
[1mStep[0m  [24/42], [94mLoss[0m : 2.43442
[1mStep[0m  [28/42], [94mLoss[0m : 2.35574
[1mStep[0m  [32/42], [94mLoss[0m : 2.47770
[1mStep[0m  [36/42], [94mLoss[0m : 2.44855
[1mStep[0m  [40/42], [94mLoss[0m : 2.23281

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36773
[1mStep[0m  [4/42], [94mLoss[0m : 2.53280
[1mStep[0m  [8/42], [94mLoss[0m : 2.47161
[1mStep[0m  [12/42], [94mLoss[0m : 2.27447
[1mStep[0m  [16/42], [94mLoss[0m : 2.49411
[1mStep[0m  [20/42], [94mLoss[0m : 2.33554
[1mStep[0m  [24/42], [94mLoss[0m : 2.58829
[1mStep[0m  [28/42], [94mLoss[0m : 2.34272
[1mStep[0m  [32/42], [94mLoss[0m : 2.46323
[1mStep[0m  [36/42], [94mLoss[0m : 2.41789
[1mStep[0m  [40/42], [94mLoss[0m : 2.66436

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45289
[1mStep[0m  [4/42], [94mLoss[0m : 2.50437
[1mStep[0m  [8/42], [94mLoss[0m : 2.49166
[1mStep[0m  [12/42], [94mLoss[0m : 2.44560
[1mStep[0m  [16/42], [94mLoss[0m : 2.46996
[1mStep[0m  [20/42], [94mLoss[0m : 2.22211
[1mStep[0m  [24/42], [94mLoss[0m : 2.34450
[1mStep[0m  [28/42], [94mLoss[0m : 2.68150
[1mStep[0m  [32/42], [94mLoss[0m : 2.68460
[1mStep[0m  [36/42], [94mLoss[0m : 2.44498
[1mStep[0m  [40/42], [94mLoss[0m : 2.27266

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26121
[1mStep[0m  [4/42], [94mLoss[0m : 2.56179
[1mStep[0m  [8/42], [94mLoss[0m : 2.72042
[1mStep[0m  [12/42], [94mLoss[0m : 2.40692
[1mStep[0m  [16/42], [94mLoss[0m : 2.48692
[1mStep[0m  [20/42], [94mLoss[0m : 2.45141
[1mStep[0m  [24/42], [94mLoss[0m : 2.35291
[1mStep[0m  [28/42], [94mLoss[0m : 2.36440
[1mStep[0m  [32/42], [94mLoss[0m : 2.44669
[1mStep[0m  [36/42], [94mLoss[0m : 2.38956
[1mStep[0m  [40/42], [94mLoss[0m : 2.52690

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65067
[1mStep[0m  [4/42], [94mLoss[0m : 2.44930
[1mStep[0m  [8/42], [94mLoss[0m : 2.45641
[1mStep[0m  [12/42], [94mLoss[0m : 2.40357
[1mStep[0m  [16/42], [94mLoss[0m : 2.39493
[1mStep[0m  [20/42], [94mLoss[0m : 2.31917
[1mStep[0m  [24/42], [94mLoss[0m : 2.61755
[1mStep[0m  [28/42], [94mLoss[0m : 2.39622
[1mStep[0m  [32/42], [94mLoss[0m : 2.41860
[1mStep[0m  [36/42], [94mLoss[0m : 2.54031
[1mStep[0m  [40/42], [94mLoss[0m : 2.35102

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54726
[1mStep[0m  [4/42], [94mLoss[0m : 2.51925
[1mStep[0m  [8/42], [94mLoss[0m : 2.58458
[1mStep[0m  [12/42], [94mLoss[0m : 2.23596
[1mStep[0m  [16/42], [94mLoss[0m : 2.39042
[1mStep[0m  [20/42], [94mLoss[0m : 2.38260
[1mStep[0m  [24/42], [94mLoss[0m : 2.16555
[1mStep[0m  [28/42], [94mLoss[0m : 2.30572
[1mStep[0m  [32/42], [94mLoss[0m : 2.40954
[1mStep[0m  [36/42], [94mLoss[0m : 2.57089
[1mStep[0m  [40/42], [94mLoss[0m : 2.51513

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45620
[1mStep[0m  [4/42], [94mLoss[0m : 2.43497
[1mStep[0m  [8/42], [94mLoss[0m : 2.37444
[1mStep[0m  [12/42], [94mLoss[0m : 2.30253
[1mStep[0m  [16/42], [94mLoss[0m : 2.53549
[1mStep[0m  [20/42], [94mLoss[0m : 2.45475
[1mStep[0m  [24/42], [94mLoss[0m : 2.49741
[1mStep[0m  [28/42], [94mLoss[0m : 2.43128
[1mStep[0m  [32/42], [94mLoss[0m : 2.42570
[1mStep[0m  [36/42], [94mLoss[0m : 2.52163
[1mStep[0m  [40/42], [94mLoss[0m : 2.44185

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32140
[1mStep[0m  [4/42], [94mLoss[0m : 2.40128
[1mStep[0m  [8/42], [94mLoss[0m : 2.44149
[1mStep[0m  [12/42], [94mLoss[0m : 2.40872
[1mStep[0m  [16/42], [94mLoss[0m : 2.48219
[1mStep[0m  [20/42], [94mLoss[0m : 2.64824
[1mStep[0m  [24/42], [94mLoss[0m : 2.35775
[1mStep[0m  [28/42], [94mLoss[0m : 2.22779
[1mStep[0m  [32/42], [94mLoss[0m : 2.50915
[1mStep[0m  [36/42], [94mLoss[0m : 2.21642
[1mStep[0m  [40/42], [94mLoss[0m : 2.58196

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32122
[1mStep[0m  [4/42], [94mLoss[0m : 2.15527
[1mStep[0m  [8/42], [94mLoss[0m : 2.44814
[1mStep[0m  [12/42], [94mLoss[0m : 2.47685
[1mStep[0m  [16/42], [94mLoss[0m : 2.22553
[1mStep[0m  [20/42], [94mLoss[0m : 2.46683
[1mStep[0m  [24/42], [94mLoss[0m : 2.29411
[1mStep[0m  [28/42], [94mLoss[0m : 2.53400
[1mStep[0m  [32/42], [94mLoss[0m : 2.50878
[1mStep[0m  [36/42], [94mLoss[0m : 2.54168
[1mStep[0m  [40/42], [94mLoss[0m : 2.53624

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22359
[1mStep[0m  [4/42], [94mLoss[0m : 2.69239
[1mStep[0m  [8/42], [94mLoss[0m : 2.55776
[1mStep[0m  [12/42], [94mLoss[0m : 2.52198
[1mStep[0m  [16/42], [94mLoss[0m : 2.39829
[1mStep[0m  [20/42], [94mLoss[0m : 2.60615
[1mStep[0m  [24/42], [94mLoss[0m : 2.36312
[1mStep[0m  [28/42], [94mLoss[0m : 2.38243
[1mStep[0m  [32/42], [94mLoss[0m : 2.30847
[1mStep[0m  [36/42], [94mLoss[0m : 2.28661
[1mStep[0m  [40/42], [94mLoss[0m : 2.55985

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41669
[1mStep[0m  [4/42], [94mLoss[0m : 2.36941
[1mStep[0m  [8/42], [94mLoss[0m : 2.38243
[1mStep[0m  [12/42], [94mLoss[0m : 2.17632
[1mStep[0m  [16/42], [94mLoss[0m : 2.40433
[1mStep[0m  [20/42], [94mLoss[0m : 2.41437
[1mStep[0m  [24/42], [94mLoss[0m : 2.33596
[1mStep[0m  [28/42], [94mLoss[0m : 2.29426
[1mStep[0m  [32/42], [94mLoss[0m : 2.48981
[1mStep[0m  [36/42], [94mLoss[0m : 2.24986
[1mStep[0m  [40/42], [94mLoss[0m : 2.61044

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58607
[1mStep[0m  [4/42], [94mLoss[0m : 2.52175
[1mStep[0m  [8/42], [94mLoss[0m : 2.22579
[1mStep[0m  [12/42], [94mLoss[0m : 2.63462
[1mStep[0m  [16/42], [94mLoss[0m : 2.44833
[1mStep[0m  [20/42], [94mLoss[0m : 2.35425
[1mStep[0m  [24/42], [94mLoss[0m : 2.37227
[1mStep[0m  [28/42], [94mLoss[0m : 2.51871
[1mStep[0m  [32/42], [94mLoss[0m : 2.47125
[1mStep[0m  [36/42], [94mLoss[0m : 2.40774
[1mStep[0m  [40/42], [94mLoss[0m : 2.52098

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50996
[1mStep[0m  [4/42], [94mLoss[0m : 2.27911
[1mStep[0m  [8/42], [94mLoss[0m : 2.54348
[1mStep[0m  [12/42], [94mLoss[0m : 2.32723
[1mStep[0m  [16/42], [94mLoss[0m : 2.40360
[1mStep[0m  [20/42], [94mLoss[0m : 2.29015
[1mStep[0m  [24/42], [94mLoss[0m : 2.55019
[1mStep[0m  [28/42], [94mLoss[0m : 2.67115
[1mStep[0m  [32/42], [94mLoss[0m : 2.39458
[1mStep[0m  [36/42], [94mLoss[0m : 2.52148
[1mStep[0m  [40/42], [94mLoss[0m : 2.54929

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47783
[1mStep[0m  [4/42], [94mLoss[0m : 2.40255
[1mStep[0m  [8/42], [94mLoss[0m : 2.43788
[1mStep[0m  [12/42], [94mLoss[0m : 2.50890
[1mStep[0m  [16/42], [94mLoss[0m : 2.42964
[1mStep[0m  [20/42], [94mLoss[0m : 2.59487
[1mStep[0m  [24/42], [94mLoss[0m : 2.45408
[1mStep[0m  [28/42], [94mLoss[0m : 2.39373
[1mStep[0m  [32/42], [94mLoss[0m : 2.36458
[1mStep[0m  [36/42], [94mLoss[0m : 2.43540
[1mStep[0m  [40/42], [94mLoss[0m : 2.42797

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42951
[1mStep[0m  [4/42], [94mLoss[0m : 2.41172
[1mStep[0m  [8/42], [94mLoss[0m : 2.36646
[1mStep[0m  [12/42], [94mLoss[0m : 2.39604
[1mStep[0m  [16/42], [94mLoss[0m : 2.34323
[1mStep[0m  [20/42], [94mLoss[0m : 2.50751
[1mStep[0m  [24/42], [94mLoss[0m : 2.47896
[1mStep[0m  [28/42], [94mLoss[0m : 2.29459
[1mStep[0m  [32/42], [94mLoss[0m : 2.39434
[1mStep[0m  [36/42], [94mLoss[0m : 2.38836
[1mStep[0m  [40/42], [94mLoss[0m : 2.45123

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40963
[1mStep[0m  [4/42], [94mLoss[0m : 2.45011
[1mStep[0m  [8/42], [94mLoss[0m : 2.41967
[1mStep[0m  [12/42], [94mLoss[0m : 2.34178
[1mStep[0m  [16/42], [94mLoss[0m : 2.31143
[1mStep[0m  [20/42], [94mLoss[0m : 2.38906
[1mStep[0m  [24/42], [94mLoss[0m : 2.49246
[1mStep[0m  [28/42], [94mLoss[0m : 2.36330
[1mStep[0m  [32/42], [94mLoss[0m : 2.35219
[1mStep[0m  [36/42], [94mLoss[0m : 2.28635
[1mStep[0m  [40/42], [94mLoss[0m : 2.38384

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31353
[1mStep[0m  [4/42], [94mLoss[0m : 2.46458
[1mStep[0m  [8/42], [94mLoss[0m : 2.49104
[1mStep[0m  [12/42], [94mLoss[0m : 2.50256
[1mStep[0m  [16/42], [94mLoss[0m : 2.38302
[1mStep[0m  [20/42], [94mLoss[0m : 2.56043
[1mStep[0m  [24/42], [94mLoss[0m : 2.45868
[1mStep[0m  [28/42], [94mLoss[0m : 2.33113
[1mStep[0m  [32/42], [94mLoss[0m : 2.38024
[1mStep[0m  [36/42], [94mLoss[0m : 2.39550
[1mStep[0m  [40/42], [94mLoss[0m : 2.19425

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46571
[1mStep[0m  [4/42], [94mLoss[0m : 2.51717
[1mStep[0m  [8/42], [94mLoss[0m : 2.28334
[1mStep[0m  [12/42], [94mLoss[0m : 2.38083
[1mStep[0m  [16/42], [94mLoss[0m : 2.45733
[1mStep[0m  [20/42], [94mLoss[0m : 2.30634
[1mStep[0m  [24/42], [94mLoss[0m : 2.37394
[1mStep[0m  [28/42], [94mLoss[0m : 2.31943
[1mStep[0m  [32/42], [94mLoss[0m : 2.27875
[1mStep[0m  [36/42], [94mLoss[0m : 2.66043
[1mStep[0m  [40/42], [94mLoss[0m : 2.51192

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43242
[1mStep[0m  [4/42], [94mLoss[0m : 2.33340
[1mStep[0m  [8/42], [94mLoss[0m : 2.24769
[1mStep[0m  [12/42], [94mLoss[0m : 2.54389
[1mStep[0m  [16/42], [94mLoss[0m : 2.37373
[1mStep[0m  [20/42], [94mLoss[0m : 2.53653
[1mStep[0m  [24/42], [94mLoss[0m : 2.31087
[1mStep[0m  [28/42], [94mLoss[0m : 2.25844
[1mStep[0m  [32/42], [94mLoss[0m : 2.55672
[1mStep[0m  [36/42], [94mLoss[0m : 2.53579
[1mStep[0m  [40/42], [94mLoss[0m : 2.40254

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.327
====================================

Phase 1 - Evaluation MAE:  2.3265717710767473
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.63042
[1mStep[0m  [4/42], [94mLoss[0m : 2.25122
[1mStep[0m  [8/42], [94mLoss[0m : 2.38413
[1mStep[0m  [12/42], [94mLoss[0m : 2.41948
[1mStep[0m  [16/42], [94mLoss[0m : 2.61123
[1mStep[0m  [20/42], [94mLoss[0m : 2.23069
[1mStep[0m  [24/42], [94mLoss[0m : 2.44200
[1mStep[0m  [28/42], [94mLoss[0m : 2.44632
[1mStep[0m  [32/42], [94mLoss[0m : 2.54355
[1mStep[0m  [36/42], [94mLoss[0m : 2.58081
[1mStep[0m  [40/42], [94mLoss[0m : 2.26276

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23787
[1mStep[0m  [4/42], [94mLoss[0m : 2.38347
[1mStep[0m  [8/42], [94mLoss[0m : 2.51314
[1mStep[0m  [12/42], [94mLoss[0m : 2.54087
[1mStep[0m  [16/42], [94mLoss[0m : 2.37696
[1mStep[0m  [20/42], [94mLoss[0m : 2.27453
[1mStep[0m  [24/42], [94mLoss[0m : 2.36110
[1mStep[0m  [28/42], [94mLoss[0m : 2.40409
[1mStep[0m  [32/42], [94mLoss[0m : 2.28256
[1mStep[0m  [36/42], [94mLoss[0m : 2.49323
[1mStep[0m  [40/42], [94mLoss[0m : 2.47281

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29072
[1mStep[0m  [4/42], [94mLoss[0m : 2.19635
[1mStep[0m  [8/42], [94mLoss[0m : 2.32809
[1mStep[0m  [12/42], [94mLoss[0m : 2.35540
[1mStep[0m  [16/42], [94mLoss[0m : 2.36258
[1mStep[0m  [20/42], [94mLoss[0m : 2.37860
[1mStep[0m  [24/42], [94mLoss[0m : 2.25896
[1mStep[0m  [28/42], [94mLoss[0m : 2.15537
[1mStep[0m  [32/42], [94mLoss[0m : 2.18414
[1mStep[0m  [36/42], [94mLoss[0m : 2.55451
[1mStep[0m  [40/42], [94mLoss[0m : 2.36812

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38468
[1mStep[0m  [4/42], [94mLoss[0m : 2.30977
[1mStep[0m  [8/42], [94mLoss[0m : 2.22615
[1mStep[0m  [12/42], [94mLoss[0m : 2.19862
[1mStep[0m  [16/42], [94mLoss[0m : 2.21150
[1mStep[0m  [20/42], [94mLoss[0m : 2.34014
[1mStep[0m  [24/42], [94mLoss[0m : 2.14652
[1mStep[0m  [28/42], [94mLoss[0m : 2.17349
[1mStep[0m  [32/42], [94mLoss[0m : 2.28090
[1mStep[0m  [36/42], [94mLoss[0m : 2.40893
[1mStep[0m  [40/42], [94mLoss[0m : 2.19175

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21502
[1mStep[0m  [4/42], [94mLoss[0m : 1.97446
[1mStep[0m  [8/42], [94mLoss[0m : 2.18932
[1mStep[0m  [12/42], [94mLoss[0m : 2.24809
[1mStep[0m  [16/42], [94mLoss[0m : 2.06722
[1mStep[0m  [20/42], [94mLoss[0m : 2.30204
[1mStep[0m  [24/42], [94mLoss[0m : 2.25046
[1mStep[0m  [28/42], [94mLoss[0m : 1.95566
[1mStep[0m  [32/42], [94mLoss[0m : 2.18475
[1mStep[0m  [36/42], [94mLoss[0m : 2.23347
[1mStep[0m  [40/42], [94mLoss[0m : 2.27538

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07464
[1mStep[0m  [4/42], [94mLoss[0m : 2.16264
[1mStep[0m  [8/42], [94mLoss[0m : 2.18748
[1mStep[0m  [12/42], [94mLoss[0m : 2.11293
[1mStep[0m  [16/42], [94mLoss[0m : 2.09113
[1mStep[0m  [20/42], [94mLoss[0m : 2.23542
[1mStep[0m  [24/42], [94mLoss[0m : 2.03597
[1mStep[0m  [28/42], [94mLoss[0m : 2.24246
[1mStep[0m  [32/42], [94mLoss[0m : 2.26127
[1mStep[0m  [36/42], [94mLoss[0m : 2.12033
[1mStep[0m  [40/42], [94mLoss[0m : 2.19642

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12933
[1mStep[0m  [4/42], [94mLoss[0m : 2.12839
[1mStep[0m  [8/42], [94mLoss[0m : 2.19913
[1mStep[0m  [12/42], [94mLoss[0m : 2.03563
[1mStep[0m  [16/42], [94mLoss[0m : 2.20402
[1mStep[0m  [20/42], [94mLoss[0m : 1.99914
[1mStep[0m  [24/42], [94mLoss[0m : 2.33817
[1mStep[0m  [28/42], [94mLoss[0m : 2.16064
[1mStep[0m  [32/42], [94mLoss[0m : 2.08204
[1mStep[0m  [36/42], [94mLoss[0m : 2.08312
[1mStep[0m  [40/42], [94mLoss[0m : 2.07219

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93157
[1mStep[0m  [4/42], [94mLoss[0m : 2.07367
[1mStep[0m  [8/42], [94mLoss[0m : 1.81643
[1mStep[0m  [12/42], [94mLoss[0m : 2.13218
[1mStep[0m  [16/42], [94mLoss[0m : 2.34934
[1mStep[0m  [20/42], [94mLoss[0m : 1.85167
[1mStep[0m  [24/42], [94mLoss[0m : 2.02764
[1mStep[0m  [28/42], [94mLoss[0m : 2.16462
[1mStep[0m  [32/42], [94mLoss[0m : 2.24579
[1mStep[0m  [36/42], [94mLoss[0m : 2.12608
[1mStep[0m  [40/42], [94mLoss[0m : 2.13260

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29321
[1mStep[0m  [4/42], [94mLoss[0m : 2.20720
[1mStep[0m  [8/42], [94mLoss[0m : 1.97839
[1mStep[0m  [12/42], [94mLoss[0m : 1.95736
[1mStep[0m  [16/42], [94mLoss[0m : 2.08580
[1mStep[0m  [20/42], [94mLoss[0m : 1.89960
[1mStep[0m  [24/42], [94mLoss[0m : 1.96239
[1mStep[0m  [28/42], [94mLoss[0m : 2.02930
[1mStep[0m  [32/42], [94mLoss[0m : 1.84394
[1mStep[0m  [36/42], [94mLoss[0m : 2.16819
[1mStep[0m  [40/42], [94mLoss[0m : 2.10938

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09098
[1mStep[0m  [4/42], [94mLoss[0m : 2.08711
[1mStep[0m  [8/42], [94mLoss[0m : 1.84735
[1mStep[0m  [12/42], [94mLoss[0m : 1.91132
[1mStep[0m  [16/42], [94mLoss[0m : 1.93877
[1mStep[0m  [20/42], [94mLoss[0m : 1.82370
[1mStep[0m  [24/42], [94mLoss[0m : 2.08959
[1mStep[0m  [28/42], [94mLoss[0m : 1.92386
[1mStep[0m  [32/42], [94mLoss[0m : 2.03205
[1mStep[0m  [36/42], [94mLoss[0m : 1.89730
[1mStep[0m  [40/42], [94mLoss[0m : 2.03416

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01044
[1mStep[0m  [4/42], [94mLoss[0m : 1.74277
[1mStep[0m  [8/42], [94mLoss[0m : 1.93558
[1mStep[0m  [12/42], [94mLoss[0m : 1.87733
[1mStep[0m  [16/42], [94mLoss[0m : 1.94950
[1mStep[0m  [20/42], [94mLoss[0m : 1.85554
[1mStep[0m  [24/42], [94mLoss[0m : 1.98275
[1mStep[0m  [28/42], [94mLoss[0m : 1.88158
[1mStep[0m  [32/42], [94mLoss[0m : 1.92807
[1mStep[0m  [36/42], [94mLoss[0m : 1.84435
[1mStep[0m  [40/42], [94mLoss[0m : 1.73082

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80811
[1mStep[0m  [4/42], [94mLoss[0m : 1.80042
[1mStep[0m  [8/42], [94mLoss[0m : 1.85901
[1mStep[0m  [12/42], [94mLoss[0m : 1.96304
[1mStep[0m  [16/42], [94mLoss[0m : 1.84073
[1mStep[0m  [20/42], [94mLoss[0m : 1.80142
[1mStep[0m  [24/42], [94mLoss[0m : 1.86809
[1mStep[0m  [28/42], [94mLoss[0m : 1.77330
[1mStep[0m  [32/42], [94mLoss[0m : 1.89102
[1mStep[0m  [36/42], [94mLoss[0m : 1.95860
[1mStep[0m  [40/42], [94mLoss[0m : 1.86407

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72247
[1mStep[0m  [4/42], [94mLoss[0m : 1.84533
[1mStep[0m  [8/42], [94mLoss[0m : 2.00540
[1mStep[0m  [12/42], [94mLoss[0m : 1.65598
[1mStep[0m  [16/42], [94mLoss[0m : 1.92824
[1mStep[0m  [20/42], [94mLoss[0m : 1.75358
[1mStep[0m  [24/42], [94mLoss[0m : 1.93120
[1mStep[0m  [28/42], [94mLoss[0m : 1.85514
[1mStep[0m  [32/42], [94mLoss[0m : 1.63876
[1mStep[0m  [36/42], [94mLoss[0m : 1.83227
[1mStep[0m  [40/42], [94mLoss[0m : 1.73282

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81537
[1mStep[0m  [4/42], [94mLoss[0m : 1.64407
[1mStep[0m  [8/42], [94mLoss[0m : 1.69952
[1mStep[0m  [12/42], [94mLoss[0m : 1.73477
[1mStep[0m  [16/42], [94mLoss[0m : 1.64972
[1mStep[0m  [20/42], [94mLoss[0m : 1.83734
[1mStep[0m  [24/42], [94mLoss[0m : 1.86008
[1mStep[0m  [28/42], [94mLoss[0m : 1.90672
[1mStep[0m  [32/42], [94mLoss[0m : 1.85317
[1mStep[0m  [36/42], [94mLoss[0m : 1.86428
[1mStep[0m  [40/42], [94mLoss[0m : 1.83819

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64075
[1mStep[0m  [4/42], [94mLoss[0m : 1.67124
[1mStep[0m  [8/42], [94mLoss[0m : 1.70594
[1mStep[0m  [12/42], [94mLoss[0m : 1.76940
[1mStep[0m  [16/42], [94mLoss[0m : 1.70414
[1mStep[0m  [20/42], [94mLoss[0m : 1.62389
[1mStep[0m  [24/42], [94mLoss[0m : 1.60002
[1mStep[0m  [28/42], [94mLoss[0m : 1.65124
[1mStep[0m  [32/42], [94mLoss[0m : 1.86696
[1mStep[0m  [36/42], [94mLoss[0m : 1.74978
[1mStep[0m  [40/42], [94mLoss[0m : 1.82966

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65614
[1mStep[0m  [4/42], [94mLoss[0m : 1.70460
[1mStep[0m  [8/42], [94mLoss[0m : 1.49836
[1mStep[0m  [12/42], [94mLoss[0m : 1.66130
[1mStep[0m  [16/42], [94mLoss[0m : 1.69959
[1mStep[0m  [20/42], [94mLoss[0m : 1.49443
[1mStep[0m  [24/42], [94mLoss[0m : 1.87149
[1mStep[0m  [28/42], [94mLoss[0m : 1.68404
[1mStep[0m  [32/42], [94mLoss[0m : 1.85038
[1mStep[0m  [36/42], [94mLoss[0m : 1.66920
[1mStep[0m  [40/42], [94mLoss[0m : 1.60681

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69458
[1mStep[0m  [4/42], [94mLoss[0m : 1.65069
[1mStep[0m  [8/42], [94mLoss[0m : 1.68849
[1mStep[0m  [12/42], [94mLoss[0m : 1.55286
[1mStep[0m  [16/42], [94mLoss[0m : 1.68214
[1mStep[0m  [20/42], [94mLoss[0m : 1.76103
[1mStep[0m  [24/42], [94mLoss[0m : 1.58895
[1mStep[0m  [28/42], [94mLoss[0m : 1.75380
[1mStep[0m  [32/42], [94mLoss[0m : 1.57129
[1mStep[0m  [36/42], [94mLoss[0m : 1.59119
[1mStep[0m  [40/42], [94mLoss[0m : 1.56432

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67829
[1mStep[0m  [4/42], [94mLoss[0m : 1.47685
[1mStep[0m  [8/42], [94mLoss[0m : 1.54257
[1mStep[0m  [12/42], [94mLoss[0m : 1.67934
[1mStep[0m  [16/42], [94mLoss[0m : 1.57737
[1mStep[0m  [20/42], [94mLoss[0m : 1.62787
[1mStep[0m  [24/42], [94mLoss[0m : 1.74328
[1mStep[0m  [28/42], [94mLoss[0m : 1.67565
[1mStep[0m  [32/42], [94mLoss[0m : 1.77421
[1mStep[0m  [36/42], [94mLoss[0m : 1.71755
[1mStep[0m  [40/42], [94mLoss[0m : 1.77916

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37854
[1mStep[0m  [4/42], [94mLoss[0m : 1.66553
[1mStep[0m  [8/42], [94mLoss[0m : 1.55502
[1mStep[0m  [12/42], [94mLoss[0m : 1.61418
[1mStep[0m  [16/42], [94mLoss[0m : 1.49162
[1mStep[0m  [20/42], [94mLoss[0m : 1.59485
[1mStep[0m  [24/42], [94mLoss[0m : 1.70074
[1mStep[0m  [28/42], [94mLoss[0m : 1.37388
[1mStep[0m  [32/42], [94mLoss[0m : 1.68389
[1mStep[0m  [36/42], [94mLoss[0m : 1.56966
[1mStep[0m  [40/42], [94mLoss[0m : 1.68937

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46332
[1mStep[0m  [4/42], [94mLoss[0m : 1.44819
[1mStep[0m  [8/42], [94mLoss[0m : 1.46863
[1mStep[0m  [12/42], [94mLoss[0m : 1.54239
[1mStep[0m  [16/42], [94mLoss[0m : 1.39434
[1mStep[0m  [20/42], [94mLoss[0m : 1.54728
[1mStep[0m  [24/42], [94mLoss[0m : 1.60309
[1mStep[0m  [28/42], [94mLoss[0m : 1.49853
[1mStep[0m  [32/42], [94mLoss[0m : 1.66507
[1mStep[0m  [36/42], [94mLoss[0m : 1.65818
[1mStep[0m  [40/42], [94mLoss[0m : 1.52042

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.562, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47825
[1mStep[0m  [4/42], [94mLoss[0m : 1.36155
[1mStep[0m  [8/42], [94mLoss[0m : 1.58488
[1mStep[0m  [12/42], [94mLoss[0m : 1.46493
[1mStep[0m  [16/42], [94mLoss[0m : 1.40743
[1mStep[0m  [20/42], [94mLoss[0m : 1.40128
[1mStep[0m  [24/42], [94mLoss[0m : 1.42128
[1mStep[0m  [28/42], [94mLoss[0m : 1.67278
[1mStep[0m  [32/42], [94mLoss[0m : 1.50157
[1mStep[0m  [36/42], [94mLoss[0m : 1.62407
[1mStep[0m  [40/42], [94mLoss[0m : 1.63494

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.508, [92mTest[0m: 2.559, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45035
[1mStep[0m  [4/42], [94mLoss[0m : 1.50167
[1mStep[0m  [8/42], [94mLoss[0m : 1.33906
[1mStep[0m  [12/42], [94mLoss[0m : 1.43571
[1mStep[0m  [16/42], [94mLoss[0m : 1.43091
[1mStep[0m  [20/42], [94mLoss[0m : 1.32717
[1mStep[0m  [24/42], [94mLoss[0m : 1.50638
[1mStep[0m  [28/42], [94mLoss[0m : 1.61078
[1mStep[0m  [32/42], [94mLoss[0m : 1.52505
[1mStep[0m  [36/42], [94mLoss[0m : 1.55440
[1mStep[0m  [40/42], [94mLoss[0m : 1.52636

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.469, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.31213
[1mStep[0m  [4/42], [94mLoss[0m : 1.33545
[1mStep[0m  [8/42], [94mLoss[0m : 1.50553
[1mStep[0m  [12/42], [94mLoss[0m : 1.32454
[1mStep[0m  [16/42], [94mLoss[0m : 1.44944
[1mStep[0m  [20/42], [94mLoss[0m : 1.42098
[1mStep[0m  [24/42], [94mLoss[0m : 1.32147
[1mStep[0m  [28/42], [94mLoss[0m : 1.55611
[1mStep[0m  [32/42], [94mLoss[0m : 1.59183
[1mStep[0m  [36/42], [94mLoss[0m : 1.53983
[1mStep[0m  [40/42], [94mLoss[0m : 1.43569

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.448, [92mTest[0m: 2.512, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37500
[1mStep[0m  [4/42], [94mLoss[0m : 1.39640
[1mStep[0m  [8/42], [94mLoss[0m : 1.60644
[1mStep[0m  [12/42], [94mLoss[0m : 1.34364
[1mStep[0m  [16/42], [94mLoss[0m : 1.36082
[1mStep[0m  [20/42], [94mLoss[0m : 1.48646
[1mStep[0m  [24/42], [94mLoss[0m : 1.39943
[1mStep[0m  [28/42], [94mLoss[0m : 1.48363
[1mStep[0m  [32/42], [94mLoss[0m : 1.34425
[1mStep[0m  [36/42], [94mLoss[0m : 1.32572
[1mStep[0m  [40/42], [94mLoss[0m : 1.53473

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.500, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43018
[1mStep[0m  [4/42], [94mLoss[0m : 1.44065
[1mStep[0m  [8/42], [94mLoss[0m : 1.27468
[1mStep[0m  [12/42], [94mLoss[0m : 1.44839
[1mStep[0m  [16/42], [94mLoss[0m : 1.43416
[1mStep[0m  [20/42], [94mLoss[0m : 1.33926
[1mStep[0m  [24/42], [94mLoss[0m : 1.53685
[1mStep[0m  [28/42], [94mLoss[0m : 1.39234
[1mStep[0m  [32/42], [94mLoss[0m : 1.43787
[1mStep[0m  [36/42], [94mLoss[0m : 1.50421
[1mStep[0m  [40/42], [94mLoss[0m : 1.53649

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.397, [92mTest[0m: 2.556, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.472
====================================

Phase 2 - Evaluation MAE:  2.472430774143764
MAE score P1      2.326572
MAE score P2      2.472431
loss              1.396917
learning_rate         0.01
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.49848
[1mStep[0m  [8/84], [94mLoss[0m : 8.57535
[1mStep[0m  [16/84], [94mLoss[0m : 6.53577
[1mStep[0m  [24/84], [94mLoss[0m : 3.80174
[1mStep[0m  [32/84], [94mLoss[0m : 3.45576
[1mStep[0m  [40/84], [94mLoss[0m : 2.44496
[1mStep[0m  [48/84], [94mLoss[0m : 2.70806
[1mStep[0m  [56/84], [94mLoss[0m : 2.77451
[1mStep[0m  [64/84], [94mLoss[0m : 2.64469
[1mStep[0m  [72/84], [94mLoss[0m : 2.51512
[1mStep[0m  [80/84], [94mLoss[0m : 2.44312

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.130, [92mTest[0m: 10.988, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66327
[1mStep[0m  [8/84], [94mLoss[0m : 2.55353
[1mStep[0m  [16/84], [94mLoss[0m : 2.60326
[1mStep[0m  [24/84], [94mLoss[0m : 2.31958
[1mStep[0m  [32/84], [94mLoss[0m : 2.91446
[1mStep[0m  [40/84], [94mLoss[0m : 2.84550
[1mStep[0m  [48/84], [94mLoss[0m : 2.33136
[1mStep[0m  [56/84], [94mLoss[0m : 2.49869
[1mStep[0m  [64/84], [94mLoss[0m : 2.19953
[1mStep[0m  [72/84], [94mLoss[0m : 2.59353
[1mStep[0m  [80/84], [94mLoss[0m : 2.34120

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.653, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33919
[1mStep[0m  [8/84], [94mLoss[0m : 2.51160
[1mStep[0m  [16/84], [94mLoss[0m : 2.66441
[1mStep[0m  [24/84], [94mLoss[0m : 2.56132
[1mStep[0m  [32/84], [94mLoss[0m : 2.70475
[1mStep[0m  [40/84], [94mLoss[0m : 2.65547
[1mStep[0m  [48/84], [94mLoss[0m : 2.59379
[1mStep[0m  [56/84], [94mLoss[0m : 2.37171
[1mStep[0m  [64/84], [94mLoss[0m : 2.40363
[1mStep[0m  [72/84], [94mLoss[0m : 2.49423
[1mStep[0m  [80/84], [94mLoss[0m : 2.67620

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.608, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76162
[1mStep[0m  [8/84], [94mLoss[0m : 2.53173
[1mStep[0m  [16/84], [94mLoss[0m : 2.29533
[1mStep[0m  [24/84], [94mLoss[0m : 2.52954
[1mStep[0m  [32/84], [94mLoss[0m : 2.46979
[1mStep[0m  [40/84], [94mLoss[0m : 2.60253
[1mStep[0m  [48/84], [94mLoss[0m : 2.62823
[1mStep[0m  [56/84], [94mLoss[0m : 2.74111
[1mStep[0m  [64/84], [94mLoss[0m : 2.46236
[1mStep[0m  [72/84], [94mLoss[0m : 2.50652
[1mStep[0m  [80/84], [94mLoss[0m : 2.76351

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32895
[1mStep[0m  [8/84], [94mLoss[0m : 2.27343
[1mStep[0m  [16/84], [94mLoss[0m : 2.50752
[1mStep[0m  [24/84], [94mLoss[0m : 2.97407
[1mStep[0m  [32/84], [94mLoss[0m : 2.83816
[1mStep[0m  [40/84], [94mLoss[0m : 2.33774
[1mStep[0m  [48/84], [94mLoss[0m : 2.40512
[1mStep[0m  [56/84], [94mLoss[0m : 2.49202
[1mStep[0m  [64/84], [94mLoss[0m : 2.09716
[1mStep[0m  [72/84], [94mLoss[0m : 2.33586
[1mStep[0m  [80/84], [94mLoss[0m : 2.61479

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19848
[1mStep[0m  [8/84], [94mLoss[0m : 2.70094
[1mStep[0m  [16/84], [94mLoss[0m : 2.66951
[1mStep[0m  [24/84], [94mLoss[0m : 2.52543
[1mStep[0m  [32/84], [94mLoss[0m : 2.10668
[1mStep[0m  [40/84], [94mLoss[0m : 2.28831
[1mStep[0m  [48/84], [94mLoss[0m : 2.36271
[1mStep[0m  [56/84], [94mLoss[0m : 2.39174
[1mStep[0m  [64/84], [94mLoss[0m : 2.55878
[1mStep[0m  [72/84], [94mLoss[0m : 2.52121
[1mStep[0m  [80/84], [94mLoss[0m : 2.45674

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52193
[1mStep[0m  [8/84], [94mLoss[0m : 2.62100
[1mStep[0m  [16/84], [94mLoss[0m : 2.35448
[1mStep[0m  [24/84], [94mLoss[0m : 2.37626
[1mStep[0m  [32/84], [94mLoss[0m : 2.27681
[1mStep[0m  [40/84], [94mLoss[0m : 2.65006
[1mStep[0m  [48/84], [94mLoss[0m : 2.38367
[1mStep[0m  [56/84], [94mLoss[0m : 2.44637
[1mStep[0m  [64/84], [94mLoss[0m : 2.08246
[1mStep[0m  [72/84], [94mLoss[0m : 2.53028
[1mStep[0m  [80/84], [94mLoss[0m : 2.48776

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09343
[1mStep[0m  [8/84], [94mLoss[0m : 2.29134
[1mStep[0m  [16/84], [94mLoss[0m : 2.58159
[1mStep[0m  [24/84], [94mLoss[0m : 2.23592
[1mStep[0m  [32/84], [94mLoss[0m : 2.50098
[1mStep[0m  [40/84], [94mLoss[0m : 2.58111
[1mStep[0m  [48/84], [94mLoss[0m : 2.36603
[1mStep[0m  [56/84], [94mLoss[0m : 2.51696
[1mStep[0m  [64/84], [94mLoss[0m : 2.22794
[1mStep[0m  [72/84], [94mLoss[0m : 2.29969
[1mStep[0m  [80/84], [94mLoss[0m : 2.50709

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36336
[1mStep[0m  [8/84], [94mLoss[0m : 2.44009
[1mStep[0m  [16/84], [94mLoss[0m : 2.60949
[1mStep[0m  [24/84], [94mLoss[0m : 2.65403
[1mStep[0m  [32/84], [94mLoss[0m : 2.52769
[1mStep[0m  [40/84], [94mLoss[0m : 2.28644
[1mStep[0m  [48/84], [94mLoss[0m : 2.18794
[1mStep[0m  [56/84], [94mLoss[0m : 2.51745
[1mStep[0m  [64/84], [94mLoss[0m : 2.41181
[1mStep[0m  [72/84], [94mLoss[0m : 2.41576
[1mStep[0m  [80/84], [94mLoss[0m : 2.33044

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24916
[1mStep[0m  [8/84], [94mLoss[0m : 2.10270
[1mStep[0m  [16/84], [94mLoss[0m : 2.28316
[1mStep[0m  [24/84], [94mLoss[0m : 2.63225
[1mStep[0m  [32/84], [94mLoss[0m : 2.26956
[1mStep[0m  [40/84], [94mLoss[0m : 2.25069
[1mStep[0m  [48/84], [94mLoss[0m : 2.65262
[1mStep[0m  [56/84], [94mLoss[0m : 2.43408
[1mStep[0m  [64/84], [94mLoss[0m : 2.68129
[1mStep[0m  [72/84], [94mLoss[0m : 2.52677
[1mStep[0m  [80/84], [94mLoss[0m : 2.35133

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52301
[1mStep[0m  [8/84], [94mLoss[0m : 2.37150
[1mStep[0m  [16/84], [94mLoss[0m : 2.25784
[1mStep[0m  [24/84], [94mLoss[0m : 2.24851
[1mStep[0m  [32/84], [94mLoss[0m : 2.69603
[1mStep[0m  [40/84], [94mLoss[0m : 2.41870
[1mStep[0m  [48/84], [94mLoss[0m : 2.73883
[1mStep[0m  [56/84], [94mLoss[0m : 3.06309
[1mStep[0m  [64/84], [94mLoss[0m : 2.50038
[1mStep[0m  [72/84], [94mLoss[0m : 2.50640
[1mStep[0m  [80/84], [94mLoss[0m : 2.47629

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50537
[1mStep[0m  [8/84], [94mLoss[0m : 2.46313
[1mStep[0m  [16/84], [94mLoss[0m : 2.14441
[1mStep[0m  [24/84], [94mLoss[0m : 2.60587
[1mStep[0m  [32/84], [94mLoss[0m : 2.54299
[1mStep[0m  [40/84], [94mLoss[0m : 2.74237
[1mStep[0m  [48/84], [94mLoss[0m : 2.71063
[1mStep[0m  [56/84], [94mLoss[0m : 2.45428
[1mStep[0m  [64/84], [94mLoss[0m : 2.28050
[1mStep[0m  [72/84], [94mLoss[0m : 2.46126
[1mStep[0m  [80/84], [94mLoss[0m : 2.64638

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56782
[1mStep[0m  [8/84], [94mLoss[0m : 2.48501
[1mStep[0m  [16/84], [94mLoss[0m : 2.32611
[1mStep[0m  [24/84], [94mLoss[0m : 2.35369
[1mStep[0m  [32/84], [94mLoss[0m : 2.24847
[1mStep[0m  [40/84], [94mLoss[0m : 2.53251
[1mStep[0m  [48/84], [94mLoss[0m : 2.43299
[1mStep[0m  [56/84], [94mLoss[0m : 2.39474
[1mStep[0m  [64/84], [94mLoss[0m : 2.36063
[1mStep[0m  [72/84], [94mLoss[0m : 2.29860
[1mStep[0m  [80/84], [94mLoss[0m : 2.84385

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07948
[1mStep[0m  [8/84], [94mLoss[0m : 2.12377
[1mStep[0m  [16/84], [94mLoss[0m : 2.05107
[1mStep[0m  [24/84], [94mLoss[0m : 2.32698
[1mStep[0m  [32/84], [94mLoss[0m : 2.36207
[1mStep[0m  [40/84], [94mLoss[0m : 2.60589
[1mStep[0m  [48/84], [94mLoss[0m : 2.13317
[1mStep[0m  [56/84], [94mLoss[0m : 2.55585
[1mStep[0m  [64/84], [94mLoss[0m : 2.74869
[1mStep[0m  [72/84], [94mLoss[0m : 2.59229
[1mStep[0m  [80/84], [94mLoss[0m : 2.55191

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53682
[1mStep[0m  [8/84], [94mLoss[0m : 2.45298
[1mStep[0m  [16/84], [94mLoss[0m : 2.27400
[1mStep[0m  [24/84], [94mLoss[0m : 2.03238
[1mStep[0m  [32/84], [94mLoss[0m : 2.16108
[1mStep[0m  [40/84], [94mLoss[0m : 2.40100
[1mStep[0m  [48/84], [94mLoss[0m : 2.66304
[1mStep[0m  [56/84], [94mLoss[0m : 2.57624
[1mStep[0m  [64/84], [94mLoss[0m : 2.61327
[1mStep[0m  [72/84], [94mLoss[0m : 2.45890
[1mStep[0m  [80/84], [94mLoss[0m : 2.41769

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62051
[1mStep[0m  [8/84], [94mLoss[0m : 2.51442
[1mStep[0m  [16/84], [94mLoss[0m : 2.46580
[1mStep[0m  [24/84], [94mLoss[0m : 2.58445
[1mStep[0m  [32/84], [94mLoss[0m : 2.24922
[1mStep[0m  [40/84], [94mLoss[0m : 2.30673
[1mStep[0m  [48/84], [94mLoss[0m : 2.10992
[1mStep[0m  [56/84], [94mLoss[0m : 2.50829
[1mStep[0m  [64/84], [94mLoss[0m : 2.45483
[1mStep[0m  [72/84], [94mLoss[0m : 2.42045
[1mStep[0m  [80/84], [94mLoss[0m : 2.36099

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42939
[1mStep[0m  [8/84], [94mLoss[0m : 2.24735
[1mStep[0m  [16/84], [94mLoss[0m : 2.28015
[1mStep[0m  [24/84], [94mLoss[0m : 2.26147
[1mStep[0m  [32/84], [94mLoss[0m : 2.53588
[1mStep[0m  [40/84], [94mLoss[0m : 2.59347
[1mStep[0m  [48/84], [94mLoss[0m : 2.04672
[1mStep[0m  [56/84], [94mLoss[0m : 2.30445
[1mStep[0m  [64/84], [94mLoss[0m : 2.26694
[1mStep[0m  [72/84], [94mLoss[0m : 2.48321
[1mStep[0m  [80/84], [94mLoss[0m : 2.36177

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54900
[1mStep[0m  [8/84], [94mLoss[0m : 2.08806
[1mStep[0m  [16/84], [94mLoss[0m : 2.39157
[1mStep[0m  [24/84], [94mLoss[0m : 2.19660
[1mStep[0m  [32/84], [94mLoss[0m : 2.30623
[1mStep[0m  [40/84], [94mLoss[0m : 2.16214
[1mStep[0m  [48/84], [94mLoss[0m : 2.33200
[1mStep[0m  [56/84], [94mLoss[0m : 2.28745
[1mStep[0m  [64/84], [94mLoss[0m : 2.52596
[1mStep[0m  [72/84], [94mLoss[0m : 2.00782
[1mStep[0m  [80/84], [94mLoss[0m : 2.56082

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30918
[1mStep[0m  [8/84], [94mLoss[0m : 2.07666
[1mStep[0m  [16/84], [94mLoss[0m : 2.56806
[1mStep[0m  [24/84], [94mLoss[0m : 2.62159
[1mStep[0m  [32/84], [94mLoss[0m : 2.52712
[1mStep[0m  [40/84], [94mLoss[0m : 2.31356
[1mStep[0m  [48/84], [94mLoss[0m : 2.14363
[1mStep[0m  [56/84], [94mLoss[0m : 2.54043
[1mStep[0m  [64/84], [94mLoss[0m : 2.47296
[1mStep[0m  [72/84], [94mLoss[0m : 2.24290
[1mStep[0m  [80/84], [94mLoss[0m : 2.15035

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21165
[1mStep[0m  [8/84], [94mLoss[0m : 2.54814
[1mStep[0m  [16/84], [94mLoss[0m : 2.38872
[1mStep[0m  [24/84], [94mLoss[0m : 2.80730
[1mStep[0m  [32/84], [94mLoss[0m : 2.28097
[1mStep[0m  [40/84], [94mLoss[0m : 2.26788
[1mStep[0m  [48/84], [94mLoss[0m : 2.51324
[1mStep[0m  [56/84], [94mLoss[0m : 2.22655
[1mStep[0m  [64/84], [94mLoss[0m : 2.39357
[1mStep[0m  [72/84], [94mLoss[0m : 2.49336
[1mStep[0m  [80/84], [94mLoss[0m : 2.11974

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30104
[1mStep[0m  [8/84], [94mLoss[0m : 2.37772
[1mStep[0m  [16/84], [94mLoss[0m : 2.21886
[1mStep[0m  [24/84], [94mLoss[0m : 2.07552
[1mStep[0m  [32/84], [94mLoss[0m : 2.10813
[1mStep[0m  [40/84], [94mLoss[0m : 2.46842
[1mStep[0m  [48/84], [94mLoss[0m : 2.09207
[1mStep[0m  [56/84], [94mLoss[0m : 2.34730
[1mStep[0m  [64/84], [94mLoss[0m : 2.14665
[1mStep[0m  [72/84], [94mLoss[0m : 2.46558
[1mStep[0m  [80/84], [94mLoss[0m : 2.43649

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37289
[1mStep[0m  [8/84], [94mLoss[0m : 2.42521
[1mStep[0m  [16/84], [94mLoss[0m : 2.17078
[1mStep[0m  [24/84], [94mLoss[0m : 2.21445
[1mStep[0m  [32/84], [94mLoss[0m : 2.10715
[1mStep[0m  [40/84], [94mLoss[0m : 2.61006
[1mStep[0m  [48/84], [94mLoss[0m : 1.99726
[1mStep[0m  [56/84], [94mLoss[0m : 2.24687
[1mStep[0m  [64/84], [94mLoss[0m : 2.40231
[1mStep[0m  [72/84], [94mLoss[0m : 2.27249
[1mStep[0m  [80/84], [94mLoss[0m : 2.29227

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53121
[1mStep[0m  [8/84], [94mLoss[0m : 2.21917
[1mStep[0m  [16/84], [94mLoss[0m : 2.24448
[1mStep[0m  [24/84], [94mLoss[0m : 2.60226
[1mStep[0m  [32/84], [94mLoss[0m : 2.40745
[1mStep[0m  [40/84], [94mLoss[0m : 1.97983
[1mStep[0m  [48/84], [94mLoss[0m : 2.43588
[1mStep[0m  [56/84], [94mLoss[0m : 2.19282
[1mStep[0m  [64/84], [94mLoss[0m : 2.14300
[1mStep[0m  [72/84], [94mLoss[0m : 2.52040
[1mStep[0m  [80/84], [94mLoss[0m : 2.57847

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21335
[1mStep[0m  [8/84], [94mLoss[0m : 2.32409
[1mStep[0m  [16/84], [94mLoss[0m : 2.37873
[1mStep[0m  [24/84], [94mLoss[0m : 2.21403
[1mStep[0m  [32/84], [94mLoss[0m : 2.34762
[1mStep[0m  [40/84], [94mLoss[0m : 2.24540
[1mStep[0m  [48/84], [94mLoss[0m : 2.60720
[1mStep[0m  [56/84], [94mLoss[0m : 2.36749
[1mStep[0m  [64/84], [94mLoss[0m : 2.22461
[1mStep[0m  [72/84], [94mLoss[0m : 2.26956
[1mStep[0m  [80/84], [94mLoss[0m : 2.58785

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54455
[1mStep[0m  [8/84], [94mLoss[0m : 2.14136
[1mStep[0m  [16/84], [94mLoss[0m : 2.39835
[1mStep[0m  [24/84], [94mLoss[0m : 2.37331
[1mStep[0m  [32/84], [94mLoss[0m : 2.34261
[1mStep[0m  [40/84], [94mLoss[0m : 2.21106
[1mStep[0m  [48/84], [94mLoss[0m : 2.38640
[1mStep[0m  [56/84], [94mLoss[0m : 2.26824
[1mStep[0m  [64/84], [94mLoss[0m : 2.32255
[1mStep[0m  [72/84], [94mLoss[0m : 2.06611
[1mStep[0m  [80/84], [94mLoss[0m : 2.17720

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25463
[1mStep[0m  [8/84], [94mLoss[0m : 2.46320
[1mStep[0m  [16/84], [94mLoss[0m : 2.22107
[1mStep[0m  [24/84], [94mLoss[0m : 2.40165
[1mStep[0m  [32/84], [94mLoss[0m : 2.32471
[1mStep[0m  [40/84], [94mLoss[0m : 2.19901
[1mStep[0m  [48/84], [94mLoss[0m : 2.60354
[1mStep[0m  [56/84], [94mLoss[0m : 2.25201
[1mStep[0m  [64/84], [94mLoss[0m : 2.28388
[1mStep[0m  [72/84], [94mLoss[0m : 2.25742
[1mStep[0m  [80/84], [94mLoss[0m : 2.30237

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.372, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55454
[1mStep[0m  [8/84], [94mLoss[0m : 1.85457
[1mStep[0m  [16/84], [94mLoss[0m : 2.24384
[1mStep[0m  [24/84], [94mLoss[0m : 2.30455
[1mStep[0m  [32/84], [94mLoss[0m : 2.40375
[1mStep[0m  [40/84], [94mLoss[0m : 2.29130
[1mStep[0m  [48/84], [94mLoss[0m : 2.38009
[1mStep[0m  [56/84], [94mLoss[0m : 2.12212
[1mStep[0m  [64/84], [94mLoss[0m : 2.00831
[1mStep[0m  [72/84], [94mLoss[0m : 2.21046
[1mStep[0m  [80/84], [94mLoss[0m : 2.15780

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10124
[1mStep[0m  [8/84], [94mLoss[0m : 2.17942
[1mStep[0m  [16/84], [94mLoss[0m : 2.10523
[1mStep[0m  [24/84], [94mLoss[0m : 2.38544
[1mStep[0m  [32/84], [94mLoss[0m : 2.23125
[1mStep[0m  [40/84], [94mLoss[0m : 2.25654
[1mStep[0m  [48/84], [94mLoss[0m : 2.11522
[1mStep[0m  [56/84], [94mLoss[0m : 2.30318
[1mStep[0m  [64/84], [94mLoss[0m : 2.23403
[1mStep[0m  [72/84], [94mLoss[0m : 2.11278
[1mStep[0m  [80/84], [94mLoss[0m : 2.37161

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21847
[1mStep[0m  [8/84], [94mLoss[0m : 2.27208
[1mStep[0m  [16/84], [94mLoss[0m : 2.33656
[1mStep[0m  [24/84], [94mLoss[0m : 2.10758
[1mStep[0m  [32/84], [94mLoss[0m : 2.47573
[1mStep[0m  [40/84], [94mLoss[0m : 2.59471
[1mStep[0m  [48/84], [94mLoss[0m : 2.33177
[1mStep[0m  [56/84], [94mLoss[0m : 2.38501
[1mStep[0m  [64/84], [94mLoss[0m : 2.50230
[1mStep[0m  [72/84], [94mLoss[0m : 2.00802
[1mStep[0m  [80/84], [94mLoss[0m : 2.26298

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32146
[1mStep[0m  [8/84], [94mLoss[0m : 2.38422
[1mStep[0m  [16/84], [94mLoss[0m : 2.04194
[1mStep[0m  [24/84], [94mLoss[0m : 2.34500
[1mStep[0m  [32/84], [94mLoss[0m : 2.46730
[1mStep[0m  [40/84], [94mLoss[0m : 2.17221
[1mStep[0m  [48/84], [94mLoss[0m : 2.23126
[1mStep[0m  [56/84], [94mLoss[0m : 2.38325
[1mStep[0m  [64/84], [94mLoss[0m : 2.19631
[1mStep[0m  [72/84], [94mLoss[0m : 2.40181
[1mStep[0m  [80/84], [94mLoss[0m : 2.06853

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.341
====================================

Phase 1 - Evaluation MAE:  2.340526270014899
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.52654
[1mStep[0m  [8/84], [94mLoss[0m : 2.57009
[1mStep[0m  [16/84], [94mLoss[0m : 2.19233
[1mStep[0m  [24/84], [94mLoss[0m : 2.25329
[1mStep[0m  [32/84], [94mLoss[0m : 2.35225
[1mStep[0m  [40/84], [94mLoss[0m : 2.24648
[1mStep[0m  [48/84], [94mLoss[0m : 2.52851
[1mStep[0m  [56/84], [94mLoss[0m : 2.48697
[1mStep[0m  [64/84], [94mLoss[0m : 2.50611
[1mStep[0m  [72/84], [94mLoss[0m : 2.30360
[1mStep[0m  [80/84], [94mLoss[0m : 2.98074

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16880
[1mStep[0m  [8/84], [94mLoss[0m : 2.18636
[1mStep[0m  [16/84], [94mLoss[0m : 2.37601
[1mStep[0m  [24/84], [94mLoss[0m : 2.25360
[1mStep[0m  [32/84], [94mLoss[0m : 2.50246
[1mStep[0m  [40/84], [94mLoss[0m : 2.11446
[1mStep[0m  [48/84], [94mLoss[0m : 2.22986
[1mStep[0m  [56/84], [94mLoss[0m : 2.53576
[1mStep[0m  [64/84], [94mLoss[0m : 2.41445
[1mStep[0m  [72/84], [94mLoss[0m : 2.33056
[1mStep[0m  [80/84], [94mLoss[0m : 2.32133

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.681, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38359
[1mStep[0m  [8/84], [94mLoss[0m : 2.40365
[1mStep[0m  [16/84], [94mLoss[0m : 2.38592
[1mStep[0m  [24/84], [94mLoss[0m : 2.39516
[1mStep[0m  [32/84], [94mLoss[0m : 2.19970
[1mStep[0m  [40/84], [94mLoss[0m : 2.44234
[1mStep[0m  [48/84], [94mLoss[0m : 2.18622
[1mStep[0m  [56/84], [94mLoss[0m : 2.20687
[1mStep[0m  [64/84], [94mLoss[0m : 2.72888
[1mStep[0m  [72/84], [94mLoss[0m : 2.44640
[1mStep[0m  [80/84], [94mLoss[0m : 2.35665

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22144
[1mStep[0m  [8/84], [94mLoss[0m : 2.09576
[1mStep[0m  [16/84], [94mLoss[0m : 2.27752
[1mStep[0m  [24/84], [94mLoss[0m : 2.19064
[1mStep[0m  [32/84], [94mLoss[0m : 2.23442
[1mStep[0m  [40/84], [94mLoss[0m : 2.49827
[1mStep[0m  [48/84], [94mLoss[0m : 2.29884
[1mStep[0m  [56/84], [94mLoss[0m : 2.14879
[1mStep[0m  [64/84], [94mLoss[0m : 2.35740
[1mStep[0m  [72/84], [94mLoss[0m : 2.06378
[1mStep[0m  [80/84], [94mLoss[0m : 1.96451

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28728
[1mStep[0m  [8/84], [94mLoss[0m : 2.32800
[1mStep[0m  [16/84], [94mLoss[0m : 2.33131
[1mStep[0m  [24/84], [94mLoss[0m : 1.95790
[1mStep[0m  [32/84], [94mLoss[0m : 2.24828
[1mStep[0m  [40/84], [94mLoss[0m : 1.96702
[1mStep[0m  [48/84], [94mLoss[0m : 2.18788
[1mStep[0m  [56/84], [94mLoss[0m : 1.84388
[1mStep[0m  [64/84], [94mLoss[0m : 2.18570
[1mStep[0m  [72/84], [94mLoss[0m : 2.07312
[1mStep[0m  [80/84], [94mLoss[0m : 1.90945

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.146, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02043
[1mStep[0m  [8/84], [94mLoss[0m : 1.93112
[1mStep[0m  [16/84], [94mLoss[0m : 1.92585
[1mStep[0m  [24/84], [94mLoss[0m : 1.98067
[1mStep[0m  [32/84], [94mLoss[0m : 2.41790
[1mStep[0m  [40/84], [94mLoss[0m : 2.34026
[1mStep[0m  [48/84], [94mLoss[0m : 2.01037
[1mStep[0m  [56/84], [94mLoss[0m : 2.41094
[1mStep[0m  [64/84], [94mLoss[0m : 2.19555
[1mStep[0m  [72/84], [94mLoss[0m : 2.18707
[1mStep[0m  [80/84], [94mLoss[0m : 2.06031

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.107, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83316
[1mStep[0m  [8/84], [94mLoss[0m : 2.13778
[1mStep[0m  [16/84], [94mLoss[0m : 1.80622
[1mStep[0m  [24/84], [94mLoss[0m : 1.89259
[1mStep[0m  [32/84], [94mLoss[0m : 1.94431
[1mStep[0m  [40/84], [94mLoss[0m : 2.08235
[1mStep[0m  [48/84], [94mLoss[0m : 2.22981
[1mStep[0m  [56/84], [94mLoss[0m : 1.91734
[1mStep[0m  [64/84], [94mLoss[0m : 2.20942
[1mStep[0m  [72/84], [94mLoss[0m : 1.89436
[1mStep[0m  [80/84], [94mLoss[0m : 2.00174

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05345
[1mStep[0m  [8/84], [94mLoss[0m : 2.33755
[1mStep[0m  [16/84], [94mLoss[0m : 1.69832
[1mStep[0m  [24/84], [94mLoss[0m : 2.07478
[1mStep[0m  [32/84], [94mLoss[0m : 1.74223
[1mStep[0m  [40/84], [94mLoss[0m : 1.86517
[1mStep[0m  [48/84], [94mLoss[0m : 1.94265
[1mStep[0m  [56/84], [94mLoss[0m : 2.07885
[1mStep[0m  [64/84], [94mLoss[0m : 2.05630
[1mStep[0m  [72/84], [94mLoss[0m : 1.94269
[1mStep[0m  [80/84], [94mLoss[0m : 1.81698

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81630
[1mStep[0m  [8/84], [94mLoss[0m : 1.91521
[1mStep[0m  [16/84], [94mLoss[0m : 1.86622
[1mStep[0m  [24/84], [94mLoss[0m : 2.05737
[1mStep[0m  [32/84], [94mLoss[0m : 2.04484
[1mStep[0m  [40/84], [94mLoss[0m : 2.05800
[1mStep[0m  [48/84], [94mLoss[0m : 1.96720
[1mStep[0m  [56/84], [94mLoss[0m : 1.69355
[1mStep[0m  [64/84], [94mLoss[0m : 2.14917
[1mStep[0m  [72/84], [94mLoss[0m : 1.75575
[1mStep[0m  [80/84], [94mLoss[0m : 1.96699

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92815
[1mStep[0m  [8/84], [94mLoss[0m : 1.89936
[1mStep[0m  [16/84], [94mLoss[0m : 1.89665
[1mStep[0m  [24/84], [94mLoss[0m : 1.77975
[1mStep[0m  [32/84], [94mLoss[0m : 1.79124
[1mStep[0m  [40/84], [94mLoss[0m : 2.01148
[1mStep[0m  [48/84], [94mLoss[0m : 2.15828
[1mStep[0m  [56/84], [94mLoss[0m : 2.26976
[1mStep[0m  [64/84], [94mLoss[0m : 1.73531
[1mStep[0m  [72/84], [94mLoss[0m : 1.88558
[1mStep[0m  [80/84], [94mLoss[0m : 1.80747

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61603
[1mStep[0m  [8/84], [94mLoss[0m : 1.90997
[1mStep[0m  [16/84], [94mLoss[0m : 1.96342
[1mStep[0m  [24/84], [94mLoss[0m : 1.72503
[1mStep[0m  [32/84], [94mLoss[0m : 1.72180
[1mStep[0m  [40/84], [94mLoss[0m : 1.93975
[1mStep[0m  [48/84], [94mLoss[0m : 1.56959
[1mStep[0m  [56/84], [94mLoss[0m : 1.91288
[1mStep[0m  [64/84], [94mLoss[0m : 1.85358
[1mStep[0m  [72/84], [94mLoss[0m : 1.97217
[1mStep[0m  [80/84], [94mLoss[0m : 1.93351

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94989
[1mStep[0m  [8/84], [94mLoss[0m : 1.77159
[1mStep[0m  [16/84], [94mLoss[0m : 1.59376
[1mStep[0m  [24/84], [94mLoss[0m : 1.87493
[1mStep[0m  [32/84], [94mLoss[0m : 1.86202
[1mStep[0m  [40/84], [94mLoss[0m : 1.62057
[1mStep[0m  [48/84], [94mLoss[0m : 1.75410
[1mStep[0m  [56/84], [94mLoss[0m : 1.91276
[1mStep[0m  [64/84], [94mLoss[0m : 1.82562
[1mStep[0m  [72/84], [94mLoss[0m : 1.72273
[1mStep[0m  [80/84], [94mLoss[0m : 1.62253

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60929
[1mStep[0m  [8/84], [94mLoss[0m : 1.52490
[1mStep[0m  [16/84], [94mLoss[0m : 1.50226
[1mStep[0m  [24/84], [94mLoss[0m : 1.85583
[1mStep[0m  [32/84], [94mLoss[0m : 1.97237
[1mStep[0m  [40/84], [94mLoss[0m : 1.78656
[1mStep[0m  [48/84], [94mLoss[0m : 1.79675
[1mStep[0m  [56/84], [94mLoss[0m : 1.49872
[1mStep[0m  [64/84], [94mLoss[0m : 1.70331
[1mStep[0m  [72/84], [94mLoss[0m : 1.81692
[1mStep[0m  [80/84], [94mLoss[0m : 1.77319

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64327
[1mStep[0m  [8/84], [94mLoss[0m : 1.60451
[1mStep[0m  [16/84], [94mLoss[0m : 1.77936
[1mStep[0m  [24/84], [94mLoss[0m : 1.67582
[1mStep[0m  [32/84], [94mLoss[0m : 1.46859
[1mStep[0m  [40/84], [94mLoss[0m : 1.56814
[1mStep[0m  [48/84], [94mLoss[0m : 1.45569
[1mStep[0m  [56/84], [94mLoss[0m : 1.85824
[1mStep[0m  [64/84], [94mLoss[0m : 1.65479
[1mStep[0m  [72/84], [94mLoss[0m : 1.68830
[1mStep[0m  [80/84], [94mLoss[0m : 1.86069

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.528, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54582
[1mStep[0m  [8/84], [94mLoss[0m : 1.69151
[1mStep[0m  [16/84], [94mLoss[0m : 1.58014
[1mStep[0m  [24/84], [94mLoss[0m : 1.46399
[1mStep[0m  [32/84], [94mLoss[0m : 1.69123
[1mStep[0m  [40/84], [94mLoss[0m : 1.66082
[1mStep[0m  [48/84], [94mLoss[0m : 1.79544
[1mStep[0m  [56/84], [94mLoss[0m : 1.71224
[1mStep[0m  [64/84], [94mLoss[0m : 1.64212
[1mStep[0m  [72/84], [94mLoss[0m : 1.65695
[1mStep[0m  [80/84], [94mLoss[0m : 1.93474

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60472
[1mStep[0m  [8/84], [94mLoss[0m : 1.74734
[1mStep[0m  [16/84], [94mLoss[0m : 1.55280
[1mStep[0m  [24/84], [94mLoss[0m : 1.60650
[1mStep[0m  [32/84], [94mLoss[0m : 1.85387
[1mStep[0m  [40/84], [94mLoss[0m : 1.58403
[1mStep[0m  [48/84], [94mLoss[0m : 1.53479
[1mStep[0m  [56/84], [94mLoss[0m : 1.68773
[1mStep[0m  [64/84], [94mLoss[0m : 1.37860
[1mStep[0m  [72/84], [94mLoss[0m : 1.53244
[1mStep[0m  [80/84], [94mLoss[0m : 1.59508

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74470
[1mStep[0m  [8/84], [94mLoss[0m : 1.71412
[1mStep[0m  [16/84], [94mLoss[0m : 1.71232
[1mStep[0m  [24/84], [94mLoss[0m : 1.54434
[1mStep[0m  [32/84], [94mLoss[0m : 1.44785
[1mStep[0m  [40/84], [94mLoss[0m : 1.43963
[1mStep[0m  [48/84], [94mLoss[0m : 1.52343
[1mStep[0m  [56/84], [94mLoss[0m : 1.78663
[1mStep[0m  [64/84], [94mLoss[0m : 1.53870
[1mStep[0m  [72/84], [94mLoss[0m : 1.74988
[1mStep[0m  [80/84], [94mLoss[0m : 1.54768

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59777
[1mStep[0m  [8/84], [94mLoss[0m : 1.71403
[1mStep[0m  [16/84], [94mLoss[0m : 1.61117
[1mStep[0m  [24/84], [94mLoss[0m : 1.51619
[1mStep[0m  [32/84], [94mLoss[0m : 1.40598
[1mStep[0m  [40/84], [94mLoss[0m : 1.73896
[1mStep[0m  [48/84], [94mLoss[0m : 1.62075
[1mStep[0m  [56/84], [94mLoss[0m : 1.69694
[1mStep[0m  [64/84], [94mLoss[0m : 1.50416
[1mStep[0m  [72/84], [94mLoss[0m : 1.55727
[1mStep[0m  [80/84], [94mLoss[0m : 1.52795

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.565, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52461
[1mStep[0m  [8/84], [94mLoss[0m : 1.61171
[1mStep[0m  [16/84], [94mLoss[0m : 1.38715
[1mStep[0m  [24/84], [94mLoss[0m : 1.75158
[1mStep[0m  [32/84], [94mLoss[0m : 1.58053
[1mStep[0m  [40/84], [94mLoss[0m : 1.52190
[1mStep[0m  [48/84], [94mLoss[0m : 1.66503
[1mStep[0m  [56/84], [94mLoss[0m : 1.55367
[1mStep[0m  [64/84], [94mLoss[0m : 1.60224
[1mStep[0m  [72/84], [94mLoss[0m : 1.48154
[1mStep[0m  [80/84], [94mLoss[0m : 1.57708

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.559, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41426
[1mStep[0m  [8/84], [94mLoss[0m : 1.36211
[1mStep[0m  [16/84], [94mLoss[0m : 1.46658
[1mStep[0m  [24/84], [94mLoss[0m : 1.56576
[1mStep[0m  [32/84], [94mLoss[0m : 1.60280
[1mStep[0m  [40/84], [94mLoss[0m : 1.65636
[1mStep[0m  [48/84], [94mLoss[0m : 1.62149
[1mStep[0m  [56/84], [94mLoss[0m : 1.60214
[1mStep[0m  [64/84], [94mLoss[0m : 1.49378
[1mStep[0m  [72/84], [94mLoss[0m : 1.38533
[1mStep[0m  [80/84], [94mLoss[0m : 1.62892

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.528, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39165
[1mStep[0m  [8/84], [94mLoss[0m : 1.70208
[1mStep[0m  [16/84], [94mLoss[0m : 1.34673
[1mStep[0m  [24/84], [94mLoss[0m : 1.45537
[1mStep[0m  [32/84], [94mLoss[0m : 1.44705
[1mStep[0m  [40/84], [94mLoss[0m : 1.48115
[1mStep[0m  [48/84], [94mLoss[0m : 1.62069
[1mStep[0m  [56/84], [94mLoss[0m : 1.66434
[1mStep[0m  [64/84], [94mLoss[0m : 1.49127
[1mStep[0m  [72/84], [94mLoss[0m : 1.42230
[1mStep[0m  [80/84], [94mLoss[0m : 1.65272

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.474, [92mTest[0m: 2.515, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41594
[1mStep[0m  [8/84], [94mLoss[0m : 1.46410
[1mStep[0m  [16/84], [94mLoss[0m : 1.39446
[1mStep[0m  [24/84], [94mLoss[0m : 1.74883
[1mStep[0m  [32/84], [94mLoss[0m : 1.40996
[1mStep[0m  [40/84], [94mLoss[0m : 1.43395
[1mStep[0m  [48/84], [94mLoss[0m : 1.38225
[1mStep[0m  [56/84], [94mLoss[0m : 1.39362
[1mStep[0m  [64/84], [94mLoss[0m : 1.31959
[1mStep[0m  [72/84], [94mLoss[0m : 1.52430
[1mStep[0m  [80/84], [94mLoss[0m : 1.36895

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.508, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28743
[1mStep[0m  [8/84], [94mLoss[0m : 1.36754
[1mStep[0m  [16/84], [94mLoss[0m : 1.28574
[1mStep[0m  [24/84], [94mLoss[0m : 1.39565
[1mStep[0m  [32/84], [94mLoss[0m : 1.52054
[1mStep[0m  [40/84], [94mLoss[0m : 1.25870
[1mStep[0m  [48/84], [94mLoss[0m : 1.31457
[1mStep[0m  [56/84], [94mLoss[0m : 1.52713
[1mStep[0m  [64/84], [94mLoss[0m : 1.39493
[1mStep[0m  [72/84], [94mLoss[0m : 1.35899
[1mStep[0m  [80/84], [94mLoss[0m : 1.61201

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.431, [92mTest[0m: 2.594, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52838
[1mStep[0m  [8/84], [94mLoss[0m : 1.61569
[1mStep[0m  [16/84], [94mLoss[0m : 1.36646
[1mStep[0m  [24/84], [94mLoss[0m : 1.43692
[1mStep[0m  [32/84], [94mLoss[0m : 1.43197
[1mStep[0m  [40/84], [94mLoss[0m : 1.16039
[1mStep[0m  [48/84], [94mLoss[0m : 1.63107
[1mStep[0m  [56/84], [94mLoss[0m : 1.46093
[1mStep[0m  [64/84], [94mLoss[0m : 1.46491
[1mStep[0m  [72/84], [94mLoss[0m : 1.34568
[1mStep[0m  [80/84], [94mLoss[0m : 1.41584

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.430, [92mTest[0m: 2.539, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36550
[1mStep[0m  [8/84], [94mLoss[0m : 1.31103
[1mStep[0m  [16/84], [94mLoss[0m : 1.59713
[1mStep[0m  [24/84], [94mLoss[0m : 1.25374
[1mStep[0m  [32/84], [94mLoss[0m : 1.62053
[1mStep[0m  [40/84], [94mLoss[0m : 1.52096
[1mStep[0m  [48/84], [94mLoss[0m : 1.52685
[1mStep[0m  [56/84], [94mLoss[0m : 1.23488
[1mStep[0m  [64/84], [94mLoss[0m : 1.37783
[1mStep[0m  [72/84], [94mLoss[0m : 1.38042
[1mStep[0m  [80/84], [94mLoss[0m : 1.31780

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.378, [92mTest[0m: 2.547, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.586
====================================

Phase 2 - Evaluation MAE:  2.585824966430664
MAE score P1        2.340526
MAE score P2        2.585825
loss                1.378292
learning_rate           0.01
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.5
weight_decay          0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.28613
[1mStep[0m  [8/84], [94mLoss[0m : 3.20932
[1mStep[0m  [16/84], [94mLoss[0m : 3.67164
[1mStep[0m  [24/84], [94mLoss[0m : 3.72738
[1mStep[0m  [32/84], [94mLoss[0m : 2.77354
[1mStep[0m  [40/84], [94mLoss[0m : 2.79921
[1mStep[0m  [48/84], [94mLoss[0m : 2.72551
[1mStep[0m  [56/84], [94mLoss[0m : 2.28614
[1mStep[0m  [64/84], [94mLoss[0m : 2.58298
[1mStep[0m  [72/84], [94mLoss[0m : 2.78456
[1mStep[0m  [80/84], [94mLoss[0m : 2.59443

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.373, [92mTest[0m: 10.840, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54079
[1mStep[0m  [8/84], [94mLoss[0m : 2.70207
[1mStep[0m  [16/84], [94mLoss[0m : 2.40684
[1mStep[0m  [24/84], [94mLoss[0m : 2.80593
[1mStep[0m  [32/84], [94mLoss[0m : 2.52838
[1mStep[0m  [40/84], [94mLoss[0m : 2.75182
[1mStep[0m  [48/84], [94mLoss[0m : 2.60952
[1mStep[0m  [56/84], [94mLoss[0m : 2.76417
[1mStep[0m  [64/84], [94mLoss[0m : 2.51360
[1mStep[0m  [72/84], [94mLoss[0m : 2.61368
[1mStep[0m  [80/84], [94mLoss[0m : 2.53911

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.547, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28799
[1mStep[0m  [8/84], [94mLoss[0m : 2.70329
[1mStep[0m  [16/84], [94mLoss[0m : 2.78747
[1mStep[0m  [24/84], [94mLoss[0m : 2.45294
[1mStep[0m  [32/84], [94mLoss[0m : 2.45434
[1mStep[0m  [40/84], [94mLoss[0m : 2.69052
[1mStep[0m  [48/84], [94mLoss[0m : 2.35142
[1mStep[0m  [56/84], [94mLoss[0m : 2.36094
[1mStep[0m  [64/84], [94mLoss[0m : 2.66296
[1mStep[0m  [72/84], [94mLoss[0m : 2.88303
[1mStep[0m  [80/84], [94mLoss[0m : 2.87477

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34413
[1mStep[0m  [8/84], [94mLoss[0m : 2.58399
[1mStep[0m  [16/84], [94mLoss[0m : 2.45701
[1mStep[0m  [24/84], [94mLoss[0m : 2.37739
[1mStep[0m  [32/84], [94mLoss[0m : 2.51566
[1mStep[0m  [40/84], [94mLoss[0m : 2.75793
[1mStep[0m  [48/84], [94mLoss[0m : 2.49599
[1mStep[0m  [56/84], [94mLoss[0m : 2.29651
[1mStep[0m  [64/84], [94mLoss[0m : 2.29808
[1mStep[0m  [72/84], [94mLoss[0m : 2.46529
[1mStep[0m  [80/84], [94mLoss[0m : 2.38097

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64675
[1mStep[0m  [8/84], [94mLoss[0m : 2.40368
[1mStep[0m  [16/84], [94mLoss[0m : 2.22955
[1mStep[0m  [24/84], [94mLoss[0m : 2.22794
[1mStep[0m  [32/84], [94mLoss[0m : 2.44204
[1mStep[0m  [40/84], [94mLoss[0m : 2.27727
[1mStep[0m  [48/84], [94mLoss[0m : 2.54628
[1mStep[0m  [56/84], [94mLoss[0m : 2.49812
[1mStep[0m  [64/84], [94mLoss[0m : 2.33562
[1mStep[0m  [72/84], [94mLoss[0m : 2.54214
[1mStep[0m  [80/84], [94mLoss[0m : 2.36775

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55810
[1mStep[0m  [8/84], [94mLoss[0m : 2.67733
[1mStep[0m  [16/84], [94mLoss[0m : 2.36070
[1mStep[0m  [24/84], [94mLoss[0m : 2.47772
[1mStep[0m  [32/84], [94mLoss[0m : 2.60608
[1mStep[0m  [40/84], [94mLoss[0m : 2.34511
[1mStep[0m  [48/84], [94mLoss[0m : 2.49973
[1mStep[0m  [56/84], [94mLoss[0m : 2.32717
[1mStep[0m  [64/84], [94mLoss[0m : 2.27707
[1mStep[0m  [72/84], [94mLoss[0m : 2.27119
[1mStep[0m  [80/84], [94mLoss[0m : 2.42372

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20780
[1mStep[0m  [8/84], [94mLoss[0m : 2.54007
[1mStep[0m  [16/84], [94mLoss[0m : 2.59308
[1mStep[0m  [24/84], [94mLoss[0m : 2.55227
[1mStep[0m  [32/84], [94mLoss[0m : 2.83617
[1mStep[0m  [40/84], [94mLoss[0m : 2.26161
[1mStep[0m  [48/84], [94mLoss[0m : 2.25772
[1mStep[0m  [56/84], [94mLoss[0m : 2.29651
[1mStep[0m  [64/84], [94mLoss[0m : 2.45040
[1mStep[0m  [72/84], [94mLoss[0m : 2.31690
[1mStep[0m  [80/84], [94mLoss[0m : 2.33647

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27103
[1mStep[0m  [8/84], [94mLoss[0m : 2.47241
[1mStep[0m  [16/84], [94mLoss[0m : 2.58889
[1mStep[0m  [24/84], [94mLoss[0m : 2.07462
[1mStep[0m  [32/84], [94mLoss[0m : 2.61205
[1mStep[0m  [40/84], [94mLoss[0m : 2.32213
[1mStep[0m  [48/84], [94mLoss[0m : 2.47004
[1mStep[0m  [56/84], [94mLoss[0m : 2.66275
[1mStep[0m  [64/84], [94mLoss[0m : 2.50927
[1mStep[0m  [72/84], [94mLoss[0m : 2.18133
[1mStep[0m  [80/84], [94mLoss[0m : 2.36800

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77063
[1mStep[0m  [8/84], [94mLoss[0m : 2.29092
[1mStep[0m  [16/84], [94mLoss[0m : 2.66551
[1mStep[0m  [24/84], [94mLoss[0m : 2.23220
[1mStep[0m  [32/84], [94mLoss[0m : 2.21598
[1mStep[0m  [40/84], [94mLoss[0m : 2.28550
[1mStep[0m  [48/84], [94mLoss[0m : 2.44822
[1mStep[0m  [56/84], [94mLoss[0m : 2.44283
[1mStep[0m  [64/84], [94mLoss[0m : 2.55165
[1mStep[0m  [72/84], [94mLoss[0m : 2.61789
[1mStep[0m  [80/84], [94mLoss[0m : 2.43873

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.317, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26232
[1mStep[0m  [8/84], [94mLoss[0m : 2.42353
[1mStep[0m  [16/84], [94mLoss[0m : 2.43715
[1mStep[0m  [24/84], [94mLoss[0m : 2.22174
[1mStep[0m  [32/84], [94mLoss[0m : 2.55095
[1mStep[0m  [40/84], [94mLoss[0m : 2.41203
[1mStep[0m  [48/84], [94mLoss[0m : 2.52282
[1mStep[0m  [56/84], [94mLoss[0m : 2.63947
[1mStep[0m  [64/84], [94mLoss[0m : 2.45608
[1mStep[0m  [72/84], [94mLoss[0m : 2.22610
[1mStep[0m  [80/84], [94mLoss[0m : 2.32300

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.313, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40580
[1mStep[0m  [8/84], [94mLoss[0m : 2.37433
[1mStep[0m  [16/84], [94mLoss[0m : 2.38532
[1mStep[0m  [24/84], [94mLoss[0m : 2.36394
[1mStep[0m  [32/84], [94mLoss[0m : 2.48811
[1mStep[0m  [40/84], [94mLoss[0m : 2.26446
[1mStep[0m  [48/84], [94mLoss[0m : 2.54729
[1mStep[0m  [56/84], [94mLoss[0m : 2.42506
[1mStep[0m  [64/84], [94mLoss[0m : 2.57638
[1mStep[0m  [72/84], [94mLoss[0m : 2.53358
[1mStep[0m  [80/84], [94mLoss[0m : 2.29910

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57418
[1mStep[0m  [8/84], [94mLoss[0m : 2.40542
[1mStep[0m  [16/84], [94mLoss[0m : 2.11051
[1mStep[0m  [24/84], [94mLoss[0m : 2.70946
[1mStep[0m  [32/84], [94mLoss[0m : 2.80150
[1mStep[0m  [40/84], [94mLoss[0m : 2.12663
[1mStep[0m  [48/84], [94mLoss[0m : 2.50460
[1mStep[0m  [56/84], [94mLoss[0m : 2.11591
[1mStep[0m  [64/84], [94mLoss[0m : 2.56708
[1mStep[0m  [72/84], [94mLoss[0m : 2.31271
[1mStep[0m  [80/84], [94mLoss[0m : 2.35662

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.317, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30574
[1mStep[0m  [8/84], [94mLoss[0m : 2.11518
[1mStep[0m  [16/84], [94mLoss[0m : 2.35847
[1mStep[0m  [24/84], [94mLoss[0m : 2.51904
[1mStep[0m  [32/84], [94mLoss[0m : 2.52375
[1mStep[0m  [40/84], [94mLoss[0m : 2.57415
[1mStep[0m  [48/84], [94mLoss[0m : 2.27071
[1mStep[0m  [56/84], [94mLoss[0m : 2.55518
[1mStep[0m  [64/84], [94mLoss[0m : 2.63825
[1mStep[0m  [72/84], [94mLoss[0m : 2.41662
[1mStep[0m  [80/84], [94mLoss[0m : 2.53891

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.306, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44520
[1mStep[0m  [8/84], [94mLoss[0m : 2.44730
[1mStep[0m  [16/84], [94mLoss[0m : 2.73222
[1mStep[0m  [24/84], [94mLoss[0m : 2.43706
[1mStep[0m  [32/84], [94mLoss[0m : 2.38165
[1mStep[0m  [40/84], [94mLoss[0m : 2.20436
[1mStep[0m  [48/84], [94mLoss[0m : 2.65487
[1mStep[0m  [56/84], [94mLoss[0m : 2.19201
[1mStep[0m  [64/84], [94mLoss[0m : 2.14971
[1mStep[0m  [72/84], [94mLoss[0m : 2.29075
[1mStep[0m  [80/84], [94mLoss[0m : 2.47285

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.289, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26218
[1mStep[0m  [8/84], [94mLoss[0m : 2.33619
[1mStep[0m  [16/84], [94mLoss[0m : 2.42408
[1mStep[0m  [24/84], [94mLoss[0m : 1.93210
[1mStep[0m  [32/84], [94mLoss[0m : 2.35204
[1mStep[0m  [40/84], [94mLoss[0m : 2.48501
[1mStep[0m  [48/84], [94mLoss[0m : 2.08251
[1mStep[0m  [56/84], [94mLoss[0m : 2.48428
[1mStep[0m  [64/84], [94mLoss[0m : 2.37363
[1mStep[0m  [72/84], [94mLoss[0m : 2.64265
[1mStep[0m  [80/84], [94mLoss[0m : 2.27599

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65168
[1mStep[0m  [8/84], [94mLoss[0m : 2.34397
[1mStep[0m  [16/84], [94mLoss[0m : 2.25240
[1mStep[0m  [24/84], [94mLoss[0m : 2.35889
[1mStep[0m  [32/84], [94mLoss[0m : 2.14433
[1mStep[0m  [40/84], [94mLoss[0m : 2.17942
[1mStep[0m  [48/84], [94mLoss[0m : 2.58318
[1mStep[0m  [56/84], [94mLoss[0m : 2.38578
[1mStep[0m  [64/84], [94mLoss[0m : 2.41490
[1mStep[0m  [72/84], [94mLoss[0m : 2.47753
[1mStep[0m  [80/84], [94mLoss[0m : 2.67092

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29433
[1mStep[0m  [8/84], [94mLoss[0m : 2.51423
[1mStep[0m  [16/84], [94mLoss[0m : 2.17492
[1mStep[0m  [24/84], [94mLoss[0m : 2.55454
[1mStep[0m  [32/84], [94mLoss[0m : 2.43339
[1mStep[0m  [40/84], [94mLoss[0m : 2.33755
[1mStep[0m  [48/84], [94mLoss[0m : 2.52316
[1mStep[0m  [56/84], [94mLoss[0m : 2.51564
[1mStep[0m  [64/84], [94mLoss[0m : 2.36315
[1mStep[0m  [72/84], [94mLoss[0m : 2.42654
[1mStep[0m  [80/84], [94mLoss[0m : 2.38119

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.304, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21098
[1mStep[0m  [8/84], [94mLoss[0m : 2.32134
[1mStep[0m  [16/84], [94mLoss[0m : 2.33277
[1mStep[0m  [24/84], [94mLoss[0m : 2.23394
[1mStep[0m  [32/84], [94mLoss[0m : 2.38781
[1mStep[0m  [40/84], [94mLoss[0m : 2.23965
[1mStep[0m  [48/84], [94mLoss[0m : 2.45890
[1mStep[0m  [56/84], [94mLoss[0m : 2.52488
[1mStep[0m  [64/84], [94mLoss[0m : 2.48764
[1mStep[0m  [72/84], [94mLoss[0m : 2.53067
[1mStep[0m  [80/84], [94mLoss[0m : 2.54248

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.297, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58254
[1mStep[0m  [8/84], [94mLoss[0m : 2.46101
[1mStep[0m  [16/84], [94mLoss[0m : 2.23678
[1mStep[0m  [24/84], [94mLoss[0m : 2.24581
[1mStep[0m  [32/84], [94mLoss[0m : 2.60174
[1mStep[0m  [40/84], [94mLoss[0m : 2.04192
[1mStep[0m  [48/84], [94mLoss[0m : 2.39432
[1mStep[0m  [56/84], [94mLoss[0m : 1.99495
[1mStep[0m  [64/84], [94mLoss[0m : 2.44461
[1mStep[0m  [72/84], [94mLoss[0m : 2.45719
[1mStep[0m  [80/84], [94mLoss[0m : 2.49336

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.304, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10546
[1mStep[0m  [8/84], [94mLoss[0m : 1.92471
[1mStep[0m  [16/84], [94mLoss[0m : 2.50316
[1mStep[0m  [24/84], [94mLoss[0m : 2.29570
[1mStep[0m  [32/84], [94mLoss[0m : 2.39931
[1mStep[0m  [40/84], [94mLoss[0m : 2.16449
[1mStep[0m  [48/84], [94mLoss[0m : 2.27517
[1mStep[0m  [56/84], [94mLoss[0m : 2.46869
[1mStep[0m  [64/84], [94mLoss[0m : 2.21918
[1mStep[0m  [72/84], [94mLoss[0m : 2.09719
[1mStep[0m  [80/84], [94mLoss[0m : 2.42336

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.292, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66246
[1mStep[0m  [8/84], [94mLoss[0m : 1.99977
[1mStep[0m  [16/84], [94mLoss[0m : 2.50470
[1mStep[0m  [24/84], [94mLoss[0m : 2.34879
[1mStep[0m  [32/84], [94mLoss[0m : 2.34832
[1mStep[0m  [40/84], [94mLoss[0m : 2.51288
[1mStep[0m  [48/84], [94mLoss[0m : 2.38492
[1mStep[0m  [56/84], [94mLoss[0m : 2.35546
[1mStep[0m  [64/84], [94mLoss[0m : 2.53034
[1mStep[0m  [72/84], [94mLoss[0m : 2.67539
[1mStep[0m  [80/84], [94mLoss[0m : 2.42087

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.301, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44020
[1mStep[0m  [8/84], [94mLoss[0m : 2.34885
[1mStep[0m  [16/84], [94mLoss[0m : 2.38197
[1mStep[0m  [24/84], [94mLoss[0m : 1.97510
[1mStep[0m  [32/84], [94mLoss[0m : 2.52373
[1mStep[0m  [40/84], [94mLoss[0m : 2.14319
[1mStep[0m  [48/84], [94mLoss[0m : 2.29894
[1mStep[0m  [56/84], [94mLoss[0m : 2.69128
[1mStep[0m  [64/84], [94mLoss[0m : 2.23446
[1mStep[0m  [72/84], [94mLoss[0m : 2.45010
[1mStep[0m  [80/84], [94mLoss[0m : 2.41762

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.300, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21163
[1mStep[0m  [8/84], [94mLoss[0m : 2.15942
[1mStep[0m  [16/84], [94mLoss[0m : 2.33354
[1mStep[0m  [24/84], [94mLoss[0m : 2.28175
[1mStep[0m  [32/84], [94mLoss[0m : 2.49966
[1mStep[0m  [40/84], [94mLoss[0m : 2.40562
[1mStep[0m  [48/84], [94mLoss[0m : 2.57627
[1mStep[0m  [56/84], [94mLoss[0m : 2.53272
[1mStep[0m  [64/84], [94mLoss[0m : 2.25845
[1mStep[0m  [72/84], [94mLoss[0m : 2.57785
[1mStep[0m  [80/84], [94mLoss[0m : 2.18423

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.308, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22644
[1mStep[0m  [8/84], [94mLoss[0m : 2.34501
[1mStep[0m  [16/84], [94mLoss[0m : 2.48378
[1mStep[0m  [24/84], [94mLoss[0m : 2.36701
[1mStep[0m  [32/84], [94mLoss[0m : 2.26441
[1mStep[0m  [40/84], [94mLoss[0m : 2.23855
[1mStep[0m  [48/84], [94mLoss[0m : 2.22929
[1mStep[0m  [56/84], [94mLoss[0m : 2.54574
[1mStep[0m  [64/84], [94mLoss[0m : 2.32712
[1mStep[0m  [72/84], [94mLoss[0m : 2.29067
[1mStep[0m  [80/84], [94mLoss[0m : 2.23928

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.294, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36120
[1mStep[0m  [8/84], [94mLoss[0m : 2.26848
[1mStep[0m  [16/84], [94mLoss[0m : 2.02102
[1mStep[0m  [24/84], [94mLoss[0m : 2.16941
[1mStep[0m  [32/84], [94mLoss[0m : 2.18112
[1mStep[0m  [40/84], [94mLoss[0m : 2.38137
[1mStep[0m  [48/84], [94mLoss[0m : 2.22418
[1mStep[0m  [56/84], [94mLoss[0m : 2.51125
[1mStep[0m  [64/84], [94mLoss[0m : 2.13559
[1mStep[0m  [72/84], [94mLoss[0m : 2.22313
[1mStep[0m  [80/84], [94mLoss[0m : 2.35973

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.282, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10095
[1mStep[0m  [8/84], [94mLoss[0m : 1.93825
[1mStep[0m  [16/84], [94mLoss[0m : 2.15529
[1mStep[0m  [24/84], [94mLoss[0m : 2.22095
[1mStep[0m  [32/84], [94mLoss[0m : 2.29612
[1mStep[0m  [40/84], [94mLoss[0m : 2.49294
[1mStep[0m  [48/84], [94mLoss[0m : 2.34718
[1mStep[0m  [56/84], [94mLoss[0m : 2.36674
[1mStep[0m  [64/84], [94mLoss[0m : 2.11640
[1mStep[0m  [72/84], [94mLoss[0m : 2.45764
[1mStep[0m  [80/84], [94mLoss[0m : 2.11486

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.288, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34975
[1mStep[0m  [8/84], [94mLoss[0m : 2.06738
[1mStep[0m  [16/84], [94mLoss[0m : 2.23251
[1mStep[0m  [24/84], [94mLoss[0m : 2.16261
[1mStep[0m  [32/84], [94mLoss[0m : 2.22953
[1mStep[0m  [40/84], [94mLoss[0m : 2.10841
[1mStep[0m  [48/84], [94mLoss[0m : 2.21124
[1mStep[0m  [56/84], [94mLoss[0m : 2.35204
[1mStep[0m  [64/84], [94mLoss[0m : 2.16754
[1mStep[0m  [72/84], [94mLoss[0m : 2.31860
[1mStep[0m  [80/84], [94mLoss[0m : 2.20205

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.289, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02976
[1mStep[0m  [8/84], [94mLoss[0m : 2.29350
[1mStep[0m  [16/84], [94mLoss[0m : 2.73841
[1mStep[0m  [24/84], [94mLoss[0m : 2.44165
[1mStep[0m  [32/84], [94mLoss[0m : 2.21127
[1mStep[0m  [40/84], [94mLoss[0m : 2.31422
[1mStep[0m  [48/84], [94mLoss[0m : 2.37725
[1mStep[0m  [56/84], [94mLoss[0m : 2.29546
[1mStep[0m  [64/84], [94mLoss[0m : 2.37641
[1mStep[0m  [72/84], [94mLoss[0m : 2.33209
[1mStep[0m  [80/84], [94mLoss[0m : 2.39905

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.301, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23384
[1mStep[0m  [8/84], [94mLoss[0m : 2.70183
[1mStep[0m  [16/84], [94mLoss[0m : 1.97445
[1mStep[0m  [24/84], [94mLoss[0m : 2.23887
[1mStep[0m  [32/84], [94mLoss[0m : 2.42209
[1mStep[0m  [40/84], [94mLoss[0m : 2.46799
[1mStep[0m  [48/84], [94mLoss[0m : 2.23361
[1mStep[0m  [56/84], [94mLoss[0m : 2.46605
[1mStep[0m  [64/84], [94mLoss[0m : 2.61110
[1mStep[0m  [72/84], [94mLoss[0m : 2.52904
[1mStep[0m  [80/84], [94mLoss[0m : 2.65344

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.289, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94962
[1mStep[0m  [8/84], [94mLoss[0m : 2.40175
[1mStep[0m  [16/84], [94mLoss[0m : 2.36362
[1mStep[0m  [24/84], [94mLoss[0m : 2.12096
[1mStep[0m  [32/84], [94mLoss[0m : 2.48062
[1mStep[0m  [40/84], [94mLoss[0m : 1.97135
[1mStep[0m  [48/84], [94mLoss[0m : 2.33711
[1mStep[0m  [56/84], [94mLoss[0m : 2.35406
[1mStep[0m  [64/84], [94mLoss[0m : 2.77633
[1mStep[0m  [72/84], [94mLoss[0m : 2.37064
[1mStep[0m  [80/84], [94mLoss[0m : 2.15566

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.304, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.282
====================================

Phase 1 - Evaluation MAE:  2.2823092256273543
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.20345
[1mStep[0m  [8/84], [94mLoss[0m : 2.60209
[1mStep[0m  [16/84], [94mLoss[0m : 2.41305
[1mStep[0m  [24/84], [94mLoss[0m : 2.64830
[1mStep[0m  [32/84], [94mLoss[0m : 2.56866
[1mStep[0m  [40/84], [94mLoss[0m : 2.57005
[1mStep[0m  [48/84], [94mLoss[0m : 2.69535
[1mStep[0m  [56/84], [94mLoss[0m : 2.86057
[1mStep[0m  [64/84], [94mLoss[0m : 2.71738
[1mStep[0m  [72/84], [94mLoss[0m : 2.49135
[1mStep[0m  [80/84], [94mLoss[0m : 2.42014

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.275, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52577
[1mStep[0m  [8/84], [94mLoss[0m : 2.15773
[1mStep[0m  [16/84], [94mLoss[0m : 2.38951
[1mStep[0m  [24/84], [94mLoss[0m : 2.30512
[1mStep[0m  [32/84], [94mLoss[0m : 2.36248
[1mStep[0m  [40/84], [94mLoss[0m : 2.56657
[1mStep[0m  [48/84], [94mLoss[0m : 2.31057
[1mStep[0m  [56/84], [94mLoss[0m : 2.24260
[1mStep[0m  [64/84], [94mLoss[0m : 2.30882
[1mStep[0m  [72/84], [94mLoss[0m : 2.50925
[1mStep[0m  [80/84], [94mLoss[0m : 2.28211

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99986
[1mStep[0m  [8/84], [94mLoss[0m : 2.12169
[1mStep[0m  [16/84], [94mLoss[0m : 2.34811
[1mStep[0m  [24/84], [94mLoss[0m : 2.39790
[1mStep[0m  [32/84], [94mLoss[0m : 2.16374
[1mStep[0m  [40/84], [94mLoss[0m : 2.18929
[1mStep[0m  [48/84], [94mLoss[0m : 2.03351
[1mStep[0m  [56/84], [94mLoss[0m : 2.17529
[1mStep[0m  [64/84], [94mLoss[0m : 2.08816
[1mStep[0m  [72/84], [94mLoss[0m : 2.17367
[1mStep[0m  [80/84], [94mLoss[0m : 2.47986

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94516
[1mStep[0m  [8/84], [94mLoss[0m : 1.97685
[1mStep[0m  [16/84], [94mLoss[0m : 1.96886
[1mStep[0m  [24/84], [94mLoss[0m : 2.38694
[1mStep[0m  [32/84], [94mLoss[0m : 1.77554
[1mStep[0m  [40/84], [94mLoss[0m : 2.11747
[1mStep[0m  [48/84], [94mLoss[0m : 2.46764
[1mStep[0m  [56/84], [94mLoss[0m : 2.27561
[1mStep[0m  [64/84], [94mLoss[0m : 2.28007
[1mStep[0m  [72/84], [94mLoss[0m : 2.35598
[1mStep[0m  [80/84], [94mLoss[0m : 2.08986

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.129, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78147
[1mStep[0m  [8/84], [94mLoss[0m : 2.09966
[1mStep[0m  [16/84], [94mLoss[0m : 1.79873
[1mStep[0m  [24/84], [94mLoss[0m : 1.92019
[1mStep[0m  [32/84], [94mLoss[0m : 2.02798
[1mStep[0m  [40/84], [94mLoss[0m : 2.15605
[1mStep[0m  [48/84], [94mLoss[0m : 2.27601
[1mStep[0m  [56/84], [94mLoss[0m : 1.87375
[1mStep[0m  [64/84], [94mLoss[0m : 1.84124
[1mStep[0m  [72/84], [94mLoss[0m : 2.42507
[1mStep[0m  [80/84], [94mLoss[0m : 2.02773

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07877
[1mStep[0m  [8/84], [94mLoss[0m : 2.01707
[1mStep[0m  [16/84], [94mLoss[0m : 2.09836
[1mStep[0m  [24/84], [94mLoss[0m : 1.96351
[1mStep[0m  [32/84], [94mLoss[0m : 1.93673
[1mStep[0m  [40/84], [94mLoss[0m : 2.15309
[1mStep[0m  [48/84], [94mLoss[0m : 2.19701
[1mStep[0m  [56/84], [94mLoss[0m : 2.04102
[1mStep[0m  [64/84], [94mLoss[0m : 1.98104
[1mStep[0m  [72/84], [94mLoss[0m : 2.22960
[1mStep[0m  [80/84], [94mLoss[0m : 1.96559

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74978
[1mStep[0m  [8/84], [94mLoss[0m : 1.88046
[1mStep[0m  [16/84], [94mLoss[0m : 1.93202
[1mStep[0m  [24/84], [94mLoss[0m : 1.72906
[1mStep[0m  [32/84], [94mLoss[0m : 2.00484
[1mStep[0m  [40/84], [94mLoss[0m : 1.86415
[1mStep[0m  [48/84], [94mLoss[0m : 1.92075
[1mStep[0m  [56/84], [94mLoss[0m : 1.80199
[1mStep[0m  [64/84], [94mLoss[0m : 2.03696
[1mStep[0m  [72/84], [94mLoss[0m : 1.91548
[1mStep[0m  [80/84], [94mLoss[0m : 2.04957

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.530, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69065
[1mStep[0m  [8/84], [94mLoss[0m : 1.67228
[1mStep[0m  [16/84], [94mLoss[0m : 1.61595
[1mStep[0m  [24/84], [94mLoss[0m : 1.82505
[1mStep[0m  [32/84], [94mLoss[0m : 1.96032
[1mStep[0m  [40/84], [94mLoss[0m : 2.04615
[1mStep[0m  [48/84], [94mLoss[0m : 1.96445
[1mStep[0m  [56/84], [94mLoss[0m : 2.13121
[1mStep[0m  [64/84], [94mLoss[0m : 1.72424
[1mStep[0m  [72/84], [94mLoss[0m : 1.91420
[1mStep[0m  [80/84], [94mLoss[0m : 2.06221

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80901
[1mStep[0m  [8/84], [94mLoss[0m : 1.73361
[1mStep[0m  [16/84], [94mLoss[0m : 1.99402
[1mStep[0m  [24/84], [94mLoss[0m : 1.79594
[1mStep[0m  [32/84], [94mLoss[0m : 1.62032
[1mStep[0m  [40/84], [94mLoss[0m : 1.89503
[1mStep[0m  [48/84], [94mLoss[0m : 1.81654
[1mStep[0m  [56/84], [94mLoss[0m : 1.73239
[1mStep[0m  [64/84], [94mLoss[0m : 1.85709
[1mStep[0m  [72/84], [94mLoss[0m : 1.80754
[1mStep[0m  [80/84], [94mLoss[0m : 1.97644

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92480
[1mStep[0m  [8/84], [94mLoss[0m : 1.51797
[1mStep[0m  [16/84], [94mLoss[0m : 1.57323
[1mStep[0m  [24/84], [94mLoss[0m : 1.81455
[1mStep[0m  [32/84], [94mLoss[0m : 1.82889
[1mStep[0m  [40/84], [94mLoss[0m : 1.63452
[1mStep[0m  [48/84], [94mLoss[0m : 1.76832
[1mStep[0m  [56/84], [94mLoss[0m : 2.03211
[1mStep[0m  [64/84], [94mLoss[0m : 1.91439
[1mStep[0m  [72/84], [94mLoss[0m : 1.79967
[1mStep[0m  [80/84], [94mLoss[0m : 1.71601

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42518
[1mStep[0m  [8/84], [94mLoss[0m : 1.70571
[1mStep[0m  [16/84], [94mLoss[0m : 1.62066
[1mStep[0m  [24/84], [94mLoss[0m : 1.45966
[1mStep[0m  [32/84], [94mLoss[0m : 1.41871
[1mStep[0m  [40/84], [94mLoss[0m : 1.91166
[1mStep[0m  [48/84], [94mLoss[0m : 1.81431
[1mStep[0m  [56/84], [94mLoss[0m : 1.58092
[1mStep[0m  [64/84], [94mLoss[0m : 1.89483
[1mStep[0m  [72/84], [94mLoss[0m : 1.66286
[1mStep[0m  [80/84], [94mLoss[0m : 1.59121

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61229
[1mStep[0m  [8/84], [94mLoss[0m : 1.34878
[1mStep[0m  [16/84], [94mLoss[0m : 1.64093
[1mStep[0m  [24/84], [94mLoss[0m : 1.50759
[1mStep[0m  [32/84], [94mLoss[0m : 1.60521
[1mStep[0m  [40/84], [94mLoss[0m : 1.59665
[1mStep[0m  [48/84], [94mLoss[0m : 1.70916
[1mStep[0m  [56/84], [94mLoss[0m : 1.85619
[1mStep[0m  [64/84], [94mLoss[0m : 1.58971
[1mStep[0m  [72/84], [94mLoss[0m : 1.70267
[1mStep[0m  [80/84], [94mLoss[0m : 1.70518

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32931
[1mStep[0m  [8/84], [94mLoss[0m : 1.77652
[1mStep[0m  [16/84], [94mLoss[0m : 1.65381
[1mStep[0m  [24/84], [94mLoss[0m : 1.41264
[1mStep[0m  [32/84], [94mLoss[0m : 1.65355
[1mStep[0m  [40/84], [94mLoss[0m : 1.67489
[1mStep[0m  [48/84], [94mLoss[0m : 1.57710
[1mStep[0m  [56/84], [94mLoss[0m : 1.48840
[1mStep[0m  [64/84], [94mLoss[0m : 1.64515
[1mStep[0m  [72/84], [94mLoss[0m : 1.43608
[1mStep[0m  [80/84], [94mLoss[0m : 1.77210

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.628, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74811
[1mStep[0m  [8/84], [94mLoss[0m : 1.60391
[1mStep[0m  [16/84], [94mLoss[0m : 1.44266
[1mStep[0m  [24/84], [94mLoss[0m : 1.47604
[1mStep[0m  [32/84], [94mLoss[0m : 1.62063
[1mStep[0m  [40/84], [94mLoss[0m : 1.71599
[1mStep[0m  [48/84], [94mLoss[0m : 1.52268
[1mStep[0m  [56/84], [94mLoss[0m : 1.67282
[1mStep[0m  [64/84], [94mLoss[0m : 1.89908
[1mStep[0m  [72/84], [94mLoss[0m : 1.46353
[1mStep[0m  [80/84], [94mLoss[0m : 1.58583

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45316
[1mStep[0m  [8/84], [94mLoss[0m : 1.31394
[1mStep[0m  [16/84], [94mLoss[0m : 1.49483
[1mStep[0m  [24/84], [94mLoss[0m : 1.61071
[1mStep[0m  [32/84], [94mLoss[0m : 1.78853
[1mStep[0m  [40/84], [94mLoss[0m : 1.79231
[1mStep[0m  [48/84], [94mLoss[0m : 1.72866
[1mStep[0m  [56/84], [94mLoss[0m : 1.25076
[1mStep[0m  [64/84], [94mLoss[0m : 1.57109
[1mStep[0m  [72/84], [94mLoss[0m : 1.79952
[1mStep[0m  [80/84], [94mLoss[0m : 1.54103

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.565, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45612
[1mStep[0m  [8/84], [94mLoss[0m : 1.71865
[1mStep[0m  [16/84], [94mLoss[0m : 1.42413
[1mStep[0m  [24/84], [94mLoss[0m : 1.37307
[1mStep[0m  [32/84], [94mLoss[0m : 1.47489
[1mStep[0m  [40/84], [94mLoss[0m : 1.41647
[1mStep[0m  [48/84], [94mLoss[0m : 1.58144
[1mStep[0m  [56/84], [94mLoss[0m : 1.64439
[1mStep[0m  [64/84], [94mLoss[0m : 1.60100
[1mStep[0m  [72/84], [94mLoss[0m : 1.48397
[1mStep[0m  [80/84], [94mLoss[0m : 1.49971

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61023
[1mStep[0m  [8/84], [94mLoss[0m : 1.52458
[1mStep[0m  [16/84], [94mLoss[0m : 1.29992
[1mStep[0m  [24/84], [94mLoss[0m : 1.58663
[1mStep[0m  [32/84], [94mLoss[0m : 1.57824
[1mStep[0m  [40/84], [94mLoss[0m : 1.36599
[1mStep[0m  [48/84], [94mLoss[0m : 1.74439
[1mStep[0m  [56/84], [94mLoss[0m : 1.94130
[1mStep[0m  [64/84], [94mLoss[0m : 1.41848
[1mStep[0m  [72/84], [94mLoss[0m : 1.46459
[1mStep[0m  [80/84], [94mLoss[0m : 1.66932

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69923
[1mStep[0m  [8/84], [94mLoss[0m : 1.24566
[1mStep[0m  [16/84], [94mLoss[0m : 1.42254
[1mStep[0m  [24/84], [94mLoss[0m : 1.48204
[1mStep[0m  [32/84], [94mLoss[0m : 1.54245
[1mStep[0m  [40/84], [94mLoss[0m : 1.38366
[1mStep[0m  [48/84], [94mLoss[0m : 1.45821
[1mStep[0m  [56/84], [94mLoss[0m : 1.45494
[1mStep[0m  [64/84], [94mLoss[0m : 1.34569
[1mStep[0m  [72/84], [94mLoss[0m : 1.47753
[1mStep[0m  [80/84], [94mLoss[0m : 1.52122

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.466, [92mTest[0m: 2.563, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30713
[1mStep[0m  [8/84], [94mLoss[0m : 1.52829
[1mStep[0m  [16/84], [94mLoss[0m : 1.39642
[1mStep[0m  [24/84], [94mLoss[0m : 1.57799
[1mStep[0m  [32/84], [94mLoss[0m : 1.49038
[1mStep[0m  [40/84], [94mLoss[0m : 1.53745
[1mStep[0m  [48/84], [94mLoss[0m : 1.59120
[1mStep[0m  [56/84], [94mLoss[0m : 1.55340
[1mStep[0m  [64/84], [94mLoss[0m : 1.57051
[1mStep[0m  [72/84], [94mLoss[0m : 1.53753
[1mStep[0m  [80/84], [94mLoss[0m : 1.37995

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.466, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.38898
[1mStep[0m  [8/84], [94mLoss[0m : 1.39757
[1mStep[0m  [16/84], [94mLoss[0m : 1.35399
[1mStep[0m  [24/84], [94mLoss[0m : 1.40590
[1mStep[0m  [32/84], [94mLoss[0m : 1.23876
[1mStep[0m  [40/84], [94mLoss[0m : 1.39481
[1mStep[0m  [48/84], [94mLoss[0m : 1.18052
[1mStep[0m  [56/84], [94mLoss[0m : 1.26580
[1mStep[0m  [64/84], [94mLoss[0m : 1.74951
[1mStep[0m  [72/84], [94mLoss[0m : 1.21934
[1mStep[0m  [80/84], [94mLoss[0m : 1.57468

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.27154
[1mStep[0m  [8/84], [94mLoss[0m : 1.42194
[1mStep[0m  [16/84], [94mLoss[0m : 1.36123
[1mStep[0m  [24/84], [94mLoss[0m : 1.14620
[1mStep[0m  [32/84], [94mLoss[0m : 1.53868
[1mStep[0m  [40/84], [94mLoss[0m : 1.50414
[1mStep[0m  [48/84], [94mLoss[0m : 1.41668
[1mStep[0m  [56/84], [94mLoss[0m : 1.21762
[1mStep[0m  [64/84], [94mLoss[0m : 1.45582
[1mStep[0m  [72/84], [94mLoss[0m : 1.33256
[1mStep[0m  [80/84], [94mLoss[0m : 1.26363

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.368, [92mTest[0m: 2.527, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.518
====================================

Phase 2 - Evaluation MAE:  2.517828643321991
MAE score P1      2.282309
MAE score P2      2.517829
loss              1.367675
learning_rate         0.01
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.11104
[1mStep[0m  [8/84], [94mLoss[0m : 3.84463
[1mStep[0m  [16/84], [94mLoss[0m : 3.93817
[1mStep[0m  [24/84], [94mLoss[0m : 2.51519
[1mStep[0m  [32/84], [94mLoss[0m : 2.92111
[1mStep[0m  [40/84], [94mLoss[0m : 2.74570
[1mStep[0m  [48/84], [94mLoss[0m : 2.56618
[1mStep[0m  [56/84], [94mLoss[0m : 2.73488
[1mStep[0m  [64/84], [94mLoss[0m : 2.62701
[1mStep[0m  [72/84], [94mLoss[0m : 2.91620
[1mStep[0m  [80/84], [94mLoss[0m : 2.84389

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.390, [92mTest[0m: 10.990, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50081
[1mStep[0m  [8/84], [94mLoss[0m : 2.07262
[1mStep[0m  [16/84], [94mLoss[0m : 2.57127
[1mStep[0m  [24/84], [94mLoss[0m : 2.40988
[1mStep[0m  [32/84], [94mLoss[0m : 2.53751
[1mStep[0m  [40/84], [94mLoss[0m : 2.73476
[1mStep[0m  [48/84], [94mLoss[0m : 3.01367
[1mStep[0m  [56/84], [94mLoss[0m : 2.69670
[1mStep[0m  [64/84], [94mLoss[0m : 2.48829
[1mStep[0m  [72/84], [94mLoss[0m : 2.57023
[1mStep[0m  [80/84], [94mLoss[0m : 2.61430

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35949
[1mStep[0m  [8/84], [94mLoss[0m : 2.74689
[1mStep[0m  [16/84], [94mLoss[0m : 2.62646
[1mStep[0m  [24/84], [94mLoss[0m : 2.40418
[1mStep[0m  [32/84], [94mLoss[0m : 2.75722
[1mStep[0m  [40/84], [94mLoss[0m : 2.64717
[1mStep[0m  [48/84], [94mLoss[0m : 2.52860
[1mStep[0m  [56/84], [94mLoss[0m : 2.61273
[1mStep[0m  [64/84], [94mLoss[0m : 2.72743
[1mStep[0m  [72/84], [94mLoss[0m : 2.42895
[1mStep[0m  [80/84], [94mLoss[0m : 2.45554

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50960
[1mStep[0m  [8/84], [94mLoss[0m : 2.78279
[1mStep[0m  [16/84], [94mLoss[0m : 2.04549
[1mStep[0m  [24/84], [94mLoss[0m : 2.32552
[1mStep[0m  [32/84], [94mLoss[0m : 2.33406
[1mStep[0m  [40/84], [94mLoss[0m : 2.66080
[1mStep[0m  [48/84], [94mLoss[0m : 2.72035
[1mStep[0m  [56/84], [94mLoss[0m : 2.66953
[1mStep[0m  [64/84], [94mLoss[0m : 2.46511
[1mStep[0m  [72/84], [94mLoss[0m : 2.51641
[1mStep[0m  [80/84], [94mLoss[0m : 2.59442

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32261
[1mStep[0m  [8/84], [94mLoss[0m : 2.61846
[1mStep[0m  [16/84], [94mLoss[0m : 2.53191
[1mStep[0m  [24/84], [94mLoss[0m : 2.48016
[1mStep[0m  [32/84], [94mLoss[0m : 2.69865
[1mStep[0m  [40/84], [94mLoss[0m : 2.63520
[1mStep[0m  [48/84], [94mLoss[0m : 2.44116
[1mStep[0m  [56/84], [94mLoss[0m : 2.42440
[1mStep[0m  [64/84], [94mLoss[0m : 2.70506
[1mStep[0m  [72/84], [94mLoss[0m : 2.43702
[1mStep[0m  [80/84], [94mLoss[0m : 2.50949

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52056
[1mStep[0m  [8/84], [94mLoss[0m : 2.40127
[1mStep[0m  [16/84], [94mLoss[0m : 2.56546
[1mStep[0m  [24/84], [94mLoss[0m : 2.58913
[1mStep[0m  [32/84], [94mLoss[0m : 2.46578
[1mStep[0m  [40/84], [94mLoss[0m : 2.45427
[1mStep[0m  [48/84], [94mLoss[0m : 2.43628
[1mStep[0m  [56/84], [94mLoss[0m : 2.80576
[1mStep[0m  [64/84], [94mLoss[0m : 2.75147
[1mStep[0m  [72/84], [94mLoss[0m : 2.32026
[1mStep[0m  [80/84], [94mLoss[0m : 2.38917

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56988
[1mStep[0m  [8/84], [94mLoss[0m : 2.39746
[1mStep[0m  [16/84], [94mLoss[0m : 2.56775
[1mStep[0m  [24/84], [94mLoss[0m : 2.39549
[1mStep[0m  [32/84], [94mLoss[0m : 2.45384
[1mStep[0m  [40/84], [94mLoss[0m : 2.64480
[1mStep[0m  [48/84], [94mLoss[0m : 2.47631
[1mStep[0m  [56/84], [94mLoss[0m : 2.75638
[1mStep[0m  [64/84], [94mLoss[0m : 2.34253
[1mStep[0m  [72/84], [94mLoss[0m : 2.74456
[1mStep[0m  [80/84], [94mLoss[0m : 2.72414

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25166
[1mStep[0m  [8/84], [94mLoss[0m : 2.63663
[1mStep[0m  [16/84], [94mLoss[0m : 2.50714
[1mStep[0m  [24/84], [94mLoss[0m : 2.36236
[1mStep[0m  [32/84], [94mLoss[0m : 2.73318
[1mStep[0m  [40/84], [94mLoss[0m : 2.56313
[1mStep[0m  [48/84], [94mLoss[0m : 2.51979
[1mStep[0m  [56/84], [94mLoss[0m : 2.52456
[1mStep[0m  [64/84], [94mLoss[0m : 2.34540
[1mStep[0m  [72/84], [94mLoss[0m : 2.46037
[1mStep[0m  [80/84], [94mLoss[0m : 2.44762

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74075
[1mStep[0m  [8/84], [94mLoss[0m : 2.68560
[1mStep[0m  [16/84], [94mLoss[0m : 2.46526
[1mStep[0m  [24/84], [94mLoss[0m : 2.64142
[1mStep[0m  [32/84], [94mLoss[0m : 2.67163
[1mStep[0m  [40/84], [94mLoss[0m : 2.96036
[1mStep[0m  [48/84], [94mLoss[0m : 2.65060
[1mStep[0m  [56/84], [94mLoss[0m : 2.70417
[1mStep[0m  [64/84], [94mLoss[0m : 2.48409
[1mStep[0m  [72/84], [94mLoss[0m : 2.54677
[1mStep[0m  [80/84], [94mLoss[0m : 2.43275

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54320
[1mStep[0m  [8/84], [94mLoss[0m : 2.34953
[1mStep[0m  [16/84], [94mLoss[0m : 2.44815
[1mStep[0m  [24/84], [94mLoss[0m : 2.58980
[1mStep[0m  [32/84], [94mLoss[0m : 2.57541
[1mStep[0m  [40/84], [94mLoss[0m : 2.28302
[1mStep[0m  [48/84], [94mLoss[0m : 2.39870
[1mStep[0m  [56/84], [94mLoss[0m : 2.60679
[1mStep[0m  [64/84], [94mLoss[0m : 2.38086
[1mStep[0m  [72/84], [94mLoss[0m : 2.72661
[1mStep[0m  [80/84], [94mLoss[0m : 2.46835

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47411
[1mStep[0m  [8/84], [94mLoss[0m : 2.42519
[1mStep[0m  [16/84], [94mLoss[0m : 2.07224
[1mStep[0m  [24/84], [94mLoss[0m : 2.28013
[1mStep[0m  [32/84], [94mLoss[0m : 2.30798
[1mStep[0m  [40/84], [94mLoss[0m : 2.46008
[1mStep[0m  [48/84], [94mLoss[0m : 2.59113
[1mStep[0m  [56/84], [94mLoss[0m : 2.41464
[1mStep[0m  [64/84], [94mLoss[0m : 2.57967
[1mStep[0m  [72/84], [94mLoss[0m : 2.70162
[1mStep[0m  [80/84], [94mLoss[0m : 2.11759

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55286
[1mStep[0m  [8/84], [94mLoss[0m : 2.79848
[1mStep[0m  [16/84], [94mLoss[0m : 2.25792
[1mStep[0m  [24/84], [94mLoss[0m : 2.41168
[1mStep[0m  [32/84], [94mLoss[0m : 2.63496
[1mStep[0m  [40/84], [94mLoss[0m : 2.45864
[1mStep[0m  [48/84], [94mLoss[0m : 2.19287
[1mStep[0m  [56/84], [94mLoss[0m : 2.61985
[1mStep[0m  [64/84], [94mLoss[0m : 2.53747
[1mStep[0m  [72/84], [94mLoss[0m : 2.55782
[1mStep[0m  [80/84], [94mLoss[0m : 2.45983

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20358
[1mStep[0m  [8/84], [94mLoss[0m : 2.53045
[1mStep[0m  [16/84], [94mLoss[0m : 2.44700
[1mStep[0m  [24/84], [94mLoss[0m : 2.42926
[1mStep[0m  [32/84], [94mLoss[0m : 2.23145
[1mStep[0m  [40/84], [94mLoss[0m : 2.52866
[1mStep[0m  [48/84], [94mLoss[0m : 2.74405
[1mStep[0m  [56/84], [94mLoss[0m : 2.69923
[1mStep[0m  [64/84], [94mLoss[0m : 2.60853
[1mStep[0m  [72/84], [94mLoss[0m : 2.68831
[1mStep[0m  [80/84], [94mLoss[0m : 2.51362

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66990
[1mStep[0m  [8/84], [94mLoss[0m : 2.50563
[1mStep[0m  [16/84], [94mLoss[0m : 2.56985
[1mStep[0m  [24/84], [94mLoss[0m : 2.41671
[1mStep[0m  [32/84], [94mLoss[0m : 2.67034
[1mStep[0m  [40/84], [94mLoss[0m : 2.37681
[1mStep[0m  [48/84], [94mLoss[0m : 2.50480
[1mStep[0m  [56/84], [94mLoss[0m : 2.57387
[1mStep[0m  [64/84], [94mLoss[0m : 2.49179
[1mStep[0m  [72/84], [94mLoss[0m : 2.49516
[1mStep[0m  [80/84], [94mLoss[0m : 2.78684

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73987
[1mStep[0m  [8/84], [94mLoss[0m : 2.40494
[1mStep[0m  [16/84], [94mLoss[0m : 2.42881
[1mStep[0m  [24/84], [94mLoss[0m : 2.23891
[1mStep[0m  [32/84], [94mLoss[0m : 2.55377
[1mStep[0m  [40/84], [94mLoss[0m : 2.68227
[1mStep[0m  [48/84], [94mLoss[0m : 2.48301
[1mStep[0m  [56/84], [94mLoss[0m : 2.56709
[1mStep[0m  [64/84], [94mLoss[0m : 2.48132
[1mStep[0m  [72/84], [94mLoss[0m : 2.53116
[1mStep[0m  [80/84], [94mLoss[0m : 2.61407

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64360
[1mStep[0m  [8/84], [94mLoss[0m : 2.37960
[1mStep[0m  [16/84], [94mLoss[0m : 2.49501
[1mStep[0m  [24/84], [94mLoss[0m : 2.41911
[1mStep[0m  [32/84], [94mLoss[0m : 2.61041
[1mStep[0m  [40/84], [94mLoss[0m : 2.55624
[1mStep[0m  [48/84], [94mLoss[0m : 2.46384
[1mStep[0m  [56/84], [94mLoss[0m : 2.43420
[1mStep[0m  [64/84], [94mLoss[0m : 2.39858
[1mStep[0m  [72/84], [94mLoss[0m : 2.52597
[1mStep[0m  [80/84], [94mLoss[0m : 2.52581

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70211
[1mStep[0m  [8/84], [94mLoss[0m : 2.15371
[1mStep[0m  [16/84], [94mLoss[0m : 2.54857
[1mStep[0m  [24/84], [94mLoss[0m : 2.66329
[1mStep[0m  [32/84], [94mLoss[0m : 2.65133
[1mStep[0m  [40/84], [94mLoss[0m : 2.48526
[1mStep[0m  [48/84], [94mLoss[0m : 2.52152
[1mStep[0m  [56/84], [94mLoss[0m : 2.55771
[1mStep[0m  [64/84], [94mLoss[0m : 2.39895
[1mStep[0m  [72/84], [94mLoss[0m : 2.51686
[1mStep[0m  [80/84], [94mLoss[0m : 2.30996

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73753
[1mStep[0m  [8/84], [94mLoss[0m : 2.62457
[1mStep[0m  [16/84], [94mLoss[0m : 2.53805
[1mStep[0m  [24/84], [94mLoss[0m : 2.57335
[1mStep[0m  [32/84], [94mLoss[0m : 2.53339
[1mStep[0m  [40/84], [94mLoss[0m : 2.56434
[1mStep[0m  [48/84], [94mLoss[0m : 2.69718
[1mStep[0m  [56/84], [94mLoss[0m : 2.65744
[1mStep[0m  [64/84], [94mLoss[0m : 2.46754
[1mStep[0m  [72/84], [94mLoss[0m : 2.41175
[1mStep[0m  [80/84], [94mLoss[0m : 2.61161

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37413
[1mStep[0m  [8/84], [94mLoss[0m : 2.06736
[1mStep[0m  [16/84], [94mLoss[0m : 2.60466
[1mStep[0m  [24/84], [94mLoss[0m : 2.71340
[1mStep[0m  [32/84], [94mLoss[0m : 2.76955
[1mStep[0m  [40/84], [94mLoss[0m : 2.36392
[1mStep[0m  [48/84], [94mLoss[0m : 2.59291
[1mStep[0m  [56/84], [94mLoss[0m : 2.44766
[1mStep[0m  [64/84], [94mLoss[0m : 2.60968
[1mStep[0m  [72/84], [94mLoss[0m : 2.60183
[1mStep[0m  [80/84], [94mLoss[0m : 2.57804

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28528
[1mStep[0m  [8/84], [94mLoss[0m : 2.57151
[1mStep[0m  [16/84], [94mLoss[0m : 2.26795
[1mStep[0m  [24/84], [94mLoss[0m : 2.44461
[1mStep[0m  [32/84], [94mLoss[0m : 2.35075
[1mStep[0m  [40/84], [94mLoss[0m : 2.72628
[1mStep[0m  [48/84], [94mLoss[0m : 2.44487
[1mStep[0m  [56/84], [94mLoss[0m : 2.76943
[1mStep[0m  [64/84], [94mLoss[0m : 2.38811
[1mStep[0m  [72/84], [94mLoss[0m : 2.64539
[1mStep[0m  [80/84], [94mLoss[0m : 2.62854

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31457
[1mStep[0m  [8/84], [94mLoss[0m : 2.60251
[1mStep[0m  [16/84], [94mLoss[0m : 2.71219
[1mStep[0m  [24/84], [94mLoss[0m : 2.32981
[1mStep[0m  [32/84], [94mLoss[0m : 2.49520
[1mStep[0m  [40/84], [94mLoss[0m : 2.48936
[1mStep[0m  [48/84], [94mLoss[0m : 2.60840
[1mStep[0m  [56/84], [94mLoss[0m : 2.20931
[1mStep[0m  [64/84], [94mLoss[0m : 2.67771
[1mStep[0m  [72/84], [94mLoss[0m : 2.29008
[1mStep[0m  [80/84], [94mLoss[0m : 2.41131

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08241
[1mStep[0m  [8/84], [94mLoss[0m : 2.11681
[1mStep[0m  [16/84], [94mLoss[0m : 2.17666
[1mStep[0m  [24/84], [94mLoss[0m : 2.66211
[1mStep[0m  [32/84], [94mLoss[0m : 2.88494
[1mStep[0m  [40/84], [94mLoss[0m : 2.73772
[1mStep[0m  [48/84], [94mLoss[0m : 2.51424
[1mStep[0m  [56/84], [94mLoss[0m : 2.36953
[1mStep[0m  [64/84], [94mLoss[0m : 2.25892
[1mStep[0m  [72/84], [94mLoss[0m : 2.50682
[1mStep[0m  [80/84], [94mLoss[0m : 2.59815

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77030
[1mStep[0m  [8/84], [94mLoss[0m : 2.44683
[1mStep[0m  [16/84], [94mLoss[0m : 2.22264
[1mStep[0m  [24/84], [94mLoss[0m : 2.58751
[1mStep[0m  [32/84], [94mLoss[0m : 2.28249
[1mStep[0m  [40/84], [94mLoss[0m : 2.46986
[1mStep[0m  [48/84], [94mLoss[0m : 2.44658
[1mStep[0m  [56/84], [94mLoss[0m : 2.40072
[1mStep[0m  [64/84], [94mLoss[0m : 2.10414
[1mStep[0m  [72/84], [94mLoss[0m : 2.36539
[1mStep[0m  [80/84], [94mLoss[0m : 2.41771

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31632
[1mStep[0m  [8/84], [94mLoss[0m : 2.37312
[1mStep[0m  [16/84], [94mLoss[0m : 2.73352
[1mStep[0m  [24/84], [94mLoss[0m : 2.50650
[1mStep[0m  [32/84], [94mLoss[0m : 2.52563
[1mStep[0m  [40/84], [94mLoss[0m : 2.34477
[1mStep[0m  [48/84], [94mLoss[0m : 2.33045
[1mStep[0m  [56/84], [94mLoss[0m : 2.65882
[1mStep[0m  [64/84], [94mLoss[0m : 2.28527
[1mStep[0m  [72/84], [94mLoss[0m : 2.42155
[1mStep[0m  [80/84], [94mLoss[0m : 2.53679

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59412
[1mStep[0m  [8/84], [94mLoss[0m : 2.35100
[1mStep[0m  [16/84], [94mLoss[0m : 2.47031
[1mStep[0m  [24/84], [94mLoss[0m : 2.72873
[1mStep[0m  [32/84], [94mLoss[0m : 2.46270
[1mStep[0m  [40/84], [94mLoss[0m : 2.58655
[1mStep[0m  [48/84], [94mLoss[0m : 2.57775
[1mStep[0m  [56/84], [94mLoss[0m : 2.53241
[1mStep[0m  [64/84], [94mLoss[0m : 2.36623
[1mStep[0m  [72/84], [94mLoss[0m : 2.27308
[1mStep[0m  [80/84], [94mLoss[0m : 2.44994

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45051
[1mStep[0m  [8/84], [94mLoss[0m : 2.72016
[1mStep[0m  [16/84], [94mLoss[0m : 2.14915
[1mStep[0m  [24/84], [94mLoss[0m : 2.35064
[1mStep[0m  [32/84], [94mLoss[0m : 2.25091
[1mStep[0m  [40/84], [94mLoss[0m : 2.60806
[1mStep[0m  [48/84], [94mLoss[0m : 2.29437
[1mStep[0m  [56/84], [94mLoss[0m : 2.48793
[1mStep[0m  [64/84], [94mLoss[0m : 2.54881
[1mStep[0m  [72/84], [94mLoss[0m : 2.56544
[1mStep[0m  [80/84], [94mLoss[0m : 2.19587

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22634
[1mStep[0m  [8/84], [94mLoss[0m : 2.46960
[1mStep[0m  [16/84], [94mLoss[0m : 2.56270
[1mStep[0m  [24/84], [94mLoss[0m : 2.47398
[1mStep[0m  [32/84], [94mLoss[0m : 2.26005
[1mStep[0m  [40/84], [94mLoss[0m : 2.34247
[1mStep[0m  [48/84], [94mLoss[0m : 2.76930
[1mStep[0m  [56/84], [94mLoss[0m : 2.42327
[1mStep[0m  [64/84], [94mLoss[0m : 2.44904
[1mStep[0m  [72/84], [94mLoss[0m : 2.56143
[1mStep[0m  [80/84], [94mLoss[0m : 2.39139

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62093
[1mStep[0m  [8/84], [94mLoss[0m : 2.62028
[1mStep[0m  [16/84], [94mLoss[0m : 2.51979
[1mStep[0m  [24/84], [94mLoss[0m : 2.51184
[1mStep[0m  [32/84], [94mLoss[0m : 2.26948
[1mStep[0m  [40/84], [94mLoss[0m : 2.52681
[1mStep[0m  [48/84], [94mLoss[0m : 2.59349
[1mStep[0m  [56/84], [94mLoss[0m : 2.53730
[1mStep[0m  [64/84], [94mLoss[0m : 2.60013
[1mStep[0m  [72/84], [94mLoss[0m : 2.44511
[1mStep[0m  [80/84], [94mLoss[0m : 2.16471

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48100
[1mStep[0m  [8/84], [94mLoss[0m : 2.12030
[1mStep[0m  [16/84], [94mLoss[0m : 2.24551
[1mStep[0m  [24/84], [94mLoss[0m : 2.32516
[1mStep[0m  [32/84], [94mLoss[0m : 2.82411
[1mStep[0m  [40/84], [94mLoss[0m : 2.30060
[1mStep[0m  [48/84], [94mLoss[0m : 2.45171
[1mStep[0m  [56/84], [94mLoss[0m : 2.28773
[1mStep[0m  [64/84], [94mLoss[0m : 2.51486
[1mStep[0m  [72/84], [94mLoss[0m : 2.71816
[1mStep[0m  [80/84], [94mLoss[0m : 2.47974

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39506
[1mStep[0m  [8/84], [94mLoss[0m : 1.93015
[1mStep[0m  [16/84], [94mLoss[0m : 2.39356
[1mStep[0m  [24/84], [94mLoss[0m : 2.40179
[1mStep[0m  [32/84], [94mLoss[0m : 2.83471
[1mStep[0m  [40/84], [94mLoss[0m : 2.56045
[1mStep[0m  [48/84], [94mLoss[0m : 2.13626
[1mStep[0m  [56/84], [94mLoss[0m : 2.64175
[1mStep[0m  [64/84], [94mLoss[0m : 2.54922
[1mStep[0m  [72/84], [94mLoss[0m : 2.59769
[1mStep[0m  [80/84], [94mLoss[0m : 2.57065

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.340
====================================

Phase 1 - Evaluation MAE:  2.3401086245264326
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.02387
[1mStep[0m  [8/84], [94mLoss[0m : 2.39693
[1mStep[0m  [16/84], [94mLoss[0m : 2.43531
[1mStep[0m  [24/84], [94mLoss[0m : 2.48055
[1mStep[0m  [32/84], [94mLoss[0m : 2.76756
[1mStep[0m  [40/84], [94mLoss[0m : 2.37829
[1mStep[0m  [48/84], [94mLoss[0m : 2.23036
[1mStep[0m  [56/84], [94mLoss[0m : 2.37336
[1mStep[0m  [64/84], [94mLoss[0m : 2.67963
[1mStep[0m  [72/84], [94mLoss[0m : 2.73121
[1mStep[0m  [80/84], [94mLoss[0m : 2.46694

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81191
[1mStep[0m  [8/84], [94mLoss[0m : 2.10557
[1mStep[0m  [16/84], [94mLoss[0m : 2.41860
[1mStep[0m  [24/84], [94mLoss[0m : 2.31006
[1mStep[0m  [32/84], [94mLoss[0m : 2.42294
[1mStep[0m  [40/84], [94mLoss[0m : 2.24556
[1mStep[0m  [48/84], [94mLoss[0m : 2.44021
[1mStep[0m  [56/84], [94mLoss[0m : 2.57704
[1mStep[0m  [64/84], [94mLoss[0m : 2.48835
[1mStep[0m  [72/84], [94mLoss[0m : 2.46888
[1mStep[0m  [80/84], [94mLoss[0m : 2.43510

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08421
[1mStep[0m  [8/84], [94mLoss[0m : 2.39062
[1mStep[0m  [16/84], [94mLoss[0m : 2.24013
[1mStep[0m  [24/84], [94mLoss[0m : 2.46171
[1mStep[0m  [32/84], [94mLoss[0m : 2.15597
[1mStep[0m  [40/84], [94mLoss[0m : 2.14863
[1mStep[0m  [48/84], [94mLoss[0m : 2.13226
[1mStep[0m  [56/84], [94mLoss[0m : 2.30170
[1mStep[0m  [64/84], [94mLoss[0m : 2.26530
[1mStep[0m  [72/84], [94mLoss[0m : 2.40298
[1mStep[0m  [80/84], [94mLoss[0m : 2.32852

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48797
[1mStep[0m  [8/84], [94mLoss[0m : 2.25242
[1mStep[0m  [16/84], [94mLoss[0m : 2.12116
[1mStep[0m  [24/84], [94mLoss[0m : 2.17535
[1mStep[0m  [32/84], [94mLoss[0m : 2.22417
[1mStep[0m  [40/84], [94mLoss[0m : 2.07598
[1mStep[0m  [48/84], [94mLoss[0m : 1.91237
[1mStep[0m  [56/84], [94mLoss[0m : 2.22671
[1mStep[0m  [64/84], [94mLoss[0m : 2.23164
[1mStep[0m  [72/84], [94mLoss[0m : 2.08703
[1mStep[0m  [80/84], [94mLoss[0m : 2.41521

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.192, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21286
[1mStep[0m  [8/84], [94mLoss[0m : 1.86609
[1mStep[0m  [16/84], [94mLoss[0m : 2.08156
[1mStep[0m  [24/84], [94mLoss[0m : 2.00912
[1mStep[0m  [32/84], [94mLoss[0m : 2.36170
[1mStep[0m  [40/84], [94mLoss[0m : 2.09092
[1mStep[0m  [48/84], [94mLoss[0m : 2.19583
[1mStep[0m  [56/84], [94mLoss[0m : 2.12906
[1mStep[0m  [64/84], [94mLoss[0m : 2.64670
[1mStep[0m  [72/84], [94mLoss[0m : 2.08236
[1mStep[0m  [80/84], [94mLoss[0m : 2.13202

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04521
[1mStep[0m  [8/84], [94mLoss[0m : 1.78613
[1mStep[0m  [16/84], [94mLoss[0m : 2.04939
[1mStep[0m  [24/84], [94mLoss[0m : 1.95003
[1mStep[0m  [32/84], [94mLoss[0m : 2.28642
[1mStep[0m  [40/84], [94mLoss[0m : 2.04198
[1mStep[0m  [48/84], [94mLoss[0m : 2.04645
[1mStep[0m  [56/84], [94mLoss[0m : 2.24417
[1mStep[0m  [64/84], [94mLoss[0m : 2.25693
[1mStep[0m  [72/84], [94mLoss[0m : 1.86471
[1mStep[0m  [80/84], [94mLoss[0m : 1.95156

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16586
[1mStep[0m  [8/84], [94mLoss[0m : 1.93881
[1mStep[0m  [16/84], [94mLoss[0m : 2.15274
[1mStep[0m  [24/84], [94mLoss[0m : 1.94851
[1mStep[0m  [32/84], [94mLoss[0m : 2.15356
[1mStep[0m  [40/84], [94mLoss[0m : 1.96618
[1mStep[0m  [48/84], [94mLoss[0m : 2.02152
[1mStep[0m  [56/84], [94mLoss[0m : 2.01785
[1mStep[0m  [64/84], [94mLoss[0m : 2.11753
[1mStep[0m  [72/84], [94mLoss[0m : 2.47374
[1mStep[0m  [80/84], [94mLoss[0m : 2.27453

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04236
[1mStep[0m  [8/84], [94mLoss[0m : 2.14387
[1mStep[0m  [16/84], [94mLoss[0m : 1.89857
[1mStep[0m  [24/84], [94mLoss[0m : 1.91078
[1mStep[0m  [32/84], [94mLoss[0m : 2.06827
[1mStep[0m  [40/84], [94mLoss[0m : 2.19996
[1mStep[0m  [48/84], [94mLoss[0m : 2.05752
[1mStep[0m  [56/84], [94mLoss[0m : 2.01235
[1mStep[0m  [64/84], [94mLoss[0m : 1.90214
[1mStep[0m  [72/84], [94mLoss[0m : 1.87509
[1mStep[0m  [80/84], [94mLoss[0m : 2.23466

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79668
[1mStep[0m  [8/84], [94mLoss[0m : 1.91215
[1mStep[0m  [16/84], [94mLoss[0m : 1.90054
[1mStep[0m  [24/84], [94mLoss[0m : 1.94971
[1mStep[0m  [32/84], [94mLoss[0m : 1.84056
[1mStep[0m  [40/84], [94mLoss[0m : 1.96188
[1mStep[0m  [48/84], [94mLoss[0m : 1.94860
[1mStep[0m  [56/84], [94mLoss[0m : 1.86426
[1mStep[0m  [64/84], [94mLoss[0m : 1.96841
[1mStep[0m  [72/84], [94mLoss[0m : 1.94587
[1mStep[0m  [80/84], [94mLoss[0m : 1.98465

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81037
[1mStep[0m  [8/84], [94mLoss[0m : 2.02619
[1mStep[0m  [16/84], [94mLoss[0m : 1.97212
[1mStep[0m  [24/84], [94mLoss[0m : 2.04171
[1mStep[0m  [32/84], [94mLoss[0m : 1.83054
[1mStep[0m  [40/84], [94mLoss[0m : 2.18343
[1mStep[0m  [48/84], [94mLoss[0m : 1.98576
[1mStep[0m  [56/84], [94mLoss[0m : 1.77157
[1mStep[0m  [64/84], [94mLoss[0m : 1.70664
[1mStep[0m  [72/84], [94mLoss[0m : 2.19253
[1mStep[0m  [80/84], [94mLoss[0m : 2.11723

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70066
[1mStep[0m  [8/84], [94mLoss[0m : 1.68300
[1mStep[0m  [16/84], [94mLoss[0m : 1.89787
[1mStep[0m  [24/84], [94mLoss[0m : 1.76084
[1mStep[0m  [32/84], [94mLoss[0m : 1.74981
[1mStep[0m  [40/84], [94mLoss[0m : 1.84094
[1mStep[0m  [48/84], [94mLoss[0m : 2.15352
[1mStep[0m  [56/84], [94mLoss[0m : 1.63065
[1mStep[0m  [64/84], [94mLoss[0m : 1.71517
[1mStep[0m  [72/84], [94mLoss[0m : 2.23310
[1mStep[0m  [80/84], [94mLoss[0m : 2.10571

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76080
[1mStep[0m  [8/84], [94mLoss[0m : 1.67983
[1mStep[0m  [16/84], [94mLoss[0m : 1.95164
[1mStep[0m  [24/84], [94mLoss[0m : 1.81323
[1mStep[0m  [32/84], [94mLoss[0m : 1.56356
[1mStep[0m  [40/84], [94mLoss[0m : 1.63209
[1mStep[0m  [48/84], [94mLoss[0m : 1.80825
[1mStep[0m  [56/84], [94mLoss[0m : 1.75526
[1mStep[0m  [64/84], [94mLoss[0m : 1.66142
[1mStep[0m  [72/84], [94mLoss[0m : 1.87455
[1mStep[0m  [80/84], [94mLoss[0m : 1.93799

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75750
[1mStep[0m  [8/84], [94mLoss[0m : 1.67742
[1mStep[0m  [16/84], [94mLoss[0m : 1.84895
[1mStep[0m  [24/84], [94mLoss[0m : 1.85358
[1mStep[0m  [32/84], [94mLoss[0m : 1.79647
[1mStep[0m  [40/84], [94mLoss[0m : 1.88234
[1mStep[0m  [48/84], [94mLoss[0m : 1.77158
[1mStep[0m  [56/84], [94mLoss[0m : 1.93453
[1mStep[0m  [64/84], [94mLoss[0m : 1.90539
[1mStep[0m  [72/84], [94mLoss[0m : 1.74512
[1mStep[0m  [80/84], [94mLoss[0m : 2.14146

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.827, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71512
[1mStep[0m  [8/84], [94mLoss[0m : 1.61503
[1mStep[0m  [16/84], [94mLoss[0m : 1.63711
[1mStep[0m  [24/84], [94mLoss[0m : 1.76889
[1mStep[0m  [32/84], [94mLoss[0m : 1.75969
[1mStep[0m  [40/84], [94mLoss[0m : 1.87087
[1mStep[0m  [48/84], [94mLoss[0m : 1.68130
[1mStep[0m  [56/84], [94mLoss[0m : 1.60287
[1mStep[0m  [64/84], [94mLoss[0m : 1.75816
[1mStep[0m  [72/84], [94mLoss[0m : 1.76657
[1mStep[0m  [80/84], [94mLoss[0m : 1.73199

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.527, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80946
[1mStep[0m  [8/84], [94mLoss[0m : 1.36264
[1mStep[0m  [16/84], [94mLoss[0m : 1.77847
[1mStep[0m  [24/84], [94mLoss[0m : 1.34852
[1mStep[0m  [32/84], [94mLoss[0m : 1.91531
[1mStep[0m  [40/84], [94mLoss[0m : 2.07921
[1mStep[0m  [48/84], [94mLoss[0m : 1.90559
[1mStep[0m  [56/84], [94mLoss[0m : 1.64095
[1mStep[0m  [64/84], [94mLoss[0m : 1.62087
[1mStep[0m  [72/84], [94mLoss[0m : 1.66596
[1mStep[0m  [80/84], [94mLoss[0m : 1.66356

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67479
[1mStep[0m  [8/84], [94mLoss[0m : 1.67463
[1mStep[0m  [16/84], [94mLoss[0m : 1.66857
[1mStep[0m  [24/84], [94mLoss[0m : 1.69350
[1mStep[0m  [32/84], [94mLoss[0m : 1.59577
[1mStep[0m  [40/84], [94mLoss[0m : 1.75979
[1mStep[0m  [48/84], [94mLoss[0m : 1.78510
[1mStep[0m  [56/84], [94mLoss[0m : 1.53057
[1mStep[0m  [64/84], [94mLoss[0m : 1.67970
[1mStep[0m  [72/84], [94mLoss[0m : 1.47106
[1mStep[0m  [80/84], [94mLoss[0m : 1.65710

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92817
[1mStep[0m  [8/84], [94mLoss[0m : 1.66994
[1mStep[0m  [16/84], [94mLoss[0m : 1.53780
[1mStep[0m  [24/84], [94mLoss[0m : 1.82081
[1mStep[0m  [32/84], [94mLoss[0m : 1.75097
[1mStep[0m  [40/84], [94mLoss[0m : 1.73227
[1mStep[0m  [48/84], [94mLoss[0m : 1.72464
[1mStep[0m  [56/84], [94mLoss[0m : 1.48245
[1mStep[0m  [64/84], [94mLoss[0m : 1.63460
[1mStep[0m  [72/84], [94mLoss[0m : 1.66554
[1mStep[0m  [80/84], [94mLoss[0m : 1.49487

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54929
[1mStep[0m  [8/84], [94mLoss[0m : 1.52817
[1mStep[0m  [16/84], [94mLoss[0m : 1.58622
[1mStep[0m  [24/84], [94mLoss[0m : 1.95470
[1mStep[0m  [32/84], [94mLoss[0m : 1.50695
[1mStep[0m  [40/84], [94mLoss[0m : 1.54538
[1mStep[0m  [48/84], [94mLoss[0m : 1.91283
[1mStep[0m  [56/84], [94mLoss[0m : 1.57853
[1mStep[0m  [64/84], [94mLoss[0m : 1.60411
[1mStep[0m  [72/84], [94mLoss[0m : 1.61572
[1mStep[0m  [80/84], [94mLoss[0m : 1.53808

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57599
[1mStep[0m  [8/84], [94mLoss[0m : 1.43522
[1mStep[0m  [16/84], [94mLoss[0m : 1.64677
[1mStep[0m  [24/84], [94mLoss[0m : 1.61334
[1mStep[0m  [32/84], [94mLoss[0m : 1.74371
[1mStep[0m  [40/84], [94mLoss[0m : 1.88906
[1mStep[0m  [48/84], [94mLoss[0m : 1.55857
[1mStep[0m  [56/84], [94mLoss[0m : 1.59016
[1mStep[0m  [64/84], [94mLoss[0m : 1.52977
[1mStep[0m  [72/84], [94mLoss[0m : 1.68874
[1mStep[0m  [80/84], [94mLoss[0m : 1.40006

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.530, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67616
[1mStep[0m  [8/84], [94mLoss[0m : 1.60391
[1mStep[0m  [16/84], [94mLoss[0m : 1.40681
[1mStep[0m  [24/84], [94mLoss[0m : 1.50762
[1mStep[0m  [32/84], [94mLoss[0m : 1.33875
[1mStep[0m  [40/84], [94mLoss[0m : 1.62620
[1mStep[0m  [48/84], [94mLoss[0m : 1.59878
[1mStep[0m  [56/84], [94mLoss[0m : 2.08097
[1mStep[0m  [64/84], [94mLoss[0m : 1.79868
[1mStep[0m  [72/84], [94mLoss[0m : 1.62906
[1mStep[0m  [80/84], [94mLoss[0m : 1.56930

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.528, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53961
[1mStep[0m  [8/84], [94mLoss[0m : 1.58368
[1mStep[0m  [16/84], [94mLoss[0m : 1.72557
[1mStep[0m  [24/84], [94mLoss[0m : 1.35960
[1mStep[0m  [32/84], [94mLoss[0m : 1.61735
[1mStep[0m  [40/84], [94mLoss[0m : 1.37987
[1mStep[0m  [48/84], [94mLoss[0m : 1.67346
[1mStep[0m  [56/84], [94mLoss[0m : 1.23251
[1mStep[0m  [64/84], [94mLoss[0m : 1.35126
[1mStep[0m  [72/84], [94mLoss[0m : 1.63525
[1mStep[0m  [80/84], [94mLoss[0m : 1.59068

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.522, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59657
[1mStep[0m  [8/84], [94mLoss[0m : 1.51339
[1mStep[0m  [16/84], [94mLoss[0m : 1.56208
[1mStep[0m  [24/84], [94mLoss[0m : 1.57809
[1mStep[0m  [32/84], [94mLoss[0m : 1.53263
[1mStep[0m  [40/84], [94mLoss[0m : 1.69120
[1mStep[0m  [48/84], [94mLoss[0m : 1.57594
[1mStep[0m  [56/84], [94mLoss[0m : 1.46937
[1mStep[0m  [64/84], [94mLoss[0m : 1.61065
[1mStep[0m  [72/84], [94mLoss[0m : 1.60411
[1mStep[0m  [80/84], [94mLoss[0m : 1.52512

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.518, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58218
[1mStep[0m  [8/84], [94mLoss[0m : 1.43013
[1mStep[0m  [16/84], [94mLoss[0m : 1.31381
[1mStep[0m  [24/84], [94mLoss[0m : 1.41941
[1mStep[0m  [32/84], [94mLoss[0m : 1.56501
[1mStep[0m  [40/84], [94mLoss[0m : 1.39586
[1mStep[0m  [48/84], [94mLoss[0m : 1.46089
[1mStep[0m  [56/84], [94mLoss[0m : 1.28372
[1mStep[0m  [64/84], [94mLoss[0m : 1.57931
[1mStep[0m  [72/84], [94mLoss[0m : 1.38300
[1mStep[0m  [80/84], [94mLoss[0m : 1.54943

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48351
[1mStep[0m  [8/84], [94mLoss[0m : 1.34245
[1mStep[0m  [16/84], [94mLoss[0m : 1.63828
[1mStep[0m  [24/84], [94mLoss[0m : 1.40159
[1mStep[0m  [32/84], [94mLoss[0m : 1.34429
[1mStep[0m  [40/84], [94mLoss[0m : 1.49315
[1mStep[0m  [48/84], [94mLoss[0m : 1.63510
[1mStep[0m  [56/84], [94mLoss[0m : 1.53565
[1mStep[0m  [64/84], [94mLoss[0m : 1.51664
[1mStep[0m  [72/84], [94mLoss[0m : 1.61030
[1mStep[0m  [80/84], [94mLoss[0m : 1.60218

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.494, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51209
[1mStep[0m  [8/84], [94mLoss[0m : 1.46314
[1mStep[0m  [16/84], [94mLoss[0m : 1.50042
[1mStep[0m  [24/84], [94mLoss[0m : 1.30111
[1mStep[0m  [32/84], [94mLoss[0m : 1.58198
[1mStep[0m  [40/84], [94mLoss[0m : 1.55051
[1mStep[0m  [48/84], [94mLoss[0m : 1.37682
[1mStep[0m  [56/84], [94mLoss[0m : 1.36713
[1mStep[0m  [64/84], [94mLoss[0m : 1.47375
[1mStep[0m  [72/84], [94mLoss[0m : 1.51595
[1mStep[0m  [80/84], [94mLoss[0m : 1.51281

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28622
[1mStep[0m  [8/84], [94mLoss[0m : 1.54471
[1mStep[0m  [16/84], [94mLoss[0m : 1.42265
[1mStep[0m  [24/84], [94mLoss[0m : 1.31025
[1mStep[0m  [32/84], [94mLoss[0m : 1.39603
[1mStep[0m  [40/84], [94mLoss[0m : 1.51028
[1mStep[0m  [48/84], [94mLoss[0m : 1.40182
[1mStep[0m  [56/84], [94mLoss[0m : 1.41786
[1mStep[0m  [64/84], [94mLoss[0m : 1.55370
[1mStep[0m  [72/84], [94mLoss[0m : 1.64174
[1mStep[0m  [80/84], [94mLoss[0m : 1.82213

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48910
[1mStep[0m  [8/84], [94mLoss[0m : 1.48798
[1mStep[0m  [16/84], [94mLoss[0m : 1.43889
[1mStep[0m  [24/84], [94mLoss[0m : 1.40425
[1mStep[0m  [32/84], [94mLoss[0m : 1.44360
[1mStep[0m  [40/84], [94mLoss[0m : 1.48314
[1mStep[0m  [48/84], [94mLoss[0m : 1.31650
[1mStep[0m  [56/84], [94mLoss[0m : 1.44904
[1mStep[0m  [64/84], [94mLoss[0m : 1.44351
[1mStep[0m  [72/84], [94mLoss[0m : 1.43693
[1mStep[0m  [80/84], [94mLoss[0m : 1.36754

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.458, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69781
[1mStep[0m  [8/84], [94mLoss[0m : 1.33834
[1mStep[0m  [16/84], [94mLoss[0m : 1.39426
[1mStep[0m  [24/84], [94mLoss[0m : 1.40407
[1mStep[0m  [32/84], [94mLoss[0m : 1.46450
[1mStep[0m  [40/84], [94mLoss[0m : 1.40647
[1mStep[0m  [48/84], [94mLoss[0m : 1.50716
[1mStep[0m  [56/84], [94mLoss[0m : 1.29469
[1mStep[0m  [64/84], [94mLoss[0m : 1.60437
[1mStep[0m  [72/84], [94mLoss[0m : 1.46873
[1mStep[0m  [80/84], [94mLoss[0m : 1.54198

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.433, [92mTest[0m: 2.520, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.29361
[1mStep[0m  [8/84], [94mLoss[0m : 1.68764
[1mStep[0m  [16/84], [94mLoss[0m : 1.39807
[1mStep[0m  [24/84], [94mLoss[0m : 1.48526
[1mStep[0m  [32/84], [94mLoss[0m : 1.40578
[1mStep[0m  [40/84], [94mLoss[0m : 1.53110
[1mStep[0m  [48/84], [94mLoss[0m : 1.28524
[1mStep[0m  [56/84], [94mLoss[0m : 1.34792
[1mStep[0m  [64/84], [94mLoss[0m : 1.35630
[1mStep[0m  [72/84], [94mLoss[0m : 1.65076
[1mStep[0m  [80/84], [94mLoss[0m : 1.40270

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.502
====================================

Phase 2 - Evaluation MAE:  2.501653713839395
MAE score P1        2.340109
MAE score P2        2.501654
loss                1.411391
learning_rate           0.01
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.9
weight_decay           0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.04987
[1mStep[0m  [4/42], [94mLoss[0m : 10.73981
[1mStep[0m  [8/42], [94mLoss[0m : 10.37030
[1mStep[0m  [12/42], [94mLoss[0m : 10.26492
[1mStep[0m  [16/42], [94mLoss[0m : 9.59840
[1mStep[0m  [20/42], [94mLoss[0m : 9.26686
[1mStep[0m  [24/42], [94mLoss[0m : 8.18074
[1mStep[0m  [28/42], [94mLoss[0m : 7.44609
[1mStep[0m  [32/42], [94mLoss[0m : 6.83956
[1mStep[0m  [36/42], [94mLoss[0m : 6.21952
[1mStep[0m  [40/42], [94mLoss[0m : 5.82244

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.634, [92mTest[0m: 10.939, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.24243
[1mStep[0m  [4/42], [94mLoss[0m : 5.05416
[1mStep[0m  [8/42], [94mLoss[0m : 4.48725
[1mStep[0m  [12/42], [94mLoss[0m : 3.84006
[1mStep[0m  [16/42], [94mLoss[0m : 3.28729
[1mStep[0m  [20/42], [94mLoss[0m : 2.70380
[1mStep[0m  [24/42], [94mLoss[0m : 2.94021
[1mStep[0m  [28/42], [94mLoss[0m : 2.71723
[1mStep[0m  [32/42], [94mLoss[0m : 2.54487
[1mStep[0m  [36/42], [94mLoss[0m : 2.92980
[1mStep[0m  [40/42], [94mLoss[0m : 2.66547

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.474, [92mTest[0m: 5.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68529
[1mStep[0m  [4/42], [94mLoss[0m : 2.56077
[1mStep[0m  [8/42], [94mLoss[0m : 2.46187
[1mStep[0m  [12/42], [94mLoss[0m : 2.68573
[1mStep[0m  [16/42], [94mLoss[0m : 2.59655
[1mStep[0m  [20/42], [94mLoss[0m : 2.67563
[1mStep[0m  [24/42], [94mLoss[0m : 2.55493
[1mStep[0m  [28/42], [94mLoss[0m : 2.64613
[1mStep[0m  [32/42], [94mLoss[0m : 2.65866
[1mStep[0m  [36/42], [94mLoss[0m : 2.62983
[1mStep[0m  [40/42], [94mLoss[0m : 2.85378

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.561, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73090
[1mStep[0m  [4/42], [94mLoss[0m : 2.48084
[1mStep[0m  [8/42], [94mLoss[0m : 2.37441
[1mStep[0m  [12/42], [94mLoss[0m : 2.48072
[1mStep[0m  [16/42], [94mLoss[0m : 2.35109
[1mStep[0m  [20/42], [94mLoss[0m : 2.52692
[1mStep[0m  [24/42], [94mLoss[0m : 2.51589
[1mStep[0m  [28/42], [94mLoss[0m : 2.55486
[1mStep[0m  [32/42], [94mLoss[0m : 2.32653
[1mStep[0m  [36/42], [94mLoss[0m : 2.45894
[1mStep[0m  [40/42], [94mLoss[0m : 2.45834

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56692
[1mStep[0m  [4/42], [94mLoss[0m : 2.44210
[1mStep[0m  [8/42], [94mLoss[0m : 2.55768
[1mStep[0m  [12/42], [94mLoss[0m : 2.51121
[1mStep[0m  [16/42], [94mLoss[0m : 2.59220
[1mStep[0m  [20/42], [94mLoss[0m : 2.49498
[1mStep[0m  [24/42], [94mLoss[0m : 2.68835
[1mStep[0m  [28/42], [94mLoss[0m : 2.40377
[1mStep[0m  [32/42], [94mLoss[0m : 2.64040
[1mStep[0m  [36/42], [94mLoss[0m : 2.37013
[1mStep[0m  [40/42], [94mLoss[0m : 2.45077

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47298
[1mStep[0m  [4/42], [94mLoss[0m : 2.53376
[1mStep[0m  [8/42], [94mLoss[0m : 2.27739
[1mStep[0m  [12/42], [94mLoss[0m : 2.60287
[1mStep[0m  [16/42], [94mLoss[0m : 2.80241
[1mStep[0m  [20/42], [94mLoss[0m : 2.56888
[1mStep[0m  [24/42], [94mLoss[0m : 2.43280
[1mStep[0m  [28/42], [94mLoss[0m : 2.46874
[1mStep[0m  [32/42], [94mLoss[0m : 2.51917
[1mStep[0m  [36/42], [94mLoss[0m : 2.47331
[1mStep[0m  [40/42], [94mLoss[0m : 2.50588

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48740
[1mStep[0m  [4/42], [94mLoss[0m : 2.43218
[1mStep[0m  [8/42], [94mLoss[0m : 2.39367
[1mStep[0m  [12/42], [94mLoss[0m : 2.49014
[1mStep[0m  [16/42], [94mLoss[0m : 2.43205
[1mStep[0m  [20/42], [94mLoss[0m : 2.39164
[1mStep[0m  [24/42], [94mLoss[0m : 2.45029
[1mStep[0m  [28/42], [94mLoss[0m : 2.66341
[1mStep[0m  [32/42], [94mLoss[0m : 2.32993
[1mStep[0m  [36/42], [94mLoss[0m : 2.79052
[1mStep[0m  [40/42], [94mLoss[0m : 2.34135

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68342
[1mStep[0m  [4/42], [94mLoss[0m : 2.35882
[1mStep[0m  [8/42], [94mLoss[0m : 2.47607
[1mStep[0m  [12/42], [94mLoss[0m : 2.35581
[1mStep[0m  [16/42], [94mLoss[0m : 2.51207
[1mStep[0m  [20/42], [94mLoss[0m : 2.86177
[1mStep[0m  [24/42], [94mLoss[0m : 2.43039
[1mStep[0m  [28/42], [94mLoss[0m : 2.46572
[1mStep[0m  [32/42], [94mLoss[0m : 2.48808
[1mStep[0m  [36/42], [94mLoss[0m : 2.29444
[1mStep[0m  [40/42], [94mLoss[0m : 2.39744

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18474
[1mStep[0m  [4/42], [94mLoss[0m : 2.16142
[1mStep[0m  [8/42], [94mLoss[0m : 2.32864
[1mStep[0m  [12/42], [94mLoss[0m : 2.65902
[1mStep[0m  [16/42], [94mLoss[0m : 2.52230
[1mStep[0m  [20/42], [94mLoss[0m : 2.43690
[1mStep[0m  [24/42], [94mLoss[0m : 2.50324
[1mStep[0m  [28/42], [94mLoss[0m : 2.45418
[1mStep[0m  [32/42], [94mLoss[0m : 2.35738
[1mStep[0m  [36/42], [94mLoss[0m : 2.71845
[1mStep[0m  [40/42], [94mLoss[0m : 2.29142

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47042
[1mStep[0m  [4/42], [94mLoss[0m : 2.59961
[1mStep[0m  [8/42], [94mLoss[0m : 2.08285
[1mStep[0m  [12/42], [94mLoss[0m : 2.16793
[1mStep[0m  [16/42], [94mLoss[0m : 2.53585
[1mStep[0m  [20/42], [94mLoss[0m : 2.52209
[1mStep[0m  [24/42], [94mLoss[0m : 2.28700
[1mStep[0m  [28/42], [94mLoss[0m : 2.45768
[1mStep[0m  [32/42], [94mLoss[0m : 2.52006
[1mStep[0m  [36/42], [94mLoss[0m : 2.62652
[1mStep[0m  [40/42], [94mLoss[0m : 2.58237

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40562
[1mStep[0m  [4/42], [94mLoss[0m : 2.53569
[1mStep[0m  [8/42], [94mLoss[0m : 2.23302
[1mStep[0m  [12/42], [94mLoss[0m : 2.44907
[1mStep[0m  [16/42], [94mLoss[0m : 2.45843
[1mStep[0m  [20/42], [94mLoss[0m : 2.57318
[1mStep[0m  [24/42], [94mLoss[0m : 2.42127
[1mStep[0m  [28/42], [94mLoss[0m : 2.66119
[1mStep[0m  [32/42], [94mLoss[0m : 2.36141
[1mStep[0m  [36/42], [94mLoss[0m : 2.40042
[1mStep[0m  [40/42], [94mLoss[0m : 2.28951

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42334
[1mStep[0m  [4/42], [94mLoss[0m : 2.41983
[1mStep[0m  [8/42], [94mLoss[0m : 2.42192
[1mStep[0m  [12/42], [94mLoss[0m : 2.45020
[1mStep[0m  [16/42], [94mLoss[0m : 2.40457
[1mStep[0m  [20/42], [94mLoss[0m : 2.53262
[1mStep[0m  [24/42], [94mLoss[0m : 2.25860
[1mStep[0m  [28/42], [94mLoss[0m : 2.31981
[1mStep[0m  [32/42], [94mLoss[0m : 2.55709
[1mStep[0m  [36/42], [94mLoss[0m : 2.51778
[1mStep[0m  [40/42], [94mLoss[0m : 2.49723

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41922
[1mStep[0m  [4/42], [94mLoss[0m : 2.41152
[1mStep[0m  [8/42], [94mLoss[0m : 2.52164
[1mStep[0m  [12/42], [94mLoss[0m : 2.50974
[1mStep[0m  [16/42], [94mLoss[0m : 2.34152
[1mStep[0m  [20/42], [94mLoss[0m : 2.49388
[1mStep[0m  [24/42], [94mLoss[0m : 2.34915
[1mStep[0m  [28/42], [94mLoss[0m : 2.37873
[1mStep[0m  [32/42], [94mLoss[0m : 2.49919
[1mStep[0m  [36/42], [94mLoss[0m : 2.28836
[1mStep[0m  [40/42], [94mLoss[0m : 2.37455

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50678
[1mStep[0m  [4/42], [94mLoss[0m : 2.31536
[1mStep[0m  [8/42], [94mLoss[0m : 2.31713
[1mStep[0m  [12/42], [94mLoss[0m : 2.26113
[1mStep[0m  [16/42], [94mLoss[0m : 2.38739
[1mStep[0m  [20/42], [94mLoss[0m : 2.51650
[1mStep[0m  [24/42], [94mLoss[0m : 2.41973
[1mStep[0m  [28/42], [94mLoss[0m : 2.55116
[1mStep[0m  [32/42], [94mLoss[0m : 2.37812
[1mStep[0m  [36/42], [94mLoss[0m : 2.32370
[1mStep[0m  [40/42], [94mLoss[0m : 2.56771

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43655
[1mStep[0m  [4/42], [94mLoss[0m : 2.69970
[1mStep[0m  [8/42], [94mLoss[0m : 2.61528
[1mStep[0m  [12/42], [94mLoss[0m : 2.42327
[1mStep[0m  [16/42], [94mLoss[0m : 2.42287
[1mStep[0m  [20/42], [94mLoss[0m : 2.48803
[1mStep[0m  [24/42], [94mLoss[0m : 2.35536
[1mStep[0m  [28/42], [94mLoss[0m : 2.55646
[1mStep[0m  [32/42], [94mLoss[0m : 2.33811
[1mStep[0m  [36/42], [94mLoss[0m : 2.27821
[1mStep[0m  [40/42], [94mLoss[0m : 2.45396

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38315
[1mStep[0m  [4/42], [94mLoss[0m : 2.17286
[1mStep[0m  [8/42], [94mLoss[0m : 2.45457
[1mStep[0m  [12/42], [94mLoss[0m : 2.34354
[1mStep[0m  [16/42], [94mLoss[0m : 2.28200
[1mStep[0m  [20/42], [94mLoss[0m : 2.42641
[1mStep[0m  [24/42], [94mLoss[0m : 2.52135
[1mStep[0m  [28/42], [94mLoss[0m : 2.27753
[1mStep[0m  [32/42], [94mLoss[0m : 2.50851
[1mStep[0m  [36/42], [94mLoss[0m : 2.60036
[1mStep[0m  [40/42], [94mLoss[0m : 2.53501

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35495
[1mStep[0m  [4/42], [94mLoss[0m : 2.27186
[1mStep[0m  [8/42], [94mLoss[0m : 2.47315
[1mStep[0m  [12/42], [94mLoss[0m : 2.48673
[1mStep[0m  [16/42], [94mLoss[0m : 2.49945
[1mStep[0m  [20/42], [94mLoss[0m : 2.37585
[1mStep[0m  [24/42], [94mLoss[0m : 2.27713
[1mStep[0m  [28/42], [94mLoss[0m : 2.42395
[1mStep[0m  [32/42], [94mLoss[0m : 2.24228
[1mStep[0m  [36/42], [94mLoss[0m : 2.43760
[1mStep[0m  [40/42], [94mLoss[0m : 2.58335

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14788
[1mStep[0m  [4/42], [94mLoss[0m : 2.53509
[1mStep[0m  [8/42], [94mLoss[0m : 2.45459
[1mStep[0m  [12/42], [94mLoss[0m : 2.53995
[1mStep[0m  [16/42], [94mLoss[0m : 2.43961
[1mStep[0m  [20/42], [94mLoss[0m : 2.27049
[1mStep[0m  [24/42], [94mLoss[0m : 2.54604
[1mStep[0m  [28/42], [94mLoss[0m : 2.47402
[1mStep[0m  [32/42], [94mLoss[0m : 2.45958
[1mStep[0m  [36/42], [94mLoss[0m : 2.39196
[1mStep[0m  [40/42], [94mLoss[0m : 2.24327

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46254
[1mStep[0m  [4/42], [94mLoss[0m : 2.57002
[1mStep[0m  [8/42], [94mLoss[0m : 2.29435
[1mStep[0m  [12/42], [94mLoss[0m : 2.48049
[1mStep[0m  [16/42], [94mLoss[0m : 2.40706
[1mStep[0m  [20/42], [94mLoss[0m : 2.55072
[1mStep[0m  [24/42], [94mLoss[0m : 2.40540
[1mStep[0m  [28/42], [94mLoss[0m : 2.32984
[1mStep[0m  [32/42], [94mLoss[0m : 2.35830
[1mStep[0m  [36/42], [94mLoss[0m : 2.49205
[1mStep[0m  [40/42], [94mLoss[0m : 2.43447

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48740
[1mStep[0m  [4/42], [94mLoss[0m : 2.55060
[1mStep[0m  [8/42], [94mLoss[0m : 2.60845
[1mStep[0m  [12/42], [94mLoss[0m : 2.44328
[1mStep[0m  [16/42], [94mLoss[0m : 2.49765
[1mStep[0m  [20/42], [94mLoss[0m : 2.36776
[1mStep[0m  [24/42], [94mLoss[0m : 2.52571
[1mStep[0m  [28/42], [94mLoss[0m : 2.49803
[1mStep[0m  [32/42], [94mLoss[0m : 2.71878
[1mStep[0m  [36/42], [94mLoss[0m : 2.44492
[1mStep[0m  [40/42], [94mLoss[0m : 2.22333

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44428
[1mStep[0m  [4/42], [94mLoss[0m : 2.55580
[1mStep[0m  [8/42], [94mLoss[0m : 2.48988
[1mStep[0m  [12/42], [94mLoss[0m : 2.45545
[1mStep[0m  [16/42], [94mLoss[0m : 2.25496
[1mStep[0m  [20/42], [94mLoss[0m : 2.61082
[1mStep[0m  [24/42], [94mLoss[0m : 2.42964
[1mStep[0m  [28/42], [94mLoss[0m : 2.50739
[1mStep[0m  [32/42], [94mLoss[0m : 2.58148
[1mStep[0m  [36/42], [94mLoss[0m : 2.44179
[1mStep[0m  [40/42], [94mLoss[0m : 2.32303

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30776
[1mStep[0m  [4/42], [94mLoss[0m : 2.41541
[1mStep[0m  [8/42], [94mLoss[0m : 2.49924
[1mStep[0m  [12/42], [94mLoss[0m : 2.29996
[1mStep[0m  [16/42], [94mLoss[0m : 2.56371
[1mStep[0m  [20/42], [94mLoss[0m : 2.51919
[1mStep[0m  [24/42], [94mLoss[0m : 2.33394
[1mStep[0m  [28/42], [94mLoss[0m : 2.54697
[1mStep[0m  [32/42], [94mLoss[0m : 2.57370
[1mStep[0m  [36/42], [94mLoss[0m : 2.33762
[1mStep[0m  [40/42], [94mLoss[0m : 2.63305

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35302
[1mStep[0m  [4/42], [94mLoss[0m : 2.37989
[1mStep[0m  [8/42], [94mLoss[0m : 2.37979
[1mStep[0m  [12/42], [94mLoss[0m : 2.14206
[1mStep[0m  [16/42], [94mLoss[0m : 2.50168
[1mStep[0m  [20/42], [94mLoss[0m : 2.18692
[1mStep[0m  [24/42], [94mLoss[0m : 2.32205
[1mStep[0m  [28/42], [94mLoss[0m : 2.42131
[1mStep[0m  [32/42], [94mLoss[0m : 2.20248
[1mStep[0m  [36/42], [94mLoss[0m : 2.52442
[1mStep[0m  [40/42], [94mLoss[0m : 2.62880

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49899
[1mStep[0m  [4/42], [94mLoss[0m : 2.50773
[1mStep[0m  [8/42], [94mLoss[0m : 2.61334
[1mStep[0m  [12/42], [94mLoss[0m : 2.24103
[1mStep[0m  [16/42], [94mLoss[0m : 2.15473
[1mStep[0m  [20/42], [94mLoss[0m : 2.34391
[1mStep[0m  [24/42], [94mLoss[0m : 2.43563
[1mStep[0m  [28/42], [94mLoss[0m : 2.39818
[1mStep[0m  [32/42], [94mLoss[0m : 2.53974
[1mStep[0m  [36/42], [94mLoss[0m : 2.59840
[1mStep[0m  [40/42], [94mLoss[0m : 2.57626

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21662
[1mStep[0m  [4/42], [94mLoss[0m : 2.22042
[1mStep[0m  [8/42], [94mLoss[0m : 2.37690
[1mStep[0m  [12/42], [94mLoss[0m : 2.32581
[1mStep[0m  [16/42], [94mLoss[0m : 2.53236
[1mStep[0m  [20/42], [94mLoss[0m : 2.45052
[1mStep[0m  [24/42], [94mLoss[0m : 2.35064
[1mStep[0m  [28/42], [94mLoss[0m : 2.22741
[1mStep[0m  [32/42], [94mLoss[0m : 2.49080
[1mStep[0m  [36/42], [94mLoss[0m : 2.39591
[1mStep[0m  [40/42], [94mLoss[0m : 2.44551

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42711
[1mStep[0m  [4/42], [94mLoss[0m : 2.31279
[1mStep[0m  [8/42], [94mLoss[0m : 2.38809
[1mStep[0m  [12/42], [94mLoss[0m : 2.52161
[1mStep[0m  [16/42], [94mLoss[0m : 2.32797
[1mStep[0m  [20/42], [94mLoss[0m : 2.60784
[1mStep[0m  [24/42], [94mLoss[0m : 2.47215
[1mStep[0m  [28/42], [94mLoss[0m : 2.29634
[1mStep[0m  [32/42], [94mLoss[0m : 2.45035
[1mStep[0m  [36/42], [94mLoss[0m : 2.43362
[1mStep[0m  [40/42], [94mLoss[0m : 2.53865

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43591
[1mStep[0m  [4/42], [94mLoss[0m : 2.47440
[1mStep[0m  [8/42], [94mLoss[0m : 2.47921
[1mStep[0m  [12/42], [94mLoss[0m : 2.35920
[1mStep[0m  [16/42], [94mLoss[0m : 2.43644
[1mStep[0m  [20/42], [94mLoss[0m : 2.42342
[1mStep[0m  [24/42], [94mLoss[0m : 2.26162
[1mStep[0m  [28/42], [94mLoss[0m : 2.45166
[1mStep[0m  [32/42], [94mLoss[0m : 2.38839
[1mStep[0m  [36/42], [94mLoss[0m : 2.44695
[1mStep[0m  [40/42], [94mLoss[0m : 2.40256

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43560
[1mStep[0m  [4/42], [94mLoss[0m : 2.41217
[1mStep[0m  [8/42], [94mLoss[0m : 2.18091
[1mStep[0m  [12/42], [94mLoss[0m : 2.56303
[1mStep[0m  [16/42], [94mLoss[0m : 2.34414
[1mStep[0m  [20/42], [94mLoss[0m : 2.36334
[1mStep[0m  [24/42], [94mLoss[0m : 2.41671
[1mStep[0m  [28/42], [94mLoss[0m : 2.59759
[1mStep[0m  [32/42], [94mLoss[0m : 2.60310
[1mStep[0m  [36/42], [94mLoss[0m : 2.32354
[1mStep[0m  [40/42], [94mLoss[0m : 2.56788

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36787
[1mStep[0m  [4/42], [94mLoss[0m : 2.32918
[1mStep[0m  [8/42], [94mLoss[0m : 2.28574
[1mStep[0m  [12/42], [94mLoss[0m : 2.62204
[1mStep[0m  [16/42], [94mLoss[0m : 2.25684
[1mStep[0m  [20/42], [94mLoss[0m : 2.34502
[1mStep[0m  [24/42], [94mLoss[0m : 2.46894
[1mStep[0m  [28/42], [94mLoss[0m : 2.37287
[1mStep[0m  [32/42], [94mLoss[0m : 2.32118
[1mStep[0m  [36/42], [94mLoss[0m : 2.17935
[1mStep[0m  [40/42], [94mLoss[0m : 2.24322

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39601
[1mStep[0m  [4/42], [94mLoss[0m : 2.37352
[1mStep[0m  [8/42], [94mLoss[0m : 2.29636
[1mStep[0m  [12/42], [94mLoss[0m : 2.33952
[1mStep[0m  [16/42], [94mLoss[0m : 2.38294
[1mStep[0m  [20/42], [94mLoss[0m : 2.40708
[1mStep[0m  [24/42], [94mLoss[0m : 2.58602
[1mStep[0m  [28/42], [94mLoss[0m : 2.33171
[1mStep[0m  [32/42], [94mLoss[0m : 2.46140
[1mStep[0m  [36/42], [94mLoss[0m : 2.35094
[1mStep[0m  [40/42], [94mLoss[0m : 2.36244

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.332067149026053
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.38029
[1mStep[0m  [4/42], [94mLoss[0m : 2.49095
[1mStep[0m  [8/42], [94mLoss[0m : 2.48765
[1mStep[0m  [12/42], [94mLoss[0m : 2.71480
[1mStep[0m  [16/42], [94mLoss[0m : 2.57312
[1mStep[0m  [20/42], [94mLoss[0m : 2.31034
[1mStep[0m  [24/42], [94mLoss[0m : 2.50092
[1mStep[0m  [28/42], [94mLoss[0m : 2.39165
[1mStep[0m  [32/42], [94mLoss[0m : 2.35167
[1mStep[0m  [36/42], [94mLoss[0m : 2.42444
[1mStep[0m  [40/42], [94mLoss[0m : 2.36343

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17727
[1mStep[0m  [4/42], [94mLoss[0m : 2.22204
[1mStep[0m  [8/42], [94mLoss[0m : 2.35728
[1mStep[0m  [12/42], [94mLoss[0m : 2.53502
[1mStep[0m  [16/42], [94mLoss[0m : 2.42836
[1mStep[0m  [20/42], [94mLoss[0m : 2.22367
[1mStep[0m  [24/42], [94mLoss[0m : 2.33511
[1mStep[0m  [28/42], [94mLoss[0m : 2.36825
[1mStep[0m  [32/42], [94mLoss[0m : 2.29894
[1mStep[0m  [36/42], [94mLoss[0m : 2.33609
[1mStep[0m  [40/42], [94mLoss[0m : 2.43102

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23999
[1mStep[0m  [4/42], [94mLoss[0m : 2.18420
[1mStep[0m  [8/42], [94mLoss[0m : 2.22258
[1mStep[0m  [12/42], [94mLoss[0m : 2.28610
[1mStep[0m  [16/42], [94mLoss[0m : 2.24945
[1mStep[0m  [20/42], [94mLoss[0m : 2.32196
[1mStep[0m  [24/42], [94mLoss[0m : 2.15861
[1mStep[0m  [28/42], [94mLoss[0m : 2.32553
[1mStep[0m  [32/42], [94mLoss[0m : 2.17856
[1mStep[0m  [36/42], [94mLoss[0m : 2.24605
[1mStep[0m  [40/42], [94mLoss[0m : 2.21003

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.221, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06703
[1mStep[0m  [4/42], [94mLoss[0m : 2.20817
[1mStep[0m  [8/42], [94mLoss[0m : 1.79673
[1mStep[0m  [12/42], [94mLoss[0m : 1.98693
[1mStep[0m  [16/42], [94mLoss[0m : 2.00892
[1mStep[0m  [20/42], [94mLoss[0m : 2.08500
[1mStep[0m  [24/42], [94mLoss[0m : 2.31342
[1mStep[0m  [28/42], [94mLoss[0m : 2.05892
[1mStep[0m  [32/42], [94mLoss[0m : 2.27116
[1mStep[0m  [36/42], [94mLoss[0m : 1.94715
[1mStep[0m  [40/42], [94mLoss[0m : 2.20090

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08807
[1mStep[0m  [4/42], [94mLoss[0m : 1.90807
[1mStep[0m  [8/42], [94mLoss[0m : 1.87374
[1mStep[0m  [12/42], [94mLoss[0m : 1.97939
[1mStep[0m  [16/42], [94mLoss[0m : 2.05083
[1mStep[0m  [20/42], [94mLoss[0m : 1.97362
[1mStep[0m  [24/42], [94mLoss[0m : 2.12161
[1mStep[0m  [28/42], [94mLoss[0m : 2.14075
[1mStep[0m  [32/42], [94mLoss[0m : 1.99415
[1mStep[0m  [36/42], [94mLoss[0m : 1.96101
[1mStep[0m  [40/42], [94mLoss[0m : 1.93289

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06939
[1mStep[0m  [4/42], [94mLoss[0m : 1.98646
[1mStep[0m  [8/42], [94mLoss[0m : 1.88362
[1mStep[0m  [12/42], [94mLoss[0m : 1.98594
[1mStep[0m  [16/42], [94mLoss[0m : 1.84054
[1mStep[0m  [20/42], [94mLoss[0m : 1.97603
[1mStep[0m  [24/42], [94mLoss[0m : 1.92411
[1mStep[0m  [28/42], [94mLoss[0m : 2.16652
[1mStep[0m  [32/42], [94mLoss[0m : 1.90568
[1mStep[0m  [36/42], [94mLoss[0m : 2.03803
[1mStep[0m  [40/42], [94mLoss[0m : 1.90475

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11947
[1mStep[0m  [4/42], [94mLoss[0m : 1.79618
[1mStep[0m  [8/42], [94mLoss[0m : 1.87255
[1mStep[0m  [12/42], [94mLoss[0m : 1.85069
[1mStep[0m  [16/42], [94mLoss[0m : 1.91131
[1mStep[0m  [20/42], [94mLoss[0m : 2.02101
[1mStep[0m  [24/42], [94mLoss[0m : 1.80657
[1mStep[0m  [28/42], [94mLoss[0m : 2.06450
[1mStep[0m  [32/42], [94mLoss[0m : 1.80903
[1mStep[0m  [36/42], [94mLoss[0m : 2.07795
[1mStep[0m  [40/42], [94mLoss[0m : 1.81359

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84060
[1mStep[0m  [4/42], [94mLoss[0m : 1.89729
[1mStep[0m  [8/42], [94mLoss[0m : 1.70545
[1mStep[0m  [12/42], [94mLoss[0m : 1.87169
[1mStep[0m  [16/42], [94mLoss[0m : 1.91973
[1mStep[0m  [20/42], [94mLoss[0m : 1.91827
[1mStep[0m  [24/42], [94mLoss[0m : 1.82374
[1mStep[0m  [28/42], [94mLoss[0m : 2.03981
[1mStep[0m  [32/42], [94mLoss[0m : 1.96271
[1mStep[0m  [36/42], [94mLoss[0m : 1.98041
[1mStep[0m  [40/42], [94mLoss[0m : 1.94801

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86383
[1mStep[0m  [4/42], [94mLoss[0m : 1.84793
[1mStep[0m  [8/42], [94mLoss[0m : 1.79578
[1mStep[0m  [12/42], [94mLoss[0m : 1.81805
[1mStep[0m  [16/42], [94mLoss[0m : 1.77602
[1mStep[0m  [20/42], [94mLoss[0m : 1.79681
[1mStep[0m  [24/42], [94mLoss[0m : 2.05311
[1mStep[0m  [28/42], [94mLoss[0m : 1.86232
[1mStep[0m  [32/42], [94mLoss[0m : 2.09165
[1mStep[0m  [36/42], [94mLoss[0m : 1.78632
[1mStep[0m  [40/42], [94mLoss[0m : 1.97907

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83715
[1mStep[0m  [4/42], [94mLoss[0m : 1.59025
[1mStep[0m  [8/42], [94mLoss[0m : 1.66473
[1mStep[0m  [12/42], [94mLoss[0m : 1.77270
[1mStep[0m  [16/42], [94mLoss[0m : 1.79894
[1mStep[0m  [20/42], [94mLoss[0m : 1.83701
[1mStep[0m  [24/42], [94mLoss[0m : 1.66088
[1mStep[0m  [28/42], [94mLoss[0m : 1.90044
[1mStep[0m  [32/42], [94mLoss[0m : 1.81626
[1mStep[0m  [36/42], [94mLoss[0m : 2.06589
[1mStep[0m  [40/42], [94mLoss[0m : 1.87698

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51589
[1mStep[0m  [4/42], [94mLoss[0m : 1.76634
[1mStep[0m  [8/42], [94mLoss[0m : 1.74842
[1mStep[0m  [12/42], [94mLoss[0m : 1.55414
[1mStep[0m  [16/42], [94mLoss[0m : 1.62090
[1mStep[0m  [20/42], [94mLoss[0m : 2.08041
[1mStep[0m  [24/42], [94mLoss[0m : 1.77909
[1mStep[0m  [28/42], [94mLoss[0m : 1.86001
[1mStep[0m  [32/42], [94mLoss[0m : 1.96556
[1mStep[0m  [36/42], [94mLoss[0m : 1.99778
[1mStep[0m  [40/42], [94mLoss[0m : 2.11076

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82001
[1mStep[0m  [4/42], [94mLoss[0m : 1.71548
[1mStep[0m  [8/42], [94mLoss[0m : 1.74269
[1mStep[0m  [12/42], [94mLoss[0m : 1.73500
[1mStep[0m  [16/42], [94mLoss[0m : 1.89046
[1mStep[0m  [20/42], [94mLoss[0m : 1.76683
[1mStep[0m  [24/42], [94mLoss[0m : 1.67608
[1mStep[0m  [28/42], [94mLoss[0m : 1.85474
[1mStep[0m  [32/42], [94mLoss[0m : 1.85453
[1mStep[0m  [36/42], [94mLoss[0m : 2.00409
[1mStep[0m  [40/42], [94mLoss[0m : 1.73230

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79424
[1mStep[0m  [4/42], [94mLoss[0m : 1.43147
[1mStep[0m  [8/42], [94mLoss[0m : 1.67978
[1mStep[0m  [12/42], [94mLoss[0m : 1.71615
[1mStep[0m  [16/42], [94mLoss[0m : 1.70388
[1mStep[0m  [20/42], [94mLoss[0m : 1.70048
[1mStep[0m  [24/42], [94mLoss[0m : 1.77581
[1mStep[0m  [28/42], [94mLoss[0m : 1.61350
[1mStep[0m  [32/42], [94mLoss[0m : 1.82704
[1mStep[0m  [36/42], [94mLoss[0m : 1.97489
[1mStep[0m  [40/42], [94mLoss[0m : 1.78975

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69207
[1mStep[0m  [4/42], [94mLoss[0m : 1.48381
[1mStep[0m  [8/42], [94mLoss[0m : 1.63677
[1mStep[0m  [12/42], [94mLoss[0m : 1.53546
[1mStep[0m  [16/42], [94mLoss[0m : 1.74037
[1mStep[0m  [20/42], [94mLoss[0m : 1.59762
[1mStep[0m  [24/42], [94mLoss[0m : 1.65346
[1mStep[0m  [28/42], [94mLoss[0m : 1.71980
[1mStep[0m  [32/42], [94mLoss[0m : 1.60940
[1mStep[0m  [36/42], [94mLoss[0m : 1.69054
[1mStep[0m  [40/42], [94mLoss[0m : 1.81549

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63248
[1mStep[0m  [4/42], [94mLoss[0m : 1.62029
[1mStep[0m  [8/42], [94mLoss[0m : 1.63554
[1mStep[0m  [12/42], [94mLoss[0m : 1.70336
[1mStep[0m  [16/42], [94mLoss[0m : 1.65353
[1mStep[0m  [20/42], [94mLoss[0m : 1.75686
[1mStep[0m  [24/42], [94mLoss[0m : 1.70056
[1mStep[0m  [28/42], [94mLoss[0m : 1.65285
[1mStep[0m  [32/42], [94mLoss[0m : 1.76723
[1mStep[0m  [36/42], [94mLoss[0m : 1.81938
[1mStep[0m  [40/42], [94mLoss[0m : 1.73376

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66631
[1mStep[0m  [4/42], [94mLoss[0m : 1.75579
[1mStep[0m  [8/42], [94mLoss[0m : 1.48622
[1mStep[0m  [12/42], [94mLoss[0m : 1.53885
[1mStep[0m  [16/42], [94mLoss[0m : 1.52875
[1mStep[0m  [20/42], [94mLoss[0m : 1.59302
[1mStep[0m  [24/42], [94mLoss[0m : 1.52605
[1mStep[0m  [28/42], [94mLoss[0m : 1.61060
[1mStep[0m  [32/42], [94mLoss[0m : 1.59418
[1mStep[0m  [36/42], [94mLoss[0m : 1.66791
[1mStep[0m  [40/42], [94mLoss[0m : 1.59832

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62739
[1mStep[0m  [4/42], [94mLoss[0m : 1.68756
[1mStep[0m  [8/42], [94mLoss[0m : 1.53002
[1mStep[0m  [12/42], [94mLoss[0m : 1.51705
[1mStep[0m  [16/42], [94mLoss[0m : 1.74561
[1mStep[0m  [20/42], [94mLoss[0m : 1.38427
[1mStep[0m  [24/42], [94mLoss[0m : 1.71184
[1mStep[0m  [28/42], [94mLoss[0m : 1.67084
[1mStep[0m  [32/42], [94mLoss[0m : 1.76984
[1mStep[0m  [36/42], [94mLoss[0m : 1.75785
[1mStep[0m  [40/42], [94mLoss[0m : 1.53242

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48657
[1mStep[0m  [4/42], [94mLoss[0m : 1.61703
[1mStep[0m  [8/42], [94mLoss[0m : 1.44166
[1mStep[0m  [12/42], [94mLoss[0m : 1.38905
[1mStep[0m  [16/42], [94mLoss[0m : 1.54889
[1mStep[0m  [20/42], [94mLoss[0m : 1.62200
[1mStep[0m  [24/42], [94mLoss[0m : 1.64065
[1mStep[0m  [28/42], [94mLoss[0m : 1.53642
[1mStep[0m  [32/42], [94mLoss[0m : 1.58922
[1mStep[0m  [36/42], [94mLoss[0m : 1.88178
[1mStep[0m  [40/42], [94mLoss[0m : 1.71784

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49779
[1mStep[0m  [4/42], [94mLoss[0m : 1.46595
[1mStep[0m  [8/42], [94mLoss[0m : 1.60685
[1mStep[0m  [12/42], [94mLoss[0m : 1.56506
[1mStep[0m  [16/42], [94mLoss[0m : 1.62197
[1mStep[0m  [20/42], [94mLoss[0m : 1.61251
[1mStep[0m  [24/42], [94mLoss[0m : 1.57987
[1mStep[0m  [28/42], [94mLoss[0m : 1.59924
[1mStep[0m  [32/42], [94mLoss[0m : 1.78130
[1mStep[0m  [36/42], [94mLoss[0m : 1.73398
[1mStep[0m  [40/42], [94mLoss[0m : 1.38665

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.530, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61474
[1mStep[0m  [4/42], [94mLoss[0m : 1.31036
[1mStep[0m  [8/42], [94mLoss[0m : 1.53211
[1mStep[0m  [12/42], [94mLoss[0m : 1.47965
[1mStep[0m  [16/42], [94mLoss[0m : 1.45872
[1mStep[0m  [20/42], [94mLoss[0m : 1.49235
[1mStep[0m  [24/42], [94mLoss[0m : 1.41409
[1mStep[0m  [28/42], [94mLoss[0m : 1.52978
[1mStep[0m  [32/42], [94mLoss[0m : 1.67373
[1mStep[0m  [36/42], [94mLoss[0m : 1.56748
[1mStep[0m  [40/42], [94mLoss[0m : 1.74737

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42831
[1mStep[0m  [4/42], [94mLoss[0m : 1.39632
[1mStep[0m  [8/42], [94mLoss[0m : 1.64123
[1mStep[0m  [12/42], [94mLoss[0m : 1.76878
[1mStep[0m  [16/42], [94mLoss[0m : 1.66813
[1mStep[0m  [20/42], [94mLoss[0m : 1.43583
[1mStep[0m  [24/42], [94mLoss[0m : 1.69404
[1mStep[0m  [28/42], [94mLoss[0m : 1.45380
[1mStep[0m  [32/42], [94mLoss[0m : 1.58662
[1mStep[0m  [36/42], [94mLoss[0m : 1.64409
[1mStep[0m  [40/42], [94mLoss[0m : 1.65942

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.450, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.18227
[1mStep[0m  [4/42], [94mLoss[0m : 1.43486
[1mStep[0m  [8/42], [94mLoss[0m : 1.49650
[1mStep[0m  [12/42], [94mLoss[0m : 1.47871
[1mStep[0m  [16/42], [94mLoss[0m : 1.39194
[1mStep[0m  [20/42], [94mLoss[0m : 1.59039
[1mStep[0m  [24/42], [94mLoss[0m : 1.59108
[1mStep[0m  [28/42], [94mLoss[0m : 1.47504
[1mStep[0m  [32/42], [94mLoss[0m : 1.53921
[1mStep[0m  [36/42], [94mLoss[0m : 1.62633
[1mStep[0m  [40/42], [94mLoss[0m : 1.57717

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.493, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41996
[1mStep[0m  [4/42], [94mLoss[0m : 1.41192
[1mStep[0m  [8/42], [94mLoss[0m : 1.37110
[1mStep[0m  [12/42], [94mLoss[0m : 1.45365
[1mStep[0m  [16/42], [94mLoss[0m : 1.47153
[1mStep[0m  [20/42], [94mLoss[0m : 1.44067
[1mStep[0m  [24/42], [94mLoss[0m : 1.50231
[1mStep[0m  [28/42], [94mLoss[0m : 1.33527
[1mStep[0m  [32/42], [94mLoss[0m : 1.61437
[1mStep[0m  [36/42], [94mLoss[0m : 1.58838
[1mStep[0m  [40/42], [94mLoss[0m : 1.55521

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37393
[1mStep[0m  [4/42], [94mLoss[0m : 1.41919
[1mStep[0m  [8/42], [94mLoss[0m : 1.48427
[1mStep[0m  [12/42], [94mLoss[0m : 1.50299
[1mStep[0m  [16/42], [94mLoss[0m : 1.34750
[1mStep[0m  [20/42], [94mLoss[0m : 1.46997
[1mStep[0m  [24/42], [94mLoss[0m : 1.44313
[1mStep[0m  [28/42], [94mLoss[0m : 1.55880
[1mStep[0m  [32/42], [94mLoss[0m : 1.51685
[1mStep[0m  [36/42], [94mLoss[0m : 1.37334
[1mStep[0m  [40/42], [94mLoss[0m : 1.71146

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.474, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.31915
[1mStep[0m  [4/42], [94mLoss[0m : 1.51371
[1mStep[0m  [8/42], [94mLoss[0m : 1.66660
[1mStep[0m  [12/42], [94mLoss[0m : 1.45805
[1mStep[0m  [16/42], [94mLoss[0m : 1.40731
[1mStep[0m  [20/42], [94mLoss[0m : 1.37170
[1mStep[0m  [24/42], [94mLoss[0m : 1.57348
[1mStep[0m  [28/42], [94mLoss[0m : 1.42634
[1mStep[0m  [32/42], [94mLoss[0m : 1.40318
[1mStep[0m  [36/42], [94mLoss[0m : 1.78408
[1mStep[0m  [40/42], [94mLoss[0m : 1.64748

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.34870
[1mStep[0m  [4/42], [94mLoss[0m : 1.51186
[1mStep[0m  [8/42], [94mLoss[0m : 1.55158
[1mStep[0m  [12/42], [94mLoss[0m : 1.38149
[1mStep[0m  [16/42], [94mLoss[0m : 1.38615
[1mStep[0m  [20/42], [94mLoss[0m : 1.33082
[1mStep[0m  [24/42], [94mLoss[0m : 1.37176
[1mStep[0m  [28/42], [94mLoss[0m : 1.50271
[1mStep[0m  [32/42], [94mLoss[0m : 1.54773
[1mStep[0m  [36/42], [94mLoss[0m : 1.44653
[1mStep[0m  [40/42], [94mLoss[0m : 1.52977

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.35465
[1mStep[0m  [4/42], [94mLoss[0m : 1.33544
[1mStep[0m  [8/42], [94mLoss[0m : 1.43091
[1mStep[0m  [12/42], [94mLoss[0m : 1.55522
[1mStep[0m  [16/42], [94mLoss[0m : 1.31743
[1mStep[0m  [20/42], [94mLoss[0m : 1.45730
[1mStep[0m  [24/42], [94mLoss[0m : 1.39057
[1mStep[0m  [28/42], [94mLoss[0m : 1.40263
[1mStep[0m  [32/42], [94mLoss[0m : 1.49351
[1mStep[0m  [36/42], [94mLoss[0m : 1.52055
[1mStep[0m  [40/42], [94mLoss[0m : 1.50763

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.431, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36814
[1mStep[0m  [4/42], [94mLoss[0m : 1.42605
[1mStep[0m  [8/42], [94mLoss[0m : 1.43336
[1mStep[0m  [12/42], [94mLoss[0m : 1.49149
[1mStep[0m  [16/42], [94mLoss[0m : 1.34415
[1mStep[0m  [20/42], [94mLoss[0m : 1.31549
[1mStep[0m  [24/42], [94mLoss[0m : 1.33449
[1mStep[0m  [28/42], [94mLoss[0m : 1.45192
[1mStep[0m  [32/42], [94mLoss[0m : 1.54439
[1mStep[0m  [36/42], [94mLoss[0m : 1.51868
[1mStep[0m  [40/42], [94mLoss[0m : 1.64598

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.420, [92mTest[0m: 2.524, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.502
====================================

Phase 2 - Evaluation MAE:  2.501523051943098
MAE score P1      2.332067
MAE score P2      2.501523
loss              1.419657
learning_rate         0.01
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.95893
[1mStep[0m  [8/84], [94mLoss[0m : 7.71792
[1mStep[0m  [16/84], [94mLoss[0m : 3.20827
[1mStep[0m  [24/84], [94mLoss[0m : 3.53277
[1mStep[0m  [32/84], [94mLoss[0m : 2.77875
[1mStep[0m  [40/84], [94mLoss[0m : 2.56772
[1mStep[0m  [48/84], [94mLoss[0m : 2.39861
[1mStep[0m  [56/84], [94mLoss[0m : 2.31899
[1mStep[0m  [64/84], [94mLoss[0m : 3.01403
[1mStep[0m  [72/84], [94mLoss[0m : 2.44651
[1mStep[0m  [80/84], [94mLoss[0m : 2.35316

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.662, [92mTest[0m: 11.122, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43960
[1mStep[0m  [8/84], [94mLoss[0m : 2.73764
[1mStep[0m  [16/84], [94mLoss[0m : 2.74428
[1mStep[0m  [24/84], [94mLoss[0m : 2.50812
[1mStep[0m  [32/84], [94mLoss[0m : 2.53834
[1mStep[0m  [40/84], [94mLoss[0m : 2.81173
[1mStep[0m  [48/84], [94mLoss[0m : 2.28078
[1mStep[0m  [56/84], [94mLoss[0m : 2.43101
[1mStep[0m  [64/84], [94mLoss[0m : 2.81298
[1mStep[0m  [72/84], [94mLoss[0m : 2.59540
[1mStep[0m  [80/84], [94mLoss[0m : 2.64844

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80991
[1mStep[0m  [8/84], [94mLoss[0m : 2.56812
[1mStep[0m  [16/84], [94mLoss[0m : 2.56216
[1mStep[0m  [24/84], [94mLoss[0m : 2.34044
[1mStep[0m  [32/84], [94mLoss[0m : 2.79801
[1mStep[0m  [40/84], [94mLoss[0m : 2.63340
[1mStep[0m  [48/84], [94mLoss[0m : 2.49182
[1mStep[0m  [56/84], [94mLoss[0m : 2.35100
[1mStep[0m  [64/84], [94mLoss[0m : 2.14490
[1mStep[0m  [72/84], [94mLoss[0m : 2.54826
[1mStep[0m  [80/84], [94mLoss[0m : 2.32392

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63727
[1mStep[0m  [8/84], [94mLoss[0m : 2.60024
[1mStep[0m  [16/84], [94mLoss[0m : 2.42120
[1mStep[0m  [24/84], [94mLoss[0m : 2.74742
[1mStep[0m  [32/84], [94mLoss[0m : 2.61771
[1mStep[0m  [40/84], [94mLoss[0m : 2.90419
[1mStep[0m  [48/84], [94mLoss[0m : 2.57169
[1mStep[0m  [56/84], [94mLoss[0m : 2.40716
[1mStep[0m  [64/84], [94mLoss[0m : 2.51160
[1mStep[0m  [72/84], [94mLoss[0m : 2.39527
[1mStep[0m  [80/84], [94mLoss[0m : 2.58756

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77868
[1mStep[0m  [8/84], [94mLoss[0m : 2.28830
[1mStep[0m  [16/84], [94mLoss[0m : 2.15026
[1mStep[0m  [24/84], [94mLoss[0m : 2.61897
[1mStep[0m  [32/84], [94mLoss[0m : 2.32319
[1mStep[0m  [40/84], [94mLoss[0m : 2.67474
[1mStep[0m  [48/84], [94mLoss[0m : 2.30436
[1mStep[0m  [56/84], [94mLoss[0m : 2.39860
[1mStep[0m  [64/84], [94mLoss[0m : 2.57698
[1mStep[0m  [72/84], [94mLoss[0m : 2.46809
[1mStep[0m  [80/84], [94mLoss[0m : 2.38722

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20648
[1mStep[0m  [8/84], [94mLoss[0m : 2.25061
[1mStep[0m  [16/84], [94mLoss[0m : 2.53238
[1mStep[0m  [24/84], [94mLoss[0m : 2.60275
[1mStep[0m  [32/84], [94mLoss[0m : 2.64896
[1mStep[0m  [40/84], [94mLoss[0m : 2.62764
[1mStep[0m  [48/84], [94mLoss[0m : 2.53685
[1mStep[0m  [56/84], [94mLoss[0m : 2.38936
[1mStep[0m  [64/84], [94mLoss[0m : 2.16275
[1mStep[0m  [72/84], [94mLoss[0m : 2.47582
[1mStep[0m  [80/84], [94mLoss[0m : 2.63656

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57274
[1mStep[0m  [8/84], [94mLoss[0m : 2.83402
[1mStep[0m  [16/84], [94mLoss[0m : 2.40959
[1mStep[0m  [24/84], [94mLoss[0m : 2.50910
[1mStep[0m  [32/84], [94mLoss[0m : 2.68770
[1mStep[0m  [40/84], [94mLoss[0m : 2.43382
[1mStep[0m  [48/84], [94mLoss[0m : 2.28144
[1mStep[0m  [56/84], [94mLoss[0m : 2.54390
[1mStep[0m  [64/84], [94mLoss[0m : 2.53022
[1mStep[0m  [72/84], [94mLoss[0m : 2.40945
[1mStep[0m  [80/84], [94mLoss[0m : 2.45559

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77578
[1mStep[0m  [8/84], [94mLoss[0m : 2.82829
[1mStep[0m  [16/84], [94mLoss[0m : 2.61318
[1mStep[0m  [24/84], [94mLoss[0m : 2.48037
[1mStep[0m  [32/84], [94mLoss[0m : 2.27385
[1mStep[0m  [40/84], [94mLoss[0m : 2.32223
[1mStep[0m  [48/84], [94mLoss[0m : 2.29398
[1mStep[0m  [56/84], [94mLoss[0m : 2.54606
[1mStep[0m  [64/84], [94mLoss[0m : 2.10428
[1mStep[0m  [72/84], [94mLoss[0m : 2.74845
[1mStep[0m  [80/84], [94mLoss[0m : 2.34655

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40881
[1mStep[0m  [8/84], [94mLoss[0m : 2.49067
[1mStep[0m  [16/84], [94mLoss[0m : 2.50493
[1mStep[0m  [24/84], [94mLoss[0m : 2.58164
[1mStep[0m  [32/84], [94mLoss[0m : 2.41481
[1mStep[0m  [40/84], [94mLoss[0m : 2.68231
[1mStep[0m  [48/84], [94mLoss[0m : 2.38918
[1mStep[0m  [56/84], [94mLoss[0m : 2.62969
[1mStep[0m  [64/84], [94mLoss[0m : 2.67629
[1mStep[0m  [72/84], [94mLoss[0m : 2.34631
[1mStep[0m  [80/84], [94mLoss[0m : 2.54873

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27919
[1mStep[0m  [8/84], [94mLoss[0m : 2.86466
[1mStep[0m  [16/84], [94mLoss[0m : 2.31793
[1mStep[0m  [24/84], [94mLoss[0m : 2.08308
[1mStep[0m  [32/84], [94mLoss[0m : 2.24062
[1mStep[0m  [40/84], [94mLoss[0m : 2.64881
[1mStep[0m  [48/84], [94mLoss[0m : 2.26022
[1mStep[0m  [56/84], [94mLoss[0m : 3.07294
[1mStep[0m  [64/84], [94mLoss[0m : 2.49665
[1mStep[0m  [72/84], [94mLoss[0m : 2.86861
[1mStep[0m  [80/84], [94mLoss[0m : 2.27293

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62111
[1mStep[0m  [8/84], [94mLoss[0m : 2.58744
[1mStep[0m  [16/84], [94mLoss[0m : 2.16281
[1mStep[0m  [24/84], [94mLoss[0m : 3.13822
[1mStep[0m  [32/84], [94mLoss[0m : 2.43220
[1mStep[0m  [40/84], [94mLoss[0m : 2.56129
[1mStep[0m  [48/84], [94mLoss[0m : 2.85810
[1mStep[0m  [56/84], [94mLoss[0m : 2.51220
[1mStep[0m  [64/84], [94mLoss[0m : 2.55958
[1mStep[0m  [72/84], [94mLoss[0m : 2.46972
[1mStep[0m  [80/84], [94mLoss[0m : 2.27040

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31445
[1mStep[0m  [8/84], [94mLoss[0m : 2.54654
[1mStep[0m  [16/84], [94mLoss[0m : 2.47033
[1mStep[0m  [24/84], [94mLoss[0m : 2.60901
[1mStep[0m  [32/84], [94mLoss[0m : 2.72743
[1mStep[0m  [40/84], [94mLoss[0m : 2.19997
[1mStep[0m  [48/84], [94mLoss[0m : 2.38512
[1mStep[0m  [56/84], [94mLoss[0m : 2.42093
[1mStep[0m  [64/84], [94mLoss[0m : 2.22942
[1mStep[0m  [72/84], [94mLoss[0m : 2.40372
[1mStep[0m  [80/84], [94mLoss[0m : 2.43030

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24345
[1mStep[0m  [8/84], [94mLoss[0m : 2.59678
[1mStep[0m  [16/84], [94mLoss[0m : 2.51216
[1mStep[0m  [24/84], [94mLoss[0m : 2.58397
[1mStep[0m  [32/84], [94mLoss[0m : 2.73787
[1mStep[0m  [40/84], [94mLoss[0m : 2.43717
[1mStep[0m  [48/84], [94mLoss[0m : 2.21920
[1mStep[0m  [56/84], [94mLoss[0m : 2.49637
[1mStep[0m  [64/84], [94mLoss[0m : 2.35190
[1mStep[0m  [72/84], [94mLoss[0m : 2.40267
[1mStep[0m  [80/84], [94mLoss[0m : 2.56395

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54965
[1mStep[0m  [8/84], [94mLoss[0m : 2.61229
[1mStep[0m  [16/84], [94mLoss[0m : 2.30699
[1mStep[0m  [24/84], [94mLoss[0m : 2.20462
[1mStep[0m  [32/84], [94mLoss[0m : 2.22580
[1mStep[0m  [40/84], [94mLoss[0m : 2.46502
[1mStep[0m  [48/84], [94mLoss[0m : 2.50208
[1mStep[0m  [56/84], [94mLoss[0m : 2.26050
[1mStep[0m  [64/84], [94mLoss[0m : 2.40830
[1mStep[0m  [72/84], [94mLoss[0m : 2.37202
[1mStep[0m  [80/84], [94mLoss[0m : 2.60585

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30116
[1mStep[0m  [8/84], [94mLoss[0m : 2.64508
[1mStep[0m  [16/84], [94mLoss[0m : 2.16287
[1mStep[0m  [24/84], [94mLoss[0m : 2.52580
[1mStep[0m  [32/84], [94mLoss[0m : 2.44695
[1mStep[0m  [40/84], [94mLoss[0m : 2.31099
[1mStep[0m  [48/84], [94mLoss[0m : 2.40831
[1mStep[0m  [56/84], [94mLoss[0m : 2.51792
[1mStep[0m  [64/84], [94mLoss[0m : 2.38696
[1mStep[0m  [72/84], [94mLoss[0m : 2.31768
[1mStep[0m  [80/84], [94mLoss[0m : 2.49870

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46155
[1mStep[0m  [8/84], [94mLoss[0m : 2.66161
[1mStep[0m  [16/84], [94mLoss[0m : 2.55258
[1mStep[0m  [24/84], [94mLoss[0m : 2.41324
[1mStep[0m  [32/84], [94mLoss[0m : 2.33256
[1mStep[0m  [40/84], [94mLoss[0m : 2.21716
[1mStep[0m  [48/84], [94mLoss[0m : 2.40511
[1mStep[0m  [56/84], [94mLoss[0m : 2.41183
[1mStep[0m  [64/84], [94mLoss[0m : 2.51332
[1mStep[0m  [72/84], [94mLoss[0m : 2.42100
[1mStep[0m  [80/84], [94mLoss[0m : 2.41737

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38029
[1mStep[0m  [8/84], [94mLoss[0m : 2.44250
[1mStep[0m  [16/84], [94mLoss[0m : 2.08356
[1mStep[0m  [24/84], [94mLoss[0m : 2.52232
[1mStep[0m  [32/84], [94mLoss[0m : 2.17284
[1mStep[0m  [40/84], [94mLoss[0m : 2.59479
[1mStep[0m  [48/84], [94mLoss[0m : 2.44508
[1mStep[0m  [56/84], [94mLoss[0m : 2.49263
[1mStep[0m  [64/84], [94mLoss[0m : 2.35306
[1mStep[0m  [72/84], [94mLoss[0m : 2.20988
[1mStep[0m  [80/84], [94mLoss[0m : 2.50451

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42243
[1mStep[0m  [8/84], [94mLoss[0m : 2.35899
[1mStep[0m  [16/84], [94mLoss[0m : 2.26265
[1mStep[0m  [24/84], [94mLoss[0m : 2.35919
[1mStep[0m  [32/84], [94mLoss[0m : 2.40915
[1mStep[0m  [40/84], [94mLoss[0m : 2.36585
[1mStep[0m  [48/84], [94mLoss[0m : 2.58769
[1mStep[0m  [56/84], [94mLoss[0m : 2.27315
[1mStep[0m  [64/84], [94mLoss[0m : 2.43641
[1mStep[0m  [72/84], [94mLoss[0m : 2.62313
[1mStep[0m  [80/84], [94mLoss[0m : 2.47096

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47010
[1mStep[0m  [8/84], [94mLoss[0m : 2.27723
[1mStep[0m  [16/84], [94mLoss[0m : 2.28143
[1mStep[0m  [24/84], [94mLoss[0m : 2.41466
[1mStep[0m  [32/84], [94mLoss[0m : 2.43101
[1mStep[0m  [40/84], [94mLoss[0m : 2.56263
[1mStep[0m  [48/84], [94mLoss[0m : 2.69047
[1mStep[0m  [56/84], [94mLoss[0m : 2.30389
[1mStep[0m  [64/84], [94mLoss[0m : 2.33327
[1mStep[0m  [72/84], [94mLoss[0m : 2.60993
[1mStep[0m  [80/84], [94mLoss[0m : 2.41018

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52289
[1mStep[0m  [8/84], [94mLoss[0m : 2.60049
[1mStep[0m  [16/84], [94mLoss[0m : 3.04098
[1mStep[0m  [24/84], [94mLoss[0m : 2.09603
[1mStep[0m  [32/84], [94mLoss[0m : 2.27520
[1mStep[0m  [40/84], [94mLoss[0m : 2.58252
[1mStep[0m  [48/84], [94mLoss[0m : 2.38762
[1mStep[0m  [56/84], [94mLoss[0m : 2.49150
[1mStep[0m  [64/84], [94mLoss[0m : 2.50355
[1mStep[0m  [72/84], [94mLoss[0m : 2.34234
[1mStep[0m  [80/84], [94mLoss[0m : 2.43183

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69681
[1mStep[0m  [8/84], [94mLoss[0m : 2.58318
[1mStep[0m  [16/84], [94mLoss[0m : 2.55452
[1mStep[0m  [24/84], [94mLoss[0m : 2.45325
[1mStep[0m  [32/84], [94mLoss[0m : 2.10933
[1mStep[0m  [40/84], [94mLoss[0m : 2.49121
[1mStep[0m  [48/84], [94mLoss[0m : 2.47789
[1mStep[0m  [56/84], [94mLoss[0m : 2.27055
[1mStep[0m  [64/84], [94mLoss[0m : 2.51868
[1mStep[0m  [72/84], [94mLoss[0m : 2.49218
[1mStep[0m  [80/84], [94mLoss[0m : 2.32388

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38554
[1mStep[0m  [8/84], [94mLoss[0m : 2.38698
[1mStep[0m  [16/84], [94mLoss[0m : 2.50891
[1mStep[0m  [24/84], [94mLoss[0m : 2.36785
[1mStep[0m  [32/84], [94mLoss[0m : 2.49206
[1mStep[0m  [40/84], [94mLoss[0m : 2.57992
[1mStep[0m  [48/84], [94mLoss[0m : 2.52959
[1mStep[0m  [56/84], [94mLoss[0m : 2.48394
[1mStep[0m  [64/84], [94mLoss[0m : 2.60071
[1mStep[0m  [72/84], [94mLoss[0m : 2.63345
[1mStep[0m  [80/84], [94mLoss[0m : 2.22370

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39139
[1mStep[0m  [8/84], [94mLoss[0m : 2.61846
[1mStep[0m  [16/84], [94mLoss[0m : 2.26463
[1mStep[0m  [24/84], [94mLoss[0m : 2.37597
[1mStep[0m  [32/84], [94mLoss[0m : 2.59100
[1mStep[0m  [40/84], [94mLoss[0m : 2.44044
[1mStep[0m  [48/84], [94mLoss[0m : 2.58837
[1mStep[0m  [56/84], [94mLoss[0m : 2.26056
[1mStep[0m  [64/84], [94mLoss[0m : 2.44024
[1mStep[0m  [72/84], [94mLoss[0m : 2.39263
[1mStep[0m  [80/84], [94mLoss[0m : 2.42757

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40036
[1mStep[0m  [8/84], [94mLoss[0m : 2.37953
[1mStep[0m  [16/84], [94mLoss[0m : 2.33738
[1mStep[0m  [24/84], [94mLoss[0m : 2.46404
[1mStep[0m  [32/84], [94mLoss[0m : 2.29005
[1mStep[0m  [40/84], [94mLoss[0m : 2.52352
[1mStep[0m  [48/84], [94mLoss[0m : 2.55267
[1mStep[0m  [56/84], [94mLoss[0m : 2.51591
[1mStep[0m  [64/84], [94mLoss[0m : 2.40373
[1mStep[0m  [72/84], [94mLoss[0m : 2.44911
[1mStep[0m  [80/84], [94mLoss[0m : 2.42326

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49826
[1mStep[0m  [8/84], [94mLoss[0m : 2.58801
[1mStep[0m  [16/84], [94mLoss[0m : 2.49173
[1mStep[0m  [24/84], [94mLoss[0m : 2.32706
[1mStep[0m  [32/84], [94mLoss[0m : 2.36503
[1mStep[0m  [40/84], [94mLoss[0m : 2.58512
[1mStep[0m  [48/84], [94mLoss[0m : 2.45268
[1mStep[0m  [56/84], [94mLoss[0m : 2.48642
[1mStep[0m  [64/84], [94mLoss[0m : 2.46571
[1mStep[0m  [72/84], [94mLoss[0m : 2.73145
[1mStep[0m  [80/84], [94mLoss[0m : 2.32039

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75547
[1mStep[0m  [8/84], [94mLoss[0m : 2.20941
[1mStep[0m  [16/84], [94mLoss[0m : 2.32256
[1mStep[0m  [24/84], [94mLoss[0m : 2.63783
[1mStep[0m  [32/84], [94mLoss[0m : 2.62401
[1mStep[0m  [40/84], [94mLoss[0m : 2.29827
[1mStep[0m  [48/84], [94mLoss[0m : 2.46232
[1mStep[0m  [56/84], [94mLoss[0m : 2.30750
[1mStep[0m  [64/84], [94mLoss[0m : 2.30081
[1mStep[0m  [72/84], [94mLoss[0m : 2.23709
[1mStep[0m  [80/84], [94mLoss[0m : 2.69623

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55665
[1mStep[0m  [8/84], [94mLoss[0m : 2.29465
[1mStep[0m  [16/84], [94mLoss[0m : 2.27614
[1mStep[0m  [24/84], [94mLoss[0m : 2.54982
[1mStep[0m  [32/84], [94mLoss[0m : 2.61947
[1mStep[0m  [40/84], [94mLoss[0m : 2.45196
[1mStep[0m  [48/84], [94mLoss[0m : 2.53743
[1mStep[0m  [56/84], [94mLoss[0m : 2.18700
[1mStep[0m  [64/84], [94mLoss[0m : 2.27936
[1mStep[0m  [72/84], [94mLoss[0m : 2.48040
[1mStep[0m  [80/84], [94mLoss[0m : 2.55792

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40219
[1mStep[0m  [8/84], [94mLoss[0m : 2.75780
[1mStep[0m  [16/84], [94mLoss[0m : 2.15625
[1mStep[0m  [24/84], [94mLoss[0m : 2.53014
[1mStep[0m  [32/84], [94mLoss[0m : 2.62567
[1mStep[0m  [40/84], [94mLoss[0m : 2.45665
[1mStep[0m  [48/84], [94mLoss[0m : 2.28007
[1mStep[0m  [56/84], [94mLoss[0m : 2.75968
[1mStep[0m  [64/84], [94mLoss[0m : 2.55958
[1mStep[0m  [72/84], [94mLoss[0m : 2.76556
[1mStep[0m  [80/84], [94mLoss[0m : 2.19024

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22054
[1mStep[0m  [8/84], [94mLoss[0m : 2.25317
[1mStep[0m  [16/84], [94mLoss[0m : 2.48500
[1mStep[0m  [24/84], [94mLoss[0m : 2.28606
[1mStep[0m  [32/84], [94mLoss[0m : 2.73134
[1mStep[0m  [40/84], [94mLoss[0m : 2.47587
[1mStep[0m  [48/84], [94mLoss[0m : 2.60953
[1mStep[0m  [56/84], [94mLoss[0m : 1.96065
[1mStep[0m  [64/84], [94mLoss[0m : 2.36527
[1mStep[0m  [72/84], [94mLoss[0m : 2.43079
[1mStep[0m  [80/84], [94mLoss[0m : 2.31033

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.312, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60511
[1mStep[0m  [8/84], [94mLoss[0m : 2.27654
[1mStep[0m  [16/84], [94mLoss[0m : 2.37764
[1mStep[0m  [24/84], [94mLoss[0m : 2.15920
[1mStep[0m  [32/84], [94mLoss[0m : 2.29198
[1mStep[0m  [40/84], [94mLoss[0m : 2.50098
[1mStep[0m  [48/84], [94mLoss[0m : 2.47469
[1mStep[0m  [56/84], [94mLoss[0m : 2.52707
[1mStep[0m  [64/84], [94mLoss[0m : 2.45919
[1mStep[0m  [72/84], [94mLoss[0m : 2.85572
[1mStep[0m  [80/84], [94mLoss[0m : 2.60395

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.323
====================================

Phase 1 - Evaluation MAE:  2.3233880570956638
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.66722
[1mStep[0m  [8/84], [94mLoss[0m : 2.19686
[1mStep[0m  [16/84], [94mLoss[0m : 2.36479
[1mStep[0m  [24/84], [94mLoss[0m : 2.21017
[1mStep[0m  [32/84], [94mLoss[0m : 2.61205
[1mStep[0m  [40/84], [94mLoss[0m : 2.64120
[1mStep[0m  [48/84], [94mLoss[0m : 2.39191
[1mStep[0m  [56/84], [94mLoss[0m : 2.22965
[1mStep[0m  [64/84], [94mLoss[0m : 2.73380
[1mStep[0m  [72/84], [94mLoss[0m : 2.49756
[1mStep[0m  [80/84], [94mLoss[0m : 2.48740

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.317, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75617
[1mStep[0m  [8/84], [94mLoss[0m : 2.56299
[1mStep[0m  [16/84], [94mLoss[0m : 2.36211
[1mStep[0m  [24/84], [94mLoss[0m : 2.17063
[1mStep[0m  [32/84], [94mLoss[0m : 2.42315
[1mStep[0m  [40/84], [94mLoss[0m : 2.47692
[1mStep[0m  [48/84], [94mLoss[0m : 2.36366
[1mStep[0m  [56/84], [94mLoss[0m : 2.39192
[1mStep[0m  [64/84], [94mLoss[0m : 2.20454
[1mStep[0m  [72/84], [94mLoss[0m : 2.50128
[1mStep[0m  [80/84], [94mLoss[0m : 2.57066

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27835
[1mStep[0m  [8/84], [94mLoss[0m : 2.50885
[1mStep[0m  [16/84], [94mLoss[0m : 1.99884
[1mStep[0m  [24/84], [94mLoss[0m : 2.35795
[1mStep[0m  [32/84], [94mLoss[0m : 2.04760
[1mStep[0m  [40/84], [94mLoss[0m : 2.31310
[1mStep[0m  [48/84], [94mLoss[0m : 2.71460
[1mStep[0m  [56/84], [94mLoss[0m : 2.07621
[1mStep[0m  [64/84], [94mLoss[0m : 2.43169
[1mStep[0m  [72/84], [94mLoss[0m : 2.29808
[1mStep[0m  [80/84], [94mLoss[0m : 2.26055

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96344
[1mStep[0m  [8/84], [94mLoss[0m : 2.33913
[1mStep[0m  [16/84], [94mLoss[0m : 1.99878
[1mStep[0m  [24/84], [94mLoss[0m : 2.17015
[1mStep[0m  [32/84], [94mLoss[0m : 2.27615
[1mStep[0m  [40/84], [94mLoss[0m : 2.07439
[1mStep[0m  [48/84], [94mLoss[0m : 1.95500
[1mStep[0m  [56/84], [94mLoss[0m : 1.98572
[1mStep[0m  [64/84], [94mLoss[0m : 2.08192
[1mStep[0m  [72/84], [94mLoss[0m : 2.31531
[1mStep[0m  [80/84], [94mLoss[0m : 2.29824

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06268
[1mStep[0m  [8/84], [94mLoss[0m : 2.09432
[1mStep[0m  [16/84], [94mLoss[0m : 1.99026
[1mStep[0m  [24/84], [94mLoss[0m : 1.93935
[1mStep[0m  [32/84], [94mLoss[0m : 2.07219
[1mStep[0m  [40/84], [94mLoss[0m : 2.34576
[1mStep[0m  [48/84], [94mLoss[0m : 2.26106
[1mStep[0m  [56/84], [94mLoss[0m : 2.13063
[1mStep[0m  [64/84], [94mLoss[0m : 2.08769
[1mStep[0m  [72/84], [94mLoss[0m : 2.05368
[1mStep[0m  [80/84], [94mLoss[0m : 2.30799

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00654
[1mStep[0m  [8/84], [94mLoss[0m : 2.08269
[1mStep[0m  [16/84], [94mLoss[0m : 1.75087
[1mStep[0m  [24/84], [94mLoss[0m : 1.95662
[1mStep[0m  [32/84], [94mLoss[0m : 1.92739
[1mStep[0m  [40/84], [94mLoss[0m : 2.07209
[1mStep[0m  [48/84], [94mLoss[0m : 1.95362
[1mStep[0m  [56/84], [94mLoss[0m : 1.93031
[1mStep[0m  [64/84], [94mLoss[0m : 2.03690
[1mStep[0m  [72/84], [94mLoss[0m : 2.19439
[1mStep[0m  [80/84], [94mLoss[0m : 2.31188

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03736
[1mStep[0m  [8/84], [94mLoss[0m : 1.98581
[1mStep[0m  [16/84], [94mLoss[0m : 1.76184
[1mStep[0m  [24/84], [94mLoss[0m : 2.10437
[1mStep[0m  [32/84], [94mLoss[0m : 1.86254
[1mStep[0m  [40/84], [94mLoss[0m : 2.03906
[1mStep[0m  [48/84], [94mLoss[0m : 2.08850
[1mStep[0m  [56/84], [94mLoss[0m : 1.92246
[1mStep[0m  [64/84], [94mLoss[0m : 2.14987
[1mStep[0m  [72/84], [94mLoss[0m : 2.40411
[1mStep[0m  [80/84], [94mLoss[0m : 2.21299

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.047, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94739
[1mStep[0m  [8/84], [94mLoss[0m : 1.80727
[1mStep[0m  [16/84], [94mLoss[0m : 1.89578
[1mStep[0m  [24/84], [94mLoss[0m : 2.29052
[1mStep[0m  [32/84], [94mLoss[0m : 1.98657
[1mStep[0m  [40/84], [94mLoss[0m : 1.97963
[1mStep[0m  [48/84], [94mLoss[0m : 1.79628
[1mStep[0m  [56/84], [94mLoss[0m : 2.12527
[1mStep[0m  [64/84], [94mLoss[0m : 1.95536
[1mStep[0m  [72/84], [94mLoss[0m : 1.94931
[1mStep[0m  [80/84], [94mLoss[0m : 1.97473

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93213
[1mStep[0m  [8/84], [94mLoss[0m : 1.66766
[1mStep[0m  [16/84], [94mLoss[0m : 1.86385
[1mStep[0m  [24/84], [94mLoss[0m : 1.75869
[1mStep[0m  [32/84], [94mLoss[0m : 1.76881
[1mStep[0m  [40/84], [94mLoss[0m : 1.83247
[1mStep[0m  [48/84], [94mLoss[0m : 2.03668
[1mStep[0m  [56/84], [94mLoss[0m : 2.12986
[1mStep[0m  [64/84], [94mLoss[0m : 2.02939
[1mStep[0m  [72/84], [94mLoss[0m : 1.90305
[1mStep[0m  [80/84], [94mLoss[0m : 1.99364

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61321
[1mStep[0m  [8/84], [94mLoss[0m : 1.85441
[1mStep[0m  [16/84], [94mLoss[0m : 2.10697
[1mStep[0m  [24/84], [94mLoss[0m : 2.00571
[1mStep[0m  [32/84], [94mLoss[0m : 2.14062
[1mStep[0m  [40/84], [94mLoss[0m : 2.08823
[1mStep[0m  [48/84], [94mLoss[0m : 1.85280
[1mStep[0m  [56/84], [94mLoss[0m : 1.98650
[1mStep[0m  [64/84], [94mLoss[0m : 1.90709
[1mStep[0m  [72/84], [94mLoss[0m : 2.03501
[1mStep[0m  [80/84], [94mLoss[0m : 1.86500

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78488
[1mStep[0m  [8/84], [94mLoss[0m : 1.96179
[1mStep[0m  [16/84], [94mLoss[0m : 1.76867
[1mStep[0m  [24/84], [94mLoss[0m : 2.10148
[1mStep[0m  [32/84], [94mLoss[0m : 1.77187
[1mStep[0m  [40/84], [94mLoss[0m : 1.53176
[1mStep[0m  [48/84], [94mLoss[0m : 1.92673
[1mStep[0m  [56/84], [94mLoss[0m : 2.04277
[1mStep[0m  [64/84], [94mLoss[0m : 1.74796
[1mStep[0m  [72/84], [94mLoss[0m : 1.83060
[1mStep[0m  [80/84], [94mLoss[0m : 1.81560

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88435
[1mStep[0m  [8/84], [94mLoss[0m : 1.82822
[1mStep[0m  [16/84], [94mLoss[0m : 1.83044
[1mStep[0m  [24/84], [94mLoss[0m : 1.72859
[1mStep[0m  [32/84], [94mLoss[0m : 1.79097
[1mStep[0m  [40/84], [94mLoss[0m : 1.98308
[1mStep[0m  [48/84], [94mLoss[0m : 1.83774
[1mStep[0m  [56/84], [94mLoss[0m : 1.75317
[1mStep[0m  [64/84], [94mLoss[0m : 1.51168
[1mStep[0m  [72/84], [94mLoss[0m : 1.84786
[1mStep[0m  [80/84], [94mLoss[0m : 1.90824

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76628
[1mStep[0m  [8/84], [94mLoss[0m : 1.74300
[1mStep[0m  [16/84], [94mLoss[0m : 1.69886
[1mStep[0m  [24/84], [94mLoss[0m : 1.66630
[1mStep[0m  [32/84], [94mLoss[0m : 1.67533
[1mStep[0m  [40/84], [94mLoss[0m : 1.89475
[1mStep[0m  [48/84], [94mLoss[0m : 1.70304
[1mStep[0m  [56/84], [94mLoss[0m : 1.94565
[1mStep[0m  [64/84], [94mLoss[0m : 1.96930
[1mStep[0m  [72/84], [94mLoss[0m : 2.03199
[1mStep[0m  [80/84], [94mLoss[0m : 1.80105

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70741
[1mStep[0m  [8/84], [94mLoss[0m : 1.81444
[1mStep[0m  [16/84], [94mLoss[0m : 1.83334
[1mStep[0m  [24/84], [94mLoss[0m : 1.70006
[1mStep[0m  [32/84], [94mLoss[0m : 1.55005
[1mStep[0m  [40/84], [94mLoss[0m : 1.42648
[1mStep[0m  [48/84], [94mLoss[0m : 1.61459
[1mStep[0m  [56/84], [94mLoss[0m : 1.83681
[1mStep[0m  [64/84], [94mLoss[0m : 1.57186
[1mStep[0m  [72/84], [94mLoss[0m : 1.70451
[1mStep[0m  [80/84], [94mLoss[0m : 1.53383

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.742, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63282
[1mStep[0m  [8/84], [94mLoss[0m : 1.63590
[1mStep[0m  [16/84], [94mLoss[0m : 1.78452
[1mStep[0m  [24/84], [94mLoss[0m : 1.79895
[1mStep[0m  [32/84], [94mLoss[0m : 1.75985
[1mStep[0m  [40/84], [94mLoss[0m : 1.64182
[1mStep[0m  [48/84], [94mLoss[0m : 1.80071
[1mStep[0m  [56/84], [94mLoss[0m : 1.55973
[1mStep[0m  [64/84], [94mLoss[0m : 1.91999
[1mStep[0m  [72/84], [94mLoss[0m : 1.67567
[1mStep[0m  [80/84], [94mLoss[0m : 1.79802

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.724, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64926
[1mStep[0m  [8/84], [94mLoss[0m : 1.50756
[1mStep[0m  [16/84], [94mLoss[0m : 1.58314
[1mStep[0m  [24/84], [94mLoss[0m : 1.54634
[1mStep[0m  [32/84], [94mLoss[0m : 1.66878
[1mStep[0m  [40/84], [94mLoss[0m : 1.52538
[1mStep[0m  [48/84], [94mLoss[0m : 1.59373
[1mStep[0m  [56/84], [94mLoss[0m : 1.82479
[1mStep[0m  [64/84], [94mLoss[0m : 1.70367
[1mStep[0m  [72/84], [94mLoss[0m : 1.71038
[1mStep[0m  [80/84], [94mLoss[0m : 1.85822

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67372
[1mStep[0m  [8/84], [94mLoss[0m : 1.72610
[1mStep[0m  [16/84], [94mLoss[0m : 1.71476
[1mStep[0m  [24/84], [94mLoss[0m : 1.46699
[1mStep[0m  [32/84], [94mLoss[0m : 1.88371
[1mStep[0m  [40/84], [94mLoss[0m : 1.70276
[1mStep[0m  [48/84], [94mLoss[0m : 1.59961
[1mStep[0m  [56/84], [94mLoss[0m : 1.72494
[1mStep[0m  [64/84], [94mLoss[0m : 1.43920
[1mStep[0m  [72/84], [94mLoss[0m : 1.75545
[1mStep[0m  [80/84], [94mLoss[0m : 1.58823

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46071
[1mStep[0m  [8/84], [94mLoss[0m : 1.60570
[1mStep[0m  [16/84], [94mLoss[0m : 1.70786
[1mStep[0m  [24/84], [94mLoss[0m : 1.40894
[1mStep[0m  [32/84], [94mLoss[0m : 1.30652
[1mStep[0m  [40/84], [94mLoss[0m : 1.37714
[1mStep[0m  [48/84], [94mLoss[0m : 1.58277
[1mStep[0m  [56/84], [94mLoss[0m : 1.57249
[1mStep[0m  [64/84], [94mLoss[0m : 1.73369
[1mStep[0m  [72/84], [94mLoss[0m : 1.76446
[1mStep[0m  [80/84], [94mLoss[0m : 1.70350

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50607
[1mStep[0m  [8/84], [94mLoss[0m : 1.55454
[1mStep[0m  [16/84], [94mLoss[0m : 1.58946
[1mStep[0m  [24/84], [94mLoss[0m : 1.42581
[1mStep[0m  [32/84], [94mLoss[0m : 1.52624
[1mStep[0m  [40/84], [94mLoss[0m : 1.37982
[1mStep[0m  [48/84], [94mLoss[0m : 1.85499
[1mStep[0m  [56/84], [94mLoss[0m : 1.62907
[1mStep[0m  [64/84], [94mLoss[0m : 1.79559
[1mStep[0m  [72/84], [94mLoss[0m : 1.54271
[1mStep[0m  [80/84], [94mLoss[0m : 1.46856

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.583, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44378
[1mStep[0m  [8/84], [94mLoss[0m : 1.59961
[1mStep[0m  [16/84], [94mLoss[0m : 1.64208
[1mStep[0m  [24/84], [94mLoss[0m : 1.74336
[1mStep[0m  [32/84], [94mLoss[0m : 1.59363
[1mStep[0m  [40/84], [94mLoss[0m : 1.48705
[1mStep[0m  [48/84], [94mLoss[0m : 1.47400
[1mStep[0m  [56/84], [94mLoss[0m : 1.43955
[1mStep[0m  [64/84], [94mLoss[0m : 1.57553
[1mStep[0m  [72/84], [94mLoss[0m : 1.51266
[1mStep[0m  [80/84], [94mLoss[0m : 1.52258

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63712
[1mStep[0m  [8/84], [94mLoss[0m : 1.44030
[1mStep[0m  [16/84], [94mLoss[0m : 1.25014
[1mStep[0m  [24/84], [94mLoss[0m : 1.45879
[1mStep[0m  [32/84], [94mLoss[0m : 1.38631
[1mStep[0m  [40/84], [94mLoss[0m : 1.42708
[1mStep[0m  [48/84], [94mLoss[0m : 1.39950
[1mStep[0m  [56/84], [94mLoss[0m : 1.29015
[1mStep[0m  [64/84], [94mLoss[0m : 1.41262
[1mStep[0m  [72/84], [94mLoss[0m : 1.70458
[1mStep[0m  [80/84], [94mLoss[0m : 1.52908

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.512, [92mTest[0m: 2.520, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42173
[1mStep[0m  [8/84], [94mLoss[0m : 1.50854
[1mStep[0m  [16/84], [94mLoss[0m : 1.40523
[1mStep[0m  [24/84], [94mLoss[0m : 1.45334
[1mStep[0m  [32/84], [94mLoss[0m : 1.31676
[1mStep[0m  [40/84], [94mLoss[0m : 1.27764
[1mStep[0m  [48/84], [94mLoss[0m : 1.69585
[1mStep[0m  [56/84], [94mLoss[0m : 1.55585
[1mStep[0m  [64/84], [94mLoss[0m : 1.34649
[1mStep[0m  [72/84], [94mLoss[0m : 1.38303
[1mStep[0m  [80/84], [94mLoss[0m : 1.55903

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.445, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30000
[1mStep[0m  [8/84], [94mLoss[0m : 1.37444
[1mStep[0m  [16/84], [94mLoss[0m : 1.54124
[1mStep[0m  [24/84], [94mLoss[0m : 1.53074
[1mStep[0m  [32/84], [94mLoss[0m : 1.57754
[1mStep[0m  [40/84], [94mLoss[0m : 1.32692
[1mStep[0m  [48/84], [94mLoss[0m : 1.42260
[1mStep[0m  [56/84], [94mLoss[0m : 1.53350
[1mStep[0m  [64/84], [94mLoss[0m : 1.30474
[1mStep[0m  [72/84], [94mLoss[0m : 1.68584
[1mStep[0m  [80/84], [94mLoss[0m : 1.58812

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.494, [92mTest[0m: 2.534, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53466
[1mStep[0m  [8/84], [94mLoss[0m : 1.62376
[1mStep[0m  [16/84], [94mLoss[0m : 1.31260
[1mStep[0m  [24/84], [94mLoss[0m : 1.53806
[1mStep[0m  [32/84], [94mLoss[0m : 1.43907
[1mStep[0m  [40/84], [94mLoss[0m : 1.65335
[1mStep[0m  [48/84], [94mLoss[0m : 1.57700
[1mStep[0m  [56/84], [94mLoss[0m : 1.39799
[1mStep[0m  [64/84], [94mLoss[0m : 1.49945
[1mStep[0m  [72/84], [94mLoss[0m : 1.76800
[1mStep[0m  [80/84], [94mLoss[0m : 1.50204

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.546, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60582
[1mStep[0m  [8/84], [94mLoss[0m : 1.53215
[1mStep[0m  [16/84], [94mLoss[0m : 1.42294
[1mStep[0m  [24/84], [94mLoss[0m : 1.11743
[1mStep[0m  [32/84], [94mLoss[0m : 1.48165
[1mStep[0m  [40/84], [94mLoss[0m : 1.36644
[1mStep[0m  [48/84], [94mLoss[0m : 1.41337
[1mStep[0m  [56/84], [94mLoss[0m : 1.26823
[1mStep[0m  [64/84], [94mLoss[0m : 1.41477
[1mStep[0m  [72/84], [94mLoss[0m : 1.38241
[1mStep[0m  [80/84], [94mLoss[0m : 1.42802

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.435, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42903
[1mStep[0m  [8/84], [94mLoss[0m : 1.45741
[1mStep[0m  [16/84], [94mLoss[0m : 1.43867
[1mStep[0m  [24/84], [94mLoss[0m : 1.45308
[1mStep[0m  [32/84], [94mLoss[0m : 1.45878
[1mStep[0m  [40/84], [94mLoss[0m : 1.52649
[1mStep[0m  [48/84], [94mLoss[0m : 1.63630
[1mStep[0m  [56/84], [94mLoss[0m : 1.30821
[1mStep[0m  [64/84], [94mLoss[0m : 1.45872
[1mStep[0m  [72/84], [94mLoss[0m : 1.47382
[1mStep[0m  [80/84], [94mLoss[0m : 1.33981

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.420, [92mTest[0m: 2.493, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.519
====================================

Phase 2 - Evaluation MAE:  2.519312117780958
MAE score P1       2.323388
MAE score P2       2.519312
loss               1.420245
learning_rate          0.01
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay          0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.20878
[1mStep[0m  [8/84], [94mLoss[0m : 4.35779
[1mStep[0m  [16/84], [94mLoss[0m : 2.87372
[1mStep[0m  [24/84], [94mLoss[0m : 2.71925
[1mStep[0m  [32/84], [94mLoss[0m : 2.70311
[1mStep[0m  [40/84], [94mLoss[0m : 2.44501
[1mStep[0m  [48/84], [94mLoss[0m : 2.60582
[1mStep[0m  [56/84], [94mLoss[0m : 2.45541
[1mStep[0m  [64/84], [94mLoss[0m : 2.18007
[1mStep[0m  [72/84], [94mLoss[0m : 2.49712
[1mStep[0m  [80/84], [94mLoss[0m : 2.69902

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.235, [92mTest[0m: 10.809, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31408
[1mStep[0m  [8/84], [94mLoss[0m : 2.58840
[1mStep[0m  [16/84], [94mLoss[0m : 2.29310
[1mStep[0m  [24/84], [94mLoss[0m : 2.54528
[1mStep[0m  [32/84], [94mLoss[0m : 2.88111
[1mStep[0m  [40/84], [94mLoss[0m : 2.18743
[1mStep[0m  [48/84], [94mLoss[0m : 2.14046
[1mStep[0m  [56/84], [94mLoss[0m : 2.57263
[1mStep[0m  [64/84], [94mLoss[0m : 2.44394
[1mStep[0m  [72/84], [94mLoss[0m : 2.55063
[1mStep[0m  [80/84], [94mLoss[0m : 2.51351

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47790
[1mStep[0m  [8/84], [94mLoss[0m : 2.40450
[1mStep[0m  [16/84], [94mLoss[0m : 2.56211
[1mStep[0m  [24/84], [94mLoss[0m : 2.29416
[1mStep[0m  [32/84], [94mLoss[0m : 2.56293
[1mStep[0m  [40/84], [94mLoss[0m : 2.27759
[1mStep[0m  [48/84], [94mLoss[0m : 2.79408
[1mStep[0m  [56/84], [94mLoss[0m : 2.83371
[1mStep[0m  [64/84], [94mLoss[0m : 2.34421
[1mStep[0m  [72/84], [94mLoss[0m : 2.63410
[1mStep[0m  [80/84], [94mLoss[0m : 2.66318

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51576
[1mStep[0m  [8/84], [94mLoss[0m : 2.88810
[1mStep[0m  [16/84], [94mLoss[0m : 2.47136
[1mStep[0m  [24/84], [94mLoss[0m : 2.18694
[1mStep[0m  [32/84], [94mLoss[0m : 2.50994
[1mStep[0m  [40/84], [94mLoss[0m : 2.30694
[1mStep[0m  [48/84], [94mLoss[0m : 2.29536
[1mStep[0m  [56/84], [94mLoss[0m : 2.55679
[1mStep[0m  [64/84], [94mLoss[0m : 2.30859
[1mStep[0m  [72/84], [94mLoss[0m : 2.36106
[1mStep[0m  [80/84], [94mLoss[0m : 2.41537

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33731
[1mStep[0m  [8/84], [94mLoss[0m : 2.61106
[1mStep[0m  [16/84], [94mLoss[0m : 2.39521
[1mStep[0m  [24/84], [94mLoss[0m : 2.47664
[1mStep[0m  [32/84], [94mLoss[0m : 2.45853
[1mStep[0m  [40/84], [94mLoss[0m : 2.79835
[1mStep[0m  [48/84], [94mLoss[0m : 2.51075
[1mStep[0m  [56/84], [94mLoss[0m : 2.67898
[1mStep[0m  [64/84], [94mLoss[0m : 2.25577
[1mStep[0m  [72/84], [94mLoss[0m : 2.53120
[1mStep[0m  [80/84], [94mLoss[0m : 2.44233

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55556
[1mStep[0m  [8/84], [94mLoss[0m : 2.55703
[1mStep[0m  [16/84], [94mLoss[0m : 2.00577
[1mStep[0m  [24/84], [94mLoss[0m : 2.44187
[1mStep[0m  [32/84], [94mLoss[0m : 2.10186
[1mStep[0m  [40/84], [94mLoss[0m : 2.19511
[1mStep[0m  [48/84], [94mLoss[0m : 2.19709
[1mStep[0m  [56/84], [94mLoss[0m : 2.52174
[1mStep[0m  [64/84], [94mLoss[0m : 2.31370
[1mStep[0m  [72/84], [94mLoss[0m : 2.27061
[1mStep[0m  [80/84], [94mLoss[0m : 2.31947

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52317
[1mStep[0m  [8/84], [94mLoss[0m : 2.58089
[1mStep[0m  [16/84], [94mLoss[0m : 2.45479
[1mStep[0m  [24/84], [94mLoss[0m : 2.52715
[1mStep[0m  [32/84], [94mLoss[0m : 2.31987
[1mStep[0m  [40/84], [94mLoss[0m : 2.46413
[1mStep[0m  [48/84], [94mLoss[0m : 2.35865
[1mStep[0m  [56/84], [94mLoss[0m : 2.30245
[1mStep[0m  [64/84], [94mLoss[0m : 2.69838
[1mStep[0m  [72/84], [94mLoss[0m : 2.58996
[1mStep[0m  [80/84], [94mLoss[0m : 2.32665

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29973
[1mStep[0m  [8/84], [94mLoss[0m : 2.39224
[1mStep[0m  [16/84], [94mLoss[0m : 2.37565
[1mStep[0m  [24/84], [94mLoss[0m : 2.43066
[1mStep[0m  [32/84], [94mLoss[0m : 2.57044
[1mStep[0m  [40/84], [94mLoss[0m : 2.49656
[1mStep[0m  [48/84], [94mLoss[0m : 2.20339
[1mStep[0m  [56/84], [94mLoss[0m : 2.44133
[1mStep[0m  [64/84], [94mLoss[0m : 2.54457
[1mStep[0m  [72/84], [94mLoss[0m : 2.46865
[1mStep[0m  [80/84], [94mLoss[0m : 2.55143

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72260
[1mStep[0m  [8/84], [94mLoss[0m : 2.56627
[1mStep[0m  [16/84], [94mLoss[0m : 2.37937
[1mStep[0m  [24/84], [94mLoss[0m : 2.43803
[1mStep[0m  [32/84], [94mLoss[0m : 2.67854
[1mStep[0m  [40/84], [94mLoss[0m : 2.30433
[1mStep[0m  [48/84], [94mLoss[0m : 2.09252
[1mStep[0m  [56/84], [94mLoss[0m : 2.38518
[1mStep[0m  [64/84], [94mLoss[0m : 2.26582
[1mStep[0m  [72/84], [94mLoss[0m : 2.48432
[1mStep[0m  [80/84], [94mLoss[0m : 2.31855

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39699
[1mStep[0m  [8/84], [94mLoss[0m : 2.54868
[1mStep[0m  [16/84], [94mLoss[0m : 2.32500
[1mStep[0m  [24/84], [94mLoss[0m : 2.26251
[1mStep[0m  [32/84], [94mLoss[0m : 2.64760
[1mStep[0m  [40/84], [94mLoss[0m : 2.50639
[1mStep[0m  [48/84], [94mLoss[0m : 2.43149
[1mStep[0m  [56/84], [94mLoss[0m : 2.43033
[1mStep[0m  [64/84], [94mLoss[0m : 2.48397
[1mStep[0m  [72/84], [94mLoss[0m : 2.41799
[1mStep[0m  [80/84], [94mLoss[0m : 2.57760

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51751
[1mStep[0m  [8/84], [94mLoss[0m : 2.53382
[1mStep[0m  [16/84], [94mLoss[0m : 2.38798
[1mStep[0m  [24/84], [94mLoss[0m : 2.44774
[1mStep[0m  [32/84], [94mLoss[0m : 2.35565
[1mStep[0m  [40/84], [94mLoss[0m : 2.38144
[1mStep[0m  [48/84], [94mLoss[0m : 2.68526
[1mStep[0m  [56/84], [94mLoss[0m : 2.44463
[1mStep[0m  [64/84], [94mLoss[0m : 2.32875
[1mStep[0m  [72/84], [94mLoss[0m : 2.44041
[1mStep[0m  [80/84], [94mLoss[0m : 2.67515

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66811
[1mStep[0m  [8/84], [94mLoss[0m : 2.28774
[1mStep[0m  [16/84], [94mLoss[0m : 2.22534
[1mStep[0m  [24/84], [94mLoss[0m : 2.34783
[1mStep[0m  [32/84], [94mLoss[0m : 2.39099
[1mStep[0m  [40/84], [94mLoss[0m : 2.33446
[1mStep[0m  [48/84], [94mLoss[0m : 2.50824
[1mStep[0m  [56/84], [94mLoss[0m : 2.66406
[1mStep[0m  [64/84], [94mLoss[0m : 2.48378
[1mStep[0m  [72/84], [94mLoss[0m : 2.55904
[1mStep[0m  [80/84], [94mLoss[0m : 2.71185

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42636
[1mStep[0m  [8/84], [94mLoss[0m : 2.40492
[1mStep[0m  [16/84], [94mLoss[0m : 2.30807
[1mStep[0m  [24/84], [94mLoss[0m : 2.56008
[1mStep[0m  [32/84], [94mLoss[0m : 2.32638
[1mStep[0m  [40/84], [94mLoss[0m : 2.32702
[1mStep[0m  [48/84], [94mLoss[0m : 2.26089
[1mStep[0m  [56/84], [94mLoss[0m : 2.32347
[1mStep[0m  [64/84], [94mLoss[0m : 2.47638
[1mStep[0m  [72/84], [94mLoss[0m : 2.42455
[1mStep[0m  [80/84], [94mLoss[0m : 2.14525

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37376
[1mStep[0m  [8/84], [94mLoss[0m : 2.44816
[1mStep[0m  [16/84], [94mLoss[0m : 2.28795
[1mStep[0m  [24/84], [94mLoss[0m : 2.18306
[1mStep[0m  [32/84], [94mLoss[0m : 2.39942
[1mStep[0m  [40/84], [94mLoss[0m : 2.51383
[1mStep[0m  [48/84], [94mLoss[0m : 2.39065
[1mStep[0m  [56/84], [94mLoss[0m : 2.57330
[1mStep[0m  [64/84], [94mLoss[0m : 2.11140
[1mStep[0m  [72/84], [94mLoss[0m : 2.32568
[1mStep[0m  [80/84], [94mLoss[0m : 2.42221

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41646
[1mStep[0m  [8/84], [94mLoss[0m : 2.43442
[1mStep[0m  [16/84], [94mLoss[0m : 2.37769
[1mStep[0m  [24/84], [94mLoss[0m : 2.30306
[1mStep[0m  [32/84], [94mLoss[0m : 2.35072
[1mStep[0m  [40/84], [94mLoss[0m : 2.25024
[1mStep[0m  [48/84], [94mLoss[0m : 2.47198
[1mStep[0m  [56/84], [94mLoss[0m : 2.46029
[1mStep[0m  [64/84], [94mLoss[0m : 2.54411
[1mStep[0m  [72/84], [94mLoss[0m : 2.31575
[1mStep[0m  [80/84], [94mLoss[0m : 2.17199

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64702
[1mStep[0m  [8/84], [94mLoss[0m : 2.43329
[1mStep[0m  [16/84], [94mLoss[0m : 2.75439
[1mStep[0m  [24/84], [94mLoss[0m : 2.25669
[1mStep[0m  [32/84], [94mLoss[0m : 2.19893
[1mStep[0m  [40/84], [94mLoss[0m : 2.49893
[1mStep[0m  [48/84], [94mLoss[0m : 2.49436
[1mStep[0m  [56/84], [94mLoss[0m : 2.45345
[1mStep[0m  [64/84], [94mLoss[0m : 2.19275
[1mStep[0m  [72/84], [94mLoss[0m : 2.61291
[1mStep[0m  [80/84], [94mLoss[0m : 2.68698

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43232
[1mStep[0m  [8/84], [94mLoss[0m : 2.29975
[1mStep[0m  [16/84], [94mLoss[0m : 2.52544
[1mStep[0m  [24/84], [94mLoss[0m : 2.70697
[1mStep[0m  [32/84], [94mLoss[0m : 2.63363
[1mStep[0m  [40/84], [94mLoss[0m : 2.09224
[1mStep[0m  [48/84], [94mLoss[0m : 2.42629
[1mStep[0m  [56/84], [94mLoss[0m : 2.41757
[1mStep[0m  [64/84], [94mLoss[0m : 2.56741
[1mStep[0m  [72/84], [94mLoss[0m : 2.50927
[1mStep[0m  [80/84], [94mLoss[0m : 2.28198

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47895
[1mStep[0m  [8/84], [94mLoss[0m : 2.64188
[1mStep[0m  [16/84], [94mLoss[0m : 2.32102
[1mStep[0m  [24/84], [94mLoss[0m : 2.30111
[1mStep[0m  [32/84], [94mLoss[0m : 2.39427
[1mStep[0m  [40/84], [94mLoss[0m : 2.10884
[1mStep[0m  [48/84], [94mLoss[0m : 2.66812
[1mStep[0m  [56/84], [94mLoss[0m : 2.15396
[1mStep[0m  [64/84], [94mLoss[0m : 2.46137
[1mStep[0m  [72/84], [94mLoss[0m : 2.56253
[1mStep[0m  [80/84], [94mLoss[0m : 2.43920

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50806
[1mStep[0m  [8/84], [94mLoss[0m : 2.44310
[1mStep[0m  [16/84], [94mLoss[0m : 2.39986
[1mStep[0m  [24/84], [94mLoss[0m : 2.09526
[1mStep[0m  [32/84], [94mLoss[0m : 2.50279
[1mStep[0m  [40/84], [94mLoss[0m : 2.62884
[1mStep[0m  [48/84], [94mLoss[0m : 2.26856
[1mStep[0m  [56/84], [94mLoss[0m : 2.42766
[1mStep[0m  [64/84], [94mLoss[0m : 2.73920
[1mStep[0m  [72/84], [94mLoss[0m : 2.31451
[1mStep[0m  [80/84], [94mLoss[0m : 2.39461

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11786
[1mStep[0m  [8/84], [94mLoss[0m : 2.17635
[1mStep[0m  [16/84], [94mLoss[0m : 2.31549
[1mStep[0m  [24/84], [94mLoss[0m : 2.55280
[1mStep[0m  [32/84], [94mLoss[0m : 2.26983
[1mStep[0m  [40/84], [94mLoss[0m : 2.50114
[1mStep[0m  [48/84], [94mLoss[0m : 2.29853
[1mStep[0m  [56/84], [94mLoss[0m : 2.49008
[1mStep[0m  [64/84], [94mLoss[0m : 2.05725
[1mStep[0m  [72/84], [94mLoss[0m : 2.59676
[1mStep[0m  [80/84], [94mLoss[0m : 2.28978

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37638
[1mStep[0m  [8/84], [94mLoss[0m : 2.33604
[1mStep[0m  [16/84], [94mLoss[0m : 2.17530
[1mStep[0m  [24/84], [94mLoss[0m : 2.20382
[1mStep[0m  [32/84], [94mLoss[0m : 2.29919
[1mStep[0m  [40/84], [94mLoss[0m : 2.42573
[1mStep[0m  [48/84], [94mLoss[0m : 2.49606
[1mStep[0m  [56/84], [94mLoss[0m : 2.27091
[1mStep[0m  [64/84], [94mLoss[0m : 2.02147
[1mStep[0m  [72/84], [94mLoss[0m : 2.26163
[1mStep[0m  [80/84], [94mLoss[0m : 2.57419

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65275
[1mStep[0m  [8/84], [94mLoss[0m : 2.79466
[1mStep[0m  [16/84], [94mLoss[0m : 2.75865
[1mStep[0m  [24/84], [94mLoss[0m : 2.24559
[1mStep[0m  [32/84], [94mLoss[0m : 2.47224
[1mStep[0m  [40/84], [94mLoss[0m : 2.42928
[1mStep[0m  [48/84], [94mLoss[0m : 2.48462
[1mStep[0m  [56/84], [94mLoss[0m : 2.34982
[1mStep[0m  [64/84], [94mLoss[0m : 2.24230
[1mStep[0m  [72/84], [94mLoss[0m : 2.46118
[1mStep[0m  [80/84], [94mLoss[0m : 2.50561

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20163
[1mStep[0m  [8/84], [94mLoss[0m : 2.86729
[1mStep[0m  [16/84], [94mLoss[0m : 2.36826
[1mStep[0m  [24/84], [94mLoss[0m : 2.63432
[1mStep[0m  [32/84], [94mLoss[0m : 2.13965
[1mStep[0m  [40/84], [94mLoss[0m : 2.41995
[1mStep[0m  [48/84], [94mLoss[0m : 2.46055
[1mStep[0m  [56/84], [94mLoss[0m : 2.66864
[1mStep[0m  [64/84], [94mLoss[0m : 2.35821
[1mStep[0m  [72/84], [94mLoss[0m : 2.43964
[1mStep[0m  [80/84], [94mLoss[0m : 2.54289

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27020
[1mStep[0m  [8/84], [94mLoss[0m : 2.54488
[1mStep[0m  [16/84], [94mLoss[0m : 2.46143
[1mStep[0m  [24/84], [94mLoss[0m : 2.35161
[1mStep[0m  [32/84], [94mLoss[0m : 2.36664
[1mStep[0m  [40/84], [94mLoss[0m : 2.19841
[1mStep[0m  [48/84], [94mLoss[0m : 2.38125
[1mStep[0m  [56/84], [94mLoss[0m : 2.53493
[1mStep[0m  [64/84], [94mLoss[0m : 2.28619
[1mStep[0m  [72/84], [94mLoss[0m : 2.51985
[1mStep[0m  [80/84], [94mLoss[0m : 2.38779

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04609
[1mStep[0m  [8/84], [94mLoss[0m : 2.46304
[1mStep[0m  [16/84], [94mLoss[0m : 2.60670
[1mStep[0m  [24/84], [94mLoss[0m : 2.42314
[1mStep[0m  [32/84], [94mLoss[0m : 2.68004
[1mStep[0m  [40/84], [94mLoss[0m : 2.42958
[1mStep[0m  [48/84], [94mLoss[0m : 2.50615
[1mStep[0m  [56/84], [94mLoss[0m : 2.57983
[1mStep[0m  [64/84], [94mLoss[0m : 2.48665
[1mStep[0m  [72/84], [94mLoss[0m : 2.29600
[1mStep[0m  [80/84], [94mLoss[0m : 2.09668

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24589
[1mStep[0m  [8/84], [94mLoss[0m : 2.39049
[1mStep[0m  [16/84], [94mLoss[0m : 2.83084
[1mStep[0m  [24/84], [94mLoss[0m : 2.34960
[1mStep[0m  [32/84], [94mLoss[0m : 2.17459
[1mStep[0m  [40/84], [94mLoss[0m : 2.45032
[1mStep[0m  [48/84], [94mLoss[0m : 2.24830
[1mStep[0m  [56/84], [94mLoss[0m : 2.56946
[1mStep[0m  [64/84], [94mLoss[0m : 2.34764
[1mStep[0m  [72/84], [94mLoss[0m : 2.46902
[1mStep[0m  [80/84], [94mLoss[0m : 2.65731

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52272
[1mStep[0m  [8/84], [94mLoss[0m : 2.64054
[1mStep[0m  [16/84], [94mLoss[0m : 2.45610
[1mStep[0m  [24/84], [94mLoss[0m : 2.23444
[1mStep[0m  [32/84], [94mLoss[0m : 2.50098
[1mStep[0m  [40/84], [94mLoss[0m : 2.63737
[1mStep[0m  [48/84], [94mLoss[0m : 2.48584
[1mStep[0m  [56/84], [94mLoss[0m : 2.13669
[1mStep[0m  [64/84], [94mLoss[0m : 2.54477
[1mStep[0m  [72/84], [94mLoss[0m : 2.25027
[1mStep[0m  [80/84], [94mLoss[0m : 2.35658

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21747
[1mStep[0m  [8/84], [94mLoss[0m : 2.49855
[1mStep[0m  [16/84], [94mLoss[0m : 2.22589
[1mStep[0m  [24/84], [94mLoss[0m : 2.22052
[1mStep[0m  [32/84], [94mLoss[0m : 2.44862
[1mStep[0m  [40/84], [94mLoss[0m : 2.40661
[1mStep[0m  [48/84], [94mLoss[0m : 2.77724
[1mStep[0m  [56/84], [94mLoss[0m : 2.40241
[1mStep[0m  [64/84], [94mLoss[0m : 2.55693
[1mStep[0m  [72/84], [94mLoss[0m : 2.81348
[1mStep[0m  [80/84], [94mLoss[0m : 2.42590

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48207
[1mStep[0m  [8/84], [94mLoss[0m : 2.71852
[1mStep[0m  [16/84], [94mLoss[0m : 2.43049
[1mStep[0m  [24/84], [94mLoss[0m : 2.53685
[1mStep[0m  [32/84], [94mLoss[0m : 2.15726
[1mStep[0m  [40/84], [94mLoss[0m : 2.35900
[1mStep[0m  [48/84], [94mLoss[0m : 2.50555
[1mStep[0m  [56/84], [94mLoss[0m : 2.56967
[1mStep[0m  [64/84], [94mLoss[0m : 2.44912
[1mStep[0m  [72/84], [94mLoss[0m : 2.05192
[1mStep[0m  [80/84], [94mLoss[0m : 2.28977

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44076
[1mStep[0m  [8/84], [94mLoss[0m : 2.34665
[1mStep[0m  [16/84], [94mLoss[0m : 2.25268
[1mStep[0m  [24/84], [94mLoss[0m : 2.15879
[1mStep[0m  [32/84], [94mLoss[0m : 2.47562
[1mStep[0m  [40/84], [94mLoss[0m : 2.24815
[1mStep[0m  [48/84], [94mLoss[0m : 2.60191
[1mStep[0m  [56/84], [94mLoss[0m : 2.39620
[1mStep[0m  [64/84], [94mLoss[0m : 2.24614
[1mStep[0m  [72/84], [94mLoss[0m : 2.47816
[1mStep[0m  [80/84], [94mLoss[0m : 2.45991

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.323
====================================

Phase 1 - Evaluation MAE:  2.3229729618344988
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.44793
[1mStep[0m  [8/84], [94mLoss[0m : 2.55991
[1mStep[0m  [16/84], [94mLoss[0m : 2.57798
[1mStep[0m  [24/84], [94mLoss[0m : 2.54597
[1mStep[0m  [32/84], [94mLoss[0m : 2.17481
[1mStep[0m  [40/84], [94mLoss[0m : 2.40408
[1mStep[0m  [48/84], [94mLoss[0m : 2.34488
[1mStep[0m  [56/84], [94mLoss[0m : 2.18164
[1mStep[0m  [64/84], [94mLoss[0m : 2.37632
[1mStep[0m  [72/84], [94mLoss[0m : 2.46094
[1mStep[0m  [80/84], [94mLoss[0m : 2.73772

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23193
[1mStep[0m  [8/84], [94mLoss[0m : 2.29905
[1mStep[0m  [16/84], [94mLoss[0m : 2.49485
[1mStep[0m  [24/84], [94mLoss[0m : 2.45778
[1mStep[0m  [32/84], [94mLoss[0m : 2.47585
[1mStep[0m  [40/84], [94mLoss[0m : 2.34139
[1mStep[0m  [48/84], [94mLoss[0m : 2.41253
[1mStep[0m  [56/84], [94mLoss[0m : 2.38569
[1mStep[0m  [64/84], [94mLoss[0m : 2.40093
[1mStep[0m  [72/84], [94mLoss[0m : 2.35433
[1mStep[0m  [80/84], [94mLoss[0m : 2.52947

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17356
[1mStep[0m  [8/84], [94mLoss[0m : 2.08615
[1mStep[0m  [16/84], [94mLoss[0m : 2.30926
[1mStep[0m  [24/84], [94mLoss[0m : 2.32180
[1mStep[0m  [32/84], [94mLoss[0m : 2.25779
[1mStep[0m  [40/84], [94mLoss[0m : 2.13062
[1mStep[0m  [48/84], [94mLoss[0m : 2.22191
[1mStep[0m  [56/84], [94mLoss[0m : 2.34372
[1mStep[0m  [64/84], [94mLoss[0m : 2.16214
[1mStep[0m  [72/84], [94mLoss[0m : 2.66837
[1mStep[0m  [80/84], [94mLoss[0m : 2.33010

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98305
[1mStep[0m  [8/84], [94mLoss[0m : 1.95151
[1mStep[0m  [16/84], [94mLoss[0m : 1.95092
[1mStep[0m  [24/84], [94mLoss[0m : 2.22021
[1mStep[0m  [32/84], [94mLoss[0m : 2.25297
[1mStep[0m  [40/84], [94mLoss[0m : 2.01081
[1mStep[0m  [48/84], [94mLoss[0m : 2.07962
[1mStep[0m  [56/84], [94mLoss[0m : 2.39572
[1mStep[0m  [64/84], [94mLoss[0m : 2.35838
[1mStep[0m  [72/84], [94mLoss[0m : 2.39185
[1mStep[0m  [80/84], [94mLoss[0m : 2.26998

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95352
[1mStep[0m  [8/84], [94mLoss[0m : 2.10401
[1mStep[0m  [16/84], [94mLoss[0m : 2.23149
[1mStep[0m  [24/84], [94mLoss[0m : 2.30775
[1mStep[0m  [32/84], [94mLoss[0m : 1.87925
[1mStep[0m  [40/84], [94mLoss[0m : 1.92877
[1mStep[0m  [48/84], [94mLoss[0m : 2.02666
[1mStep[0m  [56/84], [94mLoss[0m : 2.10520
[1mStep[0m  [64/84], [94mLoss[0m : 2.23260
[1mStep[0m  [72/84], [94mLoss[0m : 2.21620
[1mStep[0m  [80/84], [94mLoss[0m : 1.85749

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99313
[1mStep[0m  [8/84], [94mLoss[0m : 2.00068
[1mStep[0m  [16/84], [94mLoss[0m : 1.94764
[1mStep[0m  [24/84], [94mLoss[0m : 2.09462
[1mStep[0m  [32/84], [94mLoss[0m : 2.01496
[1mStep[0m  [40/84], [94mLoss[0m : 1.95396
[1mStep[0m  [48/84], [94mLoss[0m : 1.84767
[1mStep[0m  [56/84], [94mLoss[0m : 2.11488
[1mStep[0m  [64/84], [94mLoss[0m : 1.94274
[1mStep[0m  [72/84], [94mLoss[0m : 1.90933
[1mStep[0m  [80/84], [94mLoss[0m : 2.42248

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15350
[1mStep[0m  [8/84], [94mLoss[0m : 1.80230
[1mStep[0m  [16/84], [94mLoss[0m : 1.78783
[1mStep[0m  [24/84], [94mLoss[0m : 1.54208
[1mStep[0m  [32/84], [94mLoss[0m : 1.64219
[1mStep[0m  [40/84], [94mLoss[0m : 2.07020
[1mStep[0m  [48/84], [94mLoss[0m : 1.72902
[1mStep[0m  [56/84], [94mLoss[0m : 2.06320
[1mStep[0m  [64/84], [94mLoss[0m : 1.84022
[1mStep[0m  [72/84], [94mLoss[0m : 1.77865
[1mStep[0m  [80/84], [94mLoss[0m : 2.07533

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80103
[1mStep[0m  [8/84], [94mLoss[0m : 1.77371
[1mStep[0m  [16/84], [94mLoss[0m : 2.06272
[1mStep[0m  [24/84], [94mLoss[0m : 1.80804
[1mStep[0m  [32/84], [94mLoss[0m : 2.01924
[1mStep[0m  [40/84], [94mLoss[0m : 1.90517
[1mStep[0m  [48/84], [94mLoss[0m : 1.85545
[1mStep[0m  [56/84], [94mLoss[0m : 2.15636
[1mStep[0m  [64/84], [94mLoss[0m : 1.76219
[1mStep[0m  [72/84], [94mLoss[0m : 2.08308
[1mStep[0m  [80/84], [94mLoss[0m : 2.05719

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57199
[1mStep[0m  [8/84], [94mLoss[0m : 1.89659
[1mStep[0m  [16/84], [94mLoss[0m : 1.68560
[1mStep[0m  [24/84], [94mLoss[0m : 1.80638
[1mStep[0m  [32/84], [94mLoss[0m : 1.58499
[1mStep[0m  [40/84], [94mLoss[0m : 1.81053
[1mStep[0m  [48/84], [94mLoss[0m : 1.80619
[1mStep[0m  [56/84], [94mLoss[0m : 1.97584
[1mStep[0m  [64/84], [94mLoss[0m : 1.97491
[1mStep[0m  [72/84], [94mLoss[0m : 1.82867
[1mStep[0m  [80/84], [94mLoss[0m : 1.82015

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.827, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04140
[1mStep[0m  [8/84], [94mLoss[0m : 1.92597
[1mStep[0m  [16/84], [94mLoss[0m : 1.74164
[1mStep[0m  [24/84], [94mLoss[0m : 1.69350
[1mStep[0m  [32/84], [94mLoss[0m : 1.73376
[1mStep[0m  [40/84], [94mLoss[0m : 1.75205
[1mStep[0m  [48/84], [94mLoss[0m : 1.75951
[1mStep[0m  [56/84], [94mLoss[0m : 1.71814
[1mStep[0m  [64/84], [94mLoss[0m : 2.00927
[1mStep[0m  [72/84], [94mLoss[0m : 1.71273
[1mStep[0m  [80/84], [94mLoss[0m : 1.78644

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83085
[1mStep[0m  [8/84], [94mLoss[0m : 1.75496
[1mStep[0m  [16/84], [94mLoss[0m : 1.67159
[1mStep[0m  [24/84], [94mLoss[0m : 1.88974
[1mStep[0m  [32/84], [94mLoss[0m : 1.58970
[1mStep[0m  [40/84], [94mLoss[0m : 1.61994
[1mStep[0m  [48/84], [94mLoss[0m : 1.71720
[1mStep[0m  [56/84], [94mLoss[0m : 1.71981
[1mStep[0m  [64/84], [94mLoss[0m : 1.83636
[1mStep[0m  [72/84], [94mLoss[0m : 1.57644
[1mStep[0m  [80/84], [94mLoss[0m : 1.50972

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76311
[1mStep[0m  [8/84], [94mLoss[0m : 1.43529
[1mStep[0m  [16/84], [94mLoss[0m : 1.60111
[1mStep[0m  [24/84], [94mLoss[0m : 1.87530
[1mStep[0m  [32/84], [94mLoss[0m : 1.62481
[1mStep[0m  [40/84], [94mLoss[0m : 1.67874
[1mStep[0m  [48/84], [94mLoss[0m : 1.80713
[1mStep[0m  [56/84], [94mLoss[0m : 1.90913
[1mStep[0m  [64/84], [94mLoss[0m : 1.58379
[1mStep[0m  [72/84], [94mLoss[0m : 1.87027
[1mStep[0m  [80/84], [94mLoss[0m : 1.75705

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70057
[1mStep[0m  [8/84], [94mLoss[0m : 1.43416
[1mStep[0m  [16/84], [94mLoss[0m : 1.68627
[1mStep[0m  [24/84], [94mLoss[0m : 1.78574
[1mStep[0m  [32/84], [94mLoss[0m : 1.61260
[1mStep[0m  [40/84], [94mLoss[0m : 1.45500
[1mStep[0m  [48/84], [94mLoss[0m : 1.68899
[1mStep[0m  [56/84], [94mLoss[0m : 1.63114
[1mStep[0m  [64/84], [94mLoss[0m : 1.82355
[1mStep[0m  [72/84], [94mLoss[0m : 1.77886
[1mStep[0m  [80/84], [94mLoss[0m : 1.57273

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.686, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70505
[1mStep[0m  [8/84], [94mLoss[0m : 1.46783
[1mStep[0m  [16/84], [94mLoss[0m : 1.62718
[1mStep[0m  [24/84], [94mLoss[0m : 1.59905
[1mStep[0m  [32/84], [94mLoss[0m : 1.67091
[1mStep[0m  [40/84], [94mLoss[0m : 1.49965
[1mStep[0m  [48/84], [94mLoss[0m : 1.81086
[1mStep[0m  [56/84], [94mLoss[0m : 1.64470
[1mStep[0m  [64/84], [94mLoss[0m : 1.48024
[1mStep[0m  [72/84], [94mLoss[0m : 1.64773
[1mStep[0m  [80/84], [94mLoss[0m : 1.78145

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53976
[1mStep[0m  [8/84], [94mLoss[0m : 1.28859
[1mStep[0m  [16/84], [94mLoss[0m : 1.55391
[1mStep[0m  [24/84], [94mLoss[0m : 1.81267
[1mStep[0m  [32/84], [94mLoss[0m : 1.47598
[1mStep[0m  [40/84], [94mLoss[0m : 1.49282
[1mStep[0m  [48/84], [94mLoss[0m : 1.50649
[1mStep[0m  [56/84], [94mLoss[0m : 1.63031
[1mStep[0m  [64/84], [94mLoss[0m : 1.27785
[1mStep[0m  [72/84], [94mLoss[0m : 1.67406
[1mStep[0m  [80/84], [94mLoss[0m : 1.91643

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.527, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51542
[1mStep[0m  [8/84], [94mLoss[0m : 1.54444
[1mStep[0m  [16/84], [94mLoss[0m : 1.56034
[1mStep[0m  [24/84], [94mLoss[0m : 1.71361
[1mStep[0m  [32/84], [94mLoss[0m : 1.42435
[1mStep[0m  [40/84], [94mLoss[0m : 1.42715
[1mStep[0m  [48/84], [94mLoss[0m : 1.86126
[1mStep[0m  [56/84], [94mLoss[0m : 1.72536
[1mStep[0m  [64/84], [94mLoss[0m : 1.53795
[1mStep[0m  [72/84], [94mLoss[0m : 1.76597
[1mStep[0m  [80/84], [94mLoss[0m : 1.77295

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.565, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48927
[1mStep[0m  [8/84], [94mLoss[0m : 1.53372
[1mStep[0m  [16/84], [94mLoss[0m : 1.73589
[1mStep[0m  [24/84], [94mLoss[0m : 1.40830
[1mStep[0m  [32/84], [94mLoss[0m : 1.72840
[1mStep[0m  [40/84], [94mLoss[0m : 1.36907
[1mStep[0m  [48/84], [94mLoss[0m : 1.52474
[1mStep[0m  [56/84], [94mLoss[0m : 1.45626
[1mStep[0m  [64/84], [94mLoss[0m : 1.56389
[1mStep[0m  [72/84], [94mLoss[0m : 1.53019
[1mStep[0m  [80/84], [94mLoss[0m : 1.42344

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.26492
[1mStep[0m  [8/84], [94mLoss[0m : 1.57951
[1mStep[0m  [16/84], [94mLoss[0m : 1.42302
[1mStep[0m  [24/84], [94mLoss[0m : 1.53418
[1mStep[0m  [32/84], [94mLoss[0m : 1.22924
[1mStep[0m  [40/84], [94mLoss[0m : 1.42207
[1mStep[0m  [48/84], [94mLoss[0m : 1.64876
[1mStep[0m  [56/84], [94mLoss[0m : 1.39464
[1mStep[0m  [64/84], [94mLoss[0m : 1.39573
[1mStep[0m  [72/84], [94mLoss[0m : 1.60817
[1mStep[0m  [80/84], [94mLoss[0m : 1.65885

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.527, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.16078
[1mStep[0m  [8/84], [94mLoss[0m : 1.35431
[1mStep[0m  [16/84], [94mLoss[0m : 1.33012
[1mStep[0m  [24/84], [94mLoss[0m : 1.53439
[1mStep[0m  [32/84], [94mLoss[0m : 1.51030
[1mStep[0m  [40/84], [94mLoss[0m : 1.19289
[1mStep[0m  [48/84], [94mLoss[0m : 1.27851
[1mStep[0m  [56/84], [94mLoss[0m : 1.64027
[1mStep[0m  [64/84], [94mLoss[0m : 1.42750
[1mStep[0m  [72/84], [94mLoss[0m : 1.84465
[1mStep[0m  [80/84], [94mLoss[0m : 1.49760

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66365
[1mStep[0m  [8/84], [94mLoss[0m : 1.35480
[1mStep[0m  [16/84], [94mLoss[0m : 1.30258
[1mStep[0m  [24/84], [94mLoss[0m : 1.33676
[1mStep[0m  [32/84], [94mLoss[0m : 1.40350
[1mStep[0m  [40/84], [94mLoss[0m : 1.35212
[1mStep[0m  [48/84], [94mLoss[0m : 1.52683
[1mStep[0m  [56/84], [94mLoss[0m : 1.50754
[1mStep[0m  [64/84], [94mLoss[0m : 1.59256
[1mStep[0m  [72/84], [94mLoss[0m : 1.82405
[1mStep[0m  [80/84], [94mLoss[0m : 1.57904

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.550, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68465
[1mStep[0m  [8/84], [94mLoss[0m : 1.54985
[1mStep[0m  [16/84], [94mLoss[0m : 1.41034
[1mStep[0m  [24/84], [94mLoss[0m : 1.24028
[1mStep[0m  [32/84], [94mLoss[0m : 1.34728
[1mStep[0m  [40/84], [94mLoss[0m : 1.44146
[1mStep[0m  [48/84], [94mLoss[0m : 1.60807
[1mStep[0m  [56/84], [94mLoss[0m : 1.40476
[1mStep[0m  [64/84], [94mLoss[0m : 1.27404
[1mStep[0m  [72/84], [94mLoss[0m : 1.48685
[1mStep[0m  [80/84], [94mLoss[0m : 1.42296

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.409, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.519
====================================

Phase 2 - Evaluation MAE:  2.518939963408879
MAE score P1      2.322973
MAE score P2       2.51894
loss              1.409198
learning_rate         0.01
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.54849
[1mStep[0m  [8/84], [94mLoss[0m : 10.60306
[1mStep[0m  [16/84], [94mLoss[0m : 10.45811
[1mStep[0m  [24/84], [94mLoss[0m : 10.68042
[1mStep[0m  [32/84], [94mLoss[0m : 10.27128
[1mStep[0m  [40/84], [94mLoss[0m : 9.80920
[1mStep[0m  [48/84], [94mLoss[0m : 10.11729
[1mStep[0m  [56/84], [94mLoss[0m : 9.88204
[1mStep[0m  [64/84], [94mLoss[0m : 10.16860
[1mStep[0m  [72/84], [94mLoss[0m : 10.22830
[1mStep[0m  [80/84], [94mLoss[0m : 10.23722

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.302, [92mTest[0m: 10.807, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.86610
[1mStep[0m  [8/84], [94mLoss[0m : 9.54130
[1mStep[0m  [16/84], [94mLoss[0m : 9.47136
[1mStep[0m  [24/84], [94mLoss[0m : 9.52525
[1mStep[0m  [32/84], [94mLoss[0m : 8.92400
[1mStep[0m  [40/84], [94mLoss[0m : 8.84465
[1mStep[0m  [48/84], [94mLoss[0m : 8.87021
[1mStep[0m  [56/84], [94mLoss[0m : 8.31511
[1mStep[0m  [64/84], [94mLoss[0m : 8.83995
[1mStep[0m  [72/84], [94mLoss[0m : 8.42623
[1mStep[0m  [80/84], [94mLoss[0m : 7.87989

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.907, [92mTest[0m: 9.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.94625
[1mStep[0m  [8/84], [94mLoss[0m : 7.65261
[1mStep[0m  [16/84], [94mLoss[0m : 7.35866
[1mStep[0m  [24/84], [94mLoss[0m : 7.80516
[1mStep[0m  [32/84], [94mLoss[0m : 7.64654
[1mStep[0m  [40/84], [94mLoss[0m : 6.82380
[1mStep[0m  [48/84], [94mLoss[0m : 7.10243
[1mStep[0m  [56/84], [94mLoss[0m : 5.84022
[1mStep[0m  [64/84], [94mLoss[0m : 6.07195
[1mStep[0m  [72/84], [94mLoss[0m : 6.50750
[1mStep[0m  [80/84], [94mLoss[0m : 5.67301

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.745, [92mTest[0m: 7.575, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.87866
[1mStep[0m  [8/84], [94mLoss[0m : 5.43704
[1mStep[0m  [16/84], [94mLoss[0m : 5.32316
[1mStep[0m  [24/84], [94mLoss[0m : 5.21440
[1mStep[0m  [32/84], [94mLoss[0m : 5.07962
[1mStep[0m  [40/84], [94mLoss[0m : 5.00969
[1mStep[0m  [48/84], [94mLoss[0m : 3.98319
[1mStep[0m  [56/84], [94mLoss[0m : 4.48067
[1mStep[0m  [64/84], [94mLoss[0m : 4.10404
[1mStep[0m  [72/84], [94mLoss[0m : 3.88705
[1mStep[0m  [80/84], [94mLoss[0m : 3.51663

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.467, [92mTest[0m: 4.874, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.77702
[1mStep[0m  [8/84], [94mLoss[0m : 3.34158
[1mStep[0m  [16/84], [94mLoss[0m : 3.26426
[1mStep[0m  [24/84], [94mLoss[0m : 3.30163
[1mStep[0m  [32/84], [94mLoss[0m : 3.35672
[1mStep[0m  [40/84], [94mLoss[0m : 3.00560
[1mStep[0m  [48/84], [94mLoss[0m : 2.87502
[1mStep[0m  [56/84], [94mLoss[0m : 2.80035
[1mStep[0m  [64/84], [94mLoss[0m : 2.68999
[1mStep[0m  [72/84], [94mLoss[0m : 2.82890
[1mStep[0m  [80/84], [94mLoss[0m : 2.85282

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.980, [92mTest[0m: 2.960, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93071
[1mStep[0m  [8/84], [94mLoss[0m : 2.67694
[1mStep[0m  [16/84], [94mLoss[0m : 2.47246
[1mStep[0m  [24/84], [94mLoss[0m : 2.74057
[1mStep[0m  [32/84], [94mLoss[0m : 2.88451
[1mStep[0m  [40/84], [94mLoss[0m : 2.51306
[1mStep[0m  [48/84], [94mLoss[0m : 2.67349
[1mStep[0m  [56/84], [94mLoss[0m : 2.51037
[1mStep[0m  [64/84], [94mLoss[0m : 2.62976
[1mStep[0m  [72/84], [94mLoss[0m : 2.71129
[1mStep[0m  [80/84], [94mLoss[0m : 2.64443

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84347
[1mStep[0m  [8/84], [94mLoss[0m : 2.86223
[1mStep[0m  [16/84], [94mLoss[0m : 2.64486
[1mStep[0m  [24/84], [94mLoss[0m : 2.43661
[1mStep[0m  [32/84], [94mLoss[0m : 2.75538
[1mStep[0m  [40/84], [94mLoss[0m : 2.49742
[1mStep[0m  [48/84], [94mLoss[0m : 2.58558
[1mStep[0m  [56/84], [94mLoss[0m : 2.49865
[1mStep[0m  [64/84], [94mLoss[0m : 2.25367
[1mStep[0m  [72/84], [94mLoss[0m : 2.47420
[1mStep[0m  [80/84], [94mLoss[0m : 2.60045

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41512
[1mStep[0m  [8/84], [94mLoss[0m : 2.66889
[1mStep[0m  [16/84], [94mLoss[0m : 2.62766
[1mStep[0m  [24/84], [94mLoss[0m : 2.41999
[1mStep[0m  [32/84], [94mLoss[0m : 2.53077
[1mStep[0m  [40/84], [94mLoss[0m : 2.68441
[1mStep[0m  [48/84], [94mLoss[0m : 3.08280
[1mStep[0m  [56/84], [94mLoss[0m : 2.32085
[1mStep[0m  [64/84], [94mLoss[0m : 2.44523
[1mStep[0m  [72/84], [94mLoss[0m : 2.77015
[1mStep[0m  [80/84], [94mLoss[0m : 2.85834

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60389
[1mStep[0m  [8/84], [94mLoss[0m : 2.49577
[1mStep[0m  [16/84], [94mLoss[0m : 2.39740
[1mStep[0m  [24/84], [94mLoss[0m : 2.71441
[1mStep[0m  [32/84], [94mLoss[0m : 2.54081
[1mStep[0m  [40/84], [94mLoss[0m : 2.75445
[1mStep[0m  [48/84], [94mLoss[0m : 2.86422
[1mStep[0m  [56/84], [94mLoss[0m : 2.77260
[1mStep[0m  [64/84], [94mLoss[0m : 2.44184
[1mStep[0m  [72/84], [94mLoss[0m : 2.50718
[1mStep[0m  [80/84], [94mLoss[0m : 2.63779

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61582
[1mStep[0m  [8/84], [94mLoss[0m : 2.72343
[1mStep[0m  [16/84], [94mLoss[0m : 2.50225
[1mStep[0m  [24/84], [94mLoss[0m : 2.64301
[1mStep[0m  [32/84], [94mLoss[0m : 2.66859
[1mStep[0m  [40/84], [94mLoss[0m : 2.53038
[1mStep[0m  [48/84], [94mLoss[0m : 2.70181
[1mStep[0m  [56/84], [94mLoss[0m : 2.61845
[1mStep[0m  [64/84], [94mLoss[0m : 2.22254
[1mStep[0m  [72/84], [94mLoss[0m : 2.78273
[1mStep[0m  [80/84], [94mLoss[0m : 2.46323

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78735
[1mStep[0m  [8/84], [94mLoss[0m : 2.57608
[1mStep[0m  [16/84], [94mLoss[0m : 2.29299
[1mStep[0m  [24/84], [94mLoss[0m : 2.52515
[1mStep[0m  [32/84], [94mLoss[0m : 2.62229
[1mStep[0m  [40/84], [94mLoss[0m : 2.23701
[1mStep[0m  [48/84], [94mLoss[0m : 2.52762
[1mStep[0m  [56/84], [94mLoss[0m : 2.66526
[1mStep[0m  [64/84], [94mLoss[0m : 2.49137
[1mStep[0m  [72/84], [94mLoss[0m : 2.65027
[1mStep[0m  [80/84], [94mLoss[0m : 2.61686

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40917
[1mStep[0m  [8/84], [94mLoss[0m : 2.14726
[1mStep[0m  [16/84], [94mLoss[0m : 2.27312
[1mStep[0m  [24/84], [94mLoss[0m : 2.60168
[1mStep[0m  [32/84], [94mLoss[0m : 2.85953
[1mStep[0m  [40/84], [94mLoss[0m : 2.73601
[1mStep[0m  [48/84], [94mLoss[0m : 2.77052
[1mStep[0m  [56/84], [94mLoss[0m : 2.98164
[1mStep[0m  [64/84], [94mLoss[0m : 2.41394
[1mStep[0m  [72/84], [94mLoss[0m : 2.62485
[1mStep[0m  [80/84], [94mLoss[0m : 2.75092

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47692
[1mStep[0m  [8/84], [94mLoss[0m : 2.29384
[1mStep[0m  [16/84], [94mLoss[0m : 2.58156
[1mStep[0m  [24/84], [94mLoss[0m : 2.45924
[1mStep[0m  [32/84], [94mLoss[0m : 2.78964
[1mStep[0m  [40/84], [94mLoss[0m : 2.30053
[1mStep[0m  [48/84], [94mLoss[0m : 2.30324
[1mStep[0m  [56/84], [94mLoss[0m : 2.55284
[1mStep[0m  [64/84], [94mLoss[0m : 2.30050
[1mStep[0m  [72/84], [94mLoss[0m : 2.44941
[1mStep[0m  [80/84], [94mLoss[0m : 2.17898

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78718
[1mStep[0m  [8/84], [94mLoss[0m : 2.18988
[1mStep[0m  [16/84], [94mLoss[0m : 2.69944
[1mStep[0m  [24/84], [94mLoss[0m : 2.60220
[1mStep[0m  [32/84], [94mLoss[0m : 2.13827
[1mStep[0m  [40/84], [94mLoss[0m : 2.42064
[1mStep[0m  [48/84], [94mLoss[0m : 2.65746
[1mStep[0m  [56/84], [94mLoss[0m : 2.64598
[1mStep[0m  [64/84], [94mLoss[0m : 2.30239
[1mStep[0m  [72/84], [94mLoss[0m : 2.47230
[1mStep[0m  [80/84], [94mLoss[0m : 2.40121

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27893
[1mStep[0m  [8/84], [94mLoss[0m : 2.84859
[1mStep[0m  [16/84], [94mLoss[0m : 2.41735
[1mStep[0m  [24/84], [94mLoss[0m : 2.45570
[1mStep[0m  [32/84], [94mLoss[0m : 2.27164
[1mStep[0m  [40/84], [94mLoss[0m : 2.51054
[1mStep[0m  [48/84], [94mLoss[0m : 2.72299
[1mStep[0m  [56/84], [94mLoss[0m : 2.56171
[1mStep[0m  [64/84], [94mLoss[0m : 2.39694
[1mStep[0m  [72/84], [94mLoss[0m : 2.37863
[1mStep[0m  [80/84], [94mLoss[0m : 2.32404

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68030
[1mStep[0m  [8/84], [94mLoss[0m : 2.27240
[1mStep[0m  [16/84], [94mLoss[0m : 2.41465
[1mStep[0m  [24/84], [94mLoss[0m : 2.56475
[1mStep[0m  [32/84], [94mLoss[0m : 2.44504
[1mStep[0m  [40/84], [94mLoss[0m : 2.42158
[1mStep[0m  [48/84], [94mLoss[0m : 2.67811
[1mStep[0m  [56/84], [94mLoss[0m : 2.41395
[1mStep[0m  [64/84], [94mLoss[0m : 2.56744
[1mStep[0m  [72/84], [94mLoss[0m : 2.53415
[1mStep[0m  [80/84], [94mLoss[0m : 2.61019

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60048
[1mStep[0m  [8/84], [94mLoss[0m : 2.37215
[1mStep[0m  [16/84], [94mLoss[0m : 2.51331
[1mStep[0m  [24/84], [94mLoss[0m : 2.18807
[1mStep[0m  [32/84], [94mLoss[0m : 2.53669
[1mStep[0m  [40/84], [94mLoss[0m : 2.34604
[1mStep[0m  [48/84], [94mLoss[0m : 2.46133
[1mStep[0m  [56/84], [94mLoss[0m : 2.46177
[1mStep[0m  [64/84], [94mLoss[0m : 2.67988
[1mStep[0m  [72/84], [94mLoss[0m : 2.54738
[1mStep[0m  [80/84], [94mLoss[0m : 2.62399

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44334
[1mStep[0m  [8/84], [94mLoss[0m : 2.48633
[1mStep[0m  [16/84], [94mLoss[0m : 2.37369
[1mStep[0m  [24/84], [94mLoss[0m : 2.53635
[1mStep[0m  [32/84], [94mLoss[0m : 2.49943
[1mStep[0m  [40/84], [94mLoss[0m : 2.62488
[1mStep[0m  [48/84], [94mLoss[0m : 2.64326
[1mStep[0m  [56/84], [94mLoss[0m : 2.39166
[1mStep[0m  [64/84], [94mLoss[0m : 2.40830
[1mStep[0m  [72/84], [94mLoss[0m : 2.54085
[1mStep[0m  [80/84], [94mLoss[0m : 2.32889

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65097
[1mStep[0m  [8/84], [94mLoss[0m : 2.34783
[1mStep[0m  [16/84], [94mLoss[0m : 2.67721
[1mStep[0m  [24/84], [94mLoss[0m : 2.42170
[1mStep[0m  [32/84], [94mLoss[0m : 2.61750
[1mStep[0m  [40/84], [94mLoss[0m : 2.63689
[1mStep[0m  [48/84], [94mLoss[0m : 2.52147
[1mStep[0m  [56/84], [94mLoss[0m : 2.18311
[1mStep[0m  [64/84], [94mLoss[0m : 2.66129
[1mStep[0m  [72/84], [94mLoss[0m : 2.62129
[1mStep[0m  [80/84], [94mLoss[0m : 2.48621

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44585
[1mStep[0m  [8/84], [94mLoss[0m : 2.47685
[1mStep[0m  [16/84], [94mLoss[0m : 2.77056
[1mStep[0m  [24/84], [94mLoss[0m : 2.50474
[1mStep[0m  [32/84], [94mLoss[0m : 2.58100
[1mStep[0m  [40/84], [94mLoss[0m : 2.43306
[1mStep[0m  [48/84], [94mLoss[0m : 2.41108
[1mStep[0m  [56/84], [94mLoss[0m : 2.29718
[1mStep[0m  [64/84], [94mLoss[0m : 2.54913
[1mStep[0m  [72/84], [94mLoss[0m : 2.18801
[1mStep[0m  [80/84], [94mLoss[0m : 2.66432

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58508
[1mStep[0m  [8/84], [94mLoss[0m : 2.42749
[1mStep[0m  [16/84], [94mLoss[0m : 2.80875
[1mStep[0m  [24/84], [94mLoss[0m : 2.59485
[1mStep[0m  [32/84], [94mLoss[0m : 2.38837
[1mStep[0m  [40/84], [94mLoss[0m : 2.63377
[1mStep[0m  [48/84], [94mLoss[0m : 2.70279
[1mStep[0m  [56/84], [94mLoss[0m : 2.88611
[1mStep[0m  [64/84], [94mLoss[0m : 2.50219
[1mStep[0m  [72/84], [94mLoss[0m : 2.45481
[1mStep[0m  [80/84], [94mLoss[0m : 2.49206

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.311, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44766
[1mStep[0m  [8/84], [94mLoss[0m : 2.36578
[1mStep[0m  [16/84], [94mLoss[0m : 2.51509
[1mStep[0m  [24/84], [94mLoss[0m : 2.68622
[1mStep[0m  [32/84], [94mLoss[0m : 2.59962
[1mStep[0m  [40/84], [94mLoss[0m : 2.17079
[1mStep[0m  [48/84], [94mLoss[0m : 2.56174
[1mStep[0m  [56/84], [94mLoss[0m : 2.32974
[1mStep[0m  [64/84], [94mLoss[0m : 2.53594
[1mStep[0m  [72/84], [94mLoss[0m : 2.22836
[1mStep[0m  [80/84], [94mLoss[0m : 2.51638

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45811
[1mStep[0m  [8/84], [94mLoss[0m : 1.96473
[1mStep[0m  [16/84], [94mLoss[0m : 2.28027
[1mStep[0m  [24/84], [94mLoss[0m : 2.27610
[1mStep[0m  [32/84], [94mLoss[0m : 2.52241
[1mStep[0m  [40/84], [94mLoss[0m : 2.46435
[1mStep[0m  [48/84], [94mLoss[0m : 2.43134
[1mStep[0m  [56/84], [94mLoss[0m : 2.40998
[1mStep[0m  [64/84], [94mLoss[0m : 2.53241
[1mStep[0m  [72/84], [94mLoss[0m : 2.57109
[1mStep[0m  [80/84], [94mLoss[0m : 2.49373

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46795
[1mStep[0m  [8/84], [94mLoss[0m : 2.30498
[1mStep[0m  [16/84], [94mLoss[0m : 2.48571
[1mStep[0m  [24/84], [94mLoss[0m : 2.45433
[1mStep[0m  [32/84], [94mLoss[0m : 2.38646
[1mStep[0m  [40/84], [94mLoss[0m : 2.72352
[1mStep[0m  [48/84], [94mLoss[0m : 2.34310
[1mStep[0m  [56/84], [94mLoss[0m : 2.30318
[1mStep[0m  [64/84], [94mLoss[0m : 2.26689
[1mStep[0m  [72/84], [94mLoss[0m : 2.41513
[1mStep[0m  [80/84], [94mLoss[0m : 2.71006

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58237
[1mStep[0m  [8/84], [94mLoss[0m : 2.17848
[1mStep[0m  [16/84], [94mLoss[0m : 2.65167
[1mStep[0m  [24/84], [94mLoss[0m : 2.51323
[1mStep[0m  [32/84], [94mLoss[0m : 2.51000
[1mStep[0m  [40/84], [94mLoss[0m : 2.41774
[1mStep[0m  [48/84], [94mLoss[0m : 2.41076
[1mStep[0m  [56/84], [94mLoss[0m : 2.96891
[1mStep[0m  [64/84], [94mLoss[0m : 2.49956
[1mStep[0m  [72/84], [94mLoss[0m : 1.99739
[1mStep[0m  [80/84], [94mLoss[0m : 2.63184

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32385
[1mStep[0m  [8/84], [94mLoss[0m : 2.63551
[1mStep[0m  [16/84], [94mLoss[0m : 2.61355
[1mStep[0m  [24/84], [94mLoss[0m : 2.38651
[1mStep[0m  [32/84], [94mLoss[0m : 2.67444
[1mStep[0m  [40/84], [94mLoss[0m : 2.30842
[1mStep[0m  [48/84], [94mLoss[0m : 2.48373
[1mStep[0m  [56/84], [94mLoss[0m : 2.62382
[1mStep[0m  [64/84], [94mLoss[0m : 2.38153
[1mStep[0m  [72/84], [94mLoss[0m : 2.47988
[1mStep[0m  [80/84], [94mLoss[0m : 2.67932

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58797
[1mStep[0m  [8/84], [94mLoss[0m : 2.31282
[1mStep[0m  [16/84], [94mLoss[0m : 2.70285
[1mStep[0m  [24/84], [94mLoss[0m : 2.18423
[1mStep[0m  [32/84], [94mLoss[0m : 2.13693
[1mStep[0m  [40/84], [94mLoss[0m : 2.34516
[1mStep[0m  [48/84], [94mLoss[0m : 2.41221
[1mStep[0m  [56/84], [94mLoss[0m : 2.50750
[1mStep[0m  [64/84], [94mLoss[0m : 2.22688
[1mStep[0m  [72/84], [94mLoss[0m : 2.23037
[1mStep[0m  [80/84], [94mLoss[0m : 2.19254

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40908
[1mStep[0m  [8/84], [94mLoss[0m : 2.40358
[1mStep[0m  [16/84], [94mLoss[0m : 2.44469
[1mStep[0m  [24/84], [94mLoss[0m : 2.34435
[1mStep[0m  [32/84], [94mLoss[0m : 2.47295
[1mStep[0m  [40/84], [94mLoss[0m : 2.45891
[1mStep[0m  [48/84], [94mLoss[0m : 2.55564
[1mStep[0m  [56/84], [94mLoss[0m : 2.30273
[1mStep[0m  [64/84], [94mLoss[0m : 2.29953
[1mStep[0m  [72/84], [94mLoss[0m : 2.52730
[1mStep[0m  [80/84], [94mLoss[0m : 2.40714

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23064
[1mStep[0m  [8/84], [94mLoss[0m : 2.61381
[1mStep[0m  [16/84], [94mLoss[0m : 2.29242
[1mStep[0m  [24/84], [94mLoss[0m : 2.75230
[1mStep[0m  [32/84], [94mLoss[0m : 2.61745
[1mStep[0m  [40/84], [94mLoss[0m : 2.06193
[1mStep[0m  [48/84], [94mLoss[0m : 2.47322
[1mStep[0m  [56/84], [94mLoss[0m : 2.36551
[1mStep[0m  [64/84], [94mLoss[0m : 2.67538
[1mStep[0m  [72/84], [94mLoss[0m : 2.54110
[1mStep[0m  [80/84], [94mLoss[0m : 2.24835

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56773
[1mStep[0m  [8/84], [94mLoss[0m : 2.13251
[1mStep[0m  [16/84], [94mLoss[0m : 2.06205
[1mStep[0m  [24/84], [94mLoss[0m : 2.64757
[1mStep[0m  [32/84], [94mLoss[0m : 2.41239
[1mStep[0m  [40/84], [94mLoss[0m : 2.34115
[1mStep[0m  [48/84], [94mLoss[0m : 2.42649
[1mStep[0m  [56/84], [94mLoss[0m : 2.66172
[1mStep[0m  [64/84], [94mLoss[0m : 2.47904
[1mStep[0m  [72/84], [94mLoss[0m : 2.60706
[1mStep[0m  [80/84], [94mLoss[0m : 2.54537

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.328
====================================

Phase 1 - Evaluation MAE:  2.328452851091112
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 1.96554
[1mStep[0m  [8/84], [94mLoss[0m : 2.76760
[1mStep[0m  [16/84], [94mLoss[0m : 2.55721
[1mStep[0m  [24/84], [94mLoss[0m : 2.28323
[1mStep[0m  [32/84], [94mLoss[0m : 2.31159
[1mStep[0m  [40/84], [94mLoss[0m : 2.71584
[1mStep[0m  [48/84], [94mLoss[0m : 2.58457
[1mStep[0m  [56/84], [94mLoss[0m : 2.38590
[1mStep[0m  [64/84], [94mLoss[0m : 2.50658
[1mStep[0m  [72/84], [94mLoss[0m : 2.70475
[1mStep[0m  [80/84], [94mLoss[0m : 2.38472

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34558
[1mStep[0m  [8/84], [94mLoss[0m : 2.65288
[1mStep[0m  [16/84], [94mLoss[0m : 2.54792
[1mStep[0m  [24/84], [94mLoss[0m : 2.52448
[1mStep[0m  [32/84], [94mLoss[0m : 2.17791
[1mStep[0m  [40/84], [94mLoss[0m : 2.28724
[1mStep[0m  [48/84], [94mLoss[0m : 2.30760
[1mStep[0m  [56/84], [94mLoss[0m : 2.56857
[1mStep[0m  [64/84], [94mLoss[0m : 2.24885
[1mStep[0m  [72/84], [94mLoss[0m : 2.27070
[1mStep[0m  [80/84], [94mLoss[0m : 2.45775

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31416
[1mStep[0m  [8/84], [94mLoss[0m : 2.27207
[1mStep[0m  [16/84], [94mLoss[0m : 2.54049
[1mStep[0m  [24/84], [94mLoss[0m : 2.23014
[1mStep[0m  [32/84], [94mLoss[0m : 2.47970
[1mStep[0m  [40/84], [94mLoss[0m : 2.52094
[1mStep[0m  [48/84], [94mLoss[0m : 2.33218
[1mStep[0m  [56/84], [94mLoss[0m : 2.39742
[1mStep[0m  [64/84], [94mLoss[0m : 2.48345
[1mStep[0m  [72/84], [94mLoss[0m : 2.56786
[1mStep[0m  [80/84], [94mLoss[0m : 2.38631

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49074
[1mStep[0m  [8/84], [94mLoss[0m : 2.26789
[1mStep[0m  [16/84], [94mLoss[0m : 2.58897
[1mStep[0m  [24/84], [94mLoss[0m : 2.53735
[1mStep[0m  [32/84], [94mLoss[0m : 2.36870
[1mStep[0m  [40/84], [94mLoss[0m : 2.36171
[1mStep[0m  [48/84], [94mLoss[0m : 2.16325
[1mStep[0m  [56/84], [94mLoss[0m : 2.54537
[1mStep[0m  [64/84], [94mLoss[0m : 2.39376
[1mStep[0m  [72/84], [94mLoss[0m : 2.47365
[1mStep[0m  [80/84], [94mLoss[0m : 2.33326

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63634
[1mStep[0m  [8/84], [94mLoss[0m : 2.23180
[1mStep[0m  [16/84], [94mLoss[0m : 2.19834
[1mStep[0m  [24/84], [94mLoss[0m : 2.23424
[1mStep[0m  [32/84], [94mLoss[0m : 2.35828
[1mStep[0m  [40/84], [94mLoss[0m : 1.80448
[1mStep[0m  [48/84], [94mLoss[0m : 2.42558
[1mStep[0m  [56/84], [94mLoss[0m : 1.99449
[1mStep[0m  [64/84], [94mLoss[0m : 1.99456
[1mStep[0m  [72/84], [94mLoss[0m : 2.23015
[1mStep[0m  [80/84], [94mLoss[0m : 2.33887

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07902
[1mStep[0m  [8/84], [94mLoss[0m : 2.60222
[1mStep[0m  [16/84], [94mLoss[0m : 2.21876
[1mStep[0m  [24/84], [94mLoss[0m : 2.15192
[1mStep[0m  [32/84], [94mLoss[0m : 2.24652
[1mStep[0m  [40/84], [94mLoss[0m : 2.24669
[1mStep[0m  [48/84], [94mLoss[0m : 2.44689
[1mStep[0m  [56/84], [94mLoss[0m : 2.25407
[1mStep[0m  [64/84], [94mLoss[0m : 2.42161
[1mStep[0m  [72/84], [94mLoss[0m : 2.12022
[1mStep[0m  [80/84], [94mLoss[0m : 2.37059

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31133
[1mStep[0m  [8/84], [94mLoss[0m : 2.03340
[1mStep[0m  [16/84], [94mLoss[0m : 2.43997
[1mStep[0m  [24/84], [94mLoss[0m : 2.26862
[1mStep[0m  [32/84], [94mLoss[0m : 2.00629
[1mStep[0m  [40/84], [94mLoss[0m : 2.29290
[1mStep[0m  [48/84], [94mLoss[0m : 2.31184
[1mStep[0m  [56/84], [94mLoss[0m : 1.92325
[1mStep[0m  [64/84], [94mLoss[0m : 2.12358
[1mStep[0m  [72/84], [94mLoss[0m : 2.09408
[1mStep[0m  [80/84], [94mLoss[0m : 2.25618

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98579
[1mStep[0m  [8/84], [94mLoss[0m : 2.03122
[1mStep[0m  [16/84], [94mLoss[0m : 2.20559
[1mStep[0m  [24/84], [94mLoss[0m : 2.07151
[1mStep[0m  [32/84], [94mLoss[0m : 2.13489
[1mStep[0m  [40/84], [94mLoss[0m : 2.13368
[1mStep[0m  [48/84], [94mLoss[0m : 2.19856
[1mStep[0m  [56/84], [94mLoss[0m : 2.14232
[1mStep[0m  [64/84], [94mLoss[0m : 2.00356
[1mStep[0m  [72/84], [94mLoss[0m : 2.15053
[1mStep[0m  [80/84], [94mLoss[0m : 2.30113

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98030
[1mStep[0m  [8/84], [94mLoss[0m : 2.13757
[1mStep[0m  [16/84], [94mLoss[0m : 1.98373
[1mStep[0m  [24/84], [94mLoss[0m : 2.09101
[1mStep[0m  [32/84], [94mLoss[0m : 1.76826
[1mStep[0m  [40/84], [94mLoss[0m : 2.18349
[1mStep[0m  [48/84], [94mLoss[0m : 1.88086
[1mStep[0m  [56/84], [94mLoss[0m : 2.13472
[1mStep[0m  [64/84], [94mLoss[0m : 1.72077
[1mStep[0m  [72/84], [94mLoss[0m : 1.92466
[1mStep[0m  [80/84], [94mLoss[0m : 2.08431

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86574
[1mStep[0m  [8/84], [94mLoss[0m : 1.87565
[1mStep[0m  [16/84], [94mLoss[0m : 2.14995
[1mStep[0m  [24/84], [94mLoss[0m : 2.11804
[1mStep[0m  [32/84], [94mLoss[0m : 2.22143
[1mStep[0m  [40/84], [94mLoss[0m : 2.13431
[1mStep[0m  [48/84], [94mLoss[0m : 1.86878
[1mStep[0m  [56/84], [94mLoss[0m : 2.13411
[1mStep[0m  [64/84], [94mLoss[0m : 2.17671
[1mStep[0m  [72/84], [94mLoss[0m : 1.78287
[1mStep[0m  [80/84], [94mLoss[0m : 1.98245

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75297
[1mStep[0m  [8/84], [94mLoss[0m : 2.20806
[1mStep[0m  [16/84], [94mLoss[0m : 2.05118
[1mStep[0m  [24/84], [94mLoss[0m : 2.04000
[1mStep[0m  [32/84], [94mLoss[0m : 1.82817
[1mStep[0m  [40/84], [94mLoss[0m : 1.82406
[1mStep[0m  [48/84], [94mLoss[0m : 2.07956
[1mStep[0m  [56/84], [94mLoss[0m : 1.88609
[1mStep[0m  [64/84], [94mLoss[0m : 1.92080
[1mStep[0m  [72/84], [94mLoss[0m : 1.94264
[1mStep[0m  [80/84], [94mLoss[0m : 1.93332

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95282
[1mStep[0m  [8/84], [94mLoss[0m : 1.79152
[1mStep[0m  [16/84], [94mLoss[0m : 1.93779
[1mStep[0m  [24/84], [94mLoss[0m : 1.68465
[1mStep[0m  [32/84], [94mLoss[0m : 2.07147
[1mStep[0m  [40/84], [94mLoss[0m : 1.66004
[1mStep[0m  [48/84], [94mLoss[0m : 1.96002
[1mStep[0m  [56/84], [94mLoss[0m : 1.82204
[1mStep[0m  [64/84], [94mLoss[0m : 1.95019
[1mStep[0m  [72/84], [94mLoss[0m : 1.84477
[1mStep[0m  [80/84], [94mLoss[0m : 2.09071

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81273
[1mStep[0m  [8/84], [94mLoss[0m : 2.17806
[1mStep[0m  [16/84], [94mLoss[0m : 1.92290
[1mStep[0m  [24/84], [94mLoss[0m : 1.87774
[1mStep[0m  [32/84], [94mLoss[0m : 1.81673
[1mStep[0m  [40/84], [94mLoss[0m : 1.56599
[1mStep[0m  [48/84], [94mLoss[0m : 1.79815
[1mStep[0m  [56/84], [94mLoss[0m : 1.65637
[1mStep[0m  [64/84], [94mLoss[0m : 1.97758
[1mStep[0m  [72/84], [94mLoss[0m : 2.11072
[1mStep[0m  [80/84], [94mLoss[0m : 1.93577

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72786
[1mStep[0m  [8/84], [94mLoss[0m : 1.66174
[1mStep[0m  [16/84], [94mLoss[0m : 1.94792
[1mStep[0m  [24/84], [94mLoss[0m : 1.64437
[1mStep[0m  [32/84], [94mLoss[0m : 1.79264
[1mStep[0m  [40/84], [94mLoss[0m : 1.81849
[1mStep[0m  [48/84], [94mLoss[0m : 1.87620
[1mStep[0m  [56/84], [94mLoss[0m : 1.81522
[1mStep[0m  [64/84], [94mLoss[0m : 1.95965
[1mStep[0m  [72/84], [94mLoss[0m : 1.84946
[1mStep[0m  [80/84], [94mLoss[0m : 1.82863

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85024
[1mStep[0m  [8/84], [94mLoss[0m : 1.83934
[1mStep[0m  [16/84], [94mLoss[0m : 1.67157
[1mStep[0m  [24/84], [94mLoss[0m : 2.06204
[1mStep[0m  [32/84], [94mLoss[0m : 1.80212
[1mStep[0m  [40/84], [94mLoss[0m : 2.02782
[1mStep[0m  [48/84], [94mLoss[0m : 1.70265
[1mStep[0m  [56/84], [94mLoss[0m : 2.14247
[1mStep[0m  [64/84], [94mLoss[0m : 1.97412
[1mStep[0m  [72/84], [94mLoss[0m : 1.80014
[1mStep[0m  [80/84], [94mLoss[0m : 2.02014

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53332
[1mStep[0m  [8/84], [94mLoss[0m : 1.70973
[1mStep[0m  [16/84], [94mLoss[0m : 1.60605
[1mStep[0m  [24/84], [94mLoss[0m : 1.68570
[1mStep[0m  [32/84], [94mLoss[0m : 1.59389
[1mStep[0m  [40/84], [94mLoss[0m : 1.80411
[1mStep[0m  [48/84], [94mLoss[0m : 2.03515
[1mStep[0m  [56/84], [94mLoss[0m : 1.80394
[1mStep[0m  [64/84], [94mLoss[0m : 1.60490
[1mStep[0m  [72/84], [94mLoss[0m : 1.85577
[1mStep[0m  [80/84], [94mLoss[0m : 1.99454

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91017
[1mStep[0m  [8/84], [94mLoss[0m : 1.77517
[1mStep[0m  [16/84], [94mLoss[0m : 1.56284
[1mStep[0m  [24/84], [94mLoss[0m : 1.70852
[1mStep[0m  [32/84], [94mLoss[0m : 1.90618
[1mStep[0m  [40/84], [94mLoss[0m : 1.85865
[1mStep[0m  [48/84], [94mLoss[0m : 1.66667
[1mStep[0m  [56/84], [94mLoss[0m : 1.58583
[1mStep[0m  [64/84], [94mLoss[0m : 1.81829
[1mStep[0m  [72/84], [94mLoss[0m : 1.77222
[1mStep[0m  [80/84], [94mLoss[0m : 1.73791

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79963
[1mStep[0m  [8/84], [94mLoss[0m : 1.26320
[1mStep[0m  [16/84], [94mLoss[0m : 1.65230
[1mStep[0m  [24/84], [94mLoss[0m : 1.47026
[1mStep[0m  [32/84], [94mLoss[0m : 1.94145
[1mStep[0m  [40/84], [94mLoss[0m : 1.69765
[1mStep[0m  [48/84], [94mLoss[0m : 1.78856
[1mStep[0m  [56/84], [94mLoss[0m : 1.85034
[1mStep[0m  [64/84], [94mLoss[0m : 1.58937
[1mStep[0m  [72/84], [94mLoss[0m : 1.57574
[1mStep[0m  [80/84], [94mLoss[0m : 1.53196

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65416
[1mStep[0m  [8/84], [94mLoss[0m : 1.36774
[1mStep[0m  [16/84], [94mLoss[0m : 1.65543
[1mStep[0m  [24/84], [94mLoss[0m : 1.52384
[1mStep[0m  [32/84], [94mLoss[0m : 1.57701
[1mStep[0m  [40/84], [94mLoss[0m : 1.71897
[1mStep[0m  [48/84], [94mLoss[0m : 1.63723
[1mStep[0m  [56/84], [94mLoss[0m : 1.58878
[1mStep[0m  [64/84], [94mLoss[0m : 1.76195
[1mStep[0m  [72/84], [94mLoss[0m : 1.76865
[1mStep[0m  [80/84], [94mLoss[0m : 1.61428

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47579
[1mStep[0m  [8/84], [94mLoss[0m : 1.47258
[1mStep[0m  [16/84], [94mLoss[0m : 1.54152
[1mStep[0m  [24/84], [94mLoss[0m : 1.74161
[1mStep[0m  [32/84], [94mLoss[0m : 1.66624
[1mStep[0m  [40/84], [94mLoss[0m : 1.47142
[1mStep[0m  [48/84], [94mLoss[0m : 1.68297
[1mStep[0m  [56/84], [94mLoss[0m : 1.51567
[1mStep[0m  [64/84], [94mLoss[0m : 1.78909
[1mStep[0m  [72/84], [94mLoss[0m : 1.57500
[1mStep[0m  [80/84], [94mLoss[0m : 1.69682

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.646, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33807
[1mStep[0m  [8/84], [94mLoss[0m : 1.78778
[1mStep[0m  [16/84], [94mLoss[0m : 1.80190
[1mStep[0m  [24/84], [94mLoss[0m : 1.66418
[1mStep[0m  [32/84], [94mLoss[0m : 1.53157
[1mStep[0m  [40/84], [94mLoss[0m : 1.71689
[1mStep[0m  [48/84], [94mLoss[0m : 1.61611
[1mStep[0m  [56/84], [94mLoss[0m : 1.40047
[1mStep[0m  [64/84], [94mLoss[0m : 1.47620
[1mStep[0m  [72/84], [94mLoss[0m : 1.86030
[1mStep[0m  [80/84], [94mLoss[0m : 1.50144

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.24768
[1mStep[0m  [8/84], [94mLoss[0m : 1.49957
[1mStep[0m  [16/84], [94mLoss[0m : 1.37716
[1mStep[0m  [24/84], [94mLoss[0m : 1.66724
[1mStep[0m  [32/84], [94mLoss[0m : 1.71485
[1mStep[0m  [40/84], [94mLoss[0m : 1.34431
[1mStep[0m  [48/84], [94mLoss[0m : 1.59275
[1mStep[0m  [56/84], [94mLoss[0m : 1.26202
[1mStep[0m  [64/84], [94mLoss[0m : 1.54542
[1mStep[0m  [72/84], [94mLoss[0m : 1.57537
[1mStep[0m  [80/84], [94mLoss[0m : 1.75163

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.583, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55106
[1mStep[0m  [8/84], [94mLoss[0m : 1.52324
[1mStep[0m  [16/84], [94mLoss[0m : 1.55990
[1mStep[0m  [24/84], [94mLoss[0m : 1.50890
[1mStep[0m  [32/84], [94mLoss[0m : 1.65463
[1mStep[0m  [40/84], [94mLoss[0m : 1.51010
[1mStep[0m  [48/84], [94mLoss[0m : 1.48989
[1mStep[0m  [56/84], [94mLoss[0m : 1.40837
[1mStep[0m  [64/84], [94mLoss[0m : 1.60134
[1mStep[0m  [72/84], [94mLoss[0m : 1.68455
[1mStep[0m  [80/84], [94mLoss[0m : 1.40495

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.558, [92mTest[0m: 2.462, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51331
[1mStep[0m  [8/84], [94mLoss[0m : 1.68562
[1mStep[0m  [16/84], [94mLoss[0m : 1.46518
[1mStep[0m  [24/84], [94mLoss[0m : 1.43297
[1mStep[0m  [32/84], [94mLoss[0m : 1.67787
[1mStep[0m  [40/84], [94mLoss[0m : 1.56072
[1mStep[0m  [48/84], [94mLoss[0m : 1.73490
[1mStep[0m  [56/84], [94mLoss[0m : 1.67333
[1mStep[0m  [64/84], [94mLoss[0m : 1.34653
[1mStep[0m  [72/84], [94mLoss[0m : 1.77529
[1mStep[0m  [80/84], [94mLoss[0m : 1.48927

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.533, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37811
[1mStep[0m  [8/84], [94mLoss[0m : 1.51502
[1mStep[0m  [16/84], [94mLoss[0m : 1.50244
[1mStep[0m  [24/84], [94mLoss[0m : 1.56186
[1mStep[0m  [32/84], [94mLoss[0m : 1.44125
[1mStep[0m  [40/84], [94mLoss[0m : 1.55846
[1mStep[0m  [48/84], [94mLoss[0m : 1.42581
[1mStep[0m  [56/84], [94mLoss[0m : 1.65270
[1mStep[0m  [64/84], [94mLoss[0m : 1.57728
[1mStep[0m  [72/84], [94mLoss[0m : 1.43986
[1mStep[0m  [80/84], [94mLoss[0m : 1.46004

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.481, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57565
[1mStep[0m  [8/84], [94mLoss[0m : 1.27850
[1mStep[0m  [16/84], [94mLoss[0m : 1.57304
[1mStep[0m  [24/84], [94mLoss[0m : 1.62349
[1mStep[0m  [32/84], [94mLoss[0m : 1.78040
[1mStep[0m  [40/84], [94mLoss[0m : 1.40426
[1mStep[0m  [48/84], [94mLoss[0m : 1.49324
[1mStep[0m  [56/84], [94mLoss[0m : 1.57771
[1mStep[0m  [64/84], [94mLoss[0m : 1.40012
[1mStep[0m  [72/84], [94mLoss[0m : 1.55383
[1mStep[0m  [80/84], [94mLoss[0m : 1.58177

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41536
[1mStep[0m  [8/84], [94mLoss[0m : 1.39274
[1mStep[0m  [16/84], [94mLoss[0m : 1.49629
[1mStep[0m  [24/84], [94mLoss[0m : 1.49523
[1mStep[0m  [32/84], [94mLoss[0m : 1.63891
[1mStep[0m  [40/84], [94mLoss[0m : 1.43325
[1mStep[0m  [48/84], [94mLoss[0m : 1.63183
[1mStep[0m  [56/84], [94mLoss[0m : 1.38758
[1mStep[0m  [64/84], [94mLoss[0m : 1.55722
[1mStep[0m  [72/84], [94mLoss[0m : 1.38265
[1mStep[0m  [80/84], [94mLoss[0m : 1.58054

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35213
[1mStep[0m  [8/84], [94mLoss[0m : 1.55516
[1mStep[0m  [16/84], [94mLoss[0m : 1.33744
[1mStep[0m  [24/84], [94mLoss[0m : 1.38184
[1mStep[0m  [32/84], [94mLoss[0m : 1.48627
[1mStep[0m  [40/84], [94mLoss[0m : 1.42543
[1mStep[0m  [48/84], [94mLoss[0m : 1.42861
[1mStep[0m  [56/84], [94mLoss[0m : 1.46008
[1mStep[0m  [64/84], [94mLoss[0m : 1.63265
[1mStep[0m  [72/84], [94mLoss[0m : 1.43984
[1mStep[0m  [80/84], [94mLoss[0m : 1.55698

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.465, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.18992
[1mStep[0m  [8/84], [94mLoss[0m : 1.42667
[1mStep[0m  [16/84], [94mLoss[0m : 1.57424
[1mStep[0m  [24/84], [94mLoss[0m : 1.53053
[1mStep[0m  [32/84], [94mLoss[0m : 1.39434
[1mStep[0m  [40/84], [94mLoss[0m : 1.53216
[1mStep[0m  [48/84], [94mLoss[0m : 1.45209
[1mStep[0m  [56/84], [94mLoss[0m : 1.35089
[1mStep[0m  [64/84], [94mLoss[0m : 1.40894
[1mStep[0m  [72/84], [94mLoss[0m : 1.23961
[1mStep[0m  [80/84], [94mLoss[0m : 1.61476

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.480, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50592
[1mStep[0m  [8/84], [94mLoss[0m : 1.32682
[1mStep[0m  [16/84], [94mLoss[0m : 1.27470
[1mStep[0m  [24/84], [94mLoss[0m : 1.42173
[1mStep[0m  [32/84], [94mLoss[0m : 1.38682
[1mStep[0m  [40/84], [94mLoss[0m : 1.55809
[1mStep[0m  [48/84], [94mLoss[0m : 1.38023
[1mStep[0m  [56/84], [94mLoss[0m : 1.50114
[1mStep[0m  [64/84], [94mLoss[0m : 1.42789
[1mStep[0m  [72/84], [94mLoss[0m : 1.52114
[1mStep[0m  [80/84], [94mLoss[0m : 1.32032

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.428, [92mTest[0m: 2.493, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.512
====================================

Phase 2 - Evaluation MAE:  2.5122998441968645
MAE score P1       2.328453
MAE score P2         2.5123
loss               1.428378
learning_rate          0.01
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.64615
[1mStep[0m  [16/169], [94mLoss[0m : 10.17200
[1mStep[0m  [32/169], [94mLoss[0m : 7.59596
[1mStep[0m  [48/169], [94mLoss[0m : 5.55859
[1mStep[0m  [64/169], [94mLoss[0m : 2.97407
[1mStep[0m  [80/169], [94mLoss[0m : 3.05494
[1mStep[0m  [96/169], [94mLoss[0m : 3.13162
[1mStep[0m  [112/169], [94mLoss[0m : 2.56506
[1mStep[0m  [128/169], [94mLoss[0m : 2.24001
[1mStep[0m  [144/169], [94mLoss[0m : 3.08038
[1mStep[0m  [160/169], [94mLoss[0m : 2.39592

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.524, [92mTest[0m: 11.031, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49969
[1mStep[0m  [16/169], [94mLoss[0m : 2.46202
[1mStep[0m  [32/169], [94mLoss[0m : 2.83689
[1mStep[0m  [48/169], [94mLoss[0m : 1.85488
[1mStep[0m  [64/169], [94mLoss[0m : 2.69857
[1mStep[0m  [80/169], [94mLoss[0m : 2.54139
[1mStep[0m  [96/169], [94mLoss[0m : 2.39921
[1mStep[0m  [112/169], [94mLoss[0m : 2.32372
[1mStep[0m  [128/169], [94mLoss[0m : 2.40039
[1mStep[0m  [144/169], [94mLoss[0m : 2.79924
[1mStep[0m  [160/169], [94mLoss[0m : 3.33480

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25148
[1mStep[0m  [16/169], [94mLoss[0m : 2.27586
[1mStep[0m  [32/169], [94mLoss[0m : 2.34194
[1mStep[0m  [48/169], [94mLoss[0m : 2.45850
[1mStep[0m  [64/169], [94mLoss[0m : 2.83945
[1mStep[0m  [80/169], [94mLoss[0m : 2.31661
[1mStep[0m  [96/169], [94mLoss[0m : 2.85320
[1mStep[0m  [112/169], [94mLoss[0m : 2.34766
[1mStep[0m  [128/169], [94mLoss[0m : 2.96057
[1mStep[0m  [144/169], [94mLoss[0m : 2.80381
[1mStep[0m  [160/169], [94mLoss[0m : 2.01877

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39262
[1mStep[0m  [16/169], [94mLoss[0m : 2.72446
[1mStep[0m  [32/169], [94mLoss[0m : 2.20282
[1mStep[0m  [48/169], [94mLoss[0m : 2.13083
[1mStep[0m  [64/169], [94mLoss[0m : 2.69874
[1mStep[0m  [80/169], [94mLoss[0m : 2.80430
[1mStep[0m  [96/169], [94mLoss[0m : 2.42735
[1mStep[0m  [112/169], [94mLoss[0m : 2.39894
[1mStep[0m  [128/169], [94mLoss[0m : 2.44104
[1mStep[0m  [144/169], [94mLoss[0m : 2.60821
[1mStep[0m  [160/169], [94mLoss[0m : 2.87652

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54484
[1mStep[0m  [16/169], [94mLoss[0m : 2.28904
[1mStep[0m  [32/169], [94mLoss[0m : 2.69593
[1mStep[0m  [48/169], [94mLoss[0m : 2.63112
[1mStep[0m  [64/169], [94mLoss[0m : 2.56594
[1mStep[0m  [80/169], [94mLoss[0m : 2.57543
[1mStep[0m  [96/169], [94mLoss[0m : 2.05972
[1mStep[0m  [112/169], [94mLoss[0m : 2.48040
[1mStep[0m  [128/169], [94mLoss[0m : 2.87550
[1mStep[0m  [144/169], [94mLoss[0m : 2.47037
[1mStep[0m  [160/169], [94mLoss[0m : 2.51426

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40686
[1mStep[0m  [16/169], [94mLoss[0m : 2.50424
[1mStep[0m  [32/169], [94mLoss[0m : 2.20945
[1mStep[0m  [48/169], [94mLoss[0m : 2.74042
[1mStep[0m  [64/169], [94mLoss[0m : 2.37759
[1mStep[0m  [80/169], [94mLoss[0m : 2.49661
[1mStep[0m  [96/169], [94mLoss[0m : 2.37445
[1mStep[0m  [112/169], [94mLoss[0m : 2.24432
[1mStep[0m  [128/169], [94mLoss[0m : 2.65978
[1mStep[0m  [144/169], [94mLoss[0m : 2.49828
[1mStep[0m  [160/169], [94mLoss[0m : 2.57508

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12209
[1mStep[0m  [16/169], [94mLoss[0m : 2.25847
[1mStep[0m  [32/169], [94mLoss[0m : 2.31128
[1mStep[0m  [48/169], [94mLoss[0m : 2.50949
[1mStep[0m  [64/169], [94mLoss[0m : 2.20228
[1mStep[0m  [80/169], [94mLoss[0m : 2.68222
[1mStep[0m  [96/169], [94mLoss[0m : 2.52454
[1mStep[0m  [112/169], [94mLoss[0m : 2.74590
[1mStep[0m  [128/169], [94mLoss[0m : 2.31513
[1mStep[0m  [144/169], [94mLoss[0m : 2.17512
[1mStep[0m  [160/169], [94mLoss[0m : 2.64045

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72009
[1mStep[0m  [16/169], [94mLoss[0m : 2.25619
[1mStep[0m  [32/169], [94mLoss[0m : 2.80361
[1mStep[0m  [48/169], [94mLoss[0m : 2.54636
[1mStep[0m  [64/169], [94mLoss[0m : 2.56393
[1mStep[0m  [80/169], [94mLoss[0m : 2.52169
[1mStep[0m  [96/169], [94mLoss[0m : 2.81427
[1mStep[0m  [112/169], [94mLoss[0m : 2.67501
[1mStep[0m  [128/169], [94mLoss[0m : 1.81150
[1mStep[0m  [144/169], [94mLoss[0m : 2.71083
[1mStep[0m  [160/169], [94mLoss[0m : 2.94975

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42959
[1mStep[0m  [16/169], [94mLoss[0m : 2.87619
[1mStep[0m  [32/169], [94mLoss[0m : 2.70646
[1mStep[0m  [48/169], [94mLoss[0m : 2.33955
[1mStep[0m  [64/169], [94mLoss[0m : 2.14824
[1mStep[0m  [80/169], [94mLoss[0m : 2.74992
[1mStep[0m  [96/169], [94mLoss[0m : 2.31339
[1mStep[0m  [112/169], [94mLoss[0m : 2.38059
[1mStep[0m  [128/169], [94mLoss[0m : 2.42807
[1mStep[0m  [144/169], [94mLoss[0m : 2.34171
[1mStep[0m  [160/169], [94mLoss[0m : 2.35781

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23865
[1mStep[0m  [16/169], [94mLoss[0m : 2.38417
[1mStep[0m  [32/169], [94mLoss[0m : 2.37285
[1mStep[0m  [48/169], [94mLoss[0m : 2.29500
[1mStep[0m  [64/169], [94mLoss[0m : 2.63418
[1mStep[0m  [80/169], [94mLoss[0m : 2.70942
[1mStep[0m  [96/169], [94mLoss[0m : 2.35425
[1mStep[0m  [112/169], [94mLoss[0m : 2.33400
[1mStep[0m  [128/169], [94mLoss[0m : 2.41644
[1mStep[0m  [144/169], [94mLoss[0m : 2.90207
[1mStep[0m  [160/169], [94mLoss[0m : 2.42761

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88819
[1mStep[0m  [16/169], [94mLoss[0m : 2.46264
[1mStep[0m  [32/169], [94mLoss[0m : 2.51297
[1mStep[0m  [48/169], [94mLoss[0m : 2.92922
[1mStep[0m  [64/169], [94mLoss[0m : 2.50035
[1mStep[0m  [80/169], [94mLoss[0m : 2.60652
[1mStep[0m  [96/169], [94mLoss[0m : 2.56147
[1mStep[0m  [112/169], [94mLoss[0m : 2.60354
[1mStep[0m  [128/169], [94mLoss[0m : 2.73291
[1mStep[0m  [144/169], [94mLoss[0m : 2.24976
[1mStep[0m  [160/169], [94mLoss[0m : 3.05955

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88124
[1mStep[0m  [16/169], [94mLoss[0m : 2.34291
[1mStep[0m  [32/169], [94mLoss[0m : 2.89385
[1mStep[0m  [48/169], [94mLoss[0m : 2.48874
[1mStep[0m  [64/169], [94mLoss[0m : 2.46006
[1mStep[0m  [80/169], [94mLoss[0m : 2.45134
[1mStep[0m  [96/169], [94mLoss[0m : 2.25285
[1mStep[0m  [112/169], [94mLoss[0m : 2.38950
[1mStep[0m  [128/169], [94mLoss[0m : 2.66168
[1mStep[0m  [144/169], [94mLoss[0m : 2.29487
[1mStep[0m  [160/169], [94mLoss[0m : 2.63181

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08244
[1mStep[0m  [16/169], [94mLoss[0m : 2.30073
[1mStep[0m  [32/169], [94mLoss[0m : 2.17501
[1mStep[0m  [48/169], [94mLoss[0m : 2.31571
[1mStep[0m  [64/169], [94mLoss[0m : 2.82323
[1mStep[0m  [80/169], [94mLoss[0m : 2.30115
[1mStep[0m  [96/169], [94mLoss[0m : 2.16732
[1mStep[0m  [112/169], [94mLoss[0m : 2.57914
[1mStep[0m  [128/169], [94mLoss[0m : 2.19142
[1mStep[0m  [144/169], [94mLoss[0m : 2.20250
[1mStep[0m  [160/169], [94mLoss[0m : 2.23148

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.95303
[1mStep[0m  [16/169], [94mLoss[0m : 2.67083
[1mStep[0m  [32/169], [94mLoss[0m : 2.58078
[1mStep[0m  [48/169], [94mLoss[0m : 2.48006
[1mStep[0m  [64/169], [94mLoss[0m : 2.42108
[1mStep[0m  [80/169], [94mLoss[0m : 2.55026
[1mStep[0m  [96/169], [94mLoss[0m : 2.83193
[1mStep[0m  [112/169], [94mLoss[0m : 2.51753
[1mStep[0m  [128/169], [94mLoss[0m : 2.95261
[1mStep[0m  [144/169], [94mLoss[0m : 2.59327
[1mStep[0m  [160/169], [94mLoss[0m : 2.37835

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69708
[1mStep[0m  [16/169], [94mLoss[0m : 2.12943
[1mStep[0m  [32/169], [94mLoss[0m : 2.73731
[1mStep[0m  [48/169], [94mLoss[0m : 2.21082
[1mStep[0m  [64/169], [94mLoss[0m : 2.90885
[1mStep[0m  [80/169], [94mLoss[0m : 2.49107
[1mStep[0m  [96/169], [94mLoss[0m : 2.67064
[1mStep[0m  [112/169], [94mLoss[0m : 2.68833
[1mStep[0m  [128/169], [94mLoss[0m : 2.57564
[1mStep[0m  [144/169], [94mLoss[0m : 2.35616
[1mStep[0m  [160/169], [94mLoss[0m : 2.25428

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54268
[1mStep[0m  [16/169], [94mLoss[0m : 2.42925
[1mStep[0m  [32/169], [94mLoss[0m : 2.65503
[1mStep[0m  [48/169], [94mLoss[0m : 2.43787
[1mStep[0m  [64/169], [94mLoss[0m : 2.72575
[1mStep[0m  [80/169], [94mLoss[0m : 2.61911
[1mStep[0m  [96/169], [94mLoss[0m : 2.15874
[1mStep[0m  [112/169], [94mLoss[0m : 2.48666
[1mStep[0m  [128/169], [94mLoss[0m : 2.36117
[1mStep[0m  [144/169], [94mLoss[0m : 2.36658
[1mStep[0m  [160/169], [94mLoss[0m : 2.70045

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53090
[1mStep[0m  [16/169], [94mLoss[0m : 2.11999
[1mStep[0m  [32/169], [94mLoss[0m : 2.39132
[1mStep[0m  [48/169], [94mLoss[0m : 2.13634
[1mStep[0m  [64/169], [94mLoss[0m : 2.76197
[1mStep[0m  [80/169], [94mLoss[0m : 2.01701
[1mStep[0m  [96/169], [94mLoss[0m : 2.38227
[1mStep[0m  [112/169], [94mLoss[0m : 2.61669
[1mStep[0m  [128/169], [94mLoss[0m : 2.39711
[1mStep[0m  [144/169], [94mLoss[0m : 2.34032
[1mStep[0m  [160/169], [94mLoss[0m : 2.51283

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06113
[1mStep[0m  [16/169], [94mLoss[0m : 2.26940
[1mStep[0m  [32/169], [94mLoss[0m : 2.54937
[1mStep[0m  [48/169], [94mLoss[0m : 2.52925
[1mStep[0m  [64/169], [94mLoss[0m : 2.69586
[1mStep[0m  [80/169], [94mLoss[0m : 2.21056
[1mStep[0m  [96/169], [94mLoss[0m : 2.79539
[1mStep[0m  [112/169], [94mLoss[0m : 2.30375
[1mStep[0m  [128/169], [94mLoss[0m : 2.72425
[1mStep[0m  [144/169], [94mLoss[0m : 2.43660
[1mStep[0m  [160/169], [94mLoss[0m : 2.79812

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71891
[1mStep[0m  [16/169], [94mLoss[0m : 2.80865
[1mStep[0m  [32/169], [94mLoss[0m : 2.26117
[1mStep[0m  [48/169], [94mLoss[0m : 2.36702
[1mStep[0m  [64/169], [94mLoss[0m : 2.48654
[1mStep[0m  [80/169], [94mLoss[0m : 2.12678
[1mStep[0m  [96/169], [94mLoss[0m : 2.63265
[1mStep[0m  [112/169], [94mLoss[0m : 2.50086
[1mStep[0m  [128/169], [94mLoss[0m : 2.21974
[1mStep[0m  [144/169], [94mLoss[0m : 2.30953
[1mStep[0m  [160/169], [94mLoss[0m : 2.69628

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56884
[1mStep[0m  [16/169], [94mLoss[0m : 2.34067
[1mStep[0m  [32/169], [94mLoss[0m : 2.37431
[1mStep[0m  [48/169], [94mLoss[0m : 2.48680
[1mStep[0m  [64/169], [94mLoss[0m : 2.64067
[1mStep[0m  [80/169], [94mLoss[0m : 1.83538
[1mStep[0m  [96/169], [94mLoss[0m : 2.39677
[1mStep[0m  [112/169], [94mLoss[0m : 2.60703
[1mStep[0m  [128/169], [94mLoss[0m : 2.18425
[1mStep[0m  [144/169], [94mLoss[0m : 3.11650
[1mStep[0m  [160/169], [94mLoss[0m : 2.40769

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.16266
[1mStep[0m  [16/169], [94mLoss[0m : 2.45502
[1mStep[0m  [32/169], [94mLoss[0m : 2.20543
[1mStep[0m  [48/169], [94mLoss[0m : 2.55806
[1mStep[0m  [64/169], [94mLoss[0m : 2.17607
[1mStep[0m  [80/169], [94mLoss[0m : 2.96271
[1mStep[0m  [96/169], [94mLoss[0m : 2.66815
[1mStep[0m  [112/169], [94mLoss[0m : 2.84312
[1mStep[0m  [128/169], [94mLoss[0m : 2.72707
[1mStep[0m  [144/169], [94mLoss[0m : 2.70336
[1mStep[0m  [160/169], [94mLoss[0m : 2.89704

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42925
[1mStep[0m  [16/169], [94mLoss[0m : 2.76403
[1mStep[0m  [32/169], [94mLoss[0m : 2.73680
[1mStep[0m  [48/169], [94mLoss[0m : 3.07691
[1mStep[0m  [64/169], [94mLoss[0m : 2.45818
[1mStep[0m  [80/169], [94mLoss[0m : 2.04632
[1mStep[0m  [96/169], [94mLoss[0m : 2.37725
[1mStep[0m  [112/169], [94mLoss[0m : 2.21880
[1mStep[0m  [128/169], [94mLoss[0m : 2.53472
[1mStep[0m  [144/169], [94mLoss[0m : 2.15879
[1mStep[0m  [160/169], [94mLoss[0m : 2.10976

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.345, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83645
[1mStep[0m  [16/169], [94mLoss[0m : 2.34125
[1mStep[0m  [32/169], [94mLoss[0m : 2.32033
[1mStep[0m  [48/169], [94mLoss[0m : 2.76573
[1mStep[0m  [64/169], [94mLoss[0m : 2.51292
[1mStep[0m  [80/169], [94mLoss[0m : 2.23829
[1mStep[0m  [96/169], [94mLoss[0m : 1.85822
[1mStep[0m  [112/169], [94mLoss[0m : 2.36288
[1mStep[0m  [128/169], [94mLoss[0m : 2.21730
[1mStep[0m  [144/169], [94mLoss[0m : 2.06744
[1mStep[0m  [160/169], [94mLoss[0m : 2.53127

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22101
[1mStep[0m  [16/169], [94mLoss[0m : 2.59648
[1mStep[0m  [32/169], [94mLoss[0m : 1.88800
[1mStep[0m  [48/169], [94mLoss[0m : 1.85377
[1mStep[0m  [64/169], [94mLoss[0m : 2.55304
[1mStep[0m  [80/169], [94mLoss[0m : 2.45728
[1mStep[0m  [96/169], [94mLoss[0m : 3.33601
[1mStep[0m  [112/169], [94mLoss[0m : 2.53211
[1mStep[0m  [128/169], [94mLoss[0m : 2.53800
[1mStep[0m  [144/169], [94mLoss[0m : 2.70069
[1mStep[0m  [160/169], [94mLoss[0m : 2.30433

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07261
[1mStep[0m  [16/169], [94mLoss[0m : 2.30255
[1mStep[0m  [32/169], [94mLoss[0m : 2.16280
[1mStep[0m  [48/169], [94mLoss[0m : 2.53918
[1mStep[0m  [64/169], [94mLoss[0m : 2.50785
[1mStep[0m  [80/169], [94mLoss[0m : 2.48768
[1mStep[0m  [96/169], [94mLoss[0m : 2.52455
[1mStep[0m  [112/169], [94mLoss[0m : 2.34580
[1mStep[0m  [128/169], [94mLoss[0m : 2.27407
[1mStep[0m  [144/169], [94mLoss[0m : 2.80430
[1mStep[0m  [160/169], [94mLoss[0m : 2.39493

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36279
[1mStep[0m  [16/169], [94mLoss[0m : 2.89696
[1mStep[0m  [32/169], [94mLoss[0m : 2.43970
[1mStep[0m  [48/169], [94mLoss[0m : 2.62234
[1mStep[0m  [64/169], [94mLoss[0m : 2.72681
[1mStep[0m  [80/169], [94mLoss[0m : 2.65327
[1mStep[0m  [96/169], [94mLoss[0m : 2.59394
[1mStep[0m  [112/169], [94mLoss[0m : 2.63372
[1mStep[0m  [128/169], [94mLoss[0m : 2.75985
[1mStep[0m  [144/169], [94mLoss[0m : 3.06658
[1mStep[0m  [160/169], [94mLoss[0m : 2.73103

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62530
[1mStep[0m  [16/169], [94mLoss[0m : 2.96798
[1mStep[0m  [32/169], [94mLoss[0m : 2.42772
[1mStep[0m  [48/169], [94mLoss[0m : 2.59013
[1mStep[0m  [64/169], [94mLoss[0m : 2.58571
[1mStep[0m  [80/169], [94mLoss[0m : 2.58083
[1mStep[0m  [96/169], [94mLoss[0m : 2.61706
[1mStep[0m  [112/169], [94mLoss[0m : 2.47344
[1mStep[0m  [128/169], [94mLoss[0m : 2.04351
[1mStep[0m  [144/169], [94mLoss[0m : 2.35604
[1mStep[0m  [160/169], [94mLoss[0m : 2.83687

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58359
[1mStep[0m  [16/169], [94mLoss[0m : 2.47581
[1mStep[0m  [32/169], [94mLoss[0m : 2.44645
[1mStep[0m  [48/169], [94mLoss[0m : 2.67787
[1mStep[0m  [64/169], [94mLoss[0m : 2.58463
[1mStep[0m  [80/169], [94mLoss[0m : 2.56786
[1mStep[0m  [96/169], [94mLoss[0m : 2.23057
[1mStep[0m  [112/169], [94mLoss[0m : 2.35024
[1mStep[0m  [128/169], [94mLoss[0m : 2.62627
[1mStep[0m  [144/169], [94mLoss[0m : 2.27117
[1mStep[0m  [160/169], [94mLoss[0m : 2.16275

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60223
[1mStep[0m  [16/169], [94mLoss[0m : 3.07260
[1mStep[0m  [32/169], [94mLoss[0m : 2.36480
[1mStep[0m  [48/169], [94mLoss[0m : 2.18798
[1mStep[0m  [64/169], [94mLoss[0m : 2.18409
[1mStep[0m  [80/169], [94mLoss[0m : 2.48732
[1mStep[0m  [96/169], [94mLoss[0m : 2.21250
[1mStep[0m  [112/169], [94mLoss[0m : 2.56456
[1mStep[0m  [128/169], [94mLoss[0m : 2.63734
[1mStep[0m  [144/169], [94mLoss[0m : 2.67458
[1mStep[0m  [160/169], [94mLoss[0m : 3.02385

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.14577
[1mStep[0m  [16/169], [94mLoss[0m : 2.24910
[1mStep[0m  [32/169], [94mLoss[0m : 2.47301
[1mStep[0m  [48/169], [94mLoss[0m : 2.34562
[1mStep[0m  [64/169], [94mLoss[0m : 2.61566
[1mStep[0m  [80/169], [94mLoss[0m : 2.38193
[1mStep[0m  [96/169], [94mLoss[0m : 2.43892
[1mStep[0m  [112/169], [94mLoss[0m : 2.21785
[1mStep[0m  [128/169], [94mLoss[0m : 2.38375
[1mStep[0m  [144/169], [94mLoss[0m : 2.66895
[1mStep[0m  [160/169], [94mLoss[0m : 2.97988

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.346
====================================

Phase 1 - Evaluation MAE:  2.345888621040753
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.92036
[1mStep[0m  [16/169], [94mLoss[0m : 2.39837
[1mStep[0m  [32/169], [94mLoss[0m : 2.46376
[1mStep[0m  [48/169], [94mLoss[0m : 2.53596
[1mStep[0m  [64/169], [94mLoss[0m : 2.38884
[1mStep[0m  [80/169], [94mLoss[0m : 2.46549
[1mStep[0m  [96/169], [94mLoss[0m : 2.28308
[1mStep[0m  [112/169], [94mLoss[0m : 2.39219
[1mStep[0m  [128/169], [94mLoss[0m : 2.13889
[1mStep[0m  [144/169], [94mLoss[0m : 2.64950
[1mStep[0m  [160/169], [94mLoss[0m : 2.50954

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35117
[1mStep[0m  [16/169], [94mLoss[0m : 2.69540
[1mStep[0m  [32/169], [94mLoss[0m : 2.28535
[1mStep[0m  [48/169], [94mLoss[0m : 2.65237
[1mStep[0m  [64/169], [94mLoss[0m : 2.22612
[1mStep[0m  [80/169], [94mLoss[0m : 2.24258
[1mStep[0m  [96/169], [94mLoss[0m : 2.60669
[1mStep[0m  [112/169], [94mLoss[0m : 2.70992
[1mStep[0m  [128/169], [94mLoss[0m : 2.55412
[1mStep[0m  [144/169], [94mLoss[0m : 2.00381
[1mStep[0m  [160/169], [94mLoss[0m : 2.37737

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45644
[1mStep[0m  [16/169], [94mLoss[0m : 2.33166
[1mStep[0m  [32/169], [94mLoss[0m : 2.15022
[1mStep[0m  [48/169], [94mLoss[0m : 2.49921
[1mStep[0m  [64/169], [94mLoss[0m : 2.10766
[1mStep[0m  [80/169], [94mLoss[0m : 2.24990
[1mStep[0m  [96/169], [94mLoss[0m : 2.36560
[1mStep[0m  [112/169], [94mLoss[0m : 2.61956
[1mStep[0m  [128/169], [94mLoss[0m : 2.29289
[1mStep[0m  [144/169], [94mLoss[0m : 1.97770
[1mStep[0m  [160/169], [94mLoss[0m : 2.39925

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10404
[1mStep[0m  [16/169], [94mLoss[0m : 2.00175
[1mStep[0m  [32/169], [94mLoss[0m : 2.15241
[1mStep[0m  [48/169], [94mLoss[0m : 2.50054
[1mStep[0m  [64/169], [94mLoss[0m : 1.88563
[1mStep[0m  [80/169], [94mLoss[0m : 2.43220
[1mStep[0m  [96/169], [94mLoss[0m : 2.26214
[1mStep[0m  [112/169], [94mLoss[0m : 2.05629
[1mStep[0m  [128/169], [94mLoss[0m : 2.18013
[1mStep[0m  [144/169], [94mLoss[0m : 2.34928
[1mStep[0m  [160/169], [94mLoss[0m : 1.99339

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13511
[1mStep[0m  [16/169], [94mLoss[0m : 2.41645
[1mStep[0m  [32/169], [94mLoss[0m : 2.45248
[1mStep[0m  [48/169], [94mLoss[0m : 2.65733
[1mStep[0m  [64/169], [94mLoss[0m : 2.61716
[1mStep[0m  [80/169], [94mLoss[0m : 2.33117
[1mStep[0m  [96/169], [94mLoss[0m : 2.44941
[1mStep[0m  [112/169], [94mLoss[0m : 2.51925
[1mStep[0m  [128/169], [94mLoss[0m : 2.19161
[1mStep[0m  [144/169], [94mLoss[0m : 2.97269
[1mStep[0m  [160/169], [94mLoss[0m : 2.40408

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92252
[1mStep[0m  [16/169], [94mLoss[0m : 2.34921
[1mStep[0m  [32/169], [94mLoss[0m : 2.13578
[1mStep[0m  [48/169], [94mLoss[0m : 2.32660
[1mStep[0m  [64/169], [94mLoss[0m : 2.17972
[1mStep[0m  [80/169], [94mLoss[0m : 2.43473
[1mStep[0m  [96/169], [94mLoss[0m : 2.44293
[1mStep[0m  [112/169], [94mLoss[0m : 2.31696
[1mStep[0m  [128/169], [94mLoss[0m : 2.50030
[1mStep[0m  [144/169], [94mLoss[0m : 2.59741
[1mStep[0m  [160/169], [94mLoss[0m : 2.46745

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55506
[1mStep[0m  [16/169], [94mLoss[0m : 2.19672
[1mStep[0m  [32/169], [94mLoss[0m : 2.15196
[1mStep[0m  [48/169], [94mLoss[0m : 2.34082
[1mStep[0m  [64/169], [94mLoss[0m : 2.27304
[1mStep[0m  [80/169], [94mLoss[0m : 1.96103
[1mStep[0m  [96/169], [94mLoss[0m : 2.14332
[1mStep[0m  [112/169], [94mLoss[0m : 1.99175
[1mStep[0m  [128/169], [94mLoss[0m : 2.35097
[1mStep[0m  [144/169], [94mLoss[0m : 1.99238
[1mStep[0m  [160/169], [94mLoss[0m : 2.31778

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50826
[1mStep[0m  [16/169], [94mLoss[0m : 2.30092
[1mStep[0m  [32/169], [94mLoss[0m : 2.00041
[1mStep[0m  [48/169], [94mLoss[0m : 2.08626
[1mStep[0m  [64/169], [94mLoss[0m : 2.52130
[1mStep[0m  [80/169], [94mLoss[0m : 2.06494
[1mStep[0m  [96/169], [94mLoss[0m : 2.27859
[1mStep[0m  [112/169], [94mLoss[0m : 1.98887
[1mStep[0m  [128/169], [94mLoss[0m : 2.50771
[1mStep[0m  [144/169], [94mLoss[0m : 2.50691
[1mStep[0m  [160/169], [94mLoss[0m : 2.48152

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67852
[1mStep[0m  [16/169], [94mLoss[0m : 2.09316
[1mStep[0m  [32/169], [94mLoss[0m : 2.32186
[1mStep[0m  [48/169], [94mLoss[0m : 2.13334
[1mStep[0m  [64/169], [94mLoss[0m : 2.21585
[1mStep[0m  [80/169], [94mLoss[0m : 2.30319
[1mStep[0m  [96/169], [94mLoss[0m : 2.13843
[1mStep[0m  [112/169], [94mLoss[0m : 2.03147
[1mStep[0m  [128/169], [94mLoss[0m : 2.21862
[1mStep[0m  [144/169], [94mLoss[0m : 2.25091
[1mStep[0m  [160/169], [94mLoss[0m : 2.42156

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62209
[1mStep[0m  [16/169], [94mLoss[0m : 2.51066
[1mStep[0m  [32/169], [94mLoss[0m : 2.40908
[1mStep[0m  [48/169], [94mLoss[0m : 2.09066
[1mStep[0m  [64/169], [94mLoss[0m : 2.62800
[1mStep[0m  [80/169], [94mLoss[0m : 2.04267
[1mStep[0m  [96/169], [94mLoss[0m : 2.70443
[1mStep[0m  [112/169], [94mLoss[0m : 2.82554
[1mStep[0m  [128/169], [94mLoss[0m : 2.42924
[1mStep[0m  [144/169], [94mLoss[0m : 2.62418
[1mStep[0m  [160/169], [94mLoss[0m : 2.26424

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20758
[1mStep[0m  [16/169], [94mLoss[0m : 2.48916
[1mStep[0m  [32/169], [94mLoss[0m : 1.93957
[1mStep[0m  [48/169], [94mLoss[0m : 2.18253
[1mStep[0m  [64/169], [94mLoss[0m : 2.17147
[1mStep[0m  [80/169], [94mLoss[0m : 2.35634
[1mStep[0m  [96/169], [94mLoss[0m : 2.63891
[1mStep[0m  [112/169], [94mLoss[0m : 2.49145
[1mStep[0m  [128/169], [94mLoss[0m : 2.34665
[1mStep[0m  [144/169], [94mLoss[0m : 2.10957
[1mStep[0m  [160/169], [94mLoss[0m : 2.25126

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81925
[1mStep[0m  [16/169], [94mLoss[0m : 2.03062
[1mStep[0m  [32/169], [94mLoss[0m : 2.39880
[1mStep[0m  [48/169], [94mLoss[0m : 2.00513
[1mStep[0m  [64/169], [94mLoss[0m : 2.53075
[1mStep[0m  [80/169], [94mLoss[0m : 1.85500
[1mStep[0m  [96/169], [94mLoss[0m : 2.30975
[1mStep[0m  [112/169], [94mLoss[0m : 2.34739
[1mStep[0m  [128/169], [94mLoss[0m : 2.35619
[1mStep[0m  [144/169], [94mLoss[0m : 2.38328
[1mStep[0m  [160/169], [94mLoss[0m : 2.56093

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39362
[1mStep[0m  [16/169], [94mLoss[0m : 2.53552
[1mStep[0m  [32/169], [94mLoss[0m : 2.07946
[1mStep[0m  [48/169], [94mLoss[0m : 2.14900
[1mStep[0m  [64/169], [94mLoss[0m : 2.81124
[1mStep[0m  [80/169], [94mLoss[0m : 2.38172
[1mStep[0m  [96/169], [94mLoss[0m : 2.21498
[1mStep[0m  [112/169], [94mLoss[0m : 2.33486
[1mStep[0m  [128/169], [94mLoss[0m : 2.54531
[1mStep[0m  [144/169], [94mLoss[0m : 2.29054
[1mStep[0m  [160/169], [94mLoss[0m : 2.08226

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27384
[1mStep[0m  [16/169], [94mLoss[0m : 2.38502
[1mStep[0m  [32/169], [94mLoss[0m : 2.30336
[1mStep[0m  [48/169], [94mLoss[0m : 2.04013
[1mStep[0m  [64/169], [94mLoss[0m : 1.97797
[1mStep[0m  [80/169], [94mLoss[0m : 2.44528
[1mStep[0m  [96/169], [94mLoss[0m : 2.43082
[1mStep[0m  [112/169], [94mLoss[0m : 2.28559
[1mStep[0m  [128/169], [94mLoss[0m : 2.18977
[1mStep[0m  [144/169], [94mLoss[0m : 3.19784
[1mStep[0m  [160/169], [94mLoss[0m : 1.87870

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05917
[1mStep[0m  [16/169], [94mLoss[0m : 2.61831
[1mStep[0m  [32/169], [94mLoss[0m : 2.14397
[1mStep[0m  [48/169], [94mLoss[0m : 2.26387
[1mStep[0m  [64/169], [94mLoss[0m : 2.61145
[1mStep[0m  [80/169], [94mLoss[0m : 2.12588
[1mStep[0m  [96/169], [94mLoss[0m : 2.22198
[1mStep[0m  [112/169], [94mLoss[0m : 2.13226
[1mStep[0m  [128/169], [94mLoss[0m : 2.47043
[1mStep[0m  [144/169], [94mLoss[0m : 2.22846
[1mStep[0m  [160/169], [94mLoss[0m : 1.77267

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.228, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48175
[1mStep[0m  [16/169], [94mLoss[0m : 2.26114
[1mStep[0m  [32/169], [94mLoss[0m : 2.17527
[1mStep[0m  [48/169], [94mLoss[0m : 2.41803
[1mStep[0m  [64/169], [94mLoss[0m : 2.02684
[1mStep[0m  [80/169], [94mLoss[0m : 2.00413
[1mStep[0m  [96/169], [94mLoss[0m : 2.31008
[1mStep[0m  [112/169], [94mLoss[0m : 2.25689
[1mStep[0m  [128/169], [94mLoss[0m : 2.23484
[1mStep[0m  [144/169], [94mLoss[0m : 2.01605
[1mStep[0m  [160/169], [94mLoss[0m : 1.94777

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12833
[1mStep[0m  [16/169], [94mLoss[0m : 1.84340
[1mStep[0m  [32/169], [94mLoss[0m : 2.52105
[1mStep[0m  [48/169], [94mLoss[0m : 1.73374
[1mStep[0m  [64/169], [94mLoss[0m : 2.05528
[1mStep[0m  [80/169], [94mLoss[0m : 2.42247
[1mStep[0m  [96/169], [94mLoss[0m : 2.79466
[1mStep[0m  [112/169], [94mLoss[0m : 2.34291
[1mStep[0m  [128/169], [94mLoss[0m : 1.62906
[1mStep[0m  [144/169], [94mLoss[0m : 2.32143
[1mStep[0m  [160/169], [94mLoss[0m : 2.06895

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17965
[1mStep[0m  [16/169], [94mLoss[0m : 1.61759
[1mStep[0m  [32/169], [94mLoss[0m : 2.60549
[1mStep[0m  [48/169], [94mLoss[0m : 2.17573
[1mStep[0m  [64/169], [94mLoss[0m : 1.99119
[1mStep[0m  [80/169], [94mLoss[0m : 2.36874
[1mStep[0m  [96/169], [94mLoss[0m : 2.22257
[1mStep[0m  [112/169], [94mLoss[0m : 2.08462
[1mStep[0m  [128/169], [94mLoss[0m : 2.76444
[1mStep[0m  [144/169], [94mLoss[0m : 2.39069
[1mStep[0m  [160/169], [94mLoss[0m : 2.05379

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95973
[1mStep[0m  [16/169], [94mLoss[0m : 2.15833
[1mStep[0m  [32/169], [94mLoss[0m : 2.08390
[1mStep[0m  [48/169], [94mLoss[0m : 2.40049
[1mStep[0m  [64/169], [94mLoss[0m : 2.15092
[1mStep[0m  [80/169], [94mLoss[0m : 1.95427
[1mStep[0m  [96/169], [94mLoss[0m : 2.42198
[1mStep[0m  [112/169], [94mLoss[0m : 1.84005
[1mStep[0m  [128/169], [94mLoss[0m : 2.52441
[1mStep[0m  [144/169], [94mLoss[0m : 2.24509
[1mStep[0m  [160/169], [94mLoss[0m : 2.02003

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64097
[1mStep[0m  [16/169], [94mLoss[0m : 2.20690
[1mStep[0m  [32/169], [94mLoss[0m : 2.07359
[1mStep[0m  [48/169], [94mLoss[0m : 2.39227
[1mStep[0m  [64/169], [94mLoss[0m : 2.17620
[1mStep[0m  [80/169], [94mLoss[0m : 2.50161
[1mStep[0m  [96/169], [94mLoss[0m : 2.28925
[1mStep[0m  [112/169], [94mLoss[0m : 2.24546
[1mStep[0m  [128/169], [94mLoss[0m : 2.23516
[1mStep[0m  [144/169], [94mLoss[0m : 2.58321
[1mStep[0m  [160/169], [94mLoss[0m : 2.37090

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.385, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05361
[1mStep[0m  [16/169], [94mLoss[0m : 2.24305
[1mStep[0m  [32/169], [94mLoss[0m : 2.23355
[1mStep[0m  [48/169], [94mLoss[0m : 2.52612
[1mStep[0m  [64/169], [94mLoss[0m : 2.07896
[1mStep[0m  [80/169], [94mLoss[0m : 2.12176
[1mStep[0m  [96/169], [94mLoss[0m : 2.27346
[1mStep[0m  [112/169], [94mLoss[0m : 2.34104
[1mStep[0m  [128/169], [94mLoss[0m : 2.37352
[1mStep[0m  [144/169], [94mLoss[0m : 2.38670
[1mStep[0m  [160/169], [94mLoss[0m : 2.58959

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.230, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09485
[1mStep[0m  [16/169], [94mLoss[0m : 1.84093
[1mStep[0m  [32/169], [94mLoss[0m : 2.28016
[1mStep[0m  [48/169], [94mLoss[0m : 2.14253
[1mStep[0m  [64/169], [94mLoss[0m : 2.29567
[1mStep[0m  [80/169], [94mLoss[0m : 2.03800
[1mStep[0m  [96/169], [94mLoss[0m : 2.46462
[1mStep[0m  [112/169], [94mLoss[0m : 2.00974
[1mStep[0m  [128/169], [94mLoss[0m : 2.32132
[1mStep[0m  [144/169], [94mLoss[0m : 2.02772
[1mStep[0m  [160/169], [94mLoss[0m : 2.35553

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04492
[1mStep[0m  [16/169], [94mLoss[0m : 1.87134
[1mStep[0m  [32/169], [94mLoss[0m : 1.81122
[1mStep[0m  [48/169], [94mLoss[0m : 2.34498
[1mStep[0m  [64/169], [94mLoss[0m : 2.29826
[1mStep[0m  [80/169], [94mLoss[0m : 2.15568
[1mStep[0m  [96/169], [94mLoss[0m : 2.37859
[1mStep[0m  [112/169], [94mLoss[0m : 2.05344
[1mStep[0m  [128/169], [94mLoss[0m : 2.01264
[1mStep[0m  [144/169], [94mLoss[0m : 2.63014
[1mStep[0m  [160/169], [94mLoss[0m : 2.23053

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69777
[1mStep[0m  [16/169], [94mLoss[0m : 2.02416
[1mStep[0m  [32/169], [94mLoss[0m : 1.81500
[1mStep[0m  [48/169], [94mLoss[0m : 1.96648
[1mStep[0m  [64/169], [94mLoss[0m : 2.43991
[1mStep[0m  [80/169], [94mLoss[0m : 2.03917
[1mStep[0m  [96/169], [94mLoss[0m : 2.09536
[1mStep[0m  [112/169], [94mLoss[0m : 2.06110
[1mStep[0m  [128/169], [94mLoss[0m : 2.17122
[1mStep[0m  [144/169], [94mLoss[0m : 2.15796
[1mStep[0m  [160/169], [94mLoss[0m : 2.88853

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.400, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18867
[1mStep[0m  [16/169], [94mLoss[0m : 2.28055
[1mStep[0m  [32/169], [94mLoss[0m : 2.20901
[1mStep[0m  [48/169], [94mLoss[0m : 1.99404
[1mStep[0m  [64/169], [94mLoss[0m : 2.03281
[1mStep[0m  [80/169], [94mLoss[0m : 2.69413
[1mStep[0m  [96/169], [94mLoss[0m : 2.20932
[1mStep[0m  [112/169], [94mLoss[0m : 2.31110
[1mStep[0m  [128/169], [94mLoss[0m : 2.32995
[1mStep[0m  [144/169], [94mLoss[0m : 2.04630
[1mStep[0m  [160/169], [94mLoss[0m : 2.07965

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.377, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15869
[1mStep[0m  [16/169], [94mLoss[0m : 2.28819
[1mStep[0m  [32/169], [94mLoss[0m : 2.71457
[1mStep[0m  [48/169], [94mLoss[0m : 2.35520
[1mStep[0m  [64/169], [94mLoss[0m : 1.91976
[1mStep[0m  [80/169], [94mLoss[0m : 2.33587
[1mStep[0m  [96/169], [94mLoss[0m : 2.42421
[1mStep[0m  [112/169], [94mLoss[0m : 1.73552
[1mStep[0m  [128/169], [94mLoss[0m : 2.16231
[1mStep[0m  [144/169], [94mLoss[0m : 1.97236
[1mStep[0m  [160/169], [94mLoss[0m : 2.30028

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.414, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87169
[1mStep[0m  [16/169], [94mLoss[0m : 2.33487
[1mStep[0m  [32/169], [94mLoss[0m : 2.61824
[1mStep[0m  [48/169], [94mLoss[0m : 2.09414
[1mStep[0m  [64/169], [94mLoss[0m : 2.52174
[1mStep[0m  [80/169], [94mLoss[0m : 2.60762
[1mStep[0m  [96/169], [94mLoss[0m : 2.22496
[1mStep[0m  [112/169], [94mLoss[0m : 2.09888
[1mStep[0m  [128/169], [94mLoss[0m : 2.71021
[1mStep[0m  [144/169], [94mLoss[0m : 2.16896
[1mStep[0m  [160/169], [94mLoss[0m : 2.02059

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29566
[1mStep[0m  [16/169], [94mLoss[0m : 2.42159
[1mStep[0m  [32/169], [94mLoss[0m : 2.22447
[1mStep[0m  [48/169], [94mLoss[0m : 2.14486
[1mStep[0m  [64/169], [94mLoss[0m : 2.15741
[1mStep[0m  [80/169], [94mLoss[0m : 2.44579
[1mStep[0m  [96/169], [94mLoss[0m : 2.33126
[1mStep[0m  [112/169], [94mLoss[0m : 2.22281
[1mStep[0m  [128/169], [94mLoss[0m : 2.10068
[1mStep[0m  [144/169], [94mLoss[0m : 2.45577
[1mStep[0m  [160/169], [94mLoss[0m : 2.63169

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.424, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36791
[1mStep[0m  [16/169], [94mLoss[0m : 2.42016
[1mStep[0m  [32/169], [94mLoss[0m : 2.47440
[1mStep[0m  [48/169], [94mLoss[0m : 2.62467
[1mStep[0m  [64/169], [94mLoss[0m : 2.26878
[1mStep[0m  [80/169], [94mLoss[0m : 1.97163
[1mStep[0m  [96/169], [94mLoss[0m : 2.38319
[1mStep[0m  [112/169], [94mLoss[0m : 2.29329
[1mStep[0m  [128/169], [94mLoss[0m : 2.23864
[1mStep[0m  [144/169], [94mLoss[0m : 2.35992
[1mStep[0m  [160/169], [94mLoss[0m : 2.47965

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.418, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97454
[1mStep[0m  [16/169], [94mLoss[0m : 2.21570
[1mStep[0m  [32/169], [94mLoss[0m : 2.37498
[1mStep[0m  [48/169], [94mLoss[0m : 2.03376
[1mStep[0m  [64/169], [94mLoss[0m : 2.31814
[1mStep[0m  [80/169], [94mLoss[0m : 2.35517
[1mStep[0m  [96/169], [94mLoss[0m : 2.17541
[1mStep[0m  [112/169], [94mLoss[0m : 1.92906
[1mStep[0m  [128/169], [94mLoss[0m : 2.34539
[1mStep[0m  [144/169], [94mLoss[0m : 2.41920
[1mStep[0m  [160/169], [94mLoss[0m : 2.42844

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.388
====================================

Phase 2 - Evaluation MAE:  2.3882962188550403
MAE score P1       2.345889
MAE score P2       2.388296
loss               2.226525
learning_rate          0.01
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.87340
[1mStep[0m  [8/84], [94mLoss[0m : 8.50316
[1mStep[0m  [16/84], [94mLoss[0m : 4.06134
[1mStep[0m  [24/84], [94mLoss[0m : 3.57074
[1mStep[0m  [32/84], [94mLoss[0m : 3.40660
[1mStep[0m  [40/84], [94mLoss[0m : 3.07881
[1mStep[0m  [48/84], [94mLoss[0m : 2.94691
[1mStep[0m  [56/84], [94mLoss[0m : 3.03227
[1mStep[0m  [64/84], [94mLoss[0m : 2.94185
[1mStep[0m  [72/84], [94mLoss[0m : 2.73652
[1mStep[0m  [80/84], [94mLoss[0m : 2.60442

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.019, [92mTest[0m: 10.972, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87742
[1mStep[0m  [8/84], [94mLoss[0m : 2.73902
[1mStep[0m  [16/84], [94mLoss[0m : 2.58388
[1mStep[0m  [24/84], [94mLoss[0m : 2.64330
[1mStep[0m  [32/84], [94mLoss[0m : 2.58514
[1mStep[0m  [40/84], [94mLoss[0m : 2.48208
[1mStep[0m  [48/84], [94mLoss[0m : 2.64997
[1mStep[0m  [56/84], [94mLoss[0m : 2.70547
[1mStep[0m  [64/84], [94mLoss[0m : 2.66730
[1mStep[0m  [72/84], [94mLoss[0m : 2.65214
[1mStep[0m  [80/84], [94mLoss[0m : 2.56980

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66867
[1mStep[0m  [8/84], [94mLoss[0m : 2.80795
[1mStep[0m  [16/84], [94mLoss[0m : 2.45206
[1mStep[0m  [24/84], [94mLoss[0m : 2.82177
[1mStep[0m  [32/84], [94mLoss[0m : 2.55362
[1mStep[0m  [40/84], [94mLoss[0m : 2.65442
[1mStep[0m  [48/84], [94mLoss[0m : 2.85658
[1mStep[0m  [56/84], [94mLoss[0m : 2.59192
[1mStep[0m  [64/84], [94mLoss[0m : 2.67364
[1mStep[0m  [72/84], [94mLoss[0m : 2.46303
[1mStep[0m  [80/84], [94mLoss[0m : 2.38564

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75357
[1mStep[0m  [8/84], [94mLoss[0m : 2.81963
[1mStep[0m  [16/84], [94mLoss[0m : 2.66834
[1mStep[0m  [24/84], [94mLoss[0m : 2.63826
[1mStep[0m  [32/84], [94mLoss[0m : 2.48113
[1mStep[0m  [40/84], [94mLoss[0m : 2.75869
[1mStep[0m  [48/84], [94mLoss[0m : 2.77088
[1mStep[0m  [56/84], [94mLoss[0m : 2.29201
[1mStep[0m  [64/84], [94mLoss[0m : 2.75726
[1mStep[0m  [72/84], [94mLoss[0m : 2.89429
[1mStep[0m  [80/84], [94mLoss[0m : 2.51173

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77802
[1mStep[0m  [8/84], [94mLoss[0m : 2.54057
[1mStep[0m  [16/84], [94mLoss[0m : 2.57275
[1mStep[0m  [24/84], [94mLoss[0m : 2.69519
[1mStep[0m  [32/84], [94mLoss[0m : 2.46791
[1mStep[0m  [40/84], [94mLoss[0m : 2.71742
[1mStep[0m  [48/84], [94mLoss[0m : 2.52493
[1mStep[0m  [56/84], [94mLoss[0m : 2.74792
[1mStep[0m  [64/84], [94mLoss[0m : 2.77141
[1mStep[0m  [72/84], [94mLoss[0m : 2.46840
[1mStep[0m  [80/84], [94mLoss[0m : 2.71272

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34106
[1mStep[0m  [8/84], [94mLoss[0m : 2.70680
[1mStep[0m  [16/84], [94mLoss[0m : 2.60122
[1mStep[0m  [24/84], [94mLoss[0m : 2.73366
[1mStep[0m  [32/84], [94mLoss[0m : 2.60405
[1mStep[0m  [40/84], [94mLoss[0m : 2.56694
[1mStep[0m  [48/84], [94mLoss[0m : 2.45606
[1mStep[0m  [56/84], [94mLoss[0m : 2.52993
[1mStep[0m  [64/84], [94mLoss[0m : 2.58298
[1mStep[0m  [72/84], [94mLoss[0m : 2.53135
[1mStep[0m  [80/84], [94mLoss[0m : 2.20790

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43471
[1mStep[0m  [8/84], [94mLoss[0m : 2.22886
[1mStep[0m  [16/84], [94mLoss[0m : 2.28576
[1mStep[0m  [24/84], [94mLoss[0m : 2.71629
[1mStep[0m  [32/84], [94mLoss[0m : 2.84277
[1mStep[0m  [40/84], [94mLoss[0m : 2.30138
[1mStep[0m  [48/84], [94mLoss[0m : 2.51032
[1mStep[0m  [56/84], [94mLoss[0m : 2.46426
[1mStep[0m  [64/84], [94mLoss[0m : 2.51602
[1mStep[0m  [72/84], [94mLoss[0m : 2.47845
[1mStep[0m  [80/84], [94mLoss[0m : 2.50200

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93619
[1mStep[0m  [8/84], [94mLoss[0m : 2.52067
[1mStep[0m  [16/84], [94mLoss[0m : 2.50194
[1mStep[0m  [24/84], [94mLoss[0m : 2.57974
[1mStep[0m  [32/84], [94mLoss[0m : 2.77033
[1mStep[0m  [40/84], [94mLoss[0m : 2.52854
[1mStep[0m  [48/84], [94mLoss[0m : 2.39069
[1mStep[0m  [56/84], [94mLoss[0m : 2.39583
[1mStep[0m  [64/84], [94mLoss[0m : 2.31678
[1mStep[0m  [72/84], [94mLoss[0m : 2.38932
[1mStep[0m  [80/84], [94mLoss[0m : 2.34013

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39059
[1mStep[0m  [8/84], [94mLoss[0m : 2.55169
[1mStep[0m  [16/84], [94mLoss[0m : 2.47345
[1mStep[0m  [24/84], [94mLoss[0m : 2.08559
[1mStep[0m  [32/84], [94mLoss[0m : 2.67108
[1mStep[0m  [40/84], [94mLoss[0m : 2.39946
[1mStep[0m  [48/84], [94mLoss[0m : 2.34447
[1mStep[0m  [56/84], [94mLoss[0m : 2.45322
[1mStep[0m  [64/84], [94mLoss[0m : 2.38270
[1mStep[0m  [72/84], [94mLoss[0m : 2.56268
[1mStep[0m  [80/84], [94mLoss[0m : 2.38625

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35128
[1mStep[0m  [8/84], [94mLoss[0m : 2.31004
[1mStep[0m  [16/84], [94mLoss[0m : 2.31650
[1mStep[0m  [24/84], [94mLoss[0m : 2.38779
[1mStep[0m  [32/84], [94mLoss[0m : 2.43168
[1mStep[0m  [40/84], [94mLoss[0m : 2.39553
[1mStep[0m  [48/84], [94mLoss[0m : 2.59210
[1mStep[0m  [56/84], [94mLoss[0m : 2.47285
[1mStep[0m  [64/84], [94mLoss[0m : 2.89398
[1mStep[0m  [72/84], [94mLoss[0m : 2.59938
[1mStep[0m  [80/84], [94mLoss[0m : 2.77888

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49586
[1mStep[0m  [8/84], [94mLoss[0m : 2.49610
[1mStep[0m  [16/84], [94mLoss[0m : 2.66170
[1mStep[0m  [24/84], [94mLoss[0m : 2.44987
[1mStep[0m  [32/84], [94mLoss[0m : 2.57256
[1mStep[0m  [40/84], [94mLoss[0m : 2.26639
[1mStep[0m  [48/84], [94mLoss[0m : 2.53740
[1mStep[0m  [56/84], [94mLoss[0m : 2.64933
[1mStep[0m  [64/84], [94mLoss[0m : 2.55477
[1mStep[0m  [72/84], [94mLoss[0m : 2.24216
[1mStep[0m  [80/84], [94mLoss[0m : 2.38093

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10022
[1mStep[0m  [8/84], [94mLoss[0m : 2.56268
[1mStep[0m  [16/84], [94mLoss[0m : 2.31094
[1mStep[0m  [24/84], [94mLoss[0m : 2.67735
[1mStep[0m  [32/84], [94mLoss[0m : 2.30510
[1mStep[0m  [40/84], [94mLoss[0m : 2.76338
[1mStep[0m  [48/84], [94mLoss[0m : 2.43187
[1mStep[0m  [56/84], [94mLoss[0m : 2.50212
[1mStep[0m  [64/84], [94mLoss[0m : 2.66276
[1mStep[0m  [72/84], [94mLoss[0m : 2.53992
[1mStep[0m  [80/84], [94mLoss[0m : 1.96749

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97605
[1mStep[0m  [8/84], [94mLoss[0m : 2.01132
[1mStep[0m  [16/84], [94mLoss[0m : 2.26005
[1mStep[0m  [24/84], [94mLoss[0m : 2.34419
[1mStep[0m  [32/84], [94mLoss[0m : 2.32423
[1mStep[0m  [40/84], [94mLoss[0m : 2.55277
[1mStep[0m  [48/84], [94mLoss[0m : 2.61737
[1mStep[0m  [56/84], [94mLoss[0m : 2.51950
[1mStep[0m  [64/84], [94mLoss[0m : 2.49565
[1mStep[0m  [72/84], [94mLoss[0m : 2.55742
[1mStep[0m  [80/84], [94mLoss[0m : 2.23584

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16622
[1mStep[0m  [8/84], [94mLoss[0m : 2.50802
[1mStep[0m  [16/84], [94mLoss[0m : 2.41795
[1mStep[0m  [24/84], [94mLoss[0m : 2.46714
[1mStep[0m  [32/84], [94mLoss[0m : 2.32946
[1mStep[0m  [40/84], [94mLoss[0m : 2.30465
[1mStep[0m  [48/84], [94mLoss[0m : 2.58881
[1mStep[0m  [56/84], [94mLoss[0m : 2.22629
[1mStep[0m  [64/84], [94mLoss[0m : 2.17736
[1mStep[0m  [72/84], [94mLoss[0m : 2.28795
[1mStep[0m  [80/84], [94mLoss[0m : 2.63648

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65226
[1mStep[0m  [8/84], [94mLoss[0m : 2.31511
[1mStep[0m  [16/84], [94mLoss[0m : 2.07308
[1mStep[0m  [24/84], [94mLoss[0m : 2.16816
[1mStep[0m  [32/84], [94mLoss[0m : 2.11476
[1mStep[0m  [40/84], [94mLoss[0m : 2.41046
[1mStep[0m  [48/84], [94mLoss[0m : 2.23306
[1mStep[0m  [56/84], [94mLoss[0m : 2.15374
[1mStep[0m  [64/84], [94mLoss[0m : 2.32611
[1mStep[0m  [72/84], [94mLoss[0m : 2.30520
[1mStep[0m  [80/84], [94mLoss[0m : 2.35752

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.317, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10747
[1mStep[0m  [8/84], [94mLoss[0m : 2.32071
[1mStep[0m  [16/84], [94mLoss[0m : 2.53054
[1mStep[0m  [24/84], [94mLoss[0m : 2.86039
[1mStep[0m  [32/84], [94mLoss[0m : 2.41527
[1mStep[0m  [40/84], [94mLoss[0m : 2.31558
[1mStep[0m  [48/84], [94mLoss[0m : 2.47343
[1mStep[0m  [56/84], [94mLoss[0m : 2.61121
[1mStep[0m  [64/84], [94mLoss[0m : 2.30146
[1mStep[0m  [72/84], [94mLoss[0m : 2.22860
[1mStep[0m  [80/84], [94mLoss[0m : 2.11622

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35532
[1mStep[0m  [8/84], [94mLoss[0m : 2.20821
[1mStep[0m  [16/84], [94mLoss[0m : 2.28092
[1mStep[0m  [24/84], [94mLoss[0m : 2.13301
[1mStep[0m  [32/84], [94mLoss[0m : 2.27195
[1mStep[0m  [40/84], [94mLoss[0m : 2.52628
[1mStep[0m  [48/84], [94mLoss[0m : 2.27350
[1mStep[0m  [56/84], [94mLoss[0m : 2.55530
[1mStep[0m  [64/84], [94mLoss[0m : 2.29234
[1mStep[0m  [72/84], [94mLoss[0m : 2.40137
[1mStep[0m  [80/84], [94mLoss[0m : 2.15072

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20655
[1mStep[0m  [8/84], [94mLoss[0m : 2.27291
[1mStep[0m  [16/84], [94mLoss[0m : 2.11909
[1mStep[0m  [24/84], [94mLoss[0m : 1.99205
[1mStep[0m  [32/84], [94mLoss[0m : 2.65759
[1mStep[0m  [40/84], [94mLoss[0m : 2.32048
[1mStep[0m  [48/84], [94mLoss[0m : 2.62243
[1mStep[0m  [56/84], [94mLoss[0m : 2.24110
[1mStep[0m  [64/84], [94mLoss[0m : 2.38447
[1mStep[0m  [72/84], [94mLoss[0m : 2.66104
[1mStep[0m  [80/84], [94mLoss[0m : 2.58887

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24944
[1mStep[0m  [8/84], [94mLoss[0m : 2.17511
[1mStep[0m  [16/84], [94mLoss[0m : 2.20154
[1mStep[0m  [24/84], [94mLoss[0m : 2.37866
[1mStep[0m  [32/84], [94mLoss[0m : 2.61311
[1mStep[0m  [40/84], [94mLoss[0m : 2.23236
[1mStep[0m  [48/84], [94mLoss[0m : 2.15964
[1mStep[0m  [56/84], [94mLoss[0m : 2.27022
[1mStep[0m  [64/84], [94mLoss[0m : 2.41158
[1mStep[0m  [72/84], [94mLoss[0m : 2.42891
[1mStep[0m  [80/84], [94mLoss[0m : 2.47197

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03346
[1mStep[0m  [8/84], [94mLoss[0m : 2.25997
[1mStep[0m  [16/84], [94mLoss[0m : 1.90720
[1mStep[0m  [24/84], [94mLoss[0m : 2.45026
[1mStep[0m  [32/84], [94mLoss[0m : 2.40760
[1mStep[0m  [40/84], [94mLoss[0m : 2.65350
[1mStep[0m  [48/84], [94mLoss[0m : 2.30007
[1mStep[0m  [56/84], [94mLoss[0m : 2.08876
[1mStep[0m  [64/84], [94mLoss[0m : 2.11313
[1mStep[0m  [72/84], [94mLoss[0m : 2.38484
[1mStep[0m  [80/84], [94mLoss[0m : 2.46964

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.284, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25077
[1mStep[0m  [8/84], [94mLoss[0m : 2.16500
[1mStep[0m  [16/84], [94mLoss[0m : 2.36549
[1mStep[0m  [24/84], [94mLoss[0m : 2.19925
[1mStep[0m  [32/84], [94mLoss[0m : 2.38244
[1mStep[0m  [40/84], [94mLoss[0m : 2.30995
[1mStep[0m  [48/84], [94mLoss[0m : 2.05797
[1mStep[0m  [56/84], [94mLoss[0m : 2.32685
[1mStep[0m  [64/84], [94mLoss[0m : 2.41619
[1mStep[0m  [72/84], [94mLoss[0m : 2.24674
[1mStep[0m  [80/84], [94mLoss[0m : 2.36972

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52376
[1mStep[0m  [8/84], [94mLoss[0m : 2.73987
[1mStep[0m  [16/84], [94mLoss[0m : 2.17880
[1mStep[0m  [24/84], [94mLoss[0m : 2.24105
[1mStep[0m  [32/84], [94mLoss[0m : 2.50068
[1mStep[0m  [40/84], [94mLoss[0m : 2.28130
[1mStep[0m  [48/84], [94mLoss[0m : 2.43644
[1mStep[0m  [56/84], [94mLoss[0m : 1.95436
[1mStep[0m  [64/84], [94mLoss[0m : 2.25862
[1mStep[0m  [72/84], [94mLoss[0m : 2.39416
[1mStep[0m  [80/84], [94mLoss[0m : 2.53346

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.304, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06569
[1mStep[0m  [8/84], [94mLoss[0m : 2.21226
[1mStep[0m  [16/84], [94mLoss[0m : 2.24738
[1mStep[0m  [24/84], [94mLoss[0m : 2.35301
[1mStep[0m  [32/84], [94mLoss[0m : 2.14614
[1mStep[0m  [40/84], [94mLoss[0m : 2.27201
[1mStep[0m  [48/84], [94mLoss[0m : 2.25781
[1mStep[0m  [56/84], [94mLoss[0m : 2.16487
[1mStep[0m  [64/84], [94mLoss[0m : 2.06879
[1mStep[0m  [72/84], [94mLoss[0m : 2.44554
[1mStep[0m  [80/84], [94mLoss[0m : 2.79532

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43369
[1mStep[0m  [8/84], [94mLoss[0m : 2.22299
[1mStep[0m  [16/84], [94mLoss[0m : 2.36427
[1mStep[0m  [24/84], [94mLoss[0m : 2.19067
[1mStep[0m  [32/84], [94mLoss[0m : 2.26000
[1mStep[0m  [40/84], [94mLoss[0m : 2.43820
[1mStep[0m  [48/84], [94mLoss[0m : 2.38661
[1mStep[0m  [56/84], [94mLoss[0m : 2.04358
[1mStep[0m  [64/84], [94mLoss[0m : 2.10122
[1mStep[0m  [72/84], [94mLoss[0m : 2.46726
[1mStep[0m  [80/84], [94mLoss[0m : 2.01204

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.311, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18688
[1mStep[0m  [8/84], [94mLoss[0m : 2.27444
[1mStep[0m  [16/84], [94mLoss[0m : 2.32927
[1mStep[0m  [24/84], [94mLoss[0m : 2.20276
[1mStep[0m  [32/84], [94mLoss[0m : 2.18721
[1mStep[0m  [40/84], [94mLoss[0m : 2.45440
[1mStep[0m  [48/84], [94mLoss[0m : 2.18820
[1mStep[0m  [56/84], [94mLoss[0m : 2.37095
[1mStep[0m  [64/84], [94mLoss[0m : 2.34308
[1mStep[0m  [72/84], [94mLoss[0m : 2.38266
[1mStep[0m  [80/84], [94mLoss[0m : 2.35193

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09133
[1mStep[0m  [8/84], [94mLoss[0m : 2.19998
[1mStep[0m  [16/84], [94mLoss[0m : 2.67618
[1mStep[0m  [24/84], [94mLoss[0m : 2.15820
[1mStep[0m  [32/84], [94mLoss[0m : 2.44805
[1mStep[0m  [40/84], [94mLoss[0m : 2.59118
[1mStep[0m  [48/84], [94mLoss[0m : 2.10979
[1mStep[0m  [56/84], [94mLoss[0m : 2.53909
[1mStep[0m  [64/84], [94mLoss[0m : 2.03889
[1mStep[0m  [72/84], [94mLoss[0m : 2.40671
[1mStep[0m  [80/84], [94mLoss[0m : 2.68472

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30337
[1mStep[0m  [8/84], [94mLoss[0m : 2.47457
[1mStep[0m  [16/84], [94mLoss[0m : 2.26453
[1mStep[0m  [24/84], [94mLoss[0m : 2.20135
[1mStep[0m  [32/84], [94mLoss[0m : 1.98060
[1mStep[0m  [40/84], [94mLoss[0m : 2.24778
[1mStep[0m  [48/84], [94mLoss[0m : 2.34409
[1mStep[0m  [56/84], [94mLoss[0m : 2.36873
[1mStep[0m  [64/84], [94mLoss[0m : 2.57332
[1mStep[0m  [72/84], [94mLoss[0m : 2.10756
[1mStep[0m  [80/84], [94mLoss[0m : 2.20369

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.310, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02104
[1mStep[0m  [8/84], [94mLoss[0m : 2.22292
[1mStep[0m  [16/84], [94mLoss[0m : 2.04838
[1mStep[0m  [24/84], [94mLoss[0m : 2.33844
[1mStep[0m  [32/84], [94mLoss[0m : 2.48082
[1mStep[0m  [40/84], [94mLoss[0m : 2.10556
[1mStep[0m  [48/84], [94mLoss[0m : 2.29885
[1mStep[0m  [56/84], [94mLoss[0m : 2.31205
[1mStep[0m  [64/84], [94mLoss[0m : 2.10279
[1mStep[0m  [72/84], [94mLoss[0m : 2.21421
[1mStep[0m  [80/84], [94mLoss[0m : 2.28510

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.301, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12363
[1mStep[0m  [8/84], [94mLoss[0m : 2.22840
[1mStep[0m  [16/84], [94mLoss[0m : 2.10940
[1mStep[0m  [24/84], [94mLoss[0m : 2.28524
[1mStep[0m  [32/84], [94mLoss[0m : 2.32855
[1mStep[0m  [40/84], [94mLoss[0m : 2.11915
[1mStep[0m  [48/84], [94mLoss[0m : 2.32675
[1mStep[0m  [56/84], [94mLoss[0m : 2.31170
[1mStep[0m  [64/84], [94mLoss[0m : 2.35549
[1mStep[0m  [72/84], [94mLoss[0m : 1.99823
[1mStep[0m  [80/84], [94mLoss[0m : 2.22326

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.302, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25557
[1mStep[0m  [8/84], [94mLoss[0m : 2.45024
[1mStep[0m  [16/84], [94mLoss[0m : 2.63133
[1mStep[0m  [24/84], [94mLoss[0m : 2.19171
[1mStep[0m  [32/84], [94mLoss[0m : 2.43416
[1mStep[0m  [40/84], [94mLoss[0m : 2.47564
[1mStep[0m  [48/84], [94mLoss[0m : 2.18276
[1mStep[0m  [56/84], [94mLoss[0m : 1.89961
[1mStep[0m  [64/84], [94mLoss[0m : 2.31898
[1mStep[0m  [72/84], [94mLoss[0m : 2.27166
[1mStep[0m  [80/84], [94mLoss[0m : 2.05871

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.310, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.305
====================================

Phase 1 - Evaluation MAE:  2.3053986770766124
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.37031
[1mStep[0m  [8/84], [94mLoss[0m : 2.28816
[1mStep[0m  [16/84], [94mLoss[0m : 2.31769
[1mStep[0m  [24/84], [94mLoss[0m : 2.46502
[1mStep[0m  [32/84], [94mLoss[0m : 2.34101
[1mStep[0m  [40/84], [94mLoss[0m : 2.41479
[1mStep[0m  [48/84], [94mLoss[0m : 2.24987
[1mStep[0m  [56/84], [94mLoss[0m : 2.27815
[1mStep[0m  [64/84], [94mLoss[0m : 2.61515
[1mStep[0m  [72/84], [94mLoss[0m : 2.21813
[1mStep[0m  [80/84], [94mLoss[0m : 2.51999

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.308, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54487
[1mStep[0m  [8/84], [94mLoss[0m : 2.55572
[1mStep[0m  [16/84], [94mLoss[0m : 2.30197
[1mStep[0m  [24/84], [94mLoss[0m : 2.17818
[1mStep[0m  [32/84], [94mLoss[0m : 2.12509
[1mStep[0m  [40/84], [94mLoss[0m : 2.81233
[1mStep[0m  [48/84], [94mLoss[0m : 2.26062
[1mStep[0m  [56/84], [94mLoss[0m : 2.27491
[1mStep[0m  [64/84], [94mLoss[0m : 2.34466
[1mStep[0m  [72/84], [94mLoss[0m : 2.36047
[1mStep[0m  [80/84], [94mLoss[0m : 2.54673

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39428
[1mStep[0m  [8/84], [94mLoss[0m : 2.15347
[1mStep[0m  [16/84], [94mLoss[0m : 2.17304
[1mStep[0m  [24/84], [94mLoss[0m : 2.02653
[1mStep[0m  [32/84], [94mLoss[0m : 2.10018
[1mStep[0m  [40/84], [94mLoss[0m : 2.28176
[1mStep[0m  [48/84], [94mLoss[0m : 2.27645
[1mStep[0m  [56/84], [94mLoss[0m : 2.26164
[1mStep[0m  [64/84], [94mLoss[0m : 2.09849
[1mStep[0m  [72/84], [94mLoss[0m : 2.37309
[1mStep[0m  [80/84], [94mLoss[0m : 2.27005

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34300
[1mStep[0m  [8/84], [94mLoss[0m : 1.98752
[1mStep[0m  [16/84], [94mLoss[0m : 2.31541
[1mStep[0m  [24/84], [94mLoss[0m : 2.15143
[1mStep[0m  [32/84], [94mLoss[0m : 2.27337
[1mStep[0m  [40/84], [94mLoss[0m : 2.07350
[1mStep[0m  [48/84], [94mLoss[0m : 2.03165
[1mStep[0m  [56/84], [94mLoss[0m : 2.12344
[1mStep[0m  [64/84], [94mLoss[0m : 2.21606
[1mStep[0m  [72/84], [94mLoss[0m : 2.16581
[1mStep[0m  [80/84], [94mLoss[0m : 2.22489

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04650
[1mStep[0m  [8/84], [94mLoss[0m : 1.83745
[1mStep[0m  [16/84], [94mLoss[0m : 2.33410
[1mStep[0m  [24/84], [94mLoss[0m : 2.10125
[1mStep[0m  [32/84], [94mLoss[0m : 2.27846
[1mStep[0m  [40/84], [94mLoss[0m : 1.80575
[1mStep[0m  [48/84], [94mLoss[0m : 2.14939
[1mStep[0m  [56/84], [94mLoss[0m : 1.88807
[1mStep[0m  [64/84], [94mLoss[0m : 2.22004
[1mStep[0m  [72/84], [94mLoss[0m : 2.21995
[1mStep[0m  [80/84], [94mLoss[0m : 2.41949

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.047, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41174
[1mStep[0m  [8/84], [94mLoss[0m : 2.00104
[1mStep[0m  [16/84], [94mLoss[0m : 1.78594
[1mStep[0m  [24/84], [94mLoss[0m : 2.04795
[1mStep[0m  [32/84], [94mLoss[0m : 2.10530
[1mStep[0m  [40/84], [94mLoss[0m : 1.64868
[1mStep[0m  [48/84], [94mLoss[0m : 2.01218
[1mStep[0m  [56/84], [94mLoss[0m : 2.08676
[1mStep[0m  [64/84], [94mLoss[0m : 1.98268
[1mStep[0m  [72/84], [94mLoss[0m : 2.09574
[1mStep[0m  [80/84], [94mLoss[0m : 2.03683

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01658
[1mStep[0m  [8/84], [94mLoss[0m : 1.81406
[1mStep[0m  [16/84], [94mLoss[0m : 2.13282
[1mStep[0m  [24/84], [94mLoss[0m : 1.78116
[1mStep[0m  [32/84], [94mLoss[0m : 1.76643
[1mStep[0m  [40/84], [94mLoss[0m : 2.10307
[1mStep[0m  [48/84], [94mLoss[0m : 2.15582
[1mStep[0m  [56/84], [94mLoss[0m : 1.80037
[1mStep[0m  [64/84], [94mLoss[0m : 1.90739
[1mStep[0m  [72/84], [94mLoss[0m : 1.98373
[1mStep[0m  [80/84], [94mLoss[0m : 2.30355

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61706
[1mStep[0m  [8/84], [94mLoss[0m : 2.00386
[1mStep[0m  [16/84], [94mLoss[0m : 1.89084
[1mStep[0m  [24/84], [94mLoss[0m : 1.68054
[1mStep[0m  [32/84], [94mLoss[0m : 1.69346
[1mStep[0m  [40/84], [94mLoss[0m : 1.64472
[1mStep[0m  [48/84], [94mLoss[0m : 1.90626
[1mStep[0m  [56/84], [94mLoss[0m : 2.03325
[1mStep[0m  [64/84], [94mLoss[0m : 1.71474
[1mStep[0m  [72/84], [94mLoss[0m : 1.67626
[1mStep[0m  [80/84], [94mLoss[0m : 1.98383

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.70438
[1mStep[0m  [8/84], [94mLoss[0m : 1.68772
[1mStep[0m  [16/84], [94mLoss[0m : 1.66613
[1mStep[0m  [24/84], [94mLoss[0m : 1.78757
[1mStep[0m  [32/84], [94mLoss[0m : 1.57150
[1mStep[0m  [40/84], [94mLoss[0m : 1.58319
[1mStep[0m  [48/84], [94mLoss[0m : 2.03009
[1mStep[0m  [56/84], [94mLoss[0m : 1.54198
[1mStep[0m  [64/84], [94mLoss[0m : 1.61878
[1mStep[0m  [72/84], [94mLoss[0m : 1.85260
[1mStep[0m  [80/84], [94mLoss[0m : 1.77147

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71177
[1mStep[0m  [8/84], [94mLoss[0m : 1.66980
[1mStep[0m  [16/84], [94mLoss[0m : 1.78197
[1mStep[0m  [24/84], [94mLoss[0m : 1.65162
[1mStep[0m  [32/84], [94mLoss[0m : 1.42641
[1mStep[0m  [40/84], [94mLoss[0m : 1.84569
[1mStep[0m  [48/84], [94mLoss[0m : 1.69942
[1mStep[0m  [56/84], [94mLoss[0m : 1.94995
[1mStep[0m  [64/84], [94mLoss[0m : 1.67260
[1mStep[0m  [72/84], [94mLoss[0m : 1.73142
[1mStep[0m  [80/84], [94mLoss[0m : 1.57149

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56649
[1mStep[0m  [8/84], [94mLoss[0m : 1.91086
[1mStep[0m  [16/84], [94mLoss[0m : 1.58574
[1mStep[0m  [24/84], [94mLoss[0m : 2.01009
[1mStep[0m  [32/84], [94mLoss[0m : 1.70204
[1mStep[0m  [40/84], [94mLoss[0m : 1.90320
[1mStep[0m  [48/84], [94mLoss[0m : 1.83424
[1mStep[0m  [56/84], [94mLoss[0m : 1.73225
[1mStep[0m  [64/84], [94mLoss[0m : 1.80440
[1mStep[0m  [72/84], [94mLoss[0m : 1.90553
[1mStep[0m  [80/84], [94mLoss[0m : 1.72700

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77803
[1mStep[0m  [8/84], [94mLoss[0m : 1.76430
[1mStep[0m  [16/84], [94mLoss[0m : 1.78779
[1mStep[0m  [24/84], [94mLoss[0m : 1.63327
[1mStep[0m  [32/84], [94mLoss[0m : 1.76691
[1mStep[0m  [40/84], [94mLoss[0m : 1.60746
[1mStep[0m  [48/84], [94mLoss[0m : 1.72161
[1mStep[0m  [56/84], [94mLoss[0m : 1.92264
[1mStep[0m  [64/84], [94mLoss[0m : 1.44844
[1mStep[0m  [72/84], [94mLoss[0m : 1.52748
[1mStep[0m  [80/84], [94mLoss[0m : 2.13516

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57677
[1mStep[0m  [8/84], [94mLoss[0m : 1.51449
[1mStep[0m  [16/84], [94mLoss[0m : 1.76877
[1mStep[0m  [24/84], [94mLoss[0m : 1.76584
[1mStep[0m  [32/84], [94mLoss[0m : 1.87162
[1mStep[0m  [40/84], [94mLoss[0m : 1.69787
[1mStep[0m  [48/84], [94mLoss[0m : 1.68155
[1mStep[0m  [56/84], [94mLoss[0m : 1.76660
[1mStep[0m  [64/84], [94mLoss[0m : 1.46996
[1mStep[0m  [72/84], [94mLoss[0m : 1.48500
[1mStep[0m  [80/84], [94mLoss[0m : 1.65814

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.535, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71138
[1mStep[0m  [8/84], [94mLoss[0m : 1.83536
[1mStep[0m  [16/84], [94mLoss[0m : 1.54746
[1mStep[0m  [24/84], [94mLoss[0m : 1.54885
[1mStep[0m  [32/84], [94mLoss[0m : 1.57987
[1mStep[0m  [40/84], [94mLoss[0m : 1.77958
[1mStep[0m  [48/84], [94mLoss[0m : 1.73163
[1mStep[0m  [56/84], [94mLoss[0m : 1.76200
[1mStep[0m  [64/84], [94mLoss[0m : 1.63284
[1mStep[0m  [72/84], [94mLoss[0m : 1.59281
[1mStep[0m  [80/84], [94mLoss[0m : 1.69207

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.573, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48185
[1mStep[0m  [8/84], [94mLoss[0m : 1.47620
[1mStep[0m  [16/84], [94mLoss[0m : 1.60710
[1mStep[0m  [24/84], [94mLoss[0m : 1.52319
[1mStep[0m  [32/84], [94mLoss[0m : 1.75073
[1mStep[0m  [40/84], [94mLoss[0m : 1.79329
[1mStep[0m  [48/84], [94mLoss[0m : 1.64464
[1mStep[0m  [56/84], [94mLoss[0m : 1.59454
[1mStep[0m  [64/84], [94mLoss[0m : 1.68411
[1mStep[0m  [72/84], [94mLoss[0m : 1.61621
[1mStep[0m  [80/84], [94mLoss[0m : 1.55125

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57130
[1mStep[0m  [8/84], [94mLoss[0m : 1.37340
[1mStep[0m  [16/84], [94mLoss[0m : 1.59766
[1mStep[0m  [24/84], [94mLoss[0m : 1.58758
[1mStep[0m  [32/84], [94mLoss[0m : 1.76336
[1mStep[0m  [40/84], [94mLoss[0m : 1.70744
[1mStep[0m  [48/84], [94mLoss[0m : 1.67177
[1mStep[0m  [56/84], [94mLoss[0m : 1.65093
[1mStep[0m  [64/84], [94mLoss[0m : 1.51910
[1mStep[0m  [72/84], [94mLoss[0m : 1.56689
[1mStep[0m  [80/84], [94mLoss[0m : 1.89222

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64244
[1mStep[0m  [8/84], [94mLoss[0m : 1.40236
[1mStep[0m  [16/84], [94mLoss[0m : 1.45460
[1mStep[0m  [24/84], [94mLoss[0m : 1.57309
[1mStep[0m  [32/84], [94mLoss[0m : 1.35007
[1mStep[0m  [40/84], [94mLoss[0m : 1.67443
[1mStep[0m  [48/84], [94mLoss[0m : 1.42807
[1mStep[0m  [56/84], [94mLoss[0m : 1.69547
[1mStep[0m  [64/84], [94mLoss[0m : 1.49648
[1mStep[0m  [72/84], [94mLoss[0m : 1.58110
[1mStep[0m  [80/84], [94mLoss[0m : 1.68740

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.573, [92mTest[0m: 2.527, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57748
[1mStep[0m  [8/84], [94mLoss[0m : 1.60483
[1mStep[0m  [16/84], [94mLoss[0m : 1.19701
[1mStep[0m  [24/84], [94mLoss[0m : 1.23532
[1mStep[0m  [32/84], [94mLoss[0m : 1.58672
[1mStep[0m  [40/84], [94mLoss[0m : 1.49831
[1mStep[0m  [48/84], [94mLoss[0m : 1.51534
[1mStep[0m  [56/84], [94mLoss[0m : 1.62444
[1mStep[0m  [64/84], [94mLoss[0m : 1.41919
[1mStep[0m  [72/84], [94mLoss[0m : 1.57089
[1mStep[0m  [80/84], [94mLoss[0m : 1.39928

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.531, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75332
[1mStep[0m  [8/84], [94mLoss[0m : 1.45392
[1mStep[0m  [16/84], [94mLoss[0m : 1.39034
[1mStep[0m  [24/84], [94mLoss[0m : 1.33083
[1mStep[0m  [32/84], [94mLoss[0m : 1.44189
[1mStep[0m  [40/84], [94mLoss[0m : 1.55635
[1mStep[0m  [48/84], [94mLoss[0m : 1.42316
[1mStep[0m  [56/84], [94mLoss[0m : 1.65199
[1mStep[0m  [64/84], [94mLoss[0m : 1.80063
[1mStep[0m  [72/84], [94mLoss[0m : 1.53317
[1mStep[0m  [80/84], [94mLoss[0m : 1.59091

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.510, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32354
[1mStep[0m  [8/84], [94mLoss[0m : 1.56546
[1mStep[0m  [16/84], [94mLoss[0m : 1.47678
[1mStep[0m  [24/84], [94mLoss[0m : 1.47076
[1mStep[0m  [32/84], [94mLoss[0m : 1.57478
[1mStep[0m  [40/84], [94mLoss[0m : 1.45303
[1mStep[0m  [48/84], [94mLoss[0m : 1.54684
[1mStep[0m  [56/84], [94mLoss[0m : 1.36154
[1mStep[0m  [64/84], [94mLoss[0m : 1.54455
[1mStep[0m  [72/84], [94mLoss[0m : 1.58274
[1mStep[0m  [80/84], [94mLoss[0m : 1.56970

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.559, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53882
[1mStep[0m  [8/84], [94mLoss[0m : 1.38153
[1mStep[0m  [16/84], [94mLoss[0m : 1.34420
[1mStep[0m  [24/84], [94mLoss[0m : 1.39924
[1mStep[0m  [32/84], [94mLoss[0m : 1.42380
[1mStep[0m  [40/84], [94mLoss[0m : 1.60432
[1mStep[0m  [48/84], [94mLoss[0m : 1.44844
[1mStep[0m  [56/84], [94mLoss[0m : 1.42933
[1mStep[0m  [64/84], [94mLoss[0m : 1.36958
[1mStep[0m  [72/84], [94mLoss[0m : 1.23163
[1mStep[0m  [80/84], [94mLoss[0m : 1.32661

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.544, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30201
[1mStep[0m  [8/84], [94mLoss[0m : 1.48905
[1mStep[0m  [16/84], [94mLoss[0m : 1.47697
[1mStep[0m  [24/84], [94mLoss[0m : 1.23508
[1mStep[0m  [32/84], [94mLoss[0m : 1.35632
[1mStep[0m  [40/84], [94mLoss[0m : 1.20805
[1mStep[0m  [48/84], [94mLoss[0m : 1.29164
[1mStep[0m  [56/84], [94mLoss[0m : 1.50283
[1mStep[0m  [64/84], [94mLoss[0m : 1.57813
[1mStep[0m  [72/84], [94mLoss[0m : 1.35057
[1mStep[0m  [80/84], [94mLoss[0m : 1.33304

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.407, [92mTest[0m: 2.525, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.555
====================================

Phase 2 - Evaluation MAE:  2.5550832407815114
MAE score P1       2.305399
MAE score P2       2.555083
loss               1.407227
learning_rate          0.01
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay          0.001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.08908
[1mStep[0m  [8/84], [94mLoss[0m : 10.67030
[1mStep[0m  [16/84], [94mLoss[0m : 10.77376
[1mStep[0m  [24/84], [94mLoss[0m : 10.01497
[1mStep[0m  [32/84], [94mLoss[0m : 9.63589
[1mStep[0m  [40/84], [94mLoss[0m : 9.83737
[1mStep[0m  [48/84], [94mLoss[0m : 10.11794
[1mStep[0m  [56/84], [94mLoss[0m : 9.61828
[1mStep[0m  [64/84], [94mLoss[0m : 9.99087
[1mStep[0m  [72/84], [94mLoss[0m : 8.60253
[1mStep[0m  [80/84], [94mLoss[0m : 8.21933

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.904, [92mTest[0m: 11.029, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.69458
[1mStep[0m  [8/84], [94mLoss[0m : 7.97904
[1mStep[0m  [16/84], [94mLoss[0m : 8.07287
[1mStep[0m  [24/84], [94mLoss[0m : 7.01294
[1mStep[0m  [32/84], [94mLoss[0m : 6.16123
[1mStep[0m  [40/84], [94mLoss[0m : 6.80681
[1mStep[0m  [48/84], [94mLoss[0m : 6.33526
[1mStep[0m  [56/84], [94mLoss[0m : 5.43842
[1mStep[0m  [64/84], [94mLoss[0m : 5.50664
[1mStep[0m  [72/84], [94mLoss[0m : 4.85289
[1mStep[0m  [80/84], [94mLoss[0m : 4.77897

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.488, [92mTest[0m: 8.073, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.99413
[1mStep[0m  [8/84], [94mLoss[0m : 4.33458
[1mStep[0m  [16/84], [94mLoss[0m : 3.69038
[1mStep[0m  [24/84], [94mLoss[0m : 3.23512
[1mStep[0m  [32/84], [94mLoss[0m : 2.72602
[1mStep[0m  [40/84], [94mLoss[0m : 2.78221
[1mStep[0m  [48/84], [94mLoss[0m : 2.45681
[1mStep[0m  [56/84], [94mLoss[0m : 2.70279
[1mStep[0m  [64/84], [94mLoss[0m : 2.81835
[1mStep[0m  [72/84], [94mLoss[0m : 2.67771
[1mStep[0m  [80/84], [94mLoss[0m : 2.78793

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.117, [92mTest[0m: 3.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89382
[1mStep[0m  [8/84], [94mLoss[0m : 2.74125
[1mStep[0m  [16/84], [94mLoss[0m : 2.35176
[1mStep[0m  [24/84], [94mLoss[0m : 2.69321
[1mStep[0m  [32/84], [94mLoss[0m : 2.43375
[1mStep[0m  [40/84], [94mLoss[0m : 2.52408
[1mStep[0m  [48/84], [94mLoss[0m : 2.68071
[1mStep[0m  [56/84], [94mLoss[0m : 2.48619
[1mStep[0m  [64/84], [94mLoss[0m : 2.58737
[1mStep[0m  [72/84], [94mLoss[0m : 2.70595
[1mStep[0m  [80/84], [94mLoss[0m : 2.75320

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75195
[1mStep[0m  [8/84], [94mLoss[0m : 2.24818
[1mStep[0m  [16/84], [94mLoss[0m : 2.76093
[1mStep[0m  [24/84], [94mLoss[0m : 2.68058
[1mStep[0m  [32/84], [94mLoss[0m : 2.58239
[1mStep[0m  [40/84], [94mLoss[0m : 2.56226
[1mStep[0m  [48/84], [94mLoss[0m : 2.43748
[1mStep[0m  [56/84], [94mLoss[0m : 2.20499
[1mStep[0m  [64/84], [94mLoss[0m : 2.72143
[1mStep[0m  [72/84], [94mLoss[0m : 2.47143
[1mStep[0m  [80/84], [94mLoss[0m : 2.73040

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75874
[1mStep[0m  [8/84], [94mLoss[0m : 2.79570
[1mStep[0m  [16/84], [94mLoss[0m : 2.04137
[1mStep[0m  [24/84], [94mLoss[0m : 2.44715
[1mStep[0m  [32/84], [94mLoss[0m : 2.81407
[1mStep[0m  [40/84], [94mLoss[0m : 2.45357
[1mStep[0m  [48/84], [94mLoss[0m : 2.63565
[1mStep[0m  [56/84], [94mLoss[0m : 3.03453
[1mStep[0m  [64/84], [94mLoss[0m : 2.57963
[1mStep[0m  [72/84], [94mLoss[0m : 2.70419
[1mStep[0m  [80/84], [94mLoss[0m : 2.94901

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58559
[1mStep[0m  [8/84], [94mLoss[0m : 2.28004
[1mStep[0m  [16/84], [94mLoss[0m : 2.39323
[1mStep[0m  [24/84], [94mLoss[0m : 2.62870
[1mStep[0m  [32/84], [94mLoss[0m : 2.49619
[1mStep[0m  [40/84], [94mLoss[0m : 2.78861
[1mStep[0m  [48/84], [94mLoss[0m : 2.69024
[1mStep[0m  [56/84], [94mLoss[0m : 2.62872
[1mStep[0m  [64/84], [94mLoss[0m : 2.24534
[1mStep[0m  [72/84], [94mLoss[0m : 2.52066
[1mStep[0m  [80/84], [94mLoss[0m : 2.55723

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40592
[1mStep[0m  [8/84], [94mLoss[0m : 2.09020
[1mStep[0m  [16/84], [94mLoss[0m : 2.55388
[1mStep[0m  [24/84], [94mLoss[0m : 2.49737
[1mStep[0m  [32/84], [94mLoss[0m : 2.54695
[1mStep[0m  [40/84], [94mLoss[0m : 2.76519
[1mStep[0m  [48/84], [94mLoss[0m : 2.56296
[1mStep[0m  [56/84], [94mLoss[0m : 2.54287
[1mStep[0m  [64/84], [94mLoss[0m : 2.69021
[1mStep[0m  [72/84], [94mLoss[0m : 2.25936
[1mStep[0m  [80/84], [94mLoss[0m : 2.62576

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.01325
[1mStep[0m  [8/84], [94mLoss[0m : 2.55699
[1mStep[0m  [16/84], [94mLoss[0m : 2.76581
[1mStep[0m  [24/84], [94mLoss[0m : 2.51718
[1mStep[0m  [32/84], [94mLoss[0m : 2.39369
[1mStep[0m  [40/84], [94mLoss[0m : 2.51160
[1mStep[0m  [48/84], [94mLoss[0m : 2.36339
[1mStep[0m  [56/84], [94mLoss[0m : 2.67663
[1mStep[0m  [64/84], [94mLoss[0m : 2.25531
[1mStep[0m  [72/84], [94mLoss[0m : 2.48239
[1mStep[0m  [80/84], [94mLoss[0m : 2.51394

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51471
[1mStep[0m  [8/84], [94mLoss[0m : 2.59881
[1mStep[0m  [16/84], [94mLoss[0m : 2.57970
[1mStep[0m  [24/84], [94mLoss[0m : 2.60189
[1mStep[0m  [32/84], [94mLoss[0m : 2.58316
[1mStep[0m  [40/84], [94mLoss[0m : 2.39016
[1mStep[0m  [48/84], [94mLoss[0m : 2.63517
[1mStep[0m  [56/84], [94mLoss[0m : 2.72450
[1mStep[0m  [64/84], [94mLoss[0m : 2.37671
[1mStep[0m  [72/84], [94mLoss[0m : 2.78599
[1mStep[0m  [80/84], [94mLoss[0m : 2.47233

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70315
[1mStep[0m  [8/84], [94mLoss[0m : 2.16248
[1mStep[0m  [16/84], [94mLoss[0m : 2.62292
[1mStep[0m  [24/84], [94mLoss[0m : 2.53673
[1mStep[0m  [32/84], [94mLoss[0m : 2.56710
[1mStep[0m  [40/84], [94mLoss[0m : 2.20803
[1mStep[0m  [48/84], [94mLoss[0m : 2.47955
[1mStep[0m  [56/84], [94mLoss[0m : 2.93525
[1mStep[0m  [64/84], [94mLoss[0m : 2.39318
[1mStep[0m  [72/84], [94mLoss[0m : 2.40836
[1mStep[0m  [80/84], [94mLoss[0m : 2.40642

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58844
[1mStep[0m  [8/84], [94mLoss[0m : 2.60868
[1mStep[0m  [16/84], [94mLoss[0m : 2.67301
[1mStep[0m  [24/84], [94mLoss[0m : 2.40267
[1mStep[0m  [32/84], [94mLoss[0m : 2.31251
[1mStep[0m  [40/84], [94mLoss[0m : 2.49498
[1mStep[0m  [48/84], [94mLoss[0m : 2.67807
[1mStep[0m  [56/84], [94mLoss[0m : 2.51909
[1mStep[0m  [64/84], [94mLoss[0m : 2.38158
[1mStep[0m  [72/84], [94mLoss[0m : 2.21590
[1mStep[0m  [80/84], [94mLoss[0m : 2.56017

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41214
[1mStep[0m  [8/84], [94mLoss[0m : 2.50111
[1mStep[0m  [16/84], [94mLoss[0m : 2.46101
[1mStep[0m  [24/84], [94mLoss[0m : 2.49731
[1mStep[0m  [32/84], [94mLoss[0m : 2.49083
[1mStep[0m  [40/84], [94mLoss[0m : 2.31494
[1mStep[0m  [48/84], [94mLoss[0m : 2.16361
[1mStep[0m  [56/84], [94mLoss[0m : 2.54338
[1mStep[0m  [64/84], [94mLoss[0m : 2.23286
[1mStep[0m  [72/84], [94mLoss[0m : 2.34221
[1mStep[0m  [80/84], [94mLoss[0m : 2.54861

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47083
[1mStep[0m  [8/84], [94mLoss[0m : 2.37891
[1mStep[0m  [16/84], [94mLoss[0m : 2.49270
[1mStep[0m  [24/84], [94mLoss[0m : 2.73470
[1mStep[0m  [32/84], [94mLoss[0m : 2.56588
[1mStep[0m  [40/84], [94mLoss[0m : 2.69437
[1mStep[0m  [48/84], [94mLoss[0m : 2.28000
[1mStep[0m  [56/84], [94mLoss[0m : 2.42424
[1mStep[0m  [64/84], [94mLoss[0m : 2.67598
[1mStep[0m  [72/84], [94mLoss[0m : 2.66851
[1mStep[0m  [80/84], [94mLoss[0m : 2.46866

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56646
[1mStep[0m  [8/84], [94mLoss[0m : 2.41430
[1mStep[0m  [16/84], [94mLoss[0m : 2.51979
[1mStep[0m  [24/84], [94mLoss[0m : 2.42955
[1mStep[0m  [32/84], [94mLoss[0m : 2.65046
[1mStep[0m  [40/84], [94mLoss[0m : 2.62162
[1mStep[0m  [48/84], [94mLoss[0m : 2.73938
[1mStep[0m  [56/84], [94mLoss[0m : 2.29905
[1mStep[0m  [64/84], [94mLoss[0m : 2.17971
[1mStep[0m  [72/84], [94mLoss[0m : 2.37814
[1mStep[0m  [80/84], [94mLoss[0m : 2.48886

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62587
[1mStep[0m  [8/84], [94mLoss[0m : 2.34909
[1mStep[0m  [16/84], [94mLoss[0m : 2.25717
[1mStep[0m  [24/84], [94mLoss[0m : 2.51353
[1mStep[0m  [32/84], [94mLoss[0m : 2.53562
[1mStep[0m  [40/84], [94mLoss[0m : 2.31064
[1mStep[0m  [48/84], [94mLoss[0m : 2.31067
[1mStep[0m  [56/84], [94mLoss[0m : 2.33924
[1mStep[0m  [64/84], [94mLoss[0m : 2.35353
[1mStep[0m  [72/84], [94mLoss[0m : 2.18222
[1mStep[0m  [80/84], [94mLoss[0m : 2.38289

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44404
[1mStep[0m  [8/84], [94mLoss[0m : 2.55088
[1mStep[0m  [16/84], [94mLoss[0m : 2.77687
[1mStep[0m  [24/84], [94mLoss[0m : 2.59669
[1mStep[0m  [32/84], [94mLoss[0m : 2.43332
[1mStep[0m  [40/84], [94mLoss[0m : 2.41955
[1mStep[0m  [48/84], [94mLoss[0m : 2.37882
[1mStep[0m  [56/84], [94mLoss[0m : 2.64960
[1mStep[0m  [64/84], [94mLoss[0m : 2.38768
[1mStep[0m  [72/84], [94mLoss[0m : 2.31749
[1mStep[0m  [80/84], [94mLoss[0m : 2.30559

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59718
[1mStep[0m  [8/84], [94mLoss[0m : 2.55497
[1mStep[0m  [16/84], [94mLoss[0m : 2.49239
[1mStep[0m  [24/84], [94mLoss[0m : 2.53635
[1mStep[0m  [32/84], [94mLoss[0m : 2.38982
[1mStep[0m  [40/84], [94mLoss[0m : 2.41730
[1mStep[0m  [48/84], [94mLoss[0m : 2.09686
[1mStep[0m  [56/84], [94mLoss[0m : 2.62443
[1mStep[0m  [64/84], [94mLoss[0m : 2.36240
[1mStep[0m  [72/84], [94mLoss[0m : 2.27629
[1mStep[0m  [80/84], [94mLoss[0m : 2.49941

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76627
[1mStep[0m  [8/84], [94mLoss[0m : 2.63343
[1mStep[0m  [16/84], [94mLoss[0m : 2.44528
[1mStep[0m  [24/84], [94mLoss[0m : 2.34995
[1mStep[0m  [32/84], [94mLoss[0m : 2.67525
[1mStep[0m  [40/84], [94mLoss[0m : 2.34160
[1mStep[0m  [48/84], [94mLoss[0m : 2.24612
[1mStep[0m  [56/84], [94mLoss[0m : 2.58377
[1mStep[0m  [64/84], [94mLoss[0m : 2.27391
[1mStep[0m  [72/84], [94mLoss[0m : 2.18244
[1mStep[0m  [80/84], [94mLoss[0m : 2.37186

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40430
[1mStep[0m  [8/84], [94mLoss[0m : 2.54199
[1mStep[0m  [16/84], [94mLoss[0m : 2.58380
[1mStep[0m  [24/84], [94mLoss[0m : 2.45595
[1mStep[0m  [32/84], [94mLoss[0m : 2.59107
[1mStep[0m  [40/84], [94mLoss[0m : 2.68409
[1mStep[0m  [48/84], [94mLoss[0m : 2.45556
[1mStep[0m  [56/84], [94mLoss[0m : 2.57180
[1mStep[0m  [64/84], [94mLoss[0m : 2.58116
[1mStep[0m  [72/84], [94mLoss[0m : 2.51122
[1mStep[0m  [80/84], [94mLoss[0m : 2.25787

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23860
[1mStep[0m  [8/84], [94mLoss[0m : 2.21134
[1mStep[0m  [16/84], [94mLoss[0m : 2.43863
[1mStep[0m  [24/84], [94mLoss[0m : 2.00885
[1mStep[0m  [32/84], [94mLoss[0m : 2.25901
[1mStep[0m  [40/84], [94mLoss[0m : 2.26377
[1mStep[0m  [48/84], [94mLoss[0m : 2.31904
[1mStep[0m  [56/84], [94mLoss[0m : 2.45405
[1mStep[0m  [64/84], [94mLoss[0m : 2.30807
[1mStep[0m  [72/84], [94mLoss[0m : 2.47705
[1mStep[0m  [80/84], [94mLoss[0m : 2.23269

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45231
[1mStep[0m  [8/84], [94mLoss[0m : 2.41355
[1mStep[0m  [16/84], [94mLoss[0m : 2.33021
[1mStep[0m  [24/84], [94mLoss[0m : 2.20420
[1mStep[0m  [32/84], [94mLoss[0m : 2.19817
[1mStep[0m  [40/84], [94mLoss[0m : 2.09625
[1mStep[0m  [48/84], [94mLoss[0m : 2.24277
[1mStep[0m  [56/84], [94mLoss[0m : 2.23229
[1mStep[0m  [64/84], [94mLoss[0m : 2.43400
[1mStep[0m  [72/84], [94mLoss[0m : 2.53346
[1mStep[0m  [80/84], [94mLoss[0m : 2.44582

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04408
[1mStep[0m  [8/84], [94mLoss[0m : 2.25942
[1mStep[0m  [16/84], [94mLoss[0m : 2.30135
[1mStep[0m  [24/84], [94mLoss[0m : 2.48918
[1mStep[0m  [32/84], [94mLoss[0m : 2.48512
[1mStep[0m  [40/84], [94mLoss[0m : 1.98203
[1mStep[0m  [48/84], [94mLoss[0m : 2.21284
[1mStep[0m  [56/84], [94mLoss[0m : 2.59980
[1mStep[0m  [64/84], [94mLoss[0m : 2.37827
[1mStep[0m  [72/84], [94mLoss[0m : 2.39816
[1mStep[0m  [80/84], [94mLoss[0m : 2.26081

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32821
[1mStep[0m  [8/84], [94mLoss[0m : 2.24109
[1mStep[0m  [16/84], [94mLoss[0m : 2.18065
[1mStep[0m  [24/84], [94mLoss[0m : 2.39696
[1mStep[0m  [32/84], [94mLoss[0m : 2.59971
[1mStep[0m  [40/84], [94mLoss[0m : 2.47586
[1mStep[0m  [48/84], [94mLoss[0m : 2.20041
[1mStep[0m  [56/84], [94mLoss[0m : 2.62863
[1mStep[0m  [64/84], [94mLoss[0m : 2.41576
[1mStep[0m  [72/84], [94mLoss[0m : 2.35701
[1mStep[0m  [80/84], [94mLoss[0m : 2.21296

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.310, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21278
[1mStep[0m  [8/84], [94mLoss[0m : 2.38668
[1mStep[0m  [16/84], [94mLoss[0m : 2.41758
[1mStep[0m  [24/84], [94mLoss[0m : 2.06776
[1mStep[0m  [32/84], [94mLoss[0m : 2.45354
[1mStep[0m  [40/84], [94mLoss[0m : 2.44658
[1mStep[0m  [48/84], [94mLoss[0m : 2.44464
[1mStep[0m  [56/84], [94mLoss[0m : 2.56317
[1mStep[0m  [64/84], [94mLoss[0m : 2.33963
[1mStep[0m  [72/84], [94mLoss[0m : 2.10115
[1mStep[0m  [80/84], [94mLoss[0m : 2.56310

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40407
[1mStep[0m  [8/84], [94mLoss[0m : 2.33566
[1mStep[0m  [16/84], [94mLoss[0m : 2.67944
[1mStep[0m  [24/84], [94mLoss[0m : 2.53455
[1mStep[0m  [32/84], [94mLoss[0m : 2.18463
[1mStep[0m  [40/84], [94mLoss[0m : 2.56877
[1mStep[0m  [48/84], [94mLoss[0m : 2.53476
[1mStep[0m  [56/84], [94mLoss[0m : 2.46232
[1mStep[0m  [64/84], [94mLoss[0m : 2.39416
[1mStep[0m  [72/84], [94mLoss[0m : 2.36440
[1mStep[0m  [80/84], [94mLoss[0m : 2.28248

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72641
[1mStep[0m  [8/84], [94mLoss[0m : 2.35366
[1mStep[0m  [16/84], [94mLoss[0m : 2.37267
[1mStep[0m  [24/84], [94mLoss[0m : 2.10535
[1mStep[0m  [32/84], [94mLoss[0m : 2.22510
[1mStep[0m  [40/84], [94mLoss[0m : 2.59504
[1mStep[0m  [48/84], [94mLoss[0m : 2.45239
[1mStep[0m  [56/84], [94mLoss[0m : 2.41894
[1mStep[0m  [64/84], [94mLoss[0m : 2.79183
[1mStep[0m  [72/84], [94mLoss[0m : 2.37446
[1mStep[0m  [80/84], [94mLoss[0m : 2.45004

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33002
[1mStep[0m  [8/84], [94mLoss[0m : 2.17728
[1mStep[0m  [16/84], [94mLoss[0m : 2.31106
[1mStep[0m  [24/84], [94mLoss[0m : 2.35278
[1mStep[0m  [32/84], [94mLoss[0m : 2.26345
[1mStep[0m  [40/84], [94mLoss[0m : 2.47929
[1mStep[0m  [48/84], [94mLoss[0m : 2.47215
[1mStep[0m  [56/84], [94mLoss[0m : 2.36402
[1mStep[0m  [64/84], [94mLoss[0m : 2.05872
[1mStep[0m  [72/84], [94mLoss[0m : 2.00928
[1mStep[0m  [80/84], [94mLoss[0m : 2.41299

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47712
[1mStep[0m  [8/84], [94mLoss[0m : 2.20964
[1mStep[0m  [16/84], [94mLoss[0m : 2.54219
[1mStep[0m  [24/84], [94mLoss[0m : 2.32920
[1mStep[0m  [32/84], [94mLoss[0m : 2.37894
[1mStep[0m  [40/84], [94mLoss[0m : 2.19649
[1mStep[0m  [48/84], [94mLoss[0m : 2.14858
[1mStep[0m  [56/84], [94mLoss[0m : 2.71641
[1mStep[0m  [64/84], [94mLoss[0m : 2.42790
[1mStep[0m  [72/84], [94mLoss[0m : 2.41110
[1mStep[0m  [80/84], [94mLoss[0m : 2.24152

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35421
[1mStep[0m  [8/84], [94mLoss[0m : 2.41481
[1mStep[0m  [16/84], [94mLoss[0m : 2.34875
[1mStep[0m  [24/84], [94mLoss[0m : 2.35591
[1mStep[0m  [32/84], [94mLoss[0m : 2.43693
[1mStep[0m  [40/84], [94mLoss[0m : 2.63589
[1mStep[0m  [48/84], [94mLoss[0m : 2.23989
[1mStep[0m  [56/84], [94mLoss[0m : 2.22673
[1mStep[0m  [64/84], [94mLoss[0m : 2.62992
[1mStep[0m  [72/84], [94mLoss[0m : 2.53125
[1mStep[0m  [80/84], [94mLoss[0m : 2.08778

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.332480932985033
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.25818
[1mStep[0m  [8/84], [94mLoss[0m : 2.50036
[1mStep[0m  [16/84], [94mLoss[0m : 2.34315
[1mStep[0m  [24/84], [94mLoss[0m : 2.37231
[1mStep[0m  [32/84], [94mLoss[0m : 2.57200
[1mStep[0m  [40/84], [94mLoss[0m : 2.73047
[1mStep[0m  [48/84], [94mLoss[0m : 2.18029
[1mStep[0m  [56/84], [94mLoss[0m : 2.28496
[1mStep[0m  [64/84], [94mLoss[0m : 2.75319
[1mStep[0m  [72/84], [94mLoss[0m : 2.61769
[1mStep[0m  [80/84], [94mLoss[0m : 2.73249

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54474
[1mStep[0m  [8/84], [94mLoss[0m : 2.23368
[1mStep[0m  [16/84], [94mLoss[0m : 2.36849
[1mStep[0m  [24/84], [94mLoss[0m : 2.36298
[1mStep[0m  [32/84], [94mLoss[0m : 2.26311
[1mStep[0m  [40/84], [94mLoss[0m : 2.11345
[1mStep[0m  [48/84], [94mLoss[0m : 2.69438
[1mStep[0m  [56/84], [94mLoss[0m : 2.33887
[1mStep[0m  [64/84], [94mLoss[0m : 2.18561
[1mStep[0m  [72/84], [94mLoss[0m : 2.39720
[1mStep[0m  [80/84], [94mLoss[0m : 2.35634

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24486
[1mStep[0m  [8/84], [94mLoss[0m : 2.42627
[1mStep[0m  [16/84], [94mLoss[0m : 2.26096
[1mStep[0m  [24/84], [94mLoss[0m : 2.39618
[1mStep[0m  [32/84], [94mLoss[0m : 2.10784
[1mStep[0m  [40/84], [94mLoss[0m : 2.38173
[1mStep[0m  [48/84], [94mLoss[0m : 2.37908
[1mStep[0m  [56/84], [94mLoss[0m : 2.42192
[1mStep[0m  [64/84], [94mLoss[0m : 2.43122
[1mStep[0m  [72/84], [94mLoss[0m : 2.31581
[1mStep[0m  [80/84], [94mLoss[0m : 2.27962

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30598
[1mStep[0m  [8/84], [94mLoss[0m : 2.15878
[1mStep[0m  [16/84], [94mLoss[0m : 1.95062
[1mStep[0m  [24/84], [94mLoss[0m : 2.04337
[1mStep[0m  [32/84], [94mLoss[0m : 2.05948
[1mStep[0m  [40/84], [94mLoss[0m : 2.49693
[1mStep[0m  [48/84], [94mLoss[0m : 2.27874
[1mStep[0m  [56/84], [94mLoss[0m : 2.21631
[1mStep[0m  [64/84], [94mLoss[0m : 2.54881
[1mStep[0m  [72/84], [94mLoss[0m : 2.09845
[1mStep[0m  [80/84], [94mLoss[0m : 2.35941

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38459
[1mStep[0m  [8/84], [94mLoss[0m : 2.11365
[1mStep[0m  [16/84], [94mLoss[0m : 2.35841
[1mStep[0m  [24/84], [94mLoss[0m : 2.40008
[1mStep[0m  [32/84], [94mLoss[0m : 2.23646
[1mStep[0m  [40/84], [94mLoss[0m : 2.05625
[1mStep[0m  [48/84], [94mLoss[0m : 2.13887
[1mStep[0m  [56/84], [94mLoss[0m : 1.95576
[1mStep[0m  [64/84], [94mLoss[0m : 2.28467
[1mStep[0m  [72/84], [94mLoss[0m : 2.14710
[1mStep[0m  [80/84], [94mLoss[0m : 2.10210

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01474
[1mStep[0m  [8/84], [94mLoss[0m : 1.93364
[1mStep[0m  [16/84], [94mLoss[0m : 2.23380
[1mStep[0m  [24/84], [94mLoss[0m : 2.25058
[1mStep[0m  [32/84], [94mLoss[0m : 2.12973
[1mStep[0m  [40/84], [94mLoss[0m : 1.88203
[1mStep[0m  [48/84], [94mLoss[0m : 2.09844
[1mStep[0m  [56/84], [94mLoss[0m : 2.06189
[1mStep[0m  [64/84], [94mLoss[0m : 2.14033
[1mStep[0m  [72/84], [94mLoss[0m : 2.34066
[1mStep[0m  [80/84], [94mLoss[0m : 2.28329

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08936
[1mStep[0m  [8/84], [94mLoss[0m : 2.05912
[1mStep[0m  [16/84], [94mLoss[0m : 1.61515
[1mStep[0m  [24/84], [94mLoss[0m : 2.18506
[1mStep[0m  [32/84], [94mLoss[0m : 2.11138
[1mStep[0m  [40/84], [94mLoss[0m : 2.15085
[1mStep[0m  [48/84], [94mLoss[0m : 2.18171
[1mStep[0m  [56/84], [94mLoss[0m : 2.24097
[1mStep[0m  [64/84], [94mLoss[0m : 2.15802
[1mStep[0m  [72/84], [94mLoss[0m : 2.28004
[1mStep[0m  [80/84], [94mLoss[0m : 2.03175

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06187
[1mStep[0m  [8/84], [94mLoss[0m : 1.72876
[1mStep[0m  [16/84], [94mLoss[0m : 1.82539
[1mStep[0m  [24/84], [94mLoss[0m : 1.87207
[1mStep[0m  [32/84], [94mLoss[0m : 2.07563
[1mStep[0m  [40/84], [94mLoss[0m : 1.70462
[1mStep[0m  [48/84], [94mLoss[0m : 1.86776
[1mStep[0m  [56/84], [94mLoss[0m : 2.26316
[1mStep[0m  [64/84], [94mLoss[0m : 1.94085
[1mStep[0m  [72/84], [94mLoss[0m : 2.12499
[1mStep[0m  [80/84], [94mLoss[0m : 2.01737

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96942
[1mStep[0m  [8/84], [94mLoss[0m : 1.92867
[1mStep[0m  [16/84], [94mLoss[0m : 2.21660
[1mStep[0m  [24/84], [94mLoss[0m : 2.07574
[1mStep[0m  [32/84], [94mLoss[0m : 1.85063
[1mStep[0m  [40/84], [94mLoss[0m : 1.73288
[1mStep[0m  [48/84], [94mLoss[0m : 1.84256
[1mStep[0m  [56/84], [94mLoss[0m : 1.88685
[1mStep[0m  [64/84], [94mLoss[0m : 1.90456
[1mStep[0m  [72/84], [94mLoss[0m : 2.08587
[1mStep[0m  [80/84], [94mLoss[0m : 2.02140

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58646
[1mStep[0m  [8/84], [94mLoss[0m : 1.93750
[1mStep[0m  [16/84], [94mLoss[0m : 1.81058
[1mStep[0m  [24/84], [94mLoss[0m : 1.92251
[1mStep[0m  [32/84], [94mLoss[0m : 1.78792
[1mStep[0m  [40/84], [94mLoss[0m : 1.90065
[1mStep[0m  [48/84], [94mLoss[0m : 1.86658
[1mStep[0m  [56/84], [94mLoss[0m : 1.79088
[1mStep[0m  [64/84], [94mLoss[0m : 2.03426
[1mStep[0m  [72/84], [94mLoss[0m : 1.98738
[1mStep[0m  [80/84], [94mLoss[0m : 2.02135

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63751
[1mStep[0m  [8/84], [94mLoss[0m : 1.70724
[1mStep[0m  [16/84], [94mLoss[0m : 1.86437
[1mStep[0m  [24/84], [94mLoss[0m : 1.91589
[1mStep[0m  [32/84], [94mLoss[0m : 1.87879
[1mStep[0m  [40/84], [94mLoss[0m : 1.45752
[1mStep[0m  [48/84], [94mLoss[0m : 2.19064
[1mStep[0m  [56/84], [94mLoss[0m : 1.94318
[1mStep[0m  [64/84], [94mLoss[0m : 1.89700
[1mStep[0m  [72/84], [94mLoss[0m : 1.95260
[1mStep[0m  [80/84], [94mLoss[0m : 1.72224

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67569
[1mStep[0m  [8/84], [94mLoss[0m : 1.63394
[1mStep[0m  [16/84], [94mLoss[0m : 1.79300
[1mStep[0m  [24/84], [94mLoss[0m : 1.70894
[1mStep[0m  [32/84], [94mLoss[0m : 2.04502
[1mStep[0m  [40/84], [94mLoss[0m : 1.63001
[1mStep[0m  [48/84], [94mLoss[0m : 2.10298
[1mStep[0m  [56/84], [94mLoss[0m : 1.86890
[1mStep[0m  [64/84], [94mLoss[0m : 1.72658
[1mStep[0m  [72/84], [94mLoss[0m : 1.58524
[1mStep[0m  [80/84], [94mLoss[0m : 1.72527

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74153
[1mStep[0m  [8/84], [94mLoss[0m : 1.51856
[1mStep[0m  [16/84], [94mLoss[0m : 1.56944
[1mStep[0m  [24/84], [94mLoss[0m : 1.72383
[1mStep[0m  [32/84], [94mLoss[0m : 1.85073
[1mStep[0m  [40/84], [94mLoss[0m : 1.70540
[1mStep[0m  [48/84], [94mLoss[0m : 1.70207
[1mStep[0m  [56/84], [94mLoss[0m : 1.59245
[1mStep[0m  [64/84], [94mLoss[0m : 1.82822
[1mStep[0m  [72/84], [94mLoss[0m : 1.79070
[1mStep[0m  [80/84], [94mLoss[0m : 1.61825

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65704
[1mStep[0m  [8/84], [94mLoss[0m : 1.70245
[1mStep[0m  [16/84], [94mLoss[0m : 1.63759
[1mStep[0m  [24/84], [94mLoss[0m : 1.73471
[1mStep[0m  [32/84], [94mLoss[0m : 1.80554
[1mStep[0m  [40/84], [94mLoss[0m : 1.38883
[1mStep[0m  [48/84], [94mLoss[0m : 1.67479
[1mStep[0m  [56/84], [94mLoss[0m : 1.79430
[1mStep[0m  [64/84], [94mLoss[0m : 1.73325
[1mStep[0m  [72/84], [94mLoss[0m : 1.56914
[1mStep[0m  [80/84], [94mLoss[0m : 1.72508

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72944
[1mStep[0m  [8/84], [94mLoss[0m : 1.67055
[1mStep[0m  [16/84], [94mLoss[0m : 1.81846
[1mStep[0m  [24/84], [94mLoss[0m : 1.52498
[1mStep[0m  [32/84], [94mLoss[0m : 1.57070
[1mStep[0m  [40/84], [94mLoss[0m : 1.67769
[1mStep[0m  [48/84], [94mLoss[0m : 1.56882
[1mStep[0m  [56/84], [94mLoss[0m : 1.74789
[1mStep[0m  [64/84], [94mLoss[0m : 1.70612
[1mStep[0m  [72/84], [94mLoss[0m : 1.64210
[1mStep[0m  [80/84], [94mLoss[0m : 1.67035

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69981
[1mStep[0m  [8/84], [94mLoss[0m : 1.60351
[1mStep[0m  [16/84], [94mLoss[0m : 1.65921
[1mStep[0m  [24/84], [94mLoss[0m : 1.51597
[1mStep[0m  [32/84], [94mLoss[0m : 1.60124
[1mStep[0m  [40/84], [94mLoss[0m : 1.55525
[1mStep[0m  [48/84], [94mLoss[0m : 1.77623
[1mStep[0m  [56/84], [94mLoss[0m : 1.64913
[1mStep[0m  [64/84], [94mLoss[0m : 1.49204
[1mStep[0m  [72/84], [94mLoss[0m : 1.58504
[1mStep[0m  [80/84], [94mLoss[0m : 1.81197

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68394
[1mStep[0m  [8/84], [94mLoss[0m : 1.48186
[1mStep[0m  [16/84], [94mLoss[0m : 1.39345
[1mStep[0m  [24/84], [94mLoss[0m : 1.63804
[1mStep[0m  [32/84], [94mLoss[0m : 1.73445
[1mStep[0m  [40/84], [94mLoss[0m : 1.70723
[1mStep[0m  [48/84], [94mLoss[0m : 1.45094
[1mStep[0m  [56/84], [94mLoss[0m : 1.58304
[1mStep[0m  [64/84], [94mLoss[0m : 1.46907
[1mStep[0m  [72/84], [94mLoss[0m : 1.68029
[1mStep[0m  [80/84], [94mLoss[0m : 1.52161

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52039
[1mStep[0m  [8/84], [94mLoss[0m : 1.45242
[1mStep[0m  [16/84], [94mLoss[0m : 1.58691
[1mStep[0m  [24/84], [94mLoss[0m : 1.62613
[1mStep[0m  [32/84], [94mLoss[0m : 1.42222
[1mStep[0m  [40/84], [94mLoss[0m : 1.55419
[1mStep[0m  [48/84], [94mLoss[0m : 1.52679
[1mStep[0m  [56/84], [94mLoss[0m : 1.28015
[1mStep[0m  [64/84], [94mLoss[0m : 1.41292
[1mStep[0m  [72/84], [94mLoss[0m : 1.73259
[1mStep[0m  [80/84], [94mLoss[0m : 1.58567

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49820
[1mStep[0m  [8/84], [94mLoss[0m : 1.42412
[1mStep[0m  [16/84], [94mLoss[0m : 1.43729
[1mStep[0m  [24/84], [94mLoss[0m : 1.56612
[1mStep[0m  [32/84], [94mLoss[0m : 1.52054
[1mStep[0m  [40/84], [94mLoss[0m : 1.38875
[1mStep[0m  [48/84], [94mLoss[0m : 1.54966
[1mStep[0m  [56/84], [94mLoss[0m : 1.56008
[1mStep[0m  [64/84], [94mLoss[0m : 1.57969
[1mStep[0m  [72/84], [94mLoss[0m : 1.55105
[1mStep[0m  [80/84], [94mLoss[0m : 1.56006

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51525
[1mStep[0m  [8/84], [94mLoss[0m : 1.46770
[1mStep[0m  [16/84], [94mLoss[0m : 1.51154
[1mStep[0m  [24/84], [94mLoss[0m : 1.49203
[1mStep[0m  [32/84], [94mLoss[0m : 1.62521
[1mStep[0m  [40/84], [94mLoss[0m : 1.41723
[1mStep[0m  [48/84], [94mLoss[0m : 1.59901
[1mStep[0m  [56/84], [94mLoss[0m : 1.54718
[1mStep[0m  [64/84], [94mLoss[0m : 1.45270
[1mStep[0m  [72/84], [94mLoss[0m : 1.42593
[1mStep[0m  [80/84], [94mLoss[0m : 1.49374

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.500, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36650
[1mStep[0m  [8/84], [94mLoss[0m : 1.36290
[1mStep[0m  [16/84], [94mLoss[0m : 1.41517
[1mStep[0m  [24/84], [94mLoss[0m : 1.46357
[1mStep[0m  [32/84], [94mLoss[0m : 1.26321
[1mStep[0m  [40/84], [94mLoss[0m : 1.28591
[1mStep[0m  [48/84], [94mLoss[0m : 1.20894
[1mStep[0m  [56/84], [94mLoss[0m : 1.25382
[1mStep[0m  [64/84], [94mLoss[0m : 1.29804
[1mStep[0m  [72/84], [94mLoss[0m : 1.56079
[1mStep[0m  [80/84], [94mLoss[0m : 1.56467

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.438, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.23900
[1mStep[0m  [8/84], [94mLoss[0m : 1.33260
[1mStep[0m  [16/84], [94mLoss[0m : 1.25238
[1mStep[0m  [24/84], [94mLoss[0m : 1.43376
[1mStep[0m  [32/84], [94mLoss[0m : 1.17930
[1mStep[0m  [40/84], [94mLoss[0m : 1.62570
[1mStep[0m  [48/84], [94mLoss[0m : 1.48468
[1mStep[0m  [56/84], [94mLoss[0m : 1.22802
[1mStep[0m  [64/84], [94mLoss[0m : 1.51994
[1mStep[0m  [72/84], [94mLoss[0m : 1.41495
[1mStep[0m  [80/84], [94mLoss[0m : 1.58256

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.420, [92mTest[0m: 2.542, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.19820
[1mStep[0m  [8/84], [94mLoss[0m : 1.40321
[1mStep[0m  [16/84], [94mLoss[0m : 1.39220
[1mStep[0m  [24/84], [94mLoss[0m : 1.36357
[1mStep[0m  [32/84], [94mLoss[0m : 1.63555
[1mStep[0m  [40/84], [94mLoss[0m : 1.43317
[1mStep[0m  [48/84], [94mLoss[0m : 1.37372
[1mStep[0m  [56/84], [94mLoss[0m : 1.27922
[1mStep[0m  [64/84], [94mLoss[0m : 1.38506
[1mStep[0m  [72/84], [94mLoss[0m : 1.42102
[1mStep[0m  [80/84], [94mLoss[0m : 1.28705

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.387, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.5159869364329746
MAE score P1        2.332481
MAE score P2        2.515987
loss                1.386872
learning_rate           0.01
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.5
weight_decay           0.001
Name: 10, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.38933
[1mStep[0m  [8/84], [94mLoss[0m : 9.73857
[1mStep[0m  [16/84], [94mLoss[0m : 6.76229
[1mStep[0m  [24/84], [94mLoss[0m : 5.04257
[1mStep[0m  [32/84], [94mLoss[0m : 4.11397
[1mStep[0m  [40/84], [94mLoss[0m : 2.88106
[1mStep[0m  [48/84], [94mLoss[0m : 2.83276
[1mStep[0m  [56/84], [94mLoss[0m : 2.93036
[1mStep[0m  [64/84], [94mLoss[0m : 2.59757
[1mStep[0m  [72/84], [94mLoss[0m : 2.72818
[1mStep[0m  [80/84], [94mLoss[0m : 2.76761

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.611, [92mTest[0m: 11.135, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65991
[1mStep[0m  [8/84], [94mLoss[0m : 2.78055
[1mStep[0m  [16/84], [94mLoss[0m : 2.36109
[1mStep[0m  [24/84], [94mLoss[0m : 2.79780
[1mStep[0m  [32/84], [94mLoss[0m : 2.71403
[1mStep[0m  [40/84], [94mLoss[0m : 2.81303
[1mStep[0m  [48/84], [94mLoss[0m : 2.71603
[1mStep[0m  [56/84], [94mLoss[0m : 2.70550
[1mStep[0m  [64/84], [94mLoss[0m : 2.69868
[1mStep[0m  [72/84], [94mLoss[0m : 2.59869
[1mStep[0m  [80/84], [94mLoss[0m : 2.23521

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83876
[1mStep[0m  [8/84], [94mLoss[0m : 2.86884
[1mStep[0m  [16/84], [94mLoss[0m : 2.55488
[1mStep[0m  [24/84], [94mLoss[0m : 2.88432
[1mStep[0m  [32/84], [94mLoss[0m : 2.82663
[1mStep[0m  [40/84], [94mLoss[0m : 2.68766
[1mStep[0m  [48/84], [94mLoss[0m : 2.88186
[1mStep[0m  [56/84], [94mLoss[0m : 2.63761
[1mStep[0m  [64/84], [94mLoss[0m : 2.72819
[1mStep[0m  [72/84], [94mLoss[0m : 2.53700
[1mStep[0m  [80/84], [94mLoss[0m : 2.66727

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87307
[1mStep[0m  [8/84], [94mLoss[0m : 2.53411
[1mStep[0m  [16/84], [94mLoss[0m : 2.88621
[1mStep[0m  [24/84], [94mLoss[0m : 3.05024
[1mStep[0m  [32/84], [94mLoss[0m : 2.40334
[1mStep[0m  [40/84], [94mLoss[0m : 2.71790
[1mStep[0m  [48/84], [94mLoss[0m : 2.90672
[1mStep[0m  [56/84], [94mLoss[0m : 2.69920
[1mStep[0m  [64/84], [94mLoss[0m : 2.65314
[1mStep[0m  [72/84], [94mLoss[0m : 2.73906
[1mStep[0m  [80/84], [94mLoss[0m : 2.50173

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67216
[1mStep[0m  [8/84], [94mLoss[0m : 2.55475
[1mStep[0m  [16/84], [94mLoss[0m : 2.93068
[1mStep[0m  [24/84], [94mLoss[0m : 2.46934
[1mStep[0m  [32/84], [94mLoss[0m : 2.43235
[1mStep[0m  [40/84], [94mLoss[0m : 2.83647
[1mStep[0m  [48/84], [94mLoss[0m : 2.71897
[1mStep[0m  [56/84], [94mLoss[0m : 2.98843
[1mStep[0m  [64/84], [94mLoss[0m : 2.75653
[1mStep[0m  [72/84], [94mLoss[0m : 2.49957
[1mStep[0m  [80/84], [94mLoss[0m : 2.81639

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36080
[1mStep[0m  [8/84], [94mLoss[0m : 2.37881
[1mStep[0m  [16/84], [94mLoss[0m : 2.75938
[1mStep[0m  [24/84], [94mLoss[0m : 2.90104
[1mStep[0m  [32/84], [94mLoss[0m : 2.34588
[1mStep[0m  [40/84], [94mLoss[0m : 2.79568
[1mStep[0m  [48/84], [94mLoss[0m : 2.14241
[1mStep[0m  [56/84], [94mLoss[0m : 2.68083
[1mStep[0m  [64/84], [94mLoss[0m : 2.58942
[1mStep[0m  [72/84], [94mLoss[0m : 2.62524
[1mStep[0m  [80/84], [94mLoss[0m : 2.62811

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58967
[1mStep[0m  [8/84], [94mLoss[0m : 2.74135
[1mStep[0m  [16/84], [94mLoss[0m : 2.50771
[1mStep[0m  [24/84], [94mLoss[0m : 2.33928
[1mStep[0m  [32/84], [94mLoss[0m : 2.92623
[1mStep[0m  [40/84], [94mLoss[0m : 2.83000
[1mStep[0m  [48/84], [94mLoss[0m : 2.53909
[1mStep[0m  [56/84], [94mLoss[0m : 2.57556
[1mStep[0m  [64/84], [94mLoss[0m : 2.61261
[1mStep[0m  [72/84], [94mLoss[0m : 2.50321
[1mStep[0m  [80/84], [94mLoss[0m : 2.90748

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40348
[1mStep[0m  [8/84], [94mLoss[0m : 2.17301
[1mStep[0m  [16/84], [94mLoss[0m : 2.54486
[1mStep[0m  [24/84], [94mLoss[0m : 2.67266
[1mStep[0m  [32/84], [94mLoss[0m : 2.28706
[1mStep[0m  [40/84], [94mLoss[0m : 2.71617
[1mStep[0m  [48/84], [94mLoss[0m : 2.75857
[1mStep[0m  [56/84], [94mLoss[0m : 2.66512
[1mStep[0m  [64/84], [94mLoss[0m : 2.53554
[1mStep[0m  [72/84], [94mLoss[0m : 2.46121
[1mStep[0m  [80/84], [94mLoss[0m : 2.46311

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47248
[1mStep[0m  [8/84], [94mLoss[0m : 2.72431
[1mStep[0m  [16/84], [94mLoss[0m : 2.94998
[1mStep[0m  [24/84], [94mLoss[0m : 2.47413
[1mStep[0m  [32/84], [94mLoss[0m : 2.65757
[1mStep[0m  [40/84], [94mLoss[0m : 2.78856
[1mStep[0m  [48/84], [94mLoss[0m : 2.48513
[1mStep[0m  [56/84], [94mLoss[0m : 2.49898
[1mStep[0m  [64/84], [94mLoss[0m : 2.22112
[1mStep[0m  [72/84], [94mLoss[0m : 2.61383
[1mStep[0m  [80/84], [94mLoss[0m : 2.67456

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62124
[1mStep[0m  [8/84], [94mLoss[0m : 2.56423
[1mStep[0m  [16/84], [94mLoss[0m : 2.51697
[1mStep[0m  [24/84], [94mLoss[0m : 2.74069
[1mStep[0m  [32/84], [94mLoss[0m : 2.81929
[1mStep[0m  [40/84], [94mLoss[0m : 2.78154
[1mStep[0m  [48/84], [94mLoss[0m : 2.59474
[1mStep[0m  [56/84], [94mLoss[0m : 2.53346
[1mStep[0m  [64/84], [94mLoss[0m : 2.47411
[1mStep[0m  [72/84], [94mLoss[0m : 2.70586
[1mStep[0m  [80/84], [94mLoss[0m : 2.57701

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73769
[1mStep[0m  [8/84], [94mLoss[0m : 2.53410
[1mStep[0m  [16/84], [94mLoss[0m : 2.62052
[1mStep[0m  [24/84], [94mLoss[0m : 2.61040
[1mStep[0m  [32/84], [94mLoss[0m : 2.79103
[1mStep[0m  [40/84], [94mLoss[0m : 2.59421
[1mStep[0m  [48/84], [94mLoss[0m : 2.42094
[1mStep[0m  [56/84], [94mLoss[0m : 2.69864
[1mStep[0m  [64/84], [94mLoss[0m : 3.04447
[1mStep[0m  [72/84], [94mLoss[0m : 2.83858
[1mStep[0m  [80/84], [94mLoss[0m : 2.66959

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.86776
[1mStep[0m  [8/84], [94mLoss[0m : 2.54976
[1mStep[0m  [16/84], [94mLoss[0m : 2.62179
[1mStep[0m  [24/84], [94mLoss[0m : 2.61156
[1mStep[0m  [32/84], [94mLoss[0m : 2.92297
[1mStep[0m  [40/84], [94mLoss[0m : 2.34417
[1mStep[0m  [48/84], [94mLoss[0m : 2.87027
[1mStep[0m  [56/84], [94mLoss[0m : 2.42768
[1mStep[0m  [64/84], [94mLoss[0m : 2.57083
[1mStep[0m  [72/84], [94mLoss[0m : 2.66728
[1mStep[0m  [80/84], [94mLoss[0m : 2.85025

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88455
[1mStep[0m  [8/84], [94mLoss[0m : 2.45574
[1mStep[0m  [16/84], [94mLoss[0m : 2.63666
[1mStep[0m  [24/84], [94mLoss[0m : 2.76418
[1mStep[0m  [32/84], [94mLoss[0m : 2.48348
[1mStep[0m  [40/84], [94mLoss[0m : 2.57096
[1mStep[0m  [48/84], [94mLoss[0m : 2.63916
[1mStep[0m  [56/84], [94mLoss[0m : 2.47797
[1mStep[0m  [64/84], [94mLoss[0m : 2.62400
[1mStep[0m  [72/84], [94mLoss[0m : 2.56297
[1mStep[0m  [80/84], [94mLoss[0m : 2.52640

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77252
[1mStep[0m  [8/84], [94mLoss[0m : 2.68551
[1mStep[0m  [16/84], [94mLoss[0m : 2.58823
[1mStep[0m  [24/84], [94mLoss[0m : 2.74936
[1mStep[0m  [32/84], [94mLoss[0m : 2.64883
[1mStep[0m  [40/84], [94mLoss[0m : 2.77971
[1mStep[0m  [48/84], [94mLoss[0m : 2.36467
[1mStep[0m  [56/84], [94mLoss[0m : 2.62701
[1mStep[0m  [64/84], [94mLoss[0m : 2.45716
[1mStep[0m  [72/84], [94mLoss[0m : 2.68018
[1mStep[0m  [80/84], [94mLoss[0m : 2.31703

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82019
[1mStep[0m  [8/84], [94mLoss[0m : 2.37947
[1mStep[0m  [16/84], [94mLoss[0m : 2.11048
[1mStep[0m  [24/84], [94mLoss[0m : 2.72066
[1mStep[0m  [32/84], [94mLoss[0m : 2.47848
[1mStep[0m  [40/84], [94mLoss[0m : 2.57732
[1mStep[0m  [48/84], [94mLoss[0m : 2.43790
[1mStep[0m  [56/84], [94mLoss[0m : 2.47176
[1mStep[0m  [64/84], [94mLoss[0m : 2.70178
[1mStep[0m  [72/84], [94mLoss[0m : 2.54766
[1mStep[0m  [80/84], [94mLoss[0m : 2.84702

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82855
[1mStep[0m  [8/84], [94mLoss[0m : 2.81571
[1mStep[0m  [16/84], [94mLoss[0m : 2.72958
[1mStep[0m  [24/84], [94mLoss[0m : 2.43566
[1mStep[0m  [32/84], [94mLoss[0m : 2.61436
[1mStep[0m  [40/84], [94mLoss[0m : 2.50301
[1mStep[0m  [48/84], [94mLoss[0m : 2.55496
[1mStep[0m  [56/84], [94mLoss[0m : 2.60421
[1mStep[0m  [64/84], [94mLoss[0m : 2.51706
[1mStep[0m  [72/84], [94mLoss[0m : 2.25939
[1mStep[0m  [80/84], [94mLoss[0m : 2.67122

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55723
[1mStep[0m  [8/84], [94mLoss[0m : 2.40959
[1mStep[0m  [16/84], [94mLoss[0m : 2.80349
[1mStep[0m  [24/84], [94mLoss[0m : 2.75558
[1mStep[0m  [32/84], [94mLoss[0m : 2.47815
[1mStep[0m  [40/84], [94mLoss[0m : 2.74124
[1mStep[0m  [48/84], [94mLoss[0m : 2.74504
[1mStep[0m  [56/84], [94mLoss[0m : 2.83525
[1mStep[0m  [64/84], [94mLoss[0m : 2.36857
[1mStep[0m  [72/84], [94mLoss[0m : 2.49088
[1mStep[0m  [80/84], [94mLoss[0m : 2.63768

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47606
[1mStep[0m  [8/84], [94mLoss[0m : 2.65519
[1mStep[0m  [16/84], [94mLoss[0m : 2.42727
[1mStep[0m  [24/84], [94mLoss[0m : 2.23598
[1mStep[0m  [32/84], [94mLoss[0m : 2.52648
[1mStep[0m  [40/84], [94mLoss[0m : 2.95830
[1mStep[0m  [48/84], [94mLoss[0m : 2.85289
[1mStep[0m  [56/84], [94mLoss[0m : 2.78820
[1mStep[0m  [64/84], [94mLoss[0m : 2.49752
[1mStep[0m  [72/84], [94mLoss[0m : 2.73816
[1mStep[0m  [80/84], [94mLoss[0m : 2.64504

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69989
[1mStep[0m  [8/84], [94mLoss[0m : 2.49503
[1mStep[0m  [16/84], [94mLoss[0m : 2.72474
[1mStep[0m  [24/84], [94mLoss[0m : 2.60208
[1mStep[0m  [32/84], [94mLoss[0m : 2.53849
[1mStep[0m  [40/84], [94mLoss[0m : 2.61900
[1mStep[0m  [48/84], [94mLoss[0m : 2.29961
[1mStep[0m  [56/84], [94mLoss[0m : 2.38274
[1mStep[0m  [64/84], [94mLoss[0m : 2.65666
[1mStep[0m  [72/84], [94mLoss[0m : 2.45383
[1mStep[0m  [80/84], [94mLoss[0m : 2.87730

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36329
[1mStep[0m  [8/84], [94mLoss[0m : 2.37729
[1mStep[0m  [16/84], [94mLoss[0m : 2.36079
[1mStep[0m  [24/84], [94mLoss[0m : 2.85079
[1mStep[0m  [32/84], [94mLoss[0m : 2.56768
[1mStep[0m  [40/84], [94mLoss[0m : 2.61023
[1mStep[0m  [48/84], [94mLoss[0m : 2.42390
[1mStep[0m  [56/84], [94mLoss[0m : 2.43945
[1mStep[0m  [64/84], [94mLoss[0m : 2.61269
[1mStep[0m  [72/84], [94mLoss[0m : 2.47774
[1mStep[0m  [80/84], [94mLoss[0m : 2.78802

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60461
[1mStep[0m  [8/84], [94mLoss[0m : 2.52268
[1mStep[0m  [16/84], [94mLoss[0m : 2.59702
[1mStep[0m  [24/84], [94mLoss[0m : 2.08130
[1mStep[0m  [32/84], [94mLoss[0m : 2.54522
[1mStep[0m  [40/84], [94mLoss[0m : 2.45421
[1mStep[0m  [48/84], [94mLoss[0m : 2.76783
[1mStep[0m  [56/84], [94mLoss[0m : 2.60253
[1mStep[0m  [64/84], [94mLoss[0m : 2.62575
[1mStep[0m  [72/84], [94mLoss[0m : 2.24662
[1mStep[0m  [80/84], [94mLoss[0m : 2.20227

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79014
[1mStep[0m  [8/84], [94mLoss[0m : 2.64760
[1mStep[0m  [16/84], [94mLoss[0m : 2.57702
[1mStep[0m  [24/84], [94mLoss[0m : 2.46140
[1mStep[0m  [32/84], [94mLoss[0m : 2.09140
[1mStep[0m  [40/84], [94mLoss[0m : 2.71280
[1mStep[0m  [48/84], [94mLoss[0m : 2.67276
[1mStep[0m  [56/84], [94mLoss[0m : 2.51587
[1mStep[0m  [64/84], [94mLoss[0m : 2.47852
[1mStep[0m  [72/84], [94mLoss[0m : 2.31326
[1mStep[0m  [80/84], [94mLoss[0m : 2.78308

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32964
[1mStep[0m  [8/84], [94mLoss[0m : 2.69460
[1mStep[0m  [16/84], [94mLoss[0m : 2.61006
[1mStep[0m  [24/84], [94mLoss[0m : 2.57241
[1mStep[0m  [32/84], [94mLoss[0m : 2.39277
[1mStep[0m  [40/84], [94mLoss[0m : 2.73719
[1mStep[0m  [48/84], [94mLoss[0m : 2.62655
[1mStep[0m  [56/84], [94mLoss[0m : 2.57286
[1mStep[0m  [64/84], [94mLoss[0m : 2.44380
[1mStep[0m  [72/84], [94mLoss[0m : 2.54833
[1mStep[0m  [80/84], [94mLoss[0m : 2.60376

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39419
[1mStep[0m  [8/84], [94mLoss[0m : 2.88618
[1mStep[0m  [16/84], [94mLoss[0m : 2.73148
[1mStep[0m  [24/84], [94mLoss[0m : 2.85945
[1mStep[0m  [32/84], [94mLoss[0m : 2.25392
[1mStep[0m  [40/84], [94mLoss[0m : 2.57295
[1mStep[0m  [48/84], [94mLoss[0m : 2.76282
[1mStep[0m  [56/84], [94mLoss[0m : 2.46487
[1mStep[0m  [64/84], [94mLoss[0m : 2.69527
[1mStep[0m  [72/84], [94mLoss[0m : 2.40758
[1mStep[0m  [80/84], [94mLoss[0m : 2.67410

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27810
[1mStep[0m  [8/84], [94mLoss[0m : 2.46602
[1mStep[0m  [16/84], [94mLoss[0m : 2.61829
[1mStep[0m  [24/84], [94mLoss[0m : 2.38452
[1mStep[0m  [32/84], [94mLoss[0m : 2.60344
[1mStep[0m  [40/84], [94mLoss[0m : 2.30829
[1mStep[0m  [48/84], [94mLoss[0m : 2.45021
[1mStep[0m  [56/84], [94mLoss[0m : 2.37204
[1mStep[0m  [64/84], [94mLoss[0m : 2.79266
[1mStep[0m  [72/84], [94mLoss[0m : 2.61587
[1mStep[0m  [80/84], [94mLoss[0m : 2.59666

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30580
[1mStep[0m  [8/84], [94mLoss[0m : 2.52470
[1mStep[0m  [16/84], [94mLoss[0m : 2.86762
[1mStep[0m  [24/84], [94mLoss[0m : 2.60024
[1mStep[0m  [32/84], [94mLoss[0m : 2.67536
[1mStep[0m  [40/84], [94mLoss[0m : 2.56660
[1mStep[0m  [48/84], [94mLoss[0m : 2.86244
[1mStep[0m  [56/84], [94mLoss[0m : 2.26388
[1mStep[0m  [64/84], [94mLoss[0m : 2.81655
[1mStep[0m  [72/84], [94mLoss[0m : 2.38220
[1mStep[0m  [80/84], [94mLoss[0m : 2.80923

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18489
[1mStep[0m  [8/84], [94mLoss[0m : 2.99835
[1mStep[0m  [16/84], [94mLoss[0m : 2.58454
[1mStep[0m  [24/84], [94mLoss[0m : 2.57248
[1mStep[0m  [32/84], [94mLoss[0m : 2.43011
[1mStep[0m  [40/84], [94mLoss[0m : 2.56036
[1mStep[0m  [48/84], [94mLoss[0m : 2.34358
[1mStep[0m  [56/84], [94mLoss[0m : 2.54273
[1mStep[0m  [64/84], [94mLoss[0m : 2.44376
[1mStep[0m  [72/84], [94mLoss[0m : 2.57970
[1mStep[0m  [80/84], [94mLoss[0m : 2.50878

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78569
[1mStep[0m  [8/84], [94mLoss[0m : 2.57126
[1mStep[0m  [16/84], [94mLoss[0m : 2.83469
[1mStep[0m  [24/84], [94mLoss[0m : 2.65077
[1mStep[0m  [32/84], [94mLoss[0m : 2.52685
[1mStep[0m  [40/84], [94mLoss[0m : 2.62785
[1mStep[0m  [48/84], [94mLoss[0m : 2.71667
[1mStep[0m  [56/84], [94mLoss[0m : 2.47880
[1mStep[0m  [64/84], [94mLoss[0m : 2.56526
[1mStep[0m  [72/84], [94mLoss[0m : 2.34557
[1mStep[0m  [80/84], [94mLoss[0m : 2.47839

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64286
[1mStep[0m  [8/84], [94mLoss[0m : 2.43376
[1mStep[0m  [16/84], [94mLoss[0m : 2.89347
[1mStep[0m  [24/84], [94mLoss[0m : 2.39362
[1mStep[0m  [32/84], [94mLoss[0m : 2.80374
[1mStep[0m  [40/84], [94mLoss[0m : 2.64023
[1mStep[0m  [48/84], [94mLoss[0m : 2.39610
[1mStep[0m  [56/84], [94mLoss[0m : 2.45868
[1mStep[0m  [64/84], [94mLoss[0m : 2.46973
[1mStep[0m  [72/84], [94mLoss[0m : 2.53707
[1mStep[0m  [80/84], [94mLoss[0m : 2.55634

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35394
[1mStep[0m  [8/84], [94mLoss[0m : 2.39051
[1mStep[0m  [16/84], [94mLoss[0m : 2.44178
[1mStep[0m  [24/84], [94mLoss[0m : 2.43405
[1mStep[0m  [32/84], [94mLoss[0m : 2.47084
[1mStep[0m  [40/84], [94mLoss[0m : 2.55025
[1mStep[0m  [48/84], [94mLoss[0m : 2.31438
[1mStep[0m  [56/84], [94mLoss[0m : 2.66025
[1mStep[0m  [64/84], [94mLoss[0m : 2.37046
[1mStep[0m  [72/84], [94mLoss[0m : 2.50419
[1mStep[0m  [80/84], [94mLoss[0m : 2.52964

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.3245278085981096
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.58191
[1mStep[0m  [8/84], [94mLoss[0m : 2.51178
[1mStep[0m  [16/84], [94mLoss[0m : 2.60350
[1mStep[0m  [24/84], [94mLoss[0m : 2.73332
[1mStep[0m  [32/84], [94mLoss[0m : 2.69001
[1mStep[0m  [40/84], [94mLoss[0m : 2.38875
[1mStep[0m  [48/84], [94mLoss[0m : 2.63845
[1mStep[0m  [56/84], [94mLoss[0m : 2.86162
[1mStep[0m  [64/84], [94mLoss[0m : 2.55682
[1mStep[0m  [72/84], [94mLoss[0m : 2.31052
[1mStep[0m  [80/84], [94mLoss[0m : 2.68653

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45211
[1mStep[0m  [8/84], [94mLoss[0m : 2.45909
[1mStep[0m  [16/84], [94mLoss[0m : 2.70399
[1mStep[0m  [24/84], [94mLoss[0m : 2.34386
[1mStep[0m  [32/84], [94mLoss[0m : 2.42182
[1mStep[0m  [40/84], [94mLoss[0m : 2.85881
[1mStep[0m  [48/84], [94mLoss[0m : 2.42612
[1mStep[0m  [56/84], [94mLoss[0m : 2.37911
[1mStep[0m  [64/84], [94mLoss[0m : 2.21579
[1mStep[0m  [72/84], [94mLoss[0m : 2.57909
[1mStep[0m  [80/84], [94mLoss[0m : 2.73082

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26900
[1mStep[0m  [8/84], [94mLoss[0m : 2.51361
[1mStep[0m  [16/84], [94mLoss[0m : 2.62367
[1mStep[0m  [24/84], [94mLoss[0m : 2.39101
[1mStep[0m  [32/84], [94mLoss[0m : 2.53270
[1mStep[0m  [40/84], [94mLoss[0m : 2.73811
[1mStep[0m  [48/84], [94mLoss[0m : 2.49650
[1mStep[0m  [56/84], [94mLoss[0m : 2.55387
[1mStep[0m  [64/84], [94mLoss[0m : 2.11572
[1mStep[0m  [72/84], [94mLoss[0m : 2.34981
[1mStep[0m  [80/84], [94mLoss[0m : 2.23988

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42065
[1mStep[0m  [8/84], [94mLoss[0m : 2.91582
[1mStep[0m  [16/84], [94mLoss[0m : 2.41867
[1mStep[0m  [24/84], [94mLoss[0m : 2.45759
[1mStep[0m  [32/84], [94mLoss[0m : 2.32853
[1mStep[0m  [40/84], [94mLoss[0m : 2.39597
[1mStep[0m  [48/84], [94mLoss[0m : 2.39594
[1mStep[0m  [56/84], [94mLoss[0m : 2.90416
[1mStep[0m  [64/84], [94mLoss[0m : 2.41288
[1mStep[0m  [72/84], [94mLoss[0m : 2.48684
[1mStep[0m  [80/84], [94mLoss[0m : 2.24895

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21609
[1mStep[0m  [8/84], [94mLoss[0m : 2.14948
[1mStep[0m  [16/84], [94mLoss[0m : 2.27001
[1mStep[0m  [24/84], [94mLoss[0m : 2.38727
[1mStep[0m  [32/84], [94mLoss[0m : 2.58998
[1mStep[0m  [40/84], [94mLoss[0m : 2.34545
[1mStep[0m  [48/84], [94mLoss[0m : 2.45232
[1mStep[0m  [56/84], [94mLoss[0m : 2.55879
[1mStep[0m  [64/84], [94mLoss[0m : 2.30847
[1mStep[0m  [72/84], [94mLoss[0m : 2.24015
[1mStep[0m  [80/84], [94mLoss[0m : 2.13770

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20632
[1mStep[0m  [8/84], [94mLoss[0m : 2.51598
[1mStep[0m  [16/84], [94mLoss[0m : 2.08188
[1mStep[0m  [24/84], [94mLoss[0m : 2.19497
[1mStep[0m  [32/84], [94mLoss[0m : 2.30069
[1mStep[0m  [40/84], [94mLoss[0m : 2.49005
[1mStep[0m  [48/84], [94mLoss[0m : 1.90257
[1mStep[0m  [56/84], [94mLoss[0m : 2.48849
[1mStep[0m  [64/84], [94mLoss[0m : 2.15011
[1mStep[0m  [72/84], [94mLoss[0m : 2.36777
[1mStep[0m  [80/84], [94mLoss[0m : 2.19997

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14343
[1mStep[0m  [8/84], [94mLoss[0m : 2.49602
[1mStep[0m  [16/84], [94mLoss[0m : 2.25824
[1mStep[0m  [24/84], [94mLoss[0m : 2.27863
[1mStep[0m  [32/84], [94mLoss[0m : 2.25262
[1mStep[0m  [40/84], [94mLoss[0m : 2.07674
[1mStep[0m  [48/84], [94mLoss[0m : 2.27846
[1mStep[0m  [56/84], [94mLoss[0m : 2.38776
[1mStep[0m  [64/84], [94mLoss[0m : 2.19216
[1mStep[0m  [72/84], [94mLoss[0m : 2.26567
[1mStep[0m  [80/84], [94mLoss[0m : 1.94643

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10848
[1mStep[0m  [8/84], [94mLoss[0m : 1.95702
[1mStep[0m  [16/84], [94mLoss[0m : 2.34443
[1mStep[0m  [24/84], [94mLoss[0m : 2.06223
[1mStep[0m  [32/84], [94mLoss[0m : 2.06065
[1mStep[0m  [40/84], [94mLoss[0m : 2.16855
[1mStep[0m  [48/84], [94mLoss[0m : 2.32459
[1mStep[0m  [56/84], [94mLoss[0m : 2.48210
[1mStep[0m  [64/84], [94mLoss[0m : 2.25159
[1mStep[0m  [72/84], [94mLoss[0m : 2.13226
[1mStep[0m  [80/84], [94mLoss[0m : 2.26164

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07390
[1mStep[0m  [8/84], [94mLoss[0m : 2.23873
[1mStep[0m  [16/84], [94mLoss[0m : 2.30762
[1mStep[0m  [24/84], [94mLoss[0m : 2.36958
[1mStep[0m  [32/84], [94mLoss[0m : 1.90121
[1mStep[0m  [40/84], [94mLoss[0m : 1.87097
[1mStep[0m  [48/84], [94mLoss[0m : 2.13735
[1mStep[0m  [56/84], [94mLoss[0m : 1.71443
[1mStep[0m  [64/84], [94mLoss[0m : 2.10687
[1mStep[0m  [72/84], [94mLoss[0m : 2.28179
[1mStep[0m  [80/84], [94mLoss[0m : 2.20491

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60730
[1mStep[0m  [8/84], [94mLoss[0m : 2.36894
[1mStep[0m  [16/84], [94mLoss[0m : 2.08292
[1mStep[0m  [24/84], [94mLoss[0m : 2.15949
[1mStep[0m  [32/84], [94mLoss[0m : 1.94026
[1mStep[0m  [40/84], [94mLoss[0m : 2.13696
[1mStep[0m  [48/84], [94mLoss[0m : 1.71490
[1mStep[0m  [56/84], [94mLoss[0m : 2.20014
[1mStep[0m  [64/84], [94mLoss[0m : 2.04976
[1mStep[0m  [72/84], [94mLoss[0m : 1.99273
[1mStep[0m  [80/84], [94mLoss[0m : 2.15660

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65450
[1mStep[0m  [8/84], [94mLoss[0m : 1.58335
[1mStep[0m  [16/84], [94mLoss[0m : 2.03409
[1mStep[0m  [24/84], [94mLoss[0m : 2.18584
[1mStep[0m  [32/84], [94mLoss[0m : 2.03022
[1mStep[0m  [40/84], [94mLoss[0m : 2.00183
[1mStep[0m  [48/84], [94mLoss[0m : 2.17155
[1mStep[0m  [56/84], [94mLoss[0m : 2.31795
[1mStep[0m  [64/84], [94mLoss[0m : 2.04449
[1mStep[0m  [72/84], [94mLoss[0m : 2.07400
[1mStep[0m  [80/84], [94mLoss[0m : 1.85567

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25949
[1mStep[0m  [8/84], [94mLoss[0m : 1.90397
[1mStep[0m  [16/84], [94mLoss[0m : 2.06313
[1mStep[0m  [24/84], [94mLoss[0m : 1.69121
[1mStep[0m  [32/84], [94mLoss[0m : 1.99198
[1mStep[0m  [40/84], [94mLoss[0m : 1.97260
[1mStep[0m  [48/84], [94mLoss[0m : 1.96536
[1mStep[0m  [56/84], [94mLoss[0m : 2.05777
[1mStep[0m  [64/84], [94mLoss[0m : 1.58178
[1mStep[0m  [72/84], [94mLoss[0m : 1.69607
[1mStep[0m  [80/84], [94mLoss[0m : 1.91049

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02294
[1mStep[0m  [8/84], [94mLoss[0m : 1.84964
[1mStep[0m  [16/84], [94mLoss[0m : 1.89218
[1mStep[0m  [24/84], [94mLoss[0m : 1.88481
[1mStep[0m  [32/84], [94mLoss[0m : 1.93873
[1mStep[0m  [40/84], [94mLoss[0m : 1.85615
[1mStep[0m  [48/84], [94mLoss[0m : 1.96047
[1mStep[0m  [56/84], [94mLoss[0m : 2.29643
[1mStep[0m  [64/84], [94mLoss[0m : 1.77801
[1mStep[0m  [72/84], [94mLoss[0m : 1.90437
[1mStep[0m  [80/84], [94mLoss[0m : 1.74834

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76456
[1mStep[0m  [8/84], [94mLoss[0m : 1.91415
[1mStep[0m  [16/84], [94mLoss[0m : 1.88451
[1mStep[0m  [24/84], [94mLoss[0m : 1.95333
[1mStep[0m  [32/84], [94mLoss[0m : 2.21604
[1mStep[0m  [40/84], [94mLoss[0m : 2.23033
[1mStep[0m  [48/84], [94mLoss[0m : 1.73952
[1mStep[0m  [56/84], [94mLoss[0m : 1.87061
[1mStep[0m  [64/84], [94mLoss[0m : 1.92647
[1mStep[0m  [72/84], [94mLoss[0m : 2.07147
[1mStep[0m  [80/84], [94mLoss[0m : 1.66056

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77219
[1mStep[0m  [8/84], [94mLoss[0m : 1.92150
[1mStep[0m  [16/84], [94mLoss[0m : 1.93700
[1mStep[0m  [24/84], [94mLoss[0m : 2.03086
[1mStep[0m  [32/84], [94mLoss[0m : 1.65053
[1mStep[0m  [40/84], [94mLoss[0m : 1.94565
[1mStep[0m  [48/84], [94mLoss[0m : 1.64829
[1mStep[0m  [56/84], [94mLoss[0m : 1.80118
[1mStep[0m  [64/84], [94mLoss[0m : 1.75747
[1mStep[0m  [72/84], [94mLoss[0m : 1.76373
[1mStep[0m  [80/84], [94mLoss[0m : 2.21248

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85859
[1mStep[0m  [8/84], [94mLoss[0m : 1.69298
[1mStep[0m  [16/84], [94mLoss[0m : 1.88366
[1mStep[0m  [24/84], [94mLoss[0m : 1.81826
[1mStep[0m  [32/84], [94mLoss[0m : 1.72265
[1mStep[0m  [40/84], [94mLoss[0m : 2.21406
[1mStep[0m  [48/84], [94mLoss[0m : 1.73347
[1mStep[0m  [56/84], [94mLoss[0m : 1.53250
[1mStep[0m  [64/84], [94mLoss[0m : 1.72361
[1mStep[0m  [72/84], [94mLoss[0m : 2.00427
[1mStep[0m  [80/84], [94mLoss[0m : 1.84961

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.556, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85629
[1mStep[0m  [8/84], [94mLoss[0m : 1.68201
[1mStep[0m  [16/84], [94mLoss[0m : 1.55456
[1mStep[0m  [24/84], [94mLoss[0m : 1.87009
[1mStep[0m  [32/84], [94mLoss[0m : 1.52303
[1mStep[0m  [40/84], [94mLoss[0m : 2.02313
[1mStep[0m  [48/84], [94mLoss[0m : 1.59820
[1mStep[0m  [56/84], [94mLoss[0m : 1.78705
[1mStep[0m  [64/84], [94mLoss[0m : 1.99753
[1mStep[0m  [72/84], [94mLoss[0m : 1.90704
[1mStep[0m  [80/84], [94mLoss[0m : 2.06043

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79625
[1mStep[0m  [8/84], [94mLoss[0m : 1.83598
[1mStep[0m  [16/84], [94mLoss[0m : 1.73915
[1mStep[0m  [24/84], [94mLoss[0m : 1.81019
[1mStep[0m  [32/84], [94mLoss[0m : 1.55531
[1mStep[0m  [40/84], [94mLoss[0m : 1.87979
[1mStep[0m  [48/84], [94mLoss[0m : 1.89628
[1mStep[0m  [56/84], [94mLoss[0m : 1.73086
[1mStep[0m  [64/84], [94mLoss[0m : 1.69456
[1mStep[0m  [72/84], [94mLoss[0m : 1.95994
[1mStep[0m  [80/84], [94mLoss[0m : 2.23862

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.807, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68339
[1mStep[0m  [8/84], [94mLoss[0m : 1.81654
[1mStep[0m  [16/84], [94mLoss[0m : 1.60040
[1mStep[0m  [24/84], [94mLoss[0m : 1.82068
[1mStep[0m  [32/84], [94mLoss[0m : 1.86055
[1mStep[0m  [40/84], [94mLoss[0m : 1.62795
[1mStep[0m  [48/84], [94mLoss[0m : 2.05299
[1mStep[0m  [56/84], [94mLoss[0m : 1.70134
[1mStep[0m  [64/84], [94mLoss[0m : 1.88519
[1mStep[0m  [72/84], [94mLoss[0m : 1.99027
[1mStep[0m  [80/84], [94mLoss[0m : 1.60942

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73304
[1mStep[0m  [8/84], [94mLoss[0m : 1.95791
[1mStep[0m  [16/84], [94mLoss[0m : 1.77823
[1mStep[0m  [24/84], [94mLoss[0m : 1.61409
[1mStep[0m  [32/84], [94mLoss[0m : 2.10172
[1mStep[0m  [40/84], [94mLoss[0m : 1.57287
[1mStep[0m  [48/84], [94mLoss[0m : 1.86440
[1mStep[0m  [56/84], [94mLoss[0m : 1.54643
[1mStep[0m  [64/84], [94mLoss[0m : 1.47425
[1mStep[0m  [72/84], [94mLoss[0m : 2.02848
[1mStep[0m  [80/84], [94mLoss[0m : 1.74084

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.520, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56871
[1mStep[0m  [8/84], [94mLoss[0m : 1.51034
[1mStep[0m  [16/84], [94mLoss[0m : 1.81995
[1mStep[0m  [24/84], [94mLoss[0m : 1.65122
[1mStep[0m  [32/84], [94mLoss[0m : 1.78862
[1mStep[0m  [40/84], [94mLoss[0m : 1.72810
[1mStep[0m  [48/84], [94mLoss[0m : 1.70557
[1mStep[0m  [56/84], [94mLoss[0m : 1.77346
[1mStep[0m  [64/84], [94mLoss[0m : 1.82828
[1mStep[0m  [72/84], [94mLoss[0m : 1.95818
[1mStep[0m  [80/84], [94mLoss[0m : 1.80735

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.518, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72326
[1mStep[0m  [8/84], [94mLoss[0m : 1.60396
[1mStep[0m  [16/84], [94mLoss[0m : 1.67875
[1mStep[0m  [24/84], [94mLoss[0m : 1.75844
[1mStep[0m  [32/84], [94mLoss[0m : 1.82413
[1mStep[0m  [40/84], [94mLoss[0m : 1.81622
[1mStep[0m  [48/84], [94mLoss[0m : 1.66128
[1mStep[0m  [56/84], [94mLoss[0m : 1.89362
[1mStep[0m  [64/84], [94mLoss[0m : 1.67435
[1mStep[0m  [72/84], [94mLoss[0m : 1.87487
[1mStep[0m  [80/84], [94mLoss[0m : 1.73376

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.524, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67007
[1mStep[0m  [8/84], [94mLoss[0m : 1.70121
[1mStep[0m  [16/84], [94mLoss[0m : 1.63673
[1mStep[0m  [24/84], [94mLoss[0m : 1.59689
[1mStep[0m  [32/84], [94mLoss[0m : 1.71081
[1mStep[0m  [40/84], [94mLoss[0m : 1.65904
[1mStep[0m  [48/84], [94mLoss[0m : 1.76383
[1mStep[0m  [56/84], [94mLoss[0m : 1.75303
[1mStep[0m  [64/84], [94mLoss[0m : 1.64748
[1mStep[0m  [72/84], [94mLoss[0m : 1.55518
[1mStep[0m  [80/84], [94mLoss[0m : 1.70301

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.515, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85757
[1mStep[0m  [8/84], [94mLoss[0m : 1.62820
[1mStep[0m  [16/84], [94mLoss[0m : 1.59957
[1mStep[0m  [24/84], [94mLoss[0m : 1.63642
[1mStep[0m  [32/84], [94mLoss[0m : 1.49535
[1mStep[0m  [40/84], [94mLoss[0m : 1.60535
[1mStep[0m  [48/84], [94mLoss[0m : 1.73125
[1mStep[0m  [56/84], [94mLoss[0m : 1.79352
[1mStep[0m  [64/84], [94mLoss[0m : 1.56331
[1mStep[0m  [72/84], [94mLoss[0m : 1.55909
[1mStep[0m  [80/84], [94mLoss[0m : 1.78992

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.652, [92mTest[0m: 2.530, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67830
[1mStep[0m  [8/84], [94mLoss[0m : 1.71175
[1mStep[0m  [16/84], [94mLoss[0m : 1.73230
[1mStep[0m  [24/84], [94mLoss[0m : 1.70523
[1mStep[0m  [32/84], [94mLoss[0m : 1.49421
[1mStep[0m  [40/84], [94mLoss[0m : 1.41286
[1mStep[0m  [48/84], [94mLoss[0m : 1.58700
[1mStep[0m  [56/84], [94mLoss[0m : 1.72032
[1mStep[0m  [64/84], [94mLoss[0m : 1.44662
[1mStep[0m  [72/84], [94mLoss[0m : 1.51809
[1mStep[0m  [80/84], [94mLoss[0m : 1.87807

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.637, [92mTest[0m: 2.576, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69853
[1mStep[0m  [8/84], [94mLoss[0m : 1.57673
[1mStep[0m  [16/84], [94mLoss[0m : 1.57776
[1mStep[0m  [24/84], [94mLoss[0m : 1.44460
[1mStep[0m  [32/84], [94mLoss[0m : 1.76654
[1mStep[0m  [40/84], [94mLoss[0m : 1.52101
[1mStep[0m  [48/84], [94mLoss[0m : 1.45500
[1mStep[0m  [56/84], [94mLoss[0m : 1.60539
[1mStep[0m  [64/84], [94mLoss[0m : 1.69420
[1mStep[0m  [72/84], [94mLoss[0m : 1.64602
[1mStep[0m  [80/84], [94mLoss[0m : 1.65519

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51733
[1mStep[0m  [8/84], [94mLoss[0m : 1.66023
[1mStep[0m  [16/84], [94mLoss[0m : 1.43233
[1mStep[0m  [24/84], [94mLoss[0m : 1.77991
[1mStep[0m  [32/84], [94mLoss[0m : 1.48503
[1mStep[0m  [40/84], [94mLoss[0m : 1.65963
[1mStep[0m  [48/84], [94mLoss[0m : 1.74806
[1mStep[0m  [56/84], [94mLoss[0m : 1.64327
[1mStep[0m  [64/84], [94mLoss[0m : 1.55865
[1mStep[0m  [72/84], [94mLoss[0m : 1.53406
[1mStep[0m  [80/84], [94mLoss[0m : 1.71229

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.513, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39741
[1mStep[0m  [8/84], [94mLoss[0m : 1.75801
[1mStep[0m  [16/84], [94mLoss[0m : 1.35346
[1mStep[0m  [24/84], [94mLoss[0m : 1.67280
[1mStep[0m  [32/84], [94mLoss[0m : 1.45236
[1mStep[0m  [40/84], [94mLoss[0m : 1.64728
[1mStep[0m  [48/84], [94mLoss[0m : 1.50347
[1mStep[0m  [56/84], [94mLoss[0m : 1.52122
[1mStep[0m  [64/84], [94mLoss[0m : 1.53602
[1mStep[0m  [72/84], [94mLoss[0m : 1.59891
[1mStep[0m  [80/84], [94mLoss[0m : 1.70481

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72630
[1mStep[0m  [8/84], [94mLoss[0m : 1.59314
[1mStep[0m  [16/84], [94mLoss[0m : 1.47557
[1mStep[0m  [24/84], [94mLoss[0m : 1.61078
[1mStep[0m  [32/84], [94mLoss[0m : 1.52460
[1mStep[0m  [40/84], [94mLoss[0m : 1.54277
[1mStep[0m  [48/84], [94mLoss[0m : 1.35557
[1mStep[0m  [56/84], [94mLoss[0m : 1.54306
[1mStep[0m  [64/84], [94mLoss[0m : 1.60906
[1mStep[0m  [72/84], [94mLoss[0m : 1.64816
[1mStep[0m  [80/84], [94mLoss[0m : 1.57635

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.578, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41647
[1mStep[0m  [8/84], [94mLoss[0m : 1.60137
[1mStep[0m  [16/84], [94mLoss[0m : 1.54688
[1mStep[0m  [24/84], [94mLoss[0m : 1.49458
[1mStep[0m  [32/84], [94mLoss[0m : 1.44294
[1mStep[0m  [40/84], [94mLoss[0m : 1.43551
[1mStep[0m  [48/84], [94mLoss[0m : 1.46468
[1mStep[0m  [56/84], [94mLoss[0m : 1.43335
[1mStep[0m  [64/84], [94mLoss[0m : 1.64401
[1mStep[0m  [72/84], [94mLoss[0m : 1.66645
[1mStep[0m  [80/84], [94mLoss[0m : 1.64133

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.583
====================================

Phase 2 - Evaluation MAE:  2.583204346043723
MAE score P1       2.324528
MAE score P2       2.583204
loss               1.559652
learning_rate          0.01
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay          0.001
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.60517
[1mStep[0m  [4/42], [94mLoss[0m : 8.99129
[1mStep[0m  [8/42], [94mLoss[0m : 7.48753
[1mStep[0m  [12/42], [94mLoss[0m : 5.95807
[1mStep[0m  [16/42], [94mLoss[0m : 4.84752
[1mStep[0m  [20/42], [94mLoss[0m : 3.64264
[1mStep[0m  [24/42], [94mLoss[0m : 3.24741
[1mStep[0m  [28/42], [94mLoss[0m : 2.78019
[1mStep[0m  [32/42], [94mLoss[0m : 2.94014
[1mStep[0m  [36/42], [94mLoss[0m : 2.83779
[1mStep[0m  [40/42], [94mLoss[0m : 2.85214

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.890, [92mTest[0m: 10.979, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53348
[1mStep[0m  [4/42], [94mLoss[0m : 2.71113
[1mStep[0m  [8/42], [94mLoss[0m : 2.56053
[1mStep[0m  [12/42], [94mLoss[0m : 2.34717
[1mStep[0m  [16/42], [94mLoss[0m : 2.43156
[1mStep[0m  [20/42], [94mLoss[0m : 2.64312
[1mStep[0m  [24/42], [94mLoss[0m : 2.56167
[1mStep[0m  [28/42], [94mLoss[0m : 2.37575
[1mStep[0m  [32/42], [94mLoss[0m : 2.63713
[1mStep[0m  [36/42], [94mLoss[0m : 2.45929
[1mStep[0m  [40/42], [94mLoss[0m : 2.46776

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.577, [92mTest[0m: 3.142, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47911
[1mStep[0m  [4/42], [94mLoss[0m : 2.52768
[1mStep[0m  [8/42], [94mLoss[0m : 2.63549
[1mStep[0m  [12/42], [94mLoss[0m : 2.61865
[1mStep[0m  [16/42], [94mLoss[0m : 2.54029
[1mStep[0m  [20/42], [94mLoss[0m : 2.35006
[1mStep[0m  [24/42], [94mLoss[0m : 2.45034
[1mStep[0m  [28/42], [94mLoss[0m : 2.45139
[1mStep[0m  [32/42], [94mLoss[0m : 2.52052
[1mStep[0m  [36/42], [94mLoss[0m : 2.53661
[1mStep[0m  [40/42], [94mLoss[0m : 2.67280

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.725, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56853
[1mStep[0m  [4/42], [94mLoss[0m : 2.43682
[1mStep[0m  [8/42], [94mLoss[0m : 2.62000
[1mStep[0m  [12/42], [94mLoss[0m : 2.37707
[1mStep[0m  [16/42], [94mLoss[0m : 2.55338
[1mStep[0m  [20/42], [94mLoss[0m : 2.31674
[1mStep[0m  [24/42], [94mLoss[0m : 2.28349
[1mStep[0m  [28/42], [94mLoss[0m : 2.57373
[1mStep[0m  [32/42], [94mLoss[0m : 2.40259
[1mStep[0m  [36/42], [94mLoss[0m : 2.36537
[1mStep[0m  [40/42], [94mLoss[0m : 2.57480

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.646, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68664
[1mStep[0m  [4/42], [94mLoss[0m : 2.42554
[1mStep[0m  [8/42], [94mLoss[0m : 2.39553
[1mStep[0m  [12/42], [94mLoss[0m : 2.41108
[1mStep[0m  [16/42], [94mLoss[0m : 2.65492
[1mStep[0m  [20/42], [94mLoss[0m : 2.41189
[1mStep[0m  [24/42], [94mLoss[0m : 2.60070
[1mStep[0m  [28/42], [94mLoss[0m : 2.30431
[1mStep[0m  [32/42], [94mLoss[0m : 2.42955
[1mStep[0m  [36/42], [94mLoss[0m : 2.34978
[1mStep[0m  [40/42], [94mLoss[0m : 2.57570

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.626, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37695
[1mStep[0m  [4/42], [94mLoss[0m : 2.61018
[1mStep[0m  [8/42], [94mLoss[0m : 2.35606
[1mStep[0m  [12/42], [94mLoss[0m : 2.69861
[1mStep[0m  [16/42], [94mLoss[0m : 2.46208
[1mStep[0m  [20/42], [94mLoss[0m : 2.62561
[1mStep[0m  [24/42], [94mLoss[0m : 2.39173
[1mStep[0m  [28/42], [94mLoss[0m : 2.31150
[1mStep[0m  [32/42], [94mLoss[0m : 2.42339
[1mStep[0m  [36/42], [94mLoss[0m : 2.36389
[1mStep[0m  [40/42], [94mLoss[0m : 2.42380

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.597, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48573
[1mStep[0m  [4/42], [94mLoss[0m : 2.52455
[1mStep[0m  [8/42], [94mLoss[0m : 2.46334
[1mStep[0m  [12/42], [94mLoss[0m : 2.32297
[1mStep[0m  [16/42], [94mLoss[0m : 2.32686
[1mStep[0m  [20/42], [94mLoss[0m : 2.47952
[1mStep[0m  [24/42], [94mLoss[0m : 2.30763
[1mStep[0m  [28/42], [94mLoss[0m : 2.38673
[1mStep[0m  [32/42], [94mLoss[0m : 2.49680
[1mStep[0m  [36/42], [94mLoss[0m : 2.38806
[1mStep[0m  [40/42], [94mLoss[0m : 2.42683

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.551, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32812
[1mStep[0m  [4/42], [94mLoss[0m : 2.60709
[1mStep[0m  [8/42], [94mLoss[0m : 2.71724
[1mStep[0m  [12/42], [94mLoss[0m : 2.38484
[1mStep[0m  [16/42], [94mLoss[0m : 2.32867
[1mStep[0m  [20/42], [94mLoss[0m : 2.47465
[1mStep[0m  [24/42], [94mLoss[0m : 2.54852
[1mStep[0m  [28/42], [94mLoss[0m : 2.71045
[1mStep[0m  [32/42], [94mLoss[0m : 2.41855
[1mStep[0m  [36/42], [94mLoss[0m : 2.67012
[1mStep[0m  [40/42], [94mLoss[0m : 2.54676

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49940
[1mStep[0m  [4/42], [94mLoss[0m : 2.50472
[1mStep[0m  [8/42], [94mLoss[0m : 2.30977
[1mStep[0m  [12/42], [94mLoss[0m : 2.51963
[1mStep[0m  [16/42], [94mLoss[0m : 2.33286
[1mStep[0m  [20/42], [94mLoss[0m : 2.29423
[1mStep[0m  [24/42], [94mLoss[0m : 2.45676
[1mStep[0m  [28/42], [94mLoss[0m : 2.60448
[1mStep[0m  [32/42], [94mLoss[0m : 2.43287
[1mStep[0m  [36/42], [94mLoss[0m : 2.39762
[1mStep[0m  [40/42], [94mLoss[0m : 2.33755

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.534, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59015
[1mStep[0m  [4/42], [94mLoss[0m : 2.43739
[1mStep[0m  [8/42], [94mLoss[0m : 2.37956
[1mStep[0m  [12/42], [94mLoss[0m : 2.74142
[1mStep[0m  [16/42], [94mLoss[0m : 2.31686
[1mStep[0m  [20/42], [94mLoss[0m : 2.31238
[1mStep[0m  [24/42], [94mLoss[0m : 2.36270
[1mStep[0m  [28/42], [94mLoss[0m : 2.44383
[1mStep[0m  [32/42], [94mLoss[0m : 2.34667
[1mStep[0m  [36/42], [94mLoss[0m : 2.48422
[1mStep[0m  [40/42], [94mLoss[0m : 2.54993

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44063
[1mStep[0m  [4/42], [94mLoss[0m : 2.45134
[1mStep[0m  [8/42], [94mLoss[0m : 2.35880
[1mStep[0m  [12/42], [94mLoss[0m : 2.60530
[1mStep[0m  [16/42], [94mLoss[0m : 2.30420
[1mStep[0m  [20/42], [94mLoss[0m : 2.40174
[1mStep[0m  [24/42], [94mLoss[0m : 2.35616
[1mStep[0m  [28/42], [94mLoss[0m : 2.50683
[1mStep[0m  [32/42], [94mLoss[0m : 2.34745
[1mStep[0m  [36/42], [94mLoss[0m : 2.37756
[1mStep[0m  [40/42], [94mLoss[0m : 2.49517

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46746
[1mStep[0m  [4/42], [94mLoss[0m : 2.58954
[1mStep[0m  [8/42], [94mLoss[0m : 2.50035
[1mStep[0m  [12/42], [94mLoss[0m : 2.30742
[1mStep[0m  [16/42], [94mLoss[0m : 2.31246
[1mStep[0m  [20/42], [94mLoss[0m : 2.68592
[1mStep[0m  [24/42], [94mLoss[0m : 2.41579
[1mStep[0m  [28/42], [94mLoss[0m : 2.26704
[1mStep[0m  [32/42], [94mLoss[0m : 2.38097
[1mStep[0m  [36/42], [94mLoss[0m : 2.33970
[1mStep[0m  [40/42], [94mLoss[0m : 2.57841

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31905
[1mStep[0m  [4/42], [94mLoss[0m : 2.47065
[1mStep[0m  [8/42], [94mLoss[0m : 2.30026
[1mStep[0m  [12/42], [94mLoss[0m : 2.37501
[1mStep[0m  [16/42], [94mLoss[0m : 2.54949
[1mStep[0m  [20/42], [94mLoss[0m : 2.38039
[1mStep[0m  [24/42], [94mLoss[0m : 2.46591
[1mStep[0m  [28/42], [94mLoss[0m : 2.47108
[1mStep[0m  [32/42], [94mLoss[0m : 2.49296
[1mStep[0m  [36/42], [94mLoss[0m : 2.55573
[1mStep[0m  [40/42], [94mLoss[0m : 2.37054

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22301
[1mStep[0m  [4/42], [94mLoss[0m : 2.37390
[1mStep[0m  [8/42], [94mLoss[0m : 2.26737
[1mStep[0m  [12/42], [94mLoss[0m : 2.36015
[1mStep[0m  [16/42], [94mLoss[0m : 2.55187
[1mStep[0m  [20/42], [94mLoss[0m : 2.29311
[1mStep[0m  [24/42], [94mLoss[0m : 2.36472
[1mStep[0m  [28/42], [94mLoss[0m : 2.25140
[1mStep[0m  [32/42], [94mLoss[0m : 2.49734
[1mStep[0m  [36/42], [94mLoss[0m : 2.44809
[1mStep[0m  [40/42], [94mLoss[0m : 2.24640

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47994
[1mStep[0m  [4/42], [94mLoss[0m : 2.35684
[1mStep[0m  [8/42], [94mLoss[0m : 2.45937
[1mStep[0m  [12/42], [94mLoss[0m : 2.34546
[1mStep[0m  [16/42], [94mLoss[0m : 2.28544
[1mStep[0m  [20/42], [94mLoss[0m : 2.35286
[1mStep[0m  [24/42], [94mLoss[0m : 2.39229
[1mStep[0m  [28/42], [94mLoss[0m : 2.42150
[1mStep[0m  [32/42], [94mLoss[0m : 2.62274
[1mStep[0m  [36/42], [94mLoss[0m : 2.70040
[1mStep[0m  [40/42], [94mLoss[0m : 2.38630

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47788
[1mStep[0m  [4/42], [94mLoss[0m : 2.41835
[1mStep[0m  [8/42], [94mLoss[0m : 2.26667
[1mStep[0m  [12/42], [94mLoss[0m : 2.42949
[1mStep[0m  [16/42], [94mLoss[0m : 2.33215
[1mStep[0m  [20/42], [94mLoss[0m : 2.43846
[1mStep[0m  [24/42], [94mLoss[0m : 2.48283
[1mStep[0m  [28/42], [94mLoss[0m : 2.53885
[1mStep[0m  [32/42], [94mLoss[0m : 2.45293
[1mStep[0m  [36/42], [94mLoss[0m : 2.34514
[1mStep[0m  [40/42], [94mLoss[0m : 2.41835

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68171
[1mStep[0m  [4/42], [94mLoss[0m : 2.46496
[1mStep[0m  [8/42], [94mLoss[0m : 2.32312
[1mStep[0m  [12/42], [94mLoss[0m : 2.45028
[1mStep[0m  [16/42], [94mLoss[0m : 2.32381
[1mStep[0m  [20/42], [94mLoss[0m : 2.34889
[1mStep[0m  [24/42], [94mLoss[0m : 2.31570
[1mStep[0m  [28/42], [94mLoss[0m : 2.41309
[1mStep[0m  [32/42], [94mLoss[0m : 2.35372
[1mStep[0m  [36/42], [94mLoss[0m : 2.53003
[1mStep[0m  [40/42], [94mLoss[0m : 2.50666

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.489, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41948
[1mStep[0m  [4/42], [94mLoss[0m : 2.28508
[1mStep[0m  [8/42], [94mLoss[0m : 2.23777
[1mStep[0m  [12/42], [94mLoss[0m : 2.53964
[1mStep[0m  [16/42], [94mLoss[0m : 2.27283
[1mStep[0m  [20/42], [94mLoss[0m : 2.40553
[1mStep[0m  [24/42], [94mLoss[0m : 2.35202
[1mStep[0m  [28/42], [94mLoss[0m : 2.39093
[1mStep[0m  [32/42], [94mLoss[0m : 2.31279
[1mStep[0m  [36/42], [94mLoss[0m : 2.57512
[1mStep[0m  [40/42], [94mLoss[0m : 2.59998

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38871
[1mStep[0m  [4/42], [94mLoss[0m : 2.40939
[1mStep[0m  [8/42], [94mLoss[0m : 2.49931
[1mStep[0m  [12/42], [94mLoss[0m : 2.34656
[1mStep[0m  [16/42], [94mLoss[0m : 2.32263
[1mStep[0m  [20/42], [94mLoss[0m : 2.44621
[1mStep[0m  [24/42], [94mLoss[0m : 2.33936
[1mStep[0m  [28/42], [94mLoss[0m : 2.48621
[1mStep[0m  [32/42], [94mLoss[0m : 2.28085
[1mStep[0m  [36/42], [94mLoss[0m : 2.39606
[1mStep[0m  [40/42], [94mLoss[0m : 2.46349

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33509
[1mStep[0m  [4/42], [94mLoss[0m : 2.26177
[1mStep[0m  [8/42], [94mLoss[0m : 2.27269
[1mStep[0m  [12/42], [94mLoss[0m : 2.41395
[1mStep[0m  [16/42], [94mLoss[0m : 2.21677
[1mStep[0m  [20/42], [94mLoss[0m : 2.34485
[1mStep[0m  [24/42], [94mLoss[0m : 2.35472
[1mStep[0m  [28/42], [94mLoss[0m : 2.15910
[1mStep[0m  [32/42], [94mLoss[0m : 2.27141
[1mStep[0m  [36/42], [94mLoss[0m : 2.33605
[1mStep[0m  [40/42], [94mLoss[0m : 2.45175

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11226
[1mStep[0m  [4/42], [94mLoss[0m : 2.37170
[1mStep[0m  [8/42], [94mLoss[0m : 2.42632
[1mStep[0m  [12/42], [94mLoss[0m : 2.29177
[1mStep[0m  [16/42], [94mLoss[0m : 2.32801
[1mStep[0m  [20/42], [94mLoss[0m : 2.30274
[1mStep[0m  [24/42], [94mLoss[0m : 2.48479
[1mStep[0m  [28/42], [94mLoss[0m : 2.74679
[1mStep[0m  [32/42], [94mLoss[0m : 2.36054
[1mStep[0m  [36/42], [94mLoss[0m : 2.16044
[1mStep[0m  [40/42], [94mLoss[0m : 2.43552

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.488, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25455
[1mStep[0m  [4/42], [94mLoss[0m : 2.20124
[1mStep[0m  [8/42], [94mLoss[0m : 2.68528
[1mStep[0m  [12/42], [94mLoss[0m : 2.45583
[1mStep[0m  [16/42], [94mLoss[0m : 2.39693
[1mStep[0m  [20/42], [94mLoss[0m : 2.26664
[1mStep[0m  [24/42], [94mLoss[0m : 2.21879
[1mStep[0m  [28/42], [94mLoss[0m : 2.41055
[1mStep[0m  [32/42], [94mLoss[0m : 2.37301
[1mStep[0m  [36/42], [94mLoss[0m : 2.30184
[1mStep[0m  [40/42], [94mLoss[0m : 2.33279

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11321
[1mStep[0m  [4/42], [94mLoss[0m : 2.35357
[1mStep[0m  [8/42], [94mLoss[0m : 2.52634
[1mStep[0m  [12/42], [94mLoss[0m : 2.35928
[1mStep[0m  [16/42], [94mLoss[0m : 2.39116
[1mStep[0m  [20/42], [94mLoss[0m : 2.09950
[1mStep[0m  [24/42], [94mLoss[0m : 2.27509
[1mStep[0m  [28/42], [94mLoss[0m : 2.29986
[1mStep[0m  [32/42], [94mLoss[0m : 2.30474
[1mStep[0m  [36/42], [94mLoss[0m : 2.34386
[1mStep[0m  [40/42], [94mLoss[0m : 2.61642

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.443, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35288
[1mStep[0m  [4/42], [94mLoss[0m : 2.24407
[1mStep[0m  [8/42], [94mLoss[0m : 2.18094
[1mStep[0m  [12/42], [94mLoss[0m : 2.29405
[1mStep[0m  [16/42], [94mLoss[0m : 2.51431
[1mStep[0m  [20/42], [94mLoss[0m : 2.43849
[1mStep[0m  [24/42], [94mLoss[0m : 2.28861
[1mStep[0m  [28/42], [94mLoss[0m : 2.30928
[1mStep[0m  [32/42], [94mLoss[0m : 2.30543
[1mStep[0m  [36/42], [94mLoss[0m : 2.68864
[1mStep[0m  [40/42], [94mLoss[0m : 2.26840

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.475, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36905
[1mStep[0m  [4/42], [94mLoss[0m : 2.11616
[1mStep[0m  [8/42], [94mLoss[0m : 2.20591
[1mStep[0m  [12/42], [94mLoss[0m : 2.05115
[1mStep[0m  [16/42], [94mLoss[0m : 2.50866
[1mStep[0m  [20/42], [94mLoss[0m : 2.44178
[1mStep[0m  [24/42], [94mLoss[0m : 2.47418
[1mStep[0m  [28/42], [94mLoss[0m : 2.46159
[1mStep[0m  [32/42], [94mLoss[0m : 2.63352
[1mStep[0m  [36/42], [94mLoss[0m : 2.22308
[1mStep[0m  [40/42], [94mLoss[0m : 2.55675

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.452, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25923
[1mStep[0m  [4/42], [94mLoss[0m : 2.35022
[1mStep[0m  [8/42], [94mLoss[0m : 2.43739
[1mStep[0m  [12/42], [94mLoss[0m : 2.56756
[1mStep[0m  [16/42], [94mLoss[0m : 2.50119
[1mStep[0m  [20/42], [94mLoss[0m : 2.34658
[1mStep[0m  [24/42], [94mLoss[0m : 2.42717
[1mStep[0m  [28/42], [94mLoss[0m : 2.23945
[1mStep[0m  [32/42], [94mLoss[0m : 2.20694
[1mStep[0m  [36/42], [94mLoss[0m : 2.47417
[1mStep[0m  [40/42], [94mLoss[0m : 2.31503

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50991
[1mStep[0m  [4/42], [94mLoss[0m : 2.36902
[1mStep[0m  [8/42], [94mLoss[0m : 2.43434
[1mStep[0m  [12/42], [94mLoss[0m : 2.32333
[1mStep[0m  [16/42], [94mLoss[0m : 2.42190
[1mStep[0m  [20/42], [94mLoss[0m : 2.26743
[1mStep[0m  [24/42], [94mLoss[0m : 2.27024
[1mStep[0m  [28/42], [94mLoss[0m : 2.27687
[1mStep[0m  [32/42], [94mLoss[0m : 2.32153
[1mStep[0m  [36/42], [94mLoss[0m : 2.26919
[1mStep[0m  [40/42], [94mLoss[0m : 2.54162

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.480, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22342
[1mStep[0m  [4/42], [94mLoss[0m : 2.34197
[1mStep[0m  [8/42], [94mLoss[0m : 2.28790
[1mStep[0m  [12/42], [94mLoss[0m : 2.45051
[1mStep[0m  [16/42], [94mLoss[0m : 2.51048
[1mStep[0m  [20/42], [94mLoss[0m : 2.17550
[1mStep[0m  [24/42], [94mLoss[0m : 2.32057
[1mStep[0m  [28/42], [94mLoss[0m : 2.47731
[1mStep[0m  [32/42], [94mLoss[0m : 2.39044
[1mStep[0m  [36/42], [94mLoss[0m : 2.23140
[1mStep[0m  [40/42], [94mLoss[0m : 2.31387

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.503, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15128
[1mStep[0m  [4/42], [94mLoss[0m : 2.32898
[1mStep[0m  [8/42], [94mLoss[0m : 2.37437
[1mStep[0m  [12/42], [94mLoss[0m : 2.49318
[1mStep[0m  [16/42], [94mLoss[0m : 2.17635
[1mStep[0m  [20/42], [94mLoss[0m : 2.63562
[1mStep[0m  [24/42], [94mLoss[0m : 2.41250
[1mStep[0m  [28/42], [94mLoss[0m : 2.21686
[1mStep[0m  [32/42], [94mLoss[0m : 2.31557
[1mStep[0m  [36/42], [94mLoss[0m : 2.50307
[1mStep[0m  [40/42], [94mLoss[0m : 2.34008

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.436, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15548
[1mStep[0m  [4/42], [94mLoss[0m : 2.39909
[1mStep[0m  [8/42], [94mLoss[0m : 2.36244
[1mStep[0m  [12/42], [94mLoss[0m : 2.34218
[1mStep[0m  [16/42], [94mLoss[0m : 2.29054
[1mStep[0m  [20/42], [94mLoss[0m : 2.47280
[1mStep[0m  [24/42], [94mLoss[0m : 2.29965
[1mStep[0m  [28/42], [94mLoss[0m : 2.13019
[1mStep[0m  [32/42], [94mLoss[0m : 2.29316
[1mStep[0m  [36/42], [94mLoss[0m : 2.37966
[1mStep[0m  [40/42], [94mLoss[0m : 2.24853

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.416, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.457
====================================

Phase 1 - Evaluation MAE:  2.4565723964146207
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.34474
[1mStep[0m  [4/42], [94mLoss[0m : 2.20350
[1mStep[0m  [8/42], [94mLoss[0m : 2.53378
[1mStep[0m  [12/42], [94mLoss[0m : 2.48911
[1mStep[0m  [16/42], [94mLoss[0m : 2.35847
[1mStep[0m  [20/42], [94mLoss[0m : 2.37168
[1mStep[0m  [24/42], [94mLoss[0m : 2.45609
[1mStep[0m  [28/42], [94mLoss[0m : 2.45727
[1mStep[0m  [32/42], [94mLoss[0m : 2.42306
[1mStep[0m  [36/42], [94mLoss[0m : 2.42198
[1mStep[0m  [40/42], [94mLoss[0m : 2.51513

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30862
[1mStep[0m  [4/42], [94mLoss[0m : 2.15973
[1mStep[0m  [8/42], [94mLoss[0m : 2.51501
[1mStep[0m  [12/42], [94mLoss[0m : 2.11562
[1mStep[0m  [16/42], [94mLoss[0m : 2.29815
[1mStep[0m  [20/42], [94mLoss[0m : 2.37050
[1mStep[0m  [24/42], [94mLoss[0m : 2.45410
[1mStep[0m  [28/42], [94mLoss[0m : 2.45469
[1mStep[0m  [32/42], [94mLoss[0m : 2.18017
[1mStep[0m  [36/42], [94mLoss[0m : 2.27031
[1mStep[0m  [40/42], [94mLoss[0m : 2.37930

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32137
[1mStep[0m  [4/42], [94mLoss[0m : 2.20031
[1mStep[0m  [8/42], [94mLoss[0m : 2.17371
[1mStep[0m  [12/42], [94mLoss[0m : 2.32485
[1mStep[0m  [16/42], [94mLoss[0m : 2.05937
[1mStep[0m  [20/42], [94mLoss[0m : 2.44114
[1mStep[0m  [24/42], [94mLoss[0m : 2.21994
[1mStep[0m  [28/42], [94mLoss[0m : 2.29154
[1mStep[0m  [32/42], [94mLoss[0m : 2.25544
[1mStep[0m  [36/42], [94mLoss[0m : 2.14687
[1mStep[0m  [40/42], [94mLoss[0m : 2.43835

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04250
[1mStep[0m  [4/42], [94mLoss[0m : 2.30165
[1mStep[0m  [8/42], [94mLoss[0m : 2.36604
[1mStep[0m  [12/42], [94mLoss[0m : 2.29251
[1mStep[0m  [16/42], [94mLoss[0m : 2.38316
[1mStep[0m  [20/42], [94mLoss[0m : 2.08333
[1mStep[0m  [24/42], [94mLoss[0m : 2.03770
[1mStep[0m  [28/42], [94mLoss[0m : 2.27500
[1mStep[0m  [32/42], [94mLoss[0m : 2.22465
[1mStep[0m  [36/42], [94mLoss[0m : 2.24377
[1mStep[0m  [40/42], [94mLoss[0m : 2.17165

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17575
[1mStep[0m  [4/42], [94mLoss[0m : 2.21373
[1mStep[0m  [8/42], [94mLoss[0m : 2.13749
[1mStep[0m  [12/42], [94mLoss[0m : 2.21614
[1mStep[0m  [16/42], [94mLoss[0m : 2.33871
[1mStep[0m  [20/42], [94mLoss[0m : 1.97644
[1mStep[0m  [24/42], [94mLoss[0m : 2.26699
[1mStep[0m  [28/42], [94mLoss[0m : 2.21518
[1mStep[0m  [32/42], [94mLoss[0m : 2.18574
[1mStep[0m  [36/42], [94mLoss[0m : 2.16587
[1mStep[0m  [40/42], [94mLoss[0m : 2.09463

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27098
[1mStep[0m  [4/42], [94mLoss[0m : 1.91642
[1mStep[0m  [8/42], [94mLoss[0m : 2.24519
[1mStep[0m  [12/42], [94mLoss[0m : 1.96929
[1mStep[0m  [16/42], [94mLoss[0m : 2.02312
[1mStep[0m  [20/42], [94mLoss[0m : 2.22758
[1mStep[0m  [24/42], [94mLoss[0m : 2.04291
[1mStep[0m  [28/42], [94mLoss[0m : 1.98787
[1mStep[0m  [32/42], [94mLoss[0m : 1.92136
[1mStep[0m  [36/42], [94mLoss[0m : 2.42983
[1mStep[0m  [40/42], [94mLoss[0m : 2.14197

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16723
[1mStep[0m  [4/42], [94mLoss[0m : 2.11380
[1mStep[0m  [8/42], [94mLoss[0m : 2.06497
[1mStep[0m  [12/42], [94mLoss[0m : 2.02116
[1mStep[0m  [16/42], [94mLoss[0m : 2.06666
[1mStep[0m  [20/42], [94mLoss[0m : 1.94186
[1mStep[0m  [24/42], [94mLoss[0m : 2.08657
[1mStep[0m  [28/42], [94mLoss[0m : 1.74182
[1mStep[0m  [32/42], [94mLoss[0m : 2.21039
[1mStep[0m  [36/42], [94mLoss[0m : 1.94952
[1mStep[0m  [40/42], [94mLoss[0m : 2.06377

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01454
[1mStep[0m  [4/42], [94mLoss[0m : 2.02656
[1mStep[0m  [8/42], [94mLoss[0m : 2.12074
[1mStep[0m  [12/42], [94mLoss[0m : 2.09861
[1mStep[0m  [16/42], [94mLoss[0m : 1.97268
[1mStep[0m  [20/42], [94mLoss[0m : 2.13142
[1mStep[0m  [24/42], [94mLoss[0m : 1.92367
[1mStep[0m  [28/42], [94mLoss[0m : 1.83828
[1mStep[0m  [32/42], [94mLoss[0m : 2.20907
[1mStep[0m  [36/42], [94mLoss[0m : 2.12247
[1mStep[0m  [40/42], [94mLoss[0m : 2.15404

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04392
[1mStep[0m  [4/42], [94mLoss[0m : 2.06782
[1mStep[0m  [8/42], [94mLoss[0m : 1.98500
[1mStep[0m  [12/42], [94mLoss[0m : 1.83349
[1mStep[0m  [16/42], [94mLoss[0m : 2.10707
[1mStep[0m  [20/42], [94mLoss[0m : 1.89252
[1mStep[0m  [24/42], [94mLoss[0m : 1.72783
[1mStep[0m  [28/42], [94mLoss[0m : 2.04028
[1mStep[0m  [32/42], [94mLoss[0m : 2.01561
[1mStep[0m  [36/42], [94mLoss[0m : 1.92816
[1mStep[0m  [40/42], [94mLoss[0m : 1.94646

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88221
[1mStep[0m  [4/42], [94mLoss[0m : 1.89059
[1mStep[0m  [8/42], [94mLoss[0m : 1.90556
[1mStep[0m  [12/42], [94mLoss[0m : 1.91862
[1mStep[0m  [16/42], [94mLoss[0m : 1.90214
[1mStep[0m  [20/42], [94mLoss[0m : 1.97039
[1mStep[0m  [24/42], [94mLoss[0m : 1.95680
[1mStep[0m  [28/42], [94mLoss[0m : 2.12729
[1mStep[0m  [32/42], [94mLoss[0m : 2.03698
[1mStep[0m  [36/42], [94mLoss[0m : 1.95084
[1mStep[0m  [40/42], [94mLoss[0m : 2.00572

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69668
[1mStep[0m  [4/42], [94mLoss[0m : 1.84525
[1mStep[0m  [8/42], [94mLoss[0m : 1.75745
[1mStep[0m  [12/42], [94mLoss[0m : 1.77070
[1mStep[0m  [16/42], [94mLoss[0m : 1.77436
[1mStep[0m  [20/42], [94mLoss[0m : 2.03949
[1mStep[0m  [24/42], [94mLoss[0m : 2.04949
[1mStep[0m  [28/42], [94mLoss[0m : 2.05982
[1mStep[0m  [32/42], [94mLoss[0m : 2.03540
[1mStep[0m  [36/42], [94mLoss[0m : 1.92289
[1mStep[0m  [40/42], [94mLoss[0m : 1.94325

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79285
[1mStep[0m  [4/42], [94mLoss[0m : 1.96041
[1mStep[0m  [8/42], [94mLoss[0m : 1.95265
[1mStep[0m  [12/42], [94mLoss[0m : 1.86974
[1mStep[0m  [16/42], [94mLoss[0m : 1.92973
[1mStep[0m  [20/42], [94mLoss[0m : 1.83523
[1mStep[0m  [24/42], [94mLoss[0m : 1.84137
[1mStep[0m  [28/42], [94mLoss[0m : 2.01892
[1mStep[0m  [32/42], [94mLoss[0m : 1.87455
[1mStep[0m  [36/42], [94mLoss[0m : 2.02405
[1mStep[0m  [40/42], [94mLoss[0m : 1.95640

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.513, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95090
[1mStep[0m  [4/42], [94mLoss[0m : 1.81882
[1mStep[0m  [8/42], [94mLoss[0m : 1.91997
[1mStep[0m  [12/42], [94mLoss[0m : 1.69534
[1mStep[0m  [16/42], [94mLoss[0m : 1.80706
[1mStep[0m  [20/42], [94mLoss[0m : 1.89301
[1mStep[0m  [24/42], [94mLoss[0m : 1.74118
[1mStep[0m  [28/42], [94mLoss[0m : 2.05758
[1mStep[0m  [32/42], [94mLoss[0m : 1.79314
[1mStep[0m  [36/42], [94mLoss[0m : 1.80359
[1mStep[0m  [40/42], [94mLoss[0m : 1.85935

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89823
[1mStep[0m  [4/42], [94mLoss[0m : 1.64866
[1mStep[0m  [8/42], [94mLoss[0m : 1.52068
[1mStep[0m  [12/42], [94mLoss[0m : 1.76077
[1mStep[0m  [16/42], [94mLoss[0m : 1.64979
[1mStep[0m  [20/42], [94mLoss[0m : 1.65402
[1mStep[0m  [24/42], [94mLoss[0m : 1.75010
[1mStep[0m  [28/42], [94mLoss[0m : 1.82615
[1mStep[0m  [32/42], [94mLoss[0m : 1.78227
[1mStep[0m  [36/42], [94mLoss[0m : 1.94675
[1mStep[0m  [40/42], [94mLoss[0m : 1.71682

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.534, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75458
[1mStep[0m  [4/42], [94mLoss[0m : 1.67613
[1mStep[0m  [8/42], [94mLoss[0m : 1.84669
[1mStep[0m  [12/42], [94mLoss[0m : 1.95705
[1mStep[0m  [16/42], [94mLoss[0m : 1.83129
[1mStep[0m  [20/42], [94mLoss[0m : 1.65434
[1mStep[0m  [24/42], [94mLoss[0m : 1.82463
[1mStep[0m  [28/42], [94mLoss[0m : 1.65613
[1mStep[0m  [32/42], [94mLoss[0m : 1.83803
[1mStep[0m  [36/42], [94mLoss[0m : 1.79120
[1mStep[0m  [40/42], [94mLoss[0m : 1.80083

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.749, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64603
[1mStep[0m  [4/42], [94mLoss[0m : 1.68138
[1mStep[0m  [8/42], [94mLoss[0m : 1.76215
[1mStep[0m  [12/42], [94mLoss[0m : 1.78977
[1mStep[0m  [16/42], [94mLoss[0m : 1.64539
[1mStep[0m  [20/42], [94mLoss[0m : 1.69071
[1mStep[0m  [24/42], [94mLoss[0m : 1.73524
[1mStep[0m  [28/42], [94mLoss[0m : 1.67370
[1mStep[0m  [32/42], [94mLoss[0m : 1.72069
[1mStep[0m  [36/42], [94mLoss[0m : 1.61245
[1mStep[0m  [40/42], [94mLoss[0m : 1.92216

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.536, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52375
[1mStep[0m  [4/42], [94mLoss[0m : 1.75481
[1mStep[0m  [8/42], [94mLoss[0m : 1.55588
[1mStep[0m  [12/42], [94mLoss[0m : 1.85677
[1mStep[0m  [16/42], [94mLoss[0m : 1.75245
[1mStep[0m  [20/42], [94mLoss[0m : 1.67176
[1mStep[0m  [24/42], [94mLoss[0m : 1.85562
[1mStep[0m  [28/42], [94mLoss[0m : 1.81611
[1mStep[0m  [32/42], [94mLoss[0m : 1.79709
[1mStep[0m  [36/42], [94mLoss[0m : 1.74892
[1mStep[0m  [40/42], [94mLoss[0m : 1.72791

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.529, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68974
[1mStep[0m  [4/42], [94mLoss[0m : 1.85931
[1mStep[0m  [8/42], [94mLoss[0m : 1.67570
[1mStep[0m  [12/42], [94mLoss[0m : 1.69683
[1mStep[0m  [16/42], [94mLoss[0m : 1.45385
[1mStep[0m  [20/42], [94mLoss[0m : 1.84439
[1mStep[0m  [24/42], [94mLoss[0m : 1.73390
[1mStep[0m  [28/42], [94mLoss[0m : 1.65011
[1mStep[0m  [32/42], [94mLoss[0m : 1.70709
[1mStep[0m  [36/42], [94mLoss[0m : 1.59078
[1mStep[0m  [40/42], [94mLoss[0m : 1.61375

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.658, [92mTest[0m: 2.644, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57466
[1mStep[0m  [4/42], [94mLoss[0m : 1.58524
[1mStep[0m  [8/42], [94mLoss[0m : 1.60192
[1mStep[0m  [12/42], [94mLoss[0m : 1.70525
[1mStep[0m  [16/42], [94mLoss[0m : 1.66294
[1mStep[0m  [20/42], [94mLoss[0m : 1.67598
[1mStep[0m  [24/42], [94mLoss[0m : 1.63495
[1mStep[0m  [28/42], [94mLoss[0m : 1.75432
[1mStep[0m  [32/42], [94mLoss[0m : 1.64489
[1mStep[0m  [36/42], [94mLoss[0m : 1.62827
[1mStep[0m  [40/42], [94mLoss[0m : 1.56523

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.642, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55982
[1mStep[0m  [4/42], [94mLoss[0m : 1.45586
[1mStep[0m  [8/42], [94mLoss[0m : 1.54363
[1mStep[0m  [12/42], [94mLoss[0m : 1.45117
[1mStep[0m  [16/42], [94mLoss[0m : 1.61221
[1mStep[0m  [20/42], [94mLoss[0m : 1.50080
[1mStep[0m  [24/42], [94mLoss[0m : 1.63678
[1mStep[0m  [28/42], [94mLoss[0m : 1.54539
[1mStep[0m  [32/42], [94mLoss[0m : 1.48335
[1mStep[0m  [36/42], [94mLoss[0m : 1.51399
[1mStep[0m  [40/42], [94mLoss[0m : 1.65781

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.573, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46866
[1mStep[0m  [4/42], [94mLoss[0m : 1.53104
[1mStep[0m  [8/42], [94mLoss[0m : 1.52837
[1mStep[0m  [12/42], [94mLoss[0m : 1.51329
[1mStep[0m  [16/42], [94mLoss[0m : 1.36306
[1mStep[0m  [20/42], [94mLoss[0m : 1.57667
[1mStep[0m  [24/42], [94mLoss[0m : 1.55687
[1mStep[0m  [28/42], [94mLoss[0m : 1.53820
[1mStep[0m  [32/42], [94mLoss[0m : 1.48601
[1mStep[0m  [36/42], [94mLoss[0m : 1.63708
[1mStep[0m  [40/42], [94mLoss[0m : 1.59793

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.715, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61842
[1mStep[0m  [4/42], [94mLoss[0m : 1.42868
[1mStep[0m  [8/42], [94mLoss[0m : 1.57311
[1mStep[0m  [12/42], [94mLoss[0m : 1.49979
[1mStep[0m  [16/42], [94mLoss[0m : 1.48412
[1mStep[0m  [20/42], [94mLoss[0m : 1.47994
[1mStep[0m  [24/42], [94mLoss[0m : 1.56574
[1mStep[0m  [28/42], [94mLoss[0m : 1.47642
[1mStep[0m  [32/42], [94mLoss[0m : 1.52653
[1mStep[0m  [36/42], [94mLoss[0m : 1.59125
[1mStep[0m  [40/42], [94mLoss[0m : 1.40763

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.591, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.53513
[1mStep[0m  [4/42], [94mLoss[0m : 1.44498
[1mStep[0m  [8/42], [94mLoss[0m : 1.55157
[1mStep[0m  [12/42], [94mLoss[0m : 1.56531
[1mStep[0m  [16/42], [94mLoss[0m : 1.33933
[1mStep[0m  [20/42], [94mLoss[0m : 1.54113
[1mStep[0m  [24/42], [94mLoss[0m : 1.38989
[1mStep[0m  [28/42], [94mLoss[0m : 1.48582
[1mStep[0m  [32/42], [94mLoss[0m : 1.44872
[1mStep[0m  [36/42], [94mLoss[0m : 1.51061
[1mStep[0m  [40/42], [94mLoss[0m : 1.46593

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.573, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.33549
[1mStep[0m  [4/42], [94mLoss[0m : 1.46622
[1mStep[0m  [8/42], [94mLoss[0m : 1.42147
[1mStep[0m  [12/42], [94mLoss[0m : 1.33052
[1mStep[0m  [16/42], [94mLoss[0m : 1.38862
[1mStep[0m  [20/42], [94mLoss[0m : 1.56160
[1mStep[0m  [24/42], [94mLoss[0m : 1.44946
[1mStep[0m  [28/42], [94mLoss[0m : 1.37501
[1mStep[0m  [32/42], [94mLoss[0m : 1.56252
[1mStep[0m  [36/42], [94mLoss[0m : 1.62262
[1mStep[0m  [40/42], [94mLoss[0m : 1.66145

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.641, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.584
====================================

Phase 2 - Evaluation MAE:  2.584397384098598
MAE score P1      2.456572
MAE score P2      2.584397
loss              1.463732
learning_rate         0.01
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.32721
[1mStep[0m  [8/84], [94mLoss[0m : 10.18858
[1mStep[0m  [16/84], [94mLoss[0m : 10.66975
[1mStep[0m  [24/84], [94mLoss[0m : 10.16383
[1mStep[0m  [32/84], [94mLoss[0m : 10.49632
[1mStep[0m  [40/84], [94mLoss[0m : 10.00528
[1mStep[0m  [48/84], [94mLoss[0m : 10.14650
[1mStep[0m  [56/84], [94mLoss[0m : 10.02499
[1mStep[0m  [64/84], [94mLoss[0m : 9.52278
[1mStep[0m  [72/84], [94mLoss[0m : 10.10325
[1mStep[0m  [80/84], [94mLoss[0m : 9.87578

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.285, [92mTest[0m: 10.926, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.76947
[1mStep[0m  [8/84], [94mLoss[0m : 9.41678
[1mStep[0m  [16/84], [94mLoss[0m : 9.14650
[1mStep[0m  [24/84], [94mLoss[0m : 9.25338
[1mStep[0m  [32/84], [94mLoss[0m : 9.84455
[1mStep[0m  [40/84], [94mLoss[0m : 9.20342
[1mStep[0m  [48/84], [94mLoss[0m : 9.53604
[1mStep[0m  [56/84], [94mLoss[0m : 8.42335
[1mStep[0m  [64/84], [94mLoss[0m : 8.65909
[1mStep[0m  [72/84], [94mLoss[0m : 8.72195
[1mStep[0m  [80/84], [94mLoss[0m : 8.66403

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.930, [92mTest[0m: 9.531, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.53237
[1mStep[0m  [8/84], [94mLoss[0m : 7.90252
[1mStep[0m  [16/84], [94mLoss[0m : 7.87810
[1mStep[0m  [24/84], [94mLoss[0m : 7.75464
[1mStep[0m  [32/84], [94mLoss[0m : 7.59372
[1mStep[0m  [40/84], [94mLoss[0m : 7.14858
[1mStep[0m  [48/84], [94mLoss[0m : 6.88890
[1mStep[0m  [56/84], [94mLoss[0m : 6.36054
[1mStep[0m  [64/84], [94mLoss[0m : 6.03344
[1mStep[0m  [72/84], [94mLoss[0m : 5.98335
[1mStep[0m  [80/84], [94mLoss[0m : 5.77153

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.765, [92mTest[0m: 7.615, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.18439
[1mStep[0m  [8/84], [94mLoss[0m : 4.84289
[1mStep[0m  [16/84], [94mLoss[0m : 5.09924
[1mStep[0m  [24/84], [94mLoss[0m : 5.06412
[1mStep[0m  [32/84], [94mLoss[0m : 4.46650
[1mStep[0m  [40/84], [94mLoss[0m : 4.42135
[1mStep[0m  [48/84], [94mLoss[0m : 4.17928
[1mStep[0m  [56/84], [94mLoss[0m : 4.53642
[1mStep[0m  [64/84], [94mLoss[0m : 3.74798
[1mStep[0m  [72/84], [94mLoss[0m : 3.78069
[1mStep[0m  [80/84], [94mLoss[0m : 3.26213

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.489, [92mTest[0m: 4.676, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.57664
[1mStep[0m  [8/84], [94mLoss[0m : 3.38129
[1mStep[0m  [16/84], [94mLoss[0m : 3.23588
[1mStep[0m  [24/84], [94mLoss[0m : 3.13102
[1mStep[0m  [32/84], [94mLoss[0m : 3.17426
[1mStep[0m  [40/84], [94mLoss[0m : 3.25185
[1mStep[0m  [48/84], [94mLoss[0m : 2.74791
[1mStep[0m  [56/84], [94mLoss[0m : 2.47536
[1mStep[0m  [64/84], [94mLoss[0m : 2.64897
[1mStep[0m  [72/84], [94mLoss[0m : 2.98662
[1mStep[0m  [80/84], [94mLoss[0m : 2.47438

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.997, [92mTest[0m: 2.994, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61729
[1mStep[0m  [8/84], [94mLoss[0m : 2.92698
[1mStep[0m  [16/84], [94mLoss[0m : 2.30561
[1mStep[0m  [24/84], [94mLoss[0m : 2.72003
[1mStep[0m  [32/84], [94mLoss[0m : 2.47818
[1mStep[0m  [40/84], [94mLoss[0m : 2.46651
[1mStep[0m  [48/84], [94mLoss[0m : 2.79610
[1mStep[0m  [56/84], [94mLoss[0m : 2.47786
[1mStep[0m  [64/84], [94mLoss[0m : 2.54869
[1mStep[0m  [72/84], [94mLoss[0m : 3.02602
[1mStep[0m  [80/84], [94mLoss[0m : 2.53383

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81260
[1mStep[0m  [8/84], [94mLoss[0m : 2.72882
[1mStep[0m  [16/84], [94mLoss[0m : 2.78602
[1mStep[0m  [24/84], [94mLoss[0m : 2.35273
[1mStep[0m  [32/84], [94mLoss[0m : 2.75951
[1mStep[0m  [40/84], [94mLoss[0m : 2.65096
[1mStep[0m  [48/84], [94mLoss[0m : 2.56818
[1mStep[0m  [56/84], [94mLoss[0m : 2.65549
[1mStep[0m  [64/84], [94mLoss[0m : 2.87074
[1mStep[0m  [72/84], [94mLoss[0m : 2.62573
[1mStep[0m  [80/84], [94mLoss[0m : 2.79335

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46170
[1mStep[0m  [8/84], [94mLoss[0m : 2.68109
[1mStep[0m  [16/84], [94mLoss[0m : 2.36272
[1mStep[0m  [24/84], [94mLoss[0m : 2.54823
[1mStep[0m  [32/84], [94mLoss[0m : 2.68393
[1mStep[0m  [40/84], [94mLoss[0m : 2.39566
[1mStep[0m  [48/84], [94mLoss[0m : 2.62521
[1mStep[0m  [56/84], [94mLoss[0m : 2.60962
[1mStep[0m  [64/84], [94mLoss[0m : 2.64481
[1mStep[0m  [72/84], [94mLoss[0m : 2.45106
[1mStep[0m  [80/84], [94mLoss[0m : 2.55342

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72833
[1mStep[0m  [8/84], [94mLoss[0m : 2.84411
[1mStep[0m  [16/84], [94mLoss[0m : 2.42582
[1mStep[0m  [24/84], [94mLoss[0m : 2.52598
[1mStep[0m  [32/84], [94mLoss[0m : 2.56285
[1mStep[0m  [40/84], [94mLoss[0m : 2.38119
[1mStep[0m  [48/84], [94mLoss[0m : 2.41103
[1mStep[0m  [56/84], [94mLoss[0m : 2.66381
[1mStep[0m  [64/84], [94mLoss[0m : 2.30002
[1mStep[0m  [72/84], [94mLoss[0m : 2.25576
[1mStep[0m  [80/84], [94mLoss[0m : 2.85534

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62093
[1mStep[0m  [8/84], [94mLoss[0m : 2.63238
[1mStep[0m  [16/84], [94mLoss[0m : 2.49016
[1mStep[0m  [24/84], [94mLoss[0m : 2.54269
[1mStep[0m  [32/84], [94mLoss[0m : 2.83266
[1mStep[0m  [40/84], [94mLoss[0m : 2.67996
[1mStep[0m  [48/84], [94mLoss[0m : 2.63620
[1mStep[0m  [56/84], [94mLoss[0m : 2.86931
[1mStep[0m  [64/84], [94mLoss[0m : 2.30790
[1mStep[0m  [72/84], [94mLoss[0m : 2.58516
[1mStep[0m  [80/84], [94mLoss[0m : 2.49245

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67943
[1mStep[0m  [8/84], [94mLoss[0m : 2.69208
[1mStep[0m  [16/84], [94mLoss[0m : 2.37895
[1mStep[0m  [24/84], [94mLoss[0m : 2.39244
[1mStep[0m  [32/84], [94mLoss[0m : 2.47738
[1mStep[0m  [40/84], [94mLoss[0m : 2.71544
[1mStep[0m  [48/84], [94mLoss[0m : 2.54972
[1mStep[0m  [56/84], [94mLoss[0m : 2.77666
[1mStep[0m  [64/84], [94mLoss[0m : 2.61442
[1mStep[0m  [72/84], [94mLoss[0m : 2.90282
[1mStep[0m  [80/84], [94mLoss[0m : 2.62864

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50924
[1mStep[0m  [8/84], [94mLoss[0m : 2.40571
[1mStep[0m  [16/84], [94mLoss[0m : 2.67019
[1mStep[0m  [24/84], [94mLoss[0m : 2.49861
[1mStep[0m  [32/84], [94mLoss[0m : 2.84227
[1mStep[0m  [40/84], [94mLoss[0m : 2.36047
[1mStep[0m  [48/84], [94mLoss[0m : 2.65732
[1mStep[0m  [56/84], [94mLoss[0m : 2.51778
[1mStep[0m  [64/84], [94mLoss[0m : 2.21368
[1mStep[0m  [72/84], [94mLoss[0m : 2.47627
[1mStep[0m  [80/84], [94mLoss[0m : 2.59757

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67912
[1mStep[0m  [8/84], [94mLoss[0m : 2.44210
[1mStep[0m  [16/84], [94mLoss[0m : 2.36914
[1mStep[0m  [24/84], [94mLoss[0m : 2.28799
[1mStep[0m  [32/84], [94mLoss[0m : 2.76818
[1mStep[0m  [40/84], [94mLoss[0m : 2.30874
[1mStep[0m  [48/84], [94mLoss[0m : 2.50265
[1mStep[0m  [56/84], [94mLoss[0m : 2.54847
[1mStep[0m  [64/84], [94mLoss[0m : 2.57495
[1mStep[0m  [72/84], [94mLoss[0m : 2.28682
[1mStep[0m  [80/84], [94mLoss[0m : 2.34470

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51940
[1mStep[0m  [8/84], [94mLoss[0m : 2.65007
[1mStep[0m  [16/84], [94mLoss[0m : 2.37776
[1mStep[0m  [24/84], [94mLoss[0m : 2.71693
[1mStep[0m  [32/84], [94mLoss[0m : 2.34448
[1mStep[0m  [40/84], [94mLoss[0m : 2.61948
[1mStep[0m  [48/84], [94mLoss[0m : 2.50949
[1mStep[0m  [56/84], [94mLoss[0m : 2.63358
[1mStep[0m  [64/84], [94mLoss[0m : 2.60109
[1mStep[0m  [72/84], [94mLoss[0m : 2.36022
[1mStep[0m  [80/84], [94mLoss[0m : 2.37812

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66436
[1mStep[0m  [8/84], [94mLoss[0m : 2.38088
[1mStep[0m  [16/84], [94mLoss[0m : 2.41752
[1mStep[0m  [24/84], [94mLoss[0m : 2.44809
[1mStep[0m  [32/84], [94mLoss[0m : 2.39589
[1mStep[0m  [40/84], [94mLoss[0m : 2.33527
[1mStep[0m  [48/84], [94mLoss[0m : 2.46152
[1mStep[0m  [56/84], [94mLoss[0m : 2.63051
[1mStep[0m  [64/84], [94mLoss[0m : 2.57074
[1mStep[0m  [72/84], [94mLoss[0m : 2.52220
[1mStep[0m  [80/84], [94mLoss[0m : 2.53101

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55015
[1mStep[0m  [8/84], [94mLoss[0m : 2.45266
[1mStep[0m  [16/84], [94mLoss[0m : 2.14411
[1mStep[0m  [24/84], [94mLoss[0m : 2.41862
[1mStep[0m  [32/84], [94mLoss[0m : 2.64160
[1mStep[0m  [40/84], [94mLoss[0m : 2.50978
[1mStep[0m  [48/84], [94mLoss[0m : 2.57574
[1mStep[0m  [56/84], [94mLoss[0m : 2.66381
[1mStep[0m  [64/84], [94mLoss[0m : 2.72978
[1mStep[0m  [72/84], [94mLoss[0m : 2.30648
[1mStep[0m  [80/84], [94mLoss[0m : 2.53872

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49317
[1mStep[0m  [8/84], [94mLoss[0m : 2.41192
[1mStep[0m  [16/84], [94mLoss[0m : 2.25379
[1mStep[0m  [24/84], [94mLoss[0m : 2.02616
[1mStep[0m  [32/84], [94mLoss[0m : 2.30624
[1mStep[0m  [40/84], [94mLoss[0m : 2.81249
[1mStep[0m  [48/84], [94mLoss[0m : 2.62564
[1mStep[0m  [56/84], [94mLoss[0m : 2.53797
[1mStep[0m  [64/84], [94mLoss[0m : 2.67454
[1mStep[0m  [72/84], [94mLoss[0m : 2.43279
[1mStep[0m  [80/84], [94mLoss[0m : 2.60952

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44606
[1mStep[0m  [8/84], [94mLoss[0m : 2.63207
[1mStep[0m  [16/84], [94mLoss[0m : 2.53670
[1mStep[0m  [24/84], [94mLoss[0m : 2.57061
[1mStep[0m  [32/84], [94mLoss[0m : 2.65122
[1mStep[0m  [40/84], [94mLoss[0m : 2.29215
[1mStep[0m  [48/84], [94mLoss[0m : 2.20632
[1mStep[0m  [56/84], [94mLoss[0m : 2.31459
[1mStep[0m  [64/84], [94mLoss[0m : 2.65239
[1mStep[0m  [72/84], [94mLoss[0m : 2.94483
[1mStep[0m  [80/84], [94mLoss[0m : 2.66783

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30067
[1mStep[0m  [8/84], [94mLoss[0m : 2.38009
[1mStep[0m  [16/84], [94mLoss[0m : 2.42677
[1mStep[0m  [24/84], [94mLoss[0m : 2.86811
[1mStep[0m  [32/84], [94mLoss[0m : 2.65467
[1mStep[0m  [40/84], [94mLoss[0m : 2.56289
[1mStep[0m  [48/84], [94mLoss[0m : 2.44973
[1mStep[0m  [56/84], [94mLoss[0m : 2.51010
[1mStep[0m  [64/84], [94mLoss[0m : 2.71168
[1mStep[0m  [72/84], [94mLoss[0m : 2.47529
[1mStep[0m  [80/84], [94mLoss[0m : 2.28252

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50229
[1mStep[0m  [8/84], [94mLoss[0m : 2.24260
[1mStep[0m  [16/84], [94mLoss[0m : 2.36456
[1mStep[0m  [24/84], [94mLoss[0m : 2.50557
[1mStep[0m  [32/84], [94mLoss[0m : 2.70931
[1mStep[0m  [40/84], [94mLoss[0m : 2.64424
[1mStep[0m  [48/84], [94mLoss[0m : 2.44410
[1mStep[0m  [56/84], [94mLoss[0m : 2.78966
[1mStep[0m  [64/84], [94mLoss[0m : 2.35219
[1mStep[0m  [72/84], [94mLoss[0m : 2.42671
[1mStep[0m  [80/84], [94mLoss[0m : 2.58159

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24359
[1mStep[0m  [8/84], [94mLoss[0m : 2.47086
[1mStep[0m  [16/84], [94mLoss[0m : 2.33199
[1mStep[0m  [24/84], [94mLoss[0m : 2.66799
[1mStep[0m  [32/84], [94mLoss[0m : 2.40508
[1mStep[0m  [40/84], [94mLoss[0m : 2.54315
[1mStep[0m  [48/84], [94mLoss[0m : 2.54952
[1mStep[0m  [56/84], [94mLoss[0m : 2.38197
[1mStep[0m  [64/84], [94mLoss[0m : 2.65454
[1mStep[0m  [72/84], [94mLoss[0m : 2.44781
[1mStep[0m  [80/84], [94mLoss[0m : 2.58510

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50619
[1mStep[0m  [8/84], [94mLoss[0m : 2.56058
[1mStep[0m  [16/84], [94mLoss[0m : 2.55530
[1mStep[0m  [24/84], [94mLoss[0m : 2.24327
[1mStep[0m  [32/84], [94mLoss[0m : 2.24602
[1mStep[0m  [40/84], [94mLoss[0m : 2.39857
[1mStep[0m  [48/84], [94mLoss[0m : 2.50007
[1mStep[0m  [56/84], [94mLoss[0m : 2.10970
[1mStep[0m  [64/84], [94mLoss[0m : 2.45683
[1mStep[0m  [72/84], [94mLoss[0m : 2.65544
[1mStep[0m  [80/84], [94mLoss[0m : 2.43811

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52783
[1mStep[0m  [8/84], [94mLoss[0m : 2.53036
[1mStep[0m  [16/84], [94mLoss[0m : 2.38974
[1mStep[0m  [24/84], [94mLoss[0m : 2.41702
[1mStep[0m  [32/84], [94mLoss[0m : 2.33367
[1mStep[0m  [40/84], [94mLoss[0m : 2.53266
[1mStep[0m  [48/84], [94mLoss[0m : 2.43675
[1mStep[0m  [56/84], [94mLoss[0m : 2.55111
[1mStep[0m  [64/84], [94mLoss[0m : 2.40732
[1mStep[0m  [72/84], [94mLoss[0m : 2.35263
[1mStep[0m  [80/84], [94mLoss[0m : 2.62355

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29990
[1mStep[0m  [8/84], [94mLoss[0m : 2.34016
[1mStep[0m  [16/84], [94mLoss[0m : 2.39248
[1mStep[0m  [24/84], [94mLoss[0m : 2.82723
[1mStep[0m  [32/84], [94mLoss[0m : 2.20225
[1mStep[0m  [40/84], [94mLoss[0m : 2.21091
[1mStep[0m  [48/84], [94mLoss[0m : 2.66095
[1mStep[0m  [56/84], [94mLoss[0m : 2.40683
[1mStep[0m  [64/84], [94mLoss[0m : 2.57568
[1mStep[0m  [72/84], [94mLoss[0m : 2.49583
[1mStep[0m  [80/84], [94mLoss[0m : 2.21638

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71452
[1mStep[0m  [8/84], [94mLoss[0m : 2.08945
[1mStep[0m  [16/84], [94mLoss[0m : 2.14226
[1mStep[0m  [24/84], [94mLoss[0m : 2.41276
[1mStep[0m  [32/84], [94mLoss[0m : 2.11996
[1mStep[0m  [40/84], [94mLoss[0m : 2.28371
[1mStep[0m  [48/84], [94mLoss[0m : 2.23901
[1mStep[0m  [56/84], [94mLoss[0m : 2.33459
[1mStep[0m  [64/84], [94mLoss[0m : 2.66679
[1mStep[0m  [72/84], [94mLoss[0m : 2.16111
[1mStep[0m  [80/84], [94mLoss[0m : 2.76044

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27125
[1mStep[0m  [8/84], [94mLoss[0m : 2.31680
[1mStep[0m  [16/84], [94mLoss[0m : 2.57353
[1mStep[0m  [24/84], [94mLoss[0m : 2.54376
[1mStep[0m  [32/84], [94mLoss[0m : 2.22345
[1mStep[0m  [40/84], [94mLoss[0m : 2.39622
[1mStep[0m  [48/84], [94mLoss[0m : 2.70172
[1mStep[0m  [56/84], [94mLoss[0m : 2.27551
[1mStep[0m  [64/84], [94mLoss[0m : 2.34435
[1mStep[0m  [72/84], [94mLoss[0m : 2.34316
[1mStep[0m  [80/84], [94mLoss[0m : 2.67785

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26717
[1mStep[0m  [8/84], [94mLoss[0m : 2.56415
[1mStep[0m  [16/84], [94mLoss[0m : 2.20915
[1mStep[0m  [24/84], [94mLoss[0m : 2.30653
[1mStep[0m  [32/84], [94mLoss[0m : 2.30550
[1mStep[0m  [40/84], [94mLoss[0m : 2.51105
[1mStep[0m  [48/84], [94mLoss[0m : 2.57489
[1mStep[0m  [56/84], [94mLoss[0m : 2.30621
[1mStep[0m  [64/84], [94mLoss[0m : 2.06802
[1mStep[0m  [72/84], [94mLoss[0m : 2.36903
[1mStep[0m  [80/84], [94mLoss[0m : 2.30470

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.314, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64847
[1mStep[0m  [8/84], [94mLoss[0m : 2.41000
[1mStep[0m  [16/84], [94mLoss[0m : 2.55041
[1mStep[0m  [24/84], [94mLoss[0m : 2.34606
[1mStep[0m  [32/84], [94mLoss[0m : 2.48343
[1mStep[0m  [40/84], [94mLoss[0m : 2.40445
[1mStep[0m  [48/84], [94mLoss[0m : 2.31875
[1mStep[0m  [56/84], [94mLoss[0m : 2.33717
[1mStep[0m  [64/84], [94mLoss[0m : 2.59686
[1mStep[0m  [72/84], [94mLoss[0m : 2.29891
[1mStep[0m  [80/84], [94mLoss[0m : 2.35641

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30625
[1mStep[0m  [8/84], [94mLoss[0m : 2.39490
[1mStep[0m  [16/84], [94mLoss[0m : 2.11497
[1mStep[0m  [24/84], [94mLoss[0m : 2.31368
[1mStep[0m  [32/84], [94mLoss[0m : 2.21255
[1mStep[0m  [40/84], [94mLoss[0m : 2.61772
[1mStep[0m  [48/84], [94mLoss[0m : 2.53371
[1mStep[0m  [56/84], [94mLoss[0m : 2.54481
[1mStep[0m  [64/84], [94mLoss[0m : 2.45047
[1mStep[0m  [72/84], [94mLoss[0m : 2.22125
[1mStep[0m  [80/84], [94mLoss[0m : 2.51128

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43559
[1mStep[0m  [8/84], [94mLoss[0m : 2.45144
[1mStep[0m  [16/84], [94mLoss[0m : 2.41302
[1mStep[0m  [24/84], [94mLoss[0m : 2.61506
[1mStep[0m  [32/84], [94mLoss[0m : 2.60495
[1mStep[0m  [40/84], [94mLoss[0m : 2.21077
[1mStep[0m  [48/84], [94mLoss[0m : 2.18283
[1mStep[0m  [56/84], [94mLoss[0m : 2.57293
[1mStep[0m  [64/84], [94mLoss[0m : 2.60278
[1mStep[0m  [72/84], [94mLoss[0m : 2.41235
[1mStep[0m  [80/84], [94mLoss[0m : 2.65868

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.326113130365099
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.23082
[1mStep[0m  [8/84], [94mLoss[0m : 2.61359
[1mStep[0m  [16/84], [94mLoss[0m : 2.54664
[1mStep[0m  [24/84], [94mLoss[0m : 2.40636
[1mStep[0m  [32/84], [94mLoss[0m : 2.44326
[1mStep[0m  [40/84], [94mLoss[0m : 2.43326
[1mStep[0m  [48/84], [94mLoss[0m : 2.48515
[1mStep[0m  [56/84], [94mLoss[0m : 2.50176
[1mStep[0m  [64/84], [94mLoss[0m : 2.28490
[1mStep[0m  [72/84], [94mLoss[0m : 2.59070
[1mStep[0m  [80/84], [94mLoss[0m : 2.19368

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49236
[1mStep[0m  [8/84], [94mLoss[0m : 2.21654
[1mStep[0m  [16/84], [94mLoss[0m : 2.55969
[1mStep[0m  [24/84], [94mLoss[0m : 2.37239
[1mStep[0m  [32/84], [94mLoss[0m : 2.75142
[1mStep[0m  [40/84], [94mLoss[0m : 2.26612
[1mStep[0m  [48/84], [94mLoss[0m : 2.42700
[1mStep[0m  [56/84], [94mLoss[0m : 2.37209
[1mStep[0m  [64/84], [94mLoss[0m : 2.41808
[1mStep[0m  [72/84], [94mLoss[0m : 2.16152
[1mStep[0m  [80/84], [94mLoss[0m : 2.64909

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02357
[1mStep[0m  [8/84], [94mLoss[0m : 2.39950
[1mStep[0m  [16/84], [94mLoss[0m : 2.44709
[1mStep[0m  [24/84], [94mLoss[0m : 2.23376
[1mStep[0m  [32/84], [94mLoss[0m : 2.41758
[1mStep[0m  [40/84], [94mLoss[0m : 2.67646
[1mStep[0m  [48/84], [94mLoss[0m : 2.31691
[1mStep[0m  [56/84], [94mLoss[0m : 2.51665
[1mStep[0m  [64/84], [94mLoss[0m : 2.29903
[1mStep[0m  [72/84], [94mLoss[0m : 2.45373
[1mStep[0m  [80/84], [94mLoss[0m : 2.56926

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27414
[1mStep[0m  [8/84], [94mLoss[0m : 2.29128
[1mStep[0m  [16/84], [94mLoss[0m : 2.32482
[1mStep[0m  [24/84], [94mLoss[0m : 2.44234
[1mStep[0m  [32/84], [94mLoss[0m : 2.52519
[1mStep[0m  [40/84], [94mLoss[0m : 2.05890
[1mStep[0m  [48/84], [94mLoss[0m : 2.16185
[1mStep[0m  [56/84], [94mLoss[0m : 2.24832
[1mStep[0m  [64/84], [94mLoss[0m : 2.58065
[1mStep[0m  [72/84], [94mLoss[0m : 2.70785
[1mStep[0m  [80/84], [94mLoss[0m : 2.60646

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45233
[1mStep[0m  [8/84], [94mLoss[0m : 2.03659
[1mStep[0m  [16/84], [94mLoss[0m : 2.18575
[1mStep[0m  [24/84], [94mLoss[0m : 2.37060
[1mStep[0m  [32/84], [94mLoss[0m : 2.15754
[1mStep[0m  [40/84], [94mLoss[0m : 2.07027
[1mStep[0m  [48/84], [94mLoss[0m : 2.51631
[1mStep[0m  [56/84], [94mLoss[0m : 2.21401
[1mStep[0m  [64/84], [94mLoss[0m : 2.34161
[1mStep[0m  [72/84], [94mLoss[0m : 2.42136
[1mStep[0m  [80/84], [94mLoss[0m : 2.52279

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31790
[1mStep[0m  [8/84], [94mLoss[0m : 1.97883
[1mStep[0m  [16/84], [94mLoss[0m : 2.29072
[1mStep[0m  [24/84], [94mLoss[0m : 1.94515
[1mStep[0m  [32/84], [94mLoss[0m : 2.09909
[1mStep[0m  [40/84], [94mLoss[0m : 2.22123
[1mStep[0m  [48/84], [94mLoss[0m : 2.03959
[1mStep[0m  [56/84], [94mLoss[0m : 2.03389
[1mStep[0m  [64/84], [94mLoss[0m : 1.98466
[1mStep[0m  [72/84], [94mLoss[0m : 2.11720
[1mStep[0m  [80/84], [94mLoss[0m : 2.18785

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31194
[1mStep[0m  [8/84], [94mLoss[0m : 2.08191
[1mStep[0m  [16/84], [94mLoss[0m : 2.09189
[1mStep[0m  [24/84], [94mLoss[0m : 2.23566
[1mStep[0m  [32/84], [94mLoss[0m : 2.01955
[1mStep[0m  [40/84], [94mLoss[0m : 2.22096
[1mStep[0m  [48/84], [94mLoss[0m : 1.97008
[1mStep[0m  [56/84], [94mLoss[0m : 2.16747
[1mStep[0m  [64/84], [94mLoss[0m : 2.00196
[1mStep[0m  [72/84], [94mLoss[0m : 2.05718
[1mStep[0m  [80/84], [94mLoss[0m : 2.26221

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.137, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09468
[1mStep[0m  [8/84], [94mLoss[0m : 1.71269
[1mStep[0m  [16/84], [94mLoss[0m : 2.34642
[1mStep[0m  [24/84], [94mLoss[0m : 2.20474
[1mStep[0m  [32/84], [94mLoss[0m : 1.93371
[1mStep[0m  [40/84], [94mLoss[0m : 2.10692
[1mStep[0m  [48/84], [94mLoss[0m : 2.20483
[1mStep[0m  [56/84], [94mLoss[0m : 2.14917
[1mStep[0m  [64/84], [94mLoss[0m : 2.53312
[1mStep[0m  [72/84], [94mLoss[0m : 1.80199
[1mStep[0m  [80/84], [94mLoss[0m : 2.09656

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15540
[1mStep[0m  [8/84], [94mLoss[0m : 1.75068
[1mStep[0m  [16/84], [94mLoss[0m : 2.09431
[1mStep[0m  [24/84], [94mLoss[0m : 1.92362
[1mStep[0m  [32/84], [94mLoss[0m : 1.87503
[1mStep[0m  [40/84], [94mLoss[0m : 2.24925
[1mStep[0m  [48/84], [94mLoss[0m : 2.21580
[1mStep[0m  [56/84], [94mLoss[0m : 1.85897
[1mStep[0m  [64/84], [94mLoss[0m : 2.28392
[1mStep[0m  [72/84], [94mLoss[0m : 2.04984
[1mStep[0m  [80/84], [94mLoss[0m : 2.00516

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04797
[1mStep[0m  [8/84], [94mLoss[0m : 2.11671
[1mStep[0m  [16/84], [94mLoss[0m : 1.72243
[1mStep[0m  [24/84], [94mLoss[0m : 1.72174
[1mStep[0m  [32/84], [94mLoss[0m : 1.77738
[1mStep[0m  [40/84], [94mLoss[0m : 2.11478
[1mStep[0m  [48/84], [94mLoss[0m : 1.68283
[1mStep[0m  [56/84], [94mLoss[0m : 2.16949
[1mStep[0m  [64/84], [94mLoss[0m : 2.22809
[1mStep[0m  [72/84], [94mLoss[0m : 2.16841
[1mStep[0m  [80/84], [94mLoss[0m : 1.96714

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.970, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78624
[1mStep[0m  [8/84], [94mLoss[0m : 1.85956
[1mStep[0m  [16/84], [94mLoss[0m : 1.92488
[1mStep[0m  [24/84], [94mLoss[0m : 2.25849
[1mStep[0m  [32/84], [94mLoss[0m : 1.96770
[1mStep[0m  [40/84], [94mLoss[0m : 1.95743
[1mStep[0m  [48/84], [94mLoss[0m : 2.02030
[1mStep[0m  [56/84], [94mLoss[0m : 1.62174
[1mStep[0m  [64/84], [94mLoss[0m : 1.92750
[1mStep[0m  [72/84], [94mLoss[0m : 1.79066
[1mStep[0m  [80/84], [94mLoss[0m : 1.95494

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79576
[1mStep[0m  [8/84], [94mLoss[0m : 1.85018
[1mStep[0m  [16/84], [94mLoss[0m : 1.82696
[1mStep[0m  [24/84], [94mLoss[0m : 1.76419
[1mStep[0m  [32/84], [94mLoss[0m : 2.03512
[1mStep[0m  [40/84], [94mLoss[0m : 1.77373
[1mStep[0m  [48/84], [94mLoss[0m : 2.10987
[1mStep[0m  [56/84], [94mLoss[0m : 2.02992
[1mStep[0m  [64/84], [94mLoss[0m : 2.10813
[1mStep[0m  [72/84], [94mLoss[0m : 1.83768
[1mStep[0m  [80/84], [94mLoss[0m : 1.94931

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77125
[1mStep[0m  [8/84], [94mLoss[0m : 1.99299
[1mStep[0m  [16/84], [94mLoss[0m : 1.95832
[1mStep[0m  [24/84], [94mLoss[0m : 1.63530
[1mStep[0m  [32/84], [94mLoss[0m : 1.89565
[1mStep[0m  [40/84], [94mLoss[0m : 1.68246
[1mStep[0m  [48/84], [94mLoss[0m : 1.79437
[1mStep[0m  [56/84], [94mLoss[0m : 1.83408
[1mStep[0m  [64/84], [94mLoss[0m : 1.70493
[1mStep[0m  [72/84], [94mLoss[0m : 1.98898
[1mStep[0m  [80/84], [94mLoss[0m : 1.82179

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78238
[1mStep[0m  [8/84], [94mLoss[0m : 1.94251
[1mStep[0m  [16/84], [94mLoss[0m : 1.77622
[1mStep[0m  [24/84], [94mLoss[0m : 1.77079
[1mStep[0m  [32/84], [94mLoss[0m : 1.66758
[1mStep[0m  [40/84], [94mLoss[0m : 1.81922
[1mStep[0m  [48/84], [94mLoss[0m : 2.03481
[1mStep[0m  [56/84], [94mLoss[0m : 1.64893
[1mStep[0m  [64/84], [94mLoss[0m : 1.84436
[1mStep[0m  [72/84], [94mLoss[0m : 1.73451
[1mStep[0m  [80/84], [94mLoss[0m : 1.80472

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.815, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94321
[1mStep[0m  [8/84], [94mLoss[0m : 1.59375
[1mStep[0m  [16/84], [94mLoss[0m : 1.68012
[1mStep[0m  [24/84], [94mLoss[0m : 1.68002
[1mStep[0m  [32/84], [94mLoss[0m : 1.70496
[1mStep[0m  [40/84], [94mLoss[0m : 1.75159
[1mStep[0m  [48/84], [94mLoss[0m : 1.87766
[1mStep[0m  [56/84], [94mLoss[0m : 1.54119
[1mStep[0m  [64/84], [94mLoss[0m : 1.94865
[1mStep[0m  [72/84], [94mLoss[0m : 1.71629
[1mStep[0m  [80/84], [94mLoss[0m : 1.64961

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44912
[1mStep[0m  [8/84], [94mLoss[0m : 1.74166
[1mStep[0m  [16/84], [94mLoss[0m : 1.77769
[1mStep[0m  [24/84], [94mLoss[0m : 1.77745
[1mStep[0m  [32/84], [94mLoss[0m : 1.98975
[1mStep[0m  [40/84], [94mLoss[0m : 1.93528
[1mStep[0m  [48/84], [94mLoss[0m : 1.95766
[1mStep[0m  [56/84], [94mLoss[0m : 1.63158
[1mStep[0m  [64/84], [94mLoss[0m : 1.94648
[1mStep[0m  [72/84], [94mLoss[0m : 2.06254
[1mStep[0m  [80/84], [94mLoss[0m : 1.70579

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.514, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82216
[1mStep[0m  [8/84], [94mLoss[0m : 1.86986
[1mStep[0m  [16/84], [94mLoss[0m : 1.58145
[1mStep[0m  [24/84], [94mLoss[0m : 1.75748
[1mStep[0m  [32/84], [94mLoss[0m : 1.76835
[1mStep[0m  [40/84], [94mLoss[0m : 1.68124
[1mStep[0m  [48/84], [94mLoss[0m : 1.74605
[1mStep[0m  [56/84], [94mLoss[0m : 1.66148
[1mStep[0m  [64/84], [94mLoss[0m : 1.55149
[1mStep[0m  [72/84], [94mLoss[0m : 2.12298
[1mStep[0m  [80/84], [94mLoss[0m : 1.92869

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.721, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76355
[1mStep[0m  [8/84], [94mLoss[0m : 1.68453
[1mStep[0m  [16/84], [94mLoss[0m : 1.53705
[1mStep[0m  [24/84], [94mLoss[0m : 1.77919
[1mStep[0m  [32/84], [94mLoss[0m : 1.86055
[1mStep[0m  [40/84], [94mLoss[0m : 1.48897
[1mStep[0m  [48/84], [94mLoss[0m : 1.60813
[1mStep[0m  [56/84], [94mLoss[0m : 1.74077
[1mStep[0m  [64/84], [94mLoss[0m : 1.84749
[1mStep[0m  [72/84], [94mLoss[0m : 1.56348
[1mStep[0m  [80/84], [94mLoss[0m : 1.85015

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67781
[1mStep[0m  [8/84], [94mLoss[0m : 1.58434
[1mStep[0m  [16/84], [94mLoss[0m : 1.69978
[1mStep[0m  [24/84], [94mLoss[0m : 2.01750
[1mStep[0m  [32/84], [94mLoss[0m : 1.65973
[1mStep[0m  [40/84], [94mLoss[0m : 1.54520
[1mStep[0m  [48/84], [94mLoss[0m : 1.73132
[1mStep[0m  [56/84], [94mLoss[0m : 1.67878
[1mStep[0m  [64/84], [94mLoss[0m : 2.01408
[1mStep[0m  [72/84], [94mLoss[0m : 1.55243
[1mStep[0m  [80/84], [94mLoss[0m : 1.72846

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49569
[1mStep[0m  [8/84], [94mLoss[0m : 1.56046
[1mStep[0m  [16/84], [94mLoss[0m : 1.49714
[1mStep[0m  [24/84], [94mLoss[0m : 1.53216
[1mStep[0m  [32/84], [94mLoss[0m : 1.69533
[1mStep[0m  [40/84], [94mLoss[0m : 1.43721
[1mStep[0m  [48/84], [94mLoss[0m : 1.70130
[1mStep[0m  [56/84], [94mLoss[0m : 1.52359
[1mStep[0m  [64/84], [94mLoss[0m : 1.83213
[1mStep[0m  [72/84], [94mLoss[0m : 1.55102
[1mStep[0m  [80/84], [94mLoss[0m : 1.71743

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.498, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80976
[1mStep[0m  [8/84], [94mLoss[0m : 1.56278
[1mStep[0m  [16/84], [94mLoss[0m : 1.81336
[1mStep[0m  [24/84], [94mLoss[0m : 1.57628
[1mStep[0m  [32/84], [94mLoss[0m : 1.67083
[1mStep[0m  [40/84], [94mLoss[0m : 1.63568
[1mStep[0m  [48/84], [94mLoss[0m : 1.60920
[1mStep[0m  [56/84], [94mLoss[0m : 1.33560
[1mStep[0m  [64/84], [94mLoss[0m : 1.83503
[1mStep[0m  [72/84], [94mLoss[0m : 1.40093
[1mStep[0m  [80/84], [94mLoss[0m : 1.64060

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59955
[1mStep[0m  [8/84], [94mLoss[0m : 1.63904
[1mStep[0m  [16/84], [94mLoss[0m : 1.59071
[1mStep[0m  [24/84], [94mLoss[0m : 1.64159
[1mStep[0m  [32/84], [94mLoss[0m : 1.61545
[1mStep[0m  [40/84], [94mLoss[0m : 1.78786
[1mStep[0m  [48/84], [94mLoss[0m : 1.70143
[1mStep[0m  [56/84], [94mLoss[0m : 1.60538
[1mStep[0m  [64/84], [94mLoss[0m : 1.43428
[1mStep[0m  [72/84], [94mLoss[0m : 1.79119
[1mStep[0m  [80/84], [94mLoss[0m : 1.63956

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53714
[1mStep[0m  [8/84], [94mLoss[0m : 1.42829
[1mStep[0m  [16/84], [94mLoss[0m : 1.45530
[1mStep[0m  [24/84], [94mLoss[0m : 1.43002
[1mStep[0m  [32/84], [94mLoss[0m : 1.53077
[1mStep[0m  [40/84], [94mLoss[0m : 1.51170
[1mStep[0m  [48/84], [94mLoss[0m : 1.64314
[1mStep[0m  [56/84], [94mLoss[0m : 1.36132
[1mStep[0m  [64/84], [94mLoss[0m : 1.71339
[1mStep[0m  [72/84], [94mLoss[0m : 1.43843
[1mStep[0m  [80/84], [94mLoss[0m : 1.37679

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35125
[1mStep[0m  [8/84], [94mLoss[0m : 1.54842
[1mStep[0m  [16/84], [94mLoss[0m : 1.75409
[1mStep[0m  [24/84], [94mLoss[0m : 1.29318
[1mStep[0m  [32/84], [94mLoss[0m : 1.48001
[1mStep[0m  [40/84], [94mLoss[0m : 1.42221
[1mStep[0m  [48/84], [94mLoss[0m : 1.51808
[1mStep[0m  [56/84], [94mLoss[0m : 1.38360
[1mStep[0m  [64/84], [94mLoss[0m : 1.67832
[1mStep[0m  [72/84], [94mLoss[0m : 1.52063
[1mStep[0m  [80/84], [94mLoss[0m : 1.53323

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43633
[1mStep[0m  [8/84], [94mLoss[0m : 1.36316
[1mStep[0m  [16/84], [94mLoss[0m : 1.65214
[1mStep[0m  [24/84], [94mLoss[0m : 1.50073
[1mStep[0m  [32/84], [94mLoss[0m : 1.29091
[1mStep[0m  [40/84], [94mLoss[0m : 1.40128
[1mStep[0m  [48/84], [94mLoss[0m : 1.45966
[1mStep[0m  [56/84], [94mLoss[0m : 1.62078
[1mStep[0m  [64/84], [94mLoss[0m : 1.69006
[1mStep[0m  [72/84], [94mLoss[0m : 1.53258
[1mStep[0m  [80/84], [94mLoss[0m : 1.68349

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.514, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.23234
[1mStep[0m  [8/84], [94mLoss[0m : 1.50727
[1mStep[0m  [16/84], [94mLoss[0m : 1.71874
[1mStep[0m  [24/84], [94mLoss[0m : 1.40886
[1mStep[0m  [32/84], [94mLoss[0m : 1.46104
[1mStep[0m  [40/84], [94mLoss[0m : 1.50621
[1mStep[0m  [48/84], [94mLoss[0m : 1.31251
[1mStep[0m  [56/84], [94mLoss[0m : 1.49409
[1mStep[0m  [64/84], [94mLoss[0m : 1.53411
[1mStep[0m  [72/84], [94mLoss[0m : 1.50025
[1mStep[0m  [80/84], [94mLoss[0m : 1.59721

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.547, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51920
[1mStep[0m  [8/84], [94mLoss[0m : 1.61783
[1mStep[0m  [16/84], [94mLoss[0m : 1.48691
[1mStep[0m  [24/84], [94mLoss[0m : 1.54295
[1mStep[0m  [32/84], [94mLoss[0m : 1.68457
[1mStep[0m  [40/84], [94mLoss[0m : 1.40359
[1mStep[0m  [48/84], [94mLoss[0m : 1.55033
[1mStep[0m  [56/84], [94mLoss[0m : 1.56146
[1mStep[0m  [64/84], [94mLoss[0m : 1.38730
[1mStep[0m  [72/84], [94mLoss[0m : 1.65153
[1mStep[0m  [80/84], [94mLoss[0m : 1.54759

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.535, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47074
[1mStep[0m  [8/84], [94mLoss[0m : 1.56952
[1mStep[0m  [16/84], [94mLoss[0m : 1.42854
[1mStep[0m  [24/84], [94mLoss[0m : 1.31088
[1mStep[0m  [32/84], [94mLoss[0m : 1.55867
[1mStep[0m  [40/84], [94mLoss[0m : 1.35408
[1mStep[0m  [48/84], [94mLoss[0m : 1.34487
[1mStep[0m  [56/84], [94mLoss[0m : 1.55892
[1mStep[0m  [64/84], [94mLoss[0m : 1.54354
[1mStep[0m  [72/84], [94mLoss[0m : 1.38204
[1mStep[0m  [80/84], [94mLoss[0m : 1.36104

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.502, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57164
[1mStep[0m  [8/84], [94mLoss[0m : 1.24381
[1mStep[0m  [16/84], [94mLoss[0m : 1.49950
[1mStep[0m  [24/84], [94mLoss[0m : 1.37819
[1mStep[0m  [32/84], [94mLoss[0m : 1.44701
[1mStep[0m  [40/84], [94mLoss[0m : 1.58691
[1mStep[0m  [48/84], [94mLoss[0m : 1.43414
[1mStep[0m  [56/84], [94mLoss[0m : 1.30846
[1mStep[0m  [64/84], [94mLoss[0m : 1.42009
[1mStep[0m  [72/84], [94mLoss[0m : 1.55291
[1mStep[0m  [80/84], [94mLoss[0m : 1.47825

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.434, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52832
[1mStep[0m  [8/84], [94mLoss[0m : 1.37764
[1mStep[0m  [16/84], [94mLoss[0m : 1.33634
[1mStep[0m  [24/84], [94mLoss[0m : 1.24896
[1mStep[0m  [32/84], [94mLoss[0m : 1.39951
[1mStep[0m  [40/84], [94mLoss[0m : 1.17097
[1mStep[0m  [48/84], [94mLoss[0m : 1.52837
[1mStep[0m  [56/84], [94mLoss[0m : 1.48086
[1mStep[0m  [64/84], [94mLoss[0m : 1.45150
[1mStep[0m  [72/84], [94mLoss[0m : 1.54738
[1mStep[0m  [80/84], [94mLoss[0m : 1.45273

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.425, [92mTest[0m: 2.468, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.461
====================================

Phase 2 - Evaluation MAE:  2.4614414402416775
MAE score P1       2.326113
MAE score P2       2.461441
loss               1.424547
learning_rate          0.01
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 13, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.11766
[1mStep[0m  [4/42], [94mLoss[0m : 6.68054
[1mStep[0m  [8/42], [94mLoss[0m : 3.22513
[1mStep[0m  [12/42], [94mLoss[0m : 5.02622
[1mStep[0m  [16/42], [94mLoss[0m : 3.31460
[1mStep[0m  [20/42], [94mLoss[0m : 2.46664
[1mStep[0m  [24/42], [94mLoss[0m : 3.44129
[1mStep[0m  [28/42], [94mLoss[0m : 2.83926
[1mStep[0m  [32/42], [94mLoss[0m : 3.01522
[1mStep[0m  [36/42], [94mLoss[0m : 3.03525
[1mStep[0m  [40/42], [94mLoss[0m : 2.60485

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.104, [92mTest[0m: 10.953, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.22903
[1mStep[0m  [4/42], [94mLoss[0m : 2.32203
[1mStep[0m  [8/42], [94mLoss[0m : 2.53003
[1mStep[0m  [12/42], [94mLoss[0m : 2.86629
[1mStep[0m  [16/42], [94mLoss[0m : 2.54578
[1mStep[0m  [20/42], [94mLoss[0m : 2.39645
[1mStep[0m  [24/42], [94mLoss[0m : 2.51814
[1mStep[0m  [28/42], [94mLoss[0m : 2.58151
[1mStep[0m  [32/42], [94mLoss[0m : 2.48838
[1mStep[0m  [36/42], [94mLoss[0m : 2.58605
[1mStep[0m  [40/42], [94mLoss[0m : 2.56000

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.608, [92mTest[0m: 3.518, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35433
[1mStep[0m  [4/42], [94mLoss[0m : 2.63547
[1mStep[0m  [8/42], [94mLoss[0m : 2.42188
[1mStep[0m  [12/42], [94mLoss[0m : 2.58763
[1mStep[0m  [16/42], [94mLoss[0m : 2.59362
[1mStep[0m  [20/42], [94mLoss[0m : 2.29456
[1mStep[0m  [24/42], [94mLoss[0m : 2.44364
[1mStep[0m  [28/42], [94mLoss[0m : 2.70687
[1mStep[0m  [32/42], [94mLoss[0m : 2.47161
[1mStep[0m  [36/42], [94mLoss[0m : 2.50603
[1mStep[0m  [40/42], [94mLoss[0m : 2.49433

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.611, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35530
[1mStep[0m  [4/42], [94mLoss[0m : 2.67940
[1mStep[0m  [8/42], [94mLoss[0m : 2.33184
[1mStep[0m  [12/42], [94mLoss[0m : 2.56090
[1mStep[0m  [16/42], [94mLoss[0m : 2.67358
[1mStep[0m  [20/42], [94mLoss[0m : 2.40151
[1mStep[0m  [24/42], [94mLoss[0m : 2.74555
[1mStep[0m  [28/42], [94mLoss[0m : 2.44959
[1mStep[0m  [32/42], [94mLoss[0m : 2.38019
[1mStep[0m  [36/42], [94mLoss[0m : 2.47573
[1mStep[0m  [40/42], [94mLoss[0m : 2.40793

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36412
[1mStep[0m  [4/42], [94mLoss[0m : 2.37407
[1mStep[0m  [8/42], [94mLoss[0m : 2.38250
[1mStep[0m  [12/42], [94mLoss[0m : 2.54676
[1mStep[0m  [16/42], [94mLoss[0m : 2.36776
[1mStep[0m  [20/42], [94mLoss[0m : 2.46711
[1mStep[0m  [24/42], [94mLoss[0m : 2.46037
[1mStep[0m  [28/42], [94mLoss[0m : 2.37884
[1mStep[0m  [32/42], [94mLoss[0m : 2.31578
[1mStep[0m  [36/42], [94mLoss[0m : 2.36636
[1mStep[0m  [40/42], [94mLoss[0m : 2.74623

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39989
[1mStep[0m  [4/42], [94mLoss[0m : 2.37715
[1mStep[0m  [8/42], [94mLoss[0m : 2.29657
[1mStep[0m  [12/42], [94mLoss[0m : 2.65766
[1mStep[0m  [16/42], [94mLoss[0m : 2.30514
[1mStep[0m  [20/42], [94mLoss[0m : 2.38675
[1mStep[0m  [24/42], [94mLoss[0m : 2.40849
[1mStep[0m  [28/42], [94mLoss[0m : 2.47405
[1mStep[0m  [32/42], [94mLoss[0m : 2.40794
[1mStep[0m  [36/42], [94mLoss[0m : 2.48650
[1mStep[0m  [40/42], [94mLoss[0m : 2.49615

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42445
[1mStep[0m  [4/42], [94mLoss[0m : 2.42105
[1mStep[0m  [8/42], [94mLoss[0m : 2.41041
[1mStep[0m  [12/42], [94mLoss[0m : 2.26128
[1mStep[0m  [16/42], [94mLoss[0m : 2.39165
[1mStep[0m  [20/42], [94mLoss[0m : 2.34165
[1mStep[0m  [24/42], [94mLoss[0m : 2.46071
[1mStep[0m  [28/42], [94mLoss[0m : 2.41144
[1mStep[0m  [32/42], [94mLoss[0m : 2.55655
[1mStep[0m  [36/42], [94mLoss[0m : 2.39523
[1mStep[0m  [40/42], [94mLoss[0m : 2.48039

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33839
[1mStep[0m  [4/42], [94mLoss[0m : 2.22738
[1mStep[0m  [8/42], [94mLoss[0m : 2.38994
[1mStep[0m  [12/42], [94mLoss[0m : 2.43031
[1mStep[0m  [16/42], [94mLoss[0m : 2.27720
[1mStep[0m  [20/42], [94mLoss[0m : 2.27260
[1mStep[0m  [24/42], [94mLoss[0m : 2.32594
[1mStep[0m  [28/42], [94mLoss[0m : 2.37045
[1mStep[0m  [32/42], [94mLoss[0m : 2.61200
[1mStep[0m  [36/42], [94mLoss[0m : 2.69415
[1mStep[0m  [40/42], [94mLoss[0m : 2.64081

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30640
[1mStep[0m  [4/42], [94mLoss[0m : 2.54224
[1mStep[0m  [8/42], [94mLoss[0m : 2.38436
[1mStep[0m  [12/42], [94mLoss[0m : 2.50419
[1mStep[0m  [16/42], [94mLoss[0m : 2.38468
[1mStep[0m  [20/42], [94mLoss[0m : 2.27500
[1mStep[0m  [24/42], [94mLoss[0m : 2.49666
[1mStep[0m  [28/42], [94mLoss[0m : 2.32167
[1mStep[0m  [32/42], [94mLoss[0m : 2.32366
[1mStep[0m  [36/42], [94mLoss[0m : 2.41423
[1mStep[0m  [40/42], [94mLoss[0m : 2.60564

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36223
[1mStep[0m  [4/42], [94mLoss[0m : 2.44059
[1mStep[0m  [8/42], [94mLoss[0m : 2.29982
[1mStep[0m  [12/42], [94mLoss[0m : 2.32207
[1mStep[0m  [16/42], [94mLoss[0m : 2.41053
[1mStep[0m  [20/42], [94mLoss[0m : 2.54984
[1mStep[0m  [24/42], [94mLoss[0m : 2.59203
[1mStep[0m  [28/42], [94mLoss[0m : 2.32931
[1mStep[0m  [32/42], [94mLoss[0m : 2.47329
[1mStep[0m  [36/42], [94mLoss[0m : 2.33225
[1mStep[0m  [40/42], [94mLoss[0m : 2.35574

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43373
[1mStep[0m  [4/42], [94mLoss[0m : 2.48273
[1mStep[0m  [8/42], [94mLoss[0m : 2.42788
[1mStep[0m  [12/42], [94mLoss[0m : 2.56950
[1mStep[0m  [16/42], [94mLoss[0m : 2.53543
[1mStep[0m  [20/42], [94mLoss[0m : 2.48340
[1mStep[0m  [24/42], [94mLoss[0m : 2.32894
[1mStep[0m  [28/42], [94mLoss[0m : 2.39718
[1mStep[0m  [32/42], [94mLoss[0m : 2.37908
[1mStep[0m  [36/42], [94mLoss[0m : 2.30693
[1mStep[0m  [40/42], [94mLoss[0m : 2.30553

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41545
[1mStep[0m  [4/42], [94mLoss[0m : 2.37545
[1mStep[0m  [8/42], [94mLoss[0m : 2.43273
[1mStep[0m  [12/42], [94mLoss[0m : 2.18892
[1mStep[0m  [16/42], [94mLoss[0m : 2.16651
[1mStep[0m  [20/42], [94mLoss[0m : 2.36546
[1mStep[0m  [24/42], [94mLoss[0m : 2.40121
[1mStep[0m  [28/42], [94mLoss[0m : 2.48219
[1mStep[0m  [32/42], [94mLoss[0m : 2.35795
[1mStep[0m  [36/42], [94mLoss[0m : 2.38781
[1mStep[0m  [40/42], [94mLoss[0m : 2.37629

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20824
[1mStep[0m  [4/42], [94mLoss[0m : 2.52296
[1mStep[0m  [8/42], [94mLoss[0m : 2.43226
[1mStep[0m  [12/42], [94mLoss[0m : 2.47556
[1mStep[0m  [16/42], [94mLoss[0m : 2.25429
[1mStep[0m  [20/42], [94mLoss[0m : 2.46937
[1mStep[0m  [24/42], [94mLoss[0m : 2.42709
[1mStep[0m  [28/42], [94mLoss[0m : 2.39292
[1mStep[0m  [32/42], [94mLoss[0m : 2.40705
[1mStep[0m  [36/42], [94mLoss[0m : 2.20803
[1mStep[0m  [40/42], [94mLoss[0m : 2.28291

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29289
[1mStep[0m  [4/42], [94mLoss[0m : 2.54343
[1mStep[0m  [8/42], [94mLoss[0m : 2.40686
[1mStep[0m  [12/42], [94mLoss[0m : 2.37787
[1mStep[0m  [16/42], [94mLoss[0m : 2.31121
[1mStep[0m  [20/42], [94mLoss[0m : 2.70507
[1mStep[0m  [24/42], [94mLoss[0m : 2.56007
[1mStep[0m  [28/42], [94mLoss[0m : 2.21214
[1mStep[0m  [32/42], [94mLoss[0m : 2.31831
[1mStep[0m  [36/42], [94mLoss[0m : 2.42447
[1mStep[0m  [40/42], [94mLoss[0m : 2.25924

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22095
[1mStep[0m  [4/42], [94mLoss[0m : 2.53547
[1mStep[0m  [8/42], [94mLoss[0m : 2.35285
[1mStep[0m  [12/42], [94mLoss[0m : 2.33546
[1mStep[0m  [16/42], [94mLoss[0m : 2.39498
[1mStep[0m  [20/42], [94mLoss[0m : 2.35280
[1mStep[0m  [24/42], [94mLoss[0m : 2.38552
[1mStep[0m  [28/42], [94mLoss[0m : 2.40428
[1mStep[0m  [32/42], [94mLoss[0m : 2.34764
[1mStep[0m  [36/42], [94mLoss[0m : 2.48451
[1mStep[0m  [40/42], [94mLoss[0m : 2.47637

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37046
[1mStep[0m  [4/42], [94mLoss[0m : 2.28130
[1mStep[0m  [8/42], [94mLoss[0m : 2.33052
[1mStep[0m  [12/42], [94mLoss[0m : 2.44141
[1mStep[0m  [16/42], [94mLoss[0m : 2.69918
[1mStep[0m  [20/42], [94mLoss[0m : 2.34865
[1mStep[0m  [24/42], [94mLoss[0m : 2.64370
[1mStep[0m  [28/42], [94mLoss[0m : 2.27260
[1mStep[0m  [32/42], [94mLoss[0m : 2.61879
[1mStep[0m  [36/42], [94mLoss[0m : 2.30542
[1mStep[0m  [40/42], [94mLoss[0m : 2.24834

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24583
[1mStep[0m  [4/42], [94mLoss[0m : 2.33740
[1mStep[0m  [8/42], [94mLoss[0m : 1.97647
[1mStep[0m  [12/42], [94mLoss[0m : 2.12515
[1mStep[0m  [16/42], [94mLoss[0m : 2.06440
[1mStep[0m  [20/42], [94mLoss[0m : 2.43050
[1mStep[0m  [24/42], [94mLoss[0m : 2.39355
[1mStep[0m  [28/42], [94mLoss[0m : 2.41105
[1mStep[0m  [32/42], [94mLoss[0m : 2.10853
[1mStep[0m  [36/42], [94mLoss[0m : 2.32297
[1mStep[0m  [40/42], [94mLoss[0m : 2.44693

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25574
[1mStep[0m  [4/42], [94mLoss[0m : 2.43048
[1mStep[0m  [8/42], [94mLoss[0m : 2.32784
[1mStep[0m  [12/42], [94mLoss[0m : 2.40631
[1mStep[0m  [16/42], [94mLoss[0m : 2.30679
[1mStep[0m  [20/42], [94mLoss[0m : 2.27796
[1mStep[0m  [24/42], [94mLoss[0m : 2.53532
[1mStep[0m  [28/42], [94mLoss[0m : 2.24156
[1mStep[0m  [32/42], [94mLoss[0m : 2.36931
[1mStep[0m  [36/42], [94mLoss[0m : 2.44759
[1mStep[0m  [40/42], [94mLoss[0m : 2.34060

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.311, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35709
[1mStep[0m  [4/42], [94mLoss[0m : 2.32586
[1mStep[0m  [8/42], [94mLoss[0m : 2.25803
[1mStep[0m  [12/42], [94mLoss[0m : 2.53184
[1mStep[0m  [16/42], [94mLoss[0m : 2.37488
[1mStep[0m  [20/42], [94mLoss[0m : 2.35446
[1mStep[0m  [24/42], [94mLoss[0m : 2.22030
[1mStep[0m  [28/42], [94mLoss[0m : 2.19969
[1mStep[0m  [32/42], [94mLoss[0m : 2.36048
[1mStep[0m  [36/42], [94mLoss[0m : 2.48826
[1mStep[0m  [40/42], [94mLoss[0m : 2.24395

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.282, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42056
[1mStep[0m  [4/42], [94mLoss[0m : 2.45559
[1mStep[0m  [8/42], [94mLoss[0m : 2.29707
[1mStep[0m  [12/42], [94mLoss[0m : 2.29958
[1mStep[0m  [16/42], [94mLoss[0m : 2.27840
[1mStep[0m  [20/42], [94mLoss[0m : 2.32853
[1mStep[0m  [24/42], [94mLoss[0m : 2.28820
[1mStep[0m  [28/42], [94mLoss[0m : 2.34873
[1mStep[0m  [32/42], [94mLoss[0m : 2.34201
[1mStep[0m  [36/42], [94mLoss[0m : 2.34065
[1mStep[0m  [40/42], [94mLoss[0m : 2.22637

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.290, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34010
[1mStep[0m  [4/42], [94mLoss[0m : 2.30451
[1mStep[0m  [8/42], [94mLoss[0m : 2.41398
[1mStep[0m  [12/42], [94mLoss[0m : 2.38896
[1mStep[0m  [16/42], [94mLoss[0m : 2.15321
[1mStep[0m  [20/42], [94mLoss[0m : 2.29756
[1mStep[0m  [24/42], [94mLoss[0m : 2.29727
[1mStep[0m  [28/42], [94mLoss[0m : 2.45691
[1mStep[0m  [32/42], [94mLoss[0m : 2.43032
[1mStep[0m  [36/42], [94mLoss[0m : 2.28574
[1mStep[0m  [40/42], [94mLoss[0m : 2.16939

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23443
[1mStep[0m  [4/42], [94mLoss[0m : 2.24752
[1mStep[0m  [8/42], [94mLoss[0m : 2.37455
[1mStep[0m  [12/42], [94mLoss[0m : 2.25206
[1mStep[0m  [16/42], [94mLoss[0m : 2.32456
[1mStep[0m  [20/42], [94mLoss[0m : 2.21804
[1mStep[0m  [24/42], [94mLoss[0m : 2.46167
[1mStep[0m  [28/42], [94mLoss[0m : 2.43503
[1mStep[0m  [32/42], [94mLoss[0m : 2.42798
[1mStep[0m  [36/42], [94mLoss[0m : 2.35017
[1mStep[0m  [40/42], [94mLoss[0m : 2.52163

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.300, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33362
[1mStep[0m  [4/42], [94mLoss[0m : 2.41942
[1mStep[0m  [8/42], [94mLoss[0m : 2.48199
[1mStep[0m  [12/42], [94mLoss[0m : 2.31072
[1mStep[0m  [16/42], [94mLoss[0m : 2.43724
[1mStep[0m  [20/42], [94mLoss[0m : 2.22104
[1mStep[0m  [24/42], [94mLoss[0m : 2.13088
[1mStep[0m  [28/42], [94mLoss[0m : 2.32280
[1mStep[0m  [32/42], [94mLoss[0m : 2.31230
[1mStep[0m  [36/42], [94mLoss[0m : 2.18477
[1mStep[0m  [40/42], [94mLoss[0m : 2.19444

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.289, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22555
[1mStep[0m  [4/42], [94mLoss[0m : 2.37939
[1mStep[0m  [8/42], [94mLoss[0m : 2.16758
[1mStep[0m  [12/42], [94mLoss[0m : 2.26904
[1mStep[0m  [16/42], [94mLoss[0m : 2.39636
[1mStep[0m  [20/42], [94mLoss[0m : 2.36132
[1mStep[0m  [24/42], [94mLoss[0m : 2.20128
[1mStep[0m  [28/42], [94mLoss[0m : 2.30396
[1mStep[0m  [32/42], [94mLoss[0m : 2.30407
[1mStep[0m  [36/42], [94mLoss[0m : 2.33662
[1mStep[0m  [40/42], [94mLoss[0m : 2.30397

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.281, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52257
[1mStep[0m  [4/42], [94mLoss[0m : 2.25738
[1mStep[0m  [8/42], [94mLoss[0m : 2.49948
[1mStep[0m  [12/42], [94mLoss[0m : 2.47151
[1mStep[0m  [16/42], [94mLoss[0m : 2.32415
[1mStep[0m  [20/42], [94mLoss[0m : 2.28305
[1mStep[0m  [24/42], [94mLoss[0m : 2.39567
[1mStep[0m  [28/42], [94mLoss[0m : 2.20050
[1mStep[0m  [32/42], [94mLoss[0m : 2.15889
[1mStep[0m  [36/42], [94mLoss[0m : 2.25237
[1mStep[0m  [40/42], [94mLoss[0m : 2.60069

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49460
[1mStep[0m  [4/42], [94mLoss[0m : 2.26957
[1mStep[0m  [8/42], [94mLoss[0m : 2.52471
[1mStep[0m  [12/42], [94mLoss[0m : 2.45747
[1mStep[0m  [16/42], [94mLoss[0m : 2.23785
[1mStep[0m  [20/42], [94mLoss[0m : 2.53211
[1mStep[0m  [24/42], [94mLoss[0m : 2.11796
[1mStep[0m  [28/42], [94mLoss[0m : 2.19439
[1mStep[0m  [32/42], [94mLoss[0m : 2.23609
[1mStep[0m  [36/42], [94mLoss[0m : 2.27303
[1mStep[0m  [40/42], [94mLoss[0m : 2.23958

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.278, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11071
[1mStep[0m  [4/42], [94mLoss[0m : 2.25200
[1mStep[0m  [8/42], [94mLoss[0m : 2.28285
[1mStep[0m  [12/42], [94mLoss[0m : 2.38537
[1mStep[0m  [16/42], [94mLoss[0m : 2.50875
[1mStep[0m  [20/42], [94mLoss[0m : 2.40460
[1mStep[0m  [24/42], [94mLoss[0m : 2.50102
[1mStep[0m  [28/42], [94mLoss[0m : 2.34306
[1mStep[0m  [32/42], [94mLoss[0m : 2.23987
[1mStep[0m  [36/42], [94mLoss[0m : 2.21473
[1mStep[0m  [40/42], [94mLoss[0m : 2.27964

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.291, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30903
[1mStep[0m  [4/42], [94mLoss[0m : 2.22220
[1mStep[0m  [8/42], [94mLoss[0m : 2.10131
[1mStep[0m  [12/42], [94mLoss[0m : 2.29939
[1mStep[0m  [16/42], [94mLoss[0m : 2.19771
[1mStep[0m  [20/42], [94mLoss[0m : 2.27954
[1mStep[0m  [24/42], [94mLoss[0m : 2.21896
[1mStep[0m  [28/42], [94mLoss[0m : 2.61550
[1mStep[0m  [32/42], [94mLoss[0m : 2.42331
[1mStep[0m  [36/42], [94mLoss[0m : 2.20355
[1mStep[0m  [40/42], [94mLoss[0m : 2.15182

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.302, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41122
[1mStep[0m  [4/42], [94mLoss[0m : 2.48681
[1mStep[0m  [8/42], [94mLoss[0m : 2.50225
[1mStep[0m  [12/42], [94mLoss[0m : 2.34137
[1mStep[0m  [16/42], [94mLoss[0m : 2.33736
[1mStep[0m  [20/42], [94mLoss[0m : 2.15064
[1mStep[0m  [24/42], [94mLoss[0m : 2.30943
[1mStep[0m  [28/42], [94mLoss[0m : 2.40893
[1mStep[0m  [32/42], [94mLoss[0m : 2.55531
[1mStep[0m  [36/42], [94mLoss[0m : 2.06222
[1mStep[0m  [40/42], [94mLoss[0m : 2.21048

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.297, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26316
[1mStep[0m  [4/42], [94mLoss[0m : 2.38766
[1mStep[0m  [8/42], [94mLoss[0m : 2.14394
[1mStep[0m  [12/42], [94mLoss[0m : 2.29720
[1mStep[0m  [16/42], [94mLoss[0m : 2.20807
[1mStep[0m  [20/42], [94mLoss[0m : 2.45244
[1mStep[0m  [24/42], [94mLoss[0m : 2.25154
[1mStep[0m  [28/42], [94mLoss[0m : 2.33167
[1mStep[0m  [32/42], [94mLoss[0m : 2.28145
[1mStep[0m  [36/42], [94mLoss[0m : 2.28751
[1mStep[0m  [40/42], [94mLoss[0m : 2.41534

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.279, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.286
====================================

Phase 1 - Evaluation MAE:  2.2857485839298795
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.43649
[1mStep[0m  [4/42], [94mLoss[0m : 2.52061
[1mStep[0m  [8/42], [94mLoss[0m : 2.54387
[1mStep[0m  [12/42], [94mLoss[0m : 2.46278
[1mStep[0m  [16/42], [94mLoss[0m : 2.29243
[1mStep[0m  [20/42], [94mLoss[0m : 2.45414
[1mStep[0m  [24/42], [94mLoss[0m : 2.39413
[1mStep[0m  [28/42], [94mLoss[0m : 2.43561
[1mStep[0m  [32/42], [94mLoss[0m : 2.24864
[1mStep[0m  [36/42], [94mLoss[0m : 2.61041
[1mStep[0m  [40/42], [94mLoss[0m : 2.42346

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.280, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25123
[1mStep[0m  [4/42], [94mLoss[0m : 2.37249
[1mStep[0m  [8/42], [94mLoss[0m : 2.11759
[1mStep[0m  [12/42], [94mLoss[0m : 2.25028
[1mStep[0m  [16/42], [94mLoss[0m : 2.36848
[1mStep[0m  [20/42], [94mLoss[0m : 2.30695
[1mStep[0m  [24/42], [94mLoss[0m : 2.29653
[1mStep[0m  [28/42], [94mLoss[0m : 2.18254
[1mStep[0m  [32/42], [94mLoss[0m : 2.28940
[1mStep[0m  [36/42], [94mLoss[0m : 2.29894
[1mStep[0m  [40/42], [94mLoss[0m : 2.55829

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18089
[1mStep[0m  [4/42], [94mLoss[0m : 2.36542
[1mStep[0m  [8/42], [94mLoss[0m : 2.26316
[1mStep[0m  [12/42], [94mLoss[0m : 2.12660
[1mStep[0m  [16/42], [94mLoss[0m : 2.14718
[1mStep[0m  [20/42], [94mLoss[0m : 2.13411
[1mStep[0m  [24/42], [94mLoss[0m : 2.12344
[1mStep[0m  [28/42], [94mLoss[0m : 2.26524
[1mStep[0m  [32/42], [94mLoss[0m : 2.12782
[1mStep[0m  [36/42], [94mLoss[0m : 2.16800
[1mStep[0m  [40/42], [94mLoss[0m : 2.20909

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19636
[1mStep[0m  [4/42], [94mLoss[0m : 2.13895
[1mStep[0m  [8/42], [94mLoss[0m : 1.95916
[1mStep[0m  [12/42], [94mLoss[0m : 1.95814
[1mStep[0m  [16/42], [94mLoss[0m : 2.09264
[1mStep[0m  [20/42], [94mLoss[0m : 2.21117
[1mStep[0m  [24/42], [94mLoss[0m : 2.06440
[1mStep[0m  [28/42], [94mLoss[0m : 2.09540
[1mStep[0m  [32/42], [94mLoss[0m : 1.98303
[1mStep[0m  [36/42], [94mLoss[0m : 1.96739
[1mStep[0m  [40/42], [94mLoss[0m : 2.06221

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95760
[1mStep[0m  [4/42], [94mLoss[0m : 2.07346
[1mStep[0m  [8/42], [94mLoss[0m : 1.94073
[1mStep[0m  [12/42], [94mLoss[0m : 2.16462
[1mStep[0m  [16/42], [94mLoss[0m : 1.94771
[1mStep[0m  [20/42], [94mLoss[0m : 1.97679
[1mStep[0m  [24/42], [94mLoss[0m : 2.10626
[1mStep[0m  [28/42], [94mLoss[0m : 2.07394
[1mStep[0m  [32/42], [94mLoss[0m : 2.07375
[1mStep[0m  [36/42], [94mLoss[0m : 2.01261
[1mStep[0m  [40/42], [94mLoss[0m : 1.85777

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76965
[1mStep[0m  [4/42], [94mLoss[0m : 1.96800
[1mStep[0m  [8/42], [94mLoss[0m : 1.92496
[1mStep[0m  [12/42], [94mLoss[0m : 2.14773
[1mStep[0m  [16/42], [94mLoss[0m : 2.14292
[1mStep[0m  [20/42], [94mLoss[0m : 2.08163
[1mStep[0m  [24/42], [94mLoss[0m : 1.90703
[1mStep[0m  [28/42], [94mLoss[0m : 2.22312
[1mStep[0m  [32/42], [94mLoss[0m : 1.94743
[1mStep[0m  [36/42], [94mLoss[0m : 1.85309
[1mStep[0m  [40/42], [94mLoss[0m : 1.90215

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.972, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95789
[1mStep[0m  [4/42], [94mLoss[0m : 1.87612
[1mStep[0m  [8/42], [94mLoss[0m : 1.86396
[1mStep[0m  [12/42], [94mLoss[0m : 1.89605
[1mStep[0m  [16/42], [94mLoss[0m : 1.96828
[1mStep[0m  [20/42], [94mLoss[0m : 1.79254
[1mStep[0m  [24/42], [94mLoss[0m : 1.88520
[1mStep[0m  [28/42], [94mLoss[0m : 2.01491
[1mStep[0m  [32/42], [94mLoss[0m : 1.95989
[1mStep[0m  [36/42], [94mLoss[0m : 1.88634
[1mStep[0m  [40/42], [94mLoss[0m : 1.97610

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.514, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81851
[1mStep[0m  [4/42], [94mLoss[0m : 1.85845
[1mStep[0m  [8/42], [94mLoss[0m : 1.88324
[1mStep[0m  [12/42], [94mLoss[0m : 1.86779
[1mStep[0m  [16/42], [94mLoss[0m : 1.73392
[1mStep[0m  [20/42], [94mLoss[0m : 1.70785
[1mStep[0m  [24/42], [94mLoss[0m : 1.78866
[1mStep[0m  [28/42], [94mLoss[0m : 1.72926
[1mStep[0m  [32/42], [94mLoss[0m : 1.91540
[1mStep[0m  [36/42], [94mLoss[0m : 1.80940
[1mStep[0m  [40/42], [94mLoss[0m : 1.95021

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69107
[1mStep[0m  [4/42], [94mLoss[0m : 1.65812
[1mStep[0m  [8/42], [94mLoss[0m : 1.79161
[1mStep[0m  [12/42], [94mLoss[0m : 1.76894
[1mStep[0m  [16/42], [94mLoss[0m : 1.72923
[1mStep[0m  [20/42], [94mLoss[0m : 1.73830
[1mStep[0m  [24/42], [94mLoss[0m : 1.73724
[1mStep[0m  [28/42], [94mLoss[0m : 1.82134
[1mStep[0m  [32/42], [94mLoss[0m : 1.83930
[1mStep[0m  [36/42], [94mLoss[0m : 1.63640
[1mStep[0m  [40/42], [94mLoss[0m : 1.85084

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51822
[1mStep[0m  [4/42], [94mLoss[0m : 1.56015
[1mStep[0m  [8/42], [94mLoss[0m : 1.78942
[1mStep[0m  [12/42], [94mLoss[0m : 1.86265
[1mStep[0m  [16/42], [94mLoss[0m : 1.66232
[1mStep[0m  [20/42], [94mLoss[0m : 1.57792
[1mStep[0m  [24/42], [94mLoss[0m : 1.68579
[1mStep[0m  [28/42], [94mLoss[0m : 1.69133
[1mStep[0m  [32/42], [94mLoss[0m : 1.77707
[1mStep[0m  [36/42], [94mLoss[0m : 1.59027
[1mStep[0m  [40/42], [94mLoss[0m : 1.95783

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.721, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78257
[1mStep[0m  [4/42], [94mLoss[0m : 1.72783
[1mStep[0m  [8/42], [94mLoss[0m : 1.60045
[1mStep[0m  [12/42], [94mLoss[0m : 1.67004
[1mStep[0m  [16/42], [94mLoss[0m : 1.48903
[1mStep[0m  [20/42], [94mLoss[0m : 1.66841
[1mStep[0m  [24/42], [94mLoss[0m : 1.60117
[1mStep[0m  [28/42], [94mLoss[0m : 1.53565
[1mStep[0m  [32/42], [94mLoss[0m : 1.77132
[1mStep[0m  [36/42], [94mLoss[0m : 1.73634
[1mStep[0m  [40/42], [94mLoss[0m : 1.64866

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70084
[1mStep[0m  [4/42], [94mLoss[0m : 1.69157
[1mStep[0m  [8/42], [94mLoss[0m : 1.69730
[1mStep[0m  [12/42], [94mLoss[0m : 1.66746
[1mStep[0m  [16/42], [94mLoss[0m : 1.60118
[1mStep[0m  [20/42], [94mLoss[0m : 1.60274
[1mStep[0m  [24/42], [94mLoss[0m : 1.67044
[1mStep[0m  [28/42], [94mLoss[0m : 1.75269
[1mStep[0m  [32/42], [94mLoss[0m : 1.68903
[1mStep[0m  [36/42], [94mLoss[0m : 1.70657
[1mStep[0m  [40/42], [94mLoss[0m : 1.72743

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60410
[1mStep[0m  [4/42], [94mLoss[0m : 1.54391
[1mStep[0m  [8/42], [94mLoss[0m : 1.50048
[1mStep[0m  [12/42], [94mLoss[0m : 1.36527
[1mStep[0m  [16/42], [94mLoss[0m : 1.58138
[1mStep[0m  [20/42], [94mLoss[0m : 1.44480
[1mStep[0m  [24/42], [94mLoss[0m : 1.61532
[1mStep[0m  [28/42], [94mLoss[0m : 1.59607
[1mStep[0m  [32/42], [94mLoss[0m : 1.50960
[1mStep[0m  [36/42], [94mLoss[0m : 1.60383
[1mStep[0m  [40/42], [94mLoss[0m : 1.63688

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55795
[1mStep[0m  [4/42], [94mLoss[0m : 1.63663
[1mStep[0m  [8/42], [94mLoss[0m : 1.33592
[1mStep[0m  [12/42], [94mLoss[0m : 1.55149
[1mStep[0m  [16/42], [94mLoss[0m : 1.63227
[1mStep[0m  [20/42], [94mLoss[0m : 1.49986
[1mStep[0m  [24/42], [94mLoss[0m : 1.67506
[1mStep[0m  [28/42], [94mLoss[0m : 1.53862
[1mStep[0m  [32/42], [94mLoss[0m : 1.57105
[1mStep[0m  [36/42], [94mLoss[0m : 1.64801
[1mStep[0m  [40/42], [94mLoss[0m : 1.72331

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59321
[1mStep[0m  [4/42], [94mLoss[0m : 1.56255
[1mStep[0m  [8/42], [94mLoss[0m : 1.43936
[1mStep[0m  [12/42], [94mLoss[0m : 1.50606
[1mStep[0m  [16/42], [94mLoss[0m : 1.51347
[1mStep[0m  [20/42], [94mLoss[0m : 1.69778
[1mStep[0m  [24/42], [94mLoss[0m : 1.58725
[1mStep[0m  [28/42], [94mLoss[0m : 1.57139
[1mStep[0m  [32/42], [94mLoss[0m : 1.61949
[1mStep[0m  [36/42], [94mLoss[0m : 1.59085
[1mStep[0m  [40/42], [94mLoss[0m : 1.74584

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49368
[1mStep[0m  [4/42], [94mLoss[0m : 1.37460
[1mStep[0m  [8/42], [94mLoss[0m : 1.26739
[1mStep[0m  [12/42], [94mLoss[0m : 1.40461
[1mStep[0m  [16/42], [94mLoss[0m : 1.49405
[1mStep[0m  [20/42], [94mLoss[0m : 1.34934
[1mStep[0m  [24/42], [94mLoss[0m : 1.42005
[1mStep[0m  [28/42], [94mLoss[0m : 1.49368
[1mStep[0m  [32/42], [94mLoss[0m : 1.70986
[1mStep[0m  [36/42], [94mLoss[0m : 1.54912
[1mStep[0m  [40/42], [94mLoss[0m : 1.63861

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.468, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41797
[1mStep[0m  [4/42], [94mLoss[0m : 1.37616
[1mStep[0m  [8/42], [94mLoss[0m : 1.41072
[1mStep[0m  [12/42], [94mLoss[0m : 1.41941
[1mStep[0m  [16/42], [94mLoss[0m : 1.44994
[1mStep[0m  [20/42], [94mLoss[0m : 1.52672
[1mStep[0m  [24/42], [94mLoss[0m : 1.46009
[1mStep[0m  [28/42], [94mLoss[0m : 1.51056
[1mStep[0m  [32/42], [94mLoss[0m : 1.43861
[1mStep[0m  [36/42], [94mLoss[0m : 1.36611
[1mStep[0m  [40/42], [94mLoss[0m : 1.53217

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50177
[1mStep[0m  [4/42], [94mLoss[0m : 1.38864
[1mStep[0m  [8/42], [94mLoss[0m : 1.57793
[1mStep[0m  [12/42], [94mLoss[0m : 1.41219
[1mStep[0m  [16/42], [94mLoss[0m : 1.39311
[1mStep[0m  [20/42], [94mLoss[0m : 1.30902
[1mStep[0m  [24/42], [94mLoss[0m : 1.35802
[1mStep[0m  [28/42], [94mLoss[0m : 1.46884
[1mStep[0m  [32/42], [94mLoss[0m : 1.37065
[1mStep[0m  [36/42], [94mLoss[0m : 1.46730
[1mStep[0m  [40/42], [94mLoss[0m : 1.62607

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36844
[1mStep[0m  [4/42], [94mLoss[0m : 1.35288
[1mStep[0m  [8/42], [94mLoss[0m : 1.40263
[1mStep[0m  [12/42], [94mLoss[0m : 1.44370
[1mStep[0m  [16/42], [94mLoss[0m : 1.27339
[1mStep[0m  [20/42], [94mLoss[0m : 1.28033
[1mStep[0m  [24/42], [94mLoss[0m : 1.37443
[1mStep[0m  [28/42], [94mLoss[0m : 1.44440
[1mStep[0m  [32/42], [94mLoss[0m : 1.48644
[1mStep[0m  [36/42], [94mLoss[0m : 1.35000
[1mStep[0m  [40/42], [94mLoss[0m : 1.38380

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.386, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.16350
[1mStep[0m  [4/42], [94mLoss[0m : 1.37392
[1mStep[0m  [8/42], [94mLoss[0m : 1.34762
[1mStep[0m  [12/42], [94mLoss[0m : 1.40773
[1mStep[0m  [16/42], [94mLoss[0m : 1.40511
[1mStep[0m  [20/42], [94mLoss[0m : 1.34184
[1mStep[0m  [24/42], [94mLoss[0m : 1.44190
[1mStep[0m  [28/42], [94mLoss[0m : 1.34238
[1mStep[0m  [32/42], [94mLoss[0m : 1.26274
[1mStep[0m  [36/42], [94mLoss[0m : 1.40835
[1mStep[0m  [40/42], [94mLoss[0m : 1.39496

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.358, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 19 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.592
====================================

Phase 2 - Evaluation MAE:  2.5921722991125926
MAE score P1      2.285749
MAE score P2      2.592172
loss               1.35812
learning_rate         0.01
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.83479
[1mStep[0m  [4/42], [94mLoss[0m : 6.33385
[1mStep[0m  [8/42], [94mLoss[0m : 2.95191
[1mStep[0m  [12/42], [94mLoss[0m : 2.92361
[1mStep[0m  [16/42], [94mLoss[0m : 2.78419
[1mStep[0m  [20/42], [94mLoss[0m : 2.54418
[1mStep[0m  [24/42], [94mLoss[0m : 2.69783
[1mStep[0m  [28/42], [94mLoss[0m : 2.59376
[1mStep[0m  [32/42], [94mLoss[0m : 2.49221
[1mStep[0m  [36/42], [94mLoss[0m : 2.60498
[1mStep[0m  [40/42], [94mLoss[0m : 2.58249

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.477, [92mTest[0m: 10.942, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61691
[1mStep[0m  [4/42], [94mLoss[0m : 2.71768
[1mStep[0m  [8/42], [94mLoss[0m : 2.60412
[1mStep[0m  [12/42], [94mLoss[0m : 2.52361
[1mStep[0m  [16/42], [94mLoss[0m : 2.45275
[1mStep[0m  [20/42], [94mLoss[0m : 2.49537
[1mStep[0m  [24/42], [94mLoss[0m : 2.38404
[1mStep[0m  [28/42], [94mLoss[0m : 2.55453
[1mStep[0m  [32/42], [94mLoss[0m : 2.44342
[1mStep[0m  [36/42], [94mLoss[0m : 2.51460
[1mStep[0m  [40/42], [94mLoss[0m : 2.28797

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31738
[1mStep[0m  [4/42], [94mLoss[0m : 2.48477
[1mStep[0m  [8/42], [94mLoss[0m : 2.37431
[1mStep[0m  [12/42], [94mLoss[0m : 2.63871
[1mStep[0m  [16/42], [94mLoss[0m : 2.56651
[1mStep[0m  [20/42], [94mLoss[0m : 2.46279
[1mStep[0m  [24/42], [94mLoss[0m : 2.45874
[1mStep[0m  [28/42], [94mLoss[0m : 2.60567
[1mStep[0m  [32/42], [94mLoss[0m : 2.43285
[1mStep[0m  [36/42], [94mLoss[0m : 2.50626
[1mStep[0m  [40/42], [94mLoss[0m : 2.54246

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39556
[1mStep[0m  [4/42], [94mLoss[0m : 2.75628
[1mStep[0m  [8/42], [94mLoss[0m : 2.37130
[1mStep[0m  [12/42], [94mLoss[0m : 2.44992
[1mStep[0m  [16/42], [94mLoss[0m : 2.55833
[1mStep[0m  [20/42], [94mLoss[0m : 2.58199
[1mStep[0m  [24/42], [94mLoss[0m : 2.34058
[1mStep[0m  [28/42], [94mLoss[0m : 2.46264
[1mStep[0m  [32/42], [94mLoss[0m : 2.71568
[1mStep[0m  [36/42], [94mLoss[0m : 2.64178
[1mStep[0m  [40/42], [94mLoss[0m : 2.37003

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52400
[1mStep[0m  [4/42], [94mLoss[0m : 2.28119
[1mStep[0m  [8/42], [94mLoss[0m : 2.55919
[1mStep[0m  [12/42], [94mLoss[0m : 2.33886
[1mStep[0m  [16/42], [94mLoss[0m : 2.45451
[1mStep[0m  [20/42], [94mLoss[0m : 2.37156
[1mStep[0m  [24/42], [94mLoss[0m : 2.38356
[1mStep[0m  [28/42], [94mLoss[0m : 2.41728
[1mStep[0m  [32/42], [94mLoss[0m : 2.27505
[1mStep[0m  [36/42], [94mLoss[0m : 2.43062
[1mStep[0m  [40/42], [94mLoss[0m : 2.32877

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49045
[1mStep[0m  [4/42], [94mLoss[0m : 2.43690
[1mStep[0m  [8/42], [94mLoss[0m : 2.26775
[1mStep[0m  [12/42], [94mLoss[0m : 2.45571
[1mStep[0m  [16/42], [94mLoss[0m : 2.61933
[1mStep[0m  [20/42], [94mLoss[0m : 2.39536
[1mStep[0m  [24/42], [94mLoss[0m : 2.57840
[1mStep[0m  [28/42], [94mLoss[0m : 2.43407
[1mStep[0m  [32/42], [94mLoss[0m : 2.56688
[1mStep[0m  [36/42], [94mLoss[0m : 2.68477
[1mStep[0m  [40/42], [94mLoss[0m : 2.67410

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41706
[1mStep[0m  [4/42], [94mLoss[0m : 2.29483
[1mStep[0m  [8/42], [94mLoss[0m : 2.32985
[1mStep[0m  [12/42], [94mLoss[0m : 2.54728
[1mStep[0m  [16/42], [94mLoss[0m : 2.65237
[1mStep[0m  [20/42], [94mLoss[0m : 2.58141
[1mStep[0m  [24/42], [94mLoss[0m : 2.65098
[1mStep[0m  [28/42], [94mLoss[0m : 2.51916
[1mStep[0m  [32/42], [94mLoss[0m : 2.35708
[1mStep[0m  [36/42], [94mLoss[0m : 2.41613
[1mStep[0m  [40/42], [94mLoss[0m : 2.41185

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52595
[1mStep[0m  [4/42], [94mLoss[0m : 2.51718
[1mStep[0m  [8/42], [94mLoss[0m : 2.41957
[1mStep[0m  [12/42], [94mLoss[0m : 2.32024
[1mStep[0m  [16/42], [94mLoss[0m : 2.36110
[1mStep[0m  [20/42], [94mLoss[0m : 2.56356
[1mStep[0m  [24/42], [94mLoss[0m : 2.46515
[1mStep[0m  [28/42], [94mLoss[0m : 2.22600
[1mStep[0m  [32/42], [94mLoss[0m : 2.45110
[1mStep[0m  [36/42], [94mLoss[0m : 2.40230
[1mStep[0m  [40/42], [94mLoss[0m : 2.43966

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42461
[1mStep[0m  [4/42], [94mLoss[0m : 2.38812
[1mStep[0m  [8/42], [94mLoss[0m : 2.30345
[1mStep[0m  [12/42], [94mLoss[0m : 2.48974
[1mStep[0m  [16/42], [94mLoss[0m : 2.22079
[1mStep[0m  [20/42], [94mLoss[0m : 2.69861
[1mStep[0m  [24/42], [94mLoss[0m : 2.37459
[1mStep[0m  [28/42], [94mLoss[0m : 2.34406
[1mStep[0m  [32/42], [94mLoss[0m : 2.38034
[1mStep[0m  [36/42], [94mLoss[0m : 2.51794
[1mStep[0m  [40/42], [94mLoss[0m : 2.42686

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46236
[1mStep[0m  [4/42], [94mLoss[0m : 2.47051
[1mStep[0m  [8/42], [94mLoss[0m : 2.46114
[1mStep[0m  [12/42], [94mLoss[0m : 2.41337
[1mStep[0m  [16/42], [94mLoss[0m : 2.50125
[1mStep[0m  [20/42], [94mLoss[0m : 2.35432
[1mStep[0m  [24/42], [94mLoss[0m : 2.30355
[1mStep[0m  [28/42], [94mLoss[0m : 2.43647
[1mStep[0m  [32/42], [94mLoss[0m : 2.55313
[1mStep[0m  [36/42], [94mLoss[0m : 2.56628
[1mStep[0m  [40/42], [94mLoss[0m : 2.41090

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33082
[1mStep[0m  [4/42], [94mLoss[0m : 2.45004
[1mStep[0m  [8/42], [94mLoss[0m : 2.21254
[1mStep[0m  [12/42], [94mLoss[0m : 2.32400
[1mStep[0m  [16/42], [94mLoss[0m : 2.32660
[1mStep[0m  [20/42], [94mLoss[0m : 2.24951
[1mStep[0m  [24/42], [94mLoss[0m : 2.49681
[1mStep[0m  [28/42], [94mLoss[0m : 2.41805
[1mStep[0m  [32/42], [94mLoss[0m : 2.46877
[1mStep[0m  [36/42], [94mLoss[0m : 2.36684
[1mStep[0m  [40/42], [94mLoss[0m : 2.31925

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39262
[1mStep[0m  [4/42], [94mLoss[0m : 2.54107
[1mStep[0m  [8/42], [94mLoss[0m : 2.55703
[1mStep[0m  [12/42], [94mLoss[0m : 2.39158
[1mStep[0m  [16/42], [94mLoss[0m : 2.34032
[1mStep[0m  [20/42], [94mLoss[0m : 2.51925
[1mStep[0m  [24/42], [94mLoss[0m : 2.50479
[1mStep[0m  [28/42], [94mLoss[0m : 2.33650
[1mStep[0m  [32/42], [94mLoss[0m : 2.42012
[1mStep[0m  [36/42], [94mLoss[0m : 2.37151
[1mStep[0m  [40/42], [94mLoss[0m : 2.42861

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45316
[1mStep[0m  [4/42], [94mLoss[0m : 2.34824
[1mStep[0m  [8/42], [94mLoss[0m : 2.56031
[1mStep[0m  [12/42], [94mLoss[0m : 2.39438
[1mStep[0m  [16/42], [94mLoss[0m : 2.55388
[1mStep[0m  [20/42], [94mLoss[0m : 2.47197
[1mStep[0m  [24/42], [94mLoss[0m : 2.40860
[1mStep[0m  [28/42], [94mLoss[0m : 2.30707
[1mStep[0m  [32/42], [94mLoss[0m : 2.46132
[1mStep[0m  [36/42], [94mLoss[0m : 2.33782
[1mStep[0m  [40/42], [94mLoss[0m : 2.38287

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35443
[1mStep[0m  [4/42], [94mLoss[0m : 2.57640
[1mStep[0m  [8/42], [94mLoss[0m : 2.53593
[1mStep[0m  [12/42], [94mLoss[0m : 2.27561
[1mStep[0m  [16/42], [94mLoss[0m : 2.30203
[1mStep[0m  [20/42], [94mLoss[0m : 2.47758
[1mStep[0m  [24/42], [94mLoss[0m : 2.36527
[1mStep[0m  [28/42], [94mLoss[0m : 2.39922
[1mStep[0m  [32/42], [94mLoss[0m : 2.42939
[1mStep[0m  [36/42], [94mLoss[0m : 2.39086
[1mStep[0m  [40/42], [94mLoss[0m : 2.45831

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34121
[1mStep[0m  [4/42], [94mLoss[0m : 2.47704
[1mStep[0m  [8/42], [94mLoss[0m : 2.36049
[1mStep[0m  [12/42], [94mLoss[0m : 2.68425
[1mStep[0m  [16/42], [94mLoss[0m : 2.44798
[1mStep[0m  [20/42], [94mLoss[0m : 2.40321
[1mStep[0m  [24/42], [94mLoss[0m : 2.41647
[1mStep[0m  [28/42], [94mLoss[0m : 2.47800
[1mStep[0m  [32/42], [94mLoss[0m : 2.42192
[1mStep[0m  [36/42], [94mLoss[0m : 2.63333
[1mStep[0m  [40/42], [94mLoss[0m : 2.46704

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39002
[1mStep[0m  [4/42], [94mLoss[0m : 2.53297
[1mStep[0m  [8/42], [94mLoss[0m : 2.49870
[1mStep[0m  [12/42], [94mLoss[0m : 2.47874
[1mStep[0m  [16/42], [94mLoss[0m : 2.48397
[1mStep[0m  [20/42], [94mLoss[0m : 2.29287
[1mStep[0m  [24/42], [94mLoss[0m : 2.54618
[1mStep[0m  [28/42], [94mLoss[0m : 2.44959
[1mStep[0m  [32/42], [94mLoss[0m : 2.22756
[1mStep[0m  [36/42], [94mLoss[0m : 2.44815
[1mStep[0m  [40/42], [94mLoss[0m : 2.36818

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38599
[1mStep[0m  [4/42], [94mLoss[0m : 2.27947
[1mStep[0m  [8/42], [94mLoss[0m : 2.43327
[1mStep[0m  [12/42], [94mLoss[0m : 2.51356
[1mStep[0m  [16/42], [94mLoss[0m : 2.51085
[1mStep[0m  [20/42], [94mLoss[0m : 2.32664
[1mStep[0m  [24/42], [94mLoss[0m : 2.28888
[1mStep[0m  [28/42], [94mLoss[0m : 2.41283
[1mStep[0m  [32/42], [94mLoss[0m : 2.33165
[1mStep[0m  [36/42], [94mLoss[0m : 2.37809
[1mStep[0m  [40/42], [94mLoss[0m : 2.46676

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29965
[1mStep[0m  [4/42], [94mLoss[0m : 2.38488
[1mStep[0m  [8/42], [94mLoss[0m : 2.36205
[1mStep[0m  [12/42], [94mLoss[0m : 2.45433
[1mStep[0m  [16/42], [94mLoss[0m : 2.20676
[1mStep[0m  [20/42], [94mLoss[0m : 2.45147
[1mStep[0m  [24/42], [94mLoss[0m : 2.37886
[1mStep[0m  [28/42], [94mLoss[0m : 2.58361
[1mStep[0m  [32/42], [94mLoss[0m : 2.50066
[1mStep[0m  [36/42], [94mLoss[0m : 2.53976
[1mStep[0m  [40/42], [94mLoss[0m : 2.43407

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42701
[1mStep[0m  [4/42], [94mLoss[0m : 2.44786
[1mStep[0m  [8/42], [94mLoss[0m : 2.50992
[1mStep[0m  [12/42], [94mLoss[0m : 2.51967
[1mStep[0m  [16/42], [94mLoss[0m : 2.22593
[1mStep[0m  [20/42], [94mLoss[0m : 2.60203
[1mStep[0m  [24/42], [94mLoss[0m : 2.49226
[1mStep[0m  [28/42], [94mLoss[0m : 2.53024
[1mStep[0m  [32/42], [94mLoss[0m : 2.53840
[1mStep[0m  [36/42], [94mLoss[0m : 2.42451
[1mStep[0m  [40/42], [94mLoss[0m : 2.46153

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47711
[1mStep[0m  [4/42], [94mLoss[0m : 2.25554
[1mStep[0m  [8/42], [94mLoss[0m : 2.56342
[1mStep[0m  [12/42], [94mLoss[0m : 2.46433
[1mStep[0m  [16/42], [94mLoss[0m : 2.44636
[1mStep[0m  [20/42], [94mLoss[0m : 2.60834
[1mStep[0m  [24/42], [94mLoss[0m : 2.46273
[1mStep[0m  [28/42], [94mLoss[0m : 2.49930
[1mStep[0m  [32/42], [94mLoss[0m : 2.45811
[1mStep[0m  [36/42], [94mLoss[0m : 2.46801
[1mStep[0m  [40/42], [94mLoss[0m : 2.41191

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36181
[1mStep[0m  [4/42], [94mLoss[0m : 2.42407
[1mStep[0m  [8/42], [94mLoss[0m : 2.41855
[1mStep[0m  [12/42], [94mLoss[0m : 2.48274
[1mStep[0m  [16/42], [94mLoss[0m : 2.45400
[1mStep[0m  [20/42], [94mLoss[0m : 2.34826
[1mStep[0m  [24/42], [94mLoss[0m : 2.44476
[1mStep[0m  [28/42], [94mLoss[0m : 2.53379
[1mStep[0m  [32/42], [94mLoss[0m : 2.23835
[1mStep[0m  [36/42], [94mLoss[0m : 2.48237
[1mStep[0m  [40/42], [94mLoss[0m : 2.49505

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58279
[1mStep[0m  [4/42], [94mLoss[0m : 2.55524
[1mStep[0m  [8/42], [94mLoss[0m : 2.47411
[1mStep[0m  [12/42], [94mLoss[0m : 2.35533
[1mStep[0m  [16/42], [94mLoss[0m : 2.41347
[1mStep[0m  [20/42], [94mLoss[0m : 2.43334
[1mStep[0m  [24/42], [94mLoss[0m : 2.58502
[1mStep[0m  [28/42], [94mLoss[0m : 2.39504
[1mStep[0m  [32/42], [94mLoss[0m : 2.63022
[1mStep[0m  [36/42], [94mLoss[0m : 2.46397
[1mStep[0m  [40/42], [94mLoss[0m : 2.53232

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55633
[1mStep[0m  [4/42], [94mLoss[0m : 2.49724
[1mStep[0m  [8/42], [94mLoss[0m : 2.44996
[1mStep[0m  [12/42], [94mLoss[0m : 2.25531
[1mStep[0m  [16/42], [94mLoss[0m : 2.40543
[1mStep[0m  [20/42], [94mLoss[0m : 2.42965
[1mStep[0m  [24/42], [94mLoss[0m : 2.30071
[1mStep[0m  [28/42], [94mLoss[0m : 2.38325
[1mStep[0m  [32/42], [94mLoss[0m : 2.11804
[1mStep[0m  [36/42], [94mLoss[0m : 2.18168
[1mStep[0m  [40/42], [94mLoss[0m : 2.31636

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35300
[1mStep[0m  [4/42], [94mLoss[0m : 2.29200
[1mStep[0m  [8/42], [94mLoss[0m : 2.61312
[1mStep[0m  [12/42], [94mLoss[0m : 2.25122
[1mStep[0m  [16/42], [94mLoss[0m : 2.37734
[1mStep[0m  [20/42], [94mLoss[0m : 2.57217
[1mStep[0m  [24/42], [94mLoss[0m : 2.30853
[1mStep[0m  [28/42], [94mLoss[0m : 2.29123
[1mStep[0m  [32/42], [94mLoss[0m : 2.43817
[1mStep[0m  [36/42], [94mLoss[0m : 2.52448
[1mStep[0m  [40/42], [94mLoss[0m : 2.30957

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52345
[1mStep[0m  [4/42], [94mLoss[0m : 2.48202
[1mStep[0m  [8/42], [94mLoss[0m : 2.55618
[1mStep[0m  [12/42], [94mLoss[0m : 2.27098
[1mStep[0m  [16/42], [94mLoss[0m : 2.36548
[1mStep[0m  [20/42], [94mLoss[0m : 2.26826
[1mStep[0m  [24/42], [94mLoss[0m : 2.52213
[1mStep[0m  [28/42], [94mLoss[0m : 2.45689
[1mStep[0m  [32/42], [94mLoss[0m : 2.35940
[1mStep[0m  [36/42], [94mLoss[0m : 2.39551
[1mStep[0m  [40/42], [94mLoss[0m : 2.28183

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56608
[1mStep[0m  [4/42], [94mLoss[0m : 2.38061
[1mStep[0m  [8/42], [94mLoss[0m : 2.26281
[1mStep[0m  [12/42], [94mLoss[0m : 2.22904
[1mStep[0m  [16/42], [94mLoss[0m : 2.44435
[1mStep[0m  [20/42], [94mLoss[0m : 2.34431
[1mStep[0m  [24/42], [94mLoss[0m : 2.37971
[1mStep[0m  [28/42], [94mLoss[0m : 2.74861
[1mStep[0m  [32/42], [94mLoss[0m : 2.52355
[1mStep[0m  [36/42], [94mLoss[0m : 2.49996
[1mStep[0m  [40/42], [94mLoss[0m : 2.55755

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41321
[1mStep[0m  [4/42], [94mLoss[0m : 2.34909
[1mStep[0m  [8/42], [94mLoss[0m : 2.35329
[1mStep[0m  [12/42], [94mLoss[0m : 2.47251
[1mStep[0m  [16/42], [94mLoss[0m : 2.52281
[1mStep[0m  [20/42], [94mLoss[0m : 2.38293
[1mStep[0m  [24/42], [94mLoss[0m : 2.33226
[1mStep[0m  [28/42], [94mLoss[0m : 2.34134
[1mStep[0m  [32/42], [94mLoss[0m : 2.42262
[1mStep[0m  [36/42], [94mLoss[0m : 2.68294
[1mStep[0m  [40/42], [94mLoss[0m : 2.51365

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49082
[1mStep[0m  [4/42], [94mLoss[0m : 2.44200
[1mStep[0m  [8/42], [94mLoss[0m : 2.41217
[1mStep[0m  [12/42], [94mLoss[0m : 2.47059
[1mStep[0m  [16/42], [94mLoss[0m : 2.58246
[1mStep[0m  [20/42], [94mLoss[0m : 2.41208
[1mStep[0m  [24/42], [94mLoss[0m : 2.48436
[1mStep[0m  [28/42], [94mLoss[0m : 2.51843
[1mStep[0m  [32/42], [94mLoss[0m : 2.25926
[1mStep[0m  [36/42], [94mLoss[0m : 2.53886
[1mStep[0m  [40/42], [94mLoss[0m : 2.39044

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22010
[1mStep[0m  [4/42], [94mLoss[0m : 2.41915
[1mStep[0m  [8/42], [94mLoss[0m : 2.45971
[1mStep[0m  [12/42], [94mLoss[0m : 2.43871
[1mStep[0m  [16/42], [94mLoss[0m : 2.45354
[1mStep[0m  [20/42], [94mLoss[0m : 2.43616
[1mStep[0m  [24/42], [94mLoss[0m : 2.25792
[1mStep[0m  [28/42], [94mLoss[0m : 2.52463
[1mStep[0m  [32/42], [94mLoss[0m : 2.38851
[1mStep[0m  [36/42], [94mLoss[0m : 2.64784
[1mStep[0m  [40/42], [94mLoss[0m : 2.47032

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41125
[1mStep[0m  [4/42], [94mLoss[0m : 2.48179
[1mStep[0m  [8/42], [94mLoss[0m : 2.09870
[1mStep[0m  [12/42], [94mLoss[0m : 2.40156
[1mStep[0m  [16/42], [94mLoss[0m : 2.46306
[1mStep[0m  [20/42], [94mLoss[0m : 2.28049
[1mStep[0m  [24/42], [94mLoss[0m : 2.34531
[1mStep[0m  [28/42], [94mLoss[0m : 2.64735
[1mStep[0m  [32/42], [94mLoss[0m : 2.37710
[1mStep[0m  [36/42], [94mLoss[0m : 2.18356
[1mStep[0m  [40/42], [94mLoss[0m : 2.63754

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.3296949011938914
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.27182
[1mStep[0m  [4/42], [94mLoss[0m : 2.27949
[1mStep[0m  [8/42], [94mLoss[0m : 2.33820
[1mStep[0m  [12/42], [94mLoss[0m : 2.46850
[1mStep[0m  [16/42], [94mLoss[0m : 2.53322
[1mStep[0m  [20/42], [94mLoss[0m : 2.46666
[1mStep[0m  [24/42], [94mLoss[0m : 2.38944
[1mStep[0m  [28/42], [94mLoss[0m : 2.39558
[1mStep[0m  [32/42], [94mLoss[0m : 2.33143
[1mStep[0m  [36/42], [94mLoss[0m : 2.51754
[1mStep[0m  [40/42], [94mLoss[0m : 2.27308

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45850
[1mStep[0m  [4/42], [94mLoss[0m : 2.49472
[1mStep[0m  [8/42], [94mLoss[0m : 2.23540
[1mStep[0m  [12/42], [94mLoss[0m : 2.51059
[1mStep[0m  [16/42], [94mLoss[0m : 2.19076
[1mStep[0m  [20/42], [94mLoss[0m : 2.40484
[1mStep[0m  [24/42], [94mLoss[0m : 2.39839
[1mStep[0m  [28/42], [94mLoss[0m : 2.46197
[1mStep[0m  [32/42], [94mLoss[0m : 2.37968
[1mStep[0m  [36/42], [94mLoss[0m : 2.27453
[1mStep[0m  [40/42], [94mLoss[0m : 2.28299

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19507
[1mStep[0m  [4/42], [94mLoss[0m : 2.35525
[1mStep[0m  [8/42], [94mLoss[0m : 2.43400
[1mStep[0m  [12/42], [94mLoss[0m : 2.33702
[1mStep[0m  [16/42], [94mLoss[0m : 2.22815
[1mStep[0m  [20/42], [94mLoss[0m : 2.36166
[1mStep[0m  [24/42], [94mLoss[0m : 2.42251
[1mStep[0m  [28/42], [94mLoss[0m : 2.39286
[1mStep[0m  [32/42], [94mLoss[0m : 2.29652
[1mStep[0m  [36/42], [94mLoss[0m : 2.15856
[1mStep[0m  [40/42], [94mLoss[0m : 2.35412

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12328
[1mStep[0m  [4/42], [94mLoss[0m : 2.02993
[1mStep[0m  [8/42], [94mLoss[0m : 2.26729
[1mStep[0m  [12/42], [94mLoss[0m : 2.31975
[1mStep[0m  [16/42], [94mLoss[0m : 2.45038
[1mStep[0m  [20/42], [94mLoss[0m : 2.41011
[1mStep[0m  [24/42], [94mLoss[0m : 2.50125
[1mStep[0m  [28/42], [94mLoss[0m : 2.20551
[1mStep[0m  [32/42], [94mLoss[0m : 2.45671
[1mStep[0m  [36/42], [94mLoss[0m : 2.37774
[1mStep[0m  [40/42], [94mLoss[0m : 2.32471

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35066
[1mStep[0m  [4/42], [94mLoss[0m : 2.30269
[1mStep[0m  [8/42], [94mLoss[0m : 2.20972
[1mStep[0m  [12/42], [94mLoss[0m : 2.17266
[1mStep[0m  [16/42], [94mLoss[0m : 2.13574
[1mStep[0m  [20/42], [94mLoss[0m : 2.41714
[1mStep[0m  [24/42], [94mLoss[0m : 2.21640
[1mStep[0m  [28/42], [94mLoss[0m : 2.30563
[1mStep[0m  [32/42], [94mLoss[0m : 2.21742
[1mStep[0m  [36/42], [94mLoss[0m : 2.07734
[1mStep[0m  [40/42], [94mLoss[0m : 2.27240

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34169
[1mStep[0m  [4/42], [94mLoss[0m : 2.11309
[1mStep[0m  [8/42], [94mLoss[0m : 2.36413
[1mStep[0m  [12/42], [94mLoss[0m : 2.04642
[1mStep[0m  [16/42], [94mLoss[0m : 2.15832
[1mStep[0m  [20/42], [94mLoss[0m : 2.29172
[1mStep[0m  [24/42], [94mLoss[0m : 2.38480
[1mStep[0m  [28/42], [94mLoss[0m : 2.33544
[1mStep[0m  [32/42], [94mLoss[0m : 2.10055
[1mStep[0m  [36/42], [94mLoss[0m : 2.20115
[1mStep[0m  [40/42], [94mLoss[0m : 2.10146

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24270
[1mStep[0m  [4/42], [94mLoss[0m : 2.05553
[1mStep[0m  [8/42], [94mLoss[0m : 2.47032
[1mStep[0m  [12/42], [94mLoss[0m : 2.12953
[1mStep[0m  [16/42], [94mLoss[0m : 2.50274
[1mStep[0m  [20/42], [94mLoss[0m : 1.99732
[1mStep[0m  [24/42], [94mLoss[0m : 2.17992
[1mStep[0m  [28/42], [94mLoss[0m : 2.24243
[1mStep[0m  [32/42], [94mLoss[0m : 2.02031
[1mStep[0m  [36/42], [94mLoss[0m : 2.21017
[1mStep[0m  [40/42], [94mLoss[0m : 1.80096

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11740
[1mStep[0m  [4/42], [94mLoss[0m : 2.11514
[1mStep[0m  [8/42], [94mLoss[0m : 2.11015
[1mStep[0m  [12/42], [94mLoss[0m : 2.17222
[1mStep[0m  [16/42], [94mLoss[0m : 2.07749
[1mStep[0m  [20/42], [94mLoss[0m : 2.00034
[1mStep[0m  [24/42], [94mLoss[0m : 2.04907
[1mStep[0m  [28/42], [94mLoss[0m : 1.95092
[1mStep[0m  [32/42], [94mLoss[0m : 2.31662
[1mStep[0m  [36/42], [94mLoss[0m : 1.96314
[1mStep[0m  [40/42], [94mLoss[0m : 2.05243

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.530, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83625
[1mStep[0m  [4/42], [94mLoss[0m : 2.12514
[1mStep[0m  [8/42], [94mLoss[0m : 2.18918
[1mStep[0m  [12/42], [94mLoss[0m : 2.03460
[1mStep[0m  [16/42], [94mLoss[0m : 1.98663
[1mStep[0m  [20/42], [94mLoss[0m : 2.00107
[1mStep[0m  [24/42], [94mLoss[0m : 2.14312
[1mStep[0m  [28/42], [94mLoss[0m : 2.21658
[1mStep[0m  [32/42], [94mLoss[0m : 2.08414
[1mStep[0m  [36/42], [94mLoss[0m : 1.86417
[1mStep[0m  [40/42], [94mLoss[0m : 2.01270

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05638
[1mStep[0m  [4/42], [94mLoss[0m : 1.86650
[1mStep[0m  [8/42], [94mLoss[0m : 1.94556
[1mStep[0m  [12/42], [94mLoss[0m : 2.11242
[1mStep[0m  [16/42], [94mLoss[0m : 2.16129
[1mStep[0m  [20/42], [94mLoss[0m : 1.91356
[1mStep[0m  [24/42], [94mLoss[0m : 2.05533
[1mStep[0m  [28/42], [94mLoss[0m : 1.97315
[1mStep[0m  [32/42], [94mLoss[0m : 2.01303
[1mStep[0m  [36/42], [94mLoss[0m : 2.00540
[1mStep[0m  [40/42], [94mLoss[0m : 2.21618

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89399
[1mStep[0m  [4/42], [94mLoss[0m : 1.97354
[1mStep[0m  [8/42], [94mLoss[0m : 1.98975
[1mStep[0m  [12/42], [94mLoss[0m : 1.71633
[1mStep[0m  [16/42], [94mLoss[0m : 1.84792
[1mStep[0m  [20/42], [94mLoss[0m : 1.91442
[1mStep[0m  [24/42], [94mLoss[0m : 1.91024
[1mStep[0m  [28/42], [94mLoss[0m : 1.95245
[1mStep[0m  [32/42], [94mLoss[0m : 2.09774
[1mStep[0m  [36/42], [94mLoss[0m : 1.77145
[1mStep[0m  [40/42], [94mLoss[0m : 2.05893

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.500, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77223
[1mStep[0m  [4/42], [94mLoss[0m : 1.99263
[1mStep[0m  [8/42], [94mLoss[0m : 1.86711
[1mStep[0m  [12/42], [94mLoss[0m : 1.94695
[1mStep[0m  [16/42], [94mLoss[0m : 1.88719
[1mStep[0m  [20/42], [94mLoss[0m : 2.09275
[1mStep[0m  [24/42], [94mLoss[0m : 1.98364
[1mStep[0m  [28/42], [94mLoss[0m : 1.99663
[1mStep[0m  [32/42], [94mLoss[0m : 1.82695
[1mStep[0m  [36/42], [94mLoss[0m : 2.05570
[1mStep[0m  [40/42], [94mLoss[0m : 1.77868

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91855
[1mStep[0m  [4/42], [94mLoss[0m : 1.67621
[1mStep[0m  [8/42], [94mLoss[0m : 1.87194
[1mStep[0m  [12/42], [94mLoss[0m : 1.80338
[1mStep[0m  [16/42], [94mLoss[0m : 1.94546
[1mStep[0m  [20/42], [94mLoss[0m : 1.91600
[1mStep[0m  [24/42], [94mLoss[0m : 1.83112
[1mStep[0m  [28/42], [94mLoss[0m : 1.93985
[1mStep[0m  [32/42], [94mLoss[0m : 1.78753
[1mStep[0m  [36/42], [94mLoss[0m : 2.03322
[1mStep[0m  [40/42], [94mLoss[0m : 1.79537

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.866, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83321
[1mStep[0m  [4/42], [94mLoss[0m : 1.72360
[1mStep[0m  [8/42], [94mLoss[0m : 1.96820
[1mStep[0m  [12/42], [94mLoss[0m : 1.90569
[1mStep[0m  [16/42], [94mLoss[0m : 2.02650
[1mStep[0m  [20/42], [94mLoss[0m : 1.75693
[1mStep[0m  [24/42], [94mLoss[0m : 1.78426
[1mStep[0m  [28/42], [94mLoss[0m : 1.88735
[1mStep[0m  [32/42], [94mLoss[0m : 1.82259
[1mStep[0m  [36/42], [94mLoss[0m : 1.99855
[1mStep[0m  [40/42], [94mLoss[0m : 1.71541

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.632, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66022
[1mStep[0m  [4/42], [94mLoss[0m : 1.81567
[1mStep[0m  [8/42], [94mLoss[0m : 1.62949
[1mStep[0m  [12/42], [94mLoss[0m : 1.83152
[1mStep[0m  [16/42], [94mLoss[0m : 1.85835
[1mStep[0m  [20/42], [94mLoss[0m : 1.63315
[1mStep[0m  [24/42], [94mLoss[0m : 1.90202
[1mStep[0m  [28/42], [94mLoss[0m : 1.64281
[1mStep[0m  [32/42], [94mLoss[0m : 1.71498
[1mStep[0m  [36/42], [94mLoss[0m : 2.01241
[1mStep[0m  [40/42], [94mLoss[0m : 1.82313

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.573, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73846
[1mStep[0m  [4/42], [94mLoss[0m : 1.68359
[1mStep[0m  [8/42], [94mLoss[0m : 1.90245
[1mStep[0m  [12/42], [94mLoss[0m : 1.68716
[1mStep[0m  [16/42], [94mLoss[0m : 1.60467
[1mStep[0m  [20/42], [94mLoss[0m : 1.84351
[1mStep[0m  [24/42], [94mLoss[0m : 1.79072
[1mStep[0m  [28/42], [94mLoss[0m : 1.81662
[1mStep[0m  [32/42], [94mLoss[0m : 1.73395
[1mStep[0m  [36/42], [94mLoss[0m : 1.84674
[1mStep[0m  [40/42], [94mLoss[0m : 1.89726

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.582, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58368
[1mStep[0m  [4/42], [94mLoss[0m : 1.67834
[1mStep[0m  [8/42], [94mLoss[0m : 1.66835
[1mStep[0m  [12/42], [94mLoss[0m : 1.83183
[1mStep[0m  [16/42], [94mLoss[0m : 1.69555
[1mStep[0m  [20/42], [94mLoss[0m : 1.54451
[1mStep[0m  [24/42], [94mLoss[0m : 1.63545
[1mStep[0m  [28/42], [94mLoss[0m : 1.77293
[1mStep[0m  [32/42], [94mLoss[0m : 1.77512
[1mStep[0m  [36/42], [94mLoss[0m : 1.69267
[1mStep[0m  [40/42], [94mLoss[0m : 1.70247

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65877
[1mStep[0m  [4/42], [94mLoss[0m : 1.85370
[1mStep[0m  [8/42], [94mLoss[0m : 1.72471
[1mStep[0m  [12/42], [94mLoss[0m : 1.70329
[1mStep[0m  [16/42], [94mLoss[0m : 1.62370
[1mStep[0m  [20/42], [94mLoss[0m : 1.43372
[1mStep[0m  [24/42], [94mLoss[0m : 1.64371
[1mStep[0m  [28/42], [94mLoss[0m : 1.93658
[1mStep[0m  [32/42], [94mLoss[0m : 1.61930
[1mStep[0m  [36/42], [94mLoss[0m : 1.75298
[1mStep[0m  [40/42], [94mLoss[0m : 1.63520

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.662, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61476
[1mStep[0m  [4/42], [94mLoss[0m : 1.75845
[1mStep[0m  [8/42], [94mLoss[0m : 1.57672
[1mStep[0m  [12/42], [94mLoss[0m : 1.62044
[1mStep[0m  [16/42], [94mLoss[0m : 1.68357
[1mStep[0m  [20/42], [94mLoss[0m : 1.69473
[1mStep[0m  [24/42], [94mLoss[0m : 1.64485
[1mStep[0m  [28/42], [94mLoss[0m : 1.57848
[1mStep[0m  [32/42], [94mLoss[0m : 1.61296
[1mStep[0m  [36/42], [94mLoss[0m : 1.62817
[1mStep[0m  [40/42], [94mLoss[0m : 1.62435

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60684
[1mStep[0m  [4/42], [94mLoss[0m : 1.62196
[1mStep[0m  [8/42], [94mLoss[0m : 1.61578
[1mStep[0m  [12/42], [94mLoss[0m : 1.58299
[1mStep[0m  [16/42], [94mLoss[0m : 1.65964
[1mStep[0m  [20/42], [94mLoss[0m : 1.63496
[1mStep[0m  [24/42], [94mLoss[0m : 1.55768
[1mStep[0m  [28/42], [94mLoss[0m : 1.65237
[1mStep[0m  [32/42], [94mLoss[0m : 1.72343
[1mStep[0m  [36/42], [94mLoss[0m : 1.62918
[1mStep[0m  [40/42], [94mLoss[0m : 1.57171

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.573, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42150
[1mStep[0m  [4/42], [94mLoss[0m : 1.57876
[1mStep[0m  [8/42], [94mLoss[0m : 1.47929
[1mStep[0m  [12/42], [94mLoss[0m : 1.51715
[1mStep[0m  [16/42], [94mLoss[0m : 1.44796
[1mStep[0m  [20/42], [94mLoss[0m : 1.56240
[1mStep[0m  [24/42], [94mLoss[0m : 1.52413
[1mStep[0m  [28/42], [94mLoss[0m : 1.66671
[1mStep[0m  [32/42], [94mLoss[0m : 1.59933
[1mStep[0m  [36/42], [94mLoss[0m : 1.62873
[1mStep[0m  [40/42], [94mLoss[0m : 1.64318

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48964
[1mStep[0m  [4/42], [94mLoss[0m : 1.44381
[1mStep[0m  [8/42], [94mLoss[0m : 1.59708
[1mStep[0m  [12/42], [94mLoss[0m : 1.46308
[1mStep[0m  [16/42], [94mLoss[0m : 1.54725
[1mStep[0m  [20/42], [94mLoss[0m : 1.56399
[1mStep[0m  [24/42], [94mLoss[0m : 1.57340
[1mStep[0m  [28/42], [94mLoss[0m : 1.40081
[1mStep[0m  [32/42], [94mLoss[0m : 1.58209
[1mStep[0m  [36/42], [94mLoss[0m : 1.57680
[1mStep[0m  [40/42], [94mLoss[0m : 1.45498

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.529, [92mTest[0m: 2.540, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44527
[1mStep[0m  [4/42], [94mLoss[0m : 1.53978
[1mStep[0m  [8/42], [94mLoss[0m : 1.60552
[1mStep[0m  [12/42], [94mLoss[0m : 1.51627
[1mStep[0m  [16/42], [94mLoss[0m : 1.45869
[1mStep[0m  [20/42], [94mLoss[0m : 1.54328
[1mStep[0m  [24/42], [94mLoss[0m : 1.38065
[1mStep[0m  [28/42], [94mLoss[0m : 1.60500
[1mStep[0m  [32/42], [94mLoss[0m : 1.40752
[1mStep[0m  [36/42], [94mLoss[0m : 1.53058
[1mStep[0m  [40/42], [94mLoss[0m : 1.51017

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.600, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46445
[1mStep[0m  [4/42], [94mLoss[0m : 1.38916
[1mStep[0m  [8/42], [94mLoss[0m : 1.37785
[1mStep[0m  [12/42], [94mLoss[0m : 1.55300
[1mStep[0m  [16/42], [94mLoss[0m : 1.49945
[1mStep[0m  [20/42], [94mLoss[0m : 1.44813
[1mStep[0m  [24/42], [94mLoss[0m : 1.62432
[1mStep[0m  [28/42], [94mLoss[0m : 1.34642
[1mStep[0m  [32/42], [94mLoss[0m : 1.41929
[1mStep[0m  [36/42], [94mLoss[0m : 1.44119
[1mStep[0m  [40/42], [94mLoss[0m : 1.50250

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.561, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38880
[1mStep[0m  [4/42], [94mLoss[0m : 1.45288
[1mStep[0m  [8/42], [94mLoss[0m : 1.43509
[1mStep[0m  [12/42], [94mLoss[0m : 1.57214
[1mStep[0m  [16/42], [94mLoss[0m : 1.39563
[1mStep[0m  [20/42], [94mLoss[0m : 1.43103
[1mStep[0m  [24/42], [94mLoss[0m : 1.40945
[1mStep[0m  [28/42], [94mLoss[0m : 1.52799
[1mStep[0m  [32/42], [94mLoss[0m : 1.35806
[1mStep[0m  [36/42], [94mLoss[0m : 1.56202
[1mStep[0m  [40/42], [94mLoss[0m : 1.28528

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.606, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.24027
[1mStep[0m  [4/42], [94mLoss[0m : 1.54529
[1mStep[0m  [8/42], [94mLoss[0m : 1.38034
[1mStep[0m  [12/42], [94mLoss[0m : 1.47011
[1mStep[0m  [16/42], [94mLoss[0m : 1.47603
[1mStep[0m  [20/42], [94mLoss[0m : 1.55027
[1mStep[0m  [24/42], [94mLoss[0m : 1.21007
[1mStep[0m  [28/42], [94mLoss[0m : 1.26835
[1mStep[0m  [32/42], [94mLoss[0m : 1.47141
[1mStep[0m  [36/42], [94mLoss[0m : 1.45789
[1mStep[0m  [40/42], [94mLoss[0m : 1.48042

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.436, [92mTest[0m: 2.534, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.501
====================================

Phase 2 - Evaluation MAE:  2.5006809064320157
MAE score P1      2.329695
MAE score P2      2.500681
loss              1.436146
learning_rate         0.01
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay         0.001
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.27164
[1mStep[0m  [8/84], [94mLoss[0m : 10.41271
[1mStep[0m  [16/84], [94mLoss[0m : 9.67330
[1mStep[0m  [24/84], [94mLoss[0m : 8.87893
[1mStep[0m  [32/84], [94mLoss[0m : 7.28315
[1mStep[0m  [40/84], [94mLoss[0m : 5.66340
[1mStep[0m  [48/84], [94mLoss[0m : 4.75366
[1mStep[0m  [56/84], [94mLoss[0m : 4.32799
[1mStep[0m  [64/84], [94mLoss[0m : 3.03840
[1mStep[0m  [72/84], [94mLoss[0m : 3.22212
[1mStep[0m  [80/84], [94mLoss[0m : 2.69228

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.233, [92mTest[0m: 11.035, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70090
[1mStep[0m  [8/84], [94mLoss[0m : 2.66673
[1mStep[0m  [16/84], [94mLoss[0m : 2.61072
[1mStep[0m  [24/84], [94mLoss[0m : 2.58384
[1mStep[0m  [32/84], [94mLoss[0m : 2.45947
[1mStep[0m  [40/84], [94mLoss[0m : 2.71125
[1mStep[0m  [48/84], [94mLoss[0m : 2.89290
[1mStep[0m  [56/84], [94mLoss[0m : 2.80340
[1mStep[0m  [64/84], [94mLoss[0m : 2.57861
[1mStep[0m  [72/84], [94mLoss[0m : 2.31412
[1mStep[0m  [80/84], [94mLoss[0m : 2.89791

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.687, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54364
[1mStep[0m  [8/84], [94mLoss[0m : 2.71693
[1mStep[0m  [16/84], [94mLoss[0m : 2.35214
[1mStep[0m  [24/84], [94mLoss[0m : 2.57794
[1mStep[0m  [32/84], [94mLoss[0m : 2.32926
[1mStep[0m  [40/84], [94mLoss[0m : 2.51100
[1mStep[0m  [48/84], [94mLoss[0m : 2.61551
[1mStep[0m  [56/84], [94mLoss[0m : 2.52192
[1mStep[0m  [64/84], [94mLoss[0m : 2.75025
[1mStep[0m  [72/84], [94mLoss[0m : 2.41987
[1mStep[0m  [80/84], [94mLoss[0m : 2.39494

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84390
[1mStep[0m  [8/84], [94mLoss[0m : 2.70031
[1mStep[0m  [16/84], [94mLoss[0m : 2.63526
[1mStep[0m  [24/84], [94mLoss[0m : 2.64514
[1mStep[0m  [32/84], [94mLoss[0m : 2.31493
[1mStep[0m  [40/84], [94mLoss[0m : 2.87238
[1mStep[0m  [48/84], [94mLoss[0m : 2.41591
[1mStep[0m  [56/84], [94mLoss[0m : 2.69649
[1mStep[0m  [64/84], [94mLoss[0m : 2.94105
[1mStep[0m  [72/84], [94mLoss[0m : 2.45700
[1mStep[0m  [80/84], [94mLoss[0m : 2.52927

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27304
[1mStep[0m  [8/84], [94mLoss[0m : 2.46129
[1mStep[0m  [16/84], [94mLoss[0m : 2.51116
[1mStep[0m  [24/84], [94mLoss[0m : 2.63056
[1mStep[0m  [32/84], [94mLoss[0m : 2.43761
[1mStep[0m  [40/84], [94mLoss[0m : 2.25285
[1mStep[0m  [48/84], [94mLoss[0m : 2.57459
[1mStep[0m  [56/84], [94mLoss[0m : 2.59110
[1mStep[0m  [64/84], [94mLoss[0m : 2.61685
[1mStep[0m  [72/84], [94mLoss[0m : 2.32682
[1mStep[0m  [80/84], [94mLoss[0m : 2.03136

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38916
[1mStep[0m  [8/84], [94mLoss[0m : 2.53624
[1mStep[0m  [16/84], [94mLoss[0m : 2.19565
[1mStep[0m  [24/84], [94mLoss[0m : 2.61820
[1mStep[0m  [32/84], [94mLoss[0m : 2.92666
[1mStep[0m  [40/84], [94mLoss[0m : 2.47245
[1mStep[0m  [48/84], [94mLoss[0m : 2.46589
[1mStep[0m  [56/84], [94mLoss[0m : 2.65642
[1mStep[0m  [64/84], [94mLoss[0m : 2.49019
[1mStep[0m  [72/84], [94mLoss[0m : 2.66661
[1mStep[0m  [80/84], [94mLoss[0m : 2.65866

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57358
[1mStep[0m  [8/84], [94mLoss[0m : 2.70296
[1mStep[0m  [16/84], [94mLoss[0m : 2.78189
[1mStep[0m  [24/84], [94mLoss[0m : 2.45349
[1mStep[0m  [32/84], [94mLoss[0m : 2.52065
[1mStep[0m  [40/84], [94mLoss[0m : 2.48605
[1mStep[0m  [48/84], [94mLoss[0m : 2.62497
[1mStep[0m  [56/84], [94mLoss[0m : 2.48129
[1mStep[0m  [64/84], [94mLoss[0m : 2.25206
[1mStep[0m  [72/84], [94mLoss[0m : 2.54791
[1mStep[0m  [80/84], [94mLoss[0m : 2.32376

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42921
[1mStep[0m  [8/84], [94mLoss[0m : 2.55245
[1mStep[0m  [16/84], [94mLoss[0m : 2.61699
[1mStep[0m  [24/84], [94mLoss[0m : 2.82070
[1mStep[0m  [32/84], [94mLoss[0m : 2.34220
[1mStep[0m  [40/84], [94mLoss[0m : 2.68311
[1mStep[0m  [48/84], [94mLoss[0m : 2.51684
[1mStep[0m  [56/84], [94mLoss[0m : 2.39320
[1mStep[0m  [64/84], [94mLoss[0m : 2.43631
[1mStep[0m  [72/84], [94mLoss[0m : 2.42478
[1mStep[0m  [80/84], [94mLoss[0m : 2.26596

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44841
[1mStep[0m  [8/84], [94mLoss[0m : 2.47174
[1mStep[0m  [16/84], [94mLoss[0m : 2.75823
[1mStep[0m  [24/84], [94mLoss[0m : 2.62463
[1mStep[0m  [32/84], [94mLoss[0m : 2.16837
[1mStep[0m  [40/84], [94mLoss[0m : 2.22000
[1mStep[0m  [48/84], [94mLoss[0m : 2.64886
[1mStep[0m  [56/84], [94mLoss[0m : 2.30680
[1mStep[0m  [64/84], [94mLoss[0m : 2.50601
[1mStep[0m  [72/84], [94mLoss[0m : 2.56569
[1mStep[0m  [80/84], [94mLoss[0m : 2.41584

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67822
[1mStep[0m  [8/84], [94mLoss[0m : 2.68269
[1mStep[0m  [16/84], [94mLoss[0m : 2.33839
[1mStep[0m  [24/84], [94mLoss[0m : 2.64415
[1mStep[0m  [32/84], [94mLoss[0m : 2.58976
[1mStep[0m  [40/84], [94mLoss[0m : 2.27641
[1mStep[0m  [48/84], [94mLoss[0m : 2.76828
[1mStep[0m  [56/84], [94mLoss[0m : 2.58896
[1mStep[0m  [64/84], [94mLoss[0m : 2.43561
[1mStep[0m  [72/84], [94mLoss[0m : 2.61872
[1mStep[0m  [80/84], [94mLoss[0m : 2.80569

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47896
[1mStep[0m  [8/84], [94mLoss[0m : 2.71562
[1mStep[0m  [16/84], [94mLoss[0m : 2.35104
[1mStep[0m  [24/84], [94mLoss[0m : 2.65973
[1mStep[0m  [32/84], [94mLoss[0m : 2.57687
[1mStep[0m  [40/84], [94mLoss[0m : 2.56117
[1mStep[0m  [48/84], [94mLoss[0m : 2.83547
[1mStep[0m  [56/84], [94mLoss[0m : 2.37486
[1mStep[0m  [64/84], [94mLoss[0m : 2.45159
[1mStep[0m  [72/84], [94mLoss[0m : 2.38032
[1mStep[0m  [80/84], [94mLoss[0m : 2.48052

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61259
[1mStep[0m  [8/84], [94mLoss[0m : 2.31253
[1mStep[0m  [16/84], [94mLoss[0m : 2.58964
[1mStep[0m  [24/84], [94mLoss[0m : 2.25323
[1mStep[0m  [32/84], [94mLoss[0m : 2.48027
[1mStep[0m  [40/84], [94mLoss[0m : 2.31194
[1mStep[0m  [48/84], [94mLoss[0m : 2.25805
[1mStep[0m  [56/84], [94mLoss[0m : 2.55136
[1mStep[0m  [64/84], [94mLoss[0m : 2.16247
[1mStep[0m  [72/84], [94mLoss[0m : 2.64516
[1mStep[0m  [80/84], [94mLoss[0m : 2.36115

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66448
[1mStep[0m  [8/84], [94mLoss[0m : 2.43874
[1mStep[0m  [16/84], [94mLoss[0m : 2.71474
[1mStep[0m  [24/84], [94mLoss[0m : 2.22963
[1mStep[0m  [32/84], [94mLoss[0m : 2.38425
[1mStep[0m  [40/84], [94mLoss[0m : 2.64773
[1mStep[0m  [48/84], [94mLoss[0m : 2.50887
[1mStep[0m  [56/84], [94mLoss[0m : 2.61338
[1mStep[0m  [64/84], [94mLoss[0m : 2.65625
[1mStep[0m  [72/84], [94mLoss[0m : 2.47992
[1mStep[0m  [80/84], [94mLoss[0m : 2.48602

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20307
[1mStep[0m  [8/84], [94mLoss[0m : 2.22124
[1mStep[0m  [16/84], [94mLoss[0m : 2.19525
[1mStep[0m  [24/84], [94mLoss[0m : 2.47532
[1mStep[0m  [32/84], [94mLoss[0m : 2.49235
[1mStep[0m  [40/84], [94mLoss[0m : 2.51162
[1mStep[0m  [48/84], [94mLoss[0m : 2.53898
[1mStep[0m  [56/84], [94mLoss[0m : 2.49014
[1mStep[0m  [64/84], [94mLoss[0m : 2.56601
[1mStep[0m  [72/84], [94mLoss[0m : 2.64095
[1mStep[0m  [80/84], [94mLoss[0m : 2.66424

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22535
[1mStep[0m  [8/84], [94mLoss[0m : 2.37846
[1mStep[0m  [16/84], [94mLoss[0m : 2.16138
[1mStep[0m  [24/84], [94mLoss[0m : 2.44866
[1mStep[0m  [32/84], [94mLoss[0m : 2.47784
[1mStep[0m  [40/84], [94mLoss[0m : 2.45573
[1mStep[0m  [48/84], [94mLoss[0m : 2.57154
[1mStep[0m  [56/84], [94mLoss[0m : 2.28876
[1mStep[0m  [64/84], [94mLoss[0m : 2.57887
[1mStep[0m  [72/84], [94mLoss[0m : 2.34571
[1mStep[0m  [80/84], [94mLoss[0m : 2.52426

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26376
[1mStep[0m  [8/84], [94mLoss[0m : 2.41197
[1mStep[0m  [16/84], [94mLoss[0m : 2.29642
[1mStep[0m  [24/84], [94mLoss[0m : 2.09417
[1mStep[0m  [32/84], [94mLoss[0m : 2.71392
[1mStep[0m  [40/84], [94mLoss[0m : 2.69874
[1mStep[0m  [48/84], [94mLoss[0m : 2.49167
[1mStep[0m  [56/84], [94mLoss[0m : 2.44755
[1mStep[0m  [64/84], [94mLoss[0m : 2.71903
[1mStep[0m  [72/84], [94mLoss[0m : 2.28884
[1mStep[0m  [80/84], [94mLoss[0m : 2.42358

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59872
[1mStep[0m  [8/84], [94mLoss[0m : 2.36254
[1mStep[0m  [16/84], [94mLoss[0m : 2.29587
[1mStep[0m  [24/84], [94mLoss[0m : 2.23969
[1mStep[0m  [32/84], [94mLoss[0m : 2.33141
[1mStep[0m  [40/84], [94mLoss[0m : 2.19391
[1mStep[0m  [48/84], [94mLoss[0m : 2.80830
[1mStep[0m  [56/84], [94mLoss[0m : 2.70708
[1mStep[0m  [64/84], [94mLoss[0m : 2.51287
[1mStep[0m  [72/84], [94mLoss[0m : 2.56998
[1mStep[0m  [80/84], [94mLoss[0m : 2.51089

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55020
[1mStep[0m  [8/84], [94mLoss[0m : 2.47412
[1mStep[0m  [16/84], [94mLoss[0m : 2.40345
[1mStep[0m  [24/84], [94mLoss[0m : 2.62762
[1mStep[0m  [32/84], [94mLoss[0m : 2.56905
[1mStep[0m  [40/84], [94mLoss[0m : 2.31633
[1mStep[0m  [48/84], [94mLoss[0m : 2.22003
[1mStep[0m  [56/84], [94mLoss[0m : 2.28358
[1mStep[0m  [64/84], [94mLoss[0m : 2.45148
[1mStep[0m  [72/84], [94mLoss[0m : 2.77757
[1mStep[0m  [80/84], [94mLoss[0m : 2.42877

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37628
[1mStep[0m  [8/84], [94mLoss[0m : 2.49113
[1mStep[0m  [16/84], [94mLoss[0m : 2.58097
[1mStep[0m  [24/84], [94mLoss[0m : 2.40142
[1mStep[0m  [32/84], [94mLoss[0m : 2.30581
[1mStep[0m  [40/84], [94mLoss[0m : 2.70667
[1mStep[0m  [48/84], [94mLoss[0m : 2.48012
[1mStep[0m  [56/84], [94mLoss[0m : 2.39885
[1mStep[0m  [64/84], [94mLoss[0m : 2.40636
[1mStep[0m  [72/84], [94mLoss[0m : 2.30653
[1mStep[0m  [80/84], [94mLoss[0m : 2.46222

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19737
[1mStep[0m  [8/84], [94mLoss[0m : 2.44421
[1mStep[0m  [16/84], [94mLoss[0m : 2.23909
[1mStep[0m  [24/84], [94mLoss[0m : 2.35649
[1mStep[0m  [32/84], [94mLoss[0m : 2.37151
[1mStep[0m  [40/84], [94mLoss[0m : 2.36661
[1mStep[0m  [48/84], [94mLoss[0m : 2.29456
[1mStep[0m  [56/84], [94mLoss[0m : 2.44669
[1mStep[0m  [64/84], [94mLoss[0m : 2.29924
[1mStep[0m  [72/84], [94mLoss[0m : 2.52883
[1mStep[0m  [80/84], [94mLoss[0m : 2.41486

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67988
[1mStep[0m  [8/84], [94mLoss[0m : 2.33192
[1mStep[0m  [16/84], [94mLoss[0m : 2.16202
[1mStep[0m  [24/84], [94mLoss[0m : 2.52994
[1mStep[0m  [32/84], [94mLoss[0m : 2.56627
[1mStep[0m  [40/84], [94mLoss[0m : 2.51240
[1mStep[0m  [48/84], [94mLoss[0m : 2.40807
[1mStep[0m  [56/84], [94mLoss[0m : 2.43936
[1mStep[0m  [64/84], [94mLoss[0m : 2.80874
[1mStep[0m  [72/84], [94mLoss[0m : 2.36566
[1mStep[0m  [80/84], [94mLoss[0m : 2.37811

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43669
[1mStep[0m  [8/84], [94mLoss[0m : 2.14716
[1mStep[0m  [16/84], [94mLoss[0m : 2.56434
[1mStep[0m  [24/84], [94mLoss[0m : 2.54886
[1mStep[0m  [32/84], [94mLoss[0m : 2.46579
[1mStep[0m  [40/84], [94mLoss[0m : 2.51064
[1mStep[0m  [48/84], [94mLoss[0m : 2.90553
[1mStep[0m  [56/84], [94mLoss[0m : 2.71246
[1mStep[0m  [64/84], [94mLoss[0m : 2.35716
[1mStep[0m  [72/84], [94mLoss[0m : 2.45163
[1mStep[0m  [80/84], [94mLoss[0m : 2.76909

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35670
[1mStep[0m  [8/84], [94mLoss[0m : 2.23450
[1mStep[0m  [16/84], [94mLoss[0m : 2.64241
[1mStep[0m  [24/84], [94mLoss[0m : 2.17994
[1mStep[0m  [32/84], [94mLoss[0m : 2.42511
[1mStep[0m  [40/84], [94mLoss[0m : 2.26157
[1mStep[0m  [48/84], [94mLoss[0m : 2.58937
[1mStep[0m  [56/84], [94mLoss[0m : 2.55014
[1mStep[0m  [64/84], [94mLoss[0m : 2.49242
[1mStep[0m  [72/84], [94mLoss[0m : 2.57196
[1mStep[0m  [80/84], [94mLoss[0m : 2.54971

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28312
[1mStep[0m  [8/84], [94mLoss[0m : 2.54694
[1mStep[0m  [16/84], [94mLoss[0m : 2.53371
[1mStep[0m  [24/84], [94mLoss[0m : 2.52122
[1mStep[0m  [32/84], [94mLoss[0m : 2.48262
[1mStep[0m  [40/84], [94mLoss[0m : 2.31644
[1mStep[0m  [48/84], [94mLoss[0m : 2.59449
[1mStep[0m  [56/84], [94mLoss[0m : 2.08486
[1mStep[0m  [64/84], [94mLoss[0m : 2.35029
[1mStep[0m  [72/84], [94mLoss[0m : 2.80303
[1mStep[0m  [80/84], [94mLoss[0m : 2.62454

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35996
[1mStep[0m  [8/84], [94mLoss[0m : 2.41736
[1mStep[0m  [16/84], [94mLoss[0m : 2.38153
[1mStep[0m  [24/84], [94mLoss[0m : 2.38021
[1mStep[0m  [32/84], [94mLoss[0m : 2.77830
[1mStep[0m  [40/84], [94mLoss[0m : 2.97440
[1mStep[0m  [48/84], [94mLoss[0m : 2.54858
[1mStep[0m  [56/84], [94mLoss[0m : 2.25112
[1mStep[0m  [64/84], [94mLoss[0m : 2.55054
[1mStep[0m  [72/84], [94mLoss[0m : 2.45529
[1mStep[0m  [80/84], [94mLoss[0m : 2.31009

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72177
[1mStep[0m  [8/84], [94mLoss[0m : 2.58080
[1mStep[0m  [16/84], [94mLoss[0m : 2.47455
[1mStep[0m  [24/84], [94mLoss[0m : 2.39443
[1mStep[0m  [32/84], [94mLoss[0m : 2.73185
[1mStep[0m  [40/84], [94mLoss[0m : 2.57066
[1mStep[0m  [48/84], [94mLoss[0m : 2.76006
[1mStep[0m  [56/84], [94mLoss[0m : 2.29073
[1mStep[0m  [64/84], [94mLoss[0m : 2.75055
[1mStep[0m  [72/84], [94mLoss[0m : 2.56846
[1mStep[0m  [80/84], [94mLoss[0m : 2.50782

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55983
[1mStep[0m  [8/84], [94mLoss[0m : 2.23743
[1mStep[0m  [16/84], [94mLoss[0m : 2.50893
[1mStep[0m  [24/84], [94mLoss[0m : 2.41962
[1mStep[0m  [32/84], [94mLoss[0m : 2.50140
[1mStep[0m  [40/84], [94mLoss[0m : 2.29377
[1mStep[0m  [48/84], [94mLoss[0m : 2.59885
[1mStep[0m  [56/84], [94mLoss[0m : 2.43323
[1mStep[0m  [64/84], [94mLoss[0m : 2.30778
[1mStep[0m  [72/84], [94mLoss[0m : 2.57538
[1mStep[0m  [80/84], [94mLoss[0m : 2.69646

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47387
[1mStep[0m  [8/84], [94mLoss[0m : 2.20963
[1mStep[0m  [16/84], [94mLoss[0m : 2.09002
[1mStep[0m  [24/84], [94mLoss[0m : 2.54073
[1mStep[0m  [32/84], [94mLoss[0m : 2.08290
[1mStep[0m  [40/84], [94mLoss[0m : 2.45841
[1mStep[0m  [48/84], [94mLoss[0m : 2.55093
[1mStep[0m  [56/84], [94mLoss[0m : 2.61682
[1mStep[0m  [64/84], [94mLoss[0m : 2.36412
[1mStep[0m  [72/84], [94mLoss[0m : 2.51775
[1mStep[0m  [80/84], [94mLoss[0m : 2.59201

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.365, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91652
[1mStep[0m  [8/84], [94mLoss[0m : 2.59627
[1mStep[0m  [16/84], [94mLoss[0m : 2.61835
[1mStep[0m  [24/84], [94mLoss[0m : 2.41841
[1mStep[0m  [32/84], [94mLoss[0m : 2.67845
[1mStep[0m  [40/84], [94mLoss[0m : 2.25038
[1mStep[0m  [48/84], [94mLoss[0m : 2.52831
[1mStep[0m  [56/84], [94mLoss[0m : 2.55633
[1mStep[0m  [64/84], [94mLoss[0m : 2.56914
[1mStep[0m  [72/84], [94mLoss[0m : 2.33011
[1mStep[0m  [80/84], [94mLoss[0m : 2.17029

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34455
[1mStep[0m  [8/84], [94mLoss[0m : 2.55633
[1mStep[0m  [16/84], [94mLoss[0m : 2.58500
[1mStep[0m  [24/84], [94mLoss[0m : 2.42245
[1mStep[0m  [32/84], [94mLoss[0m : 2.37789
[1mStep[0m  [40/84], [94mLoss[0m : 2.21061
[1mStep[0m  [48/84], [94mLoss[0m : 2.19420
[1mStep[0m  [56/84], [94mLoss[0m : 2.38980
[1mStep[0m  [64/84], [94mLoss[0m : 2.39660
[1mStep[0m  [72/84], [94mLoss[0m : 2.48423
[1mStep[0m  [80/84], [94mLoss[0m : 2.65743

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.350
====================================

Phase 1 - Evaluation MAE:  2.349543980189732
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.52805
[1mStep[0m  [8/84], [94mLoss[0m : 2.35667
[1mStep[0m  [16/84], [94mLoss[0m : 2.48303
[1mStep[0m  [24/84], [94mLoss[0m : 2.61783
[1mStep[0m  [32/84], [94mLoss[0m : 2.54859
[1mStep[0m  [40/84], [94mLoss[0m : 2.73657
[1mStep[0m  [48/84], [94mLoss[0m : 2.28848
[1mStep[0m  [56/84], [94mLoss[0m : 2.16200
[1mStep[0m  [64/84], [94mLoss[0m : 2.49894
[1mStep[0m  [72/84], [94mLoss[0m : 2.36730
[1mStep[0m  [80/84], [94mLoss[0m : 2.36896

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35776
[1mStep[0m  [8/84], [94mLoss[0m : 2.66442
[1mStep[0m  [16/84], [94mLoss[0m : 2.30753
[1mStep[0m  [24/84], [94mLoss[0m : 2.13070
[1mStep[0m  [32/84], [94mLoss[0m : 2.36550
[1mStep[0m  [40/84], [94mLoss[0m : 2.37641
[1mStep[0m  [48/84], [94mLoss[0m : 2.46032
[1mStep[0m  [56/84], [94mLoss[0m : 2.50225
[1mStep[0m  [64/84], [94mLoss[0m : 2.07308
[1mStep[0m  [72/84], [94mLoss[0m : 2.34633
[1mStep[0m  [80/84], [94mLoss[0m : 2.46226

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20953
[1mStep[0m  [8/84], [94mLoss[0m : 2.20846
[1mStep[0m  [16/84], [94mLoss[0m : 2.13111
[1mStep[0m  [24/84], [94mLoss[0m : 2.46315
[1mStep[0m  [32/84], [94mLoss[0m : 2.44877
[1mStep[0m  [40/84], [94mLoss[0m : 2.33217
[1mStep[0m  [48/84], [94mLoss[0m : 2.71332
[1mStep[0m  [56/84], [94mLoss[0m : 2.19696
[1mStep[0m  [64/84], [94mLoss[0m : 2.45092
[1mStep[0m  [72/84], [94mLoss[0m : 2.28720
[1mStep[0m  [80/84], [94mLoss[0m : 2.26832

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08989
[1mStep[0m  [8/84], [94mLoss[0m : 2.04231
[1mStep[0m  [16/84], [94mLoss[0m : 2.20494
[1mStep[0m  [24/84], [94mLoss[0m : 2.32854
[1mStep[0m  [32/84], [94mLoss[0m : 2.30204
[1mStep[0m  [40/84], [94mLoss[0m : 1.93666
[1mStep[0m  [48/84], [94mLoss[0m : 2.37622
[1mStep[0m  [56/84], [94mLoss[0m : 2.04487
[1mStep[0m  [64/84], [94mLoss[0m : 2.55729
[1mStep[0m  [72/84], [94mLoss[0m : 2.48346
[1mStep[0m  [80/84], [94mLoss[0m : 1.88498

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.238, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32475
[1mStep[0m  [8/84], [94mLoss[0m : 2.19966
[1mStep[0m  [16/84], [94mLoss[0m : 2.24150
[1mStep[0m  [24/84], [94mLoss[0m : 2.17981
[1mStep[0m  [32/84], [94mLoss[0m : 2.35516
[1mStep[0m  [40/84], [94mLoss[0m : 2.10899
[1mStep[0m  [48/84], [94mLoss[0m : 2.34620
[1mStep[0m  [56/84], [94mLoss[0m : 2.29301
[1mStep[0m  [64/84], [94mLoss[0m : 2.47008
[1mStep[0m  [72/84], [94mLoss[0m : 2.34521
[1mStep[0m  [80/84], [94mLoss[0m : 2.28052

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92343
[1mStep[0m  [8/84], [94mLoss[0m : 1.93238
[1mStep[0m  [16/84], [94mLoss[0m : 2.13065
[1mStep[0m  [24/84], [94mLoss[0m : 1.93152
[1mStep[0m  [32/84], [94mLoss[0m : 2.18342
[1mStep[0m  [40/84], [94mLoss[0m : 2.16692
[1mStep[0m  [48/84], [94mLoss[0m : 2.43705
[1mStep[0m  [56/84], [94mLoss[0m : 2.07898
[1mStep[0m  [64/84], [94mLoss[0m : 2.01582
[1mStep[0m  [72/84], [94mLoss[0m : 2.15430
[1mStep[0m  [80/84], [94mLoss[0m : 2.14944

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93994
[1mStep[0m  [8/84], [94mLoss[0m : 1.95738
[1mStep[0m  [16/84], [94mLoss[0m : 1.86189
[1mStep[0m  [24/84], [94mLoss[0m : 2.01843
[1mStep[0m  [32/84], [94mLoss[0m : 2.23546
[1mStep[0m  [40/84], [94mLoss[0m : 2.14203
[1mStep[0m  [48/84], [94mLoss[0m : 2.22061
[1mStep[0m  [56/84], [94mLoss[0m : 2.06642
[1mStep[0m  [64/84], [94mLoss[0m : 2.22199
[1mStep[0m  [72/84], [94mLoss[0m : 2.37298
[1mStep[0m  [80/84], [94mLoss[0m : 2.16801

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15911
[1mStep[0m  [8/84], [94mLoss[0m : 2.30003
[1mStep[0m  [16/84], [94mLoss[0m : 2.24569
[1mStep[0m  [24/84], [94mLoss[0m : 1.90393
[1mStep[0m  [32/84], [94mLoss[0m : 2.01779
[1mStep[0m  [40/84], [94mLoss[0m : 2.16239
[1mStep[0m  [48/84], [94mLoss[0m : 2.25940
[1mStep[0m  [56/84], [94mLoss[0m : 2.22604
[1mStep[0m  [64/84], [94mLoss[0m : 2.20223
[1mStep[0m  [72/84], [94mLoss[0m : 2.31547
[1mStep[0m  [80/84], [94mLoss[0m : 1.93412

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02790
[1mStep[0m  [8/84], [94mLoss[0m : 2.11525
[1mStep[0m  [16/84], [94mLoss[0m : 1.79074
[1mStep[0m  [24/84], [94mLoss[0m : 1.89037
[1mStep[0m  [32/84], [94mLoss[0m : 2.07866
[1mStep[0m  [40/84], [94mLoss[0m : 1.93381
[1mStep[0m  [48/84], [94mLoss[0m : 1.75368
[1mStep[0m  [56/84], [94mLoss[0m : 2.05662
[1mStep[0m  [64/84], [94mLoss[0m : 2.22425
[1mStep[0m  [72/84], [94mLoss[0m : 1.88522
[1mStep[0m  [80/84], [94mLoss[0m : 2.13496

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11448
[1mStep[0m  [8/84], [94mLoss[0m : 1.98137
[1mStep[0m  [16/84], [94mLoss[0m : 1.95611
[1mStep[0m  [24/84], [94mLoss[0m : 1.97900
[1mStep[0m  [32/84], [94mLoss[0m : 2.16617
[1mStep[0m  [40/84], [94mLoss[0m : 1.93021
[1mStep[0m  [48/84], [94mLoss[0m : 2.07067
[1mStep[0m  [56/84], [94mLoss[0m : 2.37042
[1mStep[0m  [64/84], [94mLoss[0m : 2.08433
[1mStep[0m  [72/84], [94mLoss[0m : 2.01274
[1mStep[0m  [80/84], [94mLoss[0m : 2.04419

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05834
[1mStep[0m  [8/84], [94mLoss[0m : 2.13015
[1mStep[0m  [16/84], [94mLoss[0m : 1.79788
[1mStep[0m  [24/84], [94mLoss[0m : 1.82505
[1mStep[0m  [32/84], [94mLoss[0m : 1.84515
[1mStep[0m  [40/84], [94mLoss[0m : 2.08463
[1mStep[0m  [48/84], [94mLoss[0m : 2.14339
[1mStep[0m  [56/84], [94mLoss[0m : 1.96404
[1mStep[0m  [64/84], [94mLoss[0m : 2.23179
[1mStep[0m  [72/84], [94mLoss[0m : 1.99200
[1mStep[0m  [80/84], [94mLoss[0m : 2.13088

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17344
[1mStep[0m  [8/84], [94mLoss[0m : 1.73564
[1mStep[0m  [16/84], [94mLoss[0m : 1.71434
[1mStep[0m  [24/84], [94mLoss[0m : 1.94438
[1mStep[0m  [32/84], [94mLoss[0m : 1.76730
[1mStep[0m  [40/84], [94mLoss[0m : 2.01768
[1mStep[0m  [48/84], [94mLoss[0m : 2.02942
[1mStep[0m  [56/84], [94mLoss[0m : 1.94254
[1mStep[0m  [64/84], [94mLoss[0m : 2.31041
[1mStep[0m  [72/84], [94mLoss[0m : 2.12362
[1mStep[0m  [80/84], [94mLoss[0m : 2.12605

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71897
[1mStep[0m  [8/84], [94mLoss[0m : 1.91240
[1mStep[0m  [16/84], [94mLoss[0m : 2.06413
[1mStep[0m  [24/84], [94mLoss[0m : 1.79946
[1mStep[0m  [32/84], [94mLoss[0m : 2.30993
[1mStep[0m  [40/84], [94mLoss[0m : 2.01206
[1mStep[0m  [48/84], [94mLoss[0m : 1.99398
[1mStep[0m  [56/84], [94mLoss[0m : 2.09756
[1mStep[0m  [64/84], [94mLoss[0m : 1.84483
[1mStep[0m  [72/84], [94mLoss[0m : 2.14644
[1mStep[0m  [80/84], [94mLoss[0m : 2.17088

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73596
[1mStep[0m  [8/84], [94mLoss[0m : 1.83616
[1mStep[0m  [16/84], [94mLoss[0m : 2.18454
[1mStep[0m  [24/84], [94mLoss[0m : 2.25610
[1mStep[0m  [32/84], [94mLoss[0m : 1.94260
[1mStep[0m  [40/84], [94mLoss[0m : 2.27135
[1mStep[0m  [48/84], [94mLoss[0m : 1.93539
[1mStep[0m  [56/84], [94mLoss[0m : 2.17363
[1mStep[0m  [64/84], [94mLoss[0m : 1.86165
[1mStep[0m  [72/84], [94mLoss[0m : 1.98653
[1mStep[0m  [80/84], [94mLoss[0m : 2.18656

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79316
[1mStep[0m  [8/84], [94mLoss[0m : 2.02318
[1mStep[0m  [16/84], [94mLoss[0m : 1.75462
[1mStep[0m  [24/84], [94mLoss[0m : 1.79168
[1mStep[0m  [32/84], [94mLoss[0m : 1.70972
[1mStep[0m  [40/84], [94mLoss[0m : 2.22550
[1mStep[0m  [48/84], [94mLoss[0m : 2.14736
[1mStep[0m  [56/84], [94mLoss[0m : 1.86714
[1mStep[0m  [64/84], [94mLoss[0m : 1.85549
[1mStep[0m  [72/84], [94mLoss[0m : 1.75320
[1mStep[0m  [80/84], [94mLoss[0m : 1.88899

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67152
[1mStep[0m  [8/84], [94mLoss[0m : 1.75507
[1mStep[0m  [16/84], [94mLoss[0m : 1.78827
[1mStep[0m  [24/84], [94mLoss[0m : 1.98573
[1mStep[0m  [32/84], [94mLoss[0m : 1.98340
[1mStep[0m  [40/84], [94mLoss[0m : 1.88929
[1mStep[0m  [48/84], [94mLoss[0m : 1.89143
[1mStep[0m  [56/84], [94mLoss[0m : 1.90349
[1mStep[0m  [64/84], [94mLoss[0m : 1.82518
[1mStep[0m  [72/84], [94mLoss[0m : 1.57546
[1mStep[0m  [80/84], [94mLoss[0m : 2.19942

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72214
[1mStep[0m  [8/84], [94mLoss[0m : 1.90256
[1mStep[0m  [16/84], [94mLoss[0m : 2.13073
[1mStep[0m  [24/84], [94mLoss[0m : 1.65967
[1mStep[0m  [32/84], [94mLoss[0m : 1.92110
[1mStep[0m  [40/84], [94mLoss[0m : 1.83889
[1mStep[0m  [48/84], [94mLoss[0m : 2.00854
[1mStep[0m  [56/84], [94mLoss[0m : 2.21749
[1mStep[0m  [64/84], [94mLoss[0m : 1.92996
[1mStep[0m  [72/84], [94mLoss[0m : 2.28017
[1mStep[0m  [80/84], [94mLoss[0m : 1.97601

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.967, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25365
[1mStep[0m  [8/84], [94mLoss[0m : 1.95644
[1mStep[0m  [16/84], [94mLoss[0m : 1.88979
[1mStep[0m  [24/84], [94mLoss[0m : 1.99220
[1mStep[0m  [32/84], [94mLoss[0m : 2.24604
[1mStep[0m  [40/84], [94mLoss[0m : 1.96629
[1mStep[0m  [48/84], [94mLoss[0m : 1.93933
[1mStep[0m  [56/84], [94mLoss[0m : 2.15769
[1mStep[0m  [64/84], [94mLoss[0m : 1.80438
[1mStep[0m  [72/84], [94mLoss[0m : 2.13722
[1mStep[0m  [80/84], [94mLoss[0m : 2.01195

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61987
[1mStep[0m  [8/84], [94mLoss[0m : 1.96684
[1mStep[0m  [16/84], [94mLoss[0m : 1.95599
[1mStep[0m  [24/84], [94mLoss[0m : 1.93167
[1mStep[0m  [32/84], [94mLoss[0m : 1.97272
[1mStep[0m  [40/84], [94mLoss[0m : 1.78293
[1mStep[0m  [48/84], [94mLoss[0m : 2.15053
[1mStep[0m  [56/84], [94mLoss[0m : 1.97658
[1mStep[0m  [64/84], [94mLoss[0m : 1.78384
[1mStep[0m  [72/84], [94mLoss[0m : 1.71184
[1mStep[0m  [80/84], [94mLoss[0m : 1.98011

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71016
[1mStep[0m  [8/84], [94mLoss[0m : 1.78922
[1mStep[0m  [16/84], [94mLoss[0m : 1.88323
[1mStep[0m  [24/84], [94mLoss[0m : 1.99608
[1mStep[0m  [32/84], [94mLoss[0m : 1.76337
[1mStep[0m  [40/84], [94mLoss[0m : 1.98858
[1mStep[0m  [48/84], [94mLoss[0m : 1.83271
[1mStep[0m  [56/84], [94mLoss[0m : 1.82422
[1mStep[0m  [64/84], [94mLoss[0m : 1.49122
[1mStep[0m  [72/84], [94mLoss[0m : 1.90735
[1mStep[0m  [80/84], [94mLoss[0m : 1.90617

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.455, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99823
[1mStep[0m  [8/84], [94mLoss[0m : 2.05811
[1mStep[0m  [16/84], [94mLoss[0m : 2.25623
[1mStep[0m  [24/84], [94mLoss[0m : 2.00784
[1mStep[0m  [32/84], [94mLoss[0m : 1.86065
[1mStep[0m  [40/84], [94mLoss[0m : 1.72483
[1mStep[0m  [48/84], [94mLoss[0m : 2.02429
[1mStep[0m  [56/84], [94mLoss[0m : 2.00666
[1mStep[0m  [64/84], [94mLoss[0m : 1.81242
[1mStep[0m  [72/84], [94mLoss[0m : 2.19292
[1mStep[0m  [80/84], [94mLoss[0m : 1.87718

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94776
[1mStep[0m  [8/84], [94mLoss[0m : 2.04463
[1mStep[0m  [16/84], [94mLoss[0m : 1.82391
[1mStep[0m  [24/84], [94mLoss[0m : 1.53196
[1mStep[0m  [32/84], [94mLoss[0m : 1.80715
[1mStep[0m  [40/84], [94mLoss[0m : 1.74292
[1mStep[0m  [48/84], [94mLoss[0m : 1.85833
[1mStep[0m  [56/84], [94mLoss[0m : 1.85761
[1mStep[0m  [64/84], [94mLoss[0m : 2.07085
[1mStep[0m  [72/84], [94mLoss[0m : 1.85636
[1mStep[0m  [80/84], [94mLoss[0m : 1.93850

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77220
[1mStep[0m  [8/84], [94mLoss[0m : 1.71185
[1mStep[0m  [16/84], [94mLoss[0m : 1.76577
[1mStep[0m  [24/84], [94mLoss[0m : 2.03461
[1mStep[0m  [32/84], [94mLoss[0m : 2.22806
[1mStep[0m  [40/84], [94mLoss[0m : 1.75265
[1mStep[0m  [48/84], [94mLoss[0m : 2.02293
[1mStep[0m  [56/84], [94mLoss[0m : 1.84095
[1mStep[0m  [64/84], [94mLoss[0m : 2.16825
[1mStep[0m  [72/84], [94mLoss[0m : 1.93215
[1mStep[0m  [80/84], [94mLoss[0m : 2.00672

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74171
[1mStep[0m  [8/84], [94mLoss[0m : 1.76730
[1mStep[0m  [16/84], [94mLoss[0m : 1.65555
[1mStep[0m  [24/84], [94mLoss[0m : 1.71216
[1mStep[0m  [32/84], [94mLoss[0m : 1.75208
[1mStep[0m  [40/84], [94mLoss[0m : 1.78375
[1mStep[0m  [48/84], [94mLoss[0m : 1.83198
[1mStep[0m  [56/84], [94mLoss[0m : 1.92185
[1mStep[0m  [64/84], [94mLoss[0m : 1.97994
[1mStep[0m  [72/84], [94mLoss[0m : 1.82611
[1mStep[0m  [80/84], [94mLoss[0m : 2.04446

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.839, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96445
[1mStep[0m  [8/84], [94mLoss[0m : 1.66785
[1mStep[0m  [16/84], [94mLoss[0m : 1.57327
[1mStep[0m  [24/84], [94mLoss[0m : 1.67965
[1mStep[0m  [32/84], [94mLoss[0m : 1.73399
[1mStep[0m  [40/84], [94mLoss[0m : 1.50888
[1mStep[0m  [48/84], [94mLoss[0m : 2.02208
[1mStep[0m  [56/84], [94mLoss[0m : 1.93120
[1mStep[0m  [64/84], [94mLoss[0m : 1.84664
[1mStep[0m  [72/84], [94mLoss[0m : 1.68082
[1mStep[0m  [80/84], [94mLoss[0m : 1.98361

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69781
[1mStep[0m  [8/84], [94mLoss[0m : 1.96039
[1mStep[0m  [16/84], [94mLoss[0m : 1.82708
[1mStep[0m  [24/84], [94mLoss[0m : 1.91325
[1mStep[0m  [32/84], [94mLoss[0m : 1.82920
[1mStep[0m  [40/84], [94mLoss[0m : 1.91158
[1mStep[0m  [48/84], [94mLoss[0m : 1.74658
[1mStep[0m  [56/84], [94mLoss[0m : 1.82581
[1mStep[0m  [64/84], [94mLoss[0m : 1.64223
[1mStep[0m  [72/84], [94mLoss[0m : 1.66762
[1mStep[0m  [80/84], [94mLoss[0m : 2.00727

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.536, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61748
[1mStep[0m  [8/84], [94mLoss[0m : 1.79266
[1mStep[0m  [16/84], [94mLoss[0m : 1.93030
[1mStep[0m  [24/84], [94mLoss[0m : 1.67192
[1mStep[0m  [32/84], [94mLoss[0m : 1.81588
[1mStep[0m  [40/84], [94mLoss[0m : 1.97409
[1mStep[0m  [48/84], [94mLoss[0m : 1.68615
[1mStep[0m  [56/84], [94mLoss[0m : 1.81611
[1mStep[0m  [64/84], [94mLoss[0m : 1.77432
[1mStep[0m  [72/84], [94mLoss[0m : 1.87822
[1mStep[0m  [80/84], [94mLoss[0m : 1.84540

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.804, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60473
[1mStep[0m  [8/84], [94mLoss[0m : 1.58877
[1mStep[0m  [16/84], [94mLoss[0m : 1.84160
[1mStep[0m  [24/84], [94mLoss[0m : 1.76559
[1mStep[0m  [32/84], [94mLoss[0m : 1.98166
[1mStep[0m  [40/84], [94mLoss[0m : 1.97988
[1mStep[0m  [48/84], [94mLoss[0m : 1.92100
[1mStep[0m  [56/84], [94mLoss[0m : 1.76646
[1mStep[0m  [64/84], [94mLoss[0m : 2.12436
[1mStep[0m  [72/84], [94mLoss[0m : 1.99313
[1mStep[0m  [80/84], [94mLoss[0m : 1.84449

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64865
[1mStep[0m  [8/84], [94mLoss[0m : 1.49804
[1mStep[0m  [16/84], [94mLoss[0m : 1.84646
[1mStep[0m  [24/84], [94mLoss[0m : 1.78776
[1mStep[0m  [32/84], [94mLoss[0m : 1.58898
[1mStep[0m  [40/84], [94mLoss[0m : 1.87694
[1mStep[0m  [48/84], [94mLoss[0m : 1.75926
[1mStep[0m  [56/84], [94mLoss[0m : 1.71884
[1mStep[0m  [64/84], [94mLoss[0m : 1.89191
[1mStep[0m  [72/84], [94mLoss[0m : 1.73877
[1mStep[0m  [80/84], [94mLoss[0m : 1.90961

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.777, [92mTest[0m: 2.503, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79263
[1mStep[0m  [8/84], [94mLoss[0m : 1.66008
[1mStep[0m  [16/84], [94mLoss[0m : 1.91837
[1mStep[0m  [24/84], [94mLoss[0m : 1.45279
[1mStep[0m  [32/84], [94mLoss[0m : 1.84541
[1mStep[0m  [40/84], [94mLoss[0m : 1.91156
[1mStep[0m  [48/84], [94mLoss[0m : 1.67174
[1mStep[0m  [56/84], [94mLoss[0m : 1.76150
[1mStep[0m  [64/84], [94mLoss[0m : 1.90913
[1mStep[0m  [72/84], [94mLoss[0m : 1.79331
[1mStep[0m  [80/84], [94mLoss[0m : 1.72294

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.510, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.529
====================================

Phase 2 - Evaluation MAE:  2.5287246108055115
MAE score P1      2.349544
MAE score P2      2.528725
loss               1.75279
learning_rate         0.01
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 16, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.64827
[1mStep[0m  [8/84], [94mLoss[0m : 8.32998
[1mStep[0m  [16/84], [94mLoss[0m : 5.01061
[1mStep[0m  [24/84], [94mLoss[0m : 4.18012
[1mStep[0m  [32/84], [94mLoss[0m : 3.14110
[1mStep[0m  [40/84], [94mLoss[0m : 2.99118
[1mStep[0m  [48/84], [94mLoss[0m : 2.61388
[1mStep[0m  [56/84], [94mLoss[0m : 2.48039
[1mStep[0m  [64/84], [94mLoss[0m : 2.98988
[1mStep[0m  [72/84], [94mLoss[0m : 2.92021
[1mStep[0m  [80/84], [94mLoss[0m : 2.92579

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.180, [92mTest[0m: 10.835, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55454
[1mStep[0m  [8/84], [94mLoss[0m : 2.84708
[1mStep[0m  [16/84], [94mLoss[0m : 2.68457
[1mStep[0m  [24/84], [94mLoss[0m : 2.97441
[1mStep[0m  [32/84], [94mLoss[0m : 2.62435
[1mStep[0m  [40/84], [94mLoss[0m : 2.42673
[1mStep[0m  [48/84], [94mLoss[0m : 2.79266
[1mStep[0m  [56/84], [94mLoss[0m : 3.02033
[1mStep[0m  [64/84], [94mLoss[0m : 3.23688
[1mStep[0m  [72/84], [94mLoss[0m : 2.51414
[1mStep[0m  [80/84], [94mLoss[0m : 2.65676

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.730, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85031
[1mStep[0m  [8/84], [94mLoss[0m : 2.70067
[1mStep[0m  [16/84], [94mLoss[0m : 2.64658
[1mStep[0m  [24/84], [94mLoss[0m : 2.69007
[1mStep[0m  [32/84], [94mLoss[0m : 2.89495
[1mStep[0m  [40/84], [94mLoss[0m : 2.37717
[1mStep[0m  [48/84], [94mLoss[0m : 2.52805
[1mStep[0m  [56/84], [94mLoss[0m : 3.03754
[1mStep[0m  [64/84], [94mLoss[0m : 2.68198
[1mStep[0m  [72/84], [94mLoss[0m : 2.49307
[1mStep[0m  [80/84], [94mLoss[0m : 2.25800

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56266
[1mStep[0m  [8/84], [94mLoss[0m : 2.50500
[1mStep[0m  [16/84], [94mLoss[0m : 2.42199
[1mStep[0m  [24/84], [94mLoss[0m : 2.71155
[1mStep[0m  [32/84], [94mLoss[0m : 2.56968
[1mStep[0m  [40/84], [94mLoss[0m : 2.90545
[1mStep[0m  [48/84], [94mLoss[0m : 2.74502
[1mStep[0m  [56/84], [94mLoss[0m : 2.74834
[1mStep[0m  [64/84], [94mLoss[0m : 2.79205
[1mStep[0m  [72/84], [94mLoss[0m : 2.47115
[1mStep[0m  [80/84], [94mLoss[0m : 2.81662

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62753
[1mStep[0m  [8/84], [94mLoss[0m : 2.76234
[1mStep[0m  [16/84], [94mLoss[0m : 2.62055
[1mStep[0m  [24/84], [94mLoss[0m : 2.62468
[1mStep[0m  [32/84], [94mLoss[0m : 2.59707
[1mStep[0m  [40/84], [94mLoss[0m : 2.71576
[1mStep[0m  [48/84], [94mLoss[0m : 2.52390
[1mStep[0m  [56/84], [94mLoss[0m : 2.58621
[1mStep[0m  [64/84], [94mLoss[0m : 2.66338
[1mStep[0m  [72/84], [94mLoss[0m : 2.61421
[1mStep[0m  [80/84], [94mLoss[0m : 2.63353

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73123
[1mStep[0m  [8/84], [94mLoss[0m : 2.54412
[1mStep[0m  [16/84], [94mLoss[0m : 2.26059
[1mStep[0m  [24/84], [94mLoss[0m : 2.55675
[1mStep[0m  [32/84], [94mLoss[0m : 2.68067
[1mStep[0m  [40/84], [94mLoss[0m : 2.55089
[1mStep[0m  [48/84], [94mLoss[0m : 2.34989
[1mStep[0m  [56/84], [94mLoss[0m : 2.59738
[1mStep[0m  [64/84], [94mLoss[0m : 2.79628
[1mStep[0m  [72/84], [94mLoss[0m : 2.25697
[1mStep[0m  [80/84], [94mLoss[0m : 2.76578

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71057
[1mStep[0m  [8/84], [94mLoss[0m : 2.45246
[1mStep[0m  [16/84], [94mLoss[0m : 2.32331
[1mStep[0m  [24/84], [94mLoss[0m : 2.31886
[1mStep[0m  [32/84], [94mLoss[0m : 2.55156
[1mStep[0m  [40/84], [94mLoss[0m : 2.67174
[1mStep[0m  [48/84], [94mLoss[0m : 2.68628
[1mStep[0m  [56/84], [94mLoss[0m : 2.46427
[1mStep[0m  [64/84], [94mLoss[0m : 2.67435
[1mStep[0m  [72/84], [94mLoss[0m : 2.67985
[1mStep[0m  [80/84], [94mLoss[0m : 2.81727

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61079
[1mStep[0m  [8/84], [94mLoss[0m : 2.50011
[1mStep[0m  [16/84], [94mLoss[0m : 2.40035
[1mStep[0m  [24/84], [94mLoss[0m : 2.63254
[1mStep[0m  [32/84], [94mLoss[0m : 2.63011
[1mStep[0m  [40/84], [94mLoss[0m : 2.39139
[1mStep[0m  [48/84], [94mLoss[0m : 2.27531
[1mStep[0m  [56/84], [94mLoss[0m : 2.50144
[1mStep[0m  [64/84], [94mLoss[0m : 2.92094
[1mStep[0m  [72/84], [94mLoss[0m : 2.41007
[1mStep[0m  [80/84], [94mLoss[0m : 2.56785

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32791
[1mStep[0m  [8/84], [94mLoss[0m : 2.46673
[1mStep[0m  [16/84], [94mLoss[0m : 2.68657
[1mStep[0m  [24/84], [94mLoss[0m : 2.63856
[1mStep[0m  [32/84], [94mLoss[0m : 2.35303
[1mStep[0m  [40/84], [94mLoss[0m : 2.67667
[1mStep[0m  [48/84], [94mLoss[0m : 2.77227
[1mStep[0m  [56/84], [94mLoss[0m : 2.78531
[1mStep[0m  [64/84], [94mLoss[0m : 2.51551
[1mStep[0m  [72/84], [94mLoss[0m : 2.29625
[1mStep[0m  [80/84], [94mLoss[0m : 2.69271

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.315, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27604
[1mStep[0m  [8/84], [94mLoss[0m : 2.68564
[1mStep[0m  [16/84], [94mLoss[0m : 2.16208
[1mStep[0m  [24/84], [94mLoss[0m : 2.63628
[1mStep[0m  [32/84], [94mLoss[0m : 2.53224
[1mStep[0m  [40/84], [94mLoss[0m : 2.50468
[1mStep[0m  [48/84], [94mLoss[0m : 2.48068
[1mStep[0m  [56/84], [94mLoss[0m : 2.60747
[1mStep[0m  [64/84], [94mLoss[0m : 2.64461
[1mStep[0m  [72/84], [94mLoss[0m : 2.41566
[1mStep[0m  [80/84], [94mLoss[0m : 2.67562

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21343
[1mStep[0m  [8/84], [94mLoss[0m : 2.68556
[1mStep[0m  [16/84], [94mLoss[0m : 2.36803
[1mStep[0m  [24/84], [94mLoss[0m : 2.19573
[1mStep[0m  [32/84], [94mLoss[0m : 2.88867
[1mStep[0m  [40/84], [94mLoss[0m : 2.73935
[1mStep[0m  [48/84], [94mLoss[0m : 2.48070
[1mStep[0m  [56/84], [94mLoss[0m : 2.65184
[1mStep[0m  [64/84], [94mLoss[0m : 2.56202
[1mStep[0m  [72/84], [94mLoss[0m : 2.55025
[1mStep[0m  [80/84], [94mLoss[0m : 2.42040

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50835
[1mStep[0m  [8/84], [94mLoss[0m : 2.47581
[1mStep[0m  [16/84], [94mLoss[0m : 2.73155
[1mStep[0m  [24/84], [94mLoss[0m : 2.37617
[1mStep[0m  [32/84], [94mLoss[0m : 2.51477
[1mStep[0m  [40/84], [94mLoss[0m : 2.53275
[1mStep[0m  [48/84], [94mLoss[0m : 2.30217
[1mStep[0m  [56/84], [94mLoss[0m : 2.57122
[1mStep[0m  [64/84], [94mLoss[0m : 2.67568
[1mStep[0m  [72/84], [94mLoss[0m : 2.38035
[1mStep[0m  [80/84], [94mLoss[0m : 2.49205

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.315, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20011
[1mStep[0m  [8/84], [94mLoss[0m : 2.55707
[1mStep[0m  [16/84], [94mLoss[0m : 2.53821
[1mStep[0m  [24/84], [94mLoss[0m : 2.63700
[1mStep[0m  [32/84], [94mLoss[0m : 2.03893
[1mStep[0m  [40/84], [94mLoss[0m : 2.81256
[1mStep[0m  [48/84], [94mLoss[0m : 2.21697
[1mStep[0m  [56/84], [94mLoss[0m : 2.64484
[1mStep[0m  [64/84], [94mLoss[0m : 2.74952
[1mStep[0m  [72/84], [94mLoss[0m : 2.43567
[1mStep[0m  [80/84], [94mLoss[0m : 2.44293

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65298
[1mStep[0m  [8/84], [94mLoss[0m : 2.48949
[1mStep[0m  [16/84], [94mLoss[0m : 2.43275
[1mStep[0m  [24/84], [94mLoss[0m : 2.42670
[1mStep[0m  [32/84], [94mLoss[0m : 2.49777
[1mStep[0m  [40/84], [94mLoss[0m : 2.40772
[1mStep[0m  [48/84], [94mLoss[0m : 2.74478
[1mStep[0m  [56/84], [94mLoss[0m : 2.44793
[1mStep[0m  [64/84], [94mLoss[0m : 2.38014
[1mStep[0m  [72/84], [94mLoss[0m : 2.34453
[1mStep[0m  [80/84], [94mLoss[0m : 2.64349

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68743
[1mStep[0m  [8/84], [94mLoss[0m : 2.55712
[1mStep[0m  [16/84], [94mLoss[0m : 2.22565
[1mStep[0m  [24/84], [94mLoss[0m : 2.57717
[1mStep[0m  [32/84], [94mLoss[0m : 2.49287
[1mStep[0m  [40/84], [94mLoss[0m : 2.19700
[1mStep[0m  [48/84], [94mLoss[0m : 2.34426
[1mStep[0m  [56/84], [94mLoss[0m : 2.27175
[1mStep[0m  [64/84], [94mLoss[0m : 2.36142
[1mStep[0m  [72/84], [94mLoss[0m : 2.53176
[1mStep[0m  [80/84], [94mLoss[0m : 2.16337

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.309, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03914
[1mStep[0m  [8/84], [94mLoss[0m : 2.48802
[1mStep[0m  [16/84], [94mLoss[0m : 2.58070
[1mStep[0m  [24/84], [94mLoss[0m : 2.49485
[1mStep[0m  [32/84], [94mLoss[0m : 2.54125
[1mStep[0m  [40/84], [94mLoss[0m : 2.61800
[1mStep[0m  [48/84], [94mLoss[0m : 2.51835
[1mStep[0m  [56/84], [94mLoss[0m : 1.95667
[1mStep[0m  [64/84], [94mLoss[0m : 2.56981
[1mStep[0m  [72/84], [94mLoss[0m : 2.56069
[1mStep[0m  [80/84], [94mLoss[0m : 2.18426

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28817
[1mStep[0m  [8/84], [94mLoss[0m : 2.30775
[1mStep[0m  [16/84], [94mLoss[0m : 2.50830
[1mStep[0m  [24/84], [94mLoss[0m : 2.68747
[1mStep[0m  [32/84], [94mLoss[0m : 2.76632
[1mStep[0m  [40/84], [94mLoss[0m : 2.37639
[1mStep[0m  [48/84], [94mLoss[0m : 2.47461
[1mStep[0m  [56/84], [94mLoss[0m : 2.40887
[1mStep[0m  [64/84], [94mLoss[0m : 2.22273
[1mStep[0m  [72/84], [94mLoss[0m : 2.45265
[1mStep[0m  [80/84], [94mLoss[0m : 2.51664

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.305, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41666
[1mStep[0m  [8/84], [94mLoss[0m : 2.50598
[1mStep[0m  [16/84], [94mLoss[0m : 2.63433
[1mStep[0m  [24/84], [94mLoss[0m : 2.34157
[1mStep[0m  [32/84], [94mLoss[0m : 2.59666
[1mStep[0m  [40/84], [94mLoss[0m : 2.50783
[1mStep[0m  [48/84], [94mLoss[0m : 2.53394
[1mStep[0m  [56/84], [94mLoss[0m : 2.64143
[1mStep[0m  [64/84], [94mLoss[0m : 2.26137
[1mStep[0m  [72/84], [94mLoss[0m : 2.68371
[1mStep[0m  [80/84], [94mLoss[0m : 2.44879

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38366
[1mStep[0m  [8/84], [94mLoss[0m : 2.22034
[1mStep[0m  [16/84], [94mLoss[0m : 2.51314
[1mStep[0m  [24/84], [94mLoss[0m : 2.38437
[1mStep[0m  [32/84], [94mLoss[0m : 2.19470
[1mStep[0m  [40/84], [94mLoss[0m : 2.64751
[1mStep[0m  [48/84], [94mLoss[0m : 2.49603
[1mStep[0m  [56/84], [94mLoss[0m : 2.41153
[1mStep[0m  [64/84], [94mLoss[0m : 2.34561
[1mStep[0m  [72/84], [94mLoss[0m : 2.54124
[1mStep[0m  [80/84], [94mLoss[0m : 2.89349

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.293, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60247
[1mStep[0m  [8/84], [94mLoss[0m : 2.39796
[1mStep[0m  [16/84], [94mLoss[0m : 2.47762
[1mStep[0m  [24/84], [94mLoss[0m : 2.50005
[1mStep[0m  [32/84], [94mLoss[0m : 2.19836
[1mStep[0m  [40/84], [94mLoss[0m : 2.42456
[1mStep[0m  [48/84], [94mLoss[0m : 2.52536
[1mStep[0m  [56/84], [94mLoss[0m : 2.19706
[1mStep[0m  [64/84], [94mLoss[0m : 2.48873
[1mStep[0m  [72/84], [94mLoss[0m : 2.58557
[1mStep[0m  [80/84], [94mLoss[0m : 2.39769

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.302, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65944
[1mStep[0m  [8/84], [94mLoss[0m : 2.68498
[1mStep[0m  [16/84], [94mLoss[0m : 2.50891
[1mStep[0m  [24/84], [94mLoss[0m : 2.36768
[1mStep[0m  [32/84], [94mLoss[0m : 2.42139
[1mStep[0m  [40/84], [94mLoss[0m : 2.47061
[1mStep[0m  [48/84], [94mLoss[0m : 2.30486
[1mStep[0m  [56/84], [94mLoss[0m : 2.48151
[1mStep[0m  [64/84], [94mLoss[0m : 2.51615
[1mStep[0m  [72/84], [94mLoss[0m : 2.64393
[1mStep[0m  [80/84], [94mLoss[0m : 2.48661

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.304, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37796
[1mStep[0m  [8/84], [94mLoss[0m : 2.59903
[1mStep[0m  [16/84], [94mLoss[0m : 2.22574
[1mStep[0m  [24/84], [94mLoss[0m : 2.51718
[1mStep[0m  [32/84], [94mLoss[0m : 2.46409
[1mStep[0m  [40/84], [94mLoss[0m : 2.57564
[1mStep[0m  [48/84], [94mLoss[0m : 2.64788
[1mStep[0m  [56/84], [94mLoss[0m : 2.69797
[1mStep[0m  [64/84], [94mLoss[0m : 2.52336
[1mStep[0m  [72/84], [94mLoss[0m : 2.68318
[1mStep[0m  [80/84], [94mLoss[0m : 2.13099

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.295, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40727
[1mStep[0m  [8/84], [94mLoss[0m : 2.40495
[1mStep[0m  [16/84], [94mLoss[0m : 2.72169
[1mStep[0m  [24/84], [94mLoss[0m : 2.33028
[1mStep[0m  [32/84], [94mLoss[0m : 2.42965
[1mStep[0m  [40/84], [94mLoss[0m : 2.20807
[1mStep[0m  [48/84], [94mLoss[0m : 2.48723
[1mStep[0m  [56/84], [94mLoss[0m : 2.47414
[1mStep[0m  [64/84], [94mLoss[0m : 2.84061
[1mStep[0m  [72/84], [94mLoss[0m : 2.74308
[1mStep[0m  [80/84], [94mLoss[0m : 2.47892

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.305, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36013
[1mStep[0m  [8/84], [94mLoss[0m : 2.56898
[1mStep[0m  [16/84], [94mLoss[0m : 2.65876
[1mStep[0m  [24/84], [94mLoss[0m : 2.09872
[1mStep[0m  [32/84], [94mLoss[0m : 2.45437
[1mStep[0m  [40/84], [94mLoss[0m : 2.35941
[1mStep[0m  [48/84], [94mLoss[0m : 2.52990
[1mStep[0m  [56/84], [94mLoss[0m : 2.56324
[1mStep[0m  [64/84], [94mLoss[0m : 2.41233
[1mStep[0m  [72/84], [94mLoss[0m : 2.35749
[1mStep[0m  [80/84], [94mLoss[0m : 2.41882

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.298, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42610
[1mStep[0m  [8/84], [94mLoss[0m : 2.38017
[1mStep[0m  [16/84], [94mLoss[0m : 2.34875
[1mStep[0m  [24/84], [94mLoss[0m : 2.65195
[1mStep[0m  [32/84], [94mLoss[0m : 2.54472
[1mStep[0m  [40/84], [94mLoss[0m : 2.22390
[1mStep[0m  [48/84], [94mLoss[0m : 2.26296
[1mStep[0m  [56/84], [94mLoss[0m : 2.57090
[1mStep[0m  [64/84], [94mLoss[0m : 2.64306
[1mStep[0m  [72/84], [94mLoss[0m : 2.40712
[1mStep[0m  [80/84], [94mLoss[0m : 2.56411

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.298, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44650
[1mStep[0m  [8/84], [94mLoss[0m : 2.23810
[1mStep[0m  [16/84], [94mLoss[0m : 2.44433
[1mStep[0m  [24/84], [94mLoss[0m : 2.80995
[1mStep[0m  [32/84], [94mLoss[0m : 2.56869
[1mStep[0m  [40/84], [94mLoss[0m : 2.61788
[1mStep[0m  [48/84], [94mLoss[0m : 2.25635
[1mStep[0m  [56/84], [94mLoss[0m : 2.13357
[1mStep[0m  [64/84], [94mLoss[0m : 2.54295
[1mStep[0m  [72/84], [94mLoss[0m : 2.30035
[1mStep[0m  [80/84], [94mLoss[0m : 2.42735

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.275, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51180
[1mStep[0m  [8/84], [94mLoss[0m : 2.47812
[1mStep[0m  [16/84], [94mLoss[0m : 2.55648
[1mStep[0m  [24/84], [94mLoss[0m : 2.39225
[1mStep[0m  [32/84], [94mLoss[0m : 2.26865
[1mStep[0m  [40/84], [94mLoss[0m : 2.35902
[1mStep[0m  [48/84], [94mLoss[0m : 2.20316
[1mStep[0m  [56/84], [94mLoss[0m : 2.30489
[1mStep[0m  [64/84], [94mLoss[0m : 2.24407
[1mStep[0m  [72/84], [94mLoss[0m : 2.35184
[1mStep[0m  [80/84], [94mLoss[0m : 2.46173

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.291, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84874
[1mStep[0m  [8/84], [94mLoss[0m : 2.46605
[1mStep[0m  [16/84], [94mLoss[0m : 2.46985
[1mStep[0m  [24/84], [94mLoss[0m : 2.17985
[1mStep[0m  [32/84], [94mLoss[0m : 2.32550
[1mStep[0m  [40/84], [94mLoss[0m : 2.39219
[1mStep[0m  [48/84], [94mLoss[0m : 2.27926
[1mStep[0m  [56/84], [94mLoss[0m : 2.46205
[1mStep[0m  [64/84], [94mLoss[0m : 2.51843
[1mStep[0m  [72/84], [94mLoss[0m : 2.58230
[1mStep[0m  [80/84], [94mLoss[0m : 2.29914

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.292, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31542
[1mStep[0m  [8/84], [94mLoss[0m : 2.22521
[1mStep[0m  [16/84], [94mLoss[0m : 2.31311
[1mStep[0m  [24/84], [94mLoss[0m : 2.57507
[1mStep[0m  [32/84], [94mLoss[0m : 2.30954
[1mStep[0m  [40/84], [94mLoss[0m : 2.19189
[1mStep[0m  [48/84], [94mLoss[0m : 2.07772
[1mStep[0m  [56/84], [94mLoss[0m : 2.55692
[1mStep[0m  [64/84], [94mLoss[0m : 2.62158
[1mStep[0m  [72/84], [94mLoss[0m : 2.47260
[1mStep[0m  [80/84], [94mLoss[0m : 2.29406

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.291, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40305
[1mStep[0m  [8/84], [94mLoss[0m : 2.11996
[1mStep[0m  [16/84], [94mLoss[0m : 2.55383
[1mStep[0m  [24/84], [94mLoss[0m : 2.37690
[1mStep[0m  [32/84], [94mLoss[0m : 2.42421
[1mStep[0m  [40/84], [94mLoss[0m : 2.52799
[1mStep[0m  [48/84], [94mLoss[0m : 2.36313
[1mStep[0m  [56/84], [94mLoss[0m : 2.54896
[1mStep[0m  [64/84], [94mLoss[0m : 2.42127
[1mStep[0m  [72/84], [94mLoss[0m : 2.24147
[1mStep[0m  [80/84], [94mLoss[0m : 2.47112

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.288, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.287
====================================

Phase 1 - Evaluation MAE:  2.286899353776659
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.61133
[1mStep[0m  [8/84], [94mLoss[0m : 2.45343
[1mStep[0m  [16/84], [94mLoss[0m : 2.49954
[1mStep[0m  [24/84], [94mLoss[0m : 2.66101
[1mStep[0m  [32/84], [94mLoss[0m : 2.14630
[1mStep[0m  [40/84], [94mLoss[0m : 2.56533
[1mStep[0m  [48/84], [94mLoss[0m : 2.66149
[1mStep[0m  [56/84], [94mLoss[0m : 2.70651
[1mStep[0m  [64/84], [94mLoss[0m : 2.61770
[1mStep[0m  [72/84], [94mLoss[0m : 2.57382
[1mStep[0m  [80/84], [94mLoss[0m : 2.54815

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.283, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38489
[1mStep[0m  [8/84], [94mLoss[0m : 2.25072
[1mStep[0m  [16/84], [94mLoss[0m : 2.44525
[1mStep[0m  [24/84], [94mLoss[0m : 2.42583
[1mStep[0m  [32/84], [94mLoss[0m : 2.31849
[1mStep[0m  [40/84], [94mLoss[0m : 2.16724
[1mStep[0m  [48/84], [94mLoss[0m : 2.48039
[1mStep[0m  [56/84], [94mLoss[0m : 2.22518
[1mStep[0m  [64/84], [94mLoss[0m : 2.27033
[1mStep[0m  [72/84], [94mLoss[0m : 2.20579
[1mStep[0m  [80/84], [94mLoss[0m : 2.64742

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61873
[1mStep[0m  [8/84], [94mLoss[0m : 2.17015
[1mStep[0m  [16/84], [94mLoss[0m : 2.14829
[1mStep[0m  [24/84], [94mLoss[0m : 2.39585
[1mStep[0m  [32/84], [94mLoss[0m : 2.59966
[1mStep[0m  [40/84], [94mLoss[0m : 2.37538
[1mStep[0m  [48/84], [94mLoss[0m : 2.49979
[1mStep[0m  [56/84], [94mLoss[0m : 2.63785
[1mStep[0m  [64/84], [94mLoss[0m : 2.17226
[1mStep[0m  [72/84], [94mLoss[0m : 2.12438
[1mStep[0m  [80/84], [94mLoss[0m : 2.16298

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42433
[1mStep[0m  [8/84], [94mLoss[0m : 2.28985
[1mStep[0m  [16/84], [94mLoss[0m : 2.26046
[1mStep[0m  [24/84], [94mLoss[0m : 2.38999
[1mStep[0m  [32/84], [94mLoss[0m : 1.91876
[1mStep[0m  [40/84], [94mLoss[0m : 2.51626
[1mStep[0m  [48/84], [94mLoss[0m : 2.49222
[1mStep[0m  [56/84], [94mLoss[0m : 2.27174
[1mStep[0m  [64/84], [94mLoss[0m : 2.46067
[1mStep[0m  [72/84], [94mLoss[0m : 2.02703
[1mStep[0m  [80/84], [94mLoss[0m : 2.29501

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23213
[1mStep[0m  [8/84], [94mLoss[0m : 2.15515
[1mStep[0m  [16/84], [94mLoss[0m : 2.30533
[1mStep[0m  [24/84], [94mLoss[0m : 2.27650
[1mStep[0m  [32/84], [94mLoss[0m : 2.05449
[1mStep[0m  [40/84], [94mLoss[0m : 2.06466
[1mStep[0m  [48/84], [94mLoss[0m : 2.22771
[1mStep[0m  [56/84], [94mLoss[0m : 2.35246
[1mStep[0m  [64/84], [94mLoss[0m : 2.11857
[1mStep[0m  [72/84], [94mLoss[0m : 2.14396
[1mStep[0m  [80/84], [94mLoss[0m : 2.44527

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.186, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88119
[1mStep[0m  [8/84], [94mLoss[0m : 1.93906
[1mStep[0m  [16/84], [94mLoss[0m : 2.27032
[1mStep[0m  [24/84], [94mLoss[0m : 2.12584
[1mStep[0m  [32/84], [94mLoss[0m : 2.14958
[1mStep[0m  [40/84], [94mLoss[0m : 2.19912
[1mStep[0m  [48/84], [94mLoss[0m : 2.26614
[1mStep[0m  [56/84], [94mLoss[0m : 1.95966
[1mStep[0m  [64/84], [94mLoss[0m : 2.10514
[1mStep[0m  [72/84], [94mLoss[0m : 2.01961
[1mStep[0m  [80/84], [94mLoss[0m : 2.28219

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28840
[1mStep[0m  [8/84], [94mLoss[0m : 2.01655
[1mStep[0m  [16/84], [94mLoss[0m : 1.83100
[1mStep[0m  [24/84], [94mLoss[0m : 2.12062
[1mStep[0m  [32/84], [94mLoss[0m : 1.87530
[1mStep[0m  [40/84], [94mLoss[0m : 2.14868
[1mStep[0m  [48/84], [94mLoss[0m : 2.11992
[1mStep[0m  [56/84], [94mLoss[0m : 2.24018
[1mStep[0m  [64/84], [94mLoss[0m : 1.93325
[1mStep[0m  [72/84], [94mLoss[0m : 2.12082
[1mStep[0m  [80/84], [94mLoss[0m : 2.12749

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91305
[1mStep[0m  [8/84], [94mLoss[0m : 2.07544
[1mStep[0m  [16/84], [94mLoss[0m : 2.12527
[1mStep[0m  [24/84], [94mLoss[0m : 1.70618
[1mStep[0m  [32/84], [94mLoss[0m : 2.14500
[1mStep[0m  [40/84], [94mLoss[0m : 2.14104
[1mStep[0m  [48/84], [94mLoss[0m : 1.99384
[1mStep[0m  [56/84], [94mLoss[0m : 2.00763
[1mStep[0m  [64/84], [94mLoss[0m : 2.21560
[1mStep[0m  [72/84], [94mLoss[0m : 1.78426
[1mStep[0m  [80/84], [94mLoss[0m : 2.03152

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06322
[1mStep[0m  [8/84], [94mLoss[0m : 1.75958
[1mStep[0m  [16/84], [94mLoss[0m : 1.66745
[1mStep[0m  [24/84], [94mLoss[0m : 2.03771
[1mStep[0m  [32/84], [94mLoss[0m : 1.95130
[1mStep[0m  [40/84], [94mLoss[0m : 2.16072
[1mStep[0m  [48/84], [94mLoss[0m : 2.02612
[1mStep[0m  [56/84], [94mLoss[0m : 1.96299
[1mStep[0m  [64/84], [94mLoss[0m : 1.89144
[1mStep[0m  [72/84], [94mLoss[0m : 1.92545
[1mStep[0m  [80/84], [94mLoss[0m : 2.17829

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83750
[1mStep[0m  [8/84], [94mLoss[0m : 1.80489
[1mStep[0m  [16/84], [94mLoss[0m : 1.98781
[1mStep[0m  [24/84], [94mLoss[0m : 2.15850
[1mStep[0m  [32/84], [94mLoss[0m : 1.73826
[1mStep[0m  [40/84], [94mLoss[0m : 2.00092
[1mStep[0m  [48/84], [94mLoss[0m : 1.90444
[1mStep[0m  [56/84], [94mLoss[0m : 1.94449
[1mStep[0m  [64/84], [94mLoss[0m : 2.11804
[1mStep[0m  [72/84], [94mLoss[0m : 2.26527
[1mStep[0m  [80/84], [94mLoss[0m : 2.08665

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73636
[1mStep[0m  [8/84], [94mLoss[0m : 1.92756
[1mStep[0m  [16/84], [94mLoss[0m : 1.74038
[1mStep[0m  [24/84], [94mLoss[0m : 1.99272
[1mStep[0m  [32/84], [94mLoss[0m : 1.80371
[1mStep[0m  [40/84], [94mLoss[0m : 1.78828
[1mStep[0m  [48/84], [94mLoss[0m : 1.88808
[1mStep[0m  [56/84], [94mLoss[0m : 1.86469
[1mStep[0m  [64/84], [94mLoss[0m : 1.83888
[1mStep[0m  [72/84], [94mLoss[0m : 2.01314
[1mStep[0m  [80/84], [94mLoss[0m : 1.75671

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64880
[1mStep[0m  [8/84], [94mLoss[0m : 1.66887
[1mStep[0m  [16/84], [94mLoss[0m : 1.89252
[1mStep[0m  [24/84], [94mLoss[0m : 1.81477
[1mStep[0m  [32/84], [94mLoss[0m : 1.92718
[1mStep[0m  [40/84], [94mLoss[0m : 1.61418
[1mStep[0m  [48/84], [94mLoss[0m : 1.95885
[1mStep[0m  [56/84], [94mLoss[0m : 1.79958
[1mStep[0m  [64/84], [94mLoss[0m : 1.74748
[1mStep[0m  [72/84], [94mLoss[0m : 1.77751
[1mStep[0m  [80/84], [94mLoss[0m : 1.74104

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84832
[1mStep[0m  [8/84], [94mLoss[0m : 1.58702
[1mStep[0m  [16/84], [94mLoss[0m : 1.75242
[1mStep[0m  [24/84], [94mLoss[0m : 1.90284
[1mStep[0m  [32/84], [94mLoss[0m : 1.83479
[1mStep[0m  [40/84], [94mLoss[0m : 1.49451
[1mStep[0m  [48/84], [94mLoss[0m : 1.82824
[1mStep[0m  [56/84], [94mLoss[0m : 1.97983
[1mStep[0m  [64/84], [94mLoss[0m : 1.67302
[1mStep[0m  [72/84], [94mLoss[0m : 2.09092
[1mStep[0m  [80/84], [94mLoss[0m : 2.04963

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63958
[1mStep[0m  [8/84], [94mLoss[0m : 1.51252
[1mStep[0m  [16/84], [94mLoss[0m : 1.54206
[1mStep[0m  [24/84], [94mLoss[0m : 2.09097
[1mStep[0m  [32/84], [94mLoss[0m : 1.58527
[1mStep[0m  [40/84], [94mLoss[0m : 1.86680
[1mStep[0m  [48/84], [94mLoss[0m : 2.04919
[1mStep[0m  [56/84], [94mLoss[0m : 1.66072
[1mStep[0m  [64/84], [94mLoss[0m : 1.51074
[1mStep[0m  [72/84], [94mLoss[0m : 2.01425
[1mStep[0m  [80/84], [94mLoss[0m : 1.74705

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98474
[1mStep[0m  [8/84], [94mLoss[0m : 1.79395
[1mStep[0m  [16/84], [94mLoss[0m : 1.64995
[1mStep[0m  [24/84], [94mLoss[0m : 1.98707
[1mStep[0m  [32/84], [94mLoss[0m : 1.55397
[1mStep[0m  [40/84], [94mLoss[0m : 1.90484
[1mStep[0m  [48/84], [94mLoss[0m : 1.64583
[1mStep[0m  [56/84], [94mLoss[0m : 1.87205
[1mStep[0m  [64/84], [94mLoss[0m : 1.61103
[1mStep[0m  [72/84], [94mLoss[0m : 1.82541
[1mStep[0m  [80/84], [94mLoss[0m : 1.76585

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47433
[1mStep[0m  [8/84], [94mLoss[0m : 1.45889
[1mStep[0m  [16/84], [94mLoss[0m : 1.76847
[1mStep[0m  [24/84], [94mLoss[0m : 1.74108
[1mStep[0m  [32/84], [94mLoss[0m : 1.72431
[1mStep[0m  [40/84], [94mLoss[0m : 1.71596
[1mStep[0m  [48/84], [94mLoss[0m : 1.52837
[1mStep[0m  [56/84], [94mLoss[0m : 1.48560
[1mStep[0m  [64/84], [94mLoss[0m : 1.79186
[1mStep[0m  [72/84], [94mLoss[0m : 1.64341
[1mStep[0m  [80/84], [94mLoss[0m : 1.82113

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54053
[1mStep[0m  [8/84], [94mLoss[0m : 1.68471
[1mStep[0m  [16/84], [94mLoss[0m : 1.51562
[1mStep[0m  [24/84], [94mLoss[0m : 1.64460
[1mStep[0m  [32/84], [94mLoss[0m : 1.68491
[1mStep[0m  [40/84], [94mLoss[0m : 1.67745
[1mStep[0m  [48/84], [94mLoss[0m : 1.57403
[1mStep[0m  [56/84], [94mLoss[0m : 1.83465
[1mStep[0m  [64/84], [94mLoss[0m : 1.66883
[1mStep[0m  [72/84], [94mLoss[0m : 1.63488
[1mStep[0m  [80/84], [94mLoss[0m : 1.66078

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.637, [92mTest[0m: 2.513, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44153
[1mStep[0m  [8/84], [94mLoss[0m : 1.53728
[1mStep[0m  [16/84], [94mLoss[0m : 1.52672
[1mStep[0m  [24/84], [94mLoss[0m : 1.82680
[1mStep[0m  [32/84], [94mLoss[0m : 1.60457
[1mStep[0m  [40/84], [94mLoss[0m : 1.70035
[1mStep[0m  [48/84], [94mLoss[0m : 1.69615
[1mStep[0m  [56/84], [94mLoss[0m : 1.56005
[1mStep[0m  [64/84], [94mLoss[0m : 1.46865
[1mStep[0m  [72/84], [94mLoss[0m : 1.43759
[1mStep[0m  [80/84], [94mLoss[0m : 1.47217

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64252
[1mStep[0m  [8/84], [94mLoss[0m : 1.73610
[1mStep[0m  [16/84], [94mLoss[0m : 1.56119
[1mStep[0m  [24/84], [94mLoss[0m : 1.79357
[1mStep[0m  [32/84], [94mLoss[0m : 1.41926
[1mStep[0m  [40/84], [94mLoss[0m : 1.46400
[1mStep[0m  [48/84], [94mLoss[0m : 1.60705
[1mStep[0m  [56/84], [94mLoss[0m : 1.59422
[1mStep[0m  [64/84], [94mLoss[0m : 1.74018
[1mStep[0m  [72/84], [94mLoss[0m : 1.32793
[1mStep[0m  [80/84], [94mLoss[0m : 1.63600

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42490
[1mStep[0m  [8/84], [94mLoss[0m : 1.35268
[1mStep[0m  [16/84], [94mLoss[0m : 1.36502
[1mStep[0m  [24/84], [94mLoss[0m : 1.50208
[1mStep[0m  [32/84], [94mLoss[0m : 1.57231
[1mStep[0m  [40/84], [94mLoss[0m : 1.51351
[1mStep[0m  [48/84], [94mLoss[0m : 1.62752
[1mStep[0m  [56/84], [94mLoss[0m : 1.50812
[1mStep[0m  [64/84], [94mLoss[0m : 1.60129
[1mStep[0m  [72/84], [94mLoss[0m : 1.27886
[1mStep[0m  [80/84], [94mLoss[0m : 1.48456

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67277
[1mStep[0m  [8/84], [94mLoss[0m : 1.60433
[1mStep[0m  [16/84], [94mLoss[0m : 1.38075
[1mStep[0m  [24/84], [94mLoss[0m : 1.73093
[1mStep[0m  [32/84], [94mLoss[0m : 1.41760
[1mStep[0m  [40/84], [94mLoss[0m : 1.44730
[1mStep[0m  [48/84], [94mLoss[0m : 1.41800
[1mStep[0m  [56/84], [94mLoss[0m : 1.28872
[1mStep[0m  [64/84], [94mLoss[0m : 1.39600
[1mStep[0m  [72/84], [94mLoss[0m : 1.41842
[1mStep[0m  [80/84], [94mLoss[0m : 1.44644

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.512, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42918
[1mStep[0m  [8/84], [94mLoss[0m : 1.70623
[1mStep[0m  [16/84], [94mLoss[0m : 1.54561
[1mStep[0m  [24/84], [94mLoss[0m : 1.27931
[1mStep[0m  [32/84], [94mLoss[0m : 1.37526
[1mStep[0m  [40/84], [94mLoss[0m : 1.55651
[1mStep[0m  [48/84], [94mLoss[0m : 1.48734
[1mStep[0m  [56/84], [94mLoss[0m : 1.41729
[1mStep[0m  [64/84], [94mLoss[0m : 1.50749
[1mStep[0m  [72/84], [94mLoss[0m : 1.46488
[1mStep[0m  [80/84], [94mLoss[0m : 1.56513

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.482, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.38120
[1mStep[0m  [8/84], [94mLoss[0m : 1.54501
[1mStep[0m  [16/84], [94mLoss[0m : 1.42838
[1mStep[0m  [24/84], [94mLoss[0m : 1.34766
[1mStep[0m  [32/84], [94mLoss[0m : 1.57813
[1mStep[0m  [40/84], [94mLoss[0m : 1.40651
[1mStep[0m  [48/84], [94mLoss[0m : 1.57909
[1mStep[0m  [56/84], [94mLoss[0m : 1.49066
[1mStep[0m  [64/84], [94mLoss[0m : 1.47544
[1mStep[0m  [72/84], [94mLoss[0m : 1.45311
[1mStep[0m  [80/84], [94mLoss[0m : 1.78283

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.470, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.34505
[1mStep[0m  [8/84], [94mLoss[0m : 1.48587
[1mStep[0m  [16/84], [94mLoss[0m : 1.41337
[1mStep[0m  [24/84], [94mLoss[0m : 1.43121
[1mStep[0m  [32/84], [94mLoss[0m : 1.40343
[1mStep[0m  [40/84], [94mLoss[0m : 1.53075
[1mStep[0m  [48/84], [94mLoss[0m : 1.60887
[1mStep[0m  [56/84], [94mLoss[0m : 1.43056
[1mStep[0m  [64/84], [94mLoss[0m : 1.42781
[1mStep[0m  [72/84], [94mLoss[0m : 1.54347
[1mStep[0m  [80/84], [94mLoss[0m : 1.49680

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.445, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35550
[1mStep[0m  [8/84], [94mLoss[0m : 1.43655
[1mStep[0m  [16/84], [94mLoss[0m : 1.49478
[1mStep[0m  [24/84], [94mLoss[0m : 1.37355
[1mStep[0m  [32/84], [94mLoss[0m : 1.22587
[1mStep[0m  [40/84], [94mLoss[0m : 1.39011
[1mStep[0m  [48/84], [94mLoss[0m : 1.37179
[1mStep[0m  [56/84], [94mLoss[0m : 1.62853
[1mStep[0m  [64/84], [94mLoss[0m : 1.44423
[1mStep[0m  [72/84], [94mLoss[0m : 1.43762
[1mStep[0m  [80/84], [94mLoss[0m : 1.30244

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.420, [92mTest[0m: 2.482, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30253
[1mStep[0m  [8/84], [94mLoss[0m : 1.32394
[1mStep[0m  [16/84], [94mLoss[0m : 1.15283
[1mStep[0m  [24/84], [94mLoss[0m : 1.41708
[1mStep[0m  [32/84], [94mLoss[0m : 1.55835
[1mStep[0m  [40/84], [94mLoss[0m : 1.37633
[1mStep[0m  [48/84], [94mLoss[0m : 1.45311
[1mStep[0m  [56/84], [94mLoss[0m : 1.51980
[1mStep[0m  [64/84], [94mLoss[0m : 1.39535
[1mStep[0m  [72/84], [94mLoss[0m : 1.52398
[1mStep[0m  [80/84], [94mLoss[0m : 1.54178

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.500, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28846
[1mStep[0m  [8/84], [94mLoss[0m : 1.26948
[1mStep[0m  [16/84], [94mLoss[0m : 1.30208
[1mStep[0m  [24/84], [94mLoss[0m : 1.53014
[1mStep[0m  [32/84], [94mLoss[0m : 1.30957
[1mStep[0m  [40/84], [94mLoss[0m : 1.19168
[1mStep[0m  [48/84], [94mLoss[0m : 1.22798
[1mStep[0m  [56/84], [94mLoss[0m : 1.46297
[1mStep[0m  [64/84], [94mLoss[0m : 1.36030
[1mStep[0m  [72/84], [94mLoss[0m : 1.33927
[1mStep[0m  [80/84], [94mLoss[0m : 1.51453

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.385, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.473
====================================

Phase 2 - Evaluation MAE:  2.4733886548451016
MAE score P1      2.286899
MAE score P2      2.473389
loss              1.385434
learning_rate         0.01
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 17, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.95770
[1mStep[0m  [8/84], [94mLoss[0m : 11.06394
[1mStep[0m  [16/84], [94mLoss[0m : 9.98683
[1mStep[0m  [24/84], [94mLoss[0m : 10.24018
[1mStep[0m  [32/84], [94mLoss[0m : 10.24833
[1mStep[0m  [40/84], [94mLoss[0m : 9.76636
[1mStep[0m  [48/84], [94mLoss[0m : 9.18822
[1mStep[0m  [56/84], [94mLoss[0m : 8.51159
[1mStep[0m  [64/84], [94mLoss[0m : 8.58582
[1mStep[0m  [72/84], [94mLoss[0m : 7.90689
[1mStep[0m  [80/84], [94mLoss[0m : 8.30370

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.419, [92mTest[0m: 10.877, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.59099
[1mStep[0m  [8/84], [94mLoss[0m : 7.38123
[1mStep[0m  [16/84], [94mLoss[0m : 7.22262
[1mStep[0m  [24/84], [94mLoss[0m : 6.60309
[1mStep[0m  [32/84], [94mLoss[0m : 6.26455
[1mStep[0m  [40/84], [94mLoss[0m : 6.18201
[1mStep[0m  [48/84], [94mLoss[0m : 5.55120
[1mStep[0m  [56/84], [94mLoss[0m : 5.57802
[1mStep[0m  [64/84], [94mLoss[0m : 4.83360
[1mStep[0m  [72/84], [94mLoss[0m : 4.39498
[1mStep[0m  [80/84], [94mLoss[0m : 5.01756

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.038, [92mTest[0m: 7.151, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.15474
[1mStep[0m  [8/84], [94mLoss[0m : 4.02779
[1mStep[0m  [16/84], [94mLoss[0m : 3.36653
[1mStep[0m  [24/84], [94mLoss[0m : 3.04434
[1mStep[0m  [32/84], [94mLoss[0m : 2.97220
[1mStep[0m  [40/84], [94mLoss[0m : 2.84328
[1mStep[0m  [48/84], [94mLoss[0m : 2.75656
[1mStep[0m  [56/84], [94mLoss[0m : 2.60995
[1mStep[0m  [64/84], [94mLoss[0m : 3.15486
[1mStep[0m  [72/84], [94mLoss[0m : 2.92397
[1mStep[0m  [80/84], [94mLoss[0m : 2.54115

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.055, [92mTest[0m: 3.253, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63520
[1mStep[0m  [8/84], [94mLoss[0m : 2.74538
[1mStep[0m  [16/84], [94mLoss[0m : 2.48857
[1mStep[0m  [24/84], [94mLoss[0m : 3.01484
[1mStep[0m  [32/84], [94mLoss[0m : 2.65898
[1mStep[0m  [40/84], [94mLoss[0m : 2.75857
[1mStep[0m  [48/84], [94mLoss[0m : 3.03301
[1mStep[0m  [56/84], [94mLoss[0m : 2.62135
[1mStep[0m  [64/84], [94mLoss[0m : 2.84046
[1mStep[0m  [72/84], [94mLoss[0m : 2.73419
[1mStep[0m  [80/84], [94mLoss[0m : 2.87595

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48460
[1mStep[0m  [8/84], [94mLoss[0m : 2.85300
[1mStep[0m  [16/84], [94mLoss[0m : 2.56626
[1mStep[0m  [24/84], [94mLoss[0m : 2.25701
[1mStep[0m  [32/84], [94mLoss[0m : 2.50440
[1mStep[0m  [40/84], [94mLoss[0m : 2.25294
[1mStep[0m  [48/84], [94mLoss[0m : 2.10213
[1mStep[0m  [56/84], [94mLoss[0m : 2.49514
[1mStep[0m  [64/84], [94mLoss[0m : 2.14256
[1mStep[0m  [72/84], [94mLoss[0m : 2.36076
[1mStep[0m  [80/84], [94mLoss[0m : 2.29407

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50772
[1mStep[0m  [8/84], [94mLoss[0m : 2.51991
[1mStep[0m  [16/84], [94mLoss[0m : 2.20470
[1mStep[0m  [24/84], [94mLoss[0m : 2.58131
[1mStep[0m  [32/84], [94mLoss[0m : 2.42043
[1mStep[0m  [40/84], [94mLoss[0m : 2.33041
[1mStep[0m  [48/84], [94mLoss[0m : 2.60435
[1mStep[0m  [56/84], [94mLoss[0m : 2.31894
[1mStep[0m  [64/84], [94mLoss[0m : 2.75128
[1mStep[0m  [72/84], [94mLoss[0m : 2.84388
[1mStep[0m  [80/84], [94mLoss[0m : 2.73319

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53501
[1mStep[0m  [8/84], [94mLoss[0m : 2.58979
[1mStep[0m  [16/84], [94mLoss[0m : 2.34276
[1mStep[0m  [24/84], [94mLoss[0m : 2.29350
[1mStep[0m  [32/84], [94mLoss[0m : 2.54997
[1mStep[0m  [40/84], [94mLoss[0m : 2.42134
[1mStep[0m  [48/84], [94mLoss[0m : 2.53729
[1mStep[0m  [56/84], [94mLoss[0m : 2.97289
[1mStep[0m  [64/84], [94mLoss[0m : 2.33801
[1mStep[0m  [72/84], [94mLoss[0m : 2.80519
[1mStep[0m  [80/84], [94mLoss[0m : 2.53399

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73884
[1mStep[0m  [8/84], [94mLoss[0m : 2.30906
[1mStep[0m  [16/84], [94mLoss[0m : 2.53233
[1mStep[0m  [24/84], [94mLoss[0m : 2.51249
[1mStep[0m  [32/84], [94mLoss[0m : 2.51001
[1mStep[0m  [40/84], [94mLoss[0m : 2.67682
[1mStep[0m  [48/84], [94mLoss[0m : 2.47374
[1mStep[0m  [56/84], [94mLoss[0m : 2.68145
[1mStep[0m  [64/84], [94mLoss[0m : 2.37124
[1mStep[0m  [72/84], [94mLoss[0m : 2.53710
[1mStep[0m  [80/84], [94mLoss[0m : 2.20686

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87195
[1mStep[0m  [8/84], [94mLoss[0m : 2.27979
[1mStep[0m  [16/84], [94mLoss[0m : 2.50509
[1mStep[0m  [24/84], [94mLoss[0m : 2.40117
[1mStep[0m  [32/84], [94mLoss[0m : 2.52925
[1mStep[0m  [40/84], [94mLoss[0m : 2.48252
[1mStep[0m  [48/84], [94mLoss[0m : 2.80008
[1mStep[0m  [56/84], [94mLoss[0m : 2.61998
[1mStep[0m  [64/84], [94mLoss[0m : 2.36925
[1mStep[0m  [72/84], [94mLoss[0m : 2.45950
[1mStep[0m  [80/84], [94mLoss[0m : 2.44599

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41126
[1mStep[0m  [8/84], [94mLoss[0m : 2.60224
[1mStep[0m  [16/84], [94mLoss[0m : 2.38084
[1mStep[0m  [24/84], [94mLoss[0m : 2.56625
[1mStep[0m  [32/84], [94mLoss[0m : 2.50592
[1mStep[0m  [40/84], [94mLoss[0m : 2.31446
[1mStep[0m  [48/84], [94mLoss[0m : 2.42020
[1mStep[0m  [56/84], [94mLoss[0m : 2.68557
[1mStep[0m  [64/84], [94mLoss[0m : 2.56808
[1mStep[0m  [72/84], [94mLoss[0m : 2.72556
[1mStep[0m  [80/84], [94mLoss[0m : 2.26707

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45321
[1mStep[0m  [8/84], [94mLoss[0m : 2.37258
[1mStep[0m  [16/84], [94mLoss[0m : 2.35662
[1mStep[0m  [24/84], [94mLoss[0m : 2.69051
[1mStep[0m  [32/84], [94mLoss[0m : 2.45469
[1mStep[0m  [40/84], [94mLoss[0m : 2.55780
[1mStep[0m  [48/84], [94mLoss[0m : 2.49650
[1mStep[0m  [56/84], [94mLoss[0m : 2.42111
[1mStep[0m  [64/84], [94mLoss[0m : 2.14605
[1mStep[0m  [72/84], [94mLoss[0m : 2.23572
[1mStep[0m  [80/84], [94mLoss[0m : 2.34849

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39840
[1mStep[0m  [8/84], [94mLoss[0m : 2.34906
[1mStep[0m  [16/84], [94mLoss[0m : 2.37394
[1mStep[0m  [24/84], [94mLoss[0m : 2.47205
[1mStep[0m  [32/84], [94mLoss[0m : 2.36421
[1mStep[0m  [40/84], [94mLoss[0m : 2.40083
[1mStep[0m  [48/84], [94mLoss[0m : 2.35413
[1mStep[0m  [56/84], [94mLoss[0m : 2.21113
[1mStep[0m  [64/84], [94mLoss[0m : 2.46712
[1mStep[0m  [72/84], [94mLoss[0m : 2.55915
[1mStep[0m  [80/84], [94mLoss[0m : 2.49111

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32984
[1mStep[0m  [8/84], [94mLoss[0m : 2.33675
[1mStep[0m  [16/84], [94mLoss[0m : 2.26976
[1mStep[0m  [24/84], [94mLoss[0m : 2.27878
[1mStep[0m  [32/84], [94mLoss[0m : 2.14394
[1mStep[0m  [40/84], [94mLoss[0m : 2.62267
[1mStep[0m  [48/84], [94mLoss[0m : 2.24070
[1mStep[0m  [56/84], [94mLoss[0m : 2.60524
[1mStep[0m  [64/84], [94mLoss[0m : 2.57539
[1mStep[0m  [72/84], [94mLoss[0m : 2.28369
[1mStep[0m  [80/84], [94mLoss[0m : 2.18096

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40379
[1mStep[0m  [8/84], [94mLoss[0m : 2.73967
[1mStep[0m  [16/84], [94mLoss[0m : 2.15272
[1mStep[0m  [24/84], [94mLoss[0m : 2.45674
[1mStep[0m  [32/84], [94mLoss[0m : 2.62146
[1mStep[0m  [40/84], [94mLoss[0m : 2.46151
[1mStep[0m  [48/84], [94mLoss[0m : 2.71021
[1mStep[0m  [56/84], [94mLoss[0m : 2.43822
[1mStep[0m  [64/84], [94mLoss[0m : 2.22596
[1mStep[0m  [72/84], [94mLoss[0m : 2.25084
[1mStep[0m  [80/84], [94mLoss[0m : 2.51327

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49683
[1mStep[0m  [8/84], [94mLoss[0m : 2.48086
[1mStep[0m  [16/84], [94mLoss[0m : 2.31944
[1mStep[0m  [24/84], [94mLoss[0m : 2.26434
[1mStep[0m  [32/84], [94mLoss[0m : 2.46261
[1mStep[0m  [40/84], [94mLoss[0m : 2.51280
[1mStep[0m  [48/84], [94mLoss[0m : 2.23200
[1mStep[0m  [56/84], [94mLoss[0m : 2.62783
[1mStep[0m  [64/84], [94mLoss[0m : 2.41263
[1mStep[0m  [72/84], [94mLoss[0m : 2.72162
[1mStep[0m  [80/84], [94mLoss[0m : 2.28507

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30493
[1mStep[0m  [8/84], [94mLoss[0m : 2.36694
[1mStep[0m  [16/84], [94mLoss[0m : 2.67680
[1mStep[0m  [24/84], [94mLoss[0m : 2.41174
[1mStep[0m  [32/84], [94mLoss[0m : 2.66833
[1mStep[0m  [40/84], [94mLoss[0m : 2.31255
[1mStep[0m  [48/84], [94mLoss[0m : 2.26404
[1mStep[0m  [56/84], [94mLoss[0m : 2.41534
[1mStep[0m  [64/84], [94mLoss[0m : 2.28419
[1mStep[0m  [72/84], [94mLoss[0m : 2.39138
[1mStep[0m  [80/84], [94mLoss[0m : 2.38487

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62050
[1mStep[0m  [8/84], [94mLoss[0m : 2.45663
[1mStep[0m  [16/84], [94mLoss[0m : 2.45684
[1mStep[0m  [24/84], [94mLoss[0m : 2.58189
[1mStep[0m  [32/84], [94mLoss[0m : 2.53798
[1mStep[0m  [40/84], [94mLoss[0m : 2.29886
[1mStep[0m  [48/84], [94mLoss[0m : 2.46713
[1mStep[0m  [56/84], [94mLoss[0m : 2.53165
[1mStep[0m  [64/84], [94mLoss[0m : 2.39473
[1mStep[0m  [72/84], [94mLoss[0m : 2.56876
[1mStep[0m  [80/84], [94mLoss[0m : 2.47598

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50610
[1mStep[0m  [8/84], [94mLoss[0m : 2.26866
[1mStep[0m  [16/84], [94mLoss[0m : 2.47377
[1mStep[0m  [24/84], [94mLoss[0m : 2.19282
[1mStep[0m  [32/84], [94mLoss[0m : 2.23259
[1mStep[0m  [40/84], [94mLoss[0m : 2.40093
[1mStep[0m  [48/84], [94mLoss[0m : 2.60491
[1mStep[0m  [56/84], [94mLoss[0m : 2.49739
[1mStep[0m  [64/84], [94mLoss[0m : 2.77307
[1mStep[0m  [72/84], [94mLoss[0m : 2.58900
[1mStep[0m  [80/84], [94mLoss[0m : 2.67243

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53253
[1mStep[0m  [8/84], [94mLoss[0m : 2.31626
[1mStep[0m  [16/84], [94mLoss[0m : 2.34550
[1mStep[0m  [24/84], [94mLoss[0m : 2.39420
[1mStep[0m  [32/84], [94mLoss[0m : 2.41896
[1mStep[0m  [40/84], [94mLoss[0m : 2.49142
[1mStep[0m  [48/84], [94mLoss[0m : 2.41394
[1mStep[0m  [56/84], [94mLoss[0m : 2.40125
[1mStep[0m  [64/84], [94mLoss[0m : 2.42134
[1mStep[0m  [72/84], [94mLoss[0m : 2.17285
[1mStep[0m  [80/84], [94mLoss[0m : 2.34597

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19769
[1mStep[0m  [8/84], [94mLoss[0m : 2.78616
[1mStep[0m  [16/84], [94mLoss[0m : 2.36523
[1mStep[0m  [24/84], [94mLoss[0m : 2.55260
[1mStep[0m  [32/84], [94mLoss[0m : 2.29610
[1mStep[0m  [40/84], [94mLoss[0m : 2.53964
[1mStep[0m  [48/84], [94mLoss[0m : 2.45088
[1mStep[0m  [56/84], [94mLoss[0m : 2.17064
[1mStep[0m  [64/84], [94mLoss[0m : 2.61769
[1mStep[0m  [72/84], [94mLoss[0m : 2.49396
[1mStep[0m  [80/84], [94mLoss[0m : 2.23014

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.369, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34097
[1mStep[0m  [8/84], [94mLoss[0m : 2.39548
[1mStep[0m  [16/84], [94mLoss[0m : 2.53839
[1mStep[0m  [24/84], [94mLoss[0m : 2.50981
[1mStep[0m  [32/84], [94mLoss[0m : 2.53328
[1mStep[0m  [40/84], [94mLoss[0m : 2.27016
[1mStep[0m  [48/84], [94mLoss[0m : 2.40239
[1mStep[0m  [56/84], [94mLoss[0m : 2.33668
[1mStep[0m  [64/84], [94mLoss[0m : 2.58561
[1mStep[0m  [72/84], [94mLoss[0m : 2.49198
[1mStep[0m  [80/84], [94mLoss[0m : 2.40014

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43338
[1mStep[0m  [8/84], [94mLoss[0m : 2.34677
[1mStep[0m  [16/84], [94mLoss[0m : 2.45147
[1mStep[0m  [24/84], [94mLoss[0m : 2.54882
[1mStep[0m  [32/84], [94mLoss[0m : 2.68842
[1mStep[0m  [40/84], [94mLoss[0m : 2.59001
[1mStep[0m  [48/84], [94mLoss[0m : 2.33429
[1mStep[0m  [56/84], [94mLoss[0m : 2.48035
[1mStep[0m  [64/84], [94mLoss[0m : 2.47102
[1mStep[0m  [72/84], [94mLoss[0m : 2.47986
[1mStep[0m  [80/84], [94mLoss[0m : 2.39094

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.13415
[1mStep[0m  [8/84], [94mLoss[0m : 2.68642
[1mStep[0m  [16/84], [94mLoss[0m : 2.54268
[1mStep[0m  [24/84], [94mLoss[0m : 2.30011
[1mStep[0m  [32/84], [94mLoss[0m : 2.37822
[1mStep[0m  [40/84], [94mLoss[0m : 2.49609
[1mStep[0m  [48/84], [94mLoss[0m : 2.52980
[1mStep[0m  [56/84], [94mLoss[0m : 2.16316
[1mStep[0m  [64/84], [94mLoss[0m : 2.65892
[1mStep[0m  [72/84], [94mLoss[0m : 2.36112
[1mStep[0m  [80/84], [94mLoss[0m : 2.22488

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11276
[1mStep[0m  [8/84], [94mLoss[0m : 2.34345
[1mStep[0m  [16/84], [94mLoss[0m : 2.36881
[1mStep[0m  [24/84], [94mLoss[0m : 2.54737
[1mStep[0m  [32/84], [94mLoss[0m : 2.21208
[1mStep[0m  [40/84], [94mLoss[0m : 2.52352
[1mStep[0m  [48/84], [94mLoss[0m : 2.71760
[1mStep[0m  [56/84], [94mLoss[0m : 2.39972
[1mStep[0m  [64/84], [94mLoss[0m : 2.34822
[1mStep[0m  [72/84], [94mLoss[0m : 2.45283
[1mStep[0m  [80/84], [94mLoss[0m : 2.17020

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61476
[1mStep[0m  [8/84], [94mLoss[0m : 2.17282
[1mStep[0m  [16/84], [94mLoss[0m : 2.53477
[1mStep[0m  [24/84], [94mLoss[0m : 2.23773
[1mStep[0m  [32/84], [94mLoss[0m : 2.19315
[1mStep[0m  [40/84], [94mLoss[0m : 2.67621
[1mStep[0m  [48/84], [94mLoss[0m : 2.41896
[1mStep[0m  [56/84], [94mLoss[0m : 2.17011
[1mStep[0m  [64/84], [94mLoss[0m : 2.23958
[1mStep[0m  [72/84], [94mLoss[0m : 2.45428
[1mStep[0m  [80/84], [94mLoss[0m : 2.27812

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50614
[1mStep[0m  [8/84], [94mLoss[0m : 2.30748
[1mStep[0m  [16/84], [94mLoss[0m : 2.33694
[1mStep[0m  [24/84], [94mLoss[0m : 2.58970
[1mStep[0m  [32/84], [94mLoss[0m : 2.68610
[1mStep[0m  [40/84], [94mLoss[0m : 2.36639
[1mStep[0m  [48/84], [94mLoss[0m : 2.42616
[1mStep[0m  [56/84], [94mLoss[0m : 2.56643
[1mStep[0m  [64/84], [94mLoss[0m : 2.34091
[1mStep[0m  [72/84], [94mLoss[0m : 2.25088
[1mStep[0m  [80/84], [94mLoss[0m : 2.21104

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41794
[1mStep[0m  [8/84], [94mLoss[0m : 2.63062
[1mStep[0m  [16/84], [94mLoss[0m : 2.63187
[1mStep[0m  [24/84], [94mLoss[0m : 2.08797
[1mStep[0m  [32/84], [94mLoss[0m : 2.59431
[1mStep[0m  [40/84], [94mLoss[0m : 2.23113
[1mStep[0m  [48/84], [94mLoss[0m : 2.45796
[1mStep[0m  [56/84], [94mLoss[0m : 2.19895
[1mStep[0m  [64/84], [94mLoss[0m : 2.17910
[1mStep[0m  [72/84], [94mLoss[0m : 2.26765
[1mStep[0m  [80/84], [94mLoss[0m : 2.51878

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03369
[1mStep[0m  [8/84], [94mLoss[0m : 2.35720
[1mStep[0m  [16/84], [94mLoss[0m : 2.51150
[1mStep[0m  [24/84], [94mLoss[0m : 2.46046
[1mStep[0m  [32/84], [94mLoss[0m : 2.51595
[1mStep[0m  [40/84], [94mLoss[0m : 2.36349
[1mStep[0m  [48/84], [94mLoss[0m : 2.38023
[1mStep[0m  [56/84], [94mLoss[0m : 2.27060
[1mStep[0m  [64/84], [94mLoss[0m : 2.38238
[1mStep[0m  [72/84], [94mLoss[0m : 2.38233
[1mStep[0m  [80/84], [94mLoss[0m : 2.40703

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30787
[1mStep[0m  [8/84], [94mLoss[0m : 2.10699
[1mStep[0m  [16/84], [94mLoss[0m : 2.75241
[1mStep[0m  [24/84], [94mLoss[0m : 2.57060
[1mStep[0m  [32/84], [94mLoss[0m : 2.76554
[1mStep[0m  [40/84], [94mLoss[0m : 2.24460
[1mStep[0m  [48/84], [94mLoss[0m : 2.48559
[1mStep[0m  [56/84], [94mLoss[0m : 2.55712
[1mStep[0m  [64/84], [94mLoss[0m : 2.54808
[1mStep[0m  [72/84], [94mLoss[0m : 2.44198
[1mStep[0m  [80/84], [94mLoss[0m : 2.32251

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17772
[1mStep[0m  [8/84], [94mLoss[0m : 2.00387
[1mStep[0m  [16/84], [94mLoss[0m : 2.29295
[1mStep[0m  [24/84], [94mLoss[0m : 2.35728
[1mStep[0m  [32/84], [94mLoss[0m : 2.54045
[1mStep[0m  [40/84], [94mLoss[0m : 2.62792
[1mStep[0m  [48/84], [94mLoss[0m : 2.18489
[1mStep[0m  [56/84], [94mLoss[0m : 2.75626
[1mStep[0m  [64/84], [94mLoss[0m : 2.41881
[1mStep[0m  [72/84], [94mLoss[0m : 2.26966
[1mStep[0m  [80/84], [94mLoss[0m : 2.28373

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3309909190450395
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.35883
[1mStep[0m  [8/84], [94mLoss[0m : 2.52799
[1mStep[0m  [16/84], [94mLoss[0m : 2.67975
[1mStep[0m  [24/84], [94mLoss[0m : 2.32245
[1mStep[0m  [32/84], [94mLoss[0m : 2.77631
[1mStep[0m  [40/84], [94mLoss[0m : 2.52007
[1mStep[0m  [48/84], [94mLoss[0m : 2.16160
[1mStep[0m  [56/84], [94mLoss[0m : 2.09998
[1mStep[0m  [64/84], [94mLoss[0m : 2.38374
[1mStep[0m  [72/84], [94mLoss[0m : 2.94182
[1mStep[0m  [80/84], [94mLoss[0m : 2.05635

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24178
[1mStep[0m  [8/84], [94mLoss[0m : 2.50215
[1mStep[0m  [16/84], [94mLoss[0m : 2.10708
[1mStep[0m  [24/84], [94mLoss[0m : 2.44313
[1mStep[0m  [32/84], [94mLoss[0m : 2.44957
[1mStep[0m  [40/84], [94mLoss[0m : 2.16051
[1mStep[0m  [48/84], [94mLoss[0m : 2.15927
[1mStep[0m  [56/84], [94mLoss[0m : 2.27299
[1mStep[0m  [64/84], [94mLoss[0m : 2.19341
[1mStep[0m  [72/84], [94mLoss[0m : 2.39073
[1mStep[0m  [80/84], [94mLoss[0m : 2.30211

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21335
[1mStep[0m  [8/84], [94mLoss[0m : 2.40422
[1mStep[0m  [16/84], [94mLoss[0m : 2.07457
[1mStep[0m  [24/84], [94mLoss[0m : 2.33159
[1mStep[0m  [32/84], [94mLoss[0m : 2.32318
[1mStep[0m  [40/84], [94mLoss[0m : 2.45574
[1mStep[0m  [48/84], [94mLoss[0m : 2.45981
[1mStep[0m  [56/84], [94mLoss[0m : 2.09336
[1mStep[0m  [64/84], [94mLoss[0m : 2.27207
[1mStep[0m  [72/84], [94mLoss[0m : 2.15667
[1mStep[0m  [80/84], [94mLoss[0m : 2.15550

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82059
[1mStep[0m  [8/84], [94mLoss[0m : 2.31870
[1mStep[0m  [16/84], [94mLoss[0m : 2.26399
[1mStep[0m  [24/84], [94mLoss[0m : 2.46953
[1mStep[0m  [32/84], [94mLoss[0m : 2.30650
[1mStep[0m  [40/84], [94mLoss[0m : 2.48455
[1mStep[0m  [48/84], [94mLoss[0m : 2.09104
[1mStep[0m  [56/84], [94mLoss[0m : 2.30697
[1mStep[0m  [64/84], [94mLoss[0m : 2.38098
[1mStep[0m  [72/84], [94mLoss[0m : 2.18510
[1mStep[0m  [80/84], [94mLoss[0m : 2.41682

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85764
[1mStep[0m  [8/84], [94mLoss[0m : 2.28902
[1mStep[0m  [16/84], [94mLoss[0m : 2.02931
[1mStep[0m  [24/84], [94mLoss[0m : 2.51186
[1mStep[0m  [32/84], [94mLoss[0m : 2.27587
[1mStep[0m  [40/84], [94mLoss[0m : 1.96402
[1mStep[0m  [48/84], [94mLoss[0m : 2.04320
[1mStep[0m  [56/84], [94mLoss[0m : 2.18004
[1mStep[0m  [64/84], [94mLoss[0m : 1.80810
[1mStep[0m  [72/84], [94mLoss[0m : 2.10725
[1mStep[0m  [80/84], [94mLoss[0m : 2.07733

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17648
[1mStep[0m  [8/84], [94mLoss[0m : 2.11577
[1mStep[0m  [16/84], [94mLoss[0m : 1.84207
[1mStep[0m  [24/84], [94mLoss[0m : 1.96535
[1mStep[0m  [32/84], [94mLoss[0m : 2.07383
[1mStep[0m  [40/84], [94mLoss[0m : 2.28322
[1mStep[0m  [48/84], [94mLoss[0m : 2.03651
[1mStep[0m  [56/84], [94mLoss[0m : 2.12811
[1mStep[0m  [64/84], [94mLoss[0m : 2.00326
[1mStep[0m  [72/84], [94mLoss[0m : 2.08197
[1mStep[0m  [80/84], [94mLoss[0m : 2.44195

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.059, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06012
[1mStep[0m  [8/84], [94mLoss[0m : 1.64341
[1mStep[0m  [16/84], [94mLoss[0m : 2.05435
[1mStep[0m  [24/84], [94mLoss[0m : 2.03863
[1mStep[0m  [32/84], [94mLoss[0m : 1.90006
[1mStep[0m  [40/84], [94mLoss[0m : 1.63335
[1mStep[0m  [48/84], [94mLoss[0m : 2.18429
[1mStep[0m  [56/84], [94mLoss[0m : 2.16924
[1mStep[0m  [64/84], [94mLoss[0m : 2.08198
[1mStep[0m  [72/84], [94mLoss[0m : 2.17649
[1mStep[0m  [80/84], [94mLoss[0m : 2.16471

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94913
[1mStep[0m  [8/84], [94mLoss[0m : 1.88524
[1mStep[0m  [16/84], [94mLoss[0m : 1.95461
[1mStep[0m  [24/84], [94mLoss[0m : 1.77945
[1mStep[0m  [32/84], [94mLoss[0m : 1.68554
[1mStep[0m  [40/84], [94mLoss[0m : 1.80899
[1mStep[0m  [48/84], [94mLoss[0m : 2.07845
[1mStep[0m  [56/84], [94mLoss[0m : 1.54350
[1mStep[0m  [64/84], [94mLoss[0m : 2.04547
[1mStep[0m  [72/84], [94mLoss[0m : 1.86896
[1mStep[0m  [80/84], [94mLoss[0m : 1.97996

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76648
[1mStep[0m  [8/84], [94mLoss[0m : 2.19241
[1mStep[0m  [16/84], [94mLoss[0m : 1.80170
[1mStep[0m  [24/84], [94mLoss[0m : 2.02499
[1mStep[0m  [32/84], [94mLoss[0m : 1.81358
[1mStep[0m  [40/84], [94mLoss[0m : 1.97894
[1mStep[0m  [48/84], [94mLoss[0m : 1.68207
[1mStep[0m  [56/84], [94mLoss[0m : 2.00379
[1mStep[0m  [64/84], [94mLoss[0m : 2.03995
[1mStep[0m  [72/84], [94mLoss[0m : 1.83965
[1mStep[0m  [80/84], [94mLoss[0m : 2.03161

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80949
[1mStep[0m  [8/84], [94mLoss[0m : 1.77526
[1mStep[0m  [16/84], [94mLoss[0m : 1.75128
[1mStep[0m  [24/84], [94mLoss[0m : 1.93490
[1mStep[0m  [32/84], [94mLoss[0m : 1.78149
[1mStep[0m  [40/84], [94mLoss[0m : 1.91606
[1mStep[0m  [48/84], [94mLoss[0m : 1.86529
[1mStep[0m  [56/84], [94mLoss[0m : 1.89750
[1mStep[0m  [64/84], [94mLoss[0m : 1.67493
[1mStep[0m  [72/84], [94mLoss[0m : 1.83346
[1mStep[0m  [80/84], [94mLoss[0m : 1.91624

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.839, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63707
[1mStep[0m  [8/84], [94mLoss[0m : 1.56511
[1mStep[0m  [16/84], [94mLoss[0m : 1.90159
[1mStep[0m  [24/84], [94mLoss[0m : 1.67478
[1mStep[0m  [32/84], [94mLoss[0m : 1.75478
[1mStep[0m  [40/84], [94mLoss[0m : 1.91957
[1mStep[0m  [48/84], [94mLoss[0m : 1.50476
[1mStep[0m  [56/84], [94mLoss[0m : 2.00000
[1mStep[0m  [64/84], [94mLoss[0m : 2.02983
[1mStep[0m  [72/84], [94mLoss[0m : 1.75541
[1mStep[0m  [80/84], [94mLoss[0m : 1.75033

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81042
[1mStep[0m  [8/84], [94mLoss[0m : 1.59326
[1mStep[0m  [16/84], [94mLoss[0m : 1.69068
[1mStep[0m  [24/84], [94mLoss[0m : 1.75697
[1mStep[0m  [32/84], [94mLoss[0m : 1.91876
[1mStep[0m  [40/84], [94mLoss[0m : 1.83729
[1mStep[0m  [48/84], [94mLoss[0m : 1.69842
[1mStep[0m  [56/84], [94mLoss[0m : 2.01634
[1mStep[0m  [64/84], [94mLoss[0m : 1.68532
[1mStep[0m  [72/84], [94mLoss[0m : 1.83332
[1mStep[0m  [80/84], [94mLoss[0m : 1.82645

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55568
[1mStep[0m  [8/84], [94mLoss[0m : 1.62428
[1mStep[0m  [16/84], [94mLoss[0m : 1.43362
[1mStep[0m  [24/84], [94mLoss[0m : 1.42807
[1mStep[0m  [32/84], [94mLoss[0m : 1.81836
[1mStep[0m  [40/84], [94mLoss[0m : 1.66176
[1mStep[0m  [48/84], [94mLoss[0m : 1.77716
[1mStep[0m  [56/84], [94mLoss[0m : 1.60718
[1mStep[0m  [64/84], [94mLoss[0m : 1.64754
[1mStep[0m  [72/84], [94mLoss[0m : 1.68381
[1mStep[0m  [80/84], [94mLoss[0m : 1.88780

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66546
[1mStep[0m  [8/84], [94mLoss[0m : 1.42034
[1mStep[0m  [16/84], [94mLoss[0m : 1.57571
[1mStep[0m  [24/84], [94mLoss[0m : 1.72253
[1mStep[0m  [32/84], [94mLoss[0m : 1.86417
[1mStep[0m  [40/84], [94mLoss[0m : 1.73569
[1mStep[0m  [48/84], [94mLoss[0m : 1.87218
[1mStep[0m  [56/84], [94mLoss[0m : 1.77502
[1mStep[0m  [64/84], [94mLoss[0m : 1.55133
[1mStep[0m  [72/84], [94mLoss[0m : 1.80546
[1mStep[0m  [80/84], [94mLoss[0m : 1.66569

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.686, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39470
[1mStep[0m  [8/84], [94mLoss[0m : 1.70695
[1mStep[0m  [16/84], [94mLoss[0m : 1.59095
[1mStep[0m  [24/84], [94mLoss[0m : 1.64142
[1mStep[0m  [32/84], [94mLoss[0m : 1.76286
[1mStep[0m  [40/84], [94mLoss[0m : 1.45164
[1mStep[0m  [48/84], [94mLoss[0m : 1.73159
[1mStep[0m  [56/84], [94mLoss[0m : 2.01879
[1mStep[0m  [64/84], [94mLoss[0m : 1.73131
[1mStep[0m  [72/84], [94mLoss[0m : 1.78547
[1mStep[0m  [80/84], [94mLoss[0m : 1.91879

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.29004
[1mStep[0m  [8/84], [94mLoss[0m : 1.62737
[1mStep[0m  [16/84], [94mLoss[0m : 1.64777
[1mStep[0m  [24/84], [94mLoss[0m : 2.00308
[1mStep[0m  [32/84], [94mLoss[0m : 1.61509
[1mStep[0m  [40/84], [94mLoss[0m : 1.45834
[1mStep[0m  [48/84], [94mLoss[0m : 1.61132
[1mStep[0m  [56/84], [94mLoss[0m : 1.77547
[1mStep[0m  [64/84], [94mLoss[0m : 1.57499
[1mStep[0m  [72/84], [94mLoss[0m : 1.67840
[1mStep[0m  [80/84], [94mLoss[0m : 1.54222

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64669
[1mStep[0m  [8/84], [94mLoss[0m : 1.67696
[1mStep[0m  [16/84], [94mLoss[0m : 1.64105
[1mStep[0m  [24/84], [94mLoss[0m : 1.44358
[1mStep[0m  [32/84], [94mLoss[0m : 1.67140
[1mStep[0m  [40/84], [94mLoss[0m : 1.63864
[1mStep[0m  [48/84], [94mLoss[0m : 1.67326
[1mStep[0m  [56/84], [94mLoss[0m : 1.21178
[1mStep[0m  [64/84], [94mLoss[0m : 1.53056
[1mStep[0m  [72/84], [94mLoss[0m : 1.48124
[1mStep[0m  [80/84], [94mLoss[0m : 1.69261

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63387
[1mStep[0m  [8/84], [94mLoss[0m : 1.41674
[1mStep[0m  [16/84], [94mLoss[0m : 1.44159
[1mStep[0m  [24/84], [94mLoss[0m : 1.61665
[1mStep[0m  [32/84], [94mLoss[0m : 1.78840
[1mStep[0m  [40/84], [94mLoss[0m : 1.42417
[1mStep[0m  [48/84], [94mLoss[0m : 1.58752
[1mStep[0m  [56/84], [94mLoss[0m : 1.49425
[1mStep[0m  [64/84], [94mLoss[0m : 1.60535
[1mStep[0m  [72/84], [94mLoss[0m : 1.65674
[1mStep[0m  [80/84], [94mLoss[0m : 1.40818

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.546, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57392
[1mStep[0m  [8/84], [94mLoss[0m : 1.47733
[1mStep[0m  [16/84], [94mLoss[0m : 1.37377
[1mStep[0m  [24/84], [94mLoss[0m : 1.34511
[1mStep[0m  [32/84], [94mLoss[0m : 1.61181
[1mStep[0m  [40/84], [94mLoss[0m : 1.61119
[1mStep[0m  [48/84], [94mLoss[0m : 1.45781
[1mStep[0m  [56/84], [94mLoss[0m : 1.70600
[1mStep[0m  [64/84], [94mLoss[0m : 1.74647
[1mStep[0m  [72/84], [94mLoss[0m : 1.36729
[1mStep[0m  [80/84], [94mLoss[0m : 1.35519

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42337
[1mStep[0m  [8/84], [94mLoss[0m : 1.47515
[1mStep[0m  [16/84], [94mLoss[0m : 1.43903
[1mStep[0m  [24/84], [94mLoss[0m : 1.60751
[1mStep[0m  [32/84], [94mLoss[0m : 1.44910
[1mStep[0m  [40/84], [94mLoss[0m : 1.49786
[1mStep[0m  [48/84], [94mLoss[0m : 1.88431
[1mStep[0m  [56/84], [94mLoss[0m : 1.45783
[1mStep[0m  [64/84], [94mLoss[0m : 1.64839
[1mStep[0m  [72/84], [94mLoss[0m : 1.62831
[1mStep[0m  [80/84], [94mLoss[0m : 1.53570

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35210
[1mStep[0m  [8/84], [94mLoss[0m : 1.45463
[1mStep[0m  [16/84], [94mLoss[0m : 1.24800
[1mStep[0m  [24/84], [94mLoss[0m : 1.33992
[1mStep[0m  [32/84], [94mLoss[0m : 1.35358
[1mStep[0m  [40/84], [94mLoss[0m : 1.33265
[1mStep[0m  [48/84], [94mLoss[0m : 1.44068
[1mStep[0m  [56/84], [94mLoss[0m : 1.27068
[1mStep[0m  [64/84], [94mLoss[0m : 1.34458
[1mStep[0m  [72/84], [94mLoss[0m : 1.38339
[1mStep[0m  [80/84], [94mLoss[0m : 1.69629

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.446, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.22388
[1mStep[0m  [8/84], [94mLoss[0m : 1.36858
[1mStep[0m  [16/84], [94mLoss[0m : 1.49355
[1mStep[0m  [24/84], [94mLoss[0m : 1.26955
[1mStep[0m  [32/84], [94mLoss[0m : 1.29487
[1mStep[0m  [40/84], [94mLoss[0m : 1.62207
[1mStep[0m  [48/84], [94mLoss[0m : 1.50266
[1mStep[0m  [56/84], [94mLoss[0m : 1.50128
[1mStep[0m  [64/84], [94mLoss[0m : 1.46551
[1mStep[0m  [72/84], [94mLoss[0m : 1.55238
[1mStep[0m  [80/84], [94mLoss[0m : 1.52883

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.456, [92mTest[0m: 2.534, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58983
[1mStep[0m  [8/84], [94mLoss[0m : 1.38161
[1mStep[0m  [16/84], [94mLoss[0m : 1.44488
[1mStep[0m  [24/84], [94mLoss[0m : 1.21576
[1mStep[0m  [32/84], [94mLoss[0m : 1.60796
[1mStep[0m  [40/84], [94mLoss[0m : 1.49137
[1mStep[0m  [48/84], [94mLoss[0m : 1.34582
[1mStep[0m  [56/84], [94mLoss[0m : 1.51225
[1mStep[0m  [64/84], [94mLoss[0m : 1.35361
[1mStep[0m  [72/84], [94mLoss[0m : 1.49890
[1mStep[0m  [80/84], [94mLoss[0m : 1.41102

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.436, [92mTest[0m: 2.469, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.30331
[1mStep[0m  [8/84], [94mLoss[0m : 1.22908
[1mStep[0m  [16/84], [94mLoss[0m : 1.32567
[1mStep[0m  [24/84], [94mLoss[0m : 1.42060
[1mStep[0m  [32/84], [94mLoss[0m : 1.40784
[1mStep[0m  [40/84], [94mLoss[0m : 1.46011
[1mStep[0m  [48/84], [94mLoss[0m : 1.32508
[1mStep[0m  [56/84], [94mLoss[0m : 1.18617
[1mStep[0m  [64/84], [94mLoss[0m : 1.42844
[1mStep[0m  [72/84], [94mLoss[0m : 1.40613
[1mStep[0m  [80/84], [94mLoss[0m : 1.56950

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.408, [92mTest[0m: 2.446, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.34823
[1mStep[0m  [8/84], [94mLoss[0m : 1.36918
[1mStep[0m  [16/84], [94mLoss[0m : 1.31653
[1mStep[0m  [24/84], [94mLoss[0m : 1.33079
[1mStep[0m  [32/84], [94mLoss[0m : 1.43980
[1mStep[0m  [40/84], [94mLoss[0m : 1.36452
[1mStep[0m  [48/84], [94mLoss[0m : 1.44759
[1mStep[0m  [56/84], [94mLoss[0m : 1.61345
[1mStep[0m  [64/84], [94mLoss[0m : 1.39054
[1mStep[0m  [72/84], [94mLoss[0m : 1.35018
[1mStep[0m  [80/84], [94mLoss[0m : 1.50168

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.391, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.523
====================================

Phase 2 - Evaluation MAE:  2.5227075474602834
MAE score P1      2.330991
MAE score P2      2.522708
loss              1.391073
learning_rate         0.01
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 18, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.75891
[1mStep[0m  [8/84], [94mLoss[0m : 8.66288
[1mStep[0m  [16/84], [94mLoss[0m : 6.24041
[1mStep[0m  [24/84], [94mLoss[0m : 4.49565
[1mStep[0m  [32/84], [94mLoss[0m : 3.27662
[1mStep[0m  [40/84], [94mLoss[0m : 3.13766
[1mStep[0m  [48/84], [94mLoss[0m : 2.48066
[1mStep[0m  [56/84], [94mLoss[0m : 2.77817
[1mStep[0m  [64/84], [94mLoss[0m : 2.48536
[1mStep[0m  [72/84], [94mLoss[0m : 2.73360
[1mStep[0m  [80/84], [94mLoss[0m : 2.40420

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.133, [92mTest[0m: 10.712, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27272
[1mStep[0m  [8/84], [94mLoss[0m : 2.77771
[1mStep[0m  [16/84], [94mLoss[0m : 2.68746
[1mStep[0m  [24/84], [94mLoss[0m : 2.45184
[1mStep[0m  [32/84], [94mLoss[0m : 2.49025
[1mStep[0m  [40/84], [94mLoss[0m : 2.39957
[1mStep[0m  [48/84], [94mLoss[0m : 2.14388
[1mStep[0m  [56/84], [94mLoss[0m : 2.41164
[1mStep[0m  [64/84], [94mLoss[0m : 2.49941
[1mStep[0m  [72/84], [94mLoss[0m : 2.38401
[1mStep[0m  [80/84], [94mLoss[0m : 2.57926

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35632
[1mStep[0m  [8/84], [94mLoss[0m : 2.62656
[1mStep[0m  [16/84], [94mLoss[0m : 2.33300
[1mStep[0m  [24/84], [94mLoss[0m : 2.65789
[1mStep[0m  [32/84], [94mLoss[0m : 2.79728
[1mStep[0m  [40/84], [94mLoss[0m : 2.55527
[1mStep[0m  [48/84], [94mLoss[0m : 2.17596
[1mStep[0m  [56/84], [94mLoss[0m : 2.48646
[1mStep[0m  [64/84], [94mLoss[0m : 2.39467
[1mStep[0m  [72/84], [94mLoss[0m : 1.98668
[1mStep[0m  [80/84], [94mLoss[0m : 2.76992

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56755
[1mStep[0m  [8/84], [94mLoss[0m : 2.66372
[1mStep[0m  [16/84], [94mLoss[0m : 2.38030
[1mStep[0m  [24/84], [94mLoss[0m : 2.46219
[1mStep[0m  [32/84], [94mLoss[0m : 2.52123
[1mStep[0m  [40/84], [94mLoss[0m : 2.51873
[1mStep[0m  [48/84], [94mLoss[0m : 2.67817
[1mStep[0m  [56/84], [94mLoss[0m : 2.17622
[1mStep[0m  [64/84], [94mLoss[0m : 2.54082
[1mStep[0m  [72/84], [94mLoss[0m : 2.47122
[1mStep[0m  [80/84], [94mLoss[0m : 2.65596

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63693
[1mStep[0m  [8/84], [94mLoss[0m : 2.47620
[1mStep[0m  [16/84], [94mLoss[0m : 2.18604
[1mStep[0m  [24/84], [94mLoss[0m : 2.61431
[1mStep[0m  [32/84], [94mLoss[0m : 2.28460
[1mStep[0m  [40/84], [94mLoss[0m : 2.29154
[1mStep[0m  [48/84], [94mLoss[0m : 2.51337
[1mStep[0m  [56/84], [94mLoss[0m : 2.65771
[1mStep[0m  [64/84], [94mLoss[0m : 2.35205
[1mStep[0m  [72/84], [94mLoss[0m : 2.61427
[1mStep[0m  [80/84], [94mLoss[0m : 2.25214

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23005
[1mStep[0m  [8/84], [94mLoss[0m : 2.47306
[1mStep[0m  [16/84], [94mLoss[0m : 2.72679
[1mStep[0m  [24/84], [94mLoss[0m : 2.22342
[1mStep[0m  [32/84], [94mLoss[0m : 2.73176
[1mStep[0m  [40/84], [94mLoss[0m : 2.64499
[1mStep[0m  [48/84], [94mLoss[0m : 2.22631
[1mStep[0m  [56/84], [94mLoss[0m : 2.20969
[1mStep[0m  [64/84], [94mLoss[0m : 2.52842
[1mStep[0m  [72/84], [94mLoss[0m : 2.63385
[1mStep[0m  [80/84], [94mLoss[0m : 2.29104

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63632
[1mStep[0m  [8/84], [94mLoss[0m : 2.76868
[1mStep[0m  [16/84], [94mLoss[0m : 2.59518
[1mStep[0m  [24/84], [94mLoss[0m : 2.51190
[1mStep[0m  [32/84], [94mLoss[0m : 2.48881
[1mStep[0m  [40/84], [94mLoss[0m : 2.36911
[1mStep[0m  [48/84], [94mLoss[0m : 2.46112
[1mStep[0m  [56/84], [94mLoss[0m : 2.59418
[1mStep[0m  [64/84], [94mLoss[0m : 2.16999
[1mStep[0m  [72/84], [94mLoss[0m : 2.31013
[1mStep[0m  [80/84], [94mLoss[0m : 2.61916

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73036
[1mStep[0m  [8/84], [94mLoss[0m : 2.51997
[1mStep[0m  [16/84], [94mLoss[0m : 2.47188
[1mStep[0m  [24/84], [94mLoss[0m : 2.37911
[1mStep[0m  [32/84], [94mLoss[0m : 2.61427
[1mStep[0m  [40/84], [94mLoss[0m : 2.44718
[1mStep[0m  [48/84], [94mLoss[0m : 2.54111
[1mStep[0m  [56/84], [94mLoss[0m : 2.48868
[1mStep[0m  [64/84], [94mLoss[0m : 2.33292
[1mStep[0m  [72/84], [94mLoss[0m : 2.36811
[1mStep[0m  [80/84], [94mLoss[0m : 2.49729

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37030
[1mStep[0m  [8/84], [94mLoss[0m : 2.59311
[1mStep[0m  [16/84], [94mLoss[0m : 2.28802
[1mStep[0m  [24/84], [94mLoss[0m : 2.43538
[1mStep[0m  [32/84], [94mLoss[0m : 2.19366
[1mStep[0m  [40/84], [94mLoss[0m : 2.41786
[1mStep[0m  [48/84], [94mLoss[0m : 2.54952
[1mStep[0m  [56/84], [94mLoss[0m : 2.33409
[1mStep[0m  [64/84], [94mLoss[0m : 2.58443
[1mStep[0m  [72/84], [94mLoss[0m : 2.75546
[1mStep[0m  [80/84], [94mLoss[0m : 2.45791

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51329
[1mStep[0m  [8/84], [94mLoss[0m : 2.34595
[1mStep[0m  [16/84], [94mLoss[0m : 2.44903
[1mStep[0m  [24/84], [94mLoss[0m : 2.19229
[1mStep[0m  [32/84], [94mLoss[0m : 2.61861
[1mStep[0m  [40/84], [94mLoss[0m : 2.32803
[1mStep[0m  [48/84], [94mLoss[0m : 2.26394
[1mStep[0m  [56/84], [94mLoss[0m : 2.53235
[1mStep[0m  [64/84], [94mLoss[0m : 2.57853
[1mStep[0m  [72/84], [94mLoss[0m : 2.57191
[1mStep[0m  [80/84], [94mLoss[0m : 2.39267

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62729
[1mStep[0m  [8/84], [94mLoss[0m : 2.40388
[1mStep[0m  [16/84], [94mLoss[0m : 2.37282
[1mStep[0m  [24/84], [94mLoss[0m : 2.53882
[1mStep[0m  [32/84], [94mLoss[0m : 2.48240
[1mStep[0m  [40/84], [94mLoss[0m : 2.34936
[1mStep[0m  [48/84], [94mLoss[0m : 2.58485
[1mStep[0m  [56/84], [94mLoss[0m : 2.60601
[1mStep[0m  [64/84], [94mLoss[0m : 2.23028
[1mStep[0m  [72/84], [94mLoss[0m : 2.16874
[1mStep[0m  [80/84], [94mLoss[0m : 2.29757

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17015
[1mStep[0m  [8/84], [94mLoss[0m : 2.52092
[1mStep[0m  [16/84], [94mLoss[0m : 2.21650
[1mStep[0m  [24/84], [94mLoss[0m : 2.59169
[1mStep[0m  [32/84], [94mLoss[0m : 2.29854
[1mStep[0m  [40/84], [94mLoss[0m : 2.52570
[1mStep[0m  [48/84], [94mLoss[0m : 2.28742
[1mStep[0m  [56/84], [94mLoss[0m : 2.09809
[1mStep[0m  [64/84], [94mLoss[0m : 2.60139
[1mStep[0m  [72/84], [94mLoss[0m : 2.37337
[1mStep[0m  [80/84], [94mLoss[0m : 2.68893

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44024
[1mStep[0m  [8/84], [94mLoss[0m : 2.23113
[1mStep[0m  [16/84], [94mLoss[0m : 2.30891
[1mStep[0m  [24/84], [94mLoss[0m : 2.61000
[1mStep[0m  [32/84], [94mLoss[0m : 2.78005
[1mStep[0m  [40/84], [94mLoss[0m : 2.35678
[1mStep[0m  [48/84], [94mLoss[0m : 2.07676
[1mStep[0m  [56/84], [94mLoss[0m : 2.47734
[1mStep[0m  [64/84], [94mLoss[0m : 2.43117
[1mStep[0m  [72/84], [94mLoss[0m : 2.40783
[1mStep[0m  [80/84], [94mLoss[0m : 2.32472

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35395
[1mStep[0m  [8/84], [94mLoss[0m : 2.46245
[1mStep[0m  [16/84], [94mLoss[0m : 2.62752
[1mStep[0m  [24/84], [94mLoss[0m : 2.37282
[1mStep[0m  [32/84], [94mLoss[0m : 2.29018
[1mStep[0m  [40/84], [94mLoss[0m : 2.43143
[1mStep[0m  [48/84], [94mLoss[0m : 2.53002
[1mStep[0m  [56/84], [94mLoss[0m : 2.31720
[1mStep[0m  [64/84], [94mLoss[0m : 2.71965
[1mStep[0m  [72/84], [94mLoss[0m : 2.42835
[1mStep[0m  [80/84], [94mLoss[0m : 2.45079

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46949
[1mStep[0m  [8/84], [94mLoss[0m : 2.35505
[1mStep[0m  [16/84], [94mLoss[0m : 2.71299
[1mStep[0m  [24/84], [94mLoss[0m : 2.57773
[1mStep[0m  [32/84], [94mLoss[0m : 2.20338
[1mStep[0m  [40/84], [94mLoss[0m : 2.52871
[1mStep[0m  [48/84], [94mLoss[0m : 2.59478
[1mStep[0m  [56/84], [94mLoss[0m : 2.44108
[1mStep[0m  [64/84], [94mLoss[0m : 2.47204
[1mStep[0m  [72/84], [94mLoss[0m : 2.31055
[1mStep[0m  [80/84], [94mLoss[0m : 2.26175

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45164
[1mStep[0m  [8/84], [94mLoss[0m : 2.45102
[1mStep[0m  [16/84], [94mLoss[0m : 2.56411
[1mStep[0m  [24/84], [94mLoss[0m : 2.70791
[1mStep[0m  [32/84], [94mLoss[0m : 2.40648
[1mStep[0m  [40/84], [94mLoss[0m : 2.40367
[1mStep[0m  [48/84], [94mLoss[0m : 2.62510
[1mStep[0m  [56/84], [94mLoss[0m : 2.51666
[1mStep[0m  [64/84], [94mLoss[0m : 2.28301
[1mStep[0m  [72/84], [94mLoss[0m : 2.62002
[1mStep[0m  [80/84], [94mLoss[0m : 2.87395

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.318, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51806
[1mStep[0m  [8/84], [94mLoss[0m : 2.44718
[1mStep[0m  [16/84], [94mLoss[0m : 2.57638
[1mStep[0m  [24/84], [94mLoss[0m : 2.41854
[1mStep[0m  [32/84], [94mLoss[0m : 2.55659
[1mStep[0m  [40/84], [94mLoss[0m : 2.47301
[1mStep[0m  [48/84], [94mLoss[0m : 2.32348
[1mStep[0m  [56/84], [94mLoss[0m : 2.79211
[1mStep[0m  [64/84], [94mLoss[0m : 2.32909
[1mStep[0m  [72/84], [94mLoss[0m : 2.62289
[1mStep[0m  [80/84], [94mLoss[0m : 2.73578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63141
[1mStep[0m  [8/84], [94mLoss[0m : 2.49389
[1mStep[0m  [16/84], [94mLoss[0m : 2.26285
[1mStep[0m  [24/84], [94mLoss[0m : 2.41332
[1mStep[0m  [32/84], [94mLoss[0m : 2.36592
[1mStep[0m  [40/84], [94mLoss[0m : 2.33224
[1mStep[0m  [48/84], [94mLoss[0m : 2.57786
[1mStep[0m  [56/84], [94mLoss[0m : 2.07950
[1mStep[0m  [64/84], [94mLoss[0m : 2.58879
[1mStep[0m  [72/84], [94mLoss[0m : 2.30503
[1mStep[0m  [80/84], [94mLoss[0m : 2.42165

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52820
[1mStep[0m  [8/84], [94mLoss[0m : 2.60046
[1mStep[0m  [16/84], [94mLoss[0m : 2.24242
[1mStep[0m  [24/84], [94mLoss[0m : 2.46489
[1mStep[0m  [32/84], [94mLoss[0m : 2.47148
[1mStep[0m  [40/84], [94mLoss[0m : 2.29685
[1mStep[0m  [48/84], [94mLoss[0m : 2.51255
[1mStep[0m  [56/84], [94mLoss[0m : 2.25244
[1mStep[0m  [64/84], [94mLoss[0m : 2.67992
[1mStep[0m  [72/84], [94mLoss[0m : 2.46399
[1mStep[0m  [80/84], [94mLoss[0m : 2.54597

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54635
[1mStep[0m  [8/84], [94mLoss[0m : 2.44644
[1mStep[0m  [16/84], [94mLoss[0m : 2.47096
[1mStep[0m  [24/84], [94mLoss[0m : 2.17551
[1mStep[0m  [32/84], [94mLoss[0m : 2.45391
[1mStep[0m  [40/84], [94mLoss[0m : 2.48937
[1mStep[0m  [48/84], [94mLoss[0m : 2.59372
[1mStep[0m  [56/84], [94mLoss[0m : 2.15166
[1mStep[0m  [64/84], [94mLoss[0m : 2.49562
[1mStep[0m  [72/84], [94mLoss[0m : 2.43398
[1mStep[0m  [80/84], [94mLoss[0m : 2.40584

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36982
[1mStep[0m  [8/84], [94mLoss[0m : 2.32802
[1mStep[0m  [16/84], [94mLoss[0m : 2.32294
[1mStep[0m  [24/84], [94mLoss[0m : 2.39176
[1mStep[0m  [32/84], [94mLoss[0m : 2.33696
[1mStep[0m  [40/84], [94mLoss[0m : 2.36061
[1mStep[0m  [48/84], [94mLoss[0m : 2.27783
[1mStep[0m  [56/84], [94mLoss[0m : 2.10545
[1mStep[0m  [64/84], [94mLoss[0m : 2.50216
[1mStep[0m  [72/84], [94mLoss[0m : 2.56038
[1mStep[0m  [80/84], [94mLoss[0m : 2.54056

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14480
[1mStep[0m  [8/84], [94mLoss[0m : 2.55728
[1mStep[0m  [16/84], [94mLoss[0m : 2.43473
[1mStep[0m  [24/84], [94mLoss[0m : 2.43983
[1mStep[0m  [32/84], [94mLoss[0m : 2.51585
[1mStep[0m  [40/84], [94mLoss[0m : 2.42733
[1mStep[0m  [48/84], [94mLoss[0m : 2.62876
[1mStep[0m  [56/84], [94mLoss[0m : 2.83145
[1mStep[0m  [64/84], [94mLoss[0m : 2.34795
[1mStep[0m  [72/84], [94mLoss[0m : 2.37548
[1mStep[0m  [80/84], [94mLoss[0m : 2.57162

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66947
[1mStep[0m  [8/84], [94mLoss[0m : 2.42278
[1mStep[0m  [16/84], [94mLoss[0m : 2.29218
[1mStep[0m  [24/84], [94mLoss[0m : 2.47149
[1mStep[0m  [32/84], [94mLoss[0m : 2.33255
[1mStep[0m  [40/84], [94mLoss[0m : 2.28380
[1mStep[0m  [48/84], [94mLoss[0m : 2.74055
[1mStep[0m  [56/84], [94mLoss[0m : 2.75682
[1mStep[0m  [64/84], [94mLoss[0m : 2.80068
[1mStep[0m  [72/84], [94mLoss[0m : 2.40557
[1mStep[0m  [80/84], [94mLoss[0m : 2.65763

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36708
[1mStep[0m  [8/84], [94mLoss[0m : 2.47542
[1mStep[0m  [16/84], [94mLoss[0m : 2.10663
[1mStep[0m  [24/84], [94mLoss[0m : 2.37310
[1mStep[0m  [32/84], [94mLoss[0m : 2.28542
[1mStep[0m  [40/84], [94mLoss[0m : 2.27636
[1mStep[0m  [48/84], [94mLoss[0m : 2.60417
[1mStep[0m  [56/84], [94mLoss[0m : 2.40174
[1mStep[0m  [64/84], [94mLoss[0m : 2.76511
[1mStep[0m  [72/84], [94mLoss[0m : 2.67552
[1mStep[0m  [80/84], [94mLoss[0m : 2.31222

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79766
[1mStep[0m  [8/84], [94mLoss[0m : 2.48583
[1mStep[0m  [16/84], [94mLoss[0m : 2.68505
[1mStep[0m  [24/84], [94mLoss[0m : 2.43106
[1mStep[0m  [32/84], [94mLoss[0m : 2.46326
[1mStep[0m  [40/84], [94mLoss[0m : 2.44107
[1mStep[0m  [48/84], [94mLoss[0m : 2.60139
[1mStep[0m  [56/84], [94mLoss[0m : 2.48692
[1mStep[0m  [64/84], [94mLoss[0m : 2.46891
[1mStep[0m  [72/84], [94mLoss[0m : 2.46695
[1mStep[0m  [80/84], [94mLoss[0m : 2.56844

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60733
[1mStep[0m  [8/84], [94mLoss[0m : 2.41477
[1mStep[0m  [16/84], [94mLoss[0m : 2.54263
[1mStep[0m  [24/84], [94mLoss[0m : 2.25947
[1mStep[0m  [32/84], [94mLoss[0m : 2.22739
[1mStep[0m  [40/84], [94mLoss[0m : 2.74871
[1mStep[0m  [48/84], [94mLoss[0m : 2.57818
[1mStep[0m  [56/84], [94mLoss[0m : 2.28747
[1mStep[0m  [64/84], [94mLoss[0m : 2.74997
[1mStep[0m  [72/84], [94mLoss[0m : 2.48554
[1mStep[0m  [80/84], [94mLoss[0m : 2.39519

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40249
[1mStep[0m  [8/84], [94mLoss[0m : 2.18455
[1mStep[0m  [16/84], [94mLoss[0m : 2.39241
[1mStep[0m  [24/84], [94mLoss[0m : 2.33952
[1mStep[0m  [32/84], [94mLoss[0m : 2.65591
[1mStep[0m  [40/84], [94mLoss[0m : 2.40529
[1mStep[0m  [48/84], [94mLoss[0m : 2.09623
[1mStep[0m  [56/84], [94mLoss[0m : 2.45043
[1mStep[0m  [64/84], [94mLoss[0m : 2.34576
[1mStep[0m  [72/84], [94mLoss[0m : 2.73468
[1mStep[0m  [80/84], [94mLoss[0m : 2.70131

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28326
[1mStep[0m  [8/84], [94mLoss[0m : 2.80521
[1mStep[0m  [16/84], [94mLoss[0m : 2.40249
[1mStep[0m  [24/84], [94mLoss[0m : 2.52039
[1mStep[0m  [32/84], [94mLoss[0m : 2.40970
[1mStep[0m  [40/84], [94mLoss[0m : 2.80320
[1mStep[0m  [48/84], [94mLoss[0m : 2.51462
[1mStep[0m  [56/84], [94mLoss[0m : 2.59220
[1mStep[0m  [64/84], [94mLoss[0m : 2.64906
[1mStep[0m  [72/84], [94mLoss[0m : 2.11706
[1mStep[0m  [80/84], [94mLoss[0m : 2.54397

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21695
[1mStep[0m  [8/84], [94mLoss[0m : 2.32643
[1mStep[0m  [16/84], [94mLoss[0m : 2.45477
[1mStep[0m  [24/84], [94mLoss[0m : 2.49920
[1mStep[0m  [32/84], [94mLoss[0m : 3.09546
[1mStep[0m  [40/84], [94mLoss[0m : 2.72763
[1mStep[0m  [48/84], [94mLoss[0m : 2.60184
[1mStep[0m  [56/84], [94mLoss[0m : 2.29187
[1mStep[0m  [64/84], [94mLoss[0m : 2.45455
[1mStep[0m  [72/84], [94mLoss[0m : 2.48543
[1mStep[0m  [80/84], [94mLoss[0m : 2.55060

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47268
[1mStep[0m  [8/84], [94mLoss[0m : 2.69284
[1mStep[0m  [16/84], [94mLoss[0m : 2.42498
[1mStep[0m  [24/84], [94mLoss[0m : 2.76695
[1mStep[0m  [32/84], [94mLoss[0m : 2.29320
[1mStep[0m  [40/84], [94mLoss[0m : 2.29767
[1mStep[0m  [48/84], [94mLoss[0m : 2.68621
[1mStep[0m  [56/84], [94mLoss[0m : 2.31159
[1mStep[0m  [64/84], [94mLoss[0m : 2.41661
[1mStep[0m  [72/84], [94mLoss[0m : 2.39192
[1mStep[0m  [80/84], [94mLoss[0m : 2.36965

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.324
====================================

Phase 1 - Evaluation MAE:  2.3236646141324724
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.38235
[1mStep[0m  [8/84], [94mLoss[0m : 2.56098
[1mStep[0m  [16/84], [94mLoss[0m : 2.52705
[1mStep[0m  [24/84], [94mLoss[0m : 2.46852
[1mStep[0m  [32/84], [94mLoss[0m : 2.50875
[1mStep[0m  [40/84], [94mLoss[0m : 2.23113
[1mStep[0m  [48/84], [94mLoss[0m : 2.36579
[1mStep[0m  [56/84], [94mLoss[0m : 2.65136
[1mStep[0m  [64/84], [94mLoss[0m : 2.33562
[1mStep[0m  [72/84], [94mLoss[0m : 2.51072
[1mStep[0m  [80/84], [94mLoss[0m : 2.42466

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17691
[1mStep[0m  [8/84], [94mLoss[0m : 2.25998
[1mStep[0m  [16/84], [94mLoss[0m : 2.68651
[1mStep[0m  [24/84], [94mLoss[0m : 2.55452
[1mStep[0m  [32/84], [94mLoss[0m : 2.08507
[1mStep[0m  [40/84], [94mLoss[0m : 2.31913
[1mStep[0m  [48/84], [94mLoss[0m : 2.19661
[1mStep[0m  [56/84], [94mLoss[0m : 2.44817
[1mStep[0m  [64/84], [94mLoss[0m : 2.61368
[1mStep[0m  [72/84], [94mLoss[0m : 2.54311
[1mStep[0m  [80/84], [94mLoss[0m : 1.96150

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45272
[1mStep[0m  [8/84], [94mLoss[0m : 2.26505
[1mStep[0m  [16/84], [94mLoss[0m : 2.54179
[1mStep[0m  [24/84], [94mLoss[0m : 2.08649
[1mStep[0m  [32/84], [94mLoss[0m : 2.50015
[1mStep[0m  [40/84], [94mLoss[0m : 2.11972
[1mStep[0m  [48/84], [94mLoss[0m : 2.49146
[1mStep[0m  [56/84], [94mLoss[0m : 2.31740
[1mStep[0m  [64/84], [94mLoss[0m : 2.32345
[1mStep[0m  [72/84], [94mLoss[0m : 2.34896
[1mStep[0m  [80/84], [94mLoss[0m : 2.13512

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22148
[1mStep[0m  [8/84], [94mLoss[0m : 2.23312
[1mStep[0m  [16/84], [94mLoss[0m : 2.54444
[1mStep[0m  [24/84], [94mLoss[0m : 2.35986
[1mStep[0m  [32/84], [94mLoss[0m : 2.34661
[1mStep[0m  [40/84], [94mLoss[0m : 2.33760
[1mStep[0m  [48/84], [94mLoss[0m : 2.20873
[1mStep[0m  [56/84], [94mLoss[0m : 2.33314
[1mStep[0m  [64/84], [94mLoss[0m : 2.07197
[1mStep[0m  [72/84], [94mLoss[0m : 2.17746
[1mStep[0m  [80/84], [94mLoss[0m : 2.32928

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12332
[1mStep[0m  [8/84], [94mLoss[0m : 2.09515
[1mStep[0m  [16/84], [94mLoss[0m : 2.20049
[1mStep[0m  [24/84], [94mLoss[0m : 2.29488
[1mStep[0m  [32/84], [94mLoss[0m : 2.21616
[1mStep[0m  [40/84], [94mLoss[0m : 2.24232
[1mStep[0m  [48/84], [94mLoss[0m : 2.34090
[1mStep[0m  [56/84], [94mLoss[0m : 2.01794
[1mStep[0m  [64/84], [94mLoss[0m : 2.30376
[1mStep[0m  [72/84], [94mLoss[0m : 2.27323
[1mStep[0m  [80/84], [94mLoss[0m : 2.61873

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.251, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22747
[1mStep[0m  [8/84], [94mLoss[0m : 2.21089
[1mStep[0m  [16/84], [94mLoss[0m : 2.28983
[1mStep[0m  [24/84], [94mLoss[0m : 2.00414
[1mStep[0m  [32/84], [94mLoss[0m : 2.07088
[1mStep[0m  [40/84], [94mLoss[0m : 2.19908
[1mStep[0m  [48/84], [94mLoss[0m : 2.30176
[1mStep[0m  [56/84], [94mLoss[0m : 2.53163
[1mStep[0m  [64/84], [94mLoss[0m : 2.25079
[1mStep[0m  [72/84], [94mLoss[0m : 2.09979
[1mStep[0m  [80/84], [94mLoss[0m : 2.06028

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82987
[1mStep[0m  [8/84], [94mLoss[0m : 1.83336
[1mStep[0m  [16/84], [94mLoss[0m : 2.07360
[1mStep[0m  [24/84], [94mLoss[0m : 2.08571
[1mStep[0m  [32/84], [94mLoss[0m : 2.12955
[1mStep[0m  [40/84], [94mLoss[0m : 2.25760
[1mStep[0m  [48/84], [94mLoss[0m : 1.97162
[1mStep[0m  [56/84], [94mLoss[0m : 1.87062
[1mStep[0m  [64/84], [94mLoss[0m : 2.23984
[1mStep[0m  [72/84], [94mLoss[0m : 2.06523
[1mStep[0m  [80/84], [94mLoss[0m : 2.25071

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73391
[1mStep[0m  [8/84], [94mLoss[0m : 1.91803
[1mStep[0m  [16/84], [94mLoss[0m : 2.13751
[1mStep[0m  [24/84], [94mLoss[0m : 1.80399
[1mStep[0m  [32/84], [94mLoss[0m : 2.25409
[1mStep[0m  [40/84], [94mLoss[0m : 2.09885
[1mStep[0m  [48/84], [94mLoss[0m : 2.10741
[1mStep[0m  [56/84], [94mLoss[0m : 2.04125
[1mStep[0m  [64/84], [94mLoss[0m : 2.01163
[1mStep[0m  [72/84], [94mLoss[0m : 2.06087
[1mStep[0m  [80/84], [94mLoss[0m : 2.14852

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40056
[1mStep[0m  [8/84], [94mLoss[0m : 1.93498
[1mStep[0m  [16/84], [94mLoss[0m : 2.01765
[1mStep[0m  [24/84], [94mLoss[0m : 1.85607
[1mStep[0m  [32/84], [94mLoss[0m : 1.93472
[1mStep[0m  [40/84], [94mLoss[0m : 2.04446
[1mStep[0m  [48/84], [94mLoss[0m : 2.23129
[1mStep[0m  [56/84], [94mLoss[0m : 2.31968
[1mStep[0m  [64/84], [94mLoss[0m : 1.88497
[1mStep[0m  [72/84], [94mLoss[0m : 1.92332
[1mStep[0m  [80/84], [94mLoss[0m : 2.15701

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92474
[1mStep[0m  [8/84], [94mLoss[0m : 1.96616
[1mStep[0m  [16/84], [94mLoss[0m : 1.92842
[1mStep[0m  [24/84], [94mLoss[0m : 1.72962
[1mStep[0m  [32/84], [94mLoss[0m : 2.13041
[1mStep[0m  [40/84], [94mLoss[0m : 2.07539
[1mStep[0m  [48/84], [94mLoss[0m : 1.79362
[1mStep[0m  [56/84], [94mLoss[0m : 2.23837
[1mStep[0m  [64/84], [94mLoss[0m : 2.18323
[1mStep[0m  [72/84], [94mLoss[0m : 1.97023
[1mStep[0m  [80/84], [94mLoss[0m : 1.81357

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00430
[1mStep[0m  [8/84], [94mLoss[0m : 2.01906
[1mStep[0m  [16/84], [94mLoss[0m : 1.69577
[1mStep[0m  [24/84], [94mLoss[0m : 1.63979
[1mStep[0m  [32/84], [94mLoss[0m : 1.94742
[1mStep[0m  [40/84], [94mLoss[0m : 1.86923
[1mStep[0m  [48/84], [94mLoss[0m : 2.11449
[1mStep[0m  [56/84], [94mLoss[0m : 1.79662
[1mStep[0m  [64/84], [94mLoss[0m : 1.86350
[1mStep[0m  [72/84], [94mLoss[0m : 2.19919
[1mStep[0m  [80/84], [94mLoss[0m : 1.71903

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74651
[1mStep[0m  [8/84], [94mLoss[0m : 1.90688
[1mStep[0m  [16/84], [94mLoss[0m : 1.71621
[1mStep[0m  [24/84], [94mLoss[0m : 2.15333
[1mStep[0m  [32/84], [94mLoss[0m : 1.75906
[1mStep[0m  [40/84], [94mLoss[0m : 1.90592
[1mStep[0m  [48/84], [94mLoss[0m : 1.83470
[1mStep[0m  [56/84], [94mLoss[0m : 1.99201
[1mStep[0m  [64/84], [94mLoss[0m : 1.96660
[1mStep[0m  [72/84], [94mLoss[0m : 1.87259
[1mStep[0m  [80/84], [94mLoss[0m : 2.06851

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79631
[1mStep[0m  [8/84], [94mLoss[0m : 1.64014
[1mStep[0m  [16/84], [94mLoss[0m : 2.10029
[1mStep[0m  [24/84], [94mLoss[0m : 1.93111
[1mStep[0m  [32/84], [94mLoss[0m : 1.99069
[1mStep[0m  [40/84], [94mLoss[0m : 2.05493
[1mStep[0m  [48/84], [94mLoss[0m : 1.81385
[1mStep[0m  [56/84], [94mLoss[0m : 1.81793
[1mStep[0m  [64/84], [94mLoss[0m : 1.98825
[1mStep[0m  [72/84], [94mLoss[0m : 1.74937
[1mStep[0m  [80/84], [94mLoss[0m : 1.66197

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78312
[1mStep[0m  [8/84], [94mLoss[0m : 1.63347
[1mStep[0m  [16/84], [94mLoss[0m : 1.77739
[1mStep[0m  [24/84], [94mLoss[0m : 1.62227
[1mStep[0m  [32/84], [94mLoss[0m : 1.72684
[1mStep[0m  [40/84], [94mLoss[0m : 1.48974
[1mStep[0m  [48/84], [94mLoss[0m : 1.93528
[1mStep[0m  [56/84], [94mLoss[0m : 1.83470
[1mStep[0m  [64/84], [94mLoss[0m : 1.88404
[1mStep[0m  [72/84], [94mLoss[0m : 1.68658
[1mStep[0m  [80/84], [94mLoss[0m : 1.76808

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66013
[1mStep[0m  [8/84], [94mLoss[0m : 1.55305
[1mStep[0m  [16/84], [94mLoss[0m : 1.91674
[1mStep[0m  [24/84], [94mLoss[0m : 1.88008
[1mStep[0m  [32/84], [94mLoss[0m : 1.53726
[1mStep[0m  [40/84], [94mLoss[0m : 1.71983
[1mStep[0m  [48/84], [94mLoss[0m : 1.67910
[1mStep[0m  [56/84], [94mLoss[0m : 1.96010
[1mStep[0m  [64/84], [94mLoss[0m : 1.92897
[1mStep[0m  [72/84], [94mLoss[0m : 1.99132
[1mStep[0m  [80/84], [94mLoss[0m : 1.88397

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90006
[1mStep[0m  [8/84], [94mLoss[0m : 1.49251
[1mStep[0m  [16/84], [94mLoss[0m : 1.69114
[1mStep[0m  [24/84], [94mLoss[0m : 1.76849
[1mStep[0m  [32/84], [94mLoss[0m : 1.85609
[1mStep[0m  [40/84], [94mLoss[0m : 1.86110
[1mStep[0m  [48/84], [94mLoss[0m : 1.73824
[1mStep[0m  [56/84], [94mLoss[0m : 1.85383
[1mStep[0m  [64/84], [94mLoss[0m : 1.77559
[1mStep[0m  [72/84], [94mLoss[0m : 1.94541
[1mStep[0m  [80/84], [94mLoss[0m : 1.61181

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65942
[1mStep[0m  [8/84], [94mLoss[0m : 1.48386
[1mStep[0m  [16/84], [94mLoss[0m : 1.64198
[1mStep[0m  [24/84], [94mLoss[0m : 1.56554
[1mStep[0m  [32/84], [94mLoss[0m : 1.77807
[1mStep[0m  [40/84], [94mLoss[0m : 1.59158
[1mStep[0m  [48/84], [94mLoss[0m : 1.60386
[1mStep[0m  [56/84], [94mLoss[0m : 1.59449
[1mStep[0m  [64/84], [94mLoss[0m : 1.68909
[1mStep[0m  [72/84], [94mLoss[0m : 1.68586
[1mStep[0m  [80/84], [94mLoss[0m : 1.67563

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.513, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67882
[1mStep[0m  [8/84], [94mLoss[0m : 1.74543
[1mStep[0m  [16/84], [94mLoss[0m : 1.62206
[1mStep[0m  [24/84], [94mLoss[0m : 1.47694
[1mStep[0m  [32/84], [94mLoss[0m : 1.43113
[1mStep[0m  [40/84], [94mLoss[0m : 1.56614
[1mStep[0m  [48/84], [94mLoss[0m : 1.67802
[1mStep[0m  [56/84], [94mLoss[0m : 1.71511
[1mStep[0m  [64/84], [94mLoss[0m : 1.92324
[1mStep[0m  [72/84], [94mLoss[0m : 1.54321
[1mStep[0m  [80/84], [94mLoss[0m : 1.55835

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43024
[1mStep[0m  [8/84], [94mLoss[0m : 1.73658
[1mStep[0m  [16/84], [94mLoss[0m : 1.40444
[1mStep[0m  [24/84], [94mLoss[0m : 1.58889
[1mStep[0m  [32/84], [94mLoss[0m : 1.65013
[1mStep[0m  [40/84], [94mLoss[0m : 1.60771
[1mStep[0m  [48/84], [94mLoss[0m : 1.51911
[1mStep[0m  [56/84], [94mLoss[0m : 1.74362
[1mStep[0m  [64/84], [94mLoss[0m : 1.69844
[1mStep[0m  [72/84], [94mLoss[0m : 1.64506
[1mStep[0m  [80/84], [94mLoss[0m : 1.53425

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.556, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66351
[1mStep[0m  [8/84], [94mLoss[0m : 1.80266
[1mStep[0m  [16/84], [94mLoss[0m : 1.71136
[1mStep[0m  [24/84], [94mLoss[0m : 1.84627
[1mStep[0m  [32/84], [94mLoss[0m : 1.88171
[1mStep[0m  [40/84], [94mLoss[0m : 1.57267
[1mStep[0m  [48/84], [94mLoss[0m : 1.65637
[1mStep[0m  [56/84], [94mLoss[0m : 1.58407
[1mStep[0m  [64/84], [94mLoss[0m : 1.57137
[1mStep[0m  [72/84], [94mLoss[0m : 1.75437
[1mStep[0m  [80/84], [94mLoss[0m : 1.62447

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.598, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53832
[1mStep[0m  [8/84], [94mLoss[0m : 1.53772
[1mStep[0m  [16/84], [94mLoss[0m : 1.32958
[1mStep[0m  [24/84], [94mLoss[0m : 1.40653
[1mStep[0m  [32/84], [94mLoss[0m : 1.42676
[1mStep[0m  [40/84], [94mLoss[0m : 1.63328
[1mStep[0m  [48/84], [94mLoss[0m : 1.78741
[1mStep[0m  [56/84], [94mLoss[0m : 1.61445
[1mStep[0m  [64/84], [94mLoss[0m : 1.38425
[1mStep[0m  [72/84], [94mLoss[0m : 1.62025
[1mStep[0m  [80/84], [94mLoss[0m : 1.51212

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.458, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59725
[1mStep[0m  [8/84], [94mLoss[0m : 1.41309
[1mStep[0m  [16/84], [94mLoss[0m : 1.56450
[1mStep[0m  [24/84], [94mLoss[0m : 1.69798
[1mStep[0m  [32/84], [94mLoss[0m : 1.61484
[1mStep[0m  [40/84], [94mLoss[0m : 1.69414
[1mStep[0m  [48/84], [94mLoss[0m : 1.54811
[1mStep[0m  [56/84], [94mLoss[0m : 1.63926
[1mStep[0m  [64/84], [94mLoss[0m : 1.79932
[1mStep[0m  [72/84], [94mLoss[0m : 1.64511
[1mStep[0m  [80/84], [94mLoss[0m : 1.52011

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.543, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64402
[1mStep[0m  [8/84], [94mLoss[0m : 1.39374
[1mStep[0m  [16/84], [94mLoss[0m : 1.35358
[1mStep[0m  [24/84], [94mLoss[0m : 1.33334
[1mStep[0m  [32/84], [94mLoss[0m : 1.47311
[1mStep[0m  [40/84], [94mLoss[0m : 1.57116
[1mStep[0m  [48/84], [94mLoss[0m : 1.26945
[1mStep[0m  [56/84], [94mLoss[0m : 1.56377
[1mStep[0m  [64/84], [94mLoss[0m : 1.51198
[1mStep[0m  [72/84], [94mLoss[0m : 1.50237
[1mStep[0m  [80/84], [94mLoss[0m : 1.75695

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.508, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55245
[1mStep[0m  [8/84], [94mLoss[0m : 1.44915
[1mStep[0m  [16/84], [94mLoss[0m : 1.61378
[1mStep[0m  [24/84], [94mLoss[0m : 1.44845
[1mStep[0m  [32/84], [94mLoss[0m : 1.67419
[1mStep[0m  [40/84], [94mLoss[0m : 1.61478
[1mStep[0m  [48/84], [94mLoss[0m : 1.48162
[1mStep[0m  [56/84], [94mLoss[0m : 1.56316
[1mStep[0m  [64/84], [94mLoss[0m : 1.30121
[1mStep[0m  [72/84], [94mLoss[0m : 1.27241
[1mStep[0m  [80/84], [94mLoss[0m : 1.53773

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.455, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57843
[1mStep[0m  [8/84], [94mLoss[0m : 1.34609
[1mStep[0m  [16/84], [94mLoss[0m : 1.39181
[1mStep[0m  [24/84], [94mLoss[0m : 1.45703
[1mStep[0m  [32/84], [94mLoss[0m : 1.40852
[1mStep[0m  [40/84], [94mLoss[0m : 1.51730
[1mStep[0m  [48/84], [94mLoss[0m : 1.30844
[1mStep[0m  [56/84], [94mLoss[0m : 1.54826
[1mStep[0m  [64/84], [94mLoss[0m : 1.41955
[1mStep[0m  [72/84], [94mLoss[0m : 1.54751
[1mStep[0m  [80/84], [94mLoss[0m : 1.67487

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.479, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48492
[1mStep[0m  [8/84], [94mLoss[0m : 1.25064
[1mStep[0m  [16/84], [94mLoss[0m : 1.35216
[1mStep[0m  [24/84], [94mLoss[0m : 1.41414
[1mStep[0m  [32/84], [94mLoss[0m : 1.50696
[1mStep[0m  [40/84], [94mLoss[0m : 1.24906
[1mStep[0m  [48/84], [94mLoss[0m : 1.46382
[1mStep[0m  [56/84], [94mLoss[0m : 1.71715
[1mStep[0m  [64/84], [94mLoss[0m : 1.43093
[1mStep[0m  [72/84], [94mLoss[0m : 1.34279
[1mStep[0m  [80/84], [94mLoss[0m : 1.50631

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.467, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.26312
[1mStep[0m  [8/84], [94mLoss[0m : 1.37743
[1mStep[0m  [16/84], [94mLoss[0m : 1.47301
[1mStep[0m  [24/84], [94mLoss[0m : 1.38900
[1mStep[0m  [32/84], [94mLoss[0m : 1.44259
[1mStep[0m  [40/84], [94mLoss[0m : 1.54613
[1mStep[0m  [48/84], [94mLoss[0m : 1.29348
[1mStep[0m  [56/84], [94mLoss[0m : 1.56995
[1mStep[0m  [64/84], [94mLoss[0m : 1.36935
[1mStep[0m  [72/84], [94mLoss[0m : 1.54497
[1mStep[0m  [80/84], [94mLoss[0m : 1.50560

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.439, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.34943
[1mStep[0m  [8/84], [94mLoss[0m : 1.34787
[1mStep[0m  [16/84], [94mLoss[0m : 1.30806
[1mStep[0m  [24/84], [94mLoss[0m : 1.41396
[1mStep[0m  [32/84], [94mLoss[0m : 1.80197
[1mStep[0m  [40/84], [94mLoss[0m : 1.24389
[1mStep[0m  [48/84], [94mLoss[0m : 1.48749
[1mStep[0m  [56/84], [94mLoss[0m : 1.58145
[1mStep[0m  [64/84], [94mLoss[0m : 1.51695
[1mStep[0m  [72/84], [94mLoss[0m : 1.49544
[1mStep[0m  [80/84], [94mLoss[0m : 1.63755

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.444, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.38683
[1mStep[0m  [8/84], [94mLoss[0m : 1.45249
[1mStep[0m  [16/84], [94mLoss[0m : 1.34553
[1mStep[0m  [24/84], [94mLoss[0m : 1.65384
[1mStep[0m  [32/84], [94mLoss[0m : 1.55167
[1mStep[0m  [40/84], [94mLoss[0m : 1.47471
[1mStep[0m  [48/84], [94mLoss[0m : 1.54068
[1mStep[0m  [56/84], [94mLoss[0m : 1.56822
[1mStep[0m  [64/84], [94mLoss[0m : 1.43454
[1mStep[0m  [72/84], [94mLoss[0m : 1.41097
[1mStep[0m  [80/84], [94mLoss[0m : 1.42049

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.409, [92mTest[0m: 2.514, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35097
[1mStep[0m  [8/84], [94mLoss[0m : 1.26052
[1mStep[0m  [16/84], [94mLoss[0m : 1.54523
[1mStep[0m  [24/84], [94mLoss[0m : 1.33486
[1mStep[0m  [32/84], [94mLoss[0m : 1.66213
[1mStep[0m  [40/84], [94mLoss[0m : 1.30888
[1mStep[0m  [48/84], [94mLoss[0m : 1.42944
[1mStep[0m  [56/84], [94mLoss[0m : 1.36359
[1mStep[0m  [64/84], [94mLoss[0m : 1.46957
[1mStep[0m  [72/84], [94mLoss[0m : 1.28405
[1mStep[0m  [80/84], [94mLoss[0m : 1.45750

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.402, [92mTest[0m: 2.499, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.481
====================================

Phase 2 - Evaluation MAE:  2.481454142502376
MAE score P1      2.323665
MAE score P2      2.481454
loss              1.402203
learning_rate         0.01
batch_size             128
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 19, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.64720
[1mStep[0m  [4/42], [94mLoss[0m : 9.51963
[1mStep[0m  [8/42], [94mLoss[0m : 9.39744
[1mStep[0m  [12/42], [94mLoss[0m : 8.44068
[1mStep[0m  [16/42], [94mLoss[0m : 7.68782
[1mStep[0m  [20/42], [94mLoss[0m : 6.98887
[1mStep[0m  [24/42], [94mLoss[0m : 6.50818
[1mStep[0m  [28/42], [94mLoss[0m : 5.49419
[1mStep[0m  [32/42], [94mLoss[0m : 4.97758
[1mStep[0m  [36/42], [94mLoss[0m : 4.84830
[1mStep[0m  [40/42], [94mLoss[0m : 4.19418

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.134, [92mTest[0m: 10.830, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.25596
[1mStep[0m  [4/42], [94mLoss[0m : 3.53470
[1mStep[0m  [8/42], [94mLoss[0m : 3.28511
[1mStep[0m  [12/42], [94mLoss[0m : 3.20523
[1mStep[0m  [16/42], [94mLoss[0m : 2.82542
[1mStep[0m  [20/42], [94mLoss[0m : 2.76533
[1mStep[0m  [24/42], [94mLoss[0m : 2.88941
[1mStep[0m  [28/42], [94mLoss[0m : 2.78558
[1mStep[0m  [32/42], [94mLoss[0m : 2.47457
[1mStep[0m  [36/42], [94mLoss[0m : 2.81686
[1mStep[0m  [40/42], [94mLoss[0m : 2.52299

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.997, [92mTest[0m: 4.946, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62293
[1mStep[0m  [4/42], [94mLoss[0m : 2.63497
[1mStep[0m  [8/42], [94mLoss[0m : 2.80644
[1mStep[0m  [12/42], [94mLoss[0m : 2.52264
[1mStep[0m  [16/42], [94mLoss[0m : 2.46303
[1mStep[0m  [20/42], [94mLoss[0m : 2.50424
[1mStep[0m  [24/42], [94mLoss[0m : 2.49347
[1mStep[0m  [28/42], [94mLoss[0m : 2.54557
[1mStep[0m  [32/42], [94mLoss[0m : 2.49675
[1mStep[0m  [36/42], [94mLoss[0m : 2.51954
[1mStep[0m  [40/42], [94mLoss[0m : 2.60762

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.901, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43548
[1mStep[0m  [4/42], [94mLoss[0m : 2.64739
[1mStep[0m  [8/42], [94mLoss[0m : 2.78565
[1mStep[0m  [12/42], [94mLoss[0m : 2.65534
[1mStep[0m  [16/42], [94mLoss[0m : 2.59537
[1mStep[0m  [20/42], [94mLoss[0m : 2.48174
[1mStep[0m  [24/42], [94mLoss[0m : 2.41306
[1mStep[0m  [28/42], [94mLoss[0m : 2.50872
[1mStep[0m  [32/42], [94mLoss[0m : 2.61208
[1mStep[0m  [36/42], [94mLoss[0m : 2.63802
[1mStep[0m  [40/42], [94mLoss[0m : 2.47112

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.650, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74419
[1mStep[0m  [4/42], [94mLoss[0m : 2.36052
[1mStep[0m  [8/42], [94mLoss[0m : 2.47280
[1mStep[0m  [12/42], [94mLoss[0m : 2.51703
[1mStep[0m  [16/42], [94mLoss[0m : 2.57377
[1mStep[0m  [20/42], [94mLoss[0m : 2.38210
[1mStep[0m  [24/42], [94mLoss[0m : 2.73791
[1mStep[0m  [28/42], [94mLoss[0m : 2.65126
[1mStep[0m  [32/42], [94mLoss[0m : 2.39192
[1mStep[0m  [36/42], [94mLoss[0m : 2.72149
[1mStep[0m  [40/42], [94mLoss[0m : 2.63127

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68143
[1mStep[0m  [4/42], [94mLoss[0m : 2.35662
[1mStep[0m  [8/42], [94mLoss[0m : 2.81640
[1mStep[0m  [12/42], [94mLoss[0m : 2.55123
[1mStep[0m  [16/42], [94mLoss[0m : 2.61040
[1mStep[0m  [20/42], [94mLoss[0m : 2.27160
[1mStep[0m  [24/42], [94mLoss[0m : 2.53493
[1mStep[0m  [28/42], [94mLoss[0m : 2.48242
[1mStep[0m  [32/42], [94mLoss[0m : 2.48040
[1mStep[0m  [36/42], [94mLoss[0m : 2.59967
[1mStep[0m  [40/42], [94mLoss[0m : 2.41032

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.559, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39495
[1mStep[0m  [4/42], [94mLoss[0m : 2.68672
[1mStep[0m  [8/42], [94mLoss[0m : 2.52920
[1mStep[0m  [12/42], [94mLoss[0m : 2.50782
[1mStep[0m  [16/42], [94mLoss[0m : 2.45218
[1mStep[0m  [20/42], [94mLoss[0m : 2.41297
[1mStep[0m  [24/42], [94mLoss[0m : 2.54705
[1mStep[0m  [28/42], [94mLoss[0m : 2.37762
[1mStep[0m  [32/42], [94mLoss[0m : 2.44931
[1mStep[0m  [36/42], [94mLoss[0m : 2.40791
[1mStep[0m  [40/42], [94mLoss[0m : 2.64386

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.564, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77791
[1mStep[0m  [4/42], [94mLoss[0m : 2.41021
[1mStep[0m  [8/42], [94mLoss[0m : 2.49720
[1mStep[0m  [12/42], [94mLoss[0m : 2.44846
[1mStep[0m  [16/42], [94mLoss[0m : 2.58796
[1mStep[0m  [20/42], [94mLoss[0m : 2.58264
[1mStep[0m  [24/42], [94mLoss[0m : 2.63005
[1mStep[0m  [28/42], [94mLoss[0m : 2.39207
[1mStep[0m  [32/42], [94mLoss[0m : 2.48619
[1mStep[0m  [36/42], [94mLoss[0m : 2.62977
[1mStep[0m  [40/42], [94mLoss[0m : 2.54484

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58666
[1mStep[0m  [4/42], [94mLoss[0m : 2.44853
[1mStep[0m  [8/42], [94mLoss[0m : 2.55805
[1mStep[0m  [12/42], [94mLoss[0m : 2.31666
[1mStep[0m  [16/42], [94mLoss[0m : 2.49379
[1mStep[0m  [20/42], [94mLoss[0m : 2.63830
[1mStep[0m  [24/42], [94mLoss[0m : 2.53619
[1mStep[0m  [28/42], [94mLoss[0m : 2.40673
[1mStep[0m  [32/42], [94mLoss[0m : 2.40786
[1mStep[0m  [36/42], [94mLoss[0m : 2.43172
[1mStep[0m  [40/42], [94mLoss[0m : 2.59314

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55786
[1mStep[0m  [4/42], [94mLoss[0m : 2.32957
[1mStep[0m  [8/42], [94mLoss[0m : 2.43915
[1mStep[0m  [12/42], [94mLoss[0m : 2.37476
[1mStep[0m  [16/42], [94mLoss[0m : 2.64208
[1mStep[0m  [20/42], [94mLoss[0m : 2.26604
[1mStep[0m  [24/42], [94mLoss[0m : 2.65221
[1mStep[0m  [28/42], [94mLoss[0m : 2.56721
[1mStep[0m  [32/42], [94mLoss[0m : 2.48293
[1mStep[0m  [36/42], [94mLoss[0m : 2.32181
[1mStep[0m  [40/42], [94mLoss[0m : 2.51477

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47291
[1mStep[0m  [4/42], [94mLoss[0m : 2.35921
[1mStep[0m  [8/42], [94mLoss[0m : 2.13900
[1mStep[0m  [12/42], [94mLoss[0m : 2.32491
[1mStep[0m  [16/42], [94mLoss[0m : 2.35377
[1mStep[0m  [20/42], [94mLoss[0m : 2.62615
[1mStep[0m  [24/42], [94mLoss[0m : 2.45305
[1mStep[0m  [28/42], [94mLoss[0m : 2.45360
[1mStep[0m  [32/42], [94mLoss[0m : 2.56879
[1mStep[0m  [36/42], [94mLoss[0m : 2.23533
[1mStep[0m  [40/42], [94mLoss[0m : 2.39873

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42069
[1mStep[0m  [4/42], [94mLoss[0m : 2.53509
[1mStep[0m  [8/42], [94mLoss[0m : 2.26643
[1mStep[0m  [12/42], [94mLoss[0m : 2.40668
[1mStep[0m  [16/42], [94mLoss[0m : 2.54715
[1mStep[0m  [20/42], [94mLoss[0m : 2.47589
[1mStep[0m  [24/42], [94mLoss[0m : 2.63391
[1mStep[0m  [28/42], [94mLoss[0m : 2.38477
[1mStep[0m  [32/42], [94mLoss[0m : 2.47721
[1mStep[0m  [36/42], [94mLoss[0m : 2.39562
[1mStep[0m  [40/42], [94mLoss[0m : 2.39292

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36223
[1mStep[0m  [4/42], [94mLoss[0m : 2.41780
[1mStep[0m  [8/42], [94mLoss[0m : 2.32668
[1mStep[0m  [12/42], [94mLoss[0m : 2.38575
[1mStep[0m  [16/42], [94mLoss[0m : 2.42820
[1mStep[0m  [20/42], [94mLoss[0m : 2.49492
[1mStep[0m  [24/42], [94mLoss[0m : 2.49622
[1mStep[0m  [28/42], [94mLoss[0m : 2.27644
[1mStep[0m  [32/42], [94mLoss[0m : 2.51230
[1mStep[0m  [36/42], [94mLoss[0m : 2.30616
[1mStep[0m  [40/42], [94mLoss[0m : 2.52890

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43795
[1mStep[0m  [4/42], [94mLoss[0m : 2.46470
[1mStep[0m  [8/42], [94mLoss[0m : 2.59635
[1mStep[0m  [12/42], [94mLoss[0m : 2.56055
[1mStep[0m  [16/42], [94mLoss[0m : 2.30746
[1mStep[0m  [20/42], [94mLoss[0m : 2.33970
[1mStep[0m  [24/42], [94mLoss[0m : 2.36579
[1mStep[0m  [28/42], [94mLoss[0m : 2.41425
[1mStep[0m  [32/42], [94mLoss[0m : 2.31553
[1mStep[0m  [36/42], [94mLoss[0m : 2.43886
[1mStep[0m  [40/42], [94mLoss[0m : 2.50462

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40421
[1mStep[0m  [4/42], [94mLoss[0m : 2.60952
[1mStep[0m  [8/42], [94mLoss[0m : 2.53413
[1mStep[0m  [12/42], [94mLoss[0m : 2.55084
[1mStep[0m  [16/42], [94mLoss[0m : 2.27170
[1mStep[0m  [20/42], [94mLoss[0m : 2.51419
[1mStep[0m  [24/42], [94mLoss[0m : 2.43056
[1mStep[0m  [28/42], [94mLoss[0m : 2.41054
[1mStep[0m  [32/42], [94mLoss[0m : 2.64109
[1mStep[0m  [36/42], [94mLoss[0m : 2.27619
[1mStep[0m  [40/42], [94mLoss[0m : 2.54094

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50362
[1mStep[0m  [4/42], [94mLoss[0m : 2.33006
[1mStep[0m  [8/42], [94mLoss[0m : 2.36941
[1mStep[0m  [12/42], [94mLoss[0m : 2.40393
[1mStep[0m  [16/42], [94mLoss[0m : 2.62500
[1mStep[0m  [20/42], [94mLoss[0m : 2.50929
[1mStep[0m  [24/42], [94mLoss[0m : 2.57832
[1mStep[0m  [28/42], [94mLoss[0m : 2.19948
[1mStep[0m  [32/42], [94mLoss[0m : 2.51888
[1mStep[0m  [36/42], [94mLoss[0m : 2.23558
[1mStep[0m  [40/42], [94mLoss[0m : 2.59549

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31840
[1mStep[0m  [4/42], [94mLoss[0m : 2.32520
[1mStep[0m  [8/42], [94mLoss[0m : 2.60345
[1mStep[0m  [12/42], [94mLoss[0m : 2.43179
[1mStep[0m  [16/42], [94mLoss[0m : 2.64656
[1mStep[0m  [20/42], [94mLoss[0m : 2.28545
[1mStep[0m  [24/42], [94mLoss[0m : 2.29004
[1mStep[0m  [28/42], [94mLoss[0m : 2.31477
[1mStep[0m  [32/42], [94mLoss[0m : 2.37768
[1mStep[0m  [36/42], [94mLoss[0m : 2.36874
[1mStep[0m  [40/42], [94mLoss[0m : 2.44131

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53599
[1mStep[0m  [4/42], [94mLoss[0m : 2.25938
[1mStep[0m  [8/42], [94mLoss[0m : 2.21877
[1mStep[0m  [12/42], [94mLoss[0m : 2.38660
[1mStep[0m  [16/42], [94mLoss[0m : 2.26737
[1mStep[0m  [20/42], [94mLoss[0m : 2.39516
[1mStep[0m  [24/42], [94mLoss[0m : 2.36482
[1mStep[0m  [28/42], [94mLoss[0m : 2.28530
[1mStep[0m  [32/42], [94mLoss[0m : 2.40225
[1mStep[0m  [36/42], [94mLoss[0m : 2.52522
[1mStep[0m  [40/42], [94mLoss[0m : 2.62566

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53033
[1mStep[0m  [4/42], [94mLoss[0m : 2.56258
[1mStep[0m  [8/42], [94mLoss[0m : 2.43560
[1mStep[0m  [12/42], [94mLoss[0m : 2.46351
[1mStep[0m  [16/42], [94mLoss[0m : 2.51713
[1mStep[0m  [20/42], [94mLoss[0m : 2.72260
[1mStep[0m  [24/42], [94mLoss[0m : 2.40127
[1mStep[0m  [28/42], [94mLoss[0m : 2.55968
[1mStep[0m  [32/42], [94mLoss[0m : 2.29763
[1mStep[0m  [36/42], [94mLoss[0m : 2.48140
[1mStep[0m  [40/42], [94mLoss[0m : 2.22158

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10803
[1mStep[0m  [4/42], [94mLoss[0m : 2.46966
[1mStep[0m  [8/42], [94mLoss[0m : 2.27800
[1mStep[0m  [12/42], [94mLoss[0m : 2.33936
[1mStep[0m  [16/42], [94mLoss[0m : 2.32059
[1mStep[0m  [20/42], [94mLoss[0m : 2.35639
[1mStep[0m  [24/42], [94mLoss[0m : 2.42116
[1mStep[0m  [28/42], [94mLoss[0m : 2.49773
[1mStep[0m  [32/42], [94mLoss[0m : 2.48615
[1mStep[0m  [36/42], [94mLoss[0m : 2.63957
[1mStep[0m  [40/42], [94mLoss[0m : 2.40318

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.345, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46765
[1mStep[0m  [4/42], [94mLoss[0m : 2.35513
[1mStep[0m  [8/42], [94mLoss[0m : 2.41345
[1mStep[0m  [12/42], [94mLoss[0m : 2.22952
[1mStep[0m  [16/42], [94mLoss[0m : 2.22718
[1mStep[0m  [20/42], [94mLoss[0m : 2.28315
[1mStep[0m  [24/42], [94mLoss[0m : 2.22634
[1mStep[0m  [28/42], [94mLoss[0m : 2.56948
[1mStep[0m  [32/42], [94mLoss[0m : 2.50282
[1mStep[0m  [36/42], [94mLoss[0m : 2.39544
[1mStep[0m  [40/42], [94mLoss[0m : 2.35628

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.370, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52223
[1mStep[0m  [4/42], [94mLoss[0m : 2.42563
[1mStep[0m  [8/42], [94mLoss[0m : 2.33378
[1mStep[0m  [12/42], [94mLoss[0m : 2.37159
[1mStep[0m  [16/42], [94mLoss[0m : 2.25983
[1mStep[0m  [20/42], [94mLoss[0m : 2.31679
[1mStep[0m  [24/42], [94mLoss[0m : 2.33742
[1mStep[0m  [28/42], [94mLoss[0m : 2.21430
[1mStep[0m  [32/42], [94mLoss[0m : 2.51579
[1mStep[0m  [36/42], [94mLoss[0m : 2.36225
[1mStep[0m  [40/42], [94mLoss[0m : 2.39099

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.363, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38248
[1mStep[0m  [4/42], [94mLoss[0m : 2.18859
[1mStep[0m  [8/42], [94mLoss[0m : 2.34010
[1mStep[0m  [12/42], [94mLoss[0m : 2.43880
[1mStep[0m  [16/42], [94mLoss[0m : 2.43164
[1mStep[0m  [20/42], [94mLoss[0m : 2.24983
[1mStep[0m  [24/42], [94mLoss[0m : 2.28150
[1mStep[0m  [28/42], [94mLoss[0m : 2.46324
[1mStep[0m  [32/42], [94mLoss[0m : 2.24196
[1mStep[0m  [36/42], [94mLoss[0m : 2.64644
[1mStep[0m  [40/42], [94mLoss[0m : 2.31298

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36982
[1mStep[0m  [4/42], [94mLoss[0m : 2.22657
[1mStep[0m  [8/42], [94mLoss[0m : 2.39776
[1mStep[0m  [12/42], [94mLoss[0m : 2.56631
[1mStep[0m  [16/42], [94mLoss[0m : 2.29621
[1mStep[0m  [20/42], [94mLoss[0m : 2.22609
[1mStep[0m  [24/42], [94mLoss[0m : 2.31546
[1mStep[0m  [28/42], [94mLoss[0m : 2.40048
[1mStep[0m  [32/42], [94mLoss[0m : 2.33604
[1mStep[0m  [36/42], [94mLoss[0m : 2.42449
[1mStep[0m  [40/42], [94mLoss[0m : 2.51838

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.369, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32400
[1mStep[0m  [4/42], [94mLoss[0m : 2.39286
[1mStep[0m  [8/42], [94mLoss[0m : 2.53833
[1mStep[0m  [12/42], [94mLoss[0m : 2.50421
[1mStep[0m  [16/42], [94mLoss[0m : 2.22562
[1mStep[0m  [20/42], [94mLoss[0m : 2.47287
[1mStep[0m  [24/42], [94mLoss[0m : 2.46703
[1mStep[0m  [28/42], [94mLoss[0m : 2.30684
[1mStep[0m  [32/42], [94mLoss[0m : 2.47592
[1mStep[0m  [36/42], [94mLoss[0m : 2.40490
[1mStep[0m  [40/42], [94mLoss[0m : 2.19109

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36484
[1mStep[0m  [4/42], [94mLoss[0m : 2.30992
[1mStep[0m  [8/42], [94mLoss[0m : 2.22831
[1mStep[0m  [12/42], [94mLoss[0m : 2.39178
[1mStep[0m  [16/42], [94mLoss[0m : 2.24657
[1mStep[0m  [20/42], [94mLoss[0m : 2.13514
[1mStep[0m  [24/42], [94mLoss[0m : 2.39769
[1mStep[0m  [28/42], [94mLoss[0m : 2.04650
[1mStep[0m  [32/42], [94mLoss[0m : 2.29474
[1mStep[0m  [36/42], [94mLoss[0m : 2.44669
[1mStep[0m  [40/42], [94mLoss[0m : 2.29758

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30109
[1mStep[0m  [4/42], [94mLoss[0m : 2.46043
[1mStep[0m  [8/42], [94mLoss[0m : 2.49579
[1mStep[0m  [12/42], [94mLoss[0m : 2.51549
[1mStep[0m  [16/42], [94mLoss[0m : 2.19010
[1mStep[0m  [20/42], [94mLoss[0m : 2.38840
[1mStep[0m  [24/42], [94mLoss[0m : 2.21905
[1mStep[0m  [28/42], [94mLoss[0m : 2.45890
[1mStep[0m  [32/42], [94mLoss[0m : 2.45080
[1mStep[0m  [36/42], [94mLoss[0m : 2.32703
[1mStep[0m  [40/42], [94mLoss[0m : 2.26827

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35502
[1mStep[0m  [4/42], [94mLoss[0m : 2.30443
[1mStep[0m  [8/42], [94mLoss[0m : 2.23049
[1mStep[0m  [12/42], [94mLoss[0m : 2.23568
[1mStep[0m  [16/42], [94mLoss[0m : 2.25274
[1mStep[0m  [20/42], [94mLoss[0m : 2.38603
[1mStep[0m  [24/42], [94mLoss[0m : 2.49320
[1mStep[0m  [28/42], [94mLoss[0m : 2.21958
[1mStep[0m  [32/42], [94mLoss[0m : 2.74697
[1mStep[0m  [36/42], [94mLoss[0m : 2.46372
[1mStep[0m  [40/42], [94mLoss[0m : 2.33399

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42441
[1mStep[0m  [4/42], [94mLoss[0m : 2.22354
[1mStep[0m  [8/42], [94mLoss[0m : 2.12375
[1mStep[0m  [12/42], [94mLoss[0m : 2.57187
[1mStep[0m  [16/42], [94mLoss[0m : 2.31999
[1mStep[0m  [20/42], [94mLoss[0m : 2.34262
[1mStep[0m  [24/42], [94mLoss[0m : 2.14174
[1mStep[0m  [28/42], [94mLoss[0m : 2.46152
[1mStep[0m  [32/42], [94mLoss[0m : 2.44734
[1mStep[0m  [36/42], [94mLoss[0m : 2.40942
[1mStep[0m  [40/42], [94mLoss[0m : 2.27639

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50246
[1mStep[0m  [4/42], [94mLoss[0m : 2.27057
[1mStep[0m  [8/42], [94mLoss[0m : 2.35465
[1mStep[0m  [12/42], [94mLoss[0m : 2.35693
[1mStep[0m  [16/42], [94mLoss[0m : 2.37708
[1mStep[0m  [20/42], [94mLoss[0m : 2.53671
[1mStep[0m  [24/42], [94mLoss[0m : 2.49338
[1mStep[0m  [28/42], [94mLoss[0m : 2.30559
[1mStep[0m  [32/42], [94mLoss[0m : 2.38357
[1mStep[0m  [36/42], [94mLoss[0m : 2.41824
[1mStep[0m  [40/42], [94mLoss[0m : 2.24485

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.318
====================================

Phase 1 - Evaluation MAE:  2.317952036857605
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.25515
[1mStep[0m  [4/42], [94mLoss[0m : 2.55473
[1mStep[0m  [8/42], [94mLoss[0m : 2.47517
[1mStep[0m  [12/42], [94mLoss[0m : 2.36492
[1mStep[0m  [16/42], [94mLoss[0m : 2.36129
[1mStep[0m  [20/42], [94mLoss[0m : 2.33728
[1mStep[0m  [24/42], [94mLoss[0m : 2.46772
[1mStep[0m  [28/42], [94mLoss[0m : 2.59894
[1mStep[0m  [32/42], [94mLoss[0m : 2.61113
[1mStep[0m  [36/42], [94mLoss[0m : 2.28006
[1mStep[0m  [40/42], [94mLoss[0m : 2.36004

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.314, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37781
[1mStep[0m  [4/42], [94mLoss[0m : 2.50793
[1mStep[0m  [8/42], [94mLoss[0m : 2.35095
[1mStep[0m  [12/42], [94mLoss[0m : 2.38275
[1mStep[0m  [16/42], [94mLoss[0m : 2.62559
[1mStep[0m  [20/42], [94mLoss[0m : 2.33023
[1mStep[0m  [24/42], [94mLoss[0m : 2.36139
[1mStep[0m  [28/42], [94mLoss[0m : 2.49842
[1mStep[0m  [32/42], [94mLoss[0m : 2.28637
[1mStep[0m  [36/42], [94mLoss[0m : 2.30916
[1mStep[0m  [40/42], [94mLoss[0m : 2.58178

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18421
[1mStep[0m  [4/42], [94mLoss[0m : 2.29681
[1mStep[0m  [8/42], [94mLoss[0m : 2.20529
[1mStep[0m  [12/42], [94mLoss[0m : 2.38885
[1mStep[0m  [16/42], [94mLoss[0m : 2.36751
[1mStep[0m  [20/42], [94mLoss[0m : 2.11658
[1mStep[0m  [24/42], [94mLoss[0m : 2.43538
[1mStep[0m  [28/42], [94mLoss[0m : 2.42043
[1mStep[0m  [32/42], [94mLoss[0m : 2.34485
[1mStep[0m  [36/42], [94mLoss[0m : 2.34395
[1mStep[0m  [40/42], [94mLoss[0m : 2.30147

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18451
[1mStep[0m  [4/42], [94mLoss[0m : 2.21655
[1mStep[0m  [8/42], [94mLoss[0m : 2.39365
[1mStep[0m  [12/42], [94mLoss[0m : 2.18182
[1mStep[0m  [16/42], [94mLoss[0m : 2.19726
[1mStep[0m  [20/42], [94mLoss[0m : 2.36964
[1mStep[0m  [24/42], [94mLoss[0m : 2.16943
[1mStep[0m  [28/42], [94mLoss[0m : 2.25775
[1mStep[0m  [32/42], [94mLoss[0m : 2.38587
[1mStep[0m  [36/42], [94mLoss[0m : 2.18483
[1mStep[0m  [40/42], [94mLoss[0m : 2.43702

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11926
[1mStep[0m  [4/42], [94mLoss[0m : 2.25559
[1mStep[0m  [8/42], [94mLoss[0m : 2.40915
[1mStep[0m  [12/42], [94mLoss[0m : 2.51238
[1mStep[0m  [16/42], [94mLoss[0m : 2.12395
[1mStep[0m  [20/42], [94mLoss[0m : 2.28685
[1mStep[0m  [24/42], [94mLoss[0m : 2.32925
[1mStep[0m  [28/42], [94mLoss[0m : 2.09282
[1mStep[0m  [32/42], [94mLoss[0m : 2.19800
[1mStep[0m  [36/42], [94mLoss[0m : 2.22928
[1mStep[0m  [40/42], [94mLoss[0m : 2.25477

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34672
[1mStep[0m  [4/42], [94mLoss[0m : 2.28123
[1mStep[0m  [8/42], [94mLoss[0m : 2.20222
[1mStep[0m  [12/42], [94mLoss[0m : 2.35836
[1mStep[0m  [16/42], [94mLoss[0m : 2.30908
[1mStep[0m  [20/42], [94mLoss[0m : 2.16114
[1mStep[0m  [24/42], [94mLoss[0m : 1.92926
[1mStep[0m  [28/42], [94mLoss[0m : 2.14708
[1mStep[0m  [32/42], [94mLoss[0m : 2.35412
[1mStep[0m  [36/42], [94mLoss[0m : 2.16423
[1mStep[0m  [40/42], [94mLoss[0m : 2.25136

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22333
[1mStep[0m  [4/42], [94mLoss[0m : 2.19501
[1mStep[0m  [8/42], [94mLoss[0m : 1.99168
[1mStep[0m  [12/42], [94mLoss[0m : 2.22299
[1mStep[0m  [16/42], [94mLoss[0m : 2.09412
[1mStep[0m  [20/42], [94mLoss[0m : 2.06138
[1mStep[0m  [24/42], [94mLoss[0m : 2.60514
[1mStep[0m  [28/42], [94mLoss[0m : 2.09197
[1mStep[0m  [32/42], [94mLoss[0m : 2.25048
[1mStep[0m  [36/42], [94mLoss[0m : 2.30324
[1mStep[0m  [40/42], [94mLoss[0m : 2.06507

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24776
[1mStep[0m  [4/42], [94mLoss[0m : 2.10945
[1mStep[0m  [8/42], [94mLoss[0m : 2.20763
[1mStep[0m  [12/42], [94mLoss[0m : 2.01516
[1mStep[0m  [16/42], [94mLoss[0m : 1.98626
[1mStep[0m  [20/42], [94mLoss[0m : 2.01295
[1mStep[0m  [24/42], [94mLoss[0m : 2.11049
[1mStep[0m  [28/42], [94mLoss[0m : 1.97586
[1mStep[0m  [32/42], [94mLoss[0m : 2.09181
[1mStep[0m  [36/42], [94mLoss[0m : 1.98254
[1mStep[0m  [40/42], [94mLoss[0m : 1.90640

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24780
[1mStep[0m  [4/42], [94mLoss[0m : 1.98528
[1mStep[0m  [8/42], [94mLoss[0m : 2.03072
[1mStep[0m  [12/42], [94mLoss[0m : 2.18145
[1mStep[0m  [16/42], [94mLoss[0m : 2.09573
[1mStep[0m  [20/42], [94mLoss[0m : 2.17561
[1mStep[0m  [24/42], [94mLoss[0m : 2.02154
[1mStep[0m  [28/42], [94mLoss[0m : 2.02037
[1mStep[0m  [32/42], [94mLoss[0m : 1.96818
[1mStep[0m  [36/42], [94mLoss[0m : 2.01084
[1mStep[0m  [40/42], [94mLoss[0m : 2.13398

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09932
[1mStep[0m  [4/42], [94mLoss[0m : 2.00202
[1mStep[0m  [8/42], [94mLoss[0m : 1.99368
[1mStep[0m  [12/42], [94mLoss[0m : 1.97804
[1mStep[0m  [16/42], [94mLoss[0m : 2.08504
[1mStep[0m  [20/42], [94mLoss[0m : 1.83822
[1mStep[0m  [24/42], [94mLoss[0m : 2.08484
[1mStep[0m  [28/42], [94mLoss[0m : 1.98263
[1mStep[0m  [32/42], [94mLoss[0m : 1.86648
[1mStep[0m  [36/42], [94mLoss[0m : 1.92324
[1mStep[0m  [40/42], [94mLoss[0m : 1.98570

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78597
[1mStep[0m  [4/42], [94mLoss[0m : 2.20792
[1mStep[0m  [8/42], [94mLoss[0m : 1.91003
[1mStep[0m  [12/42], [94mLoss[0m : 2.00754
[1mStep[0m  [16/42], [94mLoss[0m : 1.87689
[1mStep[0m  [20/42], [94mLoss[0m : 1.79631
[1mStep[0m  [24/42], [94mLoss[0m : 1.94938
[1mStep[0m  [28/42], [94mLoss[0m : 1.78370
[1mStep[0m  [32/42], [94mLoss[0m : 1.92627
[1mStep[0m  [36/42], [94mLoss[0m : 1.90067
[1mStep[0m  [40/42], [94mLoss[0m : 2.10400

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74870
[1mStep[0m  [4/42], [94mLoss[0m : 1.95206
[1mStep[0m  [8/42], [94mLoss[0m : 2.01366
[1mStep[0m  [12/42], [94mLoss[0m : 1.90003
[1mStep[0m  [16/42], [94mLoss[0m : 2.02147
[1mStep[0m  [20/42], [94mLoss[0m : 1.87764
[1mStep[0m  [24/42], [94mLoss[0m : 1.93488
[1mStep[0m  [28/42], [94mLoss[0m : 1.78822
[1mStep[0m  [32/42], [94mLoss[0m : 1.91937
[1mStep[0m  [36/42], [94mLoss[0m : 1.94897
[1mStep[0m  [40/42], [94mLoss[0m : 1.83913

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71217
[1mStep[0m  [4/42], [94mLoss[0m : 1.90459
[1mStep[0m  [8/42], [94mLoss[0m : 1.72244
[1mStep[0m  [12/42], [94mLoss[0m : 1.87098
[1mStep[0m  [16/42], [94mLoss[0m : 2.07639
[1mStep[0m  [20/42], [94mLoss[0m : 1.86910
[1mStep[0m  [24/42], [94mLoss[0m : 1.89609
[1mStep[0m  [28/42], [94mLoss[0m : 1.72591
[1mStep[0m  [32/42], [94mLoss[0m : 2.03087
[1mStep[0m  [36/42], [94mLoss[0m : 1.77946
[1mStep[0m  [40/42], [94mLoss[0m : 1.94331

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79701
[1mStep[0m  [4/42], [94mLoss[0m : 1.74217
[1mStep[0m  [8/42], [94mLoss[0m : 1.73332
[1mStep[0m  [12/42], [94mLoss[0m : 1.94159
[1mStep[0m  [16/42], [94mLoss[0m : 1.82343
[1mStep[0m  [20/42], [94mLoss[0m : 1.79826
[1mStep[0m  [24/42], [94mLoss[0m : 1.85804
[1mStep[0m  [28/42], [94mLoss[0m : 1.84658
[1mStep[0m  [32/42], [94mLoss[0m : 1.85455
[1mStep[0m  [36/42], [94mLoss[0m : 1.73193
[1mStep[0m  [40/42], [94mLoss[0m : 1.79421

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92213
[1mStep[0m  [4/42], [94mLoss[0m : 1.73487
[1mStep[0m  [8/42], [94mLoss[0m : 1.80177
[1mStep[0m  [12/42], [94mLoss[0m : 1.85699
[1mStep[0m  [16/42], [94mLoss[0m : 1.75878
[1mStep[0m  [20/42], [94mLoss[0m : 1.93774
[1mStep[0m  [24/42], [94mLoss[0m : 1.77234
[1mStep[0m  [28/42], [94mLoss[0m : 1.76165
[1mStep[0m  [32/42], [94mLoss[0m : 1.65149
[1mStep[0m  [36/42], [94mLoss[0m : 1.80802
[1mStep[0m  [40/42], [94mLoss[0m : 1.87469

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.533, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74271
[1mStep[0m  [4/42], [94mLoss[0m : 1.61307
[1mStep[0m  [8/42], [94mLoss[0m : 1.89059
[1mStep[0m  [12/42], [94mLoss[0m : 1.75952
[1mStep[0m  [16/42], [94mLoss[0m : 1.84558
[1mStep[0m  [20/42], [94mLoss[0m : 1.78928
[1mStep[0m  [24/42], [94mLoss[0m : 1.92908
[1mStep[0m  [28/42], [94mLoss[0m : 1.78857
[1mStep[0m  [32/42], [94mLoss[0m : 1.90374
[1mStep[0m  [36/42], [94mLoss[0m : 1.79809
[1mStep[0m  [40/42], [94mLoss[0m : 1.71889

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63639
[1mStep[0m  [4/42], [94mLoss[0m : 1.55491
[1mStep[0m  [8/42], [94mLoss[0m : 1.70184
[1mStep[0m  [12/42], [94mLoss[0m : 1.73339
[1mStep[0m  [16/42], [94mLoss[0m : 1.70584
[1mStep[0m  [20/42], [94mLoss[0m : 1.65332
[1mStep[0m  [24/42], [94mLoss[0m : 1.74018
[1mStep[0m  [28/42], [94mLoss[0m : 1.75506
[1mStep[0m  [32/42], [94mLoss[0m : 1.57478
[1mStep[0m  [36/42], [94mLoss[0m : 1.85886
[1mStep[0m  [40/42], [94mLoss[0m : 1.86408

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66958
[1mStep[0m  [4/42], [94mLoss[0m : 1.53056
[1mStep[0m  [8/42], [94mLoss[0m : 1.76721
[1mStep[0m  [12/42], [94mLoss[0m : 1.53887
[1mStep[0m  [16/42], [94mLoss[0m : 1.65208
[1mStep[0m  [20/42], [94mLoss[0m : 1.67182
[1mStep[0m  [24/42], [94mLoss[0m : 1.68698
[1mStep[0m  [28/42], [94mLoss[0m : 1.75959
[1mStep[0m  [32/42], [94mLoss[0m : 1.74401
[1mStep[0m  [36/42], [94mLoss[0m : 1.78688
[1mStep[0m  [40/42], [94mLoss[0m : 1.78554

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64828
[1mStep[0m  [4/42], [94mLoss[0m : 1.55554
[1mStep[0m  [8/42], [94mLoss[0m : 1.67049
[1mStep[0m  [12/42], [94mLoss[0m : 1.54434
[1mStep[0m  [16/42], [94mLoss[0m : 1.75722
[1mStep[0m  [20/42], [94mLoss[0m : 1.66915
[1mStep[0m  [24/42], [94mLoss[0m : 1.55694
[1mStep[0m  [28/42], [94mLoss[0m : 1.62543
[1mStep[0m  [32/42], [94mLoss[0m : 1.77047
[1mStep[0m  [36/42], [94mLoss[0m : 1.55345
[1mStep[0m  [40/42], [94mLoss[0m : 1.79708

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.667, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57080
[1mStep[0m  [4/42], [94mLoss[0m : 1.52133
[1mStep[0m  [8/42], [94mLoss[0m : 1.67299
[1mStep[0m  [12/42], [94mLoss[0m : 1.56803
[1mStep[0m  [16/42], [94mLoss[0m : 1.65186
[1mStep[0m  [20/42], [94mLoss[0m : 1.66440
[1mStep[0m  [24/42], [94mLoss[0m : 1.82361
[1mStep[0m  [28/42], [94mLoss[0m : 1.55624
[1mStep[0m  [32/42], [94mLoss[0m : 1.58997
[1mStep[0m  [36/42], [94mLoss[0m : 1.78513
[1mStep[0m  [40/42], [94mLoss[0m : 1.56947

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.455, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54136
[1mStep[0m  [4/42], [94mLoss[0m : 1.65336
[1mStep[0m  [8/42], [94mLoss[0m : 1.61788
[1mStep[0m  [12/42], [94mLoss[0m : 1.62305
[1mStep[0m  [16/42], [94mLoss[0m : 1.61271
[1mStep[0m  [20/42], [94mLoss[0m : 1.62264
[1mStep[0m  [24/42], [94mLoss[0m : 1.51067
[1mStep[0m  [28/42], [94mLoss[0m : 1.62718
[1mStep[0m  [32/42], [94mLoss[0m : 1.73594
[1mStep[0m  [36/42], [94mLoss[0m : 1.55389
[1mStep[0m  [40/42], [94mLoss[0m : 1.66659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.461, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55398
[1mStep[0m  [4/42], [94mLoss[0m : 1.63373
[1mStep[0m  [8/42], [94mLoss[0m : 1.52253
[1mStep[0m  [12/42], [94mLoss[0m : 1.49510
[1mStep[0m  [16/42], [94mLoss[0m : 1.69738
[1mStep[0m  [20/42], [94mLoss[0m : 1.63105
[1mStep[0m  [24/42], [94mLoss[0m : 1.54900
[1mStep[0m  [28/42], [94mLoss[0m : 1.43236
[1mStep[0m  [32/42], [94mLoss[0m : 1.53309
[1mStep[0m  [36/42], [94mLoss[0m : 1.59633
[1mStep[0m  [40/42], [94mLoss[0m : 1.69874

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.445, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.33453
[1mStep[0m  [4/42], [94mLoss[0m : 1.64057
[1mStep[0m  [8/42], [94mLoss[0m : 1.66876
[1mStep[0m  [12/42], [94mLoss[0m : 1.49126
[1mStep[0m  [16/42], [94mLoss[0m : 1.68740
[1mStep[0m  [20/42], [94mLoss[0m : 1.57480
[1mStep[0m  [24/42], [94mLoss[0m : 1.53961
[1mStep[0m  [28/42], [94mLoss[0m : 1.45405
[1mStep[0m  [32/42], [94mLoss[0m : 1.65838
[1mStep[0m  [36/42], [94mLoss[0m : 1.59276
[1mStep[0m  [40/42], [94mLoss[0m : 1.51123

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.488, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54477
[1mStep[0m  [4/42], [94mLoss[0m : 1.53611
[1mStep[0m  [8/42], [94mLoss[0m : 1.79996
[1mStep[0m  [12/42], [94mLoss[0m : 1.57641
[1mStep[0m  [16/42], [94mLoss[0m : 1.43309
[1mStep[0m  [20/42], [94mLoss[0m : 1.52196
[1mStep[0m  [24/42], [94mLoss[0m : 1.62150
[1mStep[0m  [28/42], [94mLoss[0m : 1.50993
[1mStep[0m  [32/42], [94mLoss[0m : 1.54755
[1mStep[0m  [36/42], [94mLoss[0m : 1.43778
[1mStep[0m  [40/42], [94mLoss[0m : 1.53646

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56748
[1mStep[0m  [4/42], [94mLoss[0m : 1.64525
[1mStep[0m  [8/42], [94mLoss[0m : 1.45706
[1mStep[0m  [12/42], [94mLoss[0m : 1.50042
[1mStep[0m  [16/42], [94mLoss[0m : 1.43470
[1mStep[0m  [20/42], [94mLoss[0m : 1.57748
[1mStep[0m  [24/42], [94mLoss[0m : 1.46139
[1mStep[0m  [28/42], [94mLoss[0m : 1.46087
[1mStep[0m  [32/42], [94mLoss[0m : 1.51782
[1mStep[0m  [36/42], [94mLoss[0m : 1.44216
[1mStep[0m  [40/42], [94mLoss[0m : 1.50247

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.33427
[1mStep[0m  [4/42], [94mLoss[0m : 1.48327
[1mStep[0m  [8/42], [94mLoss[0m : 1.48973
[1mStep[0m  [12/42], [94mLoss[0m : 1.43724
[1mStep[0m  [16/42], [94mLoss[0m : 1.47466
[1mStep[0m  [20/42], [94mLoss[0m : 1.50063
[1mStep[0m  [24/42], [94mLoss[0m : 1.54726
[1mStep[0m  [28/42], [94mLoss[0m : 1.55233
[1mStep[0m  [32/42], [94mLoss[0m : 1.58933
[1mStep[0m  [36/42], [94mLoss[0m : 1.53919
[1mStep[0m  [40/42], [94mLoss[0m : 1.57909

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43523
[1mStep[0m  [4/42], [94mLoss[0m : 1.51866
[1mStep[0m  [8/42], [94mLoss[0m : 1.58520
[1mStep[0m  [12/42], [94mLoss[0m : 1.51803
[1mStep[0m  [16/42], [94mLoss[0m : 1.86558
[1mStep[0m  [20/42], [94mLoss[0m : 1.51564
[1mStep[0m  [24/42], [94mLoss[0m : 1.46225
[1mStep[0m  [28/42], [94mLoss[0m : 1.45932
[1mStep[0m  [32/42], [94mLoss[0m : 1.49130
[1mStep[0m  [36/42], [94mLoss[0m : 1.53446
[1mStep[0m  [40/42], [94mLoss[0m : 1.52428

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.461, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52618
[1mStep[0m  [4/42], [94mLoss[0m : 1.42703
[1mStep[0m  [8/42], [94mLoss[0m : 1.47932
[1mStep[0m  [12/42], [94mLoss[0m : 1.52595
[1mStep[0m  [16/42], [94mLoss[0m : 1.43660
[1mStep[0m  [20/42], [94mLoss[0m : 1.40344
[1mStep[0m  [24/42], [94mLoss[0m : 1.50137
[1mStep[0m  [28/42], [94mLoss[0m : 1.43472
[1mStep[0m  [32/42], [94mLoss[0m : 1.39033
[1mStep[0m  [36/42], [94mLoss[0m : 1.57557
[1mStep[0m  [40/42], [94mLoss[0m : 1.61031

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.492, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38084
[1mStep[0m  [4/42], [94mLoss[0m : 1.53204
[1mStep[0m  [8/42], [94mLoss[0m : 1.46132
[1mStep[0m  [12/42], [94mLoss[0m : 1.41833
[1mStep[0m  [16/42], [94mLoss[0m : 1.46671
[1mStep[0m  [20/42], [94mLoss[0m : 1.33110
[1mStep[0m  [24/42], [94mLoss[0m : 1.30257
[1mStep[0m  [28/42], [94mLoss[0m : 1.45491
[1mStep[0m  [32/42], [94mLoss[0m : 1.37378
[1mStep[0m  [36/42], [94mLoss[0m : 1.56561
[1mStep[0m  [40/42], [94mLoss[0m : 1.54030

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.436, [92mTest[0m: 2.550, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48155
[1mStep[0m  [4/42], [94mLoss[0m : 1.46921
[1mStep[0m  [8/42], [94mLoss[0m : 1.37953
[1mStep[0m  [12/42], [94mLoss[0m : 1.43481
[1mStep[0m  [16/42], [94mLoss[0m : 1.42262
[1mStep[0m  [20/42], [94mLoss[0m : 1.38933
[1mStep[0m  [24/42], [94mLoss[0m : 1.43121
[1mStep[0m  [28/42], [94mLoss[0m : 1.55931
[1mStep[0m  [32/42], [94mLoss[0m : 1.27964
[1mStep[0m  [36/42], [94mLoss[0m : 1.49078
[1mStep[0m  [40/42], [94mLoss[0m : 1.50623

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.438, [92mTest[0m: 2.488, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.465
====================================

Phase 2 - Evaluation MAE:  2.4647296326501027
MAE score P1      2.317952
MAE score P2       2.46473
loss              1.435867
learning_rate         0.01
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 20, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.98981
[1mStep[0m  [8/84], [94mLoss[0m : 6.68078
[1mStep[0m  [16/84], [94mLoss[0m : 3.93289
[1mStep[0m  [24/84], [94mLoss[0m : 3.01504
[1mStep[0m  [32/84], [94mLoss[0m : 2.71465
[1mStep[0m  [40/84], [94mLoss[0m : 3.23009
[1mStep[0m  [48/84], [94mLoss[0m : 2.67770
[1mStep[0m  [56/84], [94mLoss[0m : 2.71070
[1mStep[0m  [64/84], [94mLoss[0m : 2.43810
[1mStep[0m  [72/84], [94mLoss[0m : 2.59751
[1mStep[0m  [80/84], [94mLoss[0m : 2.49991

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.644, [92mTest[0m: 10.911, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41598
[1mStep[0m  [8/84], [94mLoss[0m : 2.49597
[1mStep[0m  [16/84], [94mLoss[0m : 2.40872
[1mStep[0m  [24/84], [94mLoss[0m : 2.50012
[1mStep[0m  [32/84], [94mLoss[0m : 2.21991
[1mStep[0m  [40/84], [94mLoss[0m : 3.05520
[1mStep[0m  [48/84], [94mLoss[0m : 2.45747
[1mStep[0m  [56/84], [94mLoss[0m : 2.50614
[1mStep[0m  [64/84], [94mLoss[0m : 2.56283
[1mStep[0m  [72/84], [94mLoss[0m : 2.74893
[1mStep[0m  [80/84], [94mLoss[0m : 2.50445

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.635, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61503
[1mStep[0m  [8/84], [94mLoss[0m : 2.77712
[1mStep[0m  [16/84], [94mLoss[0m : 2.61767
[1mStep[0m  [24/84], [94mLoss[0m : 2.26394
[1mStep[0m  [32/84], [94mLoss[0m : 2.73208
[1mStep[0m  [40/84], [94mLoss[0m : 2.32796
[1mStep[0m  [48/84], [94mLoss[0m : 2.23827
[1mStep[0m  [56/84], [94mLoss[0m : 2.67715
[1mStep[0m  [64/84], [94mLoss[0m : 2.36324
[1mStep[0m  [72/84], [94mLoss[0m : 2.51256
[1mStep[0m  [80/84], [94mLoss[0m : 2.63515

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.602, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35161
[1mStep[0m  [8/84], [94mLoss[0m : 2.62241
[1mStep[0m  [16/84], [94mLoss[0m : 2.48455
[1mStep[0m  [24/84], [94mLoss[0m : 2.19758
[1mStep[0m  [32/84], [94mLoss[0m : 2.52004
[1mStep[0m  [40/84], [94mLoss[0m : 2.55985
[1mStep[0m  [48/84], [94mLoss[0m : 2.69360
[1mStep[0m  [56/84], [94mLoss[0m : 2.27809
[1mStep[0m  [64/84], [94mLoss[0m : 2.28644
[1mStep[0m  [72/84], [94mLoss[0m : 2.41727
[1mStep[0m  [80/84], [94mLoss[0m : 2.75495

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.538, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45534
[1mStep[0m  [8/84], [94mLoss[0m : 2.47041
[1mStep[0m  [16/84], [94mLoss[0m : 2.59211
[1mStep[0m  [24/84], [94mLoss[0m : 2.68755
[1mStep[0m  [32/84], [94mLoss[0m : 2.33213
[1mStep[0m  [40/84], [94mLoss[0m : 2.36684
[1mStep[0m  [48/84], [94mLoss[0m : 2.61019
[1mStep[0m  [56/84], [94mLoss[0m : 2.51014
[1mStep[0m  [64/84], [94mLoss[0m : 2.30254
[1mStep[0m  [72/84], [94mLoss[0m : 2.67782
[1mStep[0m  [80/84], [94mLoss[0m : 2.59209

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.535, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18960
[1mStep[0m  [8/84], [94mLoss[0m : 2.28477
[1mStep[0m  [16/84], [94mLoss[0m : 2.36263
[1mStep[0m  [24/84], [94mLoss[0m : 2.51690
[1mStep[0m  [32/84], [94mLoss[0m : 2.24897
[1mStep[0m  [40/84], [94mLoss[0m : 2.50745
[1mStep[0m  [48/84], [94mLoss[0m : 2.60058
[1mStep[0m  [56/84], [94mLoss[0m : 2.59887
[1mStep[0m  [64/84], [94mLoss[0m : 2.18934
[1mStep[0m  [72/84], [94mLoss[0m : 2.58409
[1mStep[0m  [80/84], [94mLoss[0m : 2.34517

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22456
[1mStep[0m  [8/84], [94mLoss[0m : 2.48280
[1mStep[0m  [16/84], [94mLoss[0m : 2.32557
[1mStep[0m  [24/84], [94mLoss[0m : 2.68614
[1mStep[0m  [32/84], [94mLoss[0m : 2.64841
[1mStep[0m  [40/84], [94mLoss[0m : 2.20043
[1mStep[0m  [48/84], [94mLoss[0m : 2.55279
[1mStep[0m  [56/84], [94mLoss[0m : 2.58967
[1mStep[0m  [64/84], [94mLoss[0m : 2.45191
[1mStep[0m  [72/84], [94mLoss[0m : 2.30275
[1mStep[0m  [80/84], [94mLoss[0m : 2.70867

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59425
[1mStep[0m  [8/84], [94mLoss[0m : 2.26469
[1mStep[0m  [16/84], [94mLoss[0m : 2.15206
[1mStep[0m  [24/84], [94mLoss[0m : 2.33888
[1mStep[0m  [32/84], [94mLoss[0m : 2.10429
[1mStep[0m  [40/84], [94mLoss[0m : 2.29423
[1mStep[0m  [48/84], [94mLoss[0m : 2.09600
[1mStep[0m  [56/84], [94mLoss[0m : 2.53775
[1mStep[0m  [64/84], [94mLoss[0m : 2.46406
[1mStep[0m  [72/84], [94mLoss[0m : 2.54803
[1mStep[0m  [80/84], [94mLoss[0m : 2.60795

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51581
[1mStep[0m  [8/84], [94mLoss[0m : 2.21647
[1mStep[0m  [16/84], [94mLoss[0m : 2.64405
[1mStep[0m  [24/84], [94mLoss[0m : 2.52631
[1mStep[0m  [32/84], [94mLoss[0m : 2.46001
[1mStep[0m  [40/84], [94mLoss[0m : 2.26831
[1mStep[0m  [48/84], [94mLoss[0m : 2.51527
[1mStep[0m  [56/84], [94mLoss[0m : 2.52151
[1mStep[0m  [64/84], [94mLoss[0m : 2.33316
[1mStep[0m  [72/84], [94mLoss[0m : 2.42233
[1mStep[0m  [80/84], [94mLoss[0m : 2.53932

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60910
[1mStep[0m  [8/84], [94mLoss[0m : 2.42933
[1mStep[0m  [16/84], [94mLoss[0m : 2.30238
[1mStep[0m  [24/84], [94mLoss[0m : 2.31097
[1mStep[0m  [32/84], [94mLoss[0m : 2.05614
[1mStep[0m  [40/84], [94mLoss[0m : 2.35931
[1mStep[0m  [48/84], [94mLoss[0m : 2.46066
[1mStep[0m  [56/84], [94mLoss[0m : 2.27846
[1mStep[0m  [64/84], [94mLoss[0m : 2.24008
[1mStep[0m  [72/84], [94mLoss[0m : 2.47124
[1mStep[0m  [80/84], [94mLoss[0m : 2.49982

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22096
[1mStep[0m  [8/84], [94mLoss[0m : 2.11378
[1mStep[0m  [16/84], [94mLoss[0m : 2.48527
[1mStep[0m  [24/84], [94mLoss[0m : 2.39672
[1mStep[0m  [32/84], [94mLoss[0m : 2.32105
[1mStep[0m  [40/84], [94mLoss[0m : 2.38176
[1mStep[0m  [48/84], [94mLoss[0m : 2.47056
[1mStep[0m  [56/84], [94mLoss[0m : 2.38343
[1mStep[0m  [64/84], [94mLoss[0m : 2.25193
[1mStep[0m  [72/84], [94mLoss[0m : 2.61371
[1mStep[0m  [80/84], [94mLoss[0m : 2.51380

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54207
[1mStep[0m  [8/84], [94mLoss[0m : 2.39990
[1mStep[0m  [16/84], [94mLoss[0m : 2.28875
[1mStep[0m  [24/84], [94mLoss[0m : 2.47039
[1mStep[0m  [32/84], [94mLoss[0m : 2.52774
[1mStep[0m  [40/84], [94mLoss[0m : 2.22802
[1mStep[0m  [48/84], [94mLoss[0m : 2.51007
[1mStep[0m  [56/84], [94mLoss[0m : 2.07127
[1mStep[0m  [64/84], [94mLoss[0m : 2.29253
[1mStep[0m  [72/84], [94mLoss[0m : 2.37197
[1mStep[0m  [80/84], [94mLoss[0m : 2.47063

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20031
[1mStep[0m  [8/84], [94mLoss[0m : 2.56309
[1mStep[0m  [16/84], [94mLoss[0m : 2.56312
[1mStep[0m  [24/84], [94mLoss[0m : 2.36741
[1mStep[0m  [32/84], [94mLoss[0m : 2.39403
[1mStep[0m  [40/84], [94mLoss[0m : 2.07115
[1mStep[0m  [48/84], [94mLoss[0m : 2.25537
[1mStep[0m  [56/84], [94mLoss[0m : 2.58977
[1mStep[0m  [64/84], [94mLoss[0m : 2.12640
[1mStep[0m  [72/84], [94mLoss[0m : 2.51933
[1mStep[0m  [80/84], [94mLoss[0m : 2.47966

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17621
[1mStep[0m  [8/84], [94mLoss[0m : 2.42333
[1mStep[0m  [16/84], [94mLoss[0m : 2.30575
[1mStep[0m  [24/84], [94mLoss[0m : 2.81296
[1mStep[0m  [32/84], [94mLoss[0m : 2.23397
[1mStep[0m  [40/84], [94mLoss[0m : 2.41184
[1mStep[0m  [48/84], [94mLoss[0m : 2.40294
[1mStep[0m  [56/84], [94mLoss[0m : 2.27152
[1mStep[0m  [64/84], [94mLoss[0m : 2.21461
[1mStep[0m  [72/84], [94mLoss[0m : 2.36870
[1mStep[0m  [80/84], [94mLoss[0m : 2.60492

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22931
[1mStep[0m  [8/84], [94mLoss[0m : 2.64931
[1mStep[0m  [16/84], [94mLoss[0m : 2.67715
[1mStep[0m  [24/84], [94mLoss[0m : 2.53735
[1mStep[0m  [32/84], [94mLoss[0m : 2.47734
[1mStep[0m  [40/84], [94mLoss[0m : 2.54040
[1mStep[0m  [48/84], [94mLoss[0m : 2.21123
[1mStep[0m  [56/84], [94mLoss[0m : 2.35748
[1mStep[0m  [64/84], [94mLoss[0m : 2.29546
[1mStep[0m  [72/84], [94mLoss[0m : 2.21321
[1mStep[0m  [80/84], [94mLoss[0m : 2.33141

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23868
[1mStep[0m  [8/84], [94mLoss[0m : 2.15957
[1mStep[0m  [16/84], [94mLoss[0m : 2.58949
[1mStep[0m  [24/84], [94mLoss[0m : 2.46900
[1mStep[0m  [32/84], [94mLoss[0m : 2.13541
[1mStep[0m  [40/84], [94mLoss[0m : 2.66146
[1mStep[0m  [48/84], [94mLoss[0m : 2.34573
[1mStep[0m  [56/84], [94mLoss[0m : 2.27434
[1mStep[0m  [64/84], [94mLoss[0m : 2.54539
[1mStep[0m  [72/84], [94mLoss[0m : 2.33003
[1mStep[0m  [80/84], [94mLoss[0m : 2.84387

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65362
[1mStep[0m  [8/84], [94mLoss[0m : 2.22943
[1mStep[0m  [16/84], [94mLoss[0m : 2.24918
[1mStep[0m  [24/84], [94mLoss[0m : 2.56966
[1mStep[0m  [32/84], [94mLoss[0m : 2.05449
[1mStep[0m  [40/84], [94mLoss[0m : 2.65930
[1mStep[0m  [48/84], [94mLoss[0m : 2.48407
[1mStep[0m  [56/84], [94mLoss[0m : 2.29639
[1mStep[0m  [64/84], [94mLoss[0m : 2.51074
[1mStep[0m  [72/84], [94mLoss[0m : 2.75206
[1mStep[0m  [80/84], [94mLoss[0m : 2.39682

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38267
[1mStep[0m  [8/84], [94mLoss[0m : 2.40480
[1mStep[0m  [16/84], [94mLoss[0m : 2.66927
[1mStep[0m  [24/84], [94mLoss[0m : 2.34976
[1mStep[0m  [32/84], [94mLoss[0m : 2.29113
[1mStep[0m  [40/84], [94mLoss[0m : 2.39004
[1mStep[0m  [48/84], [94mLoss[0m : 2.44690
[1mStep[0m  [56/84], [94mLoss[0m : 2.00595
[1mStep[0m  [64/84], [94mLoss[0m : 2.22867
[1mStep[0m  [72/84], [94mLoss[0m : 2.30244
[1mStep[0m  [80/84], [94mLoss[0m : 2.32916

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28400
[1mStep[0m  [8/84], [94mLoss[0m : 2.58334
[1mStep[0m  [16/84], [94mLoss[0m : 2.31584
[1mStep[0m  [24/84], [94mLoss[0m : 2.41852
[1mStep[0m  [32/84], [94mLoss[0m : 2.13529
[1mStep[0m  [40/84], [94mLoss[0m : 2.25424
[1mStep[0m  [48/84], [94mLoss[0m : 2.35249
[1mStep[0m  [56/84], [94mLoss[0m : 2.53732
[1mStep[0m  [64/84], [94mLoss[0m : 2.66581
[1mStep[0m  [72/84], [94mLoss[0m : 2.49644
[1mStep[0m  [80/84], [94mLoss[0m : 2.22209

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17462
[1mStep[0m  [8/84], [94mLoss[0m : 2.43462
[1mStep[0m  [16/84], [94mLoss[0m : 2.11707
[1mStep[0m  [24/84], [94mLoss[0m : 2.28636
[1mStep[0m  [32/84], [94mLoss[0m : 2.67864
[1mStep[0m  [40/84], [94mLoss[0m : 2.38150
[1mStep[0m  [48/84], [94mLoss[0m : 2.49859
[1mStep[0m  [56/84], [94mLoss[0m : 2.31237
[1mStep[0m  [64/84], [94mLoss[0m : 2.54979
[1mStep[0m  [72/84], [94mLoss[0m : 2.18046
[1mStep[0m  [80/84], [94mLoss[0m : 2.47418

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33075
[1mStep[0m  [8/84], [94mLoss[0m : 2.31443
[1mStep[0m  [16/84], [94mLoss[0m : 2.28127
[1mStep[0m  [24/84], [94mLoss[0m : 2.56655
[1mStep[0m  [32/84], [94mLoss[0m : 2.45199
[1mStep[0m  [40/84], [94mLoss[0m : 2.29023
[1mStep[0m  [48/84], [94mLoss[0m : 2.61950
[1mStep[0m  [56/84], [94mLoss[0m : 2.68233
[1mStep[0m  [64/84], [94mLoss[0m : 2.26580
[1mStep[0m  [72/84], [94mLoss[0m : 2.54270
[1mStep[0m  [80/84], [94mLoss[0m : 2.53537

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.438, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59066
[1mStep[0m  [8/84], [94mLoss[0m : 2.38226
[1mStep[0m  [16/84], [94mLoss[0m : 2.43960
[1mStep[0m  [24/84], [94mLoss[0m : 2.25350
[1mStep[0m  [32/84], [94mLoss[0m : 2.48579
[1mStep[0m  [40/84], [94mLoss[0m : 2.69068
[1mStep[0m  [48/84], [94mLoss[0m : 2.37134
[1mStep[0m  [56/84], [94mLoss[0m : 2.26474
[1mStep[0m  [64/84], [94mLoss[0m : 2.31558
[1mStep[0m  [72/84], [94mLoss[0m : 2.29343
[1mStep[0m  [80/84], [94mLoss[0m : 2.39071

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13035
[1mStep[0m  [8/84], [94mLoss[0m : 2.39010
[1mStep[0m  [16/84], [94mLoss[0m : 2.15951
[1mStep[0m  [24/84], [94mLoss[0m : 2.38056
[1mStep[0m  [32/84], [94mLoss[0m : 2.44089
[1mStep[0m  [40/84], [94mLoss[0m : 2.12414
[1mStep[0m  [48/84], [94mLoss[0m : 2.43884
[1mStep[0m  [56/84], [94mLoss[0m : 2.24897
[1mStep[0m  [64/84], [94mLoss[0m : 2.33825
[1mStep[0m  [72/84], [94mLoss[0m : 2.31610
[1mStep[0m  [80/84], [94mLoss[0m : 2.24140

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.432, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72350
[1mStep[0m  [8/84], [94mLoss[0m : 2.39536
[1mStep[0m  [16/84], [94mLoss[0m : 2.33751
[1mStep[0m  [24/84], [94mLoss[0m : 2.55621
[1mStep[0m  [32/84], [94mLoss[0m : 2.28155
[1mStep[0m  [40/84], [94mLoss[0m : 2.27192
[1mStep[0m  [48/84], [94mLoss[0m : 2.55264
[1mStep[0m  [56/84], [94mLoss[0m : 2.11099
[1mStep[0m  [64/84], [94mLoss[0m : 1.88091
[1mStep[0m  [72/84], [94mLoss[0m : 2.29525
[1mStep[0m  [80/84], [94mLoss[0m : 2.47084

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.370, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28241
[1mStep[0m  [8/84], [94mLoss[0m : 2.43334
[1mStep[0m  [16/84], [94mLoss[0m : 2.36592
[1mStep[0m  [24/84], [94mLoss[0m : 2.34246
[1mStep[0m  [32/84], [94mLoss[0m : 2.27930
[1mStep[0m  [40/84], [94mLoss[0m : 2.20174
[1mStep[0m  [48/84], [94mLoss[0m : 2.48508
[1mStep[0m  [56/84], [94mLoss[0m : 2.24082
[1mStep[0m  [64/84], [94mLoss[0m : 2.43856
[1mStep[0m  [72/84], [94mLoss[0m : 2.50542
[1mStep[0m  [80/84], [94mLoss[0m : 2.60308

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.433, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33323
[1mStep[0m  [8/84], [94mLoss[0m : 2.38813
[1mStep[0m  [16/84], [94mLoss[0m : 2.44440
[1mStep[0m  [24/84], [94mLoss[0m : 2.33671
[1mStep[0m  [32/84], [94mLoss[0m : 2.13696
[1mStep[0m  [40/84], [94mLoss[0m : 2.20068
[1mStep[0m  [48/84], [94mLoss[0m : 2.41675
[1mStep[0m  [56/84], [94mLoss[0m : 2.02338
[1mStep[0m  [64/84], [94mLoss[0m : 2.40073
[1mStep[0m  [72/84], [94mLoss[0m : 2.26601
[1mStep[0m  [80/84], [94mLoss[0m : 2.13490

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21515
[1mStep[0m  [8/84], [94mLoss[0m : 2.30057
[1mStep[0m  [16/84], [94mLoss[0m : 2.39240
[1mStep[0m  [24/84], [94mLoss[0m : 2.32287
[1mStep[0m  [32/84], [94mLoss[0m : 2.14089
[1mStep[0m  [40/84], [94mLoss[0m : 2.34395
[1mStep[0m  [48/84], [94mLoss[0m : 2.60705
[1mStep[0m  [56/84], [94mLoss[0m : 2.46206
[1mStep[0m  [64/84], [94mLoss[0m : 2.17642
[1mStep[0m  [72/84], [94mLoss[0m : 2.17845
[1mStep[0m  [80/84], [94mLoss[0m : 2.54786

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.453, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98750
[1mStep[0m  [8/84], [94mLoss[0m : 2.33699
[1mStep[0m  [16/84], [94mLoss[0m : 2.48141
[1mStep[0m  [24/84], [94mLoss[0m : 2.41754
[1mStep[0m  [32/84], [94mLoss[0m : 2.43375
[1mStep[0m  [40/84], [94mLoss[0m : 2.47087
[1mStep[0m  [48/84], [94mLoss[0m : 2.23698
[1mStep[0m  [56/84], [94mLoss[0m : 2.28757
[1mStep[0m  [64/84], [94mLoss[0m : 2.32313
[1mStep[0m  [72/84], [94mLoss[0m : 2.29761
[1mStep[0m  [80/84], [94mLoss[0m : 2.30791

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.428, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53103
[1mStep[0m  [8/84], [94mLoss[0m : 2.26986
[1mStep[0m  [16/84], [94mLoss[0m : 2.01148
[1mStep[0m  [24/84], [94mLoss[0m : 2.33918
[1mStep[0m  [32/84], [94mLoss[0m : 2.31051
[1mStep[0m  [40/84], [94mLoss[0m : 2.05119
[1mStep[0m  [48/84], [94mLoss[0m : 2.40333
[1mStep[0m  [56/84], [94mLoss[0m : 2.26517
[1mStep[0m  [64/84], [94mLoss[0m : 2.44951
[1mStep[0m  [72/84], [94mLoss[0m : 2.70015
[1mStep[0m  [80/84], [94mLoss[0m : 2.35314

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.409, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33410
[1mStep[0m  [8/84], [94mLoss[0m : 2.32231
[1mStep[0m  [16/84], [94mLoss[0m : 2.29482
[1mStep[0m  [24/84], [94mLoss[0m : 2.30729
[1mStep[0m  [32/84], [94mLoss[0m : 2.34857
[1mStep[0m  [40/84], [94mLoss[0m : 2.52240
[1mStep[0m  [48/84], [94mLoss[0m : 1.93808
[1mStep[0m  [56/84], [94mLoss[0m : 2.58021
[1mStep[0m  [64/84], [94mLoss[0m : 2.25930
[1mStep[0m  [72/84], [94mLoss[0m : 2.38914
[1mStep[0m  [80/84], [94mLoss[0m : 2.01092

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.413, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.435
====================================

Phase 1 - Evaluation MAE:  2.434988805225917
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.08391
[1mStep[0m  [8/84], [94mLoss[0m : 2.24474
[1mStep[0m  [16/84], [94mLoss[0m : 2.28699
[1mStep[0m  [24/84], [94mLoss[0m : 2.28804
[1mStep[0m  [32/84], [94mLoss[0m : 2.34608
[1mStep[0m  [40/84], [94mLoss[0m : 2.49808
[1mStep[0m  [48/84], [94mLoss[0m : 2.46017
[1mStep[0m  [56/84], [94mLoss[0m : 2.63034
[1mStep[0m  [64/84], [94mLoss[0m : 2.25793
[1mStep[0m  [72/84], [94mLoss[0m : 2.64032
[1mStep[0m  [80/84], [94mLoss[0m : 2.05708

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12498
[1mStep[0m  [8/84], [94mLoss[0m : 2.46170
[1mStep[0m  [16/84], [94mLoss[0m : 2.55439
[1mStep[0m  [24/84], [94mLoss[0m : 2.21279
[1mStep[0m  [32/84], [94mLoss[0m : 2.33558
[1mStep[0m  [40/84], [94mLoss[0m : 2.60316
[1mStep[0m  [48/84], [94mLoss[0m : 2.39884
[1mStep[0m  [56/84], [94mLoss[0m : 2.44313
[1mStep[0m  [64/84], [94mLoss[0m : 2.43811
[1mStep[0m  [72/84], [94mLoss[0m : 2.20575
[1mStep[0m  [80/84], [94mLoss[0m : 1.97651

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.566, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37128
[1mStep[0m  [8/84], [94mLoss[0m : 2.43153
[1mStep[0m  [16/84], [94mLoss[0m : 2.17642
[1mStep[0m  [24/84], [94mLoss[0m : 2.30753
[1mStep[0m  [32/84], [94mLoss[0m : 2.06956
[1mStep[0m  [40/84], [94mLoss[0m : 2.35739
[1mStep[0m  [48/84], [94mLoss[0m : 2.30408
[1mStep[0m  [56/84], [94mLoss[0m : 2.62750
[1mStep[0m  [64/84], [94mLoss[0m : 2.29882
[1mStep[0m  [72/84], [94mLoss[0m : 2.09078
[1mStep[0m  [80/84], [94mLoss[0m : 2.27234

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19603
[1mStep[0m  [8/84], [94mLoss[0m : 2.25148
[1mStep[0m  [16/84], [94mLoss[0m : 2.09322
[1mStep[0m  [24/84], [94mLoss[0m : 2.01345
[1mStep[0m  [32/84], [94mLoss[0m : 2.64547
[1mStep[0m  [40/84], [94mLoss[0m : 2.43189
[1mStep[0m  [48/84], [94mLoss[0m : 2.19630
[1mStep[0m  [56/84], [94mLoss[0m : 2.46909
[1mStep[0m  [64/84], [94mLoss[0m : 2.45313
[1mStep[0m  [72/84], [94mLoss[0m : 2.16968
[1mStep[0m  [80/84], [94mLoss[0m : 2.35261

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99216
[1mStep[0m  [8/84], [94mLoss[0m : 2.30749
[1mStep[0m  [16/84], [94mLoss[0m : 2.13789
[1mStep[0m  [24/84], [94mLoss[0m : 2.24808
[1mStep[0m  [32/84], [94mLoss[0m : 1.89075
[1mStep[0m  [40/84], [94mLoss[0m : 2.11331
[1mStep[0m  [48/84], [94mLoss[0m : 2.26437
[1mStep[0m  [56/84], [94mLoss[0m : 1.80533
[1mStep[0m  [64/84], [94mLoss[0m : 1.97233
[1mStep[0m  [72/84], [94mLoss[0m : 1.87114
[1mStep[0m  [80/84], [94mLoss[0m : 2.14180

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20665
[1mStep[0m  [8/84], [94mLoss[0m : 2.04277
[1mStep[0m  [16/84], [94mLoss[0m : 2.00351
[1mStep[0m  [24/84], [94mLoss[0m : 2.00946
[1mStep[0m  [32/84], [94mLoss[0m : 2.37643
[1mStep[0m  [40/84], [94mLoss[0m : 2.00111
[1mStep[0m  [48/84], [94mLoss[0m : 1.89640
[1mStep[0m  [56/84], [94mLoss[0m : 1.93573
[1mStep[0m  [64/84], [94mLoss[0m : 2.03321
[1mStep[0m  [72/84], [94mLoss[0m : 2.50475
[1mStep[0m  [80/84], [94mLoss[0m : 1.85179

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92316
[1mStep[0m  [8/84], [94mLoss[0m : 2.17565
[1mStep[0m  [16/84], [94mLoss[0m : 1.85159
[1mStep[0m  [24/84], [94mLoss[0m : 1.87718
[1mStep[0m  [32/84], [94mLoss[0m : 1.95711
[1mStep[0m  [40/84], [94mLoss[0m : 2.02168
[1mStep[0m  [48/84], [94mLoss[0m : 2.05473
[1mStep[0m  [56/84], [94mLoss[0m : 1.96990
[1mStep[0m  [64/84], [94mLoss[0m : 1.75720
[1mStep[0m  [72/84], [94mLoss[0m : 2.35303
[1mStep[0m  [80/84], [94mLoss[0m : 2.09466

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79445
[1mStep[0m  [8/84], [94mLoss[0m : 1.93944
[1mStep[0m  [16/84], [94mLoss[0m : 1.84239
[1mStep[0m  [24/84], [94mLoss[0m : 1.79381
[1mStep[0m  [32/84], [94mLoss[0m : 1.74100
[1mStep[0m  [40/84], [94mLoss[0m : 1.80142
[1mStep[0m  [48/84], [94mLoss[0m : 1.88449
[1mStep[0m  [56/84], [94mLoss[0m : 2.01905
[1mStep[0m  [64/84], [94mLoss[0m : 1.85145
[1mStep[0m  [72/84], [94mLoss[0m : 1.85256
[1mStep[0m  [80/84], [94mLoss[0m : 2.07653

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87711
[1mStep[0m  [8/84], [94mLoss[0m : 1.86917
[1mStep[0m  [16/84], [94mLoss[0m : 1.93615
[1mStep[0m  [24/84], [94mLoss[0m : 1.95935
[1mStep[0m  [32/84], [94mLoss[0m : 2.01298
[1mStep[0m  [40/84], [94mLoss[0m : 1.91475
[1mStep[0m  [48/84], [94mLoss[0m : 1.94045
[1mStep[0m  [56/84], [94mLoss[0m : 1.81559
[1mStep[0m  [64/84], [94mLoss[0m : 2.06795
[1mStep[0m  [72/84], [94mLoss[0m : 1.81535
[1mStep[0m  [80/84], [94mLoss[0m : 1.87563

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94859
[1mStep[0m  [8/84], [94mLoss[0m : 1.92553
[1mStep[0m  [16/84], [94mLoss[0m : 1.92842
[1mStep[0m  [24/84], [94mLoss[0m : 1.88972
[1mStep[0m  [32/84], [94mLoss[0m : 1.91495
[1mStep[0m  [40/84], [94mLoss[0m : 2.03322
[1mStep[0m  [48/84], [94mLoss[0m : 2.02866
[1mStep[0m  [56/84], [94mLoss[0m : 1.95272
[1mStep[0m  [64/84], [94mLoss[0m : 1.98981
[1mStep[0m  [72/84], [94mLoss[0m : 2.08616
[1mStep[0m  [80/84], [94mLoss[0m : 1.62686

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78987
[1mStep[0m  [8/84], [94mLoss[0m : 1.88464
[1mStep[0m  [16/84], [94mLoss[0m : 1.73376
[1mStep[0m  [24/84], [94mLoss[0m : 1.83958
[1mStep[0m  [32/84], [94mLoss[0m : 1.74452
[1mStep[0m  [40/84], [94mLoss[0m : 1.45920
[1mStep[0m  [48/84], [94mLoss[0m : 1.95592
[1mStep[0m  [56/84], [94mLoss[0m : 1.78762
[1mStep[0m  [64/84], [94mLoss[0m : 1.82774
[1mStep[0m  [72/84], [94mLoss[0m : 2.15759
[1mStep[0m  [80/84], [94mLoss[0m : 1.77759

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78185
[1mStep[0m  [8/84], [94mLoss[0m : 1.70081
[1mStep[0m  [16/84], [94mLoss[0m : 1.60172
[1mStep[0m  [24/84], [94mLoss[0m : 1.86447
[1mStep[0m  [32/84], [94mLoss[0m : 1.85978
[1mStep[0m  [40/84], [94mLoss[0m : 1.74931
[1mStep[0m  [48/84], [94mLoss[0m : 1.64607
[1mStep[0m  [56/84], [94mLoss[0m : 1.93798
[1mStep[0m  [64/84], [94mLoss[0m : 1.62619
[1mStep[0m  [72/84], [94mLoss[0m : 1.74118
[1mStep[0m  [80/84], [94mLoss[0m : 1.87205

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75436
[1mStep[0m  [8/84], [94mLoss[0m : 1.69036
[1mStep[0m  [16/84], [94mLoss[0m : 1.60289
[1mStep[0m  [24/84], [94mLoss[0m : 1.73501
[1mStep[0m  [32/84], [94mLoss[0m : 1.76040
[1mStep[0m  [40/84], [94mLoss[0m : 2.06577
[1mStep[0m  [48/84], [94mLoss[0m : 1.88243
[1mStep[0m  [56/84], [94mLoss[0m : 1.75682
[1mStep[0m  [64/84], [94mLoss[0m : 1.67130
[1mStep[0m  [72/84], [94mLoss[0m : 1.69363
[1mStep[0m  [80/84], [94mLoss[0m : 1.89117

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81457
[1mStep[0m  [8/84], [94mLoss[0m : 1.78611
[1mStep[0m  [16/84], [94mLoss[0m : 1.48677
[1mStep[0m  [24/84], [94mLoss[0m : 1.87882
[1mStep[0m  [32/84], [94mLoss[0m : 2.04219
[1mStep[0m  [40/84], [94mLoss[0m : 1.62376
[1mStep[0m  [48/84], [94mLoss[0m : 1.56413
[1mStep[0m  [56/84], [94mLoss[0m : 1.57726
[1mStep[0m  [64/84], [94mLoss[0m : 1.94580
[1mStep[0m  [72/84], [94mLoss[0m : 1.80567
[1mStep[0m  [80/84], [94mLoss[0m : 1.64377

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81764
[1mStep[0m  [8/84], [94mLoss[0m : 1.60155
[1mStep[0m  [16/84], [94mLoss[0m : 1.83992
[1mStep[0m  [24/84], [94mLoss[0m : 1.69118
[1mStep[0m  [32/84], [94mLoss[0m : 1.76231
[1mStep[0m  [40/84], [94mLoss[0m : 1.61879
[1mStep[0m  [48/84], [94mLoss[0m : 1.54815
[1mStep[0m  [56/84], [94mLoss[0m : 1.75642
[1mStep[0m  [64/84], [94mLoss[0m : 1.61019
[1mStep[0m  [72/84], [94mLoss[0m : 1.64871
[1mStep[0m  [80/84], [94mLoss[0m : 1.85477

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.555, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43421
[1mStep[0m  [8/84], [94mLoss[0m : 1.63596
[1mStep[0m  [16/84], [94mLoss[0m : 1.62577
[1mStep[0m  [24/84], [94mLoss[0m : 1.61243
[1mStep[0m  [32/84], [94mLoss[0m : 1.72435
[1mStep[0m  [40/84], [94mLoss[0m : 1.67685
[1mStep[0m  [48/84], [94mLoss[0m : 1.46020
[1mStep[0m  [56/84], [94mLoss[0m : 1.58122
[1mStep[0m  [64/84], [94mLoss[0m : 1.36710
[1mStep[0m  [72/84], [94mLoss[0m : 1.72129
[1mStep[0m  [80/84], [94mLoss[0m : 1.54704

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63926
[1mStep[0m  [8/84], [94mLoss[0m : 1.50031
[1mStep[0m  [16/84], [94mLoss[0m : 1.54022
[1mStep[0m  [24/84], [94mLoss[0m : 1.63965
[1mStep[0m  [32/84], [94mLoss[0m : 1.50203
[1mStep[0m  [40/84], [94mLoss[0m : 1.60951
[1mStep[0m  [48/84], [94mLoss[0m : 1.44517
[1mStep[0m  [56/84], [94mLoss[0m : 1.69852
[1mStep[0m  [64/84], [94mLoss[0m : 1.52627
[1mStep[0m  [72/84], [94mLoss[0m : 1.58568
[1mStep[0m  [80/84], [94mLoss[0m : 1.87409

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44298
[1mStep[0m  [8/84], [94mLoss[0m : 1.66959
[1mStep[0m  [16/84], [94mLoss[0m : 1.68645
[1mStep[0m  [24/84], [94mLoss[0m : 1.46259
[1mStep[0m  [32/84], [94mLoss[0m : 1.62790
[1mStep[0m  [40/84], [94mLoss[0m : 1.28083
[1mStep[0m  [48/84], [94mLoss[0m : 1.42254
[1mStep[0m  [56/84], [94mLoss[0m : 1.50727
[1mStep[0m  [64/84], [94mLoss[0m : 1.46896
[1mStep[0m  [72/84], [94mLoss[0m : 1.53634
[1mStep[0m  [80/84], [94mLoss[0m : 1.49153

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36125
[1mStep[0m  [8/84], [94mLoss[0m : 1.37771
[1mStep[0m  [16/84], [94mLoss[0m : 1.38759
[1mStep[0m  [24/84], [94mLoss[0m : 1.60164
[1mStep[0m  [32/84], [94mLoss[0m : 1.39576
[1mStep[0m  [40/84], [94mLoss[0m : 1.51528
[1mStep[0m  [48/84], [94mLoss[0m : 1.60481
[1mStep[0m  [56/84], [94mLoss[0m : 1.81231
[1mStep[0m  [64/84], [94mLoss[0m : 1.48753
[1mStep[0m  [72/84], [94mLoss[0m : 1.53463
[1mStep[0m  [80/84], [94mLoss[0m : 1.75154

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.609, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.34309
[1mStep[0m  [8/84], [94mLoss[0m : 1.38928
[1mStep[0m  [16/84], [94mLoss[0m : 1.51027
[1mStep[0m  [24/84], [94mLoss[0m : 1.67076
[1mStep[0m  [32/84], [94mLoss[0m : 1.58967
[1mStep[0m  [40/84], [94mLoss[0m : 1.38128
[1mStep[0m  [48/84], [94mLoss[0m : 1.32447
[1mStep[0m  [56/84], [94mLoss[0m : 1.63034
[1mStep[0m  [64/84], [94mLoss[0m : 1.58394
[1mStep[0m  [72/84], [94mLoss[0m : 1.65209
[1mStep[0m  [80/84], [94mLoss[0m : 1.68859

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.561, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72160
[1mStep[0m  [8/84], [94mLoss[0m : 1.42351
[1mStep[0m  [16/84], [94mLoss[0m : 1.39208
[1mStep[0m  [24/84], [94mLoss[0m : 1.67790
[1mStep[0m  [32/84], [94mLoss[0m : 1.50796
[1mStep[0m  [40/84], [94mLoss[0m : 1.42509
[1mStep[0m  [48/84], [94mLoss[0m : 1.33337
[1mStep[0m  [56/84], [94mLoss[0m : 1.37869
[1mStep[0m  [64/84], [94mLoss[0m : 1.45704
[1mStep[0m  [72/84], [94mLoss[0m : 1.40357
[1mStep[0m  [80/84], [94mLoss[0m : 1.38353

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.503, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48830
[1mStep[0m  [8/84], [94mLoss[0m : 1.41629
[1mStep[0m  [16/84], [94mLoss[0m : 1.38341
[1mStep[0m  [24/84], [94mLoss[0m : 1.47590
[1mStep[0m  [32/84], [94mLoss[0m : 1.40414
[1mStep[0m  [40/84], [94mLoss[0m : 1.38540
[1mStep[0m  [48/84], [94mLoss[0m : 1.44125
[1mStep[0m  [56/84], [94mLoss[0m : 1.40260
[1mStep[0m  [64/84], [94mLoss[0m : 1.30358
[1mStep[0m  [72/84], [94mLoss[0m : 1.48549
[1mStep[0m  [80/84], [94mLoss[0m : 1.48825

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.535, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57576
[1mStep[0m  [8/84], [94mLoss[0m : 1.33076
[1mStep[0m  [16/84], [94mLoss[0m : 1.46045
[1mStep[0m  [24/84], [94mLoss[0m : 1.43296
[1mStep[0m  [32/84], [94mLoss[0m : 1.40111
[1mStep[0m  [40/84], [94mLoss[0m : 1.51788
[1mStep[0m  [48/84], [94mLoss[0m : 1.37469
[1mStep[0m  [56/84], [94mLoss[0m : 1.30611
[1mStep[0m  [64/84], [94mLoss[0m : 1.60764
[1mStep[0m  [72/84], [94mLoss[0m : 1.40495
[1mStep[0m  [80/84], [94mLoss[0m : 1.47761

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.525, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.569
====================================

Phase 2 - Evaluation MAE:  2.5692528315952847
MAE score P1      2.434989
MAE score P2      2.569253
loss              1.411428
learning_rate         0.01
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 21, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.96883
[1mStep[0m  [16/169], [94mLoss[0m : 2.74881
[1mStep[0m  [32/169], [94mLoss[0m : 2.79602
[1mStep[0m  [48/169], [94mLoss[0m : 3.43801
[1mStep[0m  [64/169], [94mLoss[0m : 3.02046
[1mStep[0m  [80/169], [94mLoss[0m : 2.27116
[1mStep[0m  [96/169], [94mLoss[0m : 2.78192
[1mStep[0m  [112/169], [94mLoss[0m : 2.57896
[1mStep[0m  [128/169], [94mLoss[0m : 2.59106
[1mStep[0m  [144/169], [94mLoss[0m : 2.38133
[1mStep[0m  [160/169], [94mLoss[0m : 2.91265

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.193, [92mTest[0m: 11.209, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54635
[1mStep[0m  [16/169], [94mLoss[0m : 2.40047
[1mStep[0m  [32/169], [94mLoss[0m : 2.41223
[1mStep[0m  [48/169], [94mLoss[0m : 2.37471
[1mStep[0m  [64/169], [94mLoss[0m : 2.76019
[1mStep[0m  [80/169], [94mLoss[0m : 2.22449
[1mStep[0m  [96/169], [94mLoss[0m : 2.33424
[1mStep[0m  [112/169], [94mLoss[0m : 2.40220
[1mStep[0m  [128/169], [94mLoss[0m : 2.81822
[1mStep[0m  [144/169], [94mLoss[0m : 2.76966
[1mStep[0m  [160/169], [94mLoss[0m : 2.52892

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44253
[1mStep[0m  [16/169], [94mLoss[0m : 2.58364
[1mStep[0m  [32/169], [94mLoss[0m : 2.74363
[1mStep[0m  [48/169], [94mLoss[0m : 2.45693
[1mStep[0m  [64/169], [94mLoss[0m : 2.17799
[1mStep[0m  [80/169], [94mLoss[0m : 3.11505
[1mStep[0m  [96/169], [94mLoss[0m : 1.99313
[1mStep[0m  [112/169], [94mLoss[0m : 2.38332
[1mStep[0m  [128/169], [94mLoss[0m : 2.62839
[1mStep[0m  [144/169], [94mLoss[0m : 2.20872
[1mStep[0m  [160/169], [94mLoss[0m : 2.77424

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98845
[1mStep[0m  [16/169], [94mLoss[0m : 2.52252
[1mStep[0m  [32/169], [94mLoss[0m : 2.51300
[1mStep[0m  [48/169], [94mLoss[0m : 3.06206
[1mStep[0m  [64/169], [94mLoss[0m : 2.72886
[1mStep[0m  [80/169], [94mLoss[0m : 2.69148
[1mStep[0m  [96/169], [94mLoss[0m : 2.69278
[1mStep[0m  [112/169], [94mLoss[0m : 2.75643
[1mStep[0m  [128/169], [94mLoss[0m : 2.24904
[1mStep[0m  [144/169], [94mLoss[0m : 2.56038
[1mStep[0m  [160/169], [94mLoss[0m : 2.54693

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88120
[1mStep[0m  [16/169], [94mLoss[0m : 2.52723
[1mStep[0m  [32/169], [94mLoss[0m : 2.82482
[1mStep[0m  [48/169], [94mLoss[0m : 2.60874
[1mStep[0m  [64/169], [94mLoss[0m : 2.27625
[1mStep[0m  [80/169], [94mLoss[0m : 2.83585
[1mStep[0m  [96/169], [94mLoss[0m : 2.40320
[1mStep[0m  [112/169], [94mLoss[0m : 2.55988
[1mStep[0m  [128/169], [94mLoss[0m : 3.07554
[1mStep[0m  [144/169], [94mLoss[0m : 2.85946
[1mStep[0m  [160/169], [94mLoss[0m : 2.94731

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52419
[1mStep[0m  [16/169], [94mLoss[0m : 3.13330
[1mStep[0m  [32/169], [94mLoss[0m : 2.33692
[1mStep[0m  [48/169], [94mLoss[0m : 2.52189
[1mStep[0m  [64/169], [94mLoss[0m : 2.32014
[1mStep[0m  [80/169], [94mLoss[0m : 3.18770
[1mStep[0m  [96/169], [94mLoss[0m : 2.08618
[1mStep[0m  [112/169], [94mLoss[0m : 2.73738
[1mStep[0m  [128/169], [94mLoss[0m : 2.69950
[1mStep[0m  [144/169], [94mLoss[0m : 2.68349
[1mStep[0m  [160/169], [94mLoss[0m : 2.87407

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40159
[1mStep[0m  [16/169], [94mLoss[0m : 2.97298
[1mStep[0m  [32/169], [94mLoss[0m : 2.69701
[1mStep[0m  [48/169], [94mLoss[0m : 2.76781
[1mStep[0m  [64/169], [94mLoss[0m : 2.02251
[1mStep[0m  [80/169], [94mLoss[0m : 2.80627
[1mStep[0m  [96/169], [94mLoss[0m : 2.91337
[1mStep[0m  [112/169], [94mLoss[0m : 2.76929
[1mStep[0m  [128/169], [94mLoss[0m : 2.78717
[1mStep[0m  [144/169], [94mLoss[0m : 2.54760
[1mStep[0m  [160/169], [94mLoss[0m : 2.39982

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76041
[1mStep[0m  [16/169], [94mLoss[0m : 2.88344
[1mStep[0m  [32/169], [94mLoss[0m : 2.69144
[1mStep[0m  [48/169], [94mLoss[0m : 2.71295
[1mStep[0m  [64/169], [94mLoss[0m : 2.61947
[1mStep[0m  [80/169], [94mLoss[0m : 2.96827
[1mStep[0m  [96/169], [94mLoss[0m : 2.87555
[1mStep[0m  [112/169], [94mLoss[0m : 2.32579
[1mStep[0m  [128/169], [94mLoss[0m : 2.39003
[1mStep[0m  [144/169], [94mLoss[0m : 2.56525
[1mStep[0m  [160/169], [94mLoss[0m : 2.77640

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76125
[1mStep[0m  [16/169], [94mLoss[0m : 2.55207
[1mStep[0m  [32/169], [94mLoss[0m : 2.73832
[1mStep[0m  [48/169], [94mLoss[0m : 2.50753
[1mStep[0m  [64/169], [94mLoss[0m : 2.98428
[1mStep[0m  [80/169], [94mLoss[0m : 2.52455
[1mStep[0m  [96/169], [94mLoss[0m : 2.64993
[1mStep[0m  [112/169], [94mLoss[0m : 2.36068
[1mStep[0m  [128/169], [94mLoss[0m : 2.90020
[1mStep[0m  [144/169], [94mLoss[0m : 2.93507
[1mStep[0m  [160/169], [94mLoss[0m : 2.16158

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46452
[1mStep[0m  [16/169], [94mLoss[0m : 2.23417
[1mStep[0m  [32/169], [94mLoss[0m : 2.60160
[1mStep[0m  [48/169], [94mLoss[0m : 2.63905
[1mStep[0m  [64/169], [94mLoss[0m : 2.70695
[1mStep[0m  [80/169], [94mLoss[0m : 2.31197
[1mStep[0m  [96/169], [94mLoss[0m : 2.46247
[1mStep[0m  [112/169], [94mLoss[0m : 2.43276
[1mStep[0m  [128/169], [94mLoss[0m : 2.90992
[1mStep[0m  [144/169], [94mLoss[0m : 2.96115
[1mStep[0m  [160/169], [94mLoss[0m : 2.99683

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34581
[1mStep[0m  [16/169], [94mLoss[0m : 2.72525
[1mStep[0m  [32/169], [94mLoss[0m : 2.52118
[1mStep[0m  [48/169], [94mLoss[0m : 2.74431
[1mStep[0m  [64/169], [94mLoss[0m : 2.82169
[1mStep[0m  [80/169], [94mLoss[0m : 3.05573
[1mStep[0m  [96/169], [94mLoss[0m : 2.71226
[1mStep[0m  [112/169], [94mLoss[0m : 2.68013
[1mStep[0m  [128/169], [94mLoss[0m : 2.57729
[1mStep[0m  [144/169], [94mLoss[0m : 2.26677
[1mStep[0m  [160/169], [94mLoss[0m : 2.19497

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61075
[1mStep[0m  [16/169], [94mLoss[0m : 2.51476
[1mStep[0m  [32/169], [94mLoss[0m : 2.33540
[1mStep[0m  [48/169], [94mLoss[0m : 2.77712
[1mStep[0m  [64/169], [94mLoss[0m : 2.66205
[1mStep[0m  [80/169], [94mLoss[0m : 2.67007
[1mStep[0m  [96/169], [94mLoss[0m : 2.44320
[1mStep[0m  [112/169], [94mLoss[0m : 2.61625
[1mStep[0m  [128/169], [94mLoss[0m : 2.90334
[1mStep[0m  [144/169], [94mLoss[0m : 2.16915
[1mStep[0m  [160/169], [94mLoss[0m : 2.53173

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75793
[1mStep[0m  [16/169], [94mLoss[0m : 2.49078
[1mStep[0m  [32/169], [94mLoss[0m : 2.40792
[1mStep[0m  [48/169], [94mLoss[0m : 2.85117
[1mStep[0m  [64/169], [94mLoss[0m : 2.00597
[1mStep[0m  [80/169], [94mLoss[0m : 2.78771
[1mStep[0m  [96/169], [94mLoss[0m : 2.52086
[1mStep[0m  [112/169], [94mLoss[0m : 2.63784
[1mStep[0m  [128/169], [94mLoss[0m : 3.04265
[1mStep[0m  [144/169], [94mLoss[0m : 2.66027
[1mStep[0m  [160/169], [94mLoss[0m : 2.56565

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81789
[1mStep[0m  [16/169], [94mLoss[0m : 2.92508
[1mStep[0m  [32/169], [94mLoss[0m : 2.25917
[1mStep[0m  [48/169], [94mLoss[0m : 2.91014
[1mStep[0m  [64/169], [94mLoss[0m : 2.43330
[1mStep[0m  [80/169], [94mLoss[0m : 2.94653
[1mStep[0m  [96/169], [94mLoss[0m : 2.85942
[1mStep[0m  [112/169], [94mLoss[0m : 2.38322
[1mStep[0m  [128/169], [94mLoss[0m : 2.68516
[1mStep[0m  [144/169], [94mLoss[0m : 2.14957
[1mStep[0m  [160/169], [94mLoss[0m : 2.62476

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41534
[1mStep[0m  [16/169], [94mLoss[0m : 2.71718
[1mStep[0m  [32/169], [94mLoss[0m : 2.96071
[1mStep[0m  [48/169], [94mLoss[0m : 2.82970
[1mStep[0m  [64/169], [94mLoss[0m : 2.44342
[1mStep[0m  [80/169], [94mLoss[0m : 2.75623
[1mStep[0m  [96/169], [94mLoss[0m : 2.34390
[1mStep[0m  [112/169], [94mLoss[0m : 2.72068
[1mStep[0m  [128/169], [94mLoss[0m : 2.76517
[1mStep[0m  [144/169], [94mLoss[0m : 2.41419
[1mStep[0m  [160/169], [94mLoss[0m : 2.35238

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98598
[1mStep[0m  [16/169], [94mLoss[0m : 2.63315
[1mStep[0m  [32/169], [94mLoss[0m : 2.63110
[1mStep[0m  [48/169], [94mLoss[0m : 2.60104
[1mStep[0m  [64/169], [94mLoss[0m : 3.10275
[1mStep[0m  [80/169], [94mLoss[0m : 2.48620
[1mStep[0m  [96/169], [94mLoss[0m : 2.43150
[1mStep[0m  [112/169], [94mLoss[0m : 2.72722
[1mStep[0m  [128/169], [94mLoss[0m : 2.50383
[1mStep[0m  [144/169], [94mLoss[0m : 2.51818
[1mStep[0m  [160/169], [94mLoss[0m : 2.43172

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67102
[1mStep[0m  [16/169], [94mLoss[0m : 2.79279
[1mStep[0m  [32/169], [94mLoss[0m : 2.68250
[1mStep[0m  [48/169], [94mLoss[0m : 2.96362
[1mStep[0m  [64/169], [94mLoss[0m : 2.91990
[1mStep[0m  [80/169], [94mLoss[0m : 2.52384
[1mStep[0m  [96/169], [94mLoss[0m : 2.20242
[1mStep[0m  [112/169], [94mLoss[0m : 2.47048
[1mStep[0m  [128/169], [94mLoss[0m : 2.66266
[1mStep[0m  [144/169], [94mLoss[0m : 2.30277
[1mStep[0m  [160/169], [94mLoss[0m : 2.59468

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15051
[1mStep[0m  [16/169], [94mLoss[0m : 2.52729
[1mStep[0m  [32/169], [94mLoss[0m : 2.75404
[1mStep[0m  [48/169], [94mLoss[0m : 2.76072
[1mStep[0m  [64/169], [94mLoss[0m : 2.53870
[1mStep[0m  [80/169], [94mLoss[0m : 2.14552
[1mStep[0m  [96/169], [94mLoss[0m : 2.97707
[1mStep[0m  [112/169], [94mLoss[0m : 2.83716
[1mStep[0m  [128/169], [94mLoss[0m : 2.22824
[1mStep[0m  [144/169], [94mLoss[0m : 2.53316
[1mStep[0m  [160/169], [94mLoss[0m : 2.62489

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46335
[1mStep[0m  [16/169], [94mLoss[0m : 2.82120
[1mStep[0m  [32/169], [94mLoss[0m : 2.31775
[1mStep[0m  [48/169], [94mLoss[0m : 2.80663
[1mStep[0m  [64/169], [94mLoss[0m : 2.48089
[1mStep[0m  [80/169], [94mLoss[0m : 2.66077
[1mStep[0m  [96/169], [94mLoss[0m : 3.03714
[1mStep[0m  [112/169], [94mLoss[0m : 2.51408
[1mStep[0m  [128/169], [94mLoss[0m : 2.10529
[1mStep[0m  [144/169], [94mLoss[0m : 2.85625
[1mStep[0m  [160/169], [94mLoss[0m : 3.23409

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40811
[1mStep[0m  [16/169], [94mLoss[0m : 2.43418
[1mStep[0m  [32/169], [94mLoss[0m : 2.53513
[1mStep[0m  [48/169], [94mLoss[0m : 2.73425
[1mStep[0m  [64/169], [94mLoss[0m : 2.79684
[1mStep[0m  [80/169], [94mLoss[0m : 2.57512
[1mStep[0m  [96/169], [94mLoss[0m : 2.70356
[1mStep[0m  [112/169], [94mLoss[0m : 3.05702
[1mStep[0m  [128/169], [94mLoss[0m : 3.01946
[1mStep[0m  [144/169], [94mLoss[0m : 2.23896
[1mStep[0m  [160/169], [94mLoss[0m : 2.68368

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41689
[1mStep[0m  [16/169], [94mLoss[0m : 3.04719
[1mStep[0m  [32/169], [94mLoss[0m : 2.77087
[1mStep[0m  [48/169], [94mLoss[0m : 2.83170
[1mStep[0m  [64/169], [94mLoss[0m : 2.74155
[1mStep[0m  [80/169], [94mLoss[0m : 2.98916
[1mStep[0m  [96/169], [94mLoss[0m : 2.69505
[1mStep[0m  [112/169], [94mLoss[0m : 2.31600
[1mStep[0m  [128/169], [94mLoss[0m : 2.92915
[1mStep[0m  [144/169], [94mLoss[0m : 2.82473
[1mStep[0m  [160/169], [94mLoss[0m : 3.07297

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17306
[1mStep[0m  [16/169], [94mLoss[0m : 2.74905
[1mStep[0m  [32/169], [94mLoss[0m : 2.23191
[1mStep[0m  [48/169], [94mLoss[0m : 2.78411
[1mStep[0m  [64/169], [94mLoss[0m : 2.74849
[1mStep[0m  [80/169], [94mLoss[0m : 2.86429
[1mStep[0m  [96/169], [94mLoss[0m : 2.43190
[1mStep[0m  [112/169], [94mLoss[0m : 2.48916
[1mStep[0m  [128/169], [94mLoss[0m : 2.65853
[1mStep[0m  [144/169], [94mLoss[0m : 2.70564
[1mStep[0m  [160/169], [94mLoss[0m : 2.63598

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.429, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56555
[1mStep[0m  [16/169], [94mLoss[0m : 2.72123
[1mStep[0m  [32/169], [94mLoss[0m : 2.11270
[1mStep[0m  [48/169], [94mLoss[0m : 2.82197
[1mStep[0m  [64/169], [94mLoss[0m : 3.04780
[1mStep[0m  [80/169], [94mLoss[0m : 2.48538
[1mStep[0m  [96/169], [94mLoss[0m : 2.79811
[1mStep[0m  [112/169], [94mLoss[0m : 2.91188
[1mStep[0m  [128/169], [94mLoss[0m : 2.51922
[1mStep[0m  [144/169], [94mLoss[0m : 2.51468
[1mStep[0m  [160/169], [94mLoss[0m : 2.72605

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.432, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26359
[1mStep[0m  [16/169], [94mLoss[0m : 2.41374
[1mStep[0m  [32/169], [94mLoss[0m : 2.70343
[1mStep[0m  [48/169], [94mLoss[0m : 2.35351
[1mStep[0m  [64/169], [94mLoss[0m : 2.89372
[1mStep[0m  [80/169], [94mLoss[0m : 2.54714
[1mStep[0m  [96/169], [94mLoss[0m : 1.92853
[1mStep[0m  [112/169], [94mLoss[0m : 2.74470
[1mStep[0m  [128/169], [94mLoss[0m : 2.37251
[1mStep[0m  [144/169], [94mLoss[0m : 2.98689
[1mStep[0m  [160/169], [94mLoss[0m : 2.73191

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59017
[1mStep[0m  [16/169], [94mLoss[0m : 2.74122
[1mStep[0m  [32/169], [94mLoss[0m : 2.50793
[1mStep[0m  [48/169], [94mLoss[0m : 2.99116
[1mStep[0m  [64/169], [94mLoss[0m : 2.83992
[1mStep[0m  [80/169], [94mLoss[0m : 2.55995
[1mStep[0m  [96/169], [94mLoss[0m : 2.55536
[1mStep[0m  [112/169], [94mLoss[0m : 2.57126
[1mStep[0m  [128/169], [94mLoss[0m : 3.08979
[1mStep[0m  [144/169], [94mLoss[0m : 2.65599
[1mStep[0m  [160/169], [94mLoss[0m : 2.80733

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.410, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35918
[1mStep[0m  [16/169], [94mLoss[0m : 2.67310
[1mStep[0m  [32/169], [94mLoss[0m : 2.47135
[1mStep[0m  [48/169], [94mLoss[0m : 2.86714
[1mStep[0m  [64/169], [94mLoss[0m : 3.39871
[1mStep[0m  [80/169], [94mLoss[0m : 2.87701
[1mStep[0m  [96/169], [94mLoss[0m : 2.48761
[1mStep[0m  [112/169], [94mLoss[0m : 2.55186
[1mStep[0m  [128/169], [94mLoss[0m : 2.65633
[1mStep[0m  [144/169], [94mLoss[0m : 2.45749
[1mStep[0m  [160/169], [94mLoss[0m : 2.82267

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.426, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88034
[1mStep[0m  [16/169], [94mLoss[0m : 2.59118
[1mStep[0m  [32/169], [94mLoss[0m : 2.97114
[1mStep[0m  [48/169], [94mLoss[0m : 3.11816
[1mStep[0m  [64/169], [94mLoss[0m : 2.88885
[1mStep[0m  [80/169], [94mLoss[0m : 2.35444
[1mStep[0m  [96/169], [94mLoss[0m : 2.93177
[1mStep[0m  [112/169], [94mLoss[0m : 2.26083
[1mStep[0m  [128/169], [94mLoss[0m : 2.58614
[1mStep[0m  [144/169], [94mLoss[0m : 2.86237
[1mStep[0m  [160/169], [94mLoss[0m : 2.43832

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.406, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57209
[1mStep[0m  [16/169], [94mLoss[0m : 2.84104
[1mStep[0m  [32/169], [94mLoss[0m : 2.76182
[1mStep[0m  [48/169], [94mLoss[0m : 2.56544
[1mStep[0m  [64/169], [94mLoss[0m : 2.44228
[1mStep[0m  [80/169], [94mLoss[0m : 2.78196
[1mStep[0m  [96/169], [94mLoss[0m : 2.88884
[1mStep[0m  [112/169], [94mLoss[0m : 2.53581
[1mStep[0m  [128/169], [94mLoss[0m : 2.23160
[1mStep[0m  [144/169], [94mLoss[0m : 2.62803
[1mStep[0m  [160/169], [94mLoss[0m : 2.22087

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.409, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73438
[1mStep[0m  [16/169], [94mLoss[0m : 3.03390
[1mStep[0m  [32/169], [94mLoss[0m : 2.40251
[1mStep[0m  [48/169], [94mLoss[0m : 2.51834
[1mStep[0m  [64/169], [94mLoss[0m : 2.40446
[1mStep[0m  [80/169], [94mLoss[0m : 2.86178
[1mStep[0m  [96/169], [94mLoss[0m : 2.18027
[1mStep[0m  [112/169], [94mLoss[0m : 2.74464
[1mStep[0m  [128/169], [94mLoss[0m : 2.85950
[1mStep[0m  [144/169], [94mLoss[0m : 2.71455
[1mStep[0m  [160/169], [94mLoss[0m : 2.59597

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.413, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60121
[1mStep[0m  [16/169], [94mLoss[0m : 2.26886
[1mStep[0m  [32/169], [94mLoss[0m : 2.70509
[1mStep[0m  [48/169], [94mLoss[0m : 2.85204
[1mStep[0m  [64/169], [94mLoss[0m : 2.54797
[1mStep[0m  [80/169], [94mLoss[0m : 2.54983
[1mStep[0m  [96/169], [94mLoss[0m : 2.36899
[1mStep[0m  [112/169], [94mLoss[0m : 2.57580
[1mStep[0m  [128/169], [94mLoss[0m : 2.63028
[1mStep[0m  [144/169], [94mLoss[0m : 2.68567
[1mStep[0m  [160/169], [94mLoss[0m : 2.73428

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.401
====================================

Phase 1 - Evaluation MAE:  2.401330828666687
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.79677
[1mStep[0m  [16/169], [94mLoss[0m : 2.63491
[1mStep[0m  [32/169], [94mLoss[0m : 3.01845
[1mStep[0m  [48/169], [94mLoss[0m : 2.89232
[1mStep[0m  [64/169], [94mLoss[0m : 3.34297
[1mStep[0m  [80/169], [94mLoss[0m : 2.48985
[1mStep[0m  [96/169], [94mLoss[0m : 2.99066
[1mStep[0m  [112/169], [94mLoss[0m : 2.44821
[1mStep[0m  [128/169], [94mLoss[0m : 2.32376
[1mStep[0m  [144/169], [94mLoss[0m : 2.82530
[1mStep[0m  [160/169], [94mLoss[0m : 2.51368

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39837
[1mStep[0m  [16/169], [94mLoss[0m : 2.73204
[1mStep[0m  [32/169], [94mLoss[0m : 2.98811
[1mStep[0m  [48/169], [94mLoss[0m : 2.46012
[1mStep[0m  [64/169], [94mLoss[0m : 2.57453
[1mStep[0m  [80/169], [94mLoss[0m : 2.93365
[1mStep[0m  [96/169], [94mLoss[0m : 2.81499
[1mStep[0m  [112/169], [94mLoss[0m : 2.48623
[1mStep[0m  [128/169], [94mLoss[0m : 2.66873
[1mStep[0m  [144/169], [94mLoss[0m : 2.62215
[1mStep[0m  [160/169], [94mLoss[0m : 2.32535

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23223
[1mStep[0m  [16/169], [94mLoss[0m : 2.55495
[1mStep[0m  [32/169], [94mLoss[0m : 2.21690
[1mStep[0m  [48/169], [94mLoss[0m : 2.43794
[1mStep[0m  [64/169], [94mLoss[0m : 2.82632
[1mStep[0m  [80/169], [94mLoss[0m : 2.36793
[1mStep[0m  [96/169], [94mLoss[0m : 2.40745
[1mStep[0m  [112/169], [94mLoss[0m : 2.20459
[1mStep[0m  [128/169], [94mLoss[0m : 2.77224
[1mStep[0m  [144/169], [94mLoss[0m : 2.41844
[1mStep[0m  [160/169], [94mLoss[0m : 2.44671

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45117
[1mStep[0m  [16/169], [94mLoss[0m : 2.26108
[1mStep[0m  [32/169], [94mLoss[0m : 2.49356
[1mStep[0m  [48/169], [94mLoss[0m : 2.25364
[1mStep[0m  [64/169], [94mLoss[0m : 2.74102
[1mStep[0m  [80/169], [94mLoss[0m : 2.45951
[1mStep[0m  [96/169], [94mLoss[0m : 2.76178
[1mStep[0m  [112/169], [94mLoss[0m : 2.66520
[1mStep[0m  [128/169], [94mLoss[0m : 2.34292
[1mStep[0m  [144/169], [94mLoss[0m : 2.52376
[1mStep[0m  [160/169], [94mLoss[0m : 2.13929

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49506
[1mStep[0m  [16/169], [94mLoss[0m : 2.72142
[1mStep[0m  [32/169], [94mLoss[0m : 2.51019
[1mStep[0m  [48/169], [94mLoss[0m : 2.71232
[1mStep[0m  [64/169], [94mLoss[0m : 2.00306
[1mStep[0m  [80/169], [94mLoss[0m : 2.40931
[1mStep[0m  [96/169], [94mLoss[0m : 2.03094
[1mStep[0m  [112/169], [94mLoss[0m : 2.58880
[1mStep[0m  [128/169], [94mLoss[0m : 3.13178
[1mStep[0m  [144/169], [94mLoss[0m : 3.17273
[1mStep[0m  [160/169], [94mLoss[0m : 2.49269

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43255
[1mStep[0m  [16/169], [94mLoss[0m : 2.25945
[1mStep[0m  [32/169], [94mLoss[0m : 2.18921
[1mStep[0m  [48/169], [94mLoss[0m : 2.34294
[1mStep[0m  [64/169], [94mLoss[0m : 1.97861
[1mStep[0m  [80/169], [94mLoss[0m : 2.69940
[1mStep[0m  [96/169], [94mLoss[0m : 2.77632
[1mStep[0m  [112/169], [94mLoss[0m : 2.38615
[1mStep[0m  [128/169], [94mLoss[0m : 2.73661
[1mStep[0m  [144/169], [94mLoss[0m : 2.60608
[1mStep[0m  [160/169], [94mLoss[0m : 2.17617

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91112
[1mStep[0m  [16/169], [94mLoss[0m : 2.49601
[1mStep[0m  [32/169], [94mLoss[0m : 2.40590
[1mStep[0m  [48/169], [94mLoss[0m : 2.15124
[1mStep[0m  [64/169], [94mLoss[0m : 2.75143
[1mStep[0m  [80/169], [94mLoss[0m : 2.52707
[1mStep[0m  [96/169], [94mLoss[0m : 2.24209
[1mStep[0m  [112/169], [94mLoss[0m : 2.09658
[1mStep[0m  [128/169], [94mLoss[0m : 2.84525
[1mStep[0m  [144/169], [94mLoss[0m : 2.35120
[1mStep[0m  [160/169], [94mLoss[0m : 2.29068

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31906
[1mStep[0m  [16/169], [94mLoss[0m : 2.09483
[1mStep[0m  [32/169], [94mLoss[0m : 2.16815
[1mStep[0m  [48/169], [94mLoss[0m : 2.34090
[1mStep[0m  [64/169], [94mLoss[0m : 2.34008
[1mStep[0m  [80/169], [94mLoss[0m : 2.54394
[1mStep[0m  [96/169], [94mLoss[0m : 3.17080
[1mStep[0m  [112/169], [94mLoss[0m : 2.09243
[1mStep[0m  [128/169], [94mLoss[0m : 2.51039
[1mStep[0m  [144/169], [94mLoss[0m : 2.14294
[1mStep[0m  [160/169], [94mLoss[0m : 3.09578

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45122
[1mStep[0m  [16/169], [94mLoss[0m : 2.52620
[1mStep[0m  [32/169], [94mLoss[0m : 2.57550
[1mStep[0m  [48/169], [94mLoss[0m : 2.69827
[1mStep[0m  [64/169], [94mLoss[0m : 2.43996
[1mStep[0m  [80/169], [94mLoss[0m : 2.72261
[1mStep[0m  [96/169], [94mLoss[0m : 2.47523
[1mStep[0m  [112/169], [94mLoss[0m : 2.42690
[1mStep[0m  [128/169], [94mLoss[0m : 2.38942
[1mStep[0m  [144/169], [94mLoss[0m : 2.39121
[1mStep[0m  [160/169], [94mLoss[0m : 2.40505

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26363
[1mStep[0m  [16/169], [94mLoss[0m : 2.58366
[1mStep[0m  [32/169], [94mLoss[0m : 2.71081
[1mStep[0m  [48/169], [94mLoss[0m : 2.56578
[1mStep[0m  [64/169], [94mLoss[0m : 2.16827
[1mStep[0m  [80/169], [94mLoss[0m : 2.42538
[1mStep[0m  [96/169], [94mLoss[0m : 2.38887
[1mStep[0m  [112/169], [94mLoss[0m : 2.81025
[1mStep[0m  [128/169], [94mLoss[0m : 1.99535
[1mStep[0m  [144/169], [94mLoss[0m : 2.41235
[1mStep[0m  [160/169], [94mLoss[0m : 2.57096

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65919
[1mStep[0m  [16/169], [94mLoss[0m : 2.49347
[1mStep[0m  [32/169], [94mLoss[0m : 2.84759
[1mStep[0m  [48/169], [94mLoss[0m : 2.17931
[1mStep[0m  [64/169], [94mLoss[0m : 2.19168
[1mStep[0m  [80/169], [94mLoss[0m : 2.22286
[1mStep[0m  [96/169], [94mLoss[0m : 2.35319
[1mStep[0m  [112/169], [94mLoss[0m : 2.68232
[1mStep[0m  [128/169], [94mLoss[0m : 2.71668
[1mStep[0m  [144/169], [94mLoss[0m : 2.52125
[1mStep[0m  [160/169], [94mLoss[0m : 2.60971

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36051
[1mStep[0m  [16/169], [94mLoss[0m : 2.12774
[1mStep[0m  [32/169], [94mLoss[0m : 2.68303
[1mStep[0m  [48/169], [94mLoss[0m : 2.26838
[1mStep[0m  [64/169], [94mLoss[0m : 2.17412
[1mStep[0m  [80/169], [94mLoss[0m : 2.55166
[1mStep[0m  [96/169], [94mLoss[0m : 2.56126
[1mStep[0m  [112/169], [94mLoss[0m : 2.50887
[1mStep[0m  [128/169], [94mLoss[0m : 2.43116
[1mStep[0m  [144/169], [94mLoss[0m : 2.60664
[1mStep[0m  [160/169], [94mLoss[0m : 2.55912

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08265
[1mStep[0m  [16/169], [94mLoss[0m : 2.63337
[1mStep[0m  [32/169], [94mLoss[0m : 2.61088
[1mStep[0m  [48/169], [94mLoss[0m : 2.62982
[1mStep[0m  [64/169], [94mLoss[0m : 2.45136
[1mStep[0m  [80/169], [94mLoss[0m : 2.35498
[1mStep[0m  [96/169], [94mLoss[0m : 2.25964
[1mStep[0m  [112/169], [94mLoss[0m : 2.81393
[1mStep[0m  [128/169], [94mLoss[0m : 2.20053
[1mStep[0m  [144/169], [94mLoss[0m : 2.20561
[1mStep[0m  [160/169], [94mLoss[0m : 2.18669

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22395
[1mStep[0m  [16/169], [94mLoss[0m : 2.65003
[1mStep[0m  [32/169], [94mLoss[0m : 2.45425
[1mStep[0m  [48/169], [94mLoss[0m : 2.67026
[1mStep[0m  [64/169], [94mLoss[0m : 2.43888
[1mStep[0m  [80/169], [94mLoss[0m : 2.87526
[1mStep[0m  [96/169], [94mLoss[0m : 2.77303
[1mStep[0m  [112/169], [94mLoss[0m : 2.91184
[1mStep[0m  [128/169], [94mLoss[0m : 1.98881
[1mStep[0m  [144/169], [94mLoss[0m : 2.66728
[1mStep[0m  [160/169], [94mLoss[0m : 2.61340

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61536
[1mStep[0m  [16/169], [94mLoss[0m : 2.40996
[1mStep[0m  [32/169], [94mLoss[0m : 2.20070
[1mStep[0m  [48/169], [94mLoss[0m : 2.95773
[1mStep[0m  [64/169], [94mLoss[0m : 2.09925
[1mStep[0m  [80/169], [94mLoss[0m : 1.97687
[1mStep[0m  [96/169], [94mLoss[0m : 3.06950
[1mStep[0m  [112/169], [94mLoss[0m : 2.45434
[1mStep[0m  [128/169], [94mLoss[0m : 2.75246
[1mStep[0m  [144/169], [94mLoss[0m : 2.21071
[1mStep[0m  [160/169], [94mLoss[0m : 2.87152

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56316
[1mStep[0m  [16/169], [94mLoss[0m : 2.57337
[1mStep[0m  [32/169], [94mLoss[0m : 2.51160
[1mStep[0m  [48/169], [94mLoss[0m : 2.27822
[1mStep[0m  [64/169], [94mLoss[0m : 2.35847
[1mStep[0m  [80/169], [94mLoss[0m : 2.35703
[1mStep[0m  [96/169], [94mLoss[0m : 2.42456
[1mStep[0m  [112/169], [94mLoss[0m : 2.64561
[1mStep[0m  [128/169], [94mLoss[0m : 2.22052
[1mStep[0m  [144/169], [94mLoss[0m : 2.38775
[1mStep[0m  [160/169], [94mLoss[0m : 2.34782

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32637
[1mStep[0m  [16/169], [94mLoss[0m : 2.52661
[1mStep[0m  [32/169], [94mLoss[0m : 2.30210
[1mStep[0m  [48/169], [94mLoss[0m : 2.27043
[1mStep[0m  [64/169], [94mLoss[0m : 2.37048
[1mStep[0m  [80/169], [94mLoss[0m : 2.28384
[1mStep[0m  [96/169], [94mLoss[0m : 2.25907
[1mStep[0m  [112/169], [94mLoss[0m : 2.62957
[1mStep[0m  [128/169], [94mLoss[0m : 2.48361
[1mStep[0m  [144/169], [94mLoss[0m : 2.32763
[1mStep[0m  [160/169], [94mLoss[0m : 2.58501

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56328
[1mStep[0m  [16/169], [94mLoss[0m : 2.49716
[1mStep[0m  [32/169], [94mLoss[0m : 2.54804
[1mStep[0m  [48/169], [94mLoss[0m : 2.63914
[1mStep[0m  [64/169], [94mLoss[0m : 2.82652
[1mStep[0m  [80/169], [94mLoss[0m : 2.45998
[1mStep[0m  [96/169], [94mLoss[0m : 2.19195
[1mStep[0m  [112/169], [94mLoss[0m : 2.55916
[1mStep[0m  [128/169], [94mLoss[0m : 2.14473
[1mStep[0m  [144/169], [94mLoss[0m : 2.54346
[1mStep[0m  [160/169], [94mLoss[0m : 3.30854

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97544
[1mStep[0m  [16/169], [94mLoss[0m : 2.58453
[1mStep[0m  [32/169], [94mLoss[0m : 2.26150
[1mStep[0m  [48/169], [94mLoss[0m : 2.48746
[1mStep[0m  [64/169], [94mLoss[0m : 2.56837
[1mStep[0m  [80/169], [94mLoss[0m : 2.41767
[1mStep[0m  [96/169], [94mLoss[0m : 2.03065
[1mStep[0m  [112/169], [94mLoss[0m : 2.10995
[1mStep[0m  [128/169], [94mLoss[0m : 2.18717
[1mStep[0m  [144/169], [94mLoss[0m : 2.62865
[1mStep[0m  [160/169], [94mLoss[0m : 2.59384

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68374
[1mStep[0m  [16/169], [94mLoss[0m : 2.48446
[1mStep[0m  [32/169], [94mLoss[0m : 2.52105
[1mStep[0m  [48/169], [94mLoss[0m : 2.76615
[1mStep[0m  [64/169], [94mLoss[0m : 2.45987
[1mStep[0m  [80/169], [94mLoss[0m : 2.81464
[1mStep[0m  [96/169], [94mLoss[0m : 2.53608
[1mStep[0m  [112/169], [94mLoss[0m : 2.61367
[1mStep[0m  [128/169], [94mLoss[0m : 2.57239
[1mStep[0m  [144/169], [94mLoss[0m : 2.66145
[1mStep[0m  [160/169], [94mLoss[0m : 2.72052

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.441, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39744
[1mStep[0m  [16/169], [94mLoss[0m : 2.70801
[1mStep[0m  [32/169], [94mLoss[0m : 2.39315
[1mStep[0m  [48/169], [94mLoss[0m : 2.36966
[1mStep[0m  [64/169], [94mLoss[0m : 2.46461
[1mStep[0m  [80/169], [94mLoss[0m : 2.33578
[1mStep[0m  [96/169], [94mLoss[0m : 2.65907
[1mStep[0m  [112/169], [94mLoss[0m : 2.85169
[1mStep[0m  [128/169], [94mLoss[0m : 2.55722
[1mStep[0m  [144/169], [94mLoss[0m : 2.75988
[1mStep[0m  [160/169], [94mLoss[0m : 2.23711

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.451, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45602
[1mStep[0m  [16/169], [94mLoss[0m : 2.61144
[1mStep[0m  [32/169], [94mLoss[0m : 2.27576
[1mStep[0m  [48/169], [94mLoss[0m : 2.53634
[1mStep[0m  [64/169], [94mLoss[0m : 2.41841
[1mStep[0m  [80/169], [94mLoss[0m : 2.56602
[1mStep[0m  [96/169], [94mLoss[0m : 2.39461
[1mStep[0m  [112/169], [94mLoss[0m : 2.17486
[1mStep[0m  [128/169], [94mLoss[0m : 2.18723
[1mStep[0m  [144/169], [94mLoss[0m : 2.00478
[1mStep[0m  [160/169], [94mLoss[0m : 2.57334

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.435, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56382
[1mStep[0m  [16/169], [94mLoss[0m : 3.10756
[1mStep[0m  [32/169], [94mLoss[0m : 2.98776
[1mStep[0m  [48/169], [94mLoss[0m : 2.59401
[1mStep[0m  [64/169], [94mLoss[0m : 2.32589
[1mStep[0m  [80/169], [94mLoss[0m : 2.64557
[1mStep[0m  [96/169], [94mLoss[0m : 2.63013
[1mStep[0m  [112/169], [94mLoss[0m : 2.28546
[1mStep[0m  [128/169], [94mLoss[0m : 2.13663
[1mStep[0m  [144/169], [94mLoss[0m : 2.49639
[1mStep[0m  [160/169], [94mLoss[0m : 2.63912

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12491
[1mStep[0m  [16/169], [94mLoss[0m : 2.55129
[1mStep[0m  [32/169], [94mLoss[0m : 2.46835
[1mStep[0m  [48/169], [94mLoss[0m : 2.60085
[1mStep[0m  [64/169], [94mLoss[0m : 2.67109
[1mStep[0m  [80/169], [94mLoss[0m : 2.66245
[1mStep[0m  [96/169], [94mLoss[0m : 2.34853
[1mStep[0m  [112/169], [94mLoss[0m : 2.50353
[1mStep[0m  [128/169], [94mLoss[0m : 2.12676
[1mStep[0m  [144/169], [94mLoss[0m : 2.67214
[1mStep[0m  [160/169], [94mLoss[0m : 2.34861

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46844
[1mStep[0m  [16/169], [94mLoss[0m : 2.03353
[1mStep[0m  [32/169], [94mLoss[0m : 2.34521
[1mStep[0m  [48/169], [94mLoss[0m : 2.55040
[1mStep[0m  [64/169], [94mLoss[0m : 2.38466
[1mStep[0m  [80/169], [94mLoss[0m : 2.15565
[1mStep[0m  [96/169], [94mLoss[0m : 2.63908
[1mStep[0m  [112/169], [94mLoss[0m : 2.65689
[1mStep[0m  [128/169], [94mLoss[0m : 2.41921
[1mStep[0m  [144/169], [94mLoss[0m : 2.08674
[1mStep[0m  [160/169], [94mLoss[0m : 2.63895

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.446, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69636
[1mStep[0m  [16/169], [94mLoss[0m : 2.51312
[1mStep[0m  [32/169], [94mLoss[0m : 2.10679
[1mStep[0m  [48/169], [94mLoss[0m : 2.72365
[1mStep[0m  [64/169], [94mLoss[0m : 2.61854
[1mStep[0m  [80/169], [94mLoss[0m : 2.71522
[1mStep[0m  [96/169], [94mLoss[0m : 2.31904
[1mStep[0m  [112/169], [94mLoss[0m : 2.66587
[1mStep[0m  [128/169], [94mLoss[0m : 2.22926
[1mStep[0m  [144/169], [94mLoss[0m : 2.42898
[1mStep[0m  [160/169], [94mLoss[0m : 3.07785

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.433, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63354
[1mStep[0m  [16/169], [94mLoss[0m : 2.01687
[1mStep[0m  [32/169], [94mLoss[0m : 2.44417
[1mStep[0m  [48/169], [94mLoss[0m : 2.39713
[1mStep[0m  [64/169], [94mLoss[0m : 2.68971
[1mStep[0m  [80/169], [94mLoss[0m : 3.13112
[1mStep[0m  [96/169], [94mLoss[0m : 2.31217
[1mStep[0m  [112/169], [94mLoss[0m : 2.54708
[1mStep[0m  [128/169], [94mLoss[0m : 2.89759
[1mStep[0m  [144/169], [94mLoss[0m : 2.29547
[1mStep[0m  [160/169], [94mLoss[0m : 2.35262

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.422, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40651
[1mStep[0m  [16/169], [94mLoss[0m : 2.81924
[1mStep[0m  [32/169], [94mLoss[0m : 2.22451
[1mStep[0m  [48/169], [94mLoss[0m : 2.42452
[1mStep[0m  [64/169], [94mLoss[0m : 2.13399
[1mStep[0m  [80/169], [94mLoss[0m : 2.50179
[1mStep[0m  [96/169], [94mLoss[0m : 2.78236
[1mStep[0m  [112/169], [94mLoss[0m : 2.69323
[1mStep[0m  [128/169], [94mLoss[0m : 2.39446
[1mStep[0m  [144/169], [94mLoss[0m : 2.33175
[1mStep[0m  [160/169], [94mLoss[0m : 2.36141

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.438, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40518
[1mStep[0m  [16/169], [94mLoss[0m : 2.63599
[1mStep[0m  [32/169], [94mLoss[0m : 2.55753
[1mStep[0m  [48/169], [94mLoss[0m : 2.48961
[1mStep[0m  [64/169], [94mLoss[0m : 2.32867
[1mStep[0m  [80/169], [94mLoss[0m : 2.44992
[1mStep[0m  [96/169], [94mLoss[0m : 2.31772
[1mStep[0m  [112/169], [94mLoss[0m : 2.80970
[1mStep[0m  [128/169], [94mLoss[0m : 2.76614
[1mStep[0m  [144/169], [94mLoss[0m : 2.81829
[1mStep[0m  [160/169], [94mLoss[0m : 2.30180

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.413, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14759
[1mStep[0m  [16/169], [94mLoss[0m : 2.05955
[1mStep[0m  [32/169], [94mLoss[0m : 2.23663
[1mStep[0m  [48/169], [94mLoss[0m : 2.66478
[1mStep[0m  [64/169], [94mLoss[0m : 2.35203
[1mStep[0m  [80/169], [94mLoss[0m : 2.13612
[1mStep[0m  [96/169], [94mLoss[0m : 2.47652
[1mStep[0m  [112/169], [94mLoss[0m : 2.45602
[1mStep[0m  [128/169], [94mLoss[0m : 2.61124
[1mStep[0m  [144/169], [94mLoss[0m : 2.27228
[1mStep[0m  [160/169], [94mLoss[0m : 2.50296

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.422, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.451
====================================

Phase 2 - Evaluation MAE:  2.4513025326388225
MAE score P1       2.401331
MAE score P2       2.451303
loss               2.457601
learning_rate          0.01
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay           0.01
Name: 22, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.92082
[1mStep[0m  [8/84], [94mLoss[0m : 10.76869
[1mStep[0m  [16/84], [94mLoss[0m : 9.89637
[1mStep[0m  [24/84], [94mLoss[0m : 8.23340
[1mStep[0m  [32/84], [94mLoss[0m : 7.14376
[1mStep[0m  [40/84], [94mLoss[0m : 6.06872
[1mStep[0m  [48/84], [94mLoss[0m : 4.76206
[1mStep[0m  [56/84], [94mLoss[0m : 3.34206
[1mStep[0m  [64/84], [94mLoss[0m : 2.86814
[1mStep[0m  [72/84], [94mLoss[0m : 2.77331
[1mStep[0m  [80/84], [94mLoss[0m : 2.60197

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.102, [92mTest[0m: 10.938, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58035
[1mStep[0m  [8/84], [94mLoss[0m : 2.36950
[1mStep[0m  [16/84], [94mLoss[0m : 2.60370
[1mStep[0m  [24/84], [94mLoss[0m : 2.53320
[1mStep[0m  [32/84], [94mLoss[0m : 2.50707
[1mStep[0m  [40/84], [94mLoss[0m : 2.28797
[1mStep[0m  [48/84], [94mLoss[0m : 2.55478
[1mStep[0m  [56/84], [94mLoss[0m : 2.51789
[1mStep[0m  [64/84], [94mLoss[0m : 2.38002
[1mStep[0m  [72/84], [94mLoss[0m : 2.63054
[1mStep[0m  [80/84], [94mLoss[0m : 2.65969

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.578, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83623
[1mStep[0m  [8/84], [94mLoss[0m : 2.62135
[1mStep[0m  [16/84], [94mLoss[0m : 2.28511
[1mStep[0m  [24/84], [94mLoss[0m : 2.14545
[1mStep[0m  [32/84], [94mLoss[0m : 2.91216
[1mStep[0m  [40/84], [94mLoss[0m : 2.75525
[1mStep[0m  [48/84], [94mLoss[0m : 2.79091
[1mStep[0m  [56/84], [94mLoss[0m : 2.38294
[1mStep[0m  [64/84], [94mLoss[0m : 2.59469
[1mStep[0m  [72/84], [94mLoss[0m : 2.21357
[1mStep[0m  [80/84], [94mLoss[0m : 2.53308

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30789
[1mStep[0m  [8/84], [94mLoss[0m : 2.21008
[1mStep[0m  [16/84], [94mLoss[0m : 2.30874
[1mStep[0m  [24/84], [94mLoss[0m : 2.55905
[1mStep[0m  [32/84], [94mLoss[0m : 2.61860
[1mStep[0m  [40/84], [94mLoss[0m : 2.58124
[1mStep[0m  [48/84], [94mLoss[0m : 2.55046
[1mStep[0m  [56/84], [94mLoss[0m : 2.47333
[1mStep[0m  [64/84], [94mLoss[0m : 2.12660
[1mStep[0m  [72/84], [94mLoss[0m : 2.46428
[1mStep[0m  [80/84], [94mLoss[0m : 2.73220

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64086
[1mStep[0m  [8/84], [94mLoss[0m : 2.14156
[1mStep[0m  [16/84], [94mLoss[0m : 2.66064
[1mStep[0m  [24/84], [94mLoss[0m : 2.32980
[1mStep[0m  [32/84], [94mLoss[0m : 2.41111
[1mStep[0m  [40/84], [94mLoss[0m : 2.71843
[1mStep[0m  [48/84], [94mLoss[0m : 2.15718
[1mStep[0m  [56/84], [94mLoss[0m : 2.49443
[1mStep[0m  [64/84], [94mLoss[0m : 2.33401
[1mStep[0m  [72/84], [94mLoss[0m : 2.66628
[1mStep[0m  [80/84], [94mLoss[0m : 2.63723

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47030
[1mStep[0m  [8/84], [94mLoss[0m : 2.47566
[1mStep[0m  [16/84], [94mLoss[0m : 2.29774
[1mStep[0m  [24/84], [94mLoss[0m : 2.55566
[1mStep[0m  [32/84], [94mLoss[0m : 2.43820
[1mStep[0m  [40/84], [94mLoss[0m : 2.38111
[1mStep[0m  [48/84], [94mLoss[0m : 2.67877
[1mStep[0m  [56/84], [94mLoss[0m : 2.47203
[1mStep[0m  [64/84], [94mLoss[0m : 2.48254
[1mStep[0m  [72/84], [94mLoss[0m : 2.27862
[1mStep[0m  [80/84], [94mLoss[0m : 2.60950

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39049
[1mStep[0m  [8/84], [94mLoss[0m : 2.38265
[1mStep[0m  [16/84], [94mLoss[0m : 2.60515
[1mStep[0m  [24/84], [94mLoss[0m : 2.54899
[1mStep[0m  [32/84], [94mLoss[0m : 2.37657
[1mStep[0m  [40/84], [94mLoss[0m : 2.62741
[1mStep[0m  [48/84], [94mLoss[0m : 2.45436
[1mStep[0m  [56/84], [94mLoss[0m : 2.38544
[1mStep[0m  [64/84], [94mLoss[0m : 2.51596
[1mStep[0m  [72/84], [94mLoss[0m : 2.19094
[1mStep[0m  [80/84], [94mLoss[0m : 2.54290

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22450
[1mStep[0m  [8/84], [94mLoss[0m : 2.33537
[1mStep[0m  [16/84], [94mLoss[0m : 2.83883
[1mStep[0m  [24/84], [94mLoss[0m : 2.40939
[1mStep[0m  [32/84], [94mLoss[0m : 2.36489
[1mStep[0m  [40/84], [94mLoss[0m : 2.27491
[1mStep[0m  [48/84], [94mLoss[0m : 2.38518
[1mStep[0m  [56/84], [94mLoss[0m : 2.37059
[1mStep[0m  [64/84], [94mLoss[0m : 2.47254
[1mStep[0m  [72/84], [94mLoss[0m : 2.37603
[1mStep[0m  [80/84], [94mLoss[0m : 2.39465

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52623
[1mStep[0m  [8/84], [94mLoss[0m : 2.32699
[1mStep[0m  [16/84], [94mLoss[0m : 2.26062
[1mStep[0m  [24/84], [94mLoss[0m : 2.56782
[1mStep[0m  [32/84], [94mLoss[0m : 2.57081
[1mStep[0m  [40/84], [94mLoss[0m : 2.55240
[1mStep[0m  [48/84], [94mLoss[0m : 2.37008
[1mStep[0m  [56/84], [94mLoss[0m : 2.61770
[1mStep[0m  [64/84], [94mLoss[0m : 2.35533
[1mStep[0m  [72/84], [94mLoss[0m : 2.51061
[1mStep[0m  [80/84], [94mLoss[0m : 2.27830

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60527
[1mStep[0m  [8/84], [94mLoss[0m : 2.26513
[1mStep[0m  [16/84], [94mLoss[0m : 2.23881
[1mStep[0m  [24/84], [94mLoss[0m : 2.62665
[1mStep[0m  [32/84], [94mLoss[0m : 2.58374
[1mStep[0m  [40/84], [94mLoss[0m : 2.49145
[1mStep[0m  [48/84], [94mLoss[0m : 2.33278
[1mStep[0m  [56/84], [94mLoss[0m : 2.40566
[1mStep[0m  [64/84], [94mLoss[0m : 2.06237
[1mStep[0m  [72/84], [94mLoss[0m : 2.58219
[1mStep[0m  [80/84], [94mLoss[0m : 2.29007

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26751
[1mStep[0m  [8/84], [94mLoss[0m : 2.34547
[1mStep[0m  [16/84], [94mLoss[0m : 2.43081
[1mStep[0m  [24/84], [94mLoss[0m : 2.21576
[1mStep[0m  [32/84], [94mLoss[0m : 2.26427
[1mStep[0m  [40/84], [94mLoss[0m : 2.24672
[1mStep[0m  [48/84], [94mLoss[0m : 2.63716
[1mStep[0m  [56/84], [94mLoss[0m : 2.26799
[1mStep[0m  [64/84], [94mLoss[0m : 2.63831
[1mStep[0m  [72/84], [94mLoss[0m : 2.36577
[1mStep[0m  [80/84], [94mLoss[0m : 2.78387

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44674
[1mStep[0m  [8/84], [94mLoss[0m : 2.27185
[1mStep[0m  [16/84], [94mLoss[0m : 2.40698
[1mStep[0m  [24/84], [94mLoss[0m : 2.68082
[1mStep[0m  [32/84], [94mLoss[0m : 2.64155
[1mStep[0m  [40/84], [94mLoss[0m : 2.43985
[1mStep[0m  [48/84], [94mLoss[0m : 2.29048
[1mStep[0m  [56/84], [94mLoss[0m : 2.35482
[1mStep[0m  [64/84], [94mLoss[0m : 2.44118
[1mStep[0m  [72/84], [94mLoss[0m : 2.23593
[1mStep[0m  [80/84], [94mLoss[0m : 2.40091

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48208
[1mStep[0m  [8/84], [94mLoss[0m : 2.15481
[1mStep[0m  [16/84], [94mLoss[0m : 2.49956
[1mStep[0m  [24/84], [94mLoss[0m : 2.13790
[1mStep[0m  [32/84], [94mLoss[0m : 2.41479
[1mStep[0m  [40/84], [94mLoss[0m : 2.21303
[1mStep[0m  [48/84], [94mLoss[0m : 2.57074
[1mStep[0m  [56/84], [94mLoss[0m : 2.25304
[1mStep[0m  [64/84], [94mLoss[0m : 2.24548
[1mStep[0m  [72/84], [94mLoss[0m : 2.28265
[1mStep[0m  [80/84], [94mLoss[0m : 2.45880

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21759
[1mStep[0m  [8/84], [94mLoss[0m : 2.36524
[1mStep[0m  [16/84], [94mLoss[0m : 2.26902
[1mStep[0m  [24/84], [94mLoss[0m : 2.09291
[1mStep[0m  [32/84], [94mLoss[0m : 2.56786
[1mStep[0m  [40/84], [94mLoss[0m : 2.26800
[1mStep[0m  [48/84], [94mLoss[0m : 2.60731
[1mStep[0m  [56/84], [94mLoss[0m : 2.80807
[1mStep[0m  [64/84], [94mLoss[0m : 2.59487
[1mStep[0m  [72/84], [94mLoss[0m : 2.25003
[1mStep[0m  [80/84], [94mLoss[0m : 2.42579

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30942
[1mStep[0m  [8/84], [94mLoss[0m : 2.32412
[1mStep[0m  [16/84], [94mLoss[0m : 2.58477
[1mStep[0m  [24/84], [94mLoss[0m : 2.24987
[1mStep[0m  [32/84], [94mLoss[0m : 2.43859
[1mStep[0m  [40/84], [94mLoss[0m : 2.46105
[1mStep[0m  [48/84], [94mLoss[0m : 2.32625
[1mStep[0m  [56/84], [94mLoss[0m : 2.08603
[1mStep[0m  [64/84], [94mLoss[0m : 2.11755
[1mStep[0m  [72/84], [94mLoss[0m : 2.41650
[1mStep[0m  [80/84], [94mLoss[0m : 2.38151

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44795
[1mStep[0m  [8/84], [94mLoss[0m : 2.36504
[1mStep[0m  [16/84], [94mLoss[0m : 2.31568
[1mStep[0m  [24/84], [94mLoss[0m : 2.48586
[1mStep[0m  [32/84], [94mLoss[0m : 2.58142
[1mStep[0m  [40/84], [94mLoss[0m : 2.41705
[1mStep[0m  [48/84], [94mLoss[0m : 2.45896
[1mStep[0m  [56/84], [94mLoss[0m : 2.19122
[1mStep[0m  [64/84], [94mLoss[0m : 2.85747
[1mStep[0m  [72/84], [94mLoss[0m : 2.43683
[1mStep[0m  [80/84], [94mLoss[0m : 2.20052

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11468
[1mStep[0m  [8/84], [94mLoss[0m : 2.43882
[1mStep[0m  [16/84], [94mLoss[0m : 2.47635
[1mStep[0m  [24/84], [94mLoss[0m : 2.43959
[1mStep[0m  [32/84], [94mLoss[0m : 2.43905
[1mStep[0m  [40/84], [94mLoss[0m : 2.39544
[1mStep[0m  [48/84], [94mLoss[0m : 2.38191
[1mStep[0m  [56/84], [94mLoss[0m : 2.43399
[1mStep[0m  [64/84], [94mLoss[0m : 2.45618
[1mStep[0m  [72/84], [94mLoss[0m : 2.49437
[1mStep[0m  [80/84], [94mLoss[0m : 2.21496

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75654
[1mStep[0m  [8/84], [94mLoss[0m : 2.11912
[1mStep[0m  [16/84], [94mLoss[0m : 2.16216
[1mStep[0m  [24/84], [94mLoss[0m : 2.50846
[1mStep[0m  [32/84], [94mLoss[0m : 2.48330
[1mStep[0m  [40/84], [94mLoss[0m : 2.44358
[1mStep[0m  [48/84], [94mLoss[0m : 2.36921
[1mStep[0m  [56/84], [94mLoss[0m : 2.25953
[1mStep[0m  [64/84], [94mLoss[0m : 2.34214
[1mStep[0m  [72/84], [94mLoss[0m : 2.18350
[1mStep[0m  [80/84], [94mLoss[0m : 2.32489

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26238
[1mStep[0m  [8/84], [94mLoss[0m : 2.45334
[1mStep[0m  [16/84], [94mLoss[0m : 2.57272
[1mStep[0m  [24/84], [94mLoss[0m : 2.42094
[1mStep[0m  [32/84], [94mLoss[0m : 2.28513
[1mStep[0m  [40/84], [94mLoss[0m : 2.51395
[1mStep[0m  [48/84], [94mLoss[0m : 2.50521
[1mStep[0m  [56/84], [94mLoss[0m : 2.25355
[1mStep[0m  [64/84], [94mLoss[0m : 2.27743
[1mStep[0m  [72/84], [94mLoss[0m : 2.73584
[1mStep[0m  [80/84], [94mLoss[0m : 2.24124

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53355
[1mStep[0m  [8/84], [94mLoss[0m : 2.45874
[1mStep[0m  [16/84], [94mLoss[0m : 2.34244
[1mStep[0m  [24/84], [94mLoss[0m : 2.33694
[1mStep[0m  [32/84], [94mLoss[0m : 2.30609
[1mStep[0m  [40/84], [94mLoss[0m : 2.36211
[1mStep[0m  [48/84], [94mLoss[0m : 2.11730
[1mStep[0m  [56/84], [94mLoss[0m : 2.43232
[1mStep[0m  [64/84], [94mLoss[0m : 2.17324
[1mStep[0m  [72/84], [94mLoss[0m : 2.40955
[1mStep[0m  [80/84], [94mLoss[0m : 2.01092

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34929
[1mStep[0m  [8/84], [94mLoss[0m : 2.20794
[1mStep[0m  [16/84], [94mLoss[0m : 2.55243
[1mStep[0m  [24/84], [94mLoss[0m : 2.16504
[1mStep[0m  [32/84], [94mLoss[0m : 2.37589
[1mStep[0m  [40/84], [94mLoss[0m : 2.68907
[1mStep[0m  [48/84], [94mLoss[0m : 1.99533
[1mStep[0m  [56/84], [94mLoss[0m : 2.62269
[1mStep[0m  [64/84], [94mLoss[0m : 2.33841
[1mStep[0m  [72/84], [94mLoss[0m : 2.38431
[1mStep[0m  [80/84], [94mLoss[0m : 2.50406

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.371, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23619
[1mStep[0m  [8/84], [94mLoss[0m : 2.11334
[1mStep[0m  [16/84], [94mLoss[0m : 2.55955
[1mStep[0m  [24/84], [94mLoss[0m : 2.42068
[1mStep[0m  [32/84], [94mLoss[0m : 2.38756
[1mStep[0m  [40/84], [94mLoss[0m : 2.56857
[1mStep[0m  [48/84], [94mLoss[0m : 2.08549
[1mStep[0m  [56/84], [94mLoss[0m : 2.58072
[1mStep[0m  [64/84], [94mLoss[0m : 2.27352
[1mStep[0m  [72/84], [94mLoss[0m : 2.50395
[1mStep[0m  [80/84], [94mLoss[0m : 2.95143

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.345, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46365
[1mStep[0m  [8/84], [94mLoss[0m : 1.95540
[1mStep[0m  [16/84], [94mLoss[0m : 2.43767
[1mStep[0m  [24/84], [94mLoss[0m : 2.26220
[1mStep[0m  [32/84], [94mLoss[0m : 2.48883
[1mStep[0m  [40/84], [94mLoss[0m : 2.25103
[1mStep[0m  [48/84], [94mLoss[0m : 2.29534
[1mStep[0m  [56/84], [94mLoss[0m : 2.66797
[1mStep[0m  [64/84], [94mLoss[0m : 2.67301
[1mStep[0m  [72/84], [94mLoss[0m : 2.46605
[1mStep[0m  [80/84], [94mLoss[0m : 2.37172

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34238
[1mStep[0m  [8/84], [94mLoss[0m : 2.24909
[1mStep[0m  [16/84], [94mLoss[0m : 2.52023
[1mStep[0m  [24/84], [94mLoss[0m : 2.29383
[1mStep[0m  [32/84], [94mLoss[0m : 2.10745
[1mStep[0m  [40/84], [94mLoss[0m : 2.32447
[1mStep[0m  [48/84], [94mLoss[0m : 2.38227
[1mStep[0m  [56/84], [94mLoss[0m : 2.19097
[1mStep[0m  [64/84], [94mLoss[0m : 2.08454
[1mStep[0m  [72/84], [94mLoss[0m : 2.08341
[1mStep[0m  [80/84], [94mLoss[0m : 2.49996

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42290
[1mStep[0m  [8/84], [94mLoss[0m : 2.15995
[1mStep[0m  [16/84], [94mLoss[0m : 2.17403
[1mStep[0m  [24/84], [94mLoss[0m : 2.23219
[1mStep[0m  [32/84], [94mLoss[0m : 2.28475
[1mStep[0m  [40/84], [94mLoss[0m : 2.20492
[1mStep[0m  [48/84], [94mLoss[0m : 2.30523
[1mStep[0m  [56/84], [94mLoss[0m : 2.59218
[1mStep[0m  [64/84], [94mLoss[0m : 2.57416
[1mStep[0m  [72/84], [94mLoss[0m : 2.21224
[1mStep[0m  [80/84], [94mLoss[0m : 2.33165

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40506
[1mStep[0m  [8/84], [94mLoss[0m : 2.34228
[1mStep[0m  [16/84], [94mLoss[0m : 2.32961
[1mStep[0m  [24/84], [94mLoss[0m : 2.33304
[1mStep[0m  [32/84], [94mLoss[0m : 2.00056
[1mStep[0m  [40/84], [94mLoss[0m : 2.36322
[1mStep[0m  [48/84], [94mLoss[0m : 2.13345
[1mStep[0m  [56/84], [94mLoss[0m : 2.49311
[1mStep[0m  [64/84], [94mLoss[0m : 2.30456
[1mStep[0m  [72/84], [94mLoss[0m : 2.34803
[1mStep[0m  [80/84], [94mLoss[0m : 2.52969

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56168
[1mStep[0m  [8/84], [94mLoss[0m : 2.43840
[1mStep[0m  [16/84], [94mLoss[0m : 2.38007
[1mStep[0m  [24/84], [94mLoss[0m : 2.48153
[1mStep[0m  [32/84], [94mLoss[0m : 2.42906
[1mStep[0m  [40/84], [94mLoss[0m : 2.31613
[1mStep[0m  [48/84], [94mLoss[0m : 2.22380
[1mStep[0m  [56/84], [94mLoss[0m : 2.24962
[1mStep[0m  [64/84], [94mLoss[0m : 2.33880
[1mStep[0m  [72/84], [94mLoss[0m : 2.06813
[1mStep[0m  [80/84], [94mLoss[0m : 2.07392

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19606
[1mStep[0m  [8/84], [94mLoss[0m : 2.38866
[1mStep[0m  [16/84], [94mLoss[0m : 2.13291
[1mStep[0m  [24/84], [94mLoss[0m : 2.04818
[1mStep[0m  [32/84], [94mLoss[0m : 2.55879
[1mStep[0m  [40/84], [94mLoss[0m : 2.49876
[1mStep[0m  [48/84], [94mLoss[0m : 2.56146
[1mStep[0m  [56/84], [94mLoss[0m : 2.27238
[1mStep[0m  [64/84], [94mLoss[0m : 2.45794
[1mStep[0m  [72/84], [94mLoss[0m : 2.09861
[1mStep[0m  [80/84], [94mLoss[0m : 2.48016

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33572
[1mStep[0m  [8/84], [94mLoss[0m : 2.25806
[1mStep[0m  [16/84], [94mLoss[0m : 2.04875
[1mStep[0m  [24/84], [94mLoss[0m : 2.18931
[1mStep[0m  [32/84], [94mLoss[0m : 2.31028
[1mStep[0m  [40/84], [94mLoss[0m : 2.52341
[1mStep[0m  [48/84], [94mLoss[0m : 2.31373
[1mStep[0m  [56/84], [94mLoss[0m : 2.75944
[1mStep[0m  [64/84], [94mLoss[0m : 2.52618
[1mStep[0m  [72/84], [94mLoss[0m : 2.27363
[1mStep[0m  [80/84], [94mLoss[0m : 2.23831

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44609
[1mStep[0m  [8/84], [94mLoss[0m : 2.19692
[1mStep[0m  [16/84], [94mLoss[0m : 2.57781
[1mStep[0m  [24/84], [94mLoss[0m : 2.43622
[1mStep[0m  [32/84], [94mLoss[0m : 2.15077
[1mStep[0m  [40/84], [94mLoss[0m : 2.19387
[1mStep[0m  [48/84], [94mLoss[0m : 2.29658
[1mStep[0m  [56/84], [94mLoss[0m : 2.47611
[1mStep[0m  [64/84], [94mLoss[0m : 2.16307
[1mStep[0m  [72/84], [94mLoss[0m : 2.59772
[1mStep[0m  [80/84], [94mLoss[0m : 2.15264

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.331392620291029
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.29737
[1mStep[0m  [8/84], [94mLoss[0m : 2.76704
[1mStep[0m  [16/84], [94mLoss[0m : 3.14264
[1mStep[0m  [24/84], [94mLoss[0m : 2.21106
[1mStep[0m  [32/84], [94mLoss[0m : 2.12440
[1mStep[0m  [40/84], [94mLoss[0m : 2.37406
[1mStep[0m  [48/84], [94mLoss[0m : 2.72956
[1mStep[0m  [56/84], [94mLoss[0m : 2.37679
[1mStep[0m  [64/84], [94mLoss[0m : 2.23732
[1mStep[0m  [72/84], [94mLoss[0m : 2.47796
[1mStep[0m  [80/84], [94mLoss[0m : 2.75857

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20143
[1mStep[0m  [8/84], [94mLoss[0m : 2.39981
[1mStep[0m  [16/84], [94mLoss[0m : 2.34947
[1mStep[0m  [24/84], [94mLoss[0m : 2.09794
[1mStep[0m  [32/84], [94mLoss[0m : 2.19408
[1mStep[0m  [40/84], [94mLoss[0m : 2.00581
[1mStep[0m  [48/84], [94mLoss[0m : 2.20785
[1mStep[0m  [56/84], [94mLoss[0m : 2.04224
[1mStep[0m  [64/84], [94mLoss[0m : 2.08415
[1mStep[0m  [72/84], [94mLoss[0m : 2.58750
[1mStep[0m  [80/84], [94mLoss[0m : 2.24667

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12666
[1mStep[0m  [8/84], [94mLoss[0m : 2.12590
[1mStep[0m  [16/84], [94mLoss[0m : 2.10574
[1mStep[0m  [24/84], [94mLoss[0m : 2.39053
[1mStep[0m  [32/84], [94mLoss[0m : 2.01530
[1mStep[0m  [40/84], [94mLoss[0m : 2.13309
[1mStep[0m  [48/84], [94mLoss[0m : 2.35647
[1mStep[0m  [56/84], [94mLoss[0m : 2.06644
[1mStep[0m  [64/84], [94mLoss[0m : 2.25061
[1mStep[0m  [72/84], [94mLoss[0m : 2.43422
[1mStep[0m  [80/84], [94mLoss[0m : 2.26758

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01626
[1mStep[0m  [8/84], [94mLoss[0m : 2.12935
[1mStep[0m  [16/84], [94mLoss[0m : 1.93655
[1mStep[0m  [24/84], [94mLoss[0m : 2.02429
[1mStep[0m  [32/84], [94mLoss[0m : 2.35007
[1mStep[0m  [40/84], [94mLoss[0m : 2.10596
[1mStep[0m  [48/84], [94mLoss[0m : 2.39564
[1mStep[0m  [56/84], [94mLoss[0m : 2.34546
[1mStep[0m  [64/84], [94mLoss[0m : 2.39104
[1mStep[0m  [72/84], [94mLoss[0m : 2.19477
[1mStep[0m  [80/84], [94mLoss[0m : 2.20818

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08560
[1mStep[0m  [8/84], [94mLoss[0m : 2.01858
[1mStep[0m  [16/84], [94mLoss[0m : 1.97658
[1mStep[0m  [24/84], [94mLoss[0m : 2.33097
[1mStep[0m  [32/84], [94mLoss[0m : 1.66397
[1mStep[0m  [40/84], [94mLoss[0m : 2.05618
[1mStep[0m  [48/84], [94mLoss[0m : 2.04415
[1mStep[0m  [56/84], [94mLoss[0m : 2.11298
[1mStep[0m  [64/84], [94mLoss[0m : 2.16403
[1mStep[0m  [72/84], [94mLoss[0m : 2.01798
[1mStep[0m  [80/84], [94mLoss[0m : 1.96363

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74843
[1mStep[0m  [8/84], [94mLoss[0m : 2.05873
[1mStep[0m  [16/84], [94mLoss[0m : 1.82728
[1mStep[0m  [24/84], [94mLoss[0m : 1.72304
[1mStep[0m  [32/84], [94mLoss[0m : 2.05568
[1mStep[0m  [40/84], [94mLoss[0m : 1.97223
[1mStep[0m  [48/84], [94mLoss[0m : 2.20736
[1mStep[0m  [56/84], [94mLoss[0m : 2.03939
[1mStep[0m  [64/84], [94mLoss[0m : 2.21460
[1mStep[0m  [72/84], [94mLoss[0m : 1.89679
[1mStep[0m  [80/84], [94mLoss[0m : 1.83713

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84727
[1mStep[0m  [8/84], [94mLoss[0m : 2.07461
[1mStep[0m  [16/84], [94mLoss[0m : 1.92319
[1mStep[0m  [24/84], [94mLoss[0m : 2.00921
[1mStep[0m  [32/84], [94mLoss[0m : 1.99490
[1mStep[0m  [40/84], [94mLoss[0m : 1.77917
[1mStep[0m  [48/84], [94mLoss[0m : 1.73614
[1mStep[0m  [56/84], [94mLoss[0m : 1.98750
[1mStep[0m  [64/84], [94mLoss[0m : 1.89290
[1mStep[0m  [72/84], [94mLoss[0m : 1.84862
[1mStep[0m  [80/84], [94mLoss[0m : 2.21412

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82610
[1mStep[0m  [8/84], [94mLoss[0m : 1.49233
[1mStep[0m  [16/84], [94mLoss[0m : 1.74217
[1mStep[0m  [24/84], [94mLoss[0m : 1.76087
[1mStep[0m  [32/84], [94mLoss[0m : 1.58622
[1mStep[0m  [40/84], [94mLoss[0m : 1.94993
[1mStep[0m  [48/84], [94mLoss[0m : 2.00604
[1mStep[0m  [56/84], [94mLoss[0m : 2.22332
[1mStep[0m  [64/84], [94mLoss[0m : 1.87476
[1mStep[0m  [72/84], [94mLoss[0m : 2.10489
[1mStep[0m  [80/84], [94mLoss[0m : 1.60855

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60689
[1mStep[0m  [8/84], [94mLoss[0m : 1.71814
[1mStep[0m  [16/84], [94mLoss[0m : 1.78414
[1mStep[0m  [24/84], [94mLoss[0m : 1.67253
[1mStep[0m  [32/84], [94mLoss[0m : 1.56147
[1mStep[0m  [40/84], [94mLoss[0m : 1.98016
[1mStep[0m  [48/84], [94mLoss[0m : 1.92860
[1mStep[0m  [56/84], [94mLoss[0m : 1.69127
[1mStep[0m  [64/84], [94mLoss[0m : 1.83744
[1mStep[0m  [72/84], [94mLoss[0m : 1.76453
[1mStep[0m  [80/84], [94mLoss[0m : 2.14851

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82569
[1mStep[0m  [8/84], [94mLoss[0m : 1.61655
[1mStep[0m  [16/84], [94mLoss[0m : 1.68171
[1mStep[0m  [24/84], [94mLoss[0m : 1.51241
[1mStep[0m  [32/84], [94mLoss[0m : 1.61181
[1mStep[0m  [40/84], [94mLoss[0m : 1.78616
[1mStep[0m  [48/84], [94mLoss[0m : 1.73967
[1mStep[0m  [56/84], [94mLoss[0m : 1.62982
[1mStep[0m  [64/84], [94mLoss[0m : 2.04386
[1mStep[0m  [72/84], [94mLoss[0m : 2.00802
[1mStep[0m  [80/84], [94mLoss[0m : 1.56895

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.751, [92mTest[0m: 2.568, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51657
[1mStep[0m  [8/84], [94mLoss[0m : 1.44546
[1mStep[0m  [16/84], [94mLoss[0m : 1.66610
[1mStep[0m  [24/84], [94mLoss[0m : 1.66401
[1mStep[0m  [32/84], [94mLoss[0m : 1.49343
[1mStep[0m  [40/84], [94mLoss[0m : 1.70712
[1mStep[0m  [48/84], [94mLoss[0m : 1.64705
[1mStep[0m  [56/84], [94mLoss[0m : 1.62790
[1mStep[0m  [64/84], [94mLoss[0m : 1.82742
[1mStep[0m  [72/84], [94mLoss[0m : 1.55117
[1mStep[0m  [80/84], [94mLoss[0m : 1.94633

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57563
[1mStep[0m  [8/84], [94mLoss[0m : 1.52908
[1mStep[0m  [16/84], [94mLoss[0m : 1.48814
[1mStep[0m  [24/84], [94mLoss[0m : 1.57120
[1mStep[0m  [32/84], [94mLoss[0m : 1.30299
[1mStep[0m  [40/84], [94mLoss[0m : 1.81986
[1mStep[0m  [48/84], [94mLoss[0m : 1.61843
[1mStep[0m  [56/84], [94mLoss[0m : 1.54753
[1mStep[0m  [64/84], [94mLoss[0m : 1.56220
[1mStep[0m  [72/84], [94mLoss[0m : 1.97894
[1mStep[0m  [80/84], [94mLoss[0m : 1.82436

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62742
[1mStep[0m  [8/84], [94mLoss[0m : 1.51704
[1mStep[0m  [16/84], [94mLoss[0m : 1.53744
[1mStep[0m  [24/84], [94mLoss[0m : 1.63849
[1mStep[0m  [32/84], [94mLoss[0m : 1.48931
[1mStep[0m  [40/84], [94mLoss[0m : 1.41519
[1mStep[0m  [48/84], [94mLoss[0m : 1.62502
[1mStep[0m  [56/84], [94mLoss[0m : 1.79744
[1mStep[0m  [64/84], [94mLoss[0m : 1.58921
[1mStep[0m  [72/84], [94mLoss[0m : 1.58039
[1mStep[0m  [80/84], [94mLoss[0m : 1.67300

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.45495
[1mStep[0m  [8/84], [94mLoss[0m : 1.44231
[1mStep[0m  [16/84], [94mLoss[0m : 1.59594
[1mStep[0m  [24/84], [94mLoss[0m : 1.85309
[1mStep[0m  [32/84], [94mLoss[0m : 1.49167
[1mStep[0m  [40/84], [94mLoss[0m : 1.41289
[1mStep[0m  [48/84], [94mLoss[0m : 1.61997
[1mStep[0m  [56/84], [94mLoss[0m : 1.47620
[1mStep[0m  [64/84], [94mLoss[0m : 1.68482
[1mStep[0m  [72/84], [94mLoss[0m : 1.45850
[1mStep[0m  [80/84], [94mLoss[0m : 1.58636

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36659
[1mStep[0m  [8/84], [94mLoss[0m : 1.58667
[1mStep[0m  [16/84], [94mLoss[0m : 1.48177
[1mStep[0m  [24/84], [94mLoss[0m : 1.43227
[1mStep[0m  [32/84], [94mLoss[0m : 1.74406
[1mStep[0m  [40/84], [94mLoss[0m : 1.54692
[1mStep[0m  [48/84], [94mLoss[0m : 1.38830
[1mStep[0m  [56/84], [94mLoss[0m : 1.33805
[1mStep[0m  [64/84], [94mLoss[0m : 1.88424
[1mStep[0m  [72/84], [94mLoss[0m : 1.27378
[1mStep[0m  [80/84], [94mLoss[0m : 1.63612

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.519, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.22284
[1mStep[0m  [8/84], [94mLoss[0m : 1.55338
[1mStep[0m  [16/84], [94mLoss[0m : 1.37921
[1mStep[0m  [24/84], [94mLoss[0m : 1.43874
[1mStep[0m  [32/84], [94mLoss[0m : 1.10878
[1mStep[0m  [40/84], [94mLoss[0m : 1.49109
[1mStep[0m  [48/84], [94mLoss[0m : 1.40514
[1mStep[0m  [56/84], [94mLoss[0m : 1.66066
[1mStep[0m  [64/84], [94mLoss[0m : 1.60597
[1mStep[0m  [72/84], [94mLoss[0m : 1.47830
[1mStep[0m  [80/84], [94mLoss[0m : 1.67038

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46738
[1mStep[0m  [8/84], [94mLoss[0m : 1.24249
[1mStep[0m  [16/84], [94mLoss[0m : 1.30195
[1mStep[0m  [24/84], [94mLoss[0m : 1.37553
[1mStep[0m  [32/84], [94mLoss[0m : 1.33388
[1mStep[0m  [40/84], [94mLoss[0m : 1.48188
[1mStep[0m  [48/84], [94mLoss[0m : 1.48979
[1mStep[0m  [56/84], [94mLoss[0m : 1.30295
[1mStep[0m  [64/84], [94mLoss[0m : 1.59076
[1mStep[0m  [72/84], [94mLoss[0m : 1.35813
[1mStep[0m  [80/84], [94mLoss[0m : 1.46309

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.24685
[1mStep[0m  [8/84], [94mLoss[0m : 1.14505
[1mStep[0m  [16/84], [94mLoss[0m : 1.27586
[1mStep[0m  [24/84], [94mLoss[0m : 1.57562
[1mStep[0m  [32/84], [94mLoss[0m : 1.31917
[1mStep[0m  [40/84], [94mLoss[0m : 1.31903
[1mStep[0m  [48/84], [94mLoss[0m : 1.42651
[1mStep[0m  [56/84], [94mLoss[0m : 1.66478
[1mStep[0m  [64/84], [94mLoss[0m : 1.19999
[1mStep[0m  [72/84], [94mLoss[0m : 1.54812
[1mStep[0m  [80/84], [94mLoss[0m : 1.41556

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.415, [92mTest[0m: 2.564, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37836
[1mStep[0m  [8/84], [94mLoss[0m : 1.29990
[1mStep[0m  [16/84], [94mLoss[0m : 1.46109
[1mStep[0m  [24/84], [94mLoss[0m : 1.45550
[1mStep[0m  [32/84], [94mLoss[0m : 1.46042
[1mStep[0m  [40/84], [94mLoss[0m : 1.29006
[1mStep[0m  [48/84], [94mLoss[0m : 1.25778
[1mStep[0m  [56/84], [94mLoss[0m : 1.40814
[1mStep[0m  [64/84], [94mLoss[0m : 1.48011
[1mStep[0m  [72/84], [94mLoss[0m : 1.48797
[1mStep[0m  [80/84], [94mLoss[0m : 1.38344

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.388, [92mTest[0m: 2.547, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.25402
[1mStep[0m  [8/84], [94mLoss[0m : 1.29715
[1mStep[0m  [16/84], [94mLoss[0m : 1.41999
[1mStep[0m  [24/84], [94mLoss[0m : 1.37518
[1mStep[0m  [32/84], [94mLoss[0m : 1.66591
[1mStep[0m  [40/84], [94mLoss[0m : 1.57346
[1mStep[0m  [48/84], [94mLoss[0m : 1.33199
[1mStep[0m  [56/84], [94mLoss[0m : 1.43854
[1mStep[0m  [64/84], [94mLoss[0m : 1.46543
[1mStep[0m  [72/84], [94mLoss[0m : 1.36266
[1mStep[0m  [80/84], [94mLoss[0m : 1.64585

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.355, [92mTest[0m: 2.515, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 19 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.537
====================================

Phase 2 - Evaluation MAE:  2.5371236971446445
MAE score P1      2.331393
MAE score P2      2.537124
loss              1.354662
learning_rate         0.01
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 23, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.68657
[1mStep[0m  [4/42], [94mLoss[0m : 10.72292
[1mStep[0m  [8/42], [94mLoss[0m : 10.89835
[1mStep[0m  [12/42], [94mLoss[0m : 11.16665
[1mStep[0m  [16/42], [94mLoss[0m : 10.60393
[1mStep[0m  [20/42], [94mLoss[0m : 10.74370
[1mStep[0m  [24/42], [94mLoss[0m : 10.68526
[1mStep[0m  [28/42], [94mLoss[0m : 10.57665
[1mStep[0m  [32/42], [94mLoss[0m : 10.42122
[1mStep[0m  [36/42], [94mLoss[0m : 10.08419
[1mStep[0m  [40/42], [94mLoss[0m : 10.02959

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.604, [92mTest[0m: 10.877, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.29132
[1mStep[0m  [4/42], [94mLoss[0m : 10.43989
[1mStep[0m  [8/42], [94mLoss[0m : 9.99624
[1mStep[0m  [12/42], [94mLoss[0m : 10.27038
[1mStep[0m  [16/42], [94mLoss[0m : 10.20589
[1mStep[0m  [20/42], [94mLoss[0m : 10.03970
[1mStep[0m  [24/42], [94mLoss[0m : 10.05198
[1mStep[0m  [28/42], [94mLoss[0m : 9.31835
[1mStep[0m  [32/42], [94mLoss[0m : 9.89119
[1mStep[0m  [36/42], [94mLoss[0m : 9.27722
[1mStep[0m  [40/42], [94mLoss[0m : 9.30543

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.831, [92mTest[0m: 10.111, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.26703
[1mStep[0m  [4/42], [94mLoss[0m : 9.28733
[1mStep[0m  [8/42], [94mLoss[0m : 8.74964
[1mStep[0m  [12/42], [94mLoss[0m : 9.15453
[1mStep[0m  [16/42], [94mLoss[0m : 9.16063
[1mStep[0m  [20/42], [94mLoss[0m : 8.65237
[1mStep[0m  [24/42], [94mLoss[0m : 8.90537
[1mStep[0m  [28/42], [94mLoss[0m : 8.40878
[1mStep[0m  [32/42], [94mLoss[0m : 8.10438
[1mStep[0m  [36/42], [94mLoss[0m : 8.28238
[1mStep[0m  [40/42], [94mLoss[0m : 8.21699

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.818, [92mTest[0m: 9.125, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.31822
[1mStep[0m  [4/42], [94mLoss[0m : 8.31771
[1mStep[0m  [8/42], [94mLoss[0m : 7.98256
[1mStep[0m  [12/42], [94mLoss[0m : 7.76674
[1mStep[0m  [16/42], [94mLoss[0m : 7.59267
[1mStep[0m  [20/42], [94mLoss[0m : 7.52167
[1mStep[0m  [24/42], [94mLoss[0m : 7.36361
[1mStep[0m  [28/42], [94mLoss[0m : 7.06733
[1mStep[0m  [32/42], [94mLoss[0m : 6.94535
[1mStep[0m  [36/42], [94mLoss[0m : 6.97534
[1mStep[0m  [40/42], [94mLoss[0m : 6.88623

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.539, [92mTest[0m: 7.873, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.03077
[1mStep[0m  [4/42], [94mLoss[0m : 6.87937
[1mStep[0m  [8/42], [94mLoss[0m : 6.73164
[1mStep[0m  [12/42], [94mLoss[0m : 6.78641
[1mStep[0m  [16/42], [94mLoss[0m : 6.32055
[1mStep[0m  [20/42], [94mLoss[0m : 6.35338
[1mStep[0m  [24/42], [94mLoss[0m : 5.97581
[1mStep[0m  [28/42], [94mLoss[0m : 6.13086
[1mStep[0m  [32/42], [94mLoss[0m : 6.27204
[1mStep[0m  [36/42], [94mLoss[0m : 5.95764
[1mStep[0m  [40/42], [94mLoss[0m : 5.94155

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.483, [92mTest[0m: 6.306, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.17045
[1mStep[0m  [4/42], [94mLoss[0m : 5.91574
[1mStep[0m  [8/42], [94mLoss[0m : 5.93531
[1mStep[0m  [12/42], [94mLoss[0m : 5.61705
[1mStep[0m  [16/42], [94mLoss[0m : 5.20220
[1mStep[0m  [20/42], [94mLoss[0m : 5.80826
[1mStep[0m  [24/42], [94mLoss[0m : 5.42993
[1mStep[0m  [28/42], [94mLoss[0m : 6.03922
[1mStep[0m  [32/42], [94mLoss[0m : 5.27920
[1mStep[0m  [36/42], [94mLoss[0m : 5.30581
[1mStep[0m  [40/42], [94mLoss[0m : 5.50144

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.628, [92mTest[0m: 5.177, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.82766
[1mStep[0m  [4/42], [94mLoss[0m : 4.80103
[1mStep[0m  [8/42], [94mLoss[0m : 5.18338
[1mStep[0m  [12/42], [94mLoss[0m : 4.91709
[1mStep[0m  [16/42], [94mLoss[0m : 4.82277
[1mStep[0m  [20/42], [94mLoss[0m : 4.82072
[1mStep[0m  [24/42], [94mLoss[0m : 4.66971
[1mStep[0m  [28/42], [94mLoss[0m : 4.54916
[1mStep[0m  [32/42], [94mLoss[0m : 5.01977
[1mStep[0m  [36/42], [94mLoss[0m : 4.53021
[1mStep[0m  [40/42], [94mLoss[0m : 4.43370

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.834, [92mTest[0m: 4.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.41854
[1mStep[0m  [4/42], [94mLoss[0m : 4.38721
[1mStep[0m  [8/42], [94mLoss[0m : 4.35339
[1mStep[0m  [12/42], [94mLoss[0m : 4.01463
[1mStep[0m  [16/42], [94mLoss[0m : 3.53644
[1mStep[0m  [20/42], [94mLoss[0m : 4.09673
[1mStep[0m  [24/42], [94mLoss[0m : 3.66802
[1mStep[0m  [28/42], [94mLoss[0m : 3.77075
[1mStep[0m  [32/42], [94mLoss[0m : 3.77657
[1mStep[0m  [36/42], [94mLoss[0m : 3.95218
[1mStep[0m  [40/42], [94mLoss[0m : 3.48031

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.986, [92mTest[0m: 3.574, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.38047
[1mStep[0m  [4/42], [94mLoss[0m : 3.19362
[1mStep[0m  [8/42], [94mLoss[0m : 3.50026
[1mStep[0m  [12/42], [94mLoss[0m : 3.41125
[1mStep[0m  [16/42], [94mLoss[0m : 3.38822
[1mStep[0m  [20/42], [94mLoss[0m : 3.02094
[1mStep[0m  [24/42], [94mLoss[0m : 2.91934
[1mStep[0m  [28/42], [94mLoss[0m : 3.07862
[1mStep[0m  [32/42], [94mLoss[0m : 2.94214
[1mStep[0m  [36/42], [94mLoss[0m : 3.04103
[1mStep[0m  [40/42], [94mLoss[0m : 3.09324

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.195, [92mTest[0m: 2.954, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74539
[1mStep[0m  [4/42], [94mLoss[0m : 3.03744
[1mStep[0m  [8/42], [94mLoss[0m : 2.91295
[1mStep[0m  [12/42], [94mLoss[0m : 2.76120
[1mStep[0m  [16/42], [94mLoss[0m : 2.63047
[1mStep[0m  [20/42], [94mLoss[0m : 2.68197
[1mStep[0m  [24/42], [94mLoss[0m : 2.63500
[1mStep[0m  [28/42], [94mLoss[0m : 2.71422
[1mStep[0m  [32/42], [94mLoss[0m : 2.56177
[1mStep[0m  [36/42], [94mLoss[0m : 2.80587
[1mStep[0m  [40/42], [94mLoss[0m : 2.59397

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.753, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48558
[1mStep[0m  [4/42], [94mLoss[0m : 2.63963
[1mStep[0m  [8/42], [94mLoss[0m : 2.54956
[1mStep[0m  [12/42], [94mLoss[0m : 2.64401
[1mStep[0m  [16/42], [94mLoss[0m : 2.51453
[1mStep[0m  [20/42], [94mLoss[0m : 2.56573
[1mStep[0m  [24/42], [94mLoss[0m : 2.59662
[1mStep[0m  [28/42], [94mLoss[0m : 2.62216
[1mStep[0m  [32/42], [94mLoss[0m : 2.51627
[1mStep[0m  [36/42], [94mLoss[0m : 2.38732
[1mStep[0m  [40/42], [94mLoss[0m : 2.58432

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50912
[1mStep[0m  [4/42], [94mLoss[0m : 2.63438
[1mStep[0m  [8/42], [94mLoss[0m : 2.54666
[1mStep[0m  [12/42], [94mLoss[0m : 2.58149
[1mStep[0m  [16/42], [94mLoss[0m : 2.66984
[1mStep[0m  [20/42], [94mLoss[0m : 2.46632
[1mStep[0m  [24/42], [94mLoss[0m : 2.70689
[1mStep[0m  [28/42], [94mLoss[0m : 2.70180
[1mStep[0m  [32/42], [94mLoss[0m : 2.76295
[1mStep[0m  [36/42], [94mLoss[0m : 2.69758
[1mStep[0m  [40/42], [94mLoss[0m : 2.50103

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71025
[1mStep[0m  [4/42], [94mLoss[0m : 2.62918
[1mStep[0m  [8/42], [94mLoss[0m : 2.62618
[1mStep[0m  [12/42], [94mLoss[0m : 2.61778
[1mStep[0m  [16/42], [94mLoss[0m : 2.52263
[1mStep[0m  [20/42], [94mLoss[0m : 2.55541
[1mStep[0m  [24/42], [94mLoss[0m : 2.45542
[1mStep[0m  [28/42], [94mLoss[0m : 2.75568
[1mStep[0m  [32/42], [94mLoss[0m : 2.32738
[1mStep[0m  [36/42], [94mLoss[0m : 2.49510
[1mStep[0m  [40/42], [94mLoss[0m : 2.60746

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45889
[1mStep[0m  [4/42], [94mLoss[0m : 2.72389
[1mStep[0m  [8/42], [94mLoss[0m : 2.39782
[1mStep[0m  [12/42], [94mLoss[0m : 2.65447
[1mStep[0m  [16/42], [94mLoss[0m : 2.64988
[1mStep[0m  [20/42], [94mLoss[0m : 2.57820
[1mStep[0m  [24/42], [94mLoss[0m : 2.33976
[1mStep[0m  [28/42], [94mLoss[0m : 2.58075
[1mStep[0m  [32/42], [94mLoss[0m : 2.33746
[1mStep[0m  [36/42], [94mLoss[0m : 2.50618
[1mStep[0m  [40/42], [94mLoss[0m : 2.65225

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55961
[1mStep[0m  [4/42], [94mLoss[0m : 2.50837
[1mStep[0m  [8/42], [94mLoss[0m : 2.43808
[1mStep[0m  [12/42], [94mLoss[0m : 2.63136
[1mStep[0m  [16/42], [94mLoss[0m : 2.63886
[1mStep[0m  [20/42], [94mLoss[0m : 2.53171
[1mStep[0m  [24/42], [94mLoss[0m : 2.80112
[1mStep[0m  [28/42], [94mLoss[0m : 2.75273
[1mStep[0m  [32/42], [94mLoss[0m : 2.48901
[1mStep[0m  [36/42], [94mLoss[0m : 2.72957
[1mStep[0m  [40/42], [94mLoss[0m : 2.45318

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34661
[1mStep[0m  [4/42], [94mLoss[0m : 2.36244
[1mStep[0m  [8/42], [94mLoss[0m : 2.49652
[1mStep[0m  [12/42], [94mLoss[0m : 2.49320
[1mStep[0m  [16/42], [94mLoss[0m : 2.43044
[1mStep[0m  [20/42], [94mLoss[0m : 2.37966
[1mStep[0m  [24/42], [94mLoss[0m : 2.53958
[1mStep[0m  [28/42], [94mLoss[0m : 2.45418
[1mStep[0m  [32/42], [94mLoss[0m : 2.72311
[1mStep[0m  [36/42], [94mLoss[0m : 2.72966
[1mStep[0m  [40/42], [94mLoss[0m : 2.39863

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47855
[1mStep[0m  [4/42], [94mLoss[0m : 2.63846
[1mStep[0m  [8/42], [94mLoss[0m : 2.67043
[1mStep[0m  [12/42], [94mLoss[0m : 2.58250
[1mStep[0m  [16/42], [94mLoss[0m : 2.60218
[1mStep[0m  [20/42], [94mLoss[0m : 2.31572
[1mStep[0m  [24/42], [94mLoss[0m : 2.64071
[1mStep[0m  [28/42], [94mLoss[0m : 2.46243
[1mStep[0m  [32/42], [94mLoss[0m : 2.50192
[1mStep[0m  [36/42], [94mLoss[0m : 2.51301
[1mStep[0m  [40/42], [94mLoss[0m : 2.29756

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51161
[1mStep[0m  [4/42], [94mLoss[0m : 2.34555
[1mStep[0m  [8/42], [94mLoss[0m : 2.30388
[1mStep[0m  [12/42], [94mLoss[0m : 2.39520
[1mStep[0m  [16/42], [94mLoss[0m : 2.51989
[1mStep[0m  [20/42], [94mLoss[0m : 2.59585
[1mStep[0m  [24/42], [94mLoss[0m : 2.53669
[1mStep[0m  [28/42], [94mLoss[0m : 2.56045
[1mStep[0m  [32/42], [94mLoss[0m : 2.39014
[1mStep[0m  [36/42], [94mLoss[0m : 2.66942
[1mStep[0m  [40/42], [94mLoss[0m : 2.59757

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39188
[1mStep[0m  [4/42], [94mLoss[0m : 2.52864
[1mStep[0m  [8/42], [94mLoss[0m : 2.45360
[1mStep[0m  [12/42], [94mLoss[0m : 2.50864
[1mStep[0m  [16/42], [94mLoss[0m : 2.56813
[1mStep[0m  [20/42], [94mLoss[0m : 2.51730
[1mStep[0m  [24/42], [94mLoss[0m : 2.44168
[1mStep[0m  [28/42], [94mLoss[0m : 2.57877
[1mStep[0m  [32/42], [94mLoss[0m : 2.58402
[1mStep[0m  [36/42], [94mLoss[0m : 2.47120
[1mStep[0m  [40/42], [94mLoss[0m : 2.46652

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46797
[1mStep[0m  [4/42], [94mLoss[0m : 2.57662
[1mStep[0m  [8/42], [94mLoss[0m : 2.47374
[1mStep[0m  [12/42], [94mLoss[0m : 2.51665
[1mStep[0m  [16/42], [94mLoss[0m : 2.51618
[1mStep[0m  [20/42], [94mLoss[0m : 2.37924
[1mStep[0m  [24/42], [94mLoss[0m : 2.61085
[1mStep[0m  [28/42], [94mLoss[0m : 2.65919
[1mStep[0m  [32/42], [94mLoss[0m : 2.62724
[1mStep[0m  [36/42], [94mLoss[0m : 2.42799
[1mStep[0m  [40/42], [94mLoss[0m : 2.33969

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.359, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66338
[1mStep[0m  [4/42], [94mLoss[0m : 2.69747
[1mStep[0m  [8/42], [94mLoss[0m : 2.39886
[1mStep[0m  [12/42], [94mLoss[0m : 2.36780
[1mStep[0m  [16/42], [94mLoss[0m : 2.51529
[1mStep[0m  [20/42], [94mLoss[0m : 2.26592
[1mStep[0m  [24/42], [94mLoss[0m : 2.64403
[1mStep[0m  [28/42], [94mLoss[0m : 2.56764
[1mStep[0m  [32/42], [94mLoss[0m : 2.51126
[1mStep[0m  [36/42], [94mLoss[0m : 2.44238
[1mStep[0m  [40/42], [94mLoss[0m : 2.34460

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56192
[1mStep[0m  [4/42], [94mLoss[0m : 2.42292
[1mStep[0m  [8/42], [94mLoss[0m : 2.44246
[1mStep[0m  [12/42], [94mLoss[0m : 2.49209
[1mStep[0m  [16/42], [94mLoss[0m : 2.61745
[1mStep[0m  [20/42], [94mLoss[0m : 2.76546
[1mStep[0m  [24/42], [94mLoss[0m : 2.52336
[1mStep[0m  [28/42], [94mLoss[0m : 2.39624
[1mStep[0m  [32/42], [94mLoss[0m : 2.83264
[1mStep[0m  [36/42], [94mLoss[0m : 2.57721
[1mStep[0m  [40/42], [94mLoss[0m : 2.39081

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36217
[1mStep[0m  [4/42], [94mLoss[0m : 2.36898
[1mStep[0m  [8/42], [94mLoss[0m : 2.67236
[1mStep[0m  [12/42], [94mLoss[0m : 2.61204
[1mStep[0m  [16/42], [94mLoss[0m : 2.61235
[1mStep[0m  [20/42], [94mLoss[0m : 2.54942
[1mStep[0m  [24/42], [94mLoss[0m : 2.46852
[1mStep[0m  [28/42], [94mLoss[0m : 2.47079
[1mStep[0m  [32/42], [94mLoss[0m : 2.53281
[1mStep[0m  [36/42], [94mLoss[0m : 2.42598
[1mStep[0m  [40/42], [94mLoss[0m : 2.80264

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.328, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39335
[1mStep[0m  [4/42], [94mLoss[0m : 2.51314
[1mStep[0m  [8/42], [94mLoss[0m : 2.40574
[1mStep[0m  [12/42], [94mLoss[0m : 2.36350
[1mStep[0m  [16/42], [94mLoss[0m : 2.49331
[1mStep[0m  [20/42], [94mLoss[0m : 2.32800
[1mStep[0m  [24/42], [94mLoss[0m : 2.61685
[1mStep[0m  [28/42], [94mLoss[0m : 2.48677
[1mStep[0m  [32/42], [94mLoss[0m : 2.38192
[1mStep[0m  [36/42], [94mLoss[0m : 2.62328
[1mStep[0m  [40/42], [94mLoss[0m : 2.58724

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29287
[1mStep[0m  [4/42], [94mLoss[0m : 2.65883
[1mStep[0m  [8/42], [94mLoss[0m : 2.67973
[1mStep[0m  [12/42], [94mLoss[0m : 2.43373
[1mStep[0m  [16/42], [94mLoss[0m : 2.33990
[1mStep[0m  [20/42], [94mLoss[0m : 2.35829
[1mStep[0m  [24/42], [94mLoss[0m : 2.55574
[1mStep[0m  [28/42], [94mLoss[0m : 2.47103
[1mStep[0m  [32/42], [94mLoss[0m : 2.39695
[1mStep[0m  [36/42], [94mLoss[0m : 2.60148
[1mStep[0m  [40/42], [94mLoss[0m : 2.37935

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.364, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69537
[1mStep[0m  [4/42], [94mLoss[0m : 2.53938
[1mStep[0m  [8/42], [94mLoss[0m : 2.70676
[1mStep[0m  [12/42], [94mLoss[0m : 2.41754
[1mStep[0m  [16/42], [94mLoss[0m : 2.20973
[1mStep[0m  [20/42], [94mLoss[0m : 2.55864
[1mStep[0m  [24/42], [94mLoss[0m : 2.35013
[1mStep[0m  [28/42], [94mLoss[0m : 2.55542
[1mStep[0m  [32/42], [94mLoss[0m : 2.23746
[1mStep[0m  [36/42], [94mLoss[0m : 2.70116
[1mStep[0m  [40/42], [94mLoss[0m : 2.68195

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64709
[1mStep[0m  [4/42], [94mLoss[0m : 2.48818
[1mStep[0m  [8/42], [94mLoss[0m : 2.21620
[1mStep[0m  [12/42], [94mLoss[0m : 2.51417
[1mStep[0m  [16/42], [94mLoss[0m : 2.33971
[1mStep[0m  [20/42], [94mLoss[0m : 2.31733
[1mStep[0m  [24/42], [94mLoss[0m : 2.55765
[1mStep[0m  [28/42], [94mLoss[0m : 2.52333
[1mStep[0m  [32/42], [94mLoss[0m : 2.36529
[1mStep[0m  [36/42], [94mLoss[0m : 2.55290
[1mStep[0m  [40/42], [94mLoss[0m : 2.50255

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.355, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36656
[1mStep[0m  [4/42], [94mLoss[0m : 2.30451
[1mStep[0m  [8/42], [94mLoss[0m : 2.32818
[1mStep[0m  [12/42], [94mLoss[0m : 2.67230
[1mStep[0m  [16/42], [94mLoss[0m : 2.59835
[1mStep[0m  [20/42], [94mLoss[0m : 2.44472
[1mStep[0m  [24/42], [94mLoss[0m : 2.55492
[1mStep[0m  [28/42], [94mLoss[0m : 2.40251
[1mStep[0m  [32/42], [94mLoss[0m : 2.55610
[1mStep[0m  [36/42], [94mLoss[0m : 2.40214
[1mStep[0m  [40/42], [94mLoss[0m : 2.45237

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53087
[1mStep[0m  [4/42], [94mLoss[0m : 2.37521
[1mStep[0m  [8/42], [94mLoss[0m : 2.39901
[1mStep[0m  [12/42], [94mLoss[0m : 2.51806
[1mStep[0m  [16/42], [94mLoss[0m : 2.28225
[1mStep[0m  [20/42], [94mLoss[0m : 2.38396
[1mStep[0m  [24/42], [94mLoss[0m : 2.65389
[1mStep[0m  [28/42], [94mLoss[0m : 2.60961
[1mStep[0m  [32/42], [94mLoss[0m : 2.33788
[1mStep[0m  [36/42], [94mLoss[0m : 2.53299
[1mStep[0m  [40/42], [94mLoss[0m : 2.40185

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37280
[1mStep[0m  [4/42], [94mLoss[0m : 2.53656
[1mStep[0m  [8/42], [94mLoss[0m : 2.32673
[1mStep[0m  [12/42], [94mLoss[0m : 2.57675
[1mStep[0m  [16/42], [94mLoss[0m : 2.31681
[1mStep[0m  [20/42], [94mLoss[0m : 2.48628
[1mStep[0m  [24/42], [94mLoss[0m : 2.25826
[1mStep[0m  [28/42], [94mLoss[0m : 2.66938
[1mStep[0m  [32/42], [94mLoss[0m : 2.49463
[1mStep[0m  [36/42], [94mLoss[0m : 2.37723
[1mStep[0m  [40/42], [94mLoss[0m : 2.31213

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.368
====================================

Phase 1 - Evaluation MAE:  2.367539473942348
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.31248
[1mStep[0m  [4/42], [94mLoss[0m : 2.58388
[1mStep[0m  [8/42], [94mLoss[0m : 2.60118
[1mStep[0m  [12/42], [94mLoss[0m : 2.73513
[1mStep[0m  [16/42], [94mLoss[0m : 2.38648
[1mStep[0m  [20/42], [94mLoss[0m : 2.73886
[1mStep[0m  [24/42], [94mLoss[0m : 2.43306
[1mStep[0m  [28/42], [94mLoss[0m : 2.37990
[1mStep[0m  [32/42], [94mLoss[0m : 2.45138
[1mStep[0m  [36/42], [94mLoss[0m : 2.80498
[1mStep[0m  [40/42], [94mLoss[0m : 2.49607

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28681
[1mStep[0m  [4/42], [94mLoss[0m : 2.32618
[1mStep[0m  [8/42], [94mLoss[0m : 2.66871
[1mStep[0m  [12/42], [94mLoss[0m : 2.53879
[1mStep[0m  [16/42], [94mLoss[0m : 2.52514
[1mStep[0m  [20/42], [94mLoss[0m : 2.50633
[1mStep[0m  [24/42], [94mLoss[0m : 2.31391
[1mStep[0m  [28/42], [94mLoss[0m : 2.43186
[1mStep[0m  [32/42], [94mLoss[0m : 2.52600
[1mStep[0m  [36/42], [94mLoss[0m : 2.35940
[1mStep[0m  [40/42], [94mLoss[0m : 2.54822

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45570
[1mStep[0m  [4/42], [94mLoss[0m : 2.42335
[1mStep[0m  [8/42], [94mLoss[0m : 2.35000
[1mStep[0m  [12/42], [94mLoss[0m : 2.42208
[1mStep[0m  [16/42], [94mLoss[0m : 2.37952
[1mStep[0m  [20/42], [94mLoss[0m : 2.29803
[1mStep[0m  [24/42], [94mLoss[0m : 2.55240
[1mStep[0m  [28/42], [94mLoss[0m : 2.24376
[1mStep[0m  [32/42], [94mLoss[0m : 2.27824
[1mStep[0m  [36/42], [94mLoss[0m : 2.53425
[1mStep[0m  [40/42], [94mLoss[0m : 2.44937

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32913
[1mStep[0m  [4/42], [94mLoss[0m : 2.21323
[1mStep[0m  [8/42], [94mLoss[0m : 2.33168
[1mStep[0m  [12/42], [94mLoss[0m : 2.33696
[1mStep[0m  [16/42], [94mLoss[0m : 2.24757
[1mStep[0m  [20/42], [94mLoss[0m : 2.48900
[1mStep[0m  [24/42], [94mLoss[0m : 2.35603
[1mStep[0m  [28/42], [94mLoss[0m : 2.26039
[1mStep[0m  [32/42], [94mLoss[0m : 2.52919
[1mStep[0m  [36/42], [94mLoss[0m : 2.29535
[1mStep[0m  [40/42], [94mLoss[0m : 2.37008

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56988
[1mStep[0m  [4/42], [94mLoss[0m : 2.42486
[1mStep[0m  [8/42], [94mLoss[0m : 2.27270
[1mStep[0m  [12/42], [94mLoss[0m : 2.22784
[1mStep[0m  [16/42], [94mLoss[0m : 2.34475
[1mStep[0m  [20/42], [94mLoss[0m : 2.20161
[1mStep[0m  [24/42], [94mLoss[0m : 2.31886
[1mStep[0m  [28/42], [94mLoss[0m : 2.47273
[1mStep[0m  [32/42], [94mLoss[0m : 2.20399
[1mStep[0m  [36/42], [94mLoss[0m : 2.38290
[1mStep[0m  [40/42], [94mLoss[0m : 2.30680

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12777
[1mStep[0m  [4/42], [94mLoss[0m : 2.45483
[1mStep[0m  [8/42], [94mLoss[0m : 2.27556
[1mStep[0m  [12/42], [94mLoss[0m : 2.22132
[1mStep[0m  [16/42], [94mLoss[0m : 2.13751
[1mStep[0m  [20/42], [94mLoss[0m : 2.25262
[1mStep[0m  [24/42], [94mLoss[0m : 2.19379
[1mStep[0m  [28/42], [94mLoss[0m : 2.08248
[1mStep[0m  [32/42], [94mLoss[0m : 2.52290
[1mStep[0m  [36/42], [94mLoss[0m : 2.40409
[1mStep[0m  [40/42], [94mLoss[0m : 2.39311

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03367
[1mStep[0m  [4/42], [94mLoss[0m : 2.29223
[1mStep[0m  [8/42], [94mLoss[0m : 2.26273
[1mStep[0m  [12/42], [94mLoss[0m : 2.10575
[1mStep[0m  [16/42], [94mLoss[0m : 2.45509
[1mStep[0m  [20/42], [94mLoss[0m : 2.01026
[1mStep[0m  [24/42], [94mLoss[0m : 2.26715
[1mStep[0m  [28/42], [94mLoss[0m : 2.36484
[1mStep[0m  [32/42], [94mLoss[0m : 2.10518
[1mStep[0m  [36/42], [94mLoss[0m : 2.15061
[1mStep[0m  [40/42], [94mLoss[0m : 2.35760

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.692, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43147
[1mStep[0m  [4/42], [94mLoss[0m : 2.07240
[1mStep[0m  [8/42], [94mLoss[0m : 2.23419
[1mStep[0m  [12/42], [94mLoss[0m : 2.03563
[1mStep[0m  [16/42], [94mLoss[0m : 2.27743
[1mStep[0m  [20/42], [94mLoss[0m : 2.03597
[1mStep[0m  [24/42], [94mLoss[0m : 2.25984
[1mStep[0m  [28/42], [94mLoss[0m : 2.13980
[1mStep[0m  [32/42], [94mLoss[0m : 2.32584
[1mStep[0m  [36/42], [94mLoss[0m : 2.49206
[1mStep[0m  [40/42], [94mLoss[0m : 2.09830

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04118
[1mStep[0m  [4/42], [94mLoss[0m : 2.05480
[1mStep[0m  [8/42], [94mLoss[0m : 2.24732
[1mStep[0m  [12/42], [94mLoss[0m : 2.29683
[1mStep[0m  [16/42], [94mLoss[0m : 1.85667
[1mStep[0m  [20/42], [94mLoss[0m : 2.30325
[1mStep[0m  [24/42], [94mLoss[0m : 2.28459
[1mStep[0m  [28/42], [94mLoss[0m : 2.18345
[1mStep[0m  [32/42], [94mLoss[0m : 2.04977
[1mStep[0m  [36/42], [94mLoss[0m : 2.06851
[1mStep[0m  [40/42], [94mLoss[0m : 2.17735

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.730, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15026
[1mStep[0m  [4/42], [94mLoss[0m : 2.02291
[1mStep[0m  [8/42], [94mLoss[0m : 2.11367
[1mStep[0m  [12/42], [94mLoss[0m : 2.18883
[1mStep[0m  [16/42], [94mLoss[0m : 2.09889
[1mStep[0m  [20/42], [94mLoss[0m : 2.05589
[1mStep[0m  [24/42], [94mLoss[0m : 2.00586
[1mStep[0m  [28/42], [94mLoss[0m : 2.16039
[1mStep[0m  [32/42], [94mLoss[0m : 2.08516
[1mStep[0m  [36/42], [94mLoss[0m : 2.34260
[1mStep[0m  [40/42], [94mLoss[0m : 1.97354

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.959, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05058
[1mStep[0m  [4/42], [94mLoss[0m : 2.24932
[1mStep[0m  [8/42], [94mLoss[0m : 2.05739
[1mStep[0m  [12/42], [94mLoss[0m : 2.03176
[1mStep[0m  [16/42], [94mLoss[0m : 2.12220
[1mStep[0m  [20/42], [94mLoss[0m : 2.07034
[1mStep[0m  [24/42], [94mLoss[0m : 1.94410
[1mStep[0m  [28/42], [94mLoss[0m : 2.06900
[1mStep[0m  [32/42], [94mLoss[0m : 2.03839
[1mStep[0m  [36/42], [94mLoss[0m : 2.00930
[1mStep[0m  [40/42], [94mLoss[0m : 2.02044

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.658, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03880
[1mStep[0m  [4/42], [94mLoss[0m : 2.06993
[1mStep[0m  [8/42], [94mLoss[0m : 1.87821
[1mStep[0m  [12/42], [94mLoss[0m : 2.20143
[1mStep[0m  [16/42], [94mLoss[0m : 2.02084
[1mStep[0m  [20/42], [94mLoss[0m : 1.99791
[1mStep[0m  [24/42], [94mLoss[0m : 2.09177
[1mStep[0m  [28/42], [94mLoss[0m : 2.13993
[1mStep[0m  [32/42], [94mLoss[0m : 1.83391
[1mStep[0m  [36/42], [94mLoss[0m : 2.10851
[1mStep[0m  [40/42], [94mLoss[0m : 1.93445

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.558, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96024
[1mStep[0m  [4/42], [94mLoss[0m : 2.07293
[1mStep[0m  [8/42], [94mLoss[0m : 1.99715
[1mStep[0m  [12/42], [94mLoss[0m : 2.03698
[1mStep[0m  [16/42], [94mLoss[0m : 1.99351
[1mStep[0m  [20/42], [94mLoss[0m : 1.86881
[1mStep[0m  [24/42], [94mLoss[0m : 1.96097
[1mStep[0m  [28/42], [94mLoss[0m : 1.93669
[1mStep[0m  [32/42], [94mLoss[0m : 2.10216
[1mStep[0m  [36/42], [94mLoss[0m : 1.99137
[1mStep[0m  [40/42], [94mLoss[0m : 1.97035

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.712, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97643
[1mStep[0m  [4/42], [94mLoss[0m : 1.79658
[1mStep[0m  [8/42], [94mLoss[0m : 1.81046
[1mStep[0m  [12/42], [94mLoss[0m : 1.76917
[1mStep[0m  [16/42], [94mLoss[0m : 1.76064
[1mStep[0m  [20/42], [94mLoss[0m : 1.98150
[1mStep[0m  [24/42], [94mLoss[0m : 1.91939
[1mStep[0m  [28/42], [94mLoss[0m : 2.02121
[1mStep[0m  [32/42], [94mLoss[0m : 1.97337
[1mStep[0m  [36/42], [94mLoss[0m : 1.98374
[1mStep[0m  [40/42], [94mLoss[0m : 1.99975

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.565, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84505
[1mStep[0m  [4/42], [94mLoss[0m : 1.83356
[1mStep[0m  [8/42], [94mLoss[0m : 1.93461
[1mStep[0m  [12/42], [94mLoss[0m : 1.98519
[1mStep[0m  [16/42], [94mLoss[0m : 1.92715
[1mStep[0m  [20/42], [94mLoss[0m : 1.89067
[1mStep[0m  [24/42], [94mLoss[0m : 2.00118
[1mStep[0m  [28/42], [94mLoss[0m : 2.01453
[1mStep[0m  [32/42], [94mLoss[0m : 1.79776
[1mStep[0m  [36/42], [94mLoss[0m : 1.90285
[1mStep[0m  [40/42], [94mLoss[0m : 1.90053

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97074
[1mStep[0m  [4/42], [94mLoss[0m : 1.84180
[1mStep[0m  [8/42], [94mLoss[0m : 1.82136
[1mStep[0m  [12/42], [94mLoss[0m : 1.77169
[1mStep[0m  [16/42], [94mLoss[0m : 1.97706
[1mStep[0m  [20/42], [94mLoss[0m : 2.08557
[1mStep[0m  [24/42], [94mLoss[0m : 2.04023
[1mStep[0m  [28/42], [94mLoss[0m : 1.84221
[1mStep[0m  [32/42], [94mLoss[0m : 2.06274
[1mStep[0m  [36/42], [94mLoss[0m : 1.94884
[1mStep[0m  [40/42], [94mLoss[0m : 1.93426

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69786
[1mStep[0m  [4/42], [94mLoss[0m : 1.88085
[1mStep[0m  [8/42], [94mLoss[0m : 2.10091
[1mStep[0m  [12/42], [94mLoss[0m : 1.75050
[1mStep[0m  [16/42], [94mLoss[0m : 1.89070
[1mStep[0m  [20/42], [94mLoss[0m : 1.98124
[1mStep[0m  [24/42], [94mLoss[0m : 2.10472
[1mStep[0m  [28/42], [94mLoss[0m : 1.81328
[1mStep[0m  [32/42], [94mLoss[0m : 2.06335
[1mStep[0m  [36/42], [94mLoss[0m : 1.90456
[1mStep[0m  [40/42], [94mLoss[0m : 1.73643

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78509
[1mStep[0m  [4/42], [94mLoss[0m : 1.78270
[1mStep[0m  [8/42], [94mLoss[0m : 1.77545
[1mStep[0m  [12/42], [94mLoss[0m : 1.79345
[1mStep[0m  [16/42], [94mLoss[0m : 1.70381
[1mStep[0m  [20/42], [94mLoss[0m : 1.72041
[1mStep[0m  [24/42], [94mLoss[0m : 1.80344
[1mStep[0m  [28/42], [94mLoss[0m : 1.70693
[1mStep[0m  [32/42], [94mLoss[0m : 1.83265
[1mStep[0m  [36/42], [94mLoss[0m : 1.94852
[1mStep[0m  [40/42], [94mLoss[0m : 1.83207

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.834, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72001
[1mStep[0m  [4/42], [94mLoss[0m : 1.70220
[1mStep[0m  [8/42], [94mLoss[0m : 1.72919
[1mStep[0m  [12/42], [94mLoss[0m : 1.91919
[1mStep[0m  [16/42], [94mLoss[0m : 1.68033
[1mStep[0m  [20/42], [94mLoss[0m : 1.91730
[1mStep[0m  [24/42], [94mLoss[0m : 1.76584
[1mStep[0m  [28/42], [94mLoss[0m : 1.81538
[1mStep[0m  [32/42], [94mLoss[0m : 1.76860
[1mStep[0m  [36/42], [94mLoss[0m : 1.77513
[1mStep[0m  [40/42], [94mLoss[0m : 1.75106

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.535, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76394
[1mStep[0m  [4/42], [94mLoss[0m : 1.74749
[1mStep[0m  [8/42], [94mLoss[0m : 1.69947
[1mStep[0m  [12/42], [94mLoss[0m : 1.67070
[1mStep[0m  [16/42], [94mLoss[0m : 1.84225
[1mStep[0m  [20/42], [94mLoss[0m : 1.63941
[1mStep[0m  [24/42], [94mLoss[0m : 1.93931
[1mStep[0m  [28/42], [94mLoss[0m : 1.72287
[1mStep[0m  [32/42], [94mLoss[0m : 1.64584
[1mStep[0m  [36/42], [94mLoss[0m : 1.64088
[1mStep[0m  [40/42], [94mLoss[0m : 1.85857

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.510, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70853
[1mStep[0m  [4/42], [94mLoss[0m : 1.80520
[1mStep[0m  [8/42], [94mLoss[0m : 2.02902
[1mStep[0m  [12/42], [94mLoss[0m : 1.69527
[1mStep[0m  [16/42], [94mLoss[0m : 1.70286
[1mStep[0m  [20/42], [94mLoss[0m : 1.50827
[1mStep[0m  [24/42], [94mLoss[0m : 1.60897
[1mStep[0m  [28/42], [94mLoss[0m : 1.83706
[1mStep[0m  [32/42], [94mLoss[0m : 1.82717
[1mStep[0m  [36/42], [94mLoss[0m : 1.85697
[1mStep[0m  [40/42], [94mLoss[0m : 1.85419

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52548
[1mStep[0m  [4/42], [94mLoss[0m : 1.83461
[1mStep[0m  [8/42], [94mLoss[0m : 1.64215
[1mStep[0m  [12/42], [94mLoss[0m : 1.62913
[1mStep[0m  [16/42], [94mLoss[0m : 1.77618
[1mStep[0m  [20/42], [94mLoss[0m : 1.59781
[1mStep[0m  [24/42], [94mLoss[0m : 1.73726
[1mStep[0m  [28/42], [94mLoss[0m : 1.80048
[1mStep[0m  [32/42], [94mLoss[0m : 1.74621
[1mStep[0m  [36/42], [94mLoss[0m : 1.69821
[1mStep[0m  [40/42], [94mLoss[0m : 1.76417

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.525, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68054
[1mStep[0m  [4/42], [94mLoss[0m : 1.80853
[1mStep[0m  [8/42], [94mLoss[0m : 1.60755
[1mStep[0m  [12/42], [94mLoss[0m : 1.62176
[1mStep[0m  [16/42], [94mLoss[0m : 1.69105
[1mStep[0m  [20/42], [94mLoss[0m : 1.74885
[1mStep[0m  [24/42], [94mLoss[0m : 1.59652
[1mStep[0m  [28/42], [94mLoss[0m : 1.76299
[1mStep[0m  [32/42], [94mLoss[0m : 1.67924
[1mStep[0m  [36/42], [94mLoss[0m : 1.76354
[1mStep[0m  [40/42], [94mLoss[0m : 1.64153

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63691
[1mStep[0m  [4/42], [94mLoss[0m : 1.60562
[1mStep[0m  [8/42], [94mLoss[0m : 1.57112
[1mStep[0m  [12/42], [94mLoss[0m : 1.44877
[1mStep[0m  [16/42], [94mLoss[0m : 1.64697
[1mStep[0m  [20/42], [94mLoss[0m : 1.60090
[1mStep[0m  [24/42], [94mLoss[0m : 1.60741
[1mStep[0m  [28/42], [94mLoss[0m : 1.86875
[1mStep[0m  [32/42], [94mLoss[0m : 1.81118
[1mStep[0m  [36/42], [94mLoss[0m : 1.70142
[1mStep[0m  [40/42], [94mLoss[0m : 1.57527

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69355
[1mStep[0m  [4/42], [94mLoss[0m : 1.50559
[1mStep[0m  [8/42], [94mLoss[0m : 1.69046
[1mStep[0m  [12/42], [94mLoss[0m : 1.44228
[1mStep[0m  [16/42], [94mLoss[0m : 1.65347
[1mStep[0m  [20/42], [94mLoss[0m : 1.65217
[1mStep[0m  [24/42], [94mLoss[0m : 1.58772
[1mStep[0m  [28/42], [94mLoss[0m : 1.57724
[1mStep[0m  [32/42], [94mLoss[0m : 1.47762
[1mStep[0m  [36/42], [94mLoss[0m : 1.64082
[1mStep[0m  [40/42], [94mLoss[0m : 1.61659

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.618, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59263
[1mStep[0m  [4/42], [94mLoss[0m : 1.58438
[1mStep[0m  [8/42], [94mLoss[0m : 1.54925
[1mStep[0m  [12/42], [94mLoss[0m : 1.65449
[1mStep[0m  [16/42], [94mLoss[0m : 1.77942
[1mStep[0m  [20/42], [94mLoss[0m : 1.61360
[1mStep[0m  [24/42], [94mLoss[0m : 1.67766
[1mStep[0m  [28/42], [94mLoss[0m : 1.85642
[1mStep[0m  [32/42], [94mLoss[0m : 1.70923
[1mStep[0m  [36/42], [94mLoss[0m : 1.66567
[1mStep[0m  [40/42], [94mLoss[0m : 1.60830

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.479, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51454
[1mStep[0m  [4/42], [94mLoss[0m : 1.58251
[1mStep[0m  [8/42], [94mLoss[0m : 1.53869
[1mStep[0m  [12/42], [94mLoss[0m : 1.52662
[1mStep[0m  [16/42], [94mLoss[0m : 1.53728
[1mStep[0m  [20/42], [94mLoss[0m : 1.62354
[1mStep[0m  [24/42], [94mLoss[0m : 1.63608
[1mStep[0m  [28/42], [94mLoss[0m : 1.53358
[1mStep[0m  [32/42], [94mLoss[0m : 1.67463
[1mStep[0m  [36/42], [94mLoss[0m : 1.51596
[1mStep[0m  [40/42], [94mLoss[0m : 1.61071

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.576, [92mTest[0m: 2.625, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60606
[1mStep[0m  [4/42], [94mLoss[0m : 1.51974
[1mStep[0m  [8/42], [94mLoss[0m : 1.36170
[1mStep[0m  [12/42], [94mLoss[0m : 1.78948
[1mStep[0m  [16/42], [94mLoss[0m : 1.53587
[1mStep[0m  [20/42], [94mLoss[0m : 1.70422
[1mStep[0m  [24/42], [94mLoss[0m : 1.55346
[1mStep[0m  [28/42], [94mLoss[0m : 1.62976
[1mStep[0m  [32/42], [94mLoss[0m : 1.53990
[1mStep[0m  [36/42], [94mLoss[0m : 1.55985
[1mStep[0m  [40/42], [94mLoss[0m : 1.81462

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.553, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40903
[1mStep[0m  [4/42], [94mLoss[0m : 1.50837
[1mStep[0m  [8/42], [94mLoss[0m : 1.47534
[1mStep[0m  [12/42], [94mLoss[0m : 1.44218
[1mStep[0m  [16/42], [94mLoss[0m : 1.51975
[1mStep[0m  [20/42], [94mLoss[0m : 1.50445
[1mStep[0m  [24/42], [94mLoss[0m : 1.53142
[1mStep[0m  [28/42], [94mLoss[0m : 1.55786
[1mStep[0m  [32/42], [94mLoss[0m : 1.58411
[1mStep[0m  [36/42], [94mLoss[0m : 1.63954
[1mStep[0m  [40/42], [94mLoss[0m : 1.63336

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66865
[1mStep[0m  [4/42], [94mLoss[0m : 1.41329
[1mStep[0m  [8/42], [94mLoss[0m : 1.47265
[1mStep[0m  [12/42], [94mLoss[0m : 1.51797
[1mStep[0m  [16/42], [94mLoss[0m : 1.47757
[1mStep[0m  [20/42], [94mLoss[0m : 1.42991
[1mStep[0m  [24/42], [94mLoss[0m : 1.68891
[1mStep[0m  [28/42], [94mLoss[0m : 1.49261
[1mStep[0m  [32/42], [94mLoss[0m : 1.53609
[1mStep[0m  [36/42], [94mLoss[0m : 1.58768
[1mStep[0m  [40/42], [94mLoss[0m : 1.61411

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.581, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.481
====================================

Phase 2 - Evaluation MAE:  2.4813553265162875
MAE score P1      2.367539
MAE score P2      2.481355
loss              1.549037
learning_rate         0.01
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 24, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.62111
[1mStep[0m  [8/84], [94mLoss[0m : 7.89557
[1mStep[0m  [16/84], [94mLoss[0m : 3.80528
[1mStep[0m  [24/84], [94mLoss[0m : 3.69237
[1mStep[0m  [32/84], [94mLoss[0m : 3.30785
[1mStep[0m  [40/84], [94mLoss[0m : 3.08383
[1mStep[0m  [48/84], [94mLoss[0m : 3.18941
[1mStep[0m  [56/84], [94mLoss[0m : 2.53804
[1mStep[0m  [64/84], [94mLoss[0m : 2.91278
[1mStep[0m  [72/84], [94mLoss[0m : 2.41147
[1mStep[0m  [80/84], [94mLoss[0m : 2.55133

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.966, [92mTest[0m: 10.890, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55756
[1mStep[0m  [8/84], [94mLoss[0m : 3.11735
[1mStep[0m  [16/84], [94mLoss[0m : 2.79537
[1mStep[0m  [24/84], [94mLoss[0m : 2.83981
[1mStep[0m  [32/84], [94mLoss[0m : 2.63316
[1mStep[0m  [40/84], [94mLoss[0m : 2.79208
[1mStep[0m  [48/84], [94mLoss[0m : 2.94892
[1mStep[0m  [56/84], [94mLoss[0m : 2.49179
[1mStep[0m  [64/84], [94mLoss[0m : 2.66130
[1mStep[0m  [72/84], [94mLoss[0m : 2.34578
[1mStep[0m  [80/84], [94mLoss[0m : 2.73367

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37052
[1mStep[0m  [8/84], [94mLoss[0m : 2.52182
[1mStep[0m  [16/84], [94mLoss[0m : 2.58091
[1mStep[0m  [24/84], [94mLoss[0m : 2.80457
[1mStep[0m  [32/84], [94mLoss[0m : 2.33300
[1mStep[0m  [40/84], [94mLoss[0m : 2.74536
[1mStep[0m  [48/84], [94mLoss[0m : 2.66033
[1mStep[0m  [56/84], [94mLoss[0m : 2.78265
[1mStep[0m  [64/84], [94mLoss[0m : 2.54698
[1mStep[0m  [72/84], [94mLoss[0m : 2.82534
[1mStep[0m  [80/84], [94mLoss[0m : 2.43738

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52231
[1mStep[0m  [8/84], [94mLoss[0m : 2.48962
[1mStep[0m  [16/84], [94mLoss[0m : 2.86600
[1mStep[0m  [24/84], [94mLoss[0m : 2.35555
[1mStep[0m  [32/84], [94mLoss[0m : 2.67018
[1mStep[0m  [40/84], [94mLoss[0m : 2.68987
[1mStep[0m  [48/84], [94mLoss[0m : 2.49915
[1mStep[0m  [56/84], [94mLoss[0m : 2.62028
[1mStep[0m  [64/84], [94mLoss[0m : 2.40735
[1mStep[0m  [72/84], [94mLoss[0m : 2.63279
[1mStep[0m  [80/84], [94mLoss[0m : 2.29287

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58884
[1mStep[0m  [8/84], [94mLoss[0m : 2.73600
[1mStep[0m  [16/84], [94mLoss[0m : 2.18855
[1mStep[0m  [24/84], [94mLoss[0m : 2.49817
[1mStep[0m  [32/84], [94mLoss[0m : 2.68700
[1mStep[0m  [40/84], [94mLoss[0m : 2.37557
[1mStep[0m  [48/84], [94mLoss[0m : 2.30758
[1mStep[0m  [56/84], [94mLoss[0m : 2.23282
[1mStep[0m  [64/84], [94mLoss[0m : 2.52266
[1mStep[0m  [72/84], [94mLoss[0m : 2.30669
[1mStep[0m  [80/84], [94mLoss[0m : 2.15146

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70374
[1mStep[0m  [8/84], [94mLoss[0m : 2.65658
[1mStep[0m  [16/84], [94mLoss[0m : 2.12884
[1mStep[0m  [24/84], [94mLoss[0m : 2.49566
[1mStep[0m  [32/84], [94mLoss[0m : 2.40609
[1mStep[0m  [40/84], [94mLoss[0m : 1.98323
[1mStep[0m  [48/84], [94mLoss[0m : 2.30744
[1mStep[0m  [56/84], [94mLoss[0m : 2.21451
[1mStep[0m  [64/84], [94mLoss[0m : 2.66708
[1mStep[0m  [72/84], [94mLoss[0m : 2.52011
[1mStep[0m  [80/84], [94mLoss[0m : 2.80975

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37379
[1mStep[0m  [8/84], [94mLoss[0m : 2.83858
[1mStep[0m  [16/84], [94mLoss[0m : 2.43629
[1mStep[0m  [24/84], [94mLoss[0m : 2.29800
[1mStep[0m  [32/84], [94mLoss[0m : 2.59068
[1mStep[0m  [40/84], [94mLoss[0m : 2.66792
[1mStep[0m  [48/84], [94mLoss[0m : 2.36832
[1mStep[0m  [56/84], [94mLoss[0m : 2.76272
[1mStep[0m  [64/84], [94mLoss[0m : 2.77203
[1mStep[0m  [72/84], [94mLoss[0m : 2.65868
[1mStep[0m  [80/84], [94mLoss[0m : 2.31242

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84124
[1mStep[0m  [8/84], [94mLoss[0m : 2.68918
[1mStep[0m  [16/84], [94mLoss[0m : 2.42984
[1mStep[0m  [24/84], [94mLoss[0m : 2.70217
[1mStep[0m  [32/84], [94mLoss[0m : 2.37503
[1mStep[0m  [40/84], [94mLoss[0m : 2.57248
[1mStep[0m  [48/84], [94mLoss[0m : 2.57599
[1mStep[0m  [56/84], [94mLoss[0m : 2.36377
[1mStep[0m  [64/84], [94mLoss[0m : 2.54811
[1mStep[0m  [72/84], [94mLoss[0m : 2.45147
[1mStep[0m  [80/84], [94mLoss[0m : 2.53322

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49242
[1mStep[0m  [8/84], [94mLoss[0m : 2.12969
[1mStep[0m  [16/84], [94mLoss[0m : 2.20678
[1mStep[0m  [24/84], [94mLoss[0m : 2.54406
[1mStep[0m  [32/84], [94mLoss[0m : 2.60610
[1mStep[0m  [40/84], [94mLoss[0m : 2.62009
[1mStep[0m  [48/84], [94mLoss[0m : 2.49334
[1mStep[0m  [56/84], [94mLoss[0m : 2.38123
[1mStep[0m  [64/84], [94mLoss[0m : 2.34457
[1mStep[0m  [72/84], [94mLoss[0m : 2.15914
[1mStep[0m  [80/84], [94mLoss[0m : 2.48221

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75830
[1mStep[0m  [8/84], [94mLoss[0m : 2.49565
[1mStep[0m  [16/84], [94mLoss[0m : 2.57230
[1mStep[0m  [24/84], [94mLoss[0m : 2.59645
[1mStep[0m  [32/84], [94mLoss[0m : 2.35723
[1mStep[0m  [40/84], [94mLoss[0m : 2.63580
[1mStep[0m  [48/84], [94mLoss[0m : 2.24428
[1mStep[0m  [56/84], [94mLoss[0m : 2.55156
[1mStep[0m  [64/84], [94mLoss[0m : 2.23522
[1mStep[0m  [72/84], [94mLoss[0m : 2.22981
[1mStep[0m  [80/84], [94mLoss[0m : 2.17935

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.319, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10400
[1mStep[0m  [8/84], [94mLoss[0m : 2.48673
[1mStep[0m  [16/84], [94mLoss[0m : 2.25541
[1mStep[0m  [24/84], [94mLoss[0m : 2.79562
[1mStep[0m  [32/84], [94mLoss[0m : 2.74815
[1mStep[0m  [40/84], [94mLoss[0m : 2.43802
[1mStep[0m  [48/84], [94mLoss[0m : 2.28972
[1mStep[0m  [56/84], [94mLoss[0m : 2.70959
[1mStep[0m  [64/84], [94mLoss[0m : 2.44440
[1mStep[0m  [72/84], [94mLoss[0m : 2.23565
[1mStep[0m  [80/84], [94mLoss[0m : 2.54584

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.316, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28875
[1mStep[0m  [8/84], [94mLoss[0m : 2.45534
[1mStep[0m  [16/84], [94mLoss[0m : 2.50090
[1mStep[0m  [24/84], [94mLoss[0m : 2.44635
[1mStep[0m  [32/84], [94mLoss[0m : 2.58923
[1mStep[0m  [40/84], [94mLoss[0m : 2.90726
[1mStep[0m  [48/84], [94mLoss[0m : 2.37076
[1mStep[0m  [56/84], [94mLoss[0m : 2.09990
[1mStep[0m  [64/84], [94mLoss[0m : 2.21222
[1mStep[0m  [72/84], [94mLoss[0m : 2.55539
[1mStep[0m  [80/84], [94mLoss[0m : 2.26417

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.319, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39924
[1mStep[0m  [8/84], [94mLoss[0m : 2.52846
[1mStep[0m  [16/84], [94mLoss[0m : 2.08379
[1mStep[0m  [24/84], [94mLoss[0m : 2.61166
[1mStep[0m  [32/84], [94mLoss[0m : 2.41334
[1mStep[0m  [40/84], [94mLoss[0m : 2.33994
[1mStep[0m  [48/84], [94mLoss[0m : 2.21082
[1mStep[0m  [56/84], [94mLoss[0m : 2.08060
[1mStep[0m  [64/84], [94mLoss[0m : 2.21453
[1mStep[0m  [72/84], [94mLoss[0m : 2.17789
[1mStep[0m  [80/84], [94mLoss[0m : 2.44835

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.307, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48770
[1mStep[0m  [8/84], [94mLoss[0m : 2.08952
[1mStep[0m  [16/84], [94mLoss[0m : 2.78511
[1mStep[0m  [24/84], [94mLoss[0m : 2.24073
[1mStep[0m  [32/84], [94mLoss[0m : 2.63730
[1mStep[0m  [40/84], [94mLoss[0m : 2.31424
[1mStep[0m  [48/84], [94mLoss[0m : 2.15976
[1mStep[0m  [56/84], [94mLoss[0m : 2.43366
[1mStep[0m  [64/84], [94mLoss[0m : 2.21628
[1mStep[0m  [72/84], [94mLoss[0m : 2.29657
[1mStep[0m  [80/84], [94mLoss[0m : 2.49481

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.314, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27920
[1mStep[0m  [8/84], [94mLoss[0m : 2.37307
[1mStep[0m  [16/84], [94mLoss[0m : 2.27627
[1mStep[0m  [24/84], [94mLoss[0m : 2.42199
[1mStep[0m  [32/84], [94mLoss[0m : 2.28884
[1mStep[0m  [40/84], [94mLoss[0m : 2.02280
[1mStep[0m  [48/84], [94mLoss[0m : 2.25985
[1mStep[0m  [56/84], [94mLoss[0m : 2.61884
[1mStep[0m  [64/84], [94mLoss[0m : 2.40154
[1mStep[0m  [72/84], [94mLoss[0m : 2.35917
[1mStep[0m  [80/84], [94mLoss[0m : 2.57634

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.311, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17771
[1mStep[0m  [8/84], [94mLoss[0m : 2.29885
[1mStep[0m  [16/84], [94mLoss[0m : 2.31657
[1mStep[0m  [24/84], [94mLoss[0m : 2.32959
[1mStep[0m  [32/84], [94mLoss[0m : 2.18714
[1mStep[0m  [40/84], [94mLoss[0m : 2.39234
[1mStep[0m  [48/84], [94mLoss[0m : 2.51154
[1mStep[0m  [56/84], [94mLoss[0m : 2.23442
[1mStep[0m  [64/84], [94mLoss[0m : 2.41301
[1mStep[0m  [72/84], [94mLoss[0m : 2.46954
[1mStep[0m  [80/84], [94mLoss[0m : 2.18759

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16275
[1mStep[0m  [8/84], [94mLoss[0m : 2.11583
[1mStep[0m  [16/84], [94mLoss[0m : 2.25434
[1mStep[0m  [24/84], [94mLoss[0m : 2.47299
[1mStep[0m  [32/84], [94mLoss[0m : 2.36935
[1mStep[0m  [40/84], [94mLoss[0m : 2.11569
[1mStep[0m  [48/84], [94mLoss[0m : 2.24196
[1mStep[0m  [56/84], [94mLoss[0m : 2.32672
[1mStep[0m  [64/84], [94mLoss[0m : 2.15932
[1mStep[0m  [72/84], [94mLoss[0m : 2.15517
[1mStep[0m  [80/84], [94mLoss[0m : 2.44945

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.311, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30575
[1mStep[0m  [8/84], [94mLoss[0m : 2.16302
[1mStep[0m  [16/84], [94mLoss[0m : 2.51938
[1mStep[0m  [24/84], [94mLoss[0m : 2.51700
[1mStep[0m  [32/84], [94mLoss[0m : 2.61006
[1mStep[0m  [40/84], [94mLoss[0m : 2.40468
[1mStep[0m  [48/84], [94mLoss[0m : 2.32790
[1mStep[0m  [56/84], [94mLoss[0m : 2.14498
[1mStep[0m  [64/84], [94mLoss[0m : 2.15327
[1mStep[0m  [72/84], [94mLoss[0m : 2.38560
[1mStep[0m  [80/84], [94mLoss[0m : 2.27756

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31638
[1mStep[0m  [8/84], [94mLoss[0m : 2.32859
[1mStep[0m  [16/84], [94mLoss[0m : 2.44754
[1mStep[0m  [24/84], [94mLoss[0m : 2.12910
[1mStep[0m  [32/84], [94mLoss[0m : 2.64130
[1mStep[0m  [40/84], [94mLoss[0m : 2.31625
[1mStep[0m  [48/84], [94mLoss[0m : 2.36946
[1mStep[0m  [56/84], [94mLoss[0m : 2.34906
[1mStep[0m  [64/84], [94mLoss[0m : 2.38425
[1mStep[0m  [72/84], [94mLoss[0m : 2.68717
[1mStep[0m  [80/84], [94mLoss[0m : 2.18817

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.301, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29467
[1mStep[0m  [8/84], [94mLoss[0m : 2.55420
[1mStep[0m  [16/84], [94mLoss[0m : 2.44703
[1mStep[0m  [24/84], [94mLoss[0m : 2.13176
[1mStep[0m  [32/84], [94mLoss[0m : 2.45905
[1mStep[0m  [40/84], [94mLoss[0m : 2.34216
[1mStep[0m  [48/84], [94mLoss[0m : 2.09257
[1mStep[0m  [56/84], [94mLoss[0m : 2.67785
[1mStep[0m  [64/84], [94mLoss[0m : 2.20775
[1mStep[0m  [72/84], [94mLoss[0m : 2.41323
[1mStep[0m  [80/84], [94mLoss[0m : 2.24688

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46479
[1mStep[0m  [8/84], [94mLoss[0m : 2.29760
[1mStep[0m  [16/84], [94mLoss[0m : 2.29045
[1mStep[0m  [24/84], [94mLoss[0m : 2.11783
[1mStep[0m  [32/84], [94mLoss[0m : 2.49175
[1mStep[0m  [40/84], [94mLoss[0m : 2.32025
[1mStep[0m  [48/84], [94mLoss[0m : 2.33082
[1mStep[0m  [56/84], [94mLoss[0m : 2.20472
[1mStep[0m  [64/84], [94mLoss[0m : 2.29368
[1mStep[0m  [72/84], [94mLoss[0m : 2.53020
[1mStep[0m  [80/84], [94mLoss[0m : 2.28832

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.312, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49276
[1mStep[0m  [8/84], [94mLoss[0m : 2.10183
[1mStep[0m  [16/84], [94mLoss[0m : 2.44497
[1mStep[0m  [24/84], [94mLoss[0m : 2.58612
[1mStep[0m  [32/84], [94mLoss[0m : 2.54811
[1mStep[0m  [40/84], [94mLoss[0m : 2.10827
[1mStep[0m  [48/84], [94mLoss[0m : 2.20207
[1mStep[0m  [56/84], [94mLoss[0m : 2.45139
[1mStep[0m  [64/84], [94mLoss[0m : 2.29895
[1mStep[0m  [72/84], [94mLoss[0m : 2.57918
[1mStep[0m  [80/84], [94mLoss[0m : 2.27837

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.298, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17493
[1mStep[0m  [8/84], [94mLoss[0m : 2.40475
[1mStep[0m  [16/84], [94mLoss[0m : 2.39878
[1mStep[0m  [24/84], [94mLoss[0m : 2.15530
[1mStep[0m  [32/84], [94mLoss[0m : 2.13978
[1mStep[0m  [40/84], [94mLoss[0m : 2.50888
[1mStep[0m  [48/84], [94mLoss[0m : 2.38204
[1mStep[0m  [56/84], [94mLoss[0m : 2.26919
[1mStep[0m  [64/84], [94mLoss[0m : 2.35626
[1mStep[0m  [72/84], [94mLoss[0m : 2.09756
[1mStep[0m  [80/84], [94mLoss[0m : 2.04391

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.310, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40719
[1mStep[0m  [8/84], [94mLoss[0m : 2.25798
[1mStep[0m  [16/84], [94mLoss[0m : 2.46136
[1mStep[0m  [24/84], [94mLoss[0m : 2.29795
[1mStep[0m  [32/84], [94mLoss[0m : 2.39310
[1mStep[0m  [40/84], [94mLoss[0m : 2.23628
[1mStep[0m  [48/84], [94mLoss[0m : 2.33455
[1mStep[0m  [56/84], [94mLoss[0m : 2.29234
[1mStep[0m  [64/84], [94mLoss[0m : 2.29129
[1mStep[0m  [72/84], [94mLoss[0m : 2.47789
[1mStep[0m  [80/84], [94mLoss[0m : 2.34444

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.295, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58669
[1mStep[0m  [8/84], [94mLoss[0m : 2.61390
[1mStep[0m  [16/84], [94mLoss[0m : 2.19587
[1mStep[0m  [24/84], [94mLoss[0m : 2.16466
[1mStep[0m  [32/84], [94mLoss[0m : 2.37264
[1mStep[0m  [40/84], [94mLoss[0m : 2.50019
[1mStep[0m  [48/84], [94mLoss[0m : 2.34971
[1mStep[0m  [56/84], [94mLoss[0m : 2.24627
[1mStep[0m  [64/84], [94mLoss[0m : 2.32629
[1mStep[0m  [72/84], [94mLoss[0m : 2.21003
[1mStep[0m  [80/84], [94mLoss[0m : 2.43097

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19242
[1mStep[0m  [8/84], [94mLoss[0m : 2.26038
[1mStep[0m  [16/84], [94mLoss[0m : 2.35136
[1mStep[0m  [24/84], [94mLoss[0m : 2.28858
[1mStep[0m  [32/84], [94mLoss[0m : 2.00057
[1mStep[0m  [40/84], [94mLoss[0m : 2.22488
[1mStep[0m  [48/84], [94mLoss[0m : 2.07534
[1mStep[0m  [56/84], [94mLoss[0m : 2.61981
[1mStep[0m  [64/84], [94mLoss[0m : 2.31764
[1mStep[0m  [72/84], [94mLoss[0m : 2.24279
[1mStep[0m  [80/84], [94mLoss[0m : 2.41174

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.296, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54395
[1mStep[0m  [8/84], [94mLoss[0m : 2.30931
[1mStep[0m  [16/84], [94mLoss[0m : 2.06223
[1mStep[0m  [24/84], [94mLoss[0m : 2.26695
[1mStep[0m  [32/84], [94mLoss[0m : 2.52199
[1mStep[0m  [40/84], [94mLoss[0m : 2.30160
[1mStep[0m  [48/84], [94mLoss[0m : 2.25857
[1mStep[0m  [56/84], [94mLoss[0m : 2.40119
[1mStep[0m  [64/84], [94mLoss[0m : 2.38163
[1mStep[0m  [72/84], [94mLoss[0m : 2.37259
[1mStep[0m  [80/84], [94mLoss[0m : 2.17772

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.299, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22366
[1mStep[0m  [8/84], [94mLoss[0m : 2.32343
[1mStep[0m  [16/84], [94mLoss[0m : 2.04208
[1mStep[0m  [24/84], [94mLoss[0m : 2.39041
[1mStep[0m  [32/84], [94mLoss[0m : 2.11313
[1mStep[0m  [40/84], [94mLoss[0m : 2.28469
[1mStep[0m  [48/84], [94mLoss[0m : 2.09338
[1mStep[0m  [56/84], [94mLoss[0m : 2.17789
[1mStep[0m  [64/84], [94mLoss[0m : 2.14703
[1mStep[0m  [72/84], [94mLoss[0m : 2.27881
[1mStep[0m  [80/84], [94mLoss[0m : 2.17194

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.286, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22634
[1mStep[0m  [8/84], [94mLoss[0m : 1.81775
[1mStep[0m  [16/84], [94mLoss[0m : 2.32545
[1mStep[0m  [24/84], [94mLoss[0m : 2.11261
[1mStep[0m  [32/84], [94mLoss[0m : 2.08211
[1mStep[0m  [40/84], [94mLoss[0m : 2.30165
[1mStep[0m  [48/84], [94mLoss[0m : 2.33230
[1mStep[0m  [56/84], [94mLoss[0m : 2.59258
[1mStep[0m  [64/84], [94mLoss[0m : 2.04164
[1mStep[0m  [72/84], [94mLoss[0m : 2.51770
[1mStep[0m  [80/84], [94mLoss[0m : 2.17821

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.279, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41156
[1mStep[0m  [8/84], [94mLoss[0m : 2.17068
[1mStep[0m  [16/84], [94mLoss[0m : 2.51477
[1mStep[0m  [24/84], [94mLoss[0m : 2.37528
[1mStep[0m  [32/84], [94mLoss[0m : 2.24823
[1mStep[0m  [40/84], [94mLoss[0m : 2.56198
[1mStep[0m  [48/84], [94mLoss[0m : 2.18745
[1mStep[0m  [56/84], [94mLoss[0m : 2.47932
[1mStep[0m  [64/84], [94mLoss[0m : 2.29432
[1mStep[0m  [72/84], [94mLoss[0m : 2.26396
[1mStep[0m  [80/84], [94mLoss[0m : 2.26607

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.293, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.301
====================================

Phase 1 - Evaluation MAE:  2.3013683472360884
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.49155
[1mStep[0m  [8/84], [94mLoss[0m : 2.57474
[1mStep[0m  [16/84], [94mLoss[0m : 2.59759
[1mStep[0m  [24/84], [94mLoss[0m : 2.69401
[1mStep[0m  [32/84], [94mLoss[0m : 2.46036
[1mStep[0m  [40/84], [94mLoss[0m : 2.39781
[1mStep[0m  [48/84], [94mLoss[0m : 2.81309
[1mStep[0m  [56/84], [94mLoss[0m : 2.43097
[1mStep[0m  [64/84], [94mLoss[0m : 2.43455
[1mStep[0m  [72/84], [94mLoss[0m : 2.41049
[1mStep[0m  [80/84], [94mLoss[0m : 2.33772

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.307, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35108
[1mStep[0m  [8/84], [94mLoss[0m : 2.57540
[1mStep[0m  [16/84], [94mLoss[0m : 2.29950
[1mStep[0m  [24/84], [94mLoss[0m : 2.28501
[1mStep[0m  [32/84], [94mLoss[0m : 1.83361
[1mStep[0m  [40/84], [94mLoss[0m : 2.38749
[1mStep[0m  [48/84], [94mLoss[0m : 2.24521
[1mStep[0m  [56/84], [94mLoss[0m : 2.19510
[1mStep[0m  [64/84], [94mLoss[0m : 2.39987
[1mStep[0m  [72/84], [94mLoss[0m : 2.27387
[1mStep[0m  [80/84], [94mLoss[0m : 2.40237

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13519
[1mStep[0m  [8/84], [94mLoss[0m : 2.06459
[1mStep[0m  [16/84], [94mLoss[0m : 2.18043
[1mStep[0m  [24/84], [94mLoss[0m : 2.10238
[1mStep[0m  [32/84], [94mLoss[0m : 2.08734
[1mStep[0m  [40/84], [94mLoss[0m : 2.29317
[1mStep[0m  [48/84], [94mLoss[0m : 2.06192
[1mStep[0m  [56/84], [94mLoss[0m : 2.14215
[1mStep[0m  [64/84], [94mLoss[0m : 2.32053
[1mStep[0m  [72/84], [94mLoss[0m : 2.42973
[1mStep[0m  [80/84], [94mLoss[0m : 2.28465

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88786
[1mStep[0m  [8/84], [94mLoss[0m : 1.99521
[1mStep[0m  [16/84], [94mLoss[0m : 2.12770
[1mStep[0m  [24/84], [94mLoss[0m : 2.18188
[1mStep[0m  [32/84], [94mLoss[0m : 2.34680
[1mStep[0m  [40/84], [94mLoss[0m : 2.19460
[1mStep[0m  [48/84], [94mLoss[0m : 1.96676
[1mStep[0m  [56/84], [94mLoss[0m : 2.09567
[1mStep[0m  [64/84], [94mLoss[0m : 2.08996
[1mStep[0m  [72/84], [94mLoss[0m : 2.05635
[1mStep[0m  [80/84], [94mLoss[0m : 2.23305

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99510
[1mStep[0m  [8/84], [94mLoss[0m : 2.15841
[1mStep[0m  [16/84], [94mLoss[0m : 2.36900
[1mStep[0m  [24/84], [94mLoss[0m : 2.18177
[1mStep[0m  [32/84], [94mLoss[0m : 2.13959
[1mStep[0m  [40/84], [94mLoss[0m : 1.94662
[1mStep[0m  [48/84], [94mLoss[0m : 2.29151
[1mStep[0m  [56/84], [94mLoss[0m : 1.98115
[1mStep[0m  [64/84], [94mLoss[0m : 2.13132
[1mStep[0m  [72/84], [94mLoss[0m : 2.15982
[1mStep[0m  [80/84], [94mLoss[0m : 1.89910

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.074, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93774
[1mStep[0m  [8/84], [94mLoss[0m : 1.85432
[1mStep[0m  [16/84], [94mLoss[0m : 2.29042
[1mStep[0m  [24/84], [94mLoss[0m : 1.70859
[1mStep[0m  [32/84], [94mLoss[0m : 2.20949
[1mStep[0m  [40/84], [94mLoss[0m : 2.14530
[1mStep[0m  [48/84], [94mLoss[0m : 1.94680
[1mStep[0m  [56/84], [94mLoss[0m : 1.81752
[1mStep[0m  [64/84], [94mLoss[0m : 2.04052
[1mStep[0m  [72/84], [94mLoss[0m : 2.39208
[1mStep[0m  [80/84], [94mLoss[0m : 2.18494

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11089
[1mStep[0m  [8/84], [94mLoss[0m : 1.94155
[1mStep[0m  [16/84], [94mLoss[0m : 1.93167
[1mStep[0m  [24/84], [94mLoss[0m : 2.07260
[1mStep[0m  [32/84], [94mLoss[0m : 1.94568
[1mStep[0m  [40/84], [94mLoss[0m : 2.03047
[1mStep[0m  [48/84], [94mLoss[0m : 1.86627
[1mStep[0m  [56/84], [94mLoss[0m : 2.04203
[1mStep[0m  [64/84], [94mLoss[0m : 2.35565
[1mStep[0m  [72/84], [94mLoss[0m : 2.35881
[1mStep[0m  [80/84], [94mLoss[0m : 2.13859

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64233
[1mStep[0m  [8/84], [94mLoss[0m : 2.00955
[1mStep[0m  [16/84], [94mLoss[0m : 1.82451
[1mStep[0m  [24/84], [94mLoss[0m : 1.65337
[1mStep[0m  [32/84], [94mLoss[0m : 1.75993
[1mStep[0m  [40/84], [94mLoss[0m : 1.74920
[1mStep[0m  [48/84], [94mLoss[0m : 1.83603
[1mStep[0m  [56/84], [94mLoss[0m : 1.79594
[1mStep[0m  [64/84], [94mLoss[0m : 1.82183
[1mStep[0m  [72/84], [94mLoss[0m : 1.93771
[1mStep[0m  [80/84], [94mLoss[0m : 2.08600

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.890, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86067
[1mStep[0m  [8/84], [94mLoss[0m : 2.03036
[1mStep[0m  [16/84], [94mLoss[0m : 1.80542
[1mStep[0m  [24/84], [94mLoss[0m : 1.68590
[1mStep[0m  [32/84], [94mLoss[0m : 1.64749
[1mStep[0m  [40/84], [94mLoss[0m : 1.65349
[1mStep[0m  [48/84], [94mLoss[0m : 1.90100
[1mStep[0m  [56/84], [94mLoss[0m : 1.77885
[1mStep[0m  [64/84], [94mLoss[0m : 1.74122
[1mStep[0m  [72/84], [94mLoss[0m : 1.72707
[1mStep[0m  [80/84], [94mLoss[0m : 1.79300

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64512
[1mStep[0m  [8/84], [94mLoss[0m : 1.79149
[1mStep[0m  [16/84], [94mLoss[0m : 1.49683
[1mStep[0m  [24/84], [94mLoss[0m : 1.63421
[1mStep[0m  [32/84], [94mLoss[0m : 1.88062
[1mStep[0m  [40/84], [94mLoss[0m : 1.98301
[1mStep[0m  [48/84], [94mLoss[0m : 2.04412
[1mStep[0m  [56/84], [94mLoss[0m : 1.78513
[1mStep[0m  [64/84], [94mLoss[0m : 1.79681
[1mStep[0m  [72/84], [94mLoss[0m : 1.78636
[1mStep[0m  [80/84], [94mLoss[0m : 1.51208

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69054
[1mStep[0m  [8/84], [94mLoss[0m : 1.52052
[1mStep[0m  [16/84], [94mLoss[0m : 1.68863
[1mStep[0m  [24/84], [94mLoss[0m : 1.91860
[1mStep[0m  [32/84], [94mLoss[0m : 1.81854
[1mStep[0m  [40/84], [94mLoss[0m : 1.65865
[1mStep[0m  [48/84], [94mLoss[0m : 1.81812
[1mStep[0m  [56/84], [94mLoss[0m : 1.52318
[1mStep[0m  [64/84], [94mLoss[0m : 1.93159
[1mStep[0m  [72/84], [94mLoss[0m : 1.82901
[1mStep[0m  [80/84], [94mLoss[0m : 1.79005

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57095
[1mStep[0m  [8/84], [94mLoss[0m : 1.80036
[1mStep[0m  [16/84], [94mLoss[0m : 1.74058
[1mStep[0m  [24/84], [94mLoss[0m : 1.60785
[1mStep[0m  [32/84], [94mLoss[0m : 1.72828
[1mStep[0m  [40/84], [94mLoss[0m : 1.65080
[1mStep[0m  [48/84], [94mLoss[0m : 1.73616
[1mStep[0m  [56/84], [94mLoss[0m : 1.79067
[1mStep[0m  [64/84], [94mLoss[0m : 1.35386
[1mStep[0m  [72/84], [94mLoss[0m : 1.70037
[1mStep[0m  [80/84], [94mLoss[0m : 1.98250

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.711, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76505
[1mStep[0m  [8/84], [94mLoss[0m : 1.69638
[1mStep[0m  [16/84], [94mLoss[0m : 1.90320
[1mStep[0m  [24/84], [94mLoss[0m : 1.38732
[1mStep[0m  [32/84], [94mLoss[0m : 1.82699
[1mStep[0m  [40/84], [94mLoss[0m : 1.50763
[1mStep[0m  [48/84], [94mLoss[0m : 1.60622
[1mStep[0m  [56/84], [94mLoss[0m : 1.75964
[1mStep[0m  [64/84], [94mLoss[0m : 1.75193
[1mStep[0m  [72/84], [94mLoss[0m : 1.83213
[1mStep[0m  [80/84], [94mLoss[0m : 1.75246

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.667, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57972
[1mStep[0m  [8/84], [94mLoss[0m : 1.41657
[1mStep[0m  [16/84], [94mLoss[0m : 1.85458
[1mStep[0m  [24/84], [94mLoss[0m : 1.74645
[1mStep[0m  [32/84], [94mLoss[0m : 1.51343
[1mStep[0m  [40/84], [94mLoss[0m : 1.55696
[1mStep[0m  [48/84], [94mLoss[0m : 1.59857
[1mStep[0m  [56/84], [94mLoss[0m : 1.46396
[1mStep[0m  [64/84], [94mLoss[0m : 1.59596
[1mStep[0m  [72/84], [94mLoss[0m : 1.53960
[1mStep[0m  [80/84], [94mLoss[0m : 1.54300

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61312
[1mStep[0m  [8/84], [94mLoss[0m : 1.50189
[1mStep[0m  [16/84], [94mLoss[0m : 1.54947
[1mStep[0m  [24/84], [94mLoss[0m : 1.56014
[1mStep[0m  [32/84], [94mLoss[0m : 1.40559
[1mStep[0m  [40/84], [94mLoss[0m : 1.64352
[1mStep[0m  [48/84], [94mLoss[0m : 1.52110
[1mStep[0m  [56/84], [94mLoss[0m : 1.61317
[1mStep[0m  [64/84], [94mLoss[0m : 1.77850
[1mStep[0m  [72/84], [94mLoss[0m : 1.70535
[1mStep[0m  [80/84], [94mLoss[0m : 1.55323

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.609, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75127
[1mStep[0m  [8/84], [94mLoss[0m : 1.49550
[1mStep[0m  [16/84], [94mLoss[0m : 1.43789
[1mStep[0m  [24/84], [94mLoss[0m : 1.42704
[1mStep[0m  [32/84], [94mLoss[0m : 1.55967
[1mStep[0m  [40/84], [94mLoss[0m : 1.44804
[1mStep[0m  [48/84], [94mLoss[0m : 1.50988
[1mStep[0m  [56/84], [94mLoss[0m : 1.41818
[1mStep[0m  [64/84], [94mLoss[0m : 1.63512
[1mStep[0m  [72/84], [94mLoss[0m : 1.42920
[1mStep[0m  [80/84], [94mLoss[0m : 1.62557

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.518, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51190
[1mStep[0m  [8/84], [94mLoss[0m : 1.41265
[1mStep[0m  [16/84], [94mLoss[0m : 1.54161
[1mStep[0m  [24/84], [94mLoss[0m : 1.47821
[1mStep[0m  [32/84], [94mLoss[0m : 1.43276
[1mStep[0m  [40/84], [94mLoss[0m : 1.57156
[1mStep[0m  [48/84], [94mLoss[0m : 1.80334
[1mStep[0m  [56/84], [94mLoss[0m : 1.77207
[1mStep[0m  [64/84], [94mLoss[0m : 1.40452
[1mStep[0m  [72/84], [94mLoss[0m : 1.50345
[1mStep[0m  [80/84], [94mLoss[0m : 1.87214

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.556, [92mTest[0m: 2.500, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.43088
[1mStep[0m  [8/84], [94mLoss[0m : 1.27724
[1mStep[0m  [16/84], [94mLoss[0m : 1.54143
[1mStep[0m  [24/84], [94mLoss[0m : 1.73940
[1mStep[0m  [32/84], [94mLoss[0m : 1.48610
[1mStep[0m  [40/84], [94mLoss[0m : 1.49545
[1mStep[0m  [48/84], [94mLoss[0m : 1.54102
[1mStep[0m  [56/84], [94mLoss[0m : 1.44485
[1mStep[0m  [64/84], [94mLoss[0m : 1.70265
[1mStep[0m  [72/84], [94mLoss[0m : 1.40421
[1mStep[0m  [80/84], [94mLoss[0m : 1.54698

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.526, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48090
[1mStep[0m  [8/84], [94mLoss[0m : 1.44386
[1mStep[0m  [16/84], [94mLoss[0m : 1.33418
[1mStep[0m  [24/84], [94mLoss[0m : 1.64183
[1mStep[0m  [32/84], [94mLoss[0m : 1.56230
[1mStep[0m  [40/84], [94mLoss[0m : 1.29236
[1mStep[0m  [48/84], [94mLoss[0m : 1.50239
[1mStep[0m  [56/84], [94mLoss[0m : 1.61732
[1mStep[0m  [64/84], [94mLoss[0m : 1.75647
[1mStep[0m  [72/84], [94mLoss[0m : 1.61581
[1mStep[0m  [80/84], [94mLoss[0m : 1.33740

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.510, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36747
[1mStep[0m  [8/84], [94mLoss[0m : 1.51754
[1mStep[0m  [16/84], [94mLoss[0m : 1.46450
[1mStep[0m  [24/84], [94mLoss[0m : 1.49285
[1mStep[0m  [32/84], [94mLoss[0m : 1.53747
[1mStep[0m  [40/84], [94mLoss[0m : 1.34709
[1mStep[0m  [48/84], [94mLoss[0m : 1.47106
[1mStep[0m  [56/84], [94mLoss[0m : 1.50703
[1mStep[0m  [64/84], [94mLoss[0m : 1.38098
[1mStep[0m  [72/84], [94mLoss[0m : 1.40711
[1mStep[0m  [80/84], [94mLoss[0m : 1.49256

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.467, [92mTest[0m: 2.580, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32189
[1mStep[0m  [8/84], [94mLoss[0m : 1.37675
[1mStep[0m  [16/84], [94mLoss[0m : 1.37143
[1mStep[0m  [24/84], [94mLoss[0m : 1.46456
[1mStep[0m  [32/84], [94mLoss[0m : 1.15127
[1mStep[0m  [40/84], [94mLoss[0m : 1.76207
[1mStep[0m  [48/84], [94mLoss[0m : 1.46432
[1mStep[0m  [56/84], [94mLoss[0m : 1.56517
[1mStep[0m  [64/84], [94mLoss[0m : 1.38555
[1mStep[0m  [72/84], [94mLoss[0m : 1.29056
[1mStep[0m  [80/84], [94mLoss[0m : 1.33622

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.408, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.25488
[1mStep[0m  [8/84], [94mLoss[0m : 1.29464
[1mStep[0m  [16/84], [94mLoss[0m : 1.40748
[1mStep[0m  [24/84], [94mLoss[0m : 1.34555
[1mStep[0m  [32/84], [94mLoss[0m : 1.45568
[1mStep[0m  [40/84], [94mLoss[0m : 1.28692
[1mStep[0m  [48/84], [94mLoss[0m : 1.55972
[1mStep[0m  [56/84], [94mLoss[0m : 1.41074
[1mStep[0m  [64/84], [94mLoss[0m : 1.53331
[1mStep[0m  [72/84], [94mLoss[0m : 1.48571
[1mStep[0m  [80/84], [94mLoss[0m : 1.33880

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.412, [92mTest[0m: 2.539, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.481
====================================

Phase 2 - Evaluation MAE:  2.480939941746848
MAE score P1       2.301368
MAE score P2        2.48094
loss               1.408202
learning_rate          0.01
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay         0.0001
Name: 25, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.34147
[1mStep[0m  [8/84], [94mLoss[0m : 5.38674
[1mStep[0m  [16/84], [94mLoss[0m : 3.46624
[1mStep[0m  [24/84], [94mLoss[0m : 3.24431
[1mStep[0m  [32/84], [94mLoss[0m : 2.93059
[1mStep[0m  [40/84], [94mLoss[0m : 2.37406
[1mStep[0m  [48/84], [94mLoss[0m : 2.42549
[1mStep[0m  [56/84], [94mLoss[0m : 2.28752
[1mStep[0m  [64/84], [94mLoss[0m : 2.32035
[1mStep[0m  [72/84], [94mLoss[0m : 2.50933
[1mStep[0m  [80/84], [94mLoss[0m : 2.19043

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.197, [92mTest[0m: 11.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56327
[1mStep[0m  [8/84], [94mLoss[0m : 2.54647
[1mStep[0m  [16/84], [94mLoss[0m : 2.51489
[1mStep[0m  [24/84], [94mLoss[0m : 2.64501
[1mStep[0m  [32/84], [94mLoss[0m : 2.51876
[1mStep[0m  [40/84], [94mLoss[0m : 2.58681
[1mStep[0m  [48/84], [94mLoss[0m : 2.69606
[1mStep[0m  [56/84], [94mLoss[0m : 2.28610
[1mStep[0m  [64/84], [94mLoss[0m : 2.35317
[1mStep[0m  [72/84], [94mLoss[0m : 2.35624
[1mStep[0m  [80/84], [94mLoss[0m : 2.51044

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24722
[1mStep[0m  [8/84], [94mLoss[0m : 2.30095
[1mStep[0m  [16/84], [94mLoss[0m : 2.49331
[1mStep[0m  [24/84], [94mLoss[0m : 2.64684
[1mStep[0m  [32/84], [94mLoss[0m : 2.53007
[1mStep[0m  [40/84], [94mLoss[0m : 2.31301
[1mStep[0m  [48/84], [94mLoss[0m : 2.64182
[1mStep[0m  [56/84], [94mLoss[0m : 2.90295
[1mStep[0m  [64/84], [94mLoss[0m : 2.46297
[1mStep[0m  [72/84], [94mLoss[0m : 2.47311
[1mStep[0m  [80/84], [94mLoss[0m : 2.39170

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48841
[1mStep[0m  [8/84], [94mLoss[0m : 2.62935
[1mStep[0m  [16/84], [94mLoss[0m : 2.25985
[1mStep[0m  [24/84], [94mLoss[0m : 2.23520
[1mStep[0m  [32/84], [94mLoss[0m : 2.43529
[1mStep[0m  [40/84], [94mLoss[0m : 2.77882
[1mStep[0m  [48/84], [94mLoss[0m : 2.68133
[1mStep[0m  [56/84], [94mLoss[0m : 2.54847
[1mStep[0m  [64/84], [94mLoss[0m : 2.49314
[1mStep[0m  [72/84], [94mLoss[0m : 2.49214
[1mStep[0m  [80/84], [94mLoss[0m : 2.46193

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31462
[1mStep[0m  [8/84], [94mLoss[0m : 2.64549
[1mStep[0m  [16/84], [94mLoss[0m : 2.65884
[1mStep[0m  [24/84], [94mLoss[0m : 2.33830
[1mStep[0m  [32/84], [94mLoss[0m : 2.40771
[1mStep[0m  [40/84], [94mLoss[0m : 2.49301
[1mStep[0m  [48/84], [94mLoss[0m : 2.48992
[1mStep[0m  [56/84], [94mLoss[0m : 2.66066
[1mStep[0m  [64/84], [94mLoss[0m : 2.32523
[1mStep[0m  [72/84], [94mLoss[0m : 2.67273
[1mStep[0m  [80/84], [94mLoss[0m : 2.22905

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18579
[1mStep[0m  [8/84], [94mLoss[0m : 2.33301
[1mStep[0m  [16/84], [94mLoss[0m : 2.47624
[1mStep[0m  [24/84], [94mLoss[0m : 2.47788
[1mStep[0m  [32/84], [94mLoss[0m : 2.33359
[1mStep[0m  [40/84], [94mLoss[0m : 2.69869
[1mStep[0m  [48/84], [94mLoss[0m : 2.62811
[1mStep[0m  [56/84], [94mLoss[0m : 2.40539
[1mStep[0m  [64/84], [94mLoss[0m : 2.55951
[1mStep[0m  [72/84], [94mLoss[0m : 2.75185
[1mStep[0m  [80/84], [94mLoss[0m : 2.38657

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42349
[1mStep[0m  [8/84], [94mLoss[0m : 2.60726
[1mStep[0m  [16/84], [94mLoss[0m : 2.45065
[1mStep[0m  [24/84], [94mLoss[0m : 2.15715
[1mStep[0m  [32/84], [94mLoss[0m : 2.38634
[1mStep[0m  [40/84], [94mLoss[0m : 2.68264
[1mStep[0m  [48/84], [94mLoss[0m : 2.64373
[1mStep[0m  [56/84], [94mLoss[0m : 2.40189
[1mStep[0m  [64/84], [94mLoss[0m : 2.57155
[1mStep[0m  [72/84], [94mLoss[0m : 2.37467
[1mStep[0m  [80/84], [94mLoss[0m : 2.32269

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79016
[1mStep[0m  [8/84], [94mLoss[0m : 2.32464
[1mStep[0m  [16/84], [94mLoss[0m : 2.41348
[1mStep[0m  [24/84], [94mLoss[0m : 2.48114
[1mStep[0m  [32/84], [94mLoss[0m : 2.71719
[1mStep[0m  [40/84], [94mLoss[0m : 2.37230
[1mStep[0m  [48/84], [94mLoss[0m : 2.38663
[1mStep[0m  [56/84], [94mLoss[0m : 2.31803
[1mStep[0m  [64/84], [94mLoss[0m : 2.21768
[1mStep[0m  [72/84], [94mLoss[0m : 2.16635
[1mStep[0m  [80/84], [94mLoss[0m : 2.51738

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81311
[1mStep[0m  [8/84], [94mLoss[0m : 2.45738
[1mStep[0m  [16/84], [94mLoss[0m : 2.49521
[1mStep[0m  [24/84], [94mLoss[0m : 2.44944
[1mStep[0m  [32/84], [94mLoss[0m : 2.35936
[1mStep[0m  [40/84], [94mLoss[0m : 2.48013
[1mStep[0m  [48/84], [94mLoss[0m : 2.67536
[1mStep[0m  [56/84], [94mLoss[0m : 2.39785
[1mStep[0m  [64/84], [94mLoss[0m : 2.44721
[1mStep[0m  [72/84], [94mLoss[0m : 2.44487
[1mStep[0m  [80/84], [94mLoss[0m : 2.48174

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67736
[1mStep[0m  [8/84], [94mLoss[0m : 2.43379
[1mStep[0m  [16/84], [94mLoss[0m : 2.17075
[1mStep[0m  [24/84], [94mLoss[0m : 2.57437
[1mStep[0m  [32/84], [94mLoss[0m : 2.16967
[1mStep[0m  [40/84], [94mLoss[0m : 2.51606
[1mStep[0m  [48/84], [94mLoss[0m : 2.22725
[1mStep[0m  [56/84], [94mLoss[0m : 2.37648
[1mStep[0m  [64/84], [94mLoss[0m : 2.43010
[1mStep[0m  [72/84], [94mLoss[0m : 2.64933
[1mStep[0m  [80/84], [94mLoss[0m : 2.40825

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70062
[1mStep[0m  [8/84], [94mLoss[0m : 2.32179
[1mStep[0m  [16/84], [94mLoss[0m : 2.48272
[1mStep[0m  [24/84], [94mLoss[0m : 2.38965
[1mStep[0m  [32/84], [94mLoss[0m : 2.47864
[1mStep[0m  [40/84], [94mLoss[0m : 2.18038
[1mStep[0m  [48/84], [94mLoss[0m : 2.49579
[1mStep[0m  [56/84], [94mLoss[0m : 2.19205
[1mStep[0m  [64/84], [94mLoss[0m : 2.60941
[1mStep[0m  [72/84], [94mLoss[0m : 2.12504
[1mStep[0m  [80/84], [94mLoss[0m : 2.45630

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46272
[1mStep[0m  [8/84], [94mLoss[0m : 2.75975
[1mStep[0m  [16/84], [94mLoss[0m : 2.50435
[1mStep[0m  [24/84], [94mLoss[0m : 2.47543
[1mStep[0m  [32/84], [94mLoss[0m : 2.45797
[1mStep[0m  [40/84], [94mLoss[0m : 2.64472
[1mStep[0m  [48/84], [94mLoss[0m : 2.31910
[1mStep[0m  [56/84], [94mLoss[0m : 2.77658
[1mStep[0m  [64/84], [94mLoss[0m : 2.82816
[1mStep[0m  [72/84], [94mLoss[0m : 2.80809
[1mStep[0m  [80/84], [94mLoss[0m : 2.47916

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33392
[1mStep[0m  [8/84], [94mLoss[0m : 2.52344
[1mStep[0m  [16/84], [94mLoss[0m : 2.38010
[1mStep[0m  [24/84], [94mLoss[0m : 2.18817
[1mStep[0m  [32/84], [94mLoss[0m : 2.54673
[1mStep[0m  [40/84], [94mLoss[0m : 2.28267
[1mStep[0m  [48/84], [94mLoss[0m : 2.63668
[1mStep[0m  [56/84], [94mLoss[0m : 2.34150
[1mStep[0m  [64/84], [94mLoss[0m : 2.41290
[1mStep[0m  [72/84], [94mLoss[0m : 2.31993
[1mStep[0m  [80/84], [94mLoss[0m : 2.66799

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31007
[1mStep[0m  [8/84], [94mLoss[0m : 2.30088
[1mStep[0m  [16/84], [94mLoss[0m : 2.62945
[1mStep[0m  [24/84], [94mLoss[0m : 2.39707
[1mStep[0m  [32/84], [94mLoss[0m : 2.36827
[1mStep[0m  [40/84], [94mLoss[0m : 2.48584
[1mStep[0m  [48/84], [94mLoss[0m : 2.40194
[1mStep[0m  [56/84], [94mLoss[0m : 2.31414
[1mStep[0m  [64/84], [94mLoss[0m : 2.50262
[1mStep[0m  [72/84], [94mLoss[0m : 2.74262
[1mStep[0m  [80/84], [94mLoss[0m : 2.26618

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45047
[1mStep[0m  [8/84], [94mLoss[0m : 2.36600
[1mStep[0m  [16/84], [94mLoss[0m : 2.76229
[1mStep[0m  [24/84], [94mLoss[0m : 2.65738
[1mStep[0m  [32/84], [94mLoss[0m : 2.47605
[1mStep[0m  [40/84], [94mLoss[0m : 2.55226
[1mStep[0m  [48/84], [94mLoss[0m : 2.72551
[1mStep[0m  [56/84], [94mLoss[0m : 2.32703
[1mStep[0m  [64/84], [94mLoss[0m : 2.56895
[1mStep[0m  [72/84], [94mLoss[0m : 2.25305
[1mStep[0m  [80/84], [94mLoss[0m : 2.41262

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66657
[1mStep[0m  [8/84], [94mLoss[0m : 2.74601
[1mStep[0m  [16/84], [94mLoss[0m : 2.61251
[1mStep[0m  [24/84], [94mLoss[0m : 2.74754
[1mStep[0m  [32/84], [94mLoss[0m : 2.33281
[1mStep[0m  [40/84], [94mLoss[0m : 2.90658
[1mStep[0m  [48/84], [94mLoss[0m : 2.39904
[1mStep[0m  [56/84], [94mLoss[0m : 2.31179
[1mStep[0m  [64/84], [94mLoss[0m : 2.45578
[1mStep[0m  [72/84], [94mLoss[0m : 2.47850
[1mStep[0m  [80/84], [94mLoss[0m : 2.44705

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28605
[1mStep[0m  [8/84], [94mLoss[0m : 2.26694
[1mStep[0m  [16/84], [94mLoss[0m : 2.41722
[1mStep[0m  [24/84], [94mLoss[0m : 2.67878
[1mStep[0m  [32/84], [94mLoss[0m : 2.37564
[1mStep[0m  [40/84], [94mLoss[0m : 2.44474
[1mStep[0m  [48/84], [94mLoss[0m : 2.28900
[1mStep[0m  [56/84], [94mLoss[0m : 2.44469
[1mStep[0m  [64/84], [94mLoss[0m : 2.17747
[1mStep[0m  [72/84], [94mLoss[0m : 2.05912
[1mStep[0m  [80/84], [94mLoss[0m : 2.52375

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52987
[1mStep[0m  [8/84], [94mLoss[0m : 2.67030
[1mStep[0m  [16/84], [94mLoss[0m : 2.33671
[1mStep[0m  [24/84], [94mLoss[0m : 2.50162
[1mStep[0m  [32/84], [94mLoss[0m : 2.48778
[1mStep[0m  [40/84], [94mLoss[0m : 2.44869
[1mStep[0m  [48/84], [94mLoss[0m : 2.18549
[1mStep[0m  [56/84], [94mLoss[0m : 2.54749
[1mStep[0m  [64/84], [94mLoss[0m : 2.44210
[1mStep[0m  [72/84], [94mLoss[0m : 2.37578
[1mStep[0m  [80/84], [94mLoss[0m : 2.39053

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46444
[1mStep[0m  [8/84], [94mLoss[0m : 2.57725
[1mStep[0m  [16/84], [94mLoss[0m : 2.46725
[1mStep[0m  [24/84], [94mLoss[0m : 2.69054
[1mStep[0m  [32/84], [94mLoss[0m : 2.27493
[1mStep[0m  [40/84], [94mLoss[0m : 2.26522
[1mStep[0m  [48/84], [94mLoss[0m : 2.41796
[1mStep[0m  [56/84], [94mLoss[0m : 2.48621
[1mStep[0m  [64/84], [94mLoss[0m : 2.54315
[1mStep[0m  [72/84], [94mLoss[0m : 2.34572
[1mStep[0m  [80/84], [94mLoss[0m : 2.48263

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24495
[1mStep[0m  [8/84], [94mLoss[0m : 2.71433
[1mStep[0m  [16/84], [94mLoss[0m : 2.62585
[1mStep[0m  [24/84], [94mLoss[0m : 2.69944
[1mStep[0m  [32/84], [94mLoss[0m : 2.50632
[1mStep[0m  [40/84], [94mLoss[0m : 2.65548
[1mStep[0m  [48/84], [94mLoss[0m : 2.70231
[1mStep[0m  [56/84], [94mLoss[0m : 2.45477
[1mStep[0m  [64/84], [94mLoss[0m : 2.42049
[1mStep[0m  [72/84], [94mLoss[0m : 2.24952
[1mStep[0m  [80/84], [94mLoss[0m : 2.60602

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55313
[1mStep[0m  [8/84], [94mLoss[0m : 2.09859
[1mStep[0m  [16/84], [94mLoss[0m : 2.70302
[1mStep[0m  [24/84], [94mLoss[0m : 2.46900
[1mStep[0m  [32/84], [94mLoss[0m : 3.03769
[1mStep[0m  [40/84], [94mLoss[0m : 2.74391
[1mStep[0m  [48/84], [94mLoss[0m : 2.36708
[1mStep[0m  [56/84], [94mLoss[0m : 2.62797
[1mStep[0m  [64/84], [94mLoss[0m : 2.28282
[1mStep[0m  [72/84], [94mLoss[0m : 2.37446
[1mStep[0m  [80/84], [94mLoss[0m : 2.47003

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72040
[1mStep[0m  [8/84], [94mLoss[0m : 2.33641
[1mStep[0m  [16/84], [94mLoss[0m : 2.20664
[1mStep[0m  [24/84], [94mLoss[0m : 2.05520
[1mStep[0m  [32/84], [94mLoss[0m : 2.51550
[1mStep[0m  [40/84], [94mLoss[0m : 2.46513
[1mStep[0m  [48/84], [94mLoss[0m : 2.63479
[1mStep[0m  [56/84], [94mLoss[0m : 2.30105
[1mStep[0m  [64/84], [94mLoss[0m : 2.23305
[1mStep[0m  [72/84], [94mLoss[0m : 2.22548
[1mStep[0m  [80/84], [94mLoss[0m : 2.40695

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36535
[1mStep[0m  [8/84], [94mLoss[0m : 2.42936
[1mStep[0m  [16/84], [94mLoss[0m : 2.45109
[1mStep[0m  [24/84], [94mLoss[0m : 2.30089
[1mStep[0m  [32/84], [94mLoss[0m : 2.17900
[1mStep[0m  [40/84], [94mLoss[0m : 2.46983
[1mStep[0m  [48/84], [94mLoss[0m : 2.32432
[1mStep[0m  [56/84], [94mLoss[0m : 2.70263
[1mStep[0m  [64/84], [94mLoss[0m : 2.50569
[1mStep[0m  [72/84], [94mLoss[0m : 2.27817
[1mStep[0m  [80/84], [94mLoss[0m : 2.42368

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47147
[1mStep[0m  [8/84], [94mLoss[0m : 2.37849
[1mStep[0m  [16/84], [94mLoss[0m : 2.04262
[1mStep[0m  [24/84], [94mLoss[0m : 2.27064
[1mStep[0m  [32/84], [94mLoss[0m : 2.52538
[1mStep[0m  [40/84], [94mLoss[0m : 2.44690
[1mStep[0m  [48/84], [94mLoss[0m : 2.33218
[1mStep[0m  [56/84], [94mLoss[0m : 2.50288
[1mStep[0m  [64/84], [94mLoss[0m : 2.74685
[1mStep[0m  [72/84], [94mLoss[0m : 2.40786
[1mStep[0m  [80/84], [94mLoss[0m : 2.56421

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46317
[1mStep[0m  [8/84], [94mLoss[0m : 2.27632
[1mStep[0m  [16/84], [94mLoss[0m : 2.10017
[1mStep[0m  [24/84], [94mLoss[0m : 2.01900
[1mStep[0m  [32/84], [94mLoss[0m : 2.57639
[1mStep[0m  [40/84], [94mLoss[0m : 2.42365
[1mStep[0m  [48/84], [94mLoss[0m : 2.35018
[1mStep[0m  [56/84], [94mLoss[0m : 2.55704
[1mStep[0m  [64/84], [94mLoss[0m : 2.47138
[1mStep[0m  [72/84], [94mLoss[0m : 2.24038
[1mStep[0m  [80/84], [94mLoss[0m : 2.45068

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.386, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45052
[1mStep[0m  [8/84], [94mLoss[0m : 2.56506
[1mStep[0m  [16/84], [94mLoss[0m : 2.60309
[1mStep[0m  [24/84], [94mLoss[0m : 2.39750
[1mStep[0m  [32/84], [94mLoss[0m : 2.35801
[1mStep[0m  [40/84], [94mLoss[0m : 2.47622
[1mStep[0m  [48/84], [94mLoss[0m : 2.23526
[1mStep[0m  [56/84], [94mLoss[0m : 2.50842
[1mStep[0m  [64/84], [94mLoss[0m : 3.02155
[1mStep[0m  [72/84], [94mLoss[0m : 2.51548
[1mStep[0m  [80/84], [94mLoss[0m : 2.27901

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65256
[1mStep[0m  [8/84], [94mLoss[0m : 2.42633
[1mStep[0m  [16/84], [94mLoss[0m : 2.23064
[1mStep[0m  [24/84], [94mLoss[0m : 2.39464
[1mStep[0m  [32/84], [94mLoss[0m : 2.91740
[1mStep[0m  [40/84], [94mLoss[0m : 2.64992
[1mStep[0m  [48/84], [94mLoss[0m : 2.20000
[1mStep[0m  [56/84], [94mLoss[0m : 2.75328
[1mStep[0m  [64/84], [94mLoss[0m : 2.43676
[1mStep[0m  [72/84], [94mLoss[0m : 2.20966
[1mStep[0m  [80/84], [94mLoss[0m : 2.48769

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30779
[1mStep[0m  [8/84], [94mLoss[0m : 2.55195
[1mStep[0m  [16/84], [94mLoss[0m : 2.67200
[1mStep[0m  [24/84], [94mLoss[0m : 2.48618
[1mStep[0m  [32/84], [94mLoss[0m : 2.54632
[1mStep[0m  [40/84], [94mLoss[0m : 2.48824
[1mStep[0m  [48/84], [94mLoss[0m : 2.41844
[1mStep[0m  [56/84], [94mLoss[0m : 2.43703
[1mStep[0m  [64/84], [94mLoss[0m : 2.31866
[1mStep[0m  [72/84], [94mLoss[0m : 2.59448
[1mStep[0m  [80/84], [94mLoss[0m : 2.76821

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49136
[1mStep[0m  [8/84], [94mLoss[0m : 2.25957
[1mStep[0m  [16/84], [94mLoss[0m : 2.59349
[1mStep[0m  [24/84], [94mLoss[0m : 2.38198
[1mStep[0m  [32/84], [94mLoss[0m : 2.36986
[1mStep[0m  [40/84], [94mLoss[0m : 2.60875
[1mStep[0m  [48/84], [94mLoss[0m : 2.35572
[1mStep[0m  [56/84], [94mLoss[0m : 2.46886
[1mStep[0m  [64/84], [94mLoss[0m : 2.50573
[1mStep[0m  [72/84], [94mLoss[0m : 2.33096
[1mStep[0m  [80/84], [94mLoss[0m : 2.48764

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26644
[1mStep[0m  [8/84], [94mLoss[0m : 2.14444
[1mStep[0m  [16/84], [94mLoss[0m : 2.54740
[1mStep[0m  [24/84], [94mLoss[0m : 2.59057
[1mStep[0m  [32/84], [94mLoss[0m : 2.60154
[1mStep[0m  [40/84], [94mLoss[0m : 2.34691
[1mStep[0m  [48/84], [94mLoss[0m : 2.21755
[1mStep[0m  [56/84], [94mLoss[0m : 2.55987
[1mStep[0m  [64/84], [94mLoss[0m : 2.23501
[1mStep[0m  [72/84], [94mLoss[0m : 2.62677
[1mStep[0m  [80/84], [94mLoss[0m : 2.43529

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.379, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.318
====================================

Phase 1 - Evaluation MAE:  2.3179767727851868
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.36148
[1mStep[0m  [8/84], [94mLoss[0m : 2.54287
[1mStep[0m  [16/84], [94mLoss[0m : 2.52423
[1mStep[0m  [24/84], [94mLoss[0m : 2.74238
[1mStep[0m  [32/84], [94mLoss[0m : 2.51581
[1mStep[0m  [40/84], [94mLoss[0m : 2.58291
[1mStep[0m  [48/84], [94mLoss[0m : 2.20712
[1mStep[0m  [56/84], [94mLoss[0m : 2.40164
[1mStep[0m  [64/84], [94mLoss[0m : 2.39625
[1mStep[0m  [72/84], [94mLoss[0m : 2.46778
[1mStep[0m  [80/84], [94mLoss[0m : 2.60015

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.315, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49483
[1mStep[0m  [8/84], [94mLoss[0m : 2.20629
[1mStep[0m  [16/84], [94mLoss[0m : 2.39779
[1mStep[0m  [24/84], [94mLoss[0m : 2.35385
[1mStep[0m  [32/84], [94mLoss[0m : 2.45742
[1mStep[0m  [40/84], [94mLoss[0m : 2.50314
[1mStep[0m  [48/84], [94mLoss[0m : 2.21941
[1mStep[0m  [56/84], [94mLoss[0m : 2.60352
[1mStep[0m  [64/84], [94mLoss[0m : 2.34414
[1mStep[0m  [72/84], [94mLoss[0m : 2.36046
[1mStep[0m  [80/84], [94mLoss[0m : 2.53765

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08451
[1mStep[0m  [8/84], [94mLoss[0m : 2.17676
[1mStep[0m  [16/84], [94mLoss[0m : 1.81697
[1mStep[0m  [24/84], [94mLoss[0m : 2.12933
[1mStep[0m  [32/84], [94mLoss[0m : 2.06165
[1mStep[0m  [40/84], [94mLoss[0m : 2.20848
[1mStep[0m  [48/84], [94mLoss[0m : 2.20252
[1mStep[0m  [56/84], [94mLoss[0m : 2.37386
[1mStep[0m  [64/84], [94mLoss[0m : 2.36041
[1mStep[0m  [72/84], [94mLoss[0m : 2.46753
[1mStep[0m  [80/84], [94mLoss[0m : 2.27251

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23375
[1mStep[0m  [8/84], [94mLoss[0m : 2.21855
[1mStep[0m  [16/84], [94mLoss[0m : 2.31329
[1mStep[0m  [24/84], [94mLoss[0m : 2.13454
[1mStep[0m  [32/84], [94mLoss[0m : 2.01155
[1mStep[0m  [40/84], [94mLoss[0m : 2.12306
[1mStep[0m  [48/84], [94mLoss[0m : 2.04977
[1mStep[0m  [56/84], [94mLoss[0m : 2.17782
[1mStep[0m  [64/84], [94mLoss[0m : 2.25142
[1mStep[0m  [72/84], [94mLoss[0m : 2.13574
[1mStep[0m  [80/84], [94mLoss[0m : 2.14935

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08321
[1mStep[0m  [8/84], [94mLoss[0m : 1.82992
[1mStep[0m  [16/84], [94mLoss[0m : 1.91008
[1mStep[0m  [24/84], [94mLoss[0m : 2.17772
[1mStep[0m  [32/84], [94mLoss[0m : 2.14995
[1mStep[0m  [40/84], [94mLoss[0m : 1.94225
[1mStep[0m  [48/84], [94mLoss[0m : 2.06290
[1mStep[0m  [56/84], [94mLoss[0m : 2.12774
[1mStep[0m  [64/84], [94mLoss[0m : 2.02485
[1mStep[0m  [72/84], [94mLoss[0m : 2.21218
[1mStep[0m  [80/84], [94mLoss[0m : 2.04090

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03304
[1mStep[0m  [8/84], [94mLoss[0m : 2.06497
[1mStep[0m  [16/84], [94mLoss[0m : 1.78388
[1mStep[0m  [24/84], [94mLoss[0m : 1.91362
[1mStep[0m  [32/84], [94mLoss[0m : 1.94230
[1mStep[0m  [40/84], [94mLoss[0m : 2.00401
[1mStep[0m  [48/84], [94mLoss[0m : 2.22804
[1mStep[0m  [56/84], [94mLoss[0m : 1.95011
[1mStep[0m  [64/84], [94mLoss[0m : 2.10379
[1mStep[0m  [72/84], [94mLoss[0m : 2.18028
[1mStep[0m  [80/84], [94mLoss[0m : 1.86805

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57428
[1mStep[0m  [8/84], [94mLoss[0m : 1.88344
[1mStep[0m  [16/84], [94mLoss[0m : 1.83507
[1mStep[0m  [24/84], [94mLoss[0m : 1.87326
[1mStep[0m  [32/84], [94mLoss[0m : 1.92135
[1mStep[0m  [40/84], [94mLoss[0m : 1.78897
[1mStep[0m  [48/84], [94mLoss[0m : 1.86619
[1mStep[0m  [56/84], [94mLoss[0m : 2.03931
[1mStep[0m  [64/84], [94mLoss[0m : 2.09408
[1mStep[0m  [72/84], [94mLoss[0m : 1.99370
[1mStep[0m  [80/84], [94mLoss[0m : 2.09971

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.955, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83102
[1mStep[0m  [8/84], [94mLoss[0m : 1.99268
[1mStep[0m  [16/84], [94mLoss[0m : 1.96583
[1mStep[0m  [24/84], [94mLoss[0m : 1.98302
[1mStep[0m  [32/84], [94mLoss[0m : 1.90382
[1mStep[0m  [40/84], [94mLoss[0m : 1.89544
[1mStep[0m  [48/84], [94mLoss[0m : 2.03827
[1mStep[0m  [56/84], [94mLoss[0m : 2.00940
[1mStep[0m  [64/84], [94mLoss[0m : 1.87148
[1mStep[0m  [72/84], [94mLoss[0m : 1.84617
[1mStep[0m  [80/84], [94mLoss[0m : 1.91874

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08013
[1mStep[0m  [8/84], [94mLoss[0m : 2.06081
[1mStep[0m  [16/84], [94mLoss[0m : 2.26441
[1mStep[0m  [24/84], [94mLoss[0m : 1.90590
[1mStep[0m  [32/84], [94mLoss[0m : 1.63898
[1mStep[0m  [40/84], [94mLoss[0m : 1.95966
[1mStep[0m  [48/84], [94mLoss[0m : 1.87080
[1mStep[0m  [56/84], [94mLoss[0m : 1.72357
[1mStep[0m  [64/84], [94mLoss[0m : 2.05620
[1mStep[0m  [72/84], [94mLoss[0m : 1.86026
[1mStep[0m  [80/84], [94mLoss[0m : 2.05096

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99272
[1mStep[0m  [8/84], [94mLoss[0m : 1.84989
[1mStep[0m  [16/84], [94mLoss[0m : 1.65897
[1mStep[0m  [24/84], [94mLoss[0m : 1.67960
[1mStep[0m  [32/84], [94mLoss[0m : 1.73819
[1mStep[0m  [40/84], [94mLoss[0m : 1.75164
[1mStep[0m  [48/84], [94mLoss[0m : 1.46455
[1mStep[0m  [56/84], [94mLoss[0m : 1.44928
[1mStep[0m  [64/84], [94mLoss[0m : 1.95483
[1mStep[0m  [72/84], [94mLoss[0m : 1.73474
[1mStep[0m  [80/84], [94mLoss[0m : 1.57486

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.564, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55549
[1mStep[0m  [8/84], [94mLoss[0m : 1.82304
[1mStep[0m  [16/84], [94mLoss[0m : 1.74107
[1mStep[0m  [24/84], [94mLoss[0m : 1.69443
[1mStep[0m  [32/84], [94mLoss[0m : 1.70263
[1mStep[0m  [40/84], [94mLoss[0m : 1.73524
[1mStep[0m  [48/84], [94mLoss[0m : 1.58504
[1mStep[0m  [56/84], [94mLoss[0m : 1.82870
[1mStep[0m  [64/84], [94mLoss[0m : 2.19245
[1mStep[0m  [72/84], [94mLoss[0m : 1.83425
[1mStep[0m  [80/84], [94mLoss[0m : 1.91200

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.756, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64849
[1mStep[0m  [8/84], [94mLoss[0m : 2.05441
[1mStep[0m  [16/84], [94mLoss[0m : 1.61113
[1mStep[0m  [24/84], [94mLoss[0m : 1.86673
[1mStep[0m  [32/84], [94mLoss[0m : 1.75152
[1mStep[0m  [40/84], [94mLoss[0m : 1.86190
[1mStep[0m  [48/84], [94mLoss[0m : 1.54186
[1mStep[0m  [56/84], [94mLoss[0m : 1.59970
[1mStep[0m  [64/84], [94mLoss[0m : 1.65983
[1mStep[0m  [72/84], [94mLoss[0m : 1.72010
[1mStep[0m  [80/84], [94mLoss[0m : 1.90182

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.518, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66003
[1mStep[0m  [8/84], [94mLoss[0m : 1.61458
[1mStep[0m  [16/84], [94mLoss[0m : 1.92296
[1mStep[0m  [24/84], [94mLoss[0m : 1.62069
[1mStep[0m  [32/84], [94mLoss[0m : 1.41447
[1mStep[0m  [40/84], [94mLoss[0m : 1.71527
[1mStep[0m  [48/84], [94mLoss[0m : 1.48470
[1mStep[0m  [56/84], [94mLoss[0m : 1.69806
[1mStep[0m  [64/84], [94mLoss[0m : 1.63030
[1mStep[0m  [72/84], [94mLoss[0m : 1.83910
[1mStep[0m  [80/84], [94mLoss[0m : 1.72915

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44733
[1mStep[0m  [8/84], [94mLoss[0m : 1.60160
[1mStep[0m  [16/84], [94mLoss[0m : 1.65725
[1mStep[0m  [24/84], [94mLoss[0m : 1.70278
[1mStep[0m  [32/84], [94mLoss[0m : 1.58717
[1mStep[0m  [40/84], [94mLoss[0m : 1.69073
[1mStep[0m  [48/84], [94mLoss[0m : 1.92213
[1mStep[0m  [56/84], [94mLoss[0m : 1.79582
[1mStep[0m  [64/84], [94mLoss[0m : 1.94744
[1mStep[0m  [72/84], [94mLoss[0m : 2.10083
[1mStep[0m  [80/84], [94mLoss[0m : 1.98815

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.528, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.35497
[1mStep[0m  [8/84], [94mLoss[0m : 1.95640
[1mStep[0m  [16/84], [94mLoss[0m : 1.43223
[1mStep[0m  [24/84], [94mLoss[0m : 1.54861
[1mStep[0m  [32/84], [94mLoss[0m : 1.84943
[1mStep[0m  [40/84], [94mLoss[0m : 1.57123
[1mStep[0m  [48/84], [94mLoss[0m : 1.51405
[1mStep[0m  [56/84], [94mLoss[0m : 1.75655
[1mStep[0m  [64/84], [94mLoss[0m : 1.68459
[1mStep[0m  [72/84], [94mLoss[0m : 1.91003
[1mStep[0m  [80/84], [94mLoss[0m : 1.75282

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.609, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64049
[1mStep[0m  [8/84], [94mLoss[0m : 1.38992
[1mStep[0m  [16/84], [94mLoss[0m : 1.55336
[1mStep[0m  [24/84], [94mLoss[0m : 1.36132
[1mStep[0m  [32/84], [94mLoss[0m : 1.58138
[1mStep[0m  [40/84], [94mLoss[0m : 1.63945
[1mStep[0m  [48/84], [94mLoss[0m : 1.69644
[1mStep[0m  [56/84], [94mLoss[0m : 1.61352
[1mStep[0m  [64/84], [94mLoss[0m : 1.59880
[1mStep[0m  [72/84], [94mLoss[0m : 1.59721
[1mStep[0m  [80/84], [94mLoss[0m : 2.01741

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46325
[1mStep[0m  [8/84], [94mLoss[0m : 1.66708
[1mStep[0m  [16/84], [94mLoss[0m : 1.57383
[1mStep[0m  [24/84], [94mLoss[0m : 1.58732
[1mStep[0m  [32/84], [94mLoss[0m : 1.72518
[1mStep[0m  [40/84], [94mLoss[0m : 1.70212
[1mStep[0m  [48/84], [94mLoss[0m : 1.68126
[1mStep[0m  [56/84], [94mLoss[0m : 1.70762
[1mStep[0m  [64/84], [94mLoss[0m : 1.72481
[1mStep[0m  [72/84], [94mLoss[0m : 1.52675
[1mStep[0m  [80/84], [94mLoss[0m : 1.62814

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55406
[1mStep[0m  [8/84], [94mLoss[0m : 1.43302
[1mStep[0m  [16/84], [94mLoss[0m : 1.61833
[1mStep[0m  [24/84], [94mLoss[0m : 1.39815
[1mStep[0m  [32/84], [94mLoss[0m : 1.69756
[1mStep[0m  [40/84], [94mLoss[0m : 1.47582
[1mStep[0m  [48/84], [94mLoss[0m : 1.53424
[1mStep[0m  [56/84], [94mLoss[0m : 1.61288
[1mStep[0m  [64/84], [94mLoss[0m : 1.72941
[1mStep[0m  [72/84], [94mLoss[0m : 1.41915
[1mStep[0m  [80/84], [94mLoss[0m : 1.88952

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52708
[1mStep[0m  [8/84], [94mLoss[0m : 1.37423
[1mStep[0m  [16/84], [94mLoss[0m : 1.51255
[1mStep[0m  [24/84], [94mLoss[0m : 1.46445
[1mStep[0m  [32/84], [94mLoss[0m : 1.24625
[1mStep[0m  [40/84], [94mLoss[0m : 1.80346
[1mStep[0m  [48/84], [94mLoss[0m : 1.60280
[1mStep[0m  [56/84], [94mLoss[0m : 1.53567
[1mStep[0m  [64/84], [94mLoss[0m : 1.63572
[1mStep[0m  [72/84], [94mLoss[0m : 1.49010
[1mStep[0m  [80/84], [94mLoss[0m : 1.45300

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.508, [92mTest[0m: 2.781, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28592
[1mStep[0m  [8/84], [94mLoss[0m : 1.62492
[1mStep[0m  [16/84], [94mLoss[0m : 1.69881
[1mStep[0m  [24/84], [94mLoss[0m : 1.43938
[1mStep[0m  [32/84], [94mLoss[0m : 1.28175
[1mStep[0m  [40/84], [94mLoss[0m : 1.32673
[1mStep[0m  [48/84], [94mLoss[0m : 1.30703
[1mStep[0m  [56/84], [94mLoss[0m : 1.40620
[1mStep[0m  [64/84], [94mLoss[0m : 1.52200
[1mStep[0m  [72/84], [94mLoss[0m : 1.40926
[1mStep[0m  [80/84], [94mLoss[0m : 1.39553

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.514, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40045
[1mStep[0m  [8/84], [94mLoss[0m : 1.25394
[1mStep[0m  [16/84], [94mLoss[0m : 1.95397
[1mStep[0m  [24/84], [94mLoss[0m : 1.51225
[1mStep[0m  [32/84], [94mLoss[0m : 1.35028
[1mStep[0m  [40/84], [94mLoss[0m : 1.41171
[1mStep[0m  [48/84], [94mLoss[0m : 1.42112
[1mStep[0m  [56/84], [94mLoss[0m : 1.72701
[1mStep[0m  [64/84], [94mLoss[0m : 1.50949
[1mStep[0m  [72/84], [94mLoss[0m : 1.62294
[1mStep[0m  [80/84], [94mLoss[0m : 1.56764

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.426, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49943
[1mStep[0m  [8/84], [94mLoss[0m : 1.35242
[1mStep[0m  [16/84], [94mLoss[0m : 1.30563
[1mStep[0m  [24/84], [94mLoss[0m : 1.38068
[1mStep[0m  [32/84], [94mLoss[0m : 1.31411
[1mStep[0m  [40/84], [94mLoss[0m : 1.36693
[1mStep[0m  [48/84], [94mLoss[0m : 1.38893
[1mStep[0m  [56/84], [94mLoss[0m : 1.45398
[1mStep[0m  [64/84], [94mLoss[0m : 1.41227
[1mStep[0m  [72/84], [94mLoss[0m : 1.59940
[1mStep[0m  [80/84], [94mLoss[0m : 1.37855

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.394, [92mTest[0m: 2.547, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42158
[1mStep[0m  [8/84], [94mLoss[0m : 1.25229
[1mStep[0m  [16/84], [94mLoss[0m : 1.39321
[1mStep[0m  [24/84], [94mLoss[0m : 1.42409
[1mStep[0m  [32/84], [94mLoss[0m : 1.37734
[1mStep[0m  [40/84], [94mLoss[0m : 1.65353
[1mStep[0m  [48/84], [94mLoss[0m : 1.45671
[1mStep[0m  [56/84], [94mLoss[0m : 1.43335
[1mStep[0m  [64/84], [94mLoss[0m : 1.44544
[1mStep[0m  [72/84], [94mLoss[0m : 1.46990
[1mStep[0m  [80/84], [94mLoss[0m : 1.67113

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.415, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.522
====================================

Phase 2 - Evaluation MAE:  2.522053965500423
MAE score P1      2.317977
MAE score P2      2.522054
loss              1.394196
learning_rate         0.01
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 26, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.89110
[1mStep[0m  [4/42], [94mLoss[0m : 11.02322
[1mStep[0m  [8/42], [94mLoss[0m : 10.50766
[1mStep[0m  [12/42], [94mLoss[0m : 10.00958
[1mStep[0m  [16/42], [94mLoss[0m : 9.54844
[1mStep[0m  [20/42], [94mLoss[0m : 8.83936
[1mStep[0m  [24/42], [94mLoss[0m : 8.02062
[1mStep[0m  [28/42], [94mLoss[0m : 7.46651
[1mStep[0m  [32/42], [94mLoss[0m : 6.41228
[1mStep[0m  [36/42], [94mLoss[0m : 6.26556
[1mStep[0m  [40/42], [94mLoss[0m : 5.93740

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.634, [92mTest[0m: 10.802, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.09542
[1mStep[0m  [4/42], [94mLoss[0m : 4.86158
[1mStep[0m  [8/42], [94mLoss[0m : 4.55862
[1mStep[0m  [12/42], [94mLoss[0m : 3.97793
[1mStep[0m  [16/42], [94mLoss[0m : 3.65808
[1mStep[0m  [20/42], [94mLoss[0m : 3.28210
[1mStep[0m  [24/42], [94mLoss[0m : 2.66146
[1mStep[0m  [28/42], [94mLoss[0m : 2.83273
[1mStep[0m  [32/42], [94mLoss[0m : 2.85056
[1mStep[0m  [36/42], [94mLoss[0m : 2.74749
[1mStep[0m  [40/42], [94mLoss[0m : 2.79753

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.557, [92mTest[0m: 5.792, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72231
[1mStep[0m  [4/42], [94mLoss[0m : 2.58752
[1mStep[0m  [8/42], [94mLoss[0m : 2.71252
[1mStep[0m  [12/42], [94mLoss[0m : 2.97534
[1mStep[0m  [16/42], [94mLoss[0m : 2.58044
[1mStep[0m  [20/42], [94mLoss[0m : 2.67542
[1mStep[0m  [24/42], [94mLoss[0m : 2.74879
[1mStep[0m  [28/42], [94mLoss[0m : 2.67440
[1mStep[0m  [32/42], [94mLoss[0m : 2.78780
[1mStep[0m  [36/42], [94mLoss[0m : 2.52670
[1mStep[0m  [40/42], [94mLoss[0m : 2.52359

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.529, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67688
[1mStep[0m  [4/42], [94mLoss[0m : 2.67868
[1mStep[0m  [8/42], [94mLoss[0m : 2.59941
[1mStep[0m  [12/42], [94mLoss[0m : 2.46218
[1mStep[0m  [16/42], [94mLoss[0m : 2.86810
[1mStep[0m  [20/42], [94mLoss[0m : 2.54700
[1mStep[0m  [24/42], [94mLoss[0m : 2.46985
[1mStep[0m  [28/42], [94mLoss[0m : 2.27316
[1mStep[0m  [32/42], [94mLoss[0m : 2.50344
[1mStep[0m  [36/42], [94mLoss[0m : 2.58492
[1mStep[0m  [40/42], [94mLoss[0m : 2.45932

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60017
[1mStep[0m  [4/42], [94mLoss[0m : 2.35162
[1mStep[0m  [8/42], [94mLoss[0m : 2.39524
[1mStep[0m  [12/42], [94mLoss[0m : 2.38656
[1mStep[0m  [16/42], [94mLoss[0m : 2.52419
[1mStep[0m  [20/42], [94mLoss[0m : 2.52324
[1mStep[0m  [24/42], [94mLoss[0m : 2.62556
[1mStep[0m  [28/42], [94mLoss[0m : 2.42628
[1mStep[0m  [32/42], [94mLoss[0m : 2.62347
[1mStep[0m  [36/42], [94mLoss[0m : 2.43754
[1mStep[0m  [40/42], [94mLoss[0m : 2.43204

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61540
[1mStep[0m  [4/42], [94mLoss[0m : 2.61349
[1mStep[0m  [8/42], [94mLoss[0m : 2.45336
[1mStep[0m  [12/42], [94mLoss[0m : 2.33715
[1mStep[0m  [16/42], [94mLoss[0m : 2.75657
[1mStep[0m  [20/42], [94mLoss[0m : 2.51997
[1mStep[0m  [24/42], [94mLoss[0m : 2.61448
[1mStep[0m  [28/42], [94mLoss[0m : 2.24727
[1mStep[0m  [32/42], [94mLoss[0m : 2.59639
[1mStep[0m  [36/42], [94mLoss[0m : 2.27780
[1mStep[0m  [40/42], [94mLoss[0m : 2.63271

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51514
[1mStep[0m  [4/42], [94mLoss[0m : 2.56980
[1mStep[0m  [8/42], [94mLoss[0m : 2.43387
[1mStep[0m  [12/42], [94mLoss[0m : 2.46133
[1mStep[0m  [16/42], [94mLoss[0m : 2.54480
[1mStep[0m  [20/42], [94mLoss[0m : 2.36793
[1mStep[0m  [24/42], [94mLoss[0m : 2.41299
[1mStep[0m  [28/42], [94mLoss[0m : 2.55991
[1mStep[0m  [32/42], [94mLoss[0m : 2.52134
[1mStep[0m  [36/42], [94mLoss[0m : 2.31442
[1mStep[0m  [40/42], [94mLoss[0m : 2.47118

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26127
[1mStep[0m  [4/42], [94mLoss[0m : 2.58678
[1mStep[0m  [8/42], [94mLoss[0m : 2.32046
[1mStep[0m  [12/42], [94mLoss[0m : 2.51511
[1mStep[0m  [16/42], [94mLoss[0m : 2.37146
[1mStep[0m  [20/42], [94mLoss[0m : 2.38000
[1mStep[0m  [24/42], [94mLoss[0m : 2.39034
[1mStep[0m  [28/42], [94mLoss[0m : 2.68259
[1mStep[0m  [32/42], [94mLoss[0m : 2.39660
[1mStep[0m  [36/42], [94mLoss[0m : 2.42932
[1mStep[0m  [40/42], [94mLoss[0m : 2.71238

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50749
[1mStep[0m  [4/42], [94mLoss[0m : 2.17177
[1mStep[0m  [8/42], [94mLoss[0m : 2.53046
[1mStep[0m  [12/42], [94mLoss[0m : 2.63446
[1mStep[0m  [16/42], [94mLoss[0m : 2.28740
[1mStep[0m  [20/42], [94mLoss[0m : 2.51234
[1mStep[0m  [24/42], [94mLoss[0m : 2.44647
[1mStep[0m  [28/42], [94mLoss[0m : 2.37802
[1mStep[0m  [32/42], [94mLoss[0m : 2.34194
[1mStep[0m  [36/42], [94mLoss[0m : 2.57151
[1mStep[0m  [40/42], [94mLoss[0m : 2.36761

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47953
[1mStep[0m  [4/42], [94mLoss[0m : 2.50171
[1mStep[0m  [8/42], [94mLoss[0m : 2.65986
[1mStep[0m  [12/42], [94mLoss[0m : 2.43585
[1mStep[0m  [16/42], [94mLoss[0m : 2.55732
[1mStep[0m  [20/42], [94mLoss[0m : 2.56885
[1mStep[0m  [24/42], [94mLoss[0m : 2.38975
[1mStep[0m  [28/42], [94mLoss[0m : 2.31340
[1mStep[0m  [32/42], [94mLoss[0m : 2.31113
[1mStep[0m  [36/42], [94mLoss[0m : 2.42596
[1mStep[0m  [40/42], [94mLoss[0m : 2.76810

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28624
[1mStep[0m  [4/42], [94mLoss[0m : 2.57367
[1mStep[0m  [8/42], [94mLoss[0m : 2.52301
[1mStep[0m  [12/42], [94mLoss[0m : 2.40971
[1mStep[0m  [16/42], [94mLoss[0m : 2.46184
[1mStep[0m  [20/42], [94mLoss[0m : 2.10639
[1mStep[0m  [24/42], [94mLoss[0m : 2.38889
[1mStep[0m  [28/42], [94mLoss[0m : 2.51682
[1mStep[0m  [32/42], [94mLoss[0m : 2.30985
[1mStep[0m  [36/42], [94mLoss[0m : 2.61489
[1mStep[0m  [40/42], [94mLoss[0m : 2.53349

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45879
[1mStep[0m  [4/42], [94mLoss[0m : 2.27468
[1mStep[0m  [8/42], [94mLoss[0m : 2.26323
[1mStep[0m  [12/42], [94mLoss[0m : 2.34316
[1mStep[0m  [16/42], [94mLoss[0m : 2.41488
[1mStep[0m  [20/42], [94mLoss[0m : 2.50734
[1mStep[0m  [24/42], [94mLoss[0m : 2.57800
[1mStep[0m  [28/42], [94mLoss[0m : 2.42454
[1mStep[0m  [32/42], [94mLoss[0m : 2.49944
[1mStep[0m  [36/42], [94mLoss[0m : 2.19772
[1mStep[0m  [40/42], [94mLoss[0m : 2.49955

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35893
[1mStep[0m  [4/42], [94mLoss[0m : 2.43388
[1mStep[0m  [8/42], [94mLoss[0m : 2.23133
[1mStep[0m  [12/42], [94mLoss[0m : 2.37987
[1mStep[0m  [16/42], [94mLoss[0m : 2.62716
[1mStep[0m  [20/42], [94mLoss[0m : 2.33113
[1mStep[0m  [24/42], [94mLoss[0m : 2.58041
[1mStep[0m  [28/42], [94mLoss[0m : 2.23757
[1mStep[0m  [32/42], [94mLoss[0m : 2.32166
[1mStep[0m  [36/42], [94mLoss[0m : 2.55595
[1mStep[0m  [40/42], [94mLoss[0m : 2.30368

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63956
[1mStep[0m  [4/42], [94mLoss[0m : 2.45180
[1mStep[0m  [8/42], [94mLoss[0m : 2.43269
[1mStep[0m  [12/42], [94mLoss[0m : 2.28707
[1mStep[0m  [16/42], [94mLoss[0m : 2.54141
[1mStep[0m  [20/42], [94mLoss[0m : 2.29252
[1mStep[0m  [24/42], [94mLoss[0m : 2.54286
[1mStep[0m  [28/42], [94mLoss[0m : 2.25676
[1mStep[0m  [32/42], [94mLoss[0m : 2.39745
[1mStep[0m  [36/42], [94mLoss[0m : 2.45718
[1mStep[0m  [40/42], [94mLoss[0m : 2.51075

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29432
[1mStep[0m  [4/42], [94mLoss[0m : 2.23451
[1mStep[0m  [8/42], [94mLoss[0m : 2.46195
[1mStep[0m  [12/42], [94mLoss[0m : 2.64583
[1mStep[0m  [16/42], [94mLoss[0m : 2.35287
[1mStep[0m  [20/42], [94mLoss[0m : 2.37397
[1mStep[0m  [24/42], [94mLoss[0m : 2.69208
[1mStep[0m  [28/42], [94mLoss[0m : 2.43432
[1mStep[0m  [32/42], [94mLoss[0m : 2.39083
[1mStep[0m  [36/42], [94mLoss[0m : 2.38467
[1mStep[0m  [40/42], [94mLoss[0m : 2.46481

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31622
[1mStep[0m  [4/42], [94mLoss[0m : 2.46092
[1mStep[0m  [8/42], [94mLoss[0m : 2.40863
[1mStep[0m  [12/42], [94mLoss[0m : 2.43503
[1mStep[0m  [16/42], [94mLoss[0m : 2.23834
[1mStep[0m  [20/42], [94mLoss[0m : 2.40974
[1mStep[0m  [24/42], [94mLoss[0m : 2.33301
[1mStep[0m  [28/42], [94mLoss[0m : 2.56638
[1mStep[0m  [32/42], [94mLoss[0m : 2.49415
[1mStep[0m  [36/42], [94mLoss[0m : 2.48201
[1mStep[0m  [40/42], [94mLoss[0m : 2.48313

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55113
[1mStep[0m  [4/42], [94mLoss[0m : 2.39660
[1mStep[0m  [8/42], [94mLoss[0m : 2.46213
[1mStep[0m  [12/42], [94mLoss[0m : 2.31176
[1mStep[0m  [16/42], [94mLoss[0m : 2.46572
[1mStep[0m  [20/42], [94mLoss[0m : 2.51974
[1mStep[0m  [24/42], [94mLoss[0m : 2.20286
[1mStep[0m  [28/42], [94mLoss[0m : 2.25363
[1mStep[0m  [32/42], [94mLoss[0m : 2.29922
[1mStep[0m  [36/42], [94mLoss[0m : 2.44746
[1mStep[0m  [40/42], [94mLoss[0m : 2.25764

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69074
[1mStep[0m  [4/42], [94mLoss[0m : 2.38552
[1mStep[0m  [8/42], [94mLoss[0m : 2.30912
[1mStep[0m  [12/42], [94mLoss[0m : 2.27647
[1mStep[0m  [16/42], [94mLoss[0m : 2.47585
[1mStep[0m  [20/42], [94mLoss[0m : 2.37477
[1mStep[0m  [24/42], [94mLoss[0m : 2.32404
[1mStep[0m  [28/42], [94mLoss[0m : 2.30453
[1mStep[0m  [32/42], [94mLoss[0m : 2.33984
[1mStep[0m  [36/42], [94mLoss[0m : 2.35269
[1mStep[0m  [40/42], [94mLoss[0m : 2.30198

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32947
[1mStep[0m  [4/42], [94mLoss[0m : 2.72078
[1mStep[0m  [8/42], [94mLoss[0m : 2.51122
[1mStep[0m  [12/42], [94mLoss[0m : 2.42719
[1mStep[0m  [16/42], [94mLoss[0m : 2.36897
[1mStep[0m  [20/42], [94mLoss[0m : 2.45636
[1mStep[0m  [24/42], [94mLoss[0m : 2.31083
[1mStep[0m  [28/42], [94mLoss[0m : 2.26362
[1mStep[0m  [32/42], [94mLoss[0m : 2.43417
[1mStep[0m  [36/42], [94mLoss[0m : 2.24240
[1mStep[0m  [40/42], [94mLoss[0m : 2.54853

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44730
[1mStep[0m  [4/42], [94mLoss[0m : 2.31770
[1mStep[0m  [8/42], [94mLoss[0m : 2.54934
[1mStep[0m  [12/42], [94mLoss[0m : 2.26734
[1mStep[0m  [16/42], [94mLoss[0m : 2.53544
[1mStep[0m  [20/42], [94mLoss[0m : 2.70218
[1mStep[0m  [24/42], [94mLoss[0m : 2.29603
[1mStep[0m  [28/42], [94mLoss[0m : 2.41208
[1mStep[0m  [32/42], [94mLoss[0m : 2.41940
[1mStep[0m  [36/42], [94mLoss[0m : 2.46687
[1mStep[0m  [40/42], [94mLoss[0m : 2.14039

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49703
[1mStep[0m  [4/42], [94mLoss[0m : 2.35562
[1mStep[0m  [8/42], [94mLoss[0m : 2.47356
[1mStep[0m  [12/42], [94mLoss[0m : 2.36996
[1mStep[0m  [16/42], [94mLoss[0m : 2.41902
[1mStep[0m  [20/42], [94mLoss[0m : 2.45485
[1mStep[0m  [24/42], [94mLoss[0m : 2.33484
[1mStep[0m  [28/42], [94mLoss[0m : 2.52929
[1mStep[0m  [32/42], [94mLoss[0m : 2.24648
[1mStep[0m  [36/42], [94mLoss[0m : 2.51020
[1mStep[0m  [40/42], [94mLoss[0m : 2.39432

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55824
[1mStep[0m  [4/42], [94mLoss[0m : 2.10721
[1mStep[0m  [8/42], [94mLoss[0m : 2.28693
[1mStep[0m  [12/42], [94mLoss[0m : 2.41436
[1mStep[0m  [16/42], [94mLoss[0m : 2.37546
[1mStep[0m  [20/42], [94mLoss[0m : 2.41146
[1mStep[0m  [24/42], [94mLoss[0m : 2.02510
[1mStep[0m  [28/42], [94mLoss[0m : 2.27471
[1mStep[0m  [32/42], [94mLoss[0m : 2.13423
[1mStep[0m  [36/42], [94mLoss[0m : 2.38729
[1mStep[0m  [40/42], [94mLoss[0m : 2.42312

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22067
[1mStep[0m  [4/42], [94mLoss[0m : 2.35912
[1mStep[0m  [8/42], [94mLoss[0m : 2.53510
[1mStep[0m  [12/42], [94mLoss[0m : 2.40157
[1mStep[0m  [16/42], [94mLoss[0m : 2.52052
[1mStep[0m  [20/42], [94mLoss[0m : 2.51997
[1mStep[0m  [24/42], [94mLoss[0m : 2.60685
[1mStep[0m  [28/42], [94mLoss[0m : 2.35052
[1mStep[0m  [32/42], [94mLoss[0m : 2.43139
[1mStep[0m  [36/42], [94mLoss[0m : 2.33673
[1mStep[0m  [40/42], [94mLoss[0m : 1.97046

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49524
[1mStep[0m  [4/42], [94mLoss[0m : 2.23278
[1mStep[0m  [8/42], [94mLoss[0m : 2.42664
[1mStep[0m  [12/42], [94mLoss[0m : 2.30688
[1mStep[0m  [16/42], [94mLoss[0m : 2.34250
[1mStep[0m  [20/42], [94mLoss[0m : 2.35005
[1mStep[0m  [24/42], [94mLoss[0m : 2.34356
[1mStep[0m  [28/42], [94mLoss[0m : 2.27319
[1mStep[0m  [32/42], [94mLoss[0m : 2.23480
[1mStep[0m  [36/42], [94mLoss[0m : 2.58707
[1mStep[0m  [40/42], [94mLoss[0m : 2.18092

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13153
[1mStep[0m  [4/42], [94mLoss[0m : 2.42516
[1mStep[0m  [8/42], [94mLoss[0m : 2.28856
[1mStep[0m  [12/42], [94mLoss[0m : 2.48014
[1mStep[0m  [16/42], [94mLoss[0m : 2.44147
[1mStep[0m  [20/42], [94mLoss[0m : 2.47222
[1mStep[0m  [24/42], [94mLoss[0m : 2.36234
[1mStep[0m  [28/42], [94mLoss[0m : 2.50978
[1mStep[0m  [32/42], [94mLoss[0m : 2.30189
[1mStep[0m  [36/42], [94mLoss[0m : 2.20889
[1mStep[0m  [40/42], [94mLoss[0m : 2.36760

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22748
[1mStep[0m  [4/42], [94mLoss[0m : 2.39959
[1mStep[0m  [8/42], [94mLoss[0m : 2.49937
[1mStep[0m  [12/42], [94mLoss[0m : 2.28464
[1mStep[0m  [16/42], [94mLoss[0m : 2.29410
[1mStep[0m  [20/42], [94mLoss[0m : 2.56683
[1mStep[0m  [24/42], [94mLoss[0m : 2.26337
[1mStep[0m  [28/42], [94mLoss[0m : 2.21144
[1mStep[0m  [32/42], [94mLoss[0m : 2.35883
[1mStep[0m  [36/42], [94mLoss[0m : 2.27800
[1mStep[0m  [40/42], [94mLoss[0m : 2.45590

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40654
[1mStep[0m  [4/42], [94mLoss[0m : 2.41370
[1mStep[0m  [8/42], [94mLoss[0m : 2.43460
[1mStep[0m  [12/42], [94mLoss[0m : 2.42775
[1mStep[0m  [16/42], [94mLoss[0m : 2.41984
[1mStep[0m  [20/42], [94mLoss[0m : 2.34947
[1mStep[0m  [24/42], [94mLoss[0m : 2.36584
[1mStep[0m  [28/42], [94mLoss[0m : 2.29875
[1mStep[0m  [32/42], [94mLoss[0m : 2.43001
[1mStep[0m  [36/42], [94mLoss[0m : 2.43195
[1mStep[0m  [40/42], [94mLoss[0m : 2.47642

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21993
[1mStep[0m  [4/42], [94mLoss[0m : 2.52978
[1mStep[0m  [8/42], [94mLoss[0m : 2.41550
[1mStep[0m  [12/42], [94mLoss[0m : 2.26846
[1mStep[0m  [16/42], [94mLoss[0m : 2.32596
[1mStep[0m  [20/42], [94mLoss[0m : 2.28007
[1mStep[0m  [24/42], [94mLoss[0m : 2.43533
[1mStep[0m  [28/42], [94mLoss[0m : 2.27063
[1mStep[0m  [32/42], [94mLoss[0m : 2.27412
[1mStep[0m  [36/42], [94mLoss[0m : 2.46157
[1mStep[0m  [40/42], [94mLoss[0m : 2.47653

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19146
[1mStep[0m  [4/42], [94mLoss[0m : 2.29050
[1mStep[0m  [8/42], [94mLoss[0m : 2.35442
[1mStep[0m  [12/42], [94mLoss[0m : 2.39936
[1mStep[0m  [16/42], [94mLoss[0m : 2.12999
[1mStep[0m  [20/42], [94mLoss[0m : 2.56184
[1mStep[0m  [24/42], [94mLoss[0m : 2.23330
[1mStep[0m  [28/42], [94mLoss[0m : 2.31494
[1mStep[0m  [32/42], [94mLoss[0m : 2.37060
[1mStep[0m  [36/42], [94mLoss[0m : 2.31337
[1mStep[0m  [40/42], [94mLoss[0m : 2.27085

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30856
[1mStep[0m  [4/42], [94mLoss[0m : 2.28569
[1mStep[0m  [8/42], [94mLoss[0m : 2.42768
[1mStep[0m  [12/42], [94mLoss[0m : 2.33445
[1mStep[0m  [16/42], [94mLoss[0m : 2.48989
[1mStep[0m  [20/42], [94mLoss[0m : 2.37805
[1mStep[0m  [24/42], [94mLoss[0m : 2.39619
[1mStep[0m  [28/42], [94mLoss[0m : 2.37816
[1mStep[0m  [32/42], [94mLoss[0m : 2.31786
[1mStep[0m  [36/42], [94mLoss[0m : 2.23163
[1mStep[0m  [40/42], [94mLoss[0m : 2.49465

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.33100380216326
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.44290
[1mStep[0m  [4/42], [94mLoss[0m : 2.48267
[1mStep[0m  [8/42], [94mLoss[0m : 2.42183
[1mStep[0m  [12/42], [94mLoss[0m : 2.60982
[1mStep[0m  [16/42], [94mLoss[0m : 2.35753
[1mStep[0m  [20/42], [94mLoss[0m : 2.57976
[1mStep[0m  [24/42], [94mLoss[0m : 2.43101
[1mStep[0m  [28/42], [94mLoss[0m : 2.54075
[1mStep[0m  [32/42], [94mLoss[0m : 2.23377
[1mStep[0m  [36/42], [94mLoss[0m : 2.39370
[1mStep[0m  [40/42], [94mLoss[0m : 2.20775

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50480
[1mStep[0m  [4/42], [94mLoss[0m : 2.05770
[1mStep[0m  [8/42], [94mLoss[0m : 2.14622
[1mStep[0m  [12/42], [94mLoss[0m : 2.19184
[1mStep[0m  [16/42], [94mLoss[0m : 2.05346
[1mStep[0m  [20/42], [94mLoss[0m : 2.37507
[1mStep[0m  [24/42], [94mLoss[0m : 2.13737
[1mStep[0m  [28/42], [94mLoss[0m : 2.11076
[1mStep[0m  [32/42], [94mLoss[0m : 2.37074
[1mStep[0m  [36/42], [94mLoss[0m : 2.41946
[1mStep[0m  [40/42], [94mLoss[0m : 2.23392

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26274
[1mStep[0m  [4/42], [94mLoss[0m : 2.10131
[1mStep[0m  [8/42], [94mLoss[0m : 2.24583
[1mStep[0m  [12/42], [94mLoss[0m : 2.44680
[1mStep[0m  [16/42], [94mLoss[0m : 2.35847
[1mStep[0m  [20/42], [94mLoss[0m : 2.27635
[1mStep[0m  [24/42], [94mLoss[0m : 2.21414
[1mStep[0m  [28/42], [94mLoss[0m : 2.08880
[1mStep[0m  [32/42], [94mLoss[0m : 2.47410
[1mStep[0m  [36/42], [94mLoss[0m : 2.19437
[1mStep[0m  [40/42], [94mLoss[0m : 1.89174

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89590
[1mStep[0m  [4/42], [94mLoss[0m : 2.10970
[1mStep[0m  [8/42], [94mLoss[0m : 2.05994
[1mStep[0m  [12/42], [94mLoss[0m : 1.99583
[1mStep[0m  [16/42], [94mLoss[0m : 2.24042
[1mStep[0m  [20/42], [94mLoss[0m : 2.15430
[1mStep[0m  [24/42], [94mLoss[0m : 2.16950
[1mStep[0m  [28/42], [94mLoss[0m : 2.10886
[1mStep[0m  [32/42], [94mLoss[0m : 2.13014
[1mStep[0m  [36/42], [94mLoss[0m : 2.11659
[1mStep[0m  [40/42], [94mLoss[0m : 2.08631

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02785
[1mStep[0m  [4/42], [94mLoss[0m : 1.88390
[1mStep[0m  [8/42], [94mLoss[0m : 1.96426
[1mStep[0m  [12/42], [94mLoss[0m : 1.83794
[1mStep[0m  [16/42], [94mLoss[0m : 2.09935
[1mStep[0m  [20/42], [94mLoss[0m : 1.97496
[1mStep[0m  [24/42], [94mLoss[0m : 2.17844
[1mStep[0m  [28/42], [94mLoss[0m : 2.01571
[1mStep[0m  [32/42], [94mLoss[0m : 1.99981
[1mStep[0m  [36/42], [94mLoss[0m : 2.03569
[1mStep[0m  [40/42], [94mLoss[0m : 2.13132

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.032, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77656
[1mStep[0m  [4/42], [94mLoss[0m : 1.77651
[1mStep[0m  [8/42], [94mLoss[0m : 1.91477
[1mStep[0m  [12/42], [94mLoss[0m : 1.85796
[1mStep[0m  [16/42], [94mLoss[0m : 1.99910
[1mStep[0m  [20/42], [94mLoss[0m : 1.99526
[1mStep[0m  [24/42], [94mLoss[0m : 2.15697
[1mStep[0m  [28/42], [94mLoss[0m : 1.95483
[1mStep[0m  [32/42], [94mLoss[0m : 1.89253
[1mStep[0m  [36/42], [94mLoss[0m : 2.12925
[1mStep[0m  [40/42], [94mLoss[0m : 1.95137

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.970, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73646
[1mStep[0m  [4/42], [94mLoss[0m : 1.85272
[1mStep[0m  [8/42], [94mLoss[0m : 1.73673
[1mStep[0m  [12/42], [94mLoss[0m : 2.13257
[1mStep[0m  [16/42], [94mLoss[0m : 1.90583
[1mStep[0m  [20/42], [94mLoss[0m : 2.02891
[1mStep[0m  [24/42], [94mLoss[0m : 1.81419
[1mStep[0m  [28/42], [94mLoss[0m : 1.98136
[1mStep[0m  [32/42], [94mLoss[0m : 1.89878
[1mStep[0m  [36/42], [94mLoss[0m : 1.80270
[1mStep[0m  [40/42], [94mLoss[0m : 1.93137

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73350
[1mStep[0m  [4/42], [94mLoss[0m : 1.76006
[1mStep[0m  [8/42], [94mLoss[0m : 1.81814
[1mStep[0m  [12/42], [94mLoss[0m : 1.69410
[1mStep[0m  [16/42], [94mLoss[0m : 1.76218
[1mStep[0m  [20/42], [94mLoss[0m : 1.81440
[1mStep[0m  [24/42], [94mLoss[0m : 1.85194
[1mStep[0m  [28/42], [94mLoss[0m : 1.92186
[1mStep[0m  [32/42], [94mLoss[0m : 1.97507
[1mStep[0m  [36/42], [94mLoss[0m : 2.03614
[1mStep[0m  [40/42], [94mLoss[0m : 1.95518

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66543
[1mStep[0m  [4/42], [94mLoss[0m : 1.62407
[1mStep[0m  [8/42], [94mLoss[0m : 1.70822
[1mStep[0m  [12/42], [94mLoss[0m : 1.76672
[1mStep[0m  [16/42], [94mLoss[0m : 1.87044
[1mStep[0m  [20/42], [94mLoss[0m : 1.98895
[1mStep[0m  [24/42], [94mLoss[0m : 1.89446
[1mStep[0m  [28/42], [94mLoss[0m : 1.87548
[1mStep[0m  [32/42], [94mLoss[0m : 1.74821
[1mStep[0m  [36/42], [94mLoss[0m : 1.71719
[1mStep[0m  [40/42], [94mLoss[0m : 1.90248

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.766, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57136
[1mStep[0m  [4/42], [94mLoss[0m : 1.55586
[1mStep[0m  [8/42], [94mLoss[0m : 1.69691
[1mStep[0m  [12/42], [94mLoss[0m : 1.84809
[1mStep[0m  [16/42], [94mLoss[0m : 1.80593
[1mStep[0m  [20/42], [94mLoss[0m : 1.72879
[1mStep[0m  [24/42], [94mLoss[0m : 1.54377
[1mStep[0m  [28/42], [94mLoss[0m : 1.73245
[1mStep[0m  [32/42], [94mLoss[0m : 1.73427
[1mStep[0m  [36/42], [94mLoss[0m : 1.82274
[1mStep[0m  [40/42], [94mLoss[0m : 1.88395

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75008
[1mStep[0m  [4/42], [94mLoss[0m : 1.41540
[1mStep[0m  [8/42], [94mLoss[0m : 1.55698
[1mStep[0m  [12/42], [94mLoss[0m : 1.48256
[1mStep[0m  [16/42], [94mLoss[0m : 1.40117
[1mStep[0m  [20/42], [94mLoss[0m : 1.57676
[1mStep[0m  [24/42], [94mLoss[0m : 1.64347
[1mStep[0m  [28/42], [94mLoss[0m : 1.61133
[1mStep[0m  [32/42], [94mLoss[0m : 1.83390
[1mStep[0m  [36/42], [94mLoss[0m : 1.69726
[1mStep[0m  [40/42], [94mLoss[0m : 1.71213

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.53875
[1mStep[0m  [4/42], [94mLoss[0m : 1.66501
[1mStep[0m  [8/42], [94mLoss[0m : 1.65172
[1mStep[0m  [12/42], [94mLoss[0m : 1.51047
[1mStep[0m  [16/42], [94mLoss[0m : 1.58811
[1mStep[0m  [20/42], [94mLoss[0m : 1.78139
[1mStep[0m  [24/42], [94mLoss[0m : 1.64825
[1mStep[0m  [28/42], [94mLoss[0m : 1.62642
[1mStep[0m  [32/42], [94mLoss[0m : 1.80809
[1mStep[0m  [36/42], [94mLoss[0m : 1.67684
[1mStep[0m  [40/42], [94mLoss[0m : 1.70495

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56939
[1mStep[0m  [4/42], [94mLoss[0m : 1.54741
[1mStep[0m  [8/42], [94mLoss[0m : 1.54323
[1mStep[0m  [12/42], [94mLoss[0m : 1.53486
[1mStep[0m  [16/42], [94mLoss[0m : 1.57301
[1mStep[0m  [20/42], [94mLoss[0m : 1.65968
[1mStep[0m  [24/42], [94mLoss[0m : 1.60999
[1mStep[0m  [28/42], [94mLoss[0m : 1.57676
[1mStep[0m  [32/42], [94mLoss[0m : 1.60241
[1mStep[0m  [36/42], [94mLoss[0m : 1.63590
[1mStep[0m  [40/42], [94mLoss[0m : 1.61945

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.578, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45176
[1mStep[0m  [4/42], [94mLoss[0m : 1.44297
[1mStep[0m  [8/42], [94mLoss[0m : 1.52008
[1mStep[0m  [12/42], [94mLoss[0m : 1.61807
[1mStep[0m  [16/42], [94mLoss[0m : 1.57398
[1mStep[0m  [20/42], [94mLoss[0m : 1.56643
[1mStep[0m  [24/42], [94mLoss[0m : 1.66990
[1mStep[0m  [28/42], [94mLoss[0m : 1.60167
[1mStep[0m  [32/42], [94mLoss[0m : 1.52168
[1mStep[0m  [36/42], [94mLoss[0m : 1.65143
[1mStep[0m  [40/42], [94mLoss[0m : 1.60854

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.489, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58393
[1mStep[0m  [4/42], [94mLoss[0m : 1.58466
[1mStep[0m  [8/42], [94mLoss[0m : 1.58308
[1mStep[0m  [12/42], [94mLoss[0m : 1.34969
[1mStep[0m  [16/42], [94mLoss[0m : 1.42148
[1mStep[0m  [20/42], [94mLoss[0m : 1.61881
[1mStep[0m  [24/42], [94mLoss[0m : 1.40660
[1mStep[0m  [28/42], [94mLoss[0m : 1.46679
[1mStep[0m  [32/42], [94mLoss[0m : 1.46170
[1mStep[0m  [36/42], [94mLoss[0m : 1.59744
[1mStep[0m  [40/42], [94mLoss[0m : 1.44945

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46570
[1mStep[0m  [4/42], [94mLoss[0m : 1.66931
[1mStep[0m  [8/42], [94mLoss[0m : 1.41070
[1mStep[0m  [12/42], [94mLoss[0m : 1.56490
[1mStep[0m  [16/42], [94mLoss[0m : 1.36822
[1mStep[0m  [20/42], [94mLoss[0m : 1.44799
[1mStep[0m  [24/42], [94mLoss[0m : 1.69246
[1mStep[0m  [28/42], [94mLoss[0m : 1.50106
[1mStep[0m  [32/42], [94mLoss[0m : 1.35096
[1mStep[0m  [36/42], [94mLoss[0m : 1.48649
[1mStep[0m  [40/42], [94mLoss[0m : 1.48553

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.30779
[1mStep[0m  [4/42], [94mLoss[0m : 1.44038
[1mStep[0m  [8/42], [94mLoss[0m : 1.34602
[1mStep[0m  [12/42], [94mLoss[0m : 1.35502
[1mStep[0m  [16/42], [94mLoss[0m : 1.40837
[1mStep[0m  [20/42], [94mLoss[0m : 1.39035
[1mStep[0m  [24/42], [94mLoss[0m : 1.44573
[1mStep[0m  [28/42], [94mLoss[0m : 1.52302
[1mStep[0m  [32/42], [94mLoss[0m : 1.52227
[1mStep[0m  [36/42], [94mLoss[0m : 1.45852
[1mStep[0m  [40/42], [94mLoss[0m : 1.46902

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.31268
[1mStep[0m  [4/42], [94mLoss[0m : 1.42159
[1mStep[0m  [8/42], [94mLoss[0m : 1.36759
[1mStep[0m  [12/42], [94mLoss[0m : 1.35837
[1mStep[0m  [16/42], [94mLoss[0m : 1.41385
[1mStep[0m  [20/42], [94mLoss[0m : 1.33162
[1mStep[0m  [24/42], [94mLoss[0m : 1.54768
[1mStep[0m  [28/42], [94mLoss[0m : 1.41683
[1mStep[0m  [32/42], [94mLoss[0m : 1.34721
[1mStep[0m  [36/42], [94mLoss[0m : 1.46902
[1mStep[0m  [40/42], [94mLoss[0m : 1.59057

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.415, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37407
[1mStep[0m  [4/42], [94mLoss[0m : 1.26050
[1mStep[0m  [8/42], [94mLoss[0m : 1.32980
[1mStep[0m  [12/42], [94mLoss[0m : 1.32252
[1mStep[0m  [16/42], [94mLoss[0m : 1.35191
[1mStep[0m  [20/42], [94mLoss[0m : 1.27180
[1mStep[0m  [24/42], [94mLoss[0m : 1.35863
[1mStep[0m  [28/42], [94mLoss[0m : 1.35115
[1mStep[0m  [32/42], [94mLoss[0m : 1.38837
[1mStep[0m  [36/42], [94mLoss[0m : 1.33536
[1mStep[0m  [40/42], [94mLoss[0m : 1.46786

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.373, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.33582
[1mStep[0m  [4/42], [94mLoss[0m : 1.35172
[1mStep[0m  [8/42], [94mLoss[0m : 1.28965
[1mStep[0m  [12/42], [94mLoss[0m : 1.34821
[1mStep[0m  [16/42], [94mLoss[0m : 1.45292
[1mStep[0m  [20/42], [94mLoss[0m : 1.34541
[1mStep[0m  [24/42], [94mLoss[0m : 1.31285
[1mStep[0m  [28/42], [94mLoss[0m : 1.33123
[1mStep[0m  [32/42], [94mLoss[0m : 1.50554
[1mStep[0m  [36/42], [94mLoss[0m : 1.31673
[1mStep[0m  [40/42], [94mLoss[0m : 1.56509

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.371, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 19 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.510
====================================

Phase 2 - Evaluation MAE:  2.509992378098624
MAE score P1      2.331004
MAE score P2      2.509992
loss              1.370881
learning_rate         0.01
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 27, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.95084
[1mStep[0m  [16/169], [94mLoss[0m : 8.37220
[1mStep[0m  [32/169], [94mLoss[0m : 6.07655
[1mStep[0m  [48/169], [94mLoss[0m : 3.75012
[1mStep[0m  [64/169], [94mLoss[0m : 2.76669
[1mStep[0m  [80/169], [94mLoss[0m : 3.32710
[1mStep[0m  [96/169], [94mLoss[0m : 2.46165
[1mStep[0m  [112/169], [94mLoss[0m : 2.31032
[1mStep[0m  [128/169], [94mLoss[0m : 2.73403
[1mStep[0m  [144/169], [94mLoss[0m : 3.15234
[1mStep[0m  [160/169], [94mLoss[0m : 2.52455

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.059, [92mTest[0m: 10.895, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.96144
[1mStep[0m  [16/169], [94mLoss[0m : 2.46117
[1mStep[0m  [32/169], [94mLoss[0m : 2.73811
[1mStep[0m  [48/169], [94mLoss[0m : 2.46593
[1mStep[0m  [64/169], [94mLoss[0m : 2.68585
[1mStep[0m  [80/169], [94mLoss[0m : 2.86052
[1mStep[0m  [96/169], [94mLoss[0m : 2.83949
[1mStep[0m  [112/169], [94mLoss[0m : 2.76451
[1mStep[0m  [128/169], [94mLoss[0m : 2.74998
[1mStep[0m  [144/169], [94mLoss[0m : 2.59404
[1mStep[0m  [160/169], [94mLoss[0m : 2.82105

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.751, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57921
[1mStep[0m  [16/169], [94mLoss[0m : 2.97485
[1mStep[0m  [32/169], [94mLoss[0m : 2.24922
[1mStep[0m  [48/169], [94mLoss[0m : 2.62015
[1mStep[0m  [64/169], [94mLoss[0m : 2.60864
[1mStep[0m  [80/169], [94mLoss[0m : 2.38523
[1mStep[0m  [96/169], [94mLoss[0m : 2.74013
[1mStep[0m  [112/169], [94mLoss[0m : 3.04859
[1mStep[0m  [128/169], [94mLoss[0m : 2.89880
[1mStep[0m  [144/169], [94mLoss[0m : 2.64220
[1mStep[0m  [160/169], [94mLoss[0m : 2.37138

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33554
[1mStep[0m  [16/169], [94mLoss[0m : 2.91923
[1mStep[0m  [32/169], [94mLoss[0m : 2.58821
[1mStep[0m  [48/169], [94mLoss[0m : 2.36999
[1mStep[0m  [64/169], [94mLoss[0m : 2.75954
[1mStep[0m  [80/169], [94mLoss[0m : 3.23393
[1mStep[0m  [96/169], [94mLoss[0m : 2.22895
[1mStep[0m  [112/169], [94mLoss[0m : 2.52685
[1mStep[0m  [128/169], [94mLoss[0m : 2.52441
[1mStep[0m  [144/169], [94mLoss[0m : 3.27560
[1mStep[0m  [160/169], [94mLoss[0m : 2.53018

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98493
[1mStep[0m  [16/169], [94mLoss[0m : 2.84279
[1mStep[0m  [32/169], [94mLoss[0m : 2.92983
[1mStep[0m  [48/169], [94mLoss[0m : 2.69352
[1mStep[0m  [64/169], [94mLoss[0m : 3.00786
[1mStep[0m  [80/169], [94mLoss[0m : 2.86230
[1mStep[0m  [96/169], [94mLoss[0m : 2.62678
[1mStep[0m  [112/169], [94mLoss[0m : 2.87908
[1mStep[0m  [128/169], [94mLoss[0m : 2.76777
[1mStep[0m  [144/169], [94mLoss[0m : 2.55988
[1mStep[0m  [160/169], [94mLoss[0m : 2.76426

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41257
[1mStep[0m  [16/169], [94mLoss[0m : 2.91400
[1mStep[0m  [32/169], [94mLoss[0m : 2.67188
[1mStep[0m  [48/169], [94mLoss[0m : 2.83025
[1mStep[0m  [64/169], [94mLoss[0m : 2.49328
[1mStep[0m  [80/169], [94mLoss[0m : 2.50317
[1mStep[0m  [96/169], [94mLoss[0m : 2.34416
[1mStep[0m  [112/169], [94mLoss[0m : 2.61179
[1mStep[0m  [128/169], [94mLoss[0m : 3.08786
[1mStep[0m  [144/169], [94mLoss[0m : 2.69520
[1mStep[0m  [160/169], [94mLoss[0m : 2.54369

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28853
[1mStep[0m  [16/169], [94mLoss[0m : 2.62562
[1mStep[0m  [32/169], [94mLoss[0m : 2.81174
[1mStep[0m  [48/169], [94mLoss[0m : 2.30038
[1mStep[0m  [64/169], [94mLoss[0m : 2.59716
[1mStep[0m  [80/169], [94mLoss[0m : 2.11478
[1mStep[0m  [96/169], [94mLoss[0m : 2.72382
[1mStep[0m  [112/169], [94mLoss[0m : 2.55218
[1mStep[0m  [128/169], [94mLoss[0m : 2.19873
[1mStep[0m  [144/169], [94mLoss[0m : 2.87443
[1mStep[0m  [160/169], [94mLoss[0m : 2.48325

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.96421
[1mStep[0m  [16/169], [94mLoss[0m : 2.24826
[1mStep[0m  [32/169], [94mLoss[0m : 2.23263
[1mStep[0m  [48/169], [94mLoss[0m : 2.56910
[1mStep[0m  [64/169], [94mLoss[0m : 3.27895
[1mStep[0m  [80/169], [94mLoss[0m : 2.39910
[1mStep[0m  [96/169], [94mLoss[0m : 2.67292
[1mStep[0m  [112/169], [94mLoss[0m : 2.96193
[1mStep[0m  [128/169], [94mLoss[0m : 2.27698
[1mStep[0m  [144/169], [94mLoss[0m : 2.59183
[1mStep[0m  [160/169], [94mLoss[0m : 2.15036

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.08335
[1mStep[0m  [16/169], [94mLoss[0m : 2.59367
[1mStep[0m  [32/169], [94mLoss[0m : 2.76062
[1mStep[0m  [48/169], [94mLoss[0m : 2.59490
[1mStep[0m  [64/169], [94mLoss[0m : 2.57521
[1mStep[0m  [80/169], [94mLoss[0m : 2.47912
[1mStep[0m  [96/169], [94mLoss[0m : 2.60011
[1mStep[0m  [112/169], [94mLoss[0m : 2.47486
[1mStep[0m  [128/169], [94mLoss[0m : 2.11254
[1mStep[0m  [144/169], [94mLoss[0m : 2.39724
[1mStep[0m  [160/169], [94mLoss[0m : 2.18657

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58281
[1mStep[0m  [16/169], [94mLoss[0m : 2.90308
[1mStep[0m  [32/169], [94mLoss[0m : 2.56747
[1mStep[0m  [48/169], [94mLoss[0m : 2.45780
[1mStep[0m  [64/169], [94mLoss[0m : 2.56900
[1mStep[0m  [80/169], [94mLoss[0m : 2.71388
[1mStep[0m  [96/169], [94mLoss[0m : 2.44590
[1mStep[0m  [112/169], [94mLoss[0m : 2.06756
[1mStep[0m  [128/169], [94mLoss[0m : 2.39602
[1mStep[0m  [144/169], [94mLoss[0m : 2.20730
[1mStep[0m  [160/169], [94mLoss[0m : 2.36729

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.97224
[1mStep[0m  [16/169], [94mLoss[0m : 2.84932
[1mStep[0m  [32/169], [94mLoss[0m : 2.88794
[1mStep[0m  [48/169], [94mLoss[0m : 2.24471
[1mStep[0m  [64/169], [94mLoss[0m : 2.33488
[1mStep[0m  [80/169], [94mLoss[0m : 2.50883
[1mStep[0m  [96/169], [94mLoss[0m : 3.00227
[1mStep[0m  [112/169], [94mLoss[0m : 2.27675
[1mStep[0m  [128/169], [94mLoss[0m : 2.43765
[1mStep[0m  [144/169], [94mLoss[0m : 2.70311
[1mStep[0m  [160/169], [94mLoss[0m : 2.40453

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43492
[1mStep[0m  [16/169], [94mLoss[0m : 2.74423
[1mStep[0m  [32/169], [94mLoss[0m : 2.71234
[1mStep[0m  [48/169], [94mLoss[0m : 2.49430
[1mStep[0m  [64/169], [94mLoss[0m : 2.35180
[1mStep[0m  [80/169], [94mLoss[0m : 2.56206
[1mStep[0m  [96/169], [94mLoss[0m : 2.22854
[1mStep[0m  [112/169], [94mLoss[0m : 2.38073
[1mStep[0m  [128/169], [94mLoss[0m : 2.16590
[1mStep[0m  [144/169], [94mLoss[0m : 2.65710
[1mStep[0m  [160/169], [94mLoss[0m : 2.26160

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55148
[1mStep[0m  [16/169], [94mLoss[0m : 2.69687
[1mStep[0m  [32/169], [94mLoss[0m : 2.32315
[1mStep[0m  [48/169], [94mLoss[0m : 1.99340
[1mStep[0m  [64/169], [94mLoss[0m : 2.58222
[1mStep[0m  [80/169], [94mLoss[0m : 3.12673
[1mStep[0m  [96/169], [94mLoss[0m : 2.28865
[1mStep[0m  [112/169], [94mLoss[0m : 2.38462
[1mStep[0m  [128/169], [94mLoss[0m : 2.40436
[1mStep[0m  [144/169], [94mLoss[0m : 2.85896
[1mStep[0m  [160/169], [94mLoss[0m : 2.59780

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26847
[1mStep[0m  [16/169], [94mLoss[0m : 2.52191
[1mStep[0m  [32/169], [94mLoss[0m : 2.35280
[1mStep[0m  [48/169], [94mLoss[0m : 2.72395
[1mStep[0m  [64/169], [94mLoss[0m : 2.37526
[1mStep[0m  [80/169], [94mLoss[0m : 2.56871
[1mStep[0m  [96/169], [94mLoss[0m : 2.68655
[1mStep[0m  [112/169], [94mLoss[0m : 3.02047
[1mStep[0m  [128/169], [94mLoss[0m : 2.45240
[1mStep[0m  [144/169], [94mLoss[0m : 2.17219
[1mStep[0m  [160/169], [94mLoss[0m : 3.01198

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44777
[1mStep[0m  [16/169], [94mLoss[0m : 2.18341
[1mStep[0m  [32/169], [94mLoss[0m : 2.49340
[1mStep[0m  [48/169], [94mLoss[0m : 2.46351
[1mStep[0m  [64/169], [94mLoss[0m : 2.53622
[1mStep[0m  [80/169], [94mLoss[0m : 2.38800
[1mStep[0m  [96/169], [94mLoss[0m : 2.56619
[1mStep[0m  [112/169], [94mLoss[0m : 2.55922
[1mStep[0m  [128/169], [94mLoss[0m : 2.52846
[1mStep[0m  [144/169], [94mLoss[0m : 2.74584
[1mStep[0m  [160/169], [94mLoss[0m : 2.57278

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51475
[1mStep[0m  [16/169], [94mLoss[0m : 2.10282
[1mStep[0m  [32/169], [94mLoss[0m : 2.83094
[1mStep[0m  [48/169], [94mLoss[0m : 2.25909
[1mStep[0m  [64/169], [94mLoss[0m : 2.11671
[1mStep[0m  [80/169], [94mLoss[0m : 2.15520
[1mStep[0m  [96/169], [94mLoss[0m : 2.21828
[1mStep[0m  [112/169], [94mLoss[0m : 3.14964
[1mStep[0m  [128/169], [94mLoss[0m : 2.13760
[1mStep[0m  [144/169], [94mLoss[0m : 2.26705
[1mStep[0m  [160/169], [94mLoss[0m : 2.11252

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42704
[1mStep[0m  [16/169], [94mLoss[0m : 2.40821
[1mStep[0m  [32/169], [94mLoss[0m : 2.27372
[1mStep[0m  [48/169], [94mLoss[0m : 2.10351
[1mStep[0m  [64/169], [94mLoss[0m : 2.38128
[1mStep[0m  [80/169], [94mLoss[0m : 2.17014
[1mStep[0m  [96/169], [94mLoss[0m : 2.79130
[1mStep[0m  [112/169], [94mLoss[0m : 2.48103
[1mStep[0m  [128/169], [94mLoss[0m : 2.61799
[1mStep[0m  [144/169], [94mLoss[0m : 2.32613
[1mStep[0m  [160/169], [94mLoss[0m : 2.46831

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.299, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49481
[1mStep[0m  [16/169], [94mLoss[0m : 2.72125
[1mStep[0m  [32/169], [94mLoss[0m : 2.48330
[1mStep[0m  [48/169], [94mLoss[0m : 2.70942
[1mStep[0m  [64/169], [94mLoss[0m : 2.55415
[1mStep[0m  [80/169], [94mLoss[0m : 2.42571
[1mStep[0m  [96/169], [94mLoss[0m : 2.26965
[1mStep[0m  [112/169], [94mLoss[0m : 2.86929
[1mStep[0m  [128/169], [94mLoss[0m : 2.84190
[1mStep[0m  [144/169], [94mLoss[0m : 2.43525
[1mStep[0m  [160/169], [94mLoss[0m : 2.39772

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83556
[1mStep[0m  [16/169], [94mLoss[0m : 2.49490
[1mStep[0m  [32/169], [94mLoss[0m : 2.70789
[1mStep[0m  [48/169], [94mLoss[0m : 2.67346
[1mStep[0m  [64/169], [94mLoss[0m : 2.31169
[1mStep[0m  [80/169], [94mLoss[0m : 2.34311
[1mStep[0m  [96/169], [94mLoss[0m : 2.55609
[1mStep[0m  [112/169], [94mLoss[0m : 2.59131
[1mStep[0m  [128/169], [94mLoss[0m : 2.04816
[1mStep[0m  [144/169], [94mLoss[0m : 2.08762
[1mStep[0m  [160/169], [94mLoss[0m : 2.84020

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.310, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21539
[1mStep[0m  [16/169], [94mLoss[0m : 2.23147
[1mStep[0m  [32/169], [94mLoss[0m : 2.19297
[1mStep[0m  [48/169], [94mLoss[0m : 2.49587
[1mStep[0m  [64/169], [94mLoss[0m : 2.40253
[1mStep[0m  [80/169], [94mLoss[0m : 2.41752
[1mStep[0m  [96/169], [94mLoss[0m : 2.82318
[1mStep[0m  [112/169], [94mLoss[0m : 2.23302
[1mStep[0m  [128/169], [94mLoss[0m : 2.59410
[1mStep[0m  [144/169], [94mLoss[0m : 2.82203
[1mStep[0m  [160/169], [94mLoss[0m : 2.38249

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49973
[1mStep[0m  [16/169], [94mLoss[0m : 2.41416
[1mStep[0m  [32/169], [94mLoss[0m : 2.26090
[1mStep[0m  [48/169], [94mLoss[0m : 2.71904
[1mStep[0m  [64/169], [94mLoss[0m : 2.30315
[1mStep[0m  [80/169], [94mLoss[0m : 2.47614
[1mStep[0m  [96/169], [94mLoss[0m : 2.62430
[1mStep[0m  [112/169], [94mLoss[0m : 2.18262
[1mStep[0m  [128/169], [94mLoss[0m : 2.66730
[1mStep[0m  [144/169], [94mLoss[0m : 2.63226
[1mStep[0m  [160/169], [94mLoss[0m : 2.52893

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.302, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94398
[1mStep[0m  [16/169], [94mLoss[0m : 2.31861
[1mStep[0m  [32/169], [94mLoss[0m : 2.76127
[1mStep[0m  [48/169], [94mLoss[0m : 3.04137
[1mStep[0m  [64/169], [94mLoss[0m : 3.04961
[1mStep[0m  [80/169], [94mLoss[0m : 2.26973
[1mStep[0m  [96/169], [94mLoss[0m : 2.32504
[1mStep[0m  [112/169], [94mLoss[0m : 1.98206
[1mStep[0m  [128/169], [94mLoss[0m : 3.04114
[1mStep[0m  [144/169], [94mLoss[0m : 2.23648
[1mStep[0m  [160/169], [94mLoss[0m : 2.27696

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.307, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25502
[1mStep[0m  [16/169], [94mLoss[0m : 2.26198
[1mStep[0m  [32/169], [94mLoss[0m : 2.58987
[1mStep[0m  [48/169], [94mLoss[0m : 1.95467
[1mStep[0m  [64/169], [94mLoss[0m : 2.17800
[1mStep[0m  [80/169], [94mLoss[0m : 2.64845
[1mStep[0m  [96/169], [94mLoss[0m : 2.42021
[1mStep[0m  [112/169], [94mLoss[0m : 2.40948
[1mStep[0m  [128/169], [94mLoss[0m : 2.34540
[1mStep[0m  [144/169], [94mLoss[0m : 2.35749
[1mStep[0m  [160/169], [94mLoss[0m : 2.63859

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85297
[1mStep[0m  [16/169], [94mLoss[0m : 1.84679
[1mStep[0m  [32/169], [94mLoss[0m : 2.55666
[1mStep[0m  [48/169], [94mLoss[0m : 2.45963
[1mStep[0m  [64/169], [94mLoss[0m : 2.71293
[1mStep[0m  [80/169], [94mLoss[0m : 2.24013
[1mStep[0m  [96/169], [94mLoss[0m : 2.58062
[1mStep[0m  [112/169], [94mLoss[0m : 2.42968
[1mStep[0m  [128/169], [94mLoss[0m : 2.39659
[1mStep[0m  [144/169], [94mLoss[0m : 2.83280
[1mStep[0m  [160/169], [94mLoss[0m : 2.32844

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29910
[1mStep[0m  [16/169], [94mLoss[0m : 2.70017
[1mStep[0m  [32/169], [94mLoss[0m : 2.52800
[1mStep[0m  [48/169], [94mLoss[0m : 2.41808
[1mStep[0m  [64/169], [94mLoss[0m : 2.63995
[1mStep[0m  [80/169], [94mLoss[0m : 2.59925
[1mStep[0m  [96/169], [94mLoss[0m : 2.58387
[1mStep[0m  [112/169], [94mLoss[0m : 2.26846
[1mStep[0m  [128/169], [94mLoss[0m : 2.49486
[1mStep[0m  [144/169], [94mLoss[0m : 2.70586
[1mStep[0m  [160/169], [94mLoss[0m : 2.12879

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.301, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29935
[1mStep[0m  [16/169], [94mLoss[0m : 1.98105
[1mStep[0m  [32/169], [94mLoss[0m : 1.92325
[1mStep[0m  [48/169], [94mLoss[0m : 2.23789
[1mStep[0m  [64/169], [94mLoss[0m : 2.47815
[1mStep[0m  [80/169], [94mLoss[0m : 2.49481
[1mStep[0m  [96/169], [94mLoss[0m : 2.17556
[1mStep[0m  [112/169], [94mLoss[0m : 2.35403
[1mStep[0m  [128/169], [94mLoss[0m : 2.36009
[1mStep[0m  [144/169], [94mLoss[0m : 2.67802
[1mStep[0m  [160/169], [94mLoss[0m : 2.66036

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.314, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47928
[1mStep[0m  [16/169], [94mLoss[0m : 2.73279
[1mStep[0m  [32/169], [94mLoss[0m : 2.83479
[1mStep[0m  [48/169], [94mLoss[0m : 2.16606
[1mStep[0m  [64/169], [94mLoss[0m : 2.71991
[1mStep[0m  [80/169], [94mLoss[0m : 2.35836
[1mStep[0m  [96/169], [94mLoss[0m : 3.04921
[1mStep[0m  [112/169], [94mLoss[0m : 2.16820
[1mStep[0m  [128/169], [94mLoss[0m : 2.82247
[1mStep[0m  [144/169], [94mLoss[0m : 2.49336
[1mStep[0m  [160/169], [94mLoss[0m : 2.26654

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60147
[1mStep[0m  [16/169], [94mLoss[0m : 2.26667
[1mStep[0m  [32/169], [94mLoss[0m : 2.21481
[1mStep[0m  [48/169], [94mLoss[0m : 2.46925
[1mStep[0m  [64/169], [94mLoss[0m : 1.88386
[1mStep[0m  [80/169], [94mLoss[0m : 2.34277
[1mStep[0m  [96/169], [94mLoss[0m : 2.71509
[1mStep[0m  [112/169], [94mLoss[0m : 2.44043
[1mStep[0m  [128/169], [94mLoss[0m : 3.03126
[1mStep[0m  [144/169], [94mLoss[0m : 2.39171
[1mStep[0m  [160/169], [94mLoss[0m : 2.53580

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19281
[1mStep[0m  [16/169], [94mLoss[0m : 2.58468
[1mStep[0m  [32/169], [94mLoss[0m : 2.10327
[1mStep[0m  [48/169], [94mLoss[0m : 2.22726
[1mStep[0m  [64/169], [94mLoss[0m : 2.25735
[1mStep[0m  [80/169], [94mLoss[0m : 2.59768
[1mStep[0m  [96/169], [94mLoss[0m : 2.24277
[1mStep[0m  [112/169], [94mLoss[0m : 2.80801
[1mStep[0m  [128/169], [94mLoss[0m : 2.53575
[1mStep[0m  [144/169], [94mLoss[0m : 2.27140
[1mStep[0m  [160/169], [94mLoss[0m : 1.88904

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.308, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76102
[1mStep[0m  [16/169], [94mLoss[0m : 1.97465
[1mStep[0m  [32/169], [94mLoss[0m : 2.37565
[1mStep[0m  [48/169], [94mLoss[0m : 2.40925
[1mStep[0m  [64/169], [94mLoss[0m : 2.49314
[1mStep[0m  [80/169], [94mLoss[0m : 2.55045
[1mStep[0m  [96/169], [94mLoss[0m : 2.08893
[1mStep[0m  [112/169], [94mLoss[0m : 2.74481
[1mStep[0m  [128/169], [94mLoss[0m : 2.32831
[1mStep[0m  [144/169], [94mLoss[0m : 2.39873
[1mStep[0m  [160/169], [94mLoss[0m : 2.42594

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.287, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.302
====================================

Phase 1 - Evaluation MAE:  2.3023761936596463
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.57587
[1mStep[0m  [16/169], [94mLoss[0m : 2.50008
[1mStep[0m  [32/169], [94mLoss[0m : 2.52639
[1mStep[0m  [48/169], [94mLoss[0m : 2.59471
[1mStep[0m  [64/169], [94mLoss[0m : 2.22256
[1mStep[0m  [80/169], [94mLoss[0m : 2.60131
[1mStep[0m  [96/169], [94mLoss[0m : 2.92519
[1mStep[0m  [112/169], [94mLoss[0m : 2.35121
[1mStep[0m  [128/169], [94mLoss[0m : 2.32520
[1mStep[0m  [144/169], [94mLoss[0m : 2.41223
[1mStep[0m  [160/169], [94mLoss[0m : 2.77899

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.302, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59173
[1mStep[0m  [16/169], [94mLoss[0m : 2.30338
[1mStep[0m  [32/169], [94mLoss[0m : 2.27727
[1mStep[0m  [48/169], [94mLoss[0m : 2.14342
[1mStep[0m  [64/169], [94mLoss[0m : 2.13431
[1mStep[0m  [80/169], [94mLoss[0m : 2.45016
[1mStep[0m  [96/169], [94mLoss[0m : 2.73692
[1mStep[0m  [112/169], [94mLoss[0m : 2.67640
[1mStep[0m  [128/169], [94mLoss[0m : 2.20620
[1mStep[0m  [144/169], [94mLoss[0m : 2.23864
[1mStep[0m  [160/169], [94mLoss[0m : 2.70677

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66391
[1mStep[0m  [16/169], [94mLoss[0m : 2.15965
[1mStep[0m  [32/169], [94mLoss[0m : 2.24084
[1mStep[0m  [48/169], [94mLoss[0m : 2.12833
[1mStep[0m  [64/169], [94mLoss[0m : 2.06253
[1mStep[0m  [80/169], [94mLoss[0m : 2.30023
[1mStep[0m  [96/169], [94mLoss[0m : 2.45703
[1mStep[0m  [112/169], [94mLoss[0m : 2.10547
[1mStep[0m  [128/169], [94mLoss[0m : 2.46159
[1mStep[0m  [144/169], [94mLoss[0m : 2.31607
[1mStep[0m  [160/169], [94mLoss[0m : 2.20151

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38791
[1mStep[0m  [16/169], [94mLoss[0m : 2.31646
[1mStep[0m  [32/169], [94mLoss[0m : 1.91171
[1mStep[0m  [48/169], [94mLoss[0m : 2.17422
[1mStep[0m  [64/169], [94mLoss[0m : 2.57197
[1mStep[0m  [80/169], [94mLoss[0m : 2.12497
[1mStep[0m  [96/169], [94mLoss[0m : 2.32405
[1mStep[0m  [112/169], [94mLoss[0m : 2.28848
[1mStep[0m  [128/169], [94mLoss[0m : 2.37184
[1mStep[0m  [144/169], [94mLoss[0m : 2.46683
[1mStep[0m  [160/169], [94mLoss[0m : 2.62863

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24446
[1mStep[0m  [16/169], [94mLoss[0m : 2.05388
[1mStep[0m  [32/169], [94mLoss[0m : 2.55154
[1mStep[0m  [48/169], [94mLoss[0m : 2.19253
[1mStep[0m  [64/169], [94mLoss[0m : 2.72734
[1mStep[0m  [80/169], [94mLoss[0m : 2.55274
[1mStep[0m  [96/169], [94mLoss[0m : 2.48751
[1mStep[0m  [112/169], [94mLoss[0m : 2.36550
[1mStep[0m  [128/169], [94mLoss[0m : 1.83503
[1mStep[0m  [144/169], [94mLoss[0m : 2.28053
[1mStep[0m  [160/169], [94mLoss[0m : 2.32775

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38371
[1mStep[0m  [16/169], [94mLoss[0m : 2.11020
[1mStep[0m  [32/169], [94mLoss[0m : 2.20778
[1mStep[0m  [48/169], [94mLoss[0m : 2.18329
[1mStep[0m  [64/169], [94mLoss[0m : 1.91498
[1mStep[0m  [80/169], [94mLoss[0m : 1.80374
[1mStep[0m  [96/169], [94mLoss[0m : 2.00713
[1mStep[0m  [112/169], [94mLoss[0m : 2.12982
[1mStep[0m  [128/169], [94mLoss[0m : 2.20703
[1mStep[0m  [144/169], [94mLoss[0m : 2.36139
[1mStep[0m  [160/169], [94mLoss[0m : 1.95070

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99777
[1mStep[0m  [16/169], [94mLoss[0m : 2.14319
[1mStep[0m  [32/169], [94mLoss[0m : 1.96092
[1mStep[0m  [48/169], [94mLoss[0m : 1.76391
[1mStep[0m  [64/169], [94mLoss[0m : 2.07379
[1mStep[0m  [80/169], [94mLoss[0m : 2.34211
[1mStep[0m  [96/169], [94mLoss[0m : 2.28230
[1mStep[0m  [112/169], [94mLoss[0m : 2.24165
[1mStep[0m  [128/169], [94mLoss[0m : 2.43896
[1mStep[0m  [144/169], [94mLoss[0m : 2.31257
[1mStep[0m  [160/169], [94mLoss[0m : 2.00542

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95688
[1mStep[0m  [16/169], [94mLoss[0m : 1.97098
[1mStep[0m  [32/169], [94mLoss[0m : 2.03826
[1mStep[0m  [48/169], [94mLoss[0m : 1.65313
[1mStep[0m  [64/169], [94mLoss[0m : 1.93322
[1mStep[0m  [80/169], [94mLoss[0m : 2.06023
[1mStep[0m  [96/169], [94mLoss[0m : 1.84156
[1mStep[0m  [112/169], [94mLoss[0m : 2.05451
[1mStep[0m  [128/169], [94mLoss[0m : 2.02411
[1mStep[0m  [144/169], [94mLoss[0m : 2.24532
[1mStep[0m  [160/169], [94mLoss[0m : 2.17171

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01381
[1mStep[0m  [16/169], [94mLoss[0m : 1.78309
[1mStep[0m  [32/169], [94mLoss[0m : 2.28244
[1mStep[0m  [48/169], [94mLoss[0m : 2.11108
[1mStep[0m  [64/169], [94mLoss[0m : 2.05461
[1mStep[0m  [80/169], [94mLoss[0m : 2.14851
[1mStep[0m  [96/169], [94mLoss[0m : 1.98995
[1mStep[0m  [112/169], [94mLoss[0m : 2.32983
[1mStep[0m  [128/169], [94mLoss[0m : 2.18328
[1mStep[0m  [144/169], [94mLoss[0m : 2.03939
[1mStep[0m  [160/169], [94mLoss[0m : 2.32917

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17979
[1mStep[0m  [16/169], [94mLoss[0m : 2.20742
[1mStep[0m  [32/169], [94mLoss[0m : 1.66168
[1mStep[0m  [48/169], [94mLoss[0m : 1.93012
[1mStep[0m  [64/169], [94mLoss[0m : 1.77689
[1mStep[0m  [80/169], [94mLoss[0m : 1.68855
[1mStep[0m  [96/169], [94mLoss[0m : 2.15629
[1mStep[0m  [112/169], [94mLoss[0m : 2.40816
[1mStep[0m  [128/169], [94mLoss[0m : 2.21692
[1mStep[0m  [144/169], [94mLoss[0m : 1.90369
[1mStep[0m  [160/169], [94mLoss[0m : 2.14958

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78265
[1mStep[0m  [16/169], [94mLoss[0m : 1.93067
[1mStep[0m  [32/169], [94mLoss[0m : 2.11303
[1mStep[0m  [48/169], [94mLoss[0m : 2.02602
[1mStep[0m  [64/169], [94mLoss[0m : 1.78383
[1mStep[0m  [80/169], [94mLoss[0m : 2.31389
[1mStep[0m  [96/169], [94mLoss[0m : 1.78318
[1mStep[0m  [112/169], [94mLoss[0m : 1.78651
[1mStep[0m  [128/169], [94mLoss[0m : 1.79227
[1mStep[0m  [144/169], [94mLoss[0m : 1.93086
[1mStep[0m  [160/169], [94mLoss[0m : 2.11433

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03031
[1mStep[0m  [16/169], [94mLoss[0m : 1.74985
[1mStep[0m  [32/169], [94mLoss[0m : 2.32100
[1mStep[0m  [48/169], [94mLoss[0m : 2.28727
[1mStep[0m  [64/169], [94mLoss[0m : 2.10679
[1mStep[0m  [80/169], [94mLoss[0m : 2.36903
[1mStep[0m  [96/169], [94mLoss[0m : 2.39913
[1mStep[0m  [112/169], [94mLoss[0m : 2.01733
[1mStep[0m  [128/169], [94mLoss[0m : 2.40599
[1mStep[0m  [144/169], [94mLoss[0m : 1.68486
[1mStep[0m  [160/169], [94mLoss[0m : 2.32148

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.967, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94621
[1mStep[0m  [16/169], [94mLoss[0m : 2.21662
[1mStep[0m  [32/169], [94mLoss[0m : 2.43222
[1mStep[0m  [48/169], [94mLoss[0m : 1.93551
[1mStep[0m  [64/169], [94mLoss[0m : 2.12688
[1mStep[0m  [80/169], [94mLoss[0m : 2.15234
[1mStep[0m  [96/169], [94mLoss[0m : 2.13634
[1mStep[0m  [112/169], [94mLoss[0m : 1.70066
[1mStep[0m  [128/169], [94mLoss[0m : 1.71357
[1mStep[0m  [144/169], [94mLoss[0m : 2.14324
[1mStep[0m  [160/169], [94mLoss[0m : 1.80227

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01899
[1mStep[0m  [16/169], [94mLoss[0m : 1.89882
[1mStep[0m  [32/169], [94mLoss[0m : 2.00598
[1mStep[0m  [48/169], [94mLoss[0m : 1.94860
[1mStep[0m  [64/169], [94mLoss[0m : 1.75417
[1mStep[0m  [80/169], [94mLoss[0m : 1.68791
[1mStep[0m  [96/169], [94mLoss[0m : 2.22963
[1mStep[0m  [112/169], [94mLoss[0m : 1.68723
[1mStep[0m  [128/169], [94mLoss[0m : 1.88963
[1mStep[0m  [144/169], [94mLoss[0m : 2.86524
[1mStep[0m  [160/169], [94mLoss[0m : 1.92286

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.902, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22353
[1mStep[0m  [16/169], [94mLoss[0m : 1.82726
[1mStep[0m  [32/169], [94mLoss[0m : 1.71090
[1mStep[0m  [48/169], [94mLoss[0m : 1.86229
[1mStep[0m  [64/169], [94mLoss[0m : 1.90082
[1mStep[0m  [80/169], [94mLoss[0m : 1.91621
[1mStep[0m  [96/169], [94mLoss[0m : 2.03102
[1mStep[0m  [112/169], [94mLoss[0m : 1.86052
[1mStep[0m  [128/169], [94mLoss[0m : 1.76979
[1mStep[0m  [144/169], [94mLoss[0m : 1.58771
[1mStep[0m  [160/169], [94mLoss[0m : 1.90757

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61590
[1mStep[0m  [16/169], [94mLoss[0m : 1.81219
[1mStep[0m  [32/169], [94mLoss[0m : 1.83993
[1mStep[0m  [48/169], [94mLoss[0m : 1.68863
[1mStep[0m  [64/169], [94mLoss[0m : 1.57129
[1mStep[0m  [80/169], [94mLoss[0m : 1.94230
[1mStep[0m  [96/169], [94mLoss[0m : 1.98646
[1mStep[0m  [112/169], [94mLoss[0m : 1.83260
[1mStep[0m  [128/169], [94mLoss[0m : 1.64089
[1mStep[0m  [144/169], [94mLoss[0m : 2.15889
[1mStep[0m  [160/169], [94mLoss[0m : 1.91248

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.850, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75047
[1mStep[0m  [16/169], [94mLoss[0m : 1.73471
[1mStep[0m  [32/169], [94mLoss[0m : 1.57695
[1mStep[0m  [48/169], [94mLoss[0m : 2.24064
[1mStep[0m  [64/169], [94mLoss[0m : 1.63599
[1mStep[0m  [80/169], [94mLoss[0m : 1.27768
[1mStep[0m  [96/169], [94mLoss[0m : 1.61351
[1mStep[0m  [112/169], [94mLoss[0m : 1.92352
[1mStep[0m  [128/169], [94mLoss[0m : 2.02543
[1mStep[0m  [144/169], [94mLoss[0m : 1.66298
[1mStep[0m  [160/169], [94mLoss[0m : 1.86759

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74606
[1mStep[0m  [16/169], [94mLoss[0m : 1.71196
[1mStep[0m  [32/169], [94mLoss[0m : 1.63742
[1mStep[0m  [48/169], [94mLoss[0m : 1.49720
[1mStep[0m  [64/169], [94mLoss[0m : 1.78010
[1mStep[0m  [80/169], [94mLoss[0m : 1.68564
[1mStep[0m  [96/169], [94mLoss[0m : 1.59544
[1mStep[0m  [112/169], [94mLoss[0m : 1.68296
[1mStep[0m  [128/169], [94mLoss[0m : 1.77566
[1mStep[0m  [144/169], [94mLoss[0m : 1.87214
[1mStep[0m  [160/169], [94mLoss[0m : 1.78802

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05644
[1mStep[0m  [16/169], [94mLoss[0m : 1.44901
[1mStep[0m  [32/169], [94mLoss[0m : 1.74885
[1mStep[0m  [48/169], [94mLoss[0m : 2.09402
[1mStep[0m  [64/169], [94mLoss[0m : 2.08899
[1mStep[0m  [80/169], [94mLoss[0m : 1.71955
[1mStep[0m  [96/169], [94mLoss[0m : 1.59029
[1mStep[0m  [112/169], [94mLoss[0m : 1.87699
[1mStep[0m  [128/169], [94mLoss[0m : 1.66583
[1mStep[0m  [144/169], [94mLoss[0m : 2.21675
[1mStep[0m  [160/169], [94mLoss[0m : 1.95990

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.799, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.24618
[1mStep[0m  [16/169], [94mLoss[0m : 1.45781
[1mStep[0m  [32/169], [94mLoss[0m : 1.69795
[1mStep[0m  [48/169], [94mLoss[0m : 1.99611
[1mStep[0m  [64/169], [94mLoss[0m : 1.71138
[1mStep[0m  [80/169], [94mLoss[0m : 2.05833
[1mStep[0m  [96/169], [94mLoss[0m : 1.66357
[1mStep[0m  [112/169], [94mLoss[0m : 1.34153
[1mStep[0m  [128/169], [94mLoss[0m : 2.11972
[1mStep[0m  [144/169], [94mLoss[0m : 2.02843
[1mStep[0m  [160/169], [94mLoss[0m : 1.83182

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57317
[1mStep[0m  [16/169], [94mLoss[0m : 1.64321
[1mStep[0m  [32/169], [94mLoss[0m : 1.90890
[1mStep[0m  [48/169], [94mLoss[0m : 1.34929
[1mStep[0m  [64/169], [94mLoss[0m : 1.35567
[1mStep[0m  [80/169], [94mLoss[0m : 1.60579
[1mStep[0m  [96/169], [94mLoss[0m : 1.91739
[1mStep[0m  [112/169], [94mLoss[0m : 1.78058
[1mStep[0m  [128/169], [94mLoss[0m : 2.18251
[1mStep[0m  [144/169], [94mLoss[0m : 2.14685
[1mStep[0m  [160/169], [94mLoss[0m : 1.67750

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.554, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42991
[1mStep[0m  [16/169], [94mLoss[0m : 1.21239
[1mStep[0m  [32/169], [94mLoss[0m : 1.69605
[1mStep[0m  [48/169], [94mLoss[0m : 1.50658
[1mStep[0m  [64/169], [94mLoss[0m : 1.83674
[1mStep[0m  [80/169], [94mLoss[0m : 1.88682
[1mStep[0m  [96/169], [94mLoss[0m : 1.69527
[1mStep[0m  [112/169], [94mLoss[0m : 1.60434
[1mStep[0m  [128/169], [94mLoss[0m : 1.64982
[1mStep[0m  [144/169], [94mLoss[0m : 1.35574
[1mStep[0m  [160/169], [94mLoss[0m : 1.77845

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.467, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54722
[1mStep[0m  [16/169], [94mLoss[0m : 1.29438
[1mStep[0m  [32/169], [94mLoss[0m : 1.75529
[1mStep[0m  [48/169], [94mLoss[0m : 1.73161
[1mStep[0m  [64/169], [94mLoss[0m : 1.51460
[1mStep[0m  [80/169], [94mLoss[0m : 1.53940
[1mStep[0m  [96/169], [94mLoss[0m : 1.81298
[1mStep[0m  [112/169], [94mLoss[0m : 1.75171
[1mStep[0m  [128/169], [94mLoss[0m : 1.64401
[1mStep[0m  [144/169], [94mLoss[0m : 1.49697
[1mStep[0m  [160/169], [94mLoss[0m : 1.86215

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72237
[1mStep[0m  [16/169], [94mLoss[0m : 1.90795
[1mStep[0m  [32/169], [94mLoss[0m : 1.49657
[1mStep[0m  [48/169], [94mLoss[0m : 1.57772
[1mStep[0m  [64/169], [94mLoss[0m : 1.80131
[1mStep[0m  [80/169], [94mLoss[0m : 1.63004
[1mStep[0m  [96/169], [94mLoss[0m : 1.50041
[1mStep[0m  [112/169], [94mLoss[0m : 1.60024
[1mStep[0m  [128/169], [94mLoss[0m : 2.29501
[1mStep[0m  [144/169], [94mLoss[0m : 1.55900
[1mStep[0m  [160/169], [94mLoss[0m : 1.58790

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.711, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87805
[1mStep[0m  [16/169], [94mLoss[0m : 1.49769
[1mStep[0m  [32/169], [94mLoss[0m : 1.59446
[1mStep[0m  [48/169], [94mLoss[0m : 1.80820
[1mStep[0m  [64/169], [94mLoss[0m : 1.72737
[1mStep[0m  [80/169], [94mLoss[0m : 1.76604
[1mStep[0m  [96/169], [94mLoss[0m : 1.81694
[1mStep[0m  [112/169], [94mLoss[0m : 1.60907
[1mStep[0m  [128/169], [94mLoss[0m : 1.77019
[1mStep[0m  [144/169], [94mLoss[0m : 1.68074
[1mStep[0m  [160/169], [94mLoss[0m : 2.22295

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.688, [92mTest[0m: 2.499, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.58958
[1mStep[0m  [16/169], [94mLoss[0m : 1.60985
[1mStep[0m  [32/169], [94mLoss[0m : 1.90600
[1mStep[0m  [48/169], [94mLoss[0m : 1.35845
[1mStep[0m  [64/169], [94mLoss[0m : 2.03633
[1mStep[0m  [80/169], [94mLoss[0m : 1.60634
[1mStep[0m  [96/169], [94mLoss[0m : 1.60063
[1mStep[0m  [112/169], [94mLoss[0m : 1.85953
[1mStep[0m  [128/169], [94mLoss[0m : 1.47590
[1mStep[0m  [144/169], [94mLoss[0m : 1.85431
[1mStep[0m  [160/169], [94mLoss[0m : 1.70637

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.662, [92mTest[0m: 2.524, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74794
[1mStep[0m  [16/169], [94mLoss[0m : 1.74517
[1mStep[0m  [32/169], [94mLoss[0m : 1.69719
[1mStep[0m  [48/169], [94mLoss[0m : 1.74641
[1mStep[0m  [64/169], [94mLoss[0m : 1.58035
[1mStep[0m  [80/169], [94mLoss[0m : 2.30104
[1mStep[0m  [96/169], [94mLoss[0m : 1.56995
[1mStep[0m  [112/169], [94mLoss[0m : 1.83794
[1mStep[0m  [128/169], [94mLoss[0m : 1.45544
[1mStep[0m  [144/169], [94mLoss[0m : 1.91715
[1mStep[0m  [160/169], [94mLoss[0m : 1.75691

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87263
[1mStep[0m  [16/169], [94mLoss[0m : 1.86355
[1mStep[0m  [32/169], [94mLoss[0m : 1.40216
[1mStep[0m  [48/169], [94mLoss[0m : 1.76597
[1mStep[0m  [64/169], [94mLoss[0m : 1.79570
[1mStep[0m  [80/169], [94mLoss[0m : 1.88672
[1mStep[0m  [96/169], [94mLoss[0m : 1.66374
[1mStep[0m  [112/169], [94mLoss[0m : 1.79353
[1mStep[0m  [128/169], [94mLoss[0m : 1.60613
[1mStep[0m  [144/169], [94mLoss[0m : 1.43728
[1mStep[0m  [160/169], [94mLoss[0m : 1.50321

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.518, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46554
[1mStep[0m  [16/169], [94mLoss[0m : 1.79699
[1mStep[0m  [32/169], [94mLoss[0m : 1.46206
[1mStep[0m  [48/169], [94mLoss[0m : 1.70934
[1mStep[0m  [64/169], [94mLoss[0m : 1.85715
[1mStep[0m  [80/169], [94mLoss[0m : 1.94587
[1mStep[0m  [96/169], [94mLoss[0m : 1.39073
[1mStep[0m  [112/169], [94mLoss[0m : 1.66550
[1mStep[0m  [128/169], [94mLoss[0m : 1.77359
[1mStep[0m  [144/169], [94mLoss[0m : 1.58633
[1mStep[0m  [160/169], [94mLoss[0m : 1.65337

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.559, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00326
[1mStep[0m  [16/169], [94mLoss[0m : 1.38734
[1mStep[0m  [32/169], [94mLoss[0m : 1.82032
[1mStep[0m  [48/169], [94mLoss[0m : 1.62878
[1mStep[0m  [64/169], [94mLoss[0m : 1.56118
[1mStep[0m  [80/169], [94mLoss[0m : 1.47413
[1mStep[0m  [96/169], [94mLoss[0m : 1.91508
[1mStep[0m  [112/169], [94mLoss[0m : 1.81972
[1mStep[0m  [128/169], [94mLoss[0m : 1.98861
[1mStep[0m  [144/169], [94mLoss[0m : 1.59947
[1mStep[0m  [160/169], [94mLoss[0m : 1.70064

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.543, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.457
====================================

Phase 2 - Evaluation MAE:  2.457402784909521
MAE score P1       2.302376
MAE score P2       2.457403
loss                 1.6334
learning_rate          0.01
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 28, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
