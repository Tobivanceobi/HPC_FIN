no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  14
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 10.76266
[1mStep[0m  [10/106], [94mLoss[0m : 9.38234
[1mStep[0m  [20/106], [94mLoss[0m : 8.01342
[1mStep[0m  [30/106], [94mLoss[0m : 5.91303
[1mStep[0m  [40/106], [94mLoss[0m : 4.51825
[1mStep[0m  [50/106], [94mLoss[0m : 3.69642
[1mStep[0m  [60/106], [94mLoss[0m : 3.19274
[1mStep[0m  [70/106], [94mLoss[0m : 3.27510
[1mStep[0m  [80/106], [94mLoss[0m : 3.30141
[1mStep[0m  [90/106], [94mLoss[0m : 2.86220
[1mStep[0m  [100/106], [94mLoss[0m : 2.98122

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.962, [92mTest[0m: 10.838, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.14236
[1mStep[0m  [10/106], [94mLoss[0m : 3.14588
[1mStep[0m  [20/106], [94mLoss[0m : 2.85360
[1mStep[0m  [30/106], [94mLoss[0m : 2.79950
[1mStep[0m  [40/106], [94mLoss[0m : 2.47684
[1mStep[0m  [50/106], [94mLoss[0m : 2.97355
[1mStep[0m  [60/106], [94mLoss[0m : 3.06068
[1mStep[0m  [70/106], [94mLoss[0m : 3.41342
[1mStep[0m  [80/106], [94mLoss[0m : 2.95832
[1mStep[0m  [90/106], [94mLoss[0m : 3.14098
[1mStep[0m  [100/106], [94mLoss[0m : 2.70857

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.920, [92mTest[0m: 3.270, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77614
[1mStep[0m  [10/106], [94mLoss[0m : 2.85316
[1mStep[0m  [20/106], [94mLoss[0m : 3.18936
[1mStep[0m  [30/106], [94mLoss[0m : 2.79358
[1mStep[0m  [40/106], [94mLoss[0m : 2.89510
[1mStep[0m  [50/106], [94mLoss[0m : 2.93952
[1mStep[0m  [60/106], [94mLoss[0m : 2.88991
[1mStep[0m  [70/106], [94mLoss[0m : 2.78941
[1mStep[0m  [80/106], [94mLoss[0m : 2.65075
[1mStep[0m  [90/106], [94mLoss[0m : 2.64744
[1mStep[0m  [100/106], [94mLoss[0m : 2.73332

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.846, [92mTest[0m: 2.611, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.00195
[1mStep[0m  [10/106], [94mLoss[0m : 2.89334
[1mStep[0m  [20/106], [94mLoss[0m : 2.73473
[1mStep[0m  [30/106], [94mLoss[0m : 2.89275
[1mStep[0m  [40/106], [94mLoss[0m : 2.62992
[1mStep[0m  [50/106], [94mLoss[0m : 3.19218
[1mStep[0m  [60/106], [94mLoss[0m : 2.99728
[1mStep[0m  [70/106], [94mLoss[0m : 2.96791
[1mStep[0m  [80/106], [94mLoss[0m : 3.00551
[1mStep[0m  [90/106], [94mLoss[0m : 2.84621
[1mStep[0m  [100/106], [94mLoss[0m : 2.59794

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.796, [92mTest[0m: 2.555, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75749
[1mStep[0m  [10/106], [94mLoss[0m : 2.99325
[1mStep[0m  [20/106], [94mLoss[0m : 2.55449
[1mStep[0m  [30/106], [94mLoss[0m : 2.77024
[1mStep[0m  [40/106], [94mLoss[0m : 2.90308
[1mStep[0m  [50/106], [94mLoss[0m : 2.69134
[1mStep[0m  [60/106], [94mLoss[0m : 3.16685
[1mStep[0m  [70/106], [94mLoss[0m : 2.78709
[1mStep[0m  [80/106], [94mLoss[0m : 3.03853
[1mStep[0m  [90/106], [94mLoss[0m : 2.72656
[1mStep[0m  [100/106], [94mLoss[0m : 2.94776

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44027
[1mStep[0m  [10/106], [94mLoss[0m : 2.68483
[1mStep[0m  [20/106], [94mLoss[0m : 2.88252
[1mStep[0m  [30/106], [94mLoss[0m : 2.66244
[1mStep[0m  [40/106], [94mLoss[0m : 2.68111
[1mStep[0m  [50/106], [94mLoss[0m : 2.71377
[1mStep[0m  [60/106], [94mLoss[0m : 2.79359
[1mStep[0m  [70/106], [94mLoss[0m : 2.78104
[1mStep[0m  [80/106], [94mLoss[0m : 2.88575
[1mStep[0m  [90/106], [94mLoss[0m : 2.77422
[1mStep[0m  [100/106], [94mLoss[0m : 2.68608

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73697
[1mStep[0m  [10/106], [94mLoss[0m : 2.53791
[1mStep[0m  [20/106], [94mLoss[0m : 2.43179
[1mStep[0m  [30/106], [94mLoss[0m : 2.57593
[1mStep[0m  [40/106], [94mLoss[0m : 2.68209
[1mStep[0m  [50/106], [94mLoss[0m : 2.85485
[1mStep[0m  [60/106], [94mLoss[0m : 2.68915
[1mStep[0m  [70/106], [94mLoss[0m : 2.94180
[1mStep[0m  [80/106], [94mLoss[0m : 2.58304
[1mStep[0m  [90/106], [94mLoss[0m : 2.97161
[1mStep[0m  [100/106], [94mLoss[0m : 2.81937

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.717, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73948
[1mStep[0m  [10/106], [94mLoss[0m : 2.87070
[1mStep[0m  [20/106], [94mLoss[0m : 2.64833
[1mStep[0m  [30/106], [94mLoss[0m : 2.53016
[1mStep[0m  [40/106], [94mLoss[0m : 2.59306
[1mStep[0m  [50/106], [94mLoss[0m : 2.45294
[1mStep[0m  [60/106], [94mLoss[0m : 3.00132
[1mStep[0m  [70/106], [94mLoss[0m : 2.69009
[1mStep[0m  [80/106], [94mLoss[0m : 2.89723
[1mStep[0m  [90/106], [94mLoss[0m : 2.32800
[1mStep[0m  [100/106], [94mLoss[0m : 2.26180

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62515
[1mStep[0m  [10/106], [94mLoss[0m : 2.69232
[1mStep[0m  [20/106], [94mLoss[0m : 2.74827
[1mStep[0m  [30/106], [94mLoss[0m : 2.62775
[1mStep[0m  [40/106], [94mLoss[0m : 2.94423
[1mStep[0m  [50/106], [94mLoss[0m : 2.68163
[1mStep[0m  [60/106], [94mLoss[0m : 2.72589
[1mStep[0m  [70/106], [94mLoss[0m : 2.77723
[1mStep[0m  [80/106], [94mLoss[0m : 2.81534
[1mStep[0m  [90/106], [94mLoss[0m : 2.51728
[1mStep[0m  [100/106], [94mLoss[0m : 3.01661

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69473
[1mStep[0m  [10/106], [94mLoss[0m : 2.64077
[1mStep[0m  [20/106], [94mLoss[0m : 2.52573
[1mStep[0m  [30/106], [94mLoss[0m : 2.49699
[1mStep[0m  [40/106], [94mLoss[0m : 2.76810
[1mStep[0m  [50/106], [94mLoss[0m : 2.58629
[1mStep[0m  [60/106], [94mLoss[0m : 2.96239
[1mStep[0m  [70/106], [94mLoss[0m : 2.86525
[1mStep[0m  [80/106], [94mLoss[0m : 2.36027
[1mStep[0m  [90/106], [94mLoss[0m : 2.54548
[1mStep[0m  [100/106], [94mLoss[0m : 2.65801

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38541
[1mStep[0m  [10/106], [94mLoss[0m : 2.96794
[1mStep[0m  [20/106], [94mLoss[0m : 2.62423
[1mStep[0m  [30/106], [94mLoss[0m : 3.05073
[1mStep[0m  [40/106], [94mLoss[0m : 2.63534
[1mStep[0m  [50/106], [94mLoss[0m : 2.63786
[1mStep[0m  [60/106], [94mLoss[0m : 2.61905
[1mStep[0m  [70/106], [94mLoss[0m : 2.74112
[1mStep[0m  [80/106], [94mLoss[0m : 3.03783
[1mStep[0m  [90/106], [94mLoss[0m : 2.56070
[1mStep[0m  [100/106], [94mLoss[0m : 2.45412

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51485
[1mStep[0m  [10/106], [94mLoss[0m : 2.50856
[1mStep[0m  [20/106], [94mLoss[0m : 2.63267
[1mStep[0m  [30/106], [94mLoss[0m : 2.51768
[1mStep[0m  [40/106], [94mLoss[0m : 2.52426
[1mStep[0m  [50/106], [94mLoss[0m : 2.76249
[1mStep[0m  [60/106], [94mLoss[0m : 2.50550
[1mStep[0m  [70/106], [94mLoss[0m : 2.73452
[1mStep[0m  [80/106], [94mLoss[0m : 2.91086
[1mStep[0m  [90/106], [94mLoss[0m : 2.79530
[1mStep[0m  [100/106], [94mLoss[0m : 2.73021

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61615
[1mStep[0m  [10/106], [94mLoss[0m : 2.33020
[1mStep[0m  [20/106], [94mLoss[0m : 3.01847
[1mStep[0m  [30/106], [94mLoss[0m : 2.26395
[1mStep[0m  [40/106], [94mLoss[0m : 2.26301
[1mStep[0m  [50/106], [94mLoss[0m : 2.04640
[1mStep[0m  [60/106], [94mLoss[0m : 2.66410
[1mStep[0m  [70/106], [94mLoss[0m : 2.89353
[1mStep[0m  [80/106], [94mLoss[0m : 2.27059
[1mStep[0m  [90/106], [94mLoss[0m : 2.45167
[1mStep[0m  [100/106], [94mLoss[0m : 2.74766

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53242
[1mStep[0m  [10/106], [94mLoss[0m : 2.60248
[1mStep[0m  [20/106], [94mLoss[0m : 2.51461
[1mStep[0m  [30/106], [94mLoss[0m : 2.68561
[1mStep[0m  [40/106], [94mLoss[0m : 3.04494
[1mStep[0m  [50/106], [94mLoss[0m : 2.51428
[1mStep[0m  [60/106], [94mLoss[0m : 2.56201
[1mStep[0m  [70/106], [94mLoss[0m : 2.78607
[1mStep[0m  [80/106], [94mLoss[0m : 2.44238
[1mStep[0m  [90/106], [94mLoss[0m : 2.52517
[1mStep[0m  [100/106], [94mLoss[0m : 2.56293

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94653
[1mStep[0m  [10/106], [94mLoss[0m : 2.68731
[1mStep[0m  [20/106], [94mLoss[0m : 2.47037
[1mStep[0m  [30/106], [94mLoss[0m : 2.46453
[1mStep[0m  [40/106], [94mLoss[0m : 2.50573
[1mStep[0m  [50/106], [94mLoss[0m : 2.44028
[1mStep[0m  [60/106], [94mLoss[0m : 2.69974
[1mStep[0m  [70/106], [94mLoss[0m : 2.46488
[1mStep[0m  [80/106], [94mLoss[0m : 2.87192
[1mStep[0m  [90/106], [94mLoss[0m : 2.72828
[1mStep[0m  [100/106], [94mLoss[0m : 2.35807

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22808
[1mStep[0m  [10/106], [94mLoss[0m : 2.68233
[1mStep[0m  [20/106], [94mLoss[0m : 2.42933
[1mStep[0m  [30/106], [94mLoss[0m : 2.70003
[1mStep[0m  [40/106], [94mLoss[0m : 2.33532
[1mStep[0m  [50/106], [94mLoss[0m : 2.58918
[1mStep[0m  [60/106], [94mLoss[0m : 2.65046
[1mStep[0m  [70/106], [94mLoss[0m : 2.79046
[1mStep[0m  [80/106], [94mLoss[0m : 2.56387
[1mStep[0m  [90/106], [94mLoss[0m : 2.59683
[1mStep[0m  [100/106], [94mLoss[0m : 2.72967

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72378
[1mStep[0m  [10/106], [94mLoss[0m : 2.65861
[1mStep[0m  [20/106], [94mLoss[0m : 2.57947
[1mStep[0m  [30/106], [94mLoss[0m : 2.27451
[1mStep[0m  [40/106], [94mLoss[0m : 2.43029
[1mStep[0m  [50/106], [94mLoss[0m : 2.67181
[1mStep[0m  [60/106], [94mLoss[0m : 2.82236
[1mStep[0m  [70/106], [94mLoss[0m : 2.36444
[1mStep[0m  [80/106], [94mLoss[0m : 2.36419
[1mStep[0m  [90/106], [94mLoss[0m : 2.68320
[1mStep[0m  [100/106], [94mLoss[0m : 2.52368

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56352
[1mStep[0m  [10/106], [94mLoss[0m : 2.43475
[1mStep[0m  [20/106], [94mLoss[0m : 2.63554
[1mStep[0m  [30/106], [94mLoss[0m : 2.66790
[1mStep[0m  [40/106], [94mLoss[0m : 2.45371
[1mStep[0m  [50/106], [94mLoss[0m : 2.66875
[1mStep[0m  [60/106], [94mLoss[0m : 2.21425
[1mStep[0m  [70/106], [94mLoss[0m : 2.19145
[1mStep[0m  [80/106], [94mLoss[0m : 2.06400
[1mStep[0m  [90/106], [94mLoss[0m : 2.50403
[1mStep[0m  [100/106], [94mLoss[0m : 2.76906

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74319
[1mStep[0m  [10/106], [94mLoss[0m : 2.56094
[1mStep[0m  [20/106], [94mLoss[0m : 2.46120
[1mStep[0m  [30/106], [94mLoss[0m : 2.49949
[1mStep[0m  [40/106], [94mLoss[0m : 2.61461
[1mStep[0m  [50/106], [94mLoss[0m : 2.49183
[1mStep[0m  [60/106], [94mLoss[0m : 2.49065
[1mStep[0m  [70/106], [94mLoss[0m : 2.64919
[1mStep[0m  [80/106], [94mLoss[0m : 2.61677
[1mStep[0m  [90/106], [94mLoss[0m : 2.53558
[1mStep[0m  [100/106], [94mLoss[0m : 2.24154

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56321
[1mStep[0m  [10/106], [94mLoss[0m : 2.90110
[1mStep[0m  [20/106], [94mLoss[0m : 2.64624
[1mStep[0m  [30/106], [94mLoss[0m : 2.49584
[1mStep[0m  [40/106], [94mLoss[0m : 2.38774
[1mStep[0m  [50/106], [94mLoss[0m : 2.29653
[1mStep[0m  [60/106], [94mLoss[0m : 2.29548
[1mStep[0m  [70/106], [94mLoss[0m : 2.34475
[1mStep[0m  [80/106], [94mLoss[0m : 2.44375
[1mStep[0m  [90/106], [94mLoss[0m : 2.37741
[1mStep[0m  [100/106], [94mLoss[0m : 2.31600

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33394
[1mStep[0m  [10/106], [94mLoss[0m : 2.56958
[1mStep[0m  [20/106], [94mLoss[0m : 2.52976
[1mStep[0m  [30/106], [94mLoss[0m : 2.23493
[1mStep[0m  [40/106], [94mLoss[0m : 2.53438
[1mStep[0m  [50/106], [94mLoss[0m : 2.42966
[1mStep[0m  [60/106], [94mLoss[0m : 2.40973
[1mStep[0m  [70/106], [94mLoss[0m : 2.56151
[1mStep[0m  [80/106], [94mLoss[0m : 2.49263
[1mStep[0m  [90/106], [94mLoss[0m : 2.69614
[1mStep[0m  [100/106], [94mLoss[0m : 2.52557

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94131
[1mStep[0m  [10/106], [94mLoss[0m : 2.36601
[1mStep[0m  [20/106], [94mLoss[0m : 2.34719
[1mStep[0m  [30/106], [94mLoss[0m : 2.50996
[1mStep[0m  [40/106], [94mLoss[0m : 2.70466
[1mStep[0m  [50/106], [94mLoss[0m : 2.58543
[1mStep[0m  [60/106], [94mLoss[0m : 2.57955
[1mStep[0m  [70/106], [94mLoss[0m : 2.45467
[1mStep[0m  [80/106], [94mLoss[0m : 2.67622
[1mStep[0m  [90/106], [94mLoss[0m : 2.49276
[1mStep[0m  [100/106], [94mLoss[0m : 2.27131

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.371, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68848
[1mStep[0m  [10/106], [94mLoss[0m : 2.58433
[1mStep[0m  [20/106], [94mLoss[0m : 2.71896
[1mStep[0m  [30/106], [94mLoss[0m : 2.62830
[1mStep[0m  [40/106], [94mLoss[0m : 2.42562
[1mStep[0m  [50/106], [94mLoss[0m : 2.61260
[1mStep[0m  [60/106], [94mLoss[0m : 2.40459
[1mStep[0m  [70/106], [94mLoss[0m : 2.45631
[1mStep[0m  [80/106], [94mLoss[0m : 2.58851
[1mStep[0m  [90/106], [94mLoss[0m : 2.55294
[1mStep[0m  [100/106], [94mLoss[0m : 2.46948

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.360, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38634
[1mStep[0m  [10/106], [94mLoss[0m : 2.25848
[1mStep[0m  [20/106], [94mLoss[0m : 2.29457
[1mStep[0m  [30/106], [94mLoss[0m : 2.32827
[1mStep[0m  [40/106], [94mLoss[0m : 2.51125
[1mStep[0m  [50/106], [94mLoss[0m : 2.73368
[1mStep[0m  [60/106], [94mLoss[0m : 2.49825
[1mStep[0m  [70/106], [94mLoss[0m : 2.57812
[1mStep[0m  [80/106], [94mLoss[0m : 2.56825
[1mStep[0m  [90/106], [94mLoss[0m : 2.58202
[1mStep[0m  [100/106], [94mLoss[0m : 2.38691

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65081
[1mStep[0m  [10/106], [94mLoss[0m : 2.70182
[1mStep[0m  [20/106], [94mLoss[0m : 2.68915
[1mStep[0m  [30/106], [94mLoss[0m : 2.54649
[1mStep[0m  [40/106], [94mLoss[0m : 2.39274
[1mStep[0m  [50/106], [94mLoss[0m : 2.43598
[1mStep[0m  [60/106], [94mLoss[0m : 2.56818
[1mStep[0m  [70/106], [94mLoss[0m : 2.23905
[1mStep[0m  [80/106], [94mLoss[0m : 2.26951
[1mStep[0m  [90/106], [94mLoss[0m : 2.82722
[1mStep[0m  [100/106], [94mLoss[0m : 2.40565

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42342
[1mStep[0m  [10/106], [94mLoss[0m : 2.20951
[1mStep[0m  [20/106], [94mLoss[0m : 2.32864
[1mStep[0m  [30/106], [94mLoss[0m : 2.34224
[1mStep[0m  [40/106], [94mLoss[0m : 2.59037
[1mStep[0m  [50/106], [94mLoss[0m : 2.28876
[1mStep[0m  [60/106], [94mLoss[0m : 2.38693
[1mStep[0m  [70/106], [94mLoss[0m : 2.72432
[1mStep[0m  [80/106], [94mLoss[0m : 2.42916
[1mStep[0m  [90/106], [94mLoss[0m : 2.40800
[1mStep[0m  [100/106], [94mLoss[0m : 2.57743

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43679
[1mStep[0m  [10/106], [94mLoss[0m : 2.63255
[1mStep[0m  [20/106], [94mLoss[0m : 2.76902
[1mStep[0m  [30/106], [94mLoss[0m : 2.39199
[1mStep[0m  [40/106], [94mLoss[0m : 2.50160
[1mStep[0m  [50/106], [94mLoss[0m : 2.15678
[1mStep[0m  [60/106], [94mLoss[0m : 2.60578
[1mStep[0m  [70/106], [94mLoss[0m : 2.59901
[1mStep[0m  [80/106], [94mLoss[0m : 2.39589
[1mStep[0m  [90/106], [94mLoss[0m : 2.31355
[1mStep[0m  [100/106], [94mLoss[0m : 2.42132

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55168
[1mStep[0m  [10/106], [94mLoss[0m : 2.63578
[1mStep[0m  [20/106], [94mLoss[0m : 2.20795
[1mStep[0m  [30/106], [94mLoss[0m : 2.55893
[1mStep[0m  [40/106], [94mLoss[0m : 2.24338
[1mStep[0m  [50/106], [94mLoss[0m : 2.41029
[1mStep[0m  [60/106], [94mLoss[0m : 2.40144
[1mStep[0m  [70/106], [94mLoss[0m : 2.43348
[1mStep[0m  [80/106], [94mLoss[0m : 2.35330
[1mStep[0m  [90/106], [94mLoss[0m : 3.08080
[1mStep[0m  [100/106], [94mLoss[0m : 2.42455

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66448
[1mStep[0m  [10/106], [94mLoss[0m : 2.36281
[1mStep[0m  [20/106], [94mLoss[0m : 2.55057
[1mStep[0m  [30/106], [94mLoss[0m : 2.63139
[1mStep[0m  [40/106], [94mLoss[0m : 2.97442
[1mStep[0m  [50/106], [94mLoss[0m : 2.81361
[1mStep[0m  [60/106], [94mLoss[0m : 2.26736
[1mStep[0m  [70/106], [94mLoss[0m : 2.65184
[1mStep[0m  [80/106], [94mLoss[0m : 2.39431
[1mStep[0m  [90/106], [94mLoss[0m : 2.48846
[1mStep[0m  [100/106], [94mLoss[0m : 2.38707

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42655
[1mStep[0m  [10/106], [94mLoss[0m : 2.32185
[1mStep[0m  [20/106], [94mLoss[0m : 2.57156
[1mStep[0m  [30/106], [94mLoss[0m : 2.64285
[1mStep[0m  [40/106], [94mLoss[0m : 2.57852
[1mStep[0m  [50/106], [94mLoss[0m : 2.18348
[1mStep[0m  [60/106], [94mLoss[0m : 2.55653
[1mStep[0m  [70/106], [94mLoss[0m : 2.54769
[1mStep[0m  [80/106], [94mLoss[0m : 2.24100
[1mStep[0m  [90/106], [94mLoss[0m : 2.13698
[1mStep[0m  [100/106], [94mLoss[0m : 2.66820

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.345
====================================

Phase 1 - Evaluation MAE:  2.344564788746384
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.74943
[1mStep[0m  [10/106], [94mLoss[0m : 2.49373
[1mStep[0m  [20/106], [94mLoss[0m : 2.42631
[1mStep[0m  [30/106], [94mLoss[0m : 3.08017
[1mStep[0m  [40/106], [94mLoss[0m : 2.53966
[1mStep[0m  [50/106], [94mLoss[0m : 2.37633
[1mStep[0m  [60/106], [94mLoss[0m : 2.46375
[1mStep[0m  [70/106], [94mLoss[0m : 2.27171
[1mStep[0m  [80/106], [94mLoss[0m : 2.44484
[1mStep[0m  [90/106], [94mLoss[0m : 2.81267
[1mStep[0m  [100/106], [94mLoss[0m : 2.64202

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44637
[1mStep[0m  [10/106], [94mLoss[0m : 2.37931
[1mStep[0m  [20/106], [94mLoss[0m : 2.69645
[1mStep[0m  [30/106], [94mLoss[0m : 2.33480
[1mStep[0m  [40/106], [94mLoss[0m : 2.32577
[1mStep[0m  [50/106], [94mLoss[0m : 2.57518
[1mStep[0m  [60/106], [94mLoss[0m : 2.52855
[1mStep[0m  [70/106], [94mLoss[0m : 2.83907
[1mStep[0m  [80/106], [94mLoss[0m : 2.33310
[1mStep[0m  [90/106], [94mLoss[0m : 2.63166
[1mStep[0m  [100/106], [94mLoss[0m : 2.67321

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36184
[1mStep[0m  [10/106], [94mLoss[0m : 2.37964
[1mStep[0m  [20/106], [94mLoss[0m : 2.50568
[1mStep[0m  [30/106], [94mLoss[0m : 2.72524
[1mStep[0m  [40/106], [94mLoss[0m : 2.46281
[1mStep[0m  [50/106], [94mLoss[0m : 2.44636
[1mStep[0m  [60/106], [94mLoss[0m : 2.39429
[1mStep[0m  [70/106], [94mLoss[0m : 2.18917
[1mStep[0m  [80/106], [94mLoss[0m : 2.25535
[1mStep[0m  [90/106], [94mLoss[0m : 2.76222
[1mStep[0m  [100/106], [94mLoss[0m : 2.49140

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32142
[1mStep[0m  [10/106], [94mLoss[0m : 2.01493
[1mStep[0m  [20/106], [94mLoss[0m : 2.33367
[1mStep[0m  [30/106], [94mLoss[0m : 2.30934
[1mStep[0m  [40/106], [94mLoss[0m : 2.04925
[1mStep[0m  [50/106], [94mLoss[0m : 2.35177
[1mStep[0m  [60/106], [94mLoss[0m : 2.32508
[1mStep[0m  [70/106], [94mLoss[0m : 2.14713
[1mStep[0m  [80/106], [94mLoss[0m : 2.26717
[1mStep[0m  [90/106], [94mLoss[0m : 2.14976
[1mStep[0m  [100/106], [94mLoss[0m : 2.19052

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07770
[1mStep[0m  [10/106], [94mLoss[0m : 2.24947
[1mStep[0m  [20/106], [94mLoss[0m : 2.09936
[1mStep[0m  [30/106], [94mLoss[0m : 2.38483
[1mStep[0m  [40/106], [94mLoss[0m : 2.14730
[1mStep[0m  [50/106], [94mLoss[0m : 2.22523
[1mStep[0m  [60/106], [94mLoss[0m : 2.35709
[1mStep[0m  [70/106], [94mLoss[0m : 2.46989
[1mStep[0m  [80/106], [94mLoss[0m : 2.17642
[1mStep[0m  [90/106], [94mLoss[0m : 2.37111
[1mStep[0m  [100/106], [94mLoss[0m : 2.32306

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44942
[1mStep[0m  [10/106], [94mLoss[0m : 2.12909
[1mStep[0m  [20/106], [94mLoss[0m : 2.32256
[1mStep[0m  [30/106], [94mLoss[0m : 2.37590
[1mStep[0m  [40/106], [94mLoss[0m : 2.03639
[1mStep[0m  [50/106], [94mLoss[0m : 2.39116
[1mStep[0m  [60/106], [94mLoss[0m : 2.29874
[1mStep[0m  [70/106], [94mLoss[0m : 2.09929
[1mStep[0m  [80/106], [94mLoss[0m : 2.30608
[1mStep[0m  [90/106], [94mLoss[0m : 2.28799
[1mStep[0m  [100/106], [94mLoss[0m : 2.47765

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.245, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17547
[1mStep[0m  [10/106], [94mLoss[0m : 2.27846
[1mStep[0m  [20/106], [94mLoss[0m : 2.07450
[1mStep[0m  [30/106], [94mLoss[0m : 2.12402
[1mStep[0m  [40/106], [94mLoss[0m : 2.25720
[1mStep[0m  [50/106], [94mLoss[0m : 2.27803
[1mStep[0m  [60/106], [94mLoss[0m : 2.44883
[1mStep[0m  [70/106], [94mLoss[0m : 2.58110
[1mStep[0m  [80/106], [94mLoss[0m : 1.94130
[1mStep[0m  [90/106], [94mLoss[0m : 2.13395
[1mStep[0m  [100/106], [94mLoss[0m : 2.14004

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14316
[1mStep[0m  [10/106], [94mLoss[0m : 2.33085
[1mStep[0m  [20/106], [94mLoss[0m : 2.23182
[1mStep[0m  [30/106], [94mLoss[0m : 2.07025
[1mStep[0m  [40/106], [94mLoss[0m : 1.97603
[1mStep[0m  [50/106], [94mLoss[0m : 2.34412
[1mStep[0m  [60/106], [94mLoss[0m : 2.36075
[1mStep[0m  [70/106], [94mLoss[0m : 2.30442
[1mStep[0m  [80/106], [94mLoss[0m : 2.01025
[1mStep[0m  [90/106], [94mLoss[0m : 2.11571
[1mStep[0m  [100/106], [94mLoss[0m : 1.99546

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07469
[1mStep[0m  [10/106], [94mLoss[0m : 1.87673
[1mStep[0m  [20/106], [94mLoss[0m : 2.07095
[1mStep[0m  [30/106], [94mLoss[0m : 2.36458
[1mStep[0m  [40/106], [94mLoss[0m : 2.32045
[1mStep[0m  [50/106], [94mLoss[0m : 2.26507
[1mStep[0m  [60/106], [94mLoss[0m : 2.43967
[1mStep[0m  [70/106], [94mLoss[0m : 2.12634
[1mStep[0m  [80/106], [94mLoss[0m : 2.02878
[1mStep[0m  [90/106], [94mLoss[0m : 2.07476
[1mStep[0m  [100/106], [94mLoss[0m : 2.14973

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98735
[1mStep[0m  [10/106], [94mLoss[0m : 1.91775
[1mStep[0m  [20/106], [94mLoss[0m : 1.94860
[1mStep[0m  [30/106], [94mLoss[0m : 1.91349
[1mStep[0m  [40/106], [94mLoss[0m : 1.99688
[1mStep[0m  [50/106], [94mLoss[0m : 1.77302
[1mStep[0m  [60/106], [94mLoss[0m : 2.08960
[1mStep[0m  [70/106], [94mLoss[0m : 2.04627
[1mStep[0m  [80/106], [94mLoss[0m : 2.09154
[1mStep[0m  [90/106], [94mLoss[0m : 1.92925
[1mStep[0m  [100/106], [94mLoss[0m : 2.03658

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12194
[1mStep[0m  [10/106], [94mLoss[0m : 1.88883
[1mStep[0m  [20/106], [94mLoss[0m : 2.24315
[1mStep[0m  [30/106], [94mLoss[0m : 1.87988
[1mStep[0m  [40/106], [94mLoss[0m : 2.18582
[1mStep[0m  [50/106], [94mLoss[0m : 1.92268
[1mStep[0m  [60/106], [94mLoss[0m : 1.84381
[1mStep[0m  [70/106], [94mLoss[0m : 2.15275
[1mStep[0m  [80/106], [94mLoss[0m : 2.35381
[1mStep[0m  [90/106], [94mLoss[0m : 2.08898
[1mStep[0m  [100/106], [94mLoss[0m : 1.93755

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.017, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.09687
[1mStep[0m  [10/106], [94mLoss[0m : 2.17855
[1mStep[0m  [20/106], [94mLoss[0m : 2.04432
[1mStep[0m  [30/106], [94mLoss[0m : 1.83879
[1mStep[0m  [40/106], [94mLoss[0m : 1.87788
[1mStep[0m  [50/106], [94mLoss[0m : 1.83278
[1mStep[0m  [60/106], [94mLoss[0m : 2.18714
[1mStep[0m  [70/106], [94mLoss[0m : 2.08525
[1mStep[0m  [80/106], [94mLoss[0m : 1.74524
[1mStep[0m  [90/106], [94mLoss[0m : 2.13871
[1mStep[0m  [100/106], [94mLoss[0m : 2.05103

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77294
[1mStep[0m  [10/106], [94mLoss[0m : 1.91257
[1mStep[0m  [20/106], [94mLoss[0m : 1.97778
[1mStep[0m  [30/106], [94mLoss[0m : 1.91367
[1mStep[0m  [40/106], [94mLoss[0m : 1.86451
[1mStep[0m  [50/106], [94mLoss[0m : 2.05215
[1mStep[0m  [60/106], [94mLoss[0m : 2.12004
[1mStep[0m  [70/106], [94mLoss[0m : 1.71768
[1mStep[0m  [80/106], [94mLoss[0m : 1.79083
[1mStep[0m  [90/106], [94mLoss[0m : 1.93977
[1mStep[0m  [100/106], [94mLoss[0m : 2.02827

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.97094
[1mStep[0m  [10/106], [94mLoss[0m : 1.81483
[1mStep[0m  [20/106], [94mLoss[0m : 1.59933
[1mStep[0m  [30/106], [94mLoss[0m : 2.04629
[1mStep[0m  [40/106], [94mLoss[0m : 1.82620
[1mStep[0m  [50/106], [94mLoss[0m : 1.88681
[1mStep[0m  [60/106], [94mLoss[0m : 1.90210
[1mStep[0m  [70/106], [94mLoss[0m : 1.86102
[1mStep[0m  [80/106], [94mLoss[0m : 2.05618
[1mStep[0m  [90/106], [94mLoss[0m : 1.92524
[1mStep[0m  [100/106], [94mLoss[0m : 1.93712

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.891, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72655
[1mStep[0m  [10/106], [94mLoss[0m : 1.73424
[1mStep[0m  [20/106], [94mLoss[0m : 1.41601
[1mStep[0m  [30/106], [94mLoss[0m : 1.86546
[1mStep[0m  [40/106], [94mLoss[0m : 1.79458
[1mStep[0m  [50/106], [94mLoss[0m : 1.90610
[1mStep[0m  [60/106], [94mLoss[0m : 1.71498
[1mStep[0m  [70/106], [94mLoss[0m : 1.98823
[1mStep[0m  [80/106], [94mLoss[0m : 2.06170
[1mStep[0m  [90/106], [94mLoss[0m : 1.96414
[1mStep[0m  [100/106], [94mLoss[0m : 1.76076

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57009
[1mStep[0m  [10/106], [94mLoss[0m : 1.62656
[1mStep[0m  [20/106], [94mLoss[0m : 1.74402
[1mStep[0m  [30/106], [94mLoss[0m : 1.79355
[1mStep[0m  [40/106], [94mLoss[0m : 1.87689
[1mStep[0m  [50/106], [94mLoss[0m : 1.74274
[1mStep[0m  [60/106], [94mLoss[0m : 1.75038
[1mStep[0m  [70/106], [94mLoss[0m : 1.86467
[1mStep[0m  [80/106], [94mLoss[0m : 2.11706
[1mStep[0m  [90/106], [94mLoss[0m : 1.94367
[1mStep[0m  [100/106], [94mLoss[0m : 1.65806

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49083
[1mStep[0m  [10/106], [94mLoss[0m : 1.68112
[1mStep[0m  [20/106], [94mLoss[0m : 1.76758
[1mStep[0m  [30/106], [94mLoss[0m : 1.43219
[1mStep[0m  [40/106], [94mLoss[0m : 2.04518
[1mStep[0m  [50/106], [94mLoss[0m : 1.74333
[1mStep[0m  [60/106], [94mLoss[0m : 1.72483
[1mStep[0m  [70/106], [94mLoss[0m : 1.64282
[1mStep[0m  [80/106], [94mLoss[0m : 1.67947
[1mStep[0m  [90/106], [94mLoss[0m : 1.59624
[1mStep[0m  [100/106], [94mLoss[0m : 1.71083

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49267
[1mStep[0m  [10/106], [94mLoss[0m : 1.72350
[1mStep[0m  [20/106], [94mLoss[0m : 1.80215
[1mStep[0m  [30/106], [94mLoss[0m : 1.65094
[1mStep[0m  [40/106], [94mLoss[0m : 1.70292
[1mStep[0m  [50/106], [94mLoss[0m : 1.97445
[1mStep[0m  [60/106], [94mLoss[0m : 1.67297
[1mStep[0m  [70/106], [94mLoss[0m : 1.98361
[1mStep[0m  [80/106], [94mLoss[0m : 1.80761
[1mStep[0m  [90/106], [94mLoss[0m : 1.74776
[1mStep[0m  [100/106], [94mLoss[0m : 1.55899

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83267
[1mStep[0m  [10/106], [94mLoss[0m : 1.71075
[1mStep[0m  [20/106], [94mLoss[0m : 1.81496
[1mStep[0m  [30/106], [94mLoss[0m : 1.62859
[1mStep[0m  [40/106], [94mLoss[0m : 1.88314
[1mStep[0m  [50/106], [94mLoss[0m : 1.72704
[1mStep[0m  [60/106], [94mLoss[0m : 1.74285
[1mStep[0m  [70/106], [94mLoss[0m : 1.92974
[1mStep[0m  [80/106], [94mLoss[0m : 1.86313
[1mStep[0m  [90/106], [94mLoss[0m : 1.54525
[1mStep[0m  [100/106], [94mLoss[0m : 1.88505

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73044
[1mStep[0m  [10/106], [94mLoss[0m : 1.77792
[1mStep[0m  [20/106], [94mLoss[0m : 1.74992
[1mStep[0m  [30/106], [94mLoss[0m : 1.84442
[1mStep[0m  [40/106], [94mLoss[0m : 1.56721
[1mStep[0m  [50/106], [94mLoss[0m : 1.55563
[1mStep[0m  [60/106], [94mLoss[0m : 1.98863
[1mStep[0m  [70/106], [94mLoss[0m : 1.83609
[1mStep[0m  [80/106], [94mLoss[0m : 1.65637
[1mStep[0m  [90/106], [94mLoss[0m : 1.81928
[1mStep[0m  [100/106], [94mLoss[0m : 1.63792

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.448, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73213
[1mStep[0m  [10/106], [94mLoss[0m : 1.65811
[1mStep[0m  [20/106], [94mLoss[0m : 1.72825
[1mStep[0m  [30/106], [94mLoss[0m : 1.65550
[1mStep[0m  [40/106], [94mLoss[0m : 1.67276
[1mStep[0m  [50/106], [94mLoss[0m : 1.62195
[1mStep[0m  [60/106], [94mLoss[0m : 1.70863
[1mStep[0m  [70/106], [94mLoss[0m : 1.71189
[1mStep[0m  [80/106], [94mLoss[0m : 1.46986
[1mStep[0m  [90/106], [94mLoss[0m : 1.54365
[1mStep[0m  [100/106], [94mLoss[0m : 1.51487

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58282
[1mStep[0m  [10/106], [94mLoss[0m : 1.69580
[1mStep[0m  [20/106], [94mLoss[0m : 1.70328
[1mStep[0m  [30/106], [94mLoss[0m : 1.53358
[1mStep[0m  [40/106], [94mLoss[0m : 1.52425
[1mStep[0m  [50/106], [94mLoss[0m : 1.91178
[1mStep[0m  [60/106], [94mLoss[0m : 1.51309
[1mStep[0m  [70/106], [94mLoss[0m : 1.66117
[1mStep[0m  [80/106], [94mLoss[0m : 1.63135
[1mStep[0m  [90/106], [94mLoss[0m : 1.46677
[1mStep[0m  [100/106], [94mLoss[0m : 1.61042

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64159
[1mStep[0m  [10/106], [94mLoss[0m : 1.67782
[1mStep[0m  [20/106], [94mLoss[0m : 1.56972
[1mStep[0m  [30/106], [94mLoss[0m : 1.62420
[1mStep[0m  [40/106], [94mLoss[0m : 1.52302
[1mStep[0m  [50/106], [94mLoss[0m : 1.45649
[1mStep[0m  [60/106], [94mLoss[0m : 1.64782
[1mStep[0m  [70/106], [94mLoss[0m : 1.57958
[1mStep[0m  [80/106], [94mLoss[0m : 1.60735
[1mStep[0m  [90/106], [94mLoss[0m : 1.71848
[1mStep[0m  [100/106], [94mLoss[0m : 1.76147

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.529, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39526
[1mStep[0m  [10/106], [94mLoss[0m : 1.80447
[1mStep[0m  [20/106], [94mLoss[0m : 1.56481
[1mStep[0m  [30/106], [94mLoss[0m : 1.40756
[1mStep[0m  [40/106], [94mLoss[0m : 1.80227
[1mStep[0m  [50/106], [94mLoss[0m : 1.65916
[1mStep[0m  [60/106], [94mLoss[0m : 1.53068
[1mStep[0m  [70/106], [94mLoss[0m : 1.44677
[1mStep[0m  [80/106], [94mLoss[0m : 1.53281
[1mStep[0m  [90/106], [94mLoss[0m : 1.53644
[1mStep[0m  [100/106], [94mLoss[0m : 1.49814

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.461, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48404
[1mStep[0m  [10/106], [94mLoss[0m : 1.41219
[1mStep[0m  [20/106], [94mLoss[0m : 1.34475
[1mStep[0m  [30/106], [94mLoss[0m : 1.43209
[1mStep[0m  [40/106], [94mLoss[0m : 1.79199
[1mStep[0m  [50/106], [94mLoss[0m : 1.65510
[1mStep[0m  [60/106], [94mLoss[0m : 1.82794
[1mStep[0m  [70/106], [94mLoss[0m : 1.60973
[1mStep[0m  [80/106], [94mLoss[0m : 1.65804
[1mStep[0m  [90/106], [94mLoss[0m : 1.66672
[1mStep[0m  [100/106], [94mLoss[0m : 1.31473

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.515, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.34045
[1mStep[0m  [10/106], [94mLoss[0m : 1.55161
[1mStep[0m  [20/106], [94mLoss[0m : 1.62103
[1mStep[0m  [30/106], [94mLoss[0m : 1.52423
[1mStep[0m  [40/106], [94mLoss[0m : 1.33800
[1mStep[0m  [50/106], [94mLoss[0m : 1.53126
[1mStep[0m  [60/106], [94mLoss[0m : 1.50442
[1mStep[0m  [70/106], [94mLoss[0m : 1.35667
[1mStep[0m  [80/106], [94mLoss[0m : 1.49602
[1mStep[0m  [90/106], [94mLoss[0m : 1.50815
[1mStep[0m  [100/106], [94mLoss[0m : 1.75646

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.524, [92mTest[0m: 2.481, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59782
[1mStep[0m  [10/106], [94mLoss[0m : 1.25664
[1mStep[0m  [20/106], [94mLoss[0m : 1.41381
[1mStep[0m  [30/106], [94mLoss[0m : 1.74663
[1mStep[0m  [40/106], [94mLoss[0m : 1.55181
[1mStep[0m  [50/106], [94mLoss[0m : 1.43693
[1mStep[0m  [60/106], [94mLoss[0m : 1.26842
[1mStep[0m  [70/106], [94mLoss[0m : 1.36273
[1mStep[0m  [80/106], [94mLoss[0m : 1.54385
[1mStep[0m  [90/106], [94mLoss[0m : 1.65423
[1mStep[0m  [100/106], [94mLoss[0m : 1.43351

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.571, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.51717
[1mStep[0m  [10/106], [94mLoss[0m : 1.56048
[1mStep[0m  [20/106], [94mLoss[0m : 1.36772
[1mStep[0m  [30/106], [94mLoss[0m : 1.34791
[1mStep[0m  [40/106], [94mLoss[0m : 1.40128
[1mStep[0m  [50/106], [94mLoss[0m : 1.52372
[1mStep[0m  [60/106], [94mLoss[0m : 1.54141
[1mStep[0m  [70/106], [94mLoss[0m : 1.59621
[1mStep[0m  [80/106], [94mLoss[0m : 1.58436
[1mStep[0m  [90/106], [94mLoss[0m : 1.29968
[1mStep[0m  [100/106], [94mLoss[0m : 1.45116

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.483, [92mTest[0m: 2.466, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.80751
[1mStep[0m  [10/106], [94mLoss[0m : 1.40358
[1mStep[0m  [20/106], [94mLoss[0m : 1.46968
[1mStep[0m  [30/106], [94mLoss[0m : 1.48494
[1mStep[0m  [40/106], [94mLoss[0m : 1.71290
[1mStep[0m  [50/106], [94mLoss[0m : 1.26706
[1mStep[0m  [60/106], [94mLoss[0m : 1.31827
[1mStep[0m  [70/106], [94mLoss[0m : 1.50607
[1mStep[0m  [80/106], [94mLoss[0m : 1.44053
[1mStep[0m  [90/106], [94mLoss[0m : 1.55317
[1mStep[0m  [100/106], [94mLoss[0m : 1.42924

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.474, [92mTest[0m: 2.510, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42697
[1mStep[0m  [10/106], [94mLoss[0m : 1.51563
[1mStep[0m  [20/106], [94mLoss[0m : 1.40642
[1mStep[0m  [30/106], [94mLoss[0m : 1.51771
[1mStep[0m  [40/106], [94mLoss[0m : 1.45879
[1mStep[0m  [50/106], [94mLoss[0m : 1.49224
[1mStep[0m  [60/106], [94mLoss[0m : 1.40799
[1mStep[0m  [70/106], [94mLoss[0m : 1.39773
[1mStep[0m  [80/106], [94mLoss[0m : 1.40089
[1mStep[0m  [90/106], [94mLoss[0m : 1.62001
[1mStep[0m  [100/106], [94mLoss[0m : 1.65635

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.460, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.495
====================================

Phase 2 - Evaluation MAE:  2.4948508424578972
MAE score P1       2.344565
MAE score P2       2.494851
loss               1.460458
learning_rate          0.01
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay          0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.57884
[1mStep[0m  [10/106], [94mLoss[0m : 10.94385
[1mStep[0m  [20/106], [94mLoss[0m : 10.21173
[1mStep[0m  [30/106], [94mLoss[0m : 10.17128
[1mStep[0m  [40/106], [94mLoss[0m : 10.15678
[1mStep[0m  [50/106], [94mLoss[0m : 10.17497
[1mStep[0m  [60/106], [94mLoss[0m : 9.78950
[1mStep[0m  [70/106], [94mLoss[0m : 9.35352
[1mStep[0m  [80/106], [94mLoss[0m : 9.30737
[1mStep[0m  [90/106], [94mLoss[0m : 9.08264
[1mStep[0m  [100/106], [94mLoss[0m : 8.88113

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.829, [92mTest[0m: 10.801, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.98073
[1mStep[0m  [10/106], [94mLoss[0m : 8.20647
[1mStep[0m  [20/106], [94mLoss[0m : 7.75961
[1mStep[0m  [30/106], [94mLoss[0m : 7.65913
[1mStep[0m  [40/106], [94mLoss[0m : 7.44936
[1mStep[0m  [50/106], [94mLoss[0m : 7.62471
[1mStep[0m  [60/106], [94mLoss[0m : 6.60638
[1mStep[0m  [70/106], [94mLoss[0m : 6.87876
[1mStep[0m  [80/106], [94mLoss[0m : 6.21898
[1mStep[0m  [90/106], [94mLoss[0m : 6.35614
[1mStep[0m  [100/106], [94mLoss[0m : 6.05337

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.309, [92mTest[0m: 7.983, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.21730
[1mStep[0m  [10/106], [94mLoss[0m : 5.52061
[1mStep[0m  [20/106], [94mLoss[0m : 5.64693
[1mStep[0m  [30/106], [94mLoss[0m : 5.59490
[1mStep[0m  [40/106], [94mLoss[0m : 5.55152
[1mStep[0m  [50/106], [94mLoss[0m : 4.78804
[1mStep[0m  [60/106], [94mLoss[0m : 5.34635
[1mStep[0m  [70/106], [94mLoss[0m : 4.71780
[1mStep[0m  [80/106], [94mLoss[0m : 4.50158
[1mStep[0m  [90/106], [94mLoss[0m : 4.30627
[1mStep[0m  [100/106], [94mLoss[0m : 4.17944

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.907, [92mTest[0m: 5.200, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.44469
[1mStep[0m  [10/106], [94mLoss[0m : 2.93288
[1mStep[0m  [20/106], [94mLoss[0m : 3.35802
[1mStep[0m  [30/106], [94mLoss[0m : 2.86934
[1mStep[0m  [40/106], [94mLoss[0m : 2.85912
[1mStep[0m  [50/106], [94mLoss[0m : 2.69754
[1mStep[0m  [60/106], [94mLoss[0m : 3.03677
[1mStep[0m  [70/106], [94mLoss[0m : 2.80225
[1mStep[0m  [80/106], [94mLoss[0m : 2.75193
[1mStep[0m  [90/106], [94mLoss[0m : 2.55666
[1mStep[0m  [100/106], [94mLoss[0m : 2.50358

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.907, [92mTest[0m: 2.962, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51008
[1mStep[0m  [10/106], [94mLoss[0m : 2.59303
[1mStep[0m  [20/106], [94mLoss[0m : 2.67153
[1mStep[0m  [30/106], [94mLoss[0m : 2.71081
[1mStep[0m  [40/106], [94mLoss[0m : 2.48668
[1mStep[0m  [50/106], [94mLoss[0m : 2.76600
[1mStep[0m  [60/106], [94mLoss[0m : 2.24447
[1mStep[0m  [70/106], [94mLoss[0m : 2.41419
[1mStep[0m  [80/106], [94mLoss[0m : 2.95492
[1mStep[0m  [90/106], [94mLoss[0m : 2.72628
[1mStep[0m  [100/106], [94mLoss[0m : 2.78922

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.542, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48890
[1mStep[0m  [10/106], [94mLoss[0m : 2.68384
[1mStep[0m  [20/106], [94mLoss[0m : 2.57897
[1mStep[0m  [30/106], [94mLoss[0m : 2.62301
[1mStep[0m  [40/106], [94mLoss[0m : 2.55124
[1mStep[0m  [50/106], [94mLoss[0m : 2.53203
[1mStep[0m  [60/106], [94mLoss[0m : 2.46632
[1mStep[0m  [70/106], [94mLoss[0m : 2.56872
[1mStep[0m  [80/106], [94mLoss[0m : 2.83973
[1mStep[0m  [90/106], [94mLoss[0m : 2.80503
[1mStep[0m  [100/106], [94mLoss[0m : 2.53255

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45070
[1mStep[0m  [10/106], [94mLoss[0m : 2.72519
[1mStep[0m  [20/106], [94mLoss[0m : 2.30826
[1mStep[0m  [30/106], [94mLoss[0m : 2.67715
[1mStep[0m  [40/106], [94mLoss[0m : 2.45253
[1mStep[0m  [50/106], [94mLoss[0m : 2.45606
[1mStep[0m  [60/106], [94mLoss[0m : 2.75018
[1mStep[0m  [70/106], [94mLoss[0m : 2.82221
[1mStep[0m  [80/106], [94mLoss[0m : 2.48225
[1mStep[0m  [90/106], [94mLoss[0m : 2.71961
[1mStep[0m  [100/106], [94mLoss[0m : 2.61039

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68461
[1mStep[0m  [10/106], [94mLoss[0m : 2.52751
[1mStep[0m  [20/106], [94mLoss[0m : 2.38193
[1mStep[0m  [30/106], [94mLoss[0m : 2.56896
[1mStep[0m  [40/106], [94mLoss[0m : 2.75758
[1mStep[0m  [50/106], [94mLoss[0m : 2.79433
[1mStep[0m  [60/106], [94mLoss[0m : 2.47107
[1mStep[0m  [70/106], [94mLoss[0m : 2.32372
[1mStep[0m  [80/106], [94mLoss[0m : 2.52244
[1mStep[0m  [90/106], [94mLoss[0m : 2.63839
[1mStep[0m  [100/106], [94mLoss[0m : 2.83479

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71243
[1mStep[0m  [10/106], [94mLoss[0m : 2.56348
[1mStep[0m  [20/106], [94mLoss[0m : 2.58205
[1mStep[0m  [30/106], [94mLoss[0m : 2.54972
[1mStep[0m  [40/106], [94mLoss[0m : 2.42885
[1mStep[0m  [50/106], [94mLoss[0m : 2.53128
[1mStep[0m  [60/106], [94mLoss[0m : 2.46280
[1mStep[0m  [70/106], [94mLoss[0m : 2.58532
[1mStep[0m  [80/106], [94mLoss[0m : 2.38740
[1mStep[0m  [90/106], [94mLoss[0m : 2.53639
[1mStep[0m  [100/106], [94mLoss[0m : 2.49346

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46664
[1mStep[0m  [10/106], [94mLoss[0m : 2.40207
[1mStep[0m  [20/106], [94mLoss[0m : 2.55277
[1mStep[0m  [30/106], [94mLoss[0m : 2.43381
[1mStep[0m  [40/106], [94mLoss[0m : 2.57530
[1mStep[0m  [50/106], [94mLoss[0m : 2.48530
[1mStep[0m  [60/106], [94mLoss[0m : 2.56993
[1mStep[0m  [70/106], [94mLoss[0m : 2.29263
[1mStep[0m  [80/106], [94mLoss[0m : 2.47037
[1mStep[0m  [90/106], [94mLoss[0m : 2.63834
[1mStep[0m  [100/106], [94mLoss[0m : 2.36716

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49178
[1mStep[0m  [10/106], [94mLoss[0m : 2.54629
[1mStep[0m  [20/106], [94mLoss[0m : 2.68355
[1mStep[0m  [30/106], [94mLoss[0m : 2.57556
[1mStep[0m  [40/106], [94mLoss[0m : 2.59721
[1mStep[0m  [50/106], [94mLoss[0m : 2.62532
[1mStep[0m  [60/106], [94mLoss[0m : 2.36407
[1mStep[0m  [70/106], [94mLoss[0m : 2.47384
[1mStep[0m  [80/106], [94mLoss[0m : 2.52150
[1mStep[0m  [90/106], [94mLoss[0m : 2.45696
[1mStep[0m  [100/106], [94mLoss[0m : 2.42410

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55320
[1mStep[0m  [10/106], [94mLoss[0m : 2.30100
[1mStep[0m  [20/106], [94mLoss[0m : 2.56926
[1mStep[0m  [30/106], [94mLoss[0m : 2.56211
[1mStep[0m  [40/106], [94mLoss[0m : 2.34611
[1mStep[0m  [50/106], [94mLoss[0m : 2.39036
[1mStep[0m  [60/106], [94mLoss[0m : 2.58020
[1mStep[0m  [70/106], [94mLoss[0m : 2.53044
[1mStep[0m  [80/106], [94mLoss[0m : 2.43705
[1mStep[0m  [90/106], [94mLoss[0m : 2.44417
[1mStep[0m  [100/106], [94mLoss[0m : 2.84236

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10948
[1mStep[0m  [10/106], [94mLoss[0m : 2.41725
[1mStep[0m  [20/106], [94mLoss[0m : 2.39994
[1mStep[0m  [30/106], [94mLoss[0m : 2.36811
[1mStep[0m  [40/106], [94mLoss[0m : 2.53421
[1mStep[0m  [50/106], [94mLoss[0m : 2.31282
[1mStep[0m  [60/106], [94mLoss[0m : 2.39537
[1mStep[0m  [70/106], [94mLoss[0m : 2.38818
[1mStep[0m  [80/106], [94mLoss[0m : 2.25990
[1mStep[0m  [90/106], [94mLoss[0m : 2.02529
[1mStep[0m  [100/106], [94mLoss[0m : 2.39194

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37632
[1mStep[0m  [10/106], [94mLoss[0m : 2.51481
[1mStep[0m  [20/106], [94mLoss[0m : 2.40690
[1mStep[0m  [30/106], [94mLoss[0m : 2.47742
[1mStep[0m  [40/106], [94mLoss[0m : 2.52013
[1mStep[0m  [50/106], [94mLoss[0m : 2.63217
[1mStep[0m  [60/106], [94mLoss[0m : 2.50725
[1mStep[0m  [70/106], [94mLoss[0m : 2.43084
[1mStep[0m  [80/106], [94mLoss[0m : 2.31490
[1mStep[0m  [90/106], [94mLoss[0m : 2.41274
[1mStep[0m  [100/106], [94mLoss[0m : 2.18983

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32172
[1mStep[0m  [10/106], [94mLoss[0m : 2.52701
[1mStep[0m  [20/106], [94mLoss[0m : 2.81459
[1mStep[0m  [30/106], [94mLoss[0m : 2.35305
[1mStep[0m  [40/106], [94mLoss[0m : 2.47592
[1mStep[0m  [50/106], [94mLoss[0m : 2.46861
[1mStep[0m  [60/106], [94mLoss[0m : 2.26648
[1mStep[0m  [70/106], [94mLoss[0m : 2.35335
[1mStep[0m  [80/106], [94mLoss[0m : 2.27915
[1mStep[0m  [90/106], [94mLoss[0m : 2.47915
[1mStep[0m  [100/106], [94mLoss[0m : 2.36942

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32069
[1mStep[0m  [10/106], [94mLoss[0m : 2.35920
[1mStep[0m  [20/106], [94mLoss[0m : 2.76873
[1mStep[0m  [30/106], [94mLoss[0m : 2.55195
[1mStep[0m  [40/106], [94mLoss[0m : 2.76435
[1mStep[0m  [50/106], [94mLoss[0m : 2.41978
[1mStep[0m  [60/106], [94mLoss[0m : 2.62295
[1mStep[0m  [70/106], [94mLoss[0m : 2.22520
[1mStep[0m  [80/106], [94mLoss[0m : 2.47196
[1mStep[0m  [90/106], [94mLoss[0m : 2.55301
[1mStep[0m  [100/106], [94mLoss[0m : 2.04975

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40977
[1mStep[0m  [10/106], [94mLoss[0m : 2.37904
[1mStep[0m  [20/106], [94mLoss[0m : 2.40207
[1mStep[0m  [30/106], [94mLoss[0m : 2.28017
[1mStep[0m  [40/106], [94mLoss[0m : 2.31193
[1mStep[0m  [50/106], [94mLoss[0m : 2.65985
[1mStep[0m  [60/106], [94mLoss[0m : 2.62156
[1mStep[0m  [70/106], [94mLoss[0m : 2.43007
[1mStep[0m  [80/106], [94mLoss[0m : 2.40575
[1mStep[0m  [90/106], [94mLoss[0m : 2.47752
[1mStep[0m  [100/106], [94mLoss[0m : 2.22578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26022
[1mStep[0m  [10/106], [94mLoss[0m : 2.53473
[1mStep[0m  [20/106], [94mLoss[0m : 2.57456
[1mStep[0m  [30/106], [94mLoss[0m : 2.55865
[1mStep[0m  [40/106], [94mLoss[0m : 2.48844
[1mStep[0m  [50/106], [94mLoss[0m : 2.12800
[1mStep[0m  [60/106], [94mLoss[0m : 2.43049
[1mStep[0m  [70/106], [94mLoss[0m : 2.16525
[1mStep[0m  [80/106], [94mLoss[0m : 2.57642
[1mStep[0m  [90/106], [94mLoss[0m : 2.48673
[1mStep[0m  [100/106], [94mLoss[0m : 2.35148

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41028
[1mStep[0m  [10/106], [94mLoss[0m : 2.14523
[1mStep[0m  [20/106], [94mLoss[0m : 2.47563
[1mStep[0m  [30/106], [94mLoss[0m : 2.51155
[1mStep[0m  [40/106], [94mLoss[0m : 1.92938
[1mStep[0m  [50/106], [94mLoss[0m : 2.46542
[1mStep[0m  [60/106], [94mLoss[0m : 2.30187
[1mStep[0m  [70/106], [94mLoss[0m : 2.48454
[1mStep[0m  [80/106], [94mLoss[0m : 2.42237
[1mStep[0m  [90/106], [94mLoss[0m : 2.23379
[1mStep[0m  [100/106], [94mLoss[0m : 2.50772

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36112
[1mStep[0m  [10/106], [94mLoss[0m : 2.56299
[1mStep[0m  [20/106], [94mLoss[0m : 2.75569
[1mStep[0m  [30/106], [94mLoss[0m : 2.42676
[1mStep[0m  [40/106], [94mLoss[0m : 2.34442
[1mStep[0m  [50/106], [94mLoss[0m : 2.44286
[1mStep[0m  [60/106], [94mLoss[0m : 2.15138
[1mStep[0m  [70/106], [94mLoss[0m : 2.39006
[1mStep[0m  [80/106], [94mLoss[0m : 2.77144
[1mStep[0m  [90/106], [94mLoss[0m : 2.27299
[1mStep[0m  [100/106], [94mLoss[0m : 2.77838

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.395, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41365
[1mStep[0m  [10/106], [94mLoss[0m : 2.56236
[1mStep[0m  [20/106], [94mLoss[0m : 2.37123
[1mStep[0m  [30/106], [94mLoss[0m : 2.18892
[1mStep[0m  [40/106], [94mLoss[0m : 2.58408
[1mStep[0m  [50/106], [94mLoss[0m : 2.69877
[1mStep[0m  [60/106], [94mLoss[0m : 2.41917
[1mStep[0m  [70/106], [94mLoss[0m : 2.50154
[1mStep[0m  [80/106], [94mLoss[0m : 2.57555
[1mStep[0m  [90/106], [94mLoss[0m : 2.43042
[1mStep[0m  [100/106], [94mLoss[0m : 2.66598

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.378, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44224
[1mStep[0m  [10/106], [94mLoss[0m : 2.30420
[1mStep[0m  [20/106], [94mLoss[0m : 2.72575
[1mStep[0m  [30/106], [94mLoss[0m : 2.04472
[1mStep[0m  [40/106], [94mLoss[0m : 2.27563
[1mStep[0m  [50/106], [94mLoss[0m : 2.50382
[1mStep[0m  [60/106], [94mLoss[0m : 2.77198
[1mStep[0m  [70/106], [94mLoss[0m : 2.48256
[1mStep[0m  [80/106], [94mLoss[0m : 2.16258
[1mStep[0m  [90/106], [94mLoss[0m : 2.21591
[1mStep[0m  [100/106], [94mLoss[0m : 2.17733

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.398, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33213
[1mStep[0m  [10/106], [94mLoss[0m : 2.55313
[1mStep[0m  [20/106], [94mLoss[0m : 2.14918
[1mStep[0m  [30/106], [94mLoss[0m : 2.40620
[1mStep[0m  [40/106], [94mLoss[0m : 2.15940
[1mStep[0m  [50/106], [94mLoss[0m : 2.51186
[1mStep[0m  [60/106], [94mLoss[0m : 2.52208
[1mStep[0m  [70/106], [94mLoss[0m : 2.56268
[1mStep[0m  [80/106], [94mLoss[0m : 2.24422
[1mStep[0m  [90/106], [94mLoss[0m : 2.31621
[1mStep[0m  [100/106], [94mLoss[0m : 2.56334

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51602
[1mStep[0m  [10/106], [94mLoss[0m : 2.02772
[1mStep[0m  [20/106], [94mLoss[0m : 2.60703
[1mStep[0m  [30/106], [94mLoss[0m : 2.40344
[1mStep[0m  [40/106], [94mLoss[0m : 2.16229
[1mStep[0m  [50/106], [94mLoss[0m : 2.12056
[1mStep[0m  [60/106], [94mLoss[0m : 2.17754
[1mStep[0m  [70/106], [94mLoss[0m : 2.51936
[1mStep[0m  [80/106], [94mLoss[0m : 2.18505
[1mStep[0m  [90/106], [94mLoss[0m : 2.47722
[1mStep[0m  [100/106], [94mLoss[0m : 2.20449

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10459
[1mStep[0m  [10/106], [94mLoss[0m : 2.10840
[1mStep[0m  [20/106], [94mLoss[0m : 2.18788
[1mStep[0m  [30/106], [94mLoss[0m : 2.37632
[1mStep[0m  [40/106], [94mLoss[0m : 2.22510
[1mStep[0m  [50/106], [94mLoss[0m : 2.81048
[1mStep[0m  [60/106], [94mLoss[0m : 2.34057
[1mStep[0m  [70/106], [94mLoss[0m : 2.31803
[1mStep[0m  [80/106], [94mLoss[0m : 2.60298
[1mStep[0m  [90/106], [94mLoss[0m : 2.41122
[1mStep[0m  [100/106], [94mLoss[0m : 2.23327

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.385, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46058
[1mStep[0m  [10/106], [94mLoss[0m : 2.49344
[1mStep[0m  [20/106], [94mLoss[0m : 2.27749
[1mStep[0m  [30/106], [94mLoss[0m : 2.48984
[1mStep[0m  [40/106], [94mLoss[0m : 2.93018
[1mStep[0m  [50/106], [94mLoss[0m : 2.54101
[1mStep[0m  [60/106], [94mLoss[0m : 2.62255
[1mStep[0m  [70/106], [94mLoss[0m : 2.26457
[1mStep[0m  [80/106], [94mLoss[0m : 2.33667
[1mStep[0m  [90/106], [94mLoss[0m : 2.20578
[1mStep[0m  [100/106], [94mLoss[0m : 2.45959

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.402, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22224
[1mStep[0m  [10/106], [94mLoss[0m : 2.19758
[1mStep[0m  [20/106], [94mLoss[0m : 2.50285
[1mStep[0m  [30/106], [94mLoss[0m : 2.59058
[1mStep[0m  [40/106], [94mLoss[0m : 2.33458
[1mStep[0m  [50/106], [94mLoss[0m : 2.21640
[1mStep[0m  [60/106], [94mLoss[0m : 2.71498
[1mStep[0m  [70/106], [94mLoss[0m : 2.35150
[1mStep[0m  [80/106], [94mLoss[0m : 2.60041
[1mStep[0m  [90/106], [94mLoss[0m : 2.18604
[1mStep[0m  [100/106], [94mLoss[0m : 2.44249

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14199
[1mStep[0m  [10/106], [94mLoss[0m : 2.73301
[1mStep[0m  [20/106], [94mLoss[0m : 2.61389
[1mStep[0m  [30/106], [94mLoss[0m : 2.58397
[1mStep[0m  [40/106], [94mLoss[0m : 2.43417
[1mStep[0m  [50/106], [94mLoss[0m : 2.69296
[1mStep[0m  [60/106], [94mLoss[0m : 2.17204
[1mStep[0m  [70/106], [94mLoss[0m : 2.39349
[1mStep[0m  [80/106], [94mLoss[0m : 2.33479
[1mStep[0m  [90/106], [94mLoss[0m : 2.25790
[1mStep[0m  [100/106], [94mLoss[0m : 2.32321

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.397, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42585
[1mStep[0m  [10/106], [94mLoss[0m : 2.17679
[1mStep[0m  [20/106], [94mLoss[0m : 2.46071
[1mStep[0m  [30/106], [94mLoss[0m : 2.29718
[1mStep[0m  [40/106], [94mLoss[0m : 2.23034
[1mStep[0m  [50/106], [94mLoss[0m : 2.27537
[1mStep[0m  [60/106], [94mLoss[0m : 2.28583
[1mStep[0m  [70/106], [94mLoss[0m : 2.44295
[1mStep[0m  [80/106], [94mLoss[0m : 2.47692
[1mStep[0m  [90/106], [94mLoss[0m : 2.30849
[1mStep[0m  [100/106], [94mLoss[0m : 2.43528

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.368, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62078
[1mStep[0m  [10/106], [94mLoss[0m : 2.50303
[1mStep[0m  [20/106], [94mLoss[0m : 2.32391
[1mStep[0m  [30/106], [94mLoss[0m : 2.22686
[1mStep[0m  [40/106], [94mLoss[0m : 2.54101
[1mStep[0m  [50/106], [94mLoss[0m : 2.17884
[1mStep[0m  [60/106], [94mLoss[0m : 2.35694
[1mStep[0m  [70/106], [94mLoss[0m : 2.46962
[1mStep[0m  [80/106], [94mLoss[0m : 2.56954
[1mStep[0m  [90/106], [94mLoss[0m : 2.65646
[1mStep[0m  [100/106], [94mLoss[0m : 2.34823

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.399, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.384
====================================

Phase 1 - Evaluation MAE:  2.384341442360068
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.69678
[1mStep[0m  [10/106], [94mLoss[0m : 2.35558
[1mStep[0m  [20/106], [94mLoss[0m : 2.75210
[1mStep[0m  [30/106], [94mLoss[0m : 2.29896
[1mStep[0m  [40/106], [94mLoss[0m : 2.24652
[1mStep[0m  [50/106], [94mLoss[0m : 2.29098
[1mStep[0m  [60/106], [94mLoss[0m : 2.71008
[1mStep[0m  [70/106], [94mLoss[0m : 2.69384
[1mStep[0m  [80/106], [94mLoss[0m : 2.47843
[1mStep[0m  [90/106], [94mLoss[0m : 2.85168
[1mStep[0m  [100/106], [94mLoss[0m : 2.37332

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46679
[1mStep[0m  [10/106], [94mLoss[0m : 2.28217
[1mStep[0m  [20/106], [94mLoss[0m : 2.22078
[1mStep[0m  [30/106], [94mLoss[0m : 2.25943
[1mStep[0m  [40/106], [94mLoss[0m : 2.55714
[1mStep[0m  [50/106], [94mLoss[0m : 2.25560
[1mStep[0m  [60/106], [94mLoss[0m : 2.31580
[1mStep[0m  [70/106], [94mLoss[0m : 2.56854
[1mStep[0m  [80/106], [94mLoss[0m : 2.37380
[1mStep[0m  [90/106], [94mLoss[0m : 2.59128
[1mStep[0m  [100/106], [94mLoss[0m : 2.58190

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.624, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06818
[1mStep[0m  [10/106], [94mLoss[0m : 2.37035
[1mStep[0m  [20/106], [94mLoss[0m : 2.07284
[1mStep[0m  [30/106], [94mLoss[0m : 2.29949
[1mStep[0m  [40/106], [94mLoss[0m : 2.45499
[1mStep[0m  [50/106], [94mLoss[0m : 2.49311
[1mStep[0m  [60/106], [94mLoss[0m : 2.59725
[1mStep[0m  [70/106], [94mLoss[0m : 2.24773
[1mStep[0m  [80/106], [94mLoss[0m : 2.50035
[1mStep[0m  [90/106], [94mLoss[0m : 2.20673
[1mStep[0m  [100/106], [94mLoss[0m : 2.51486

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.553, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16174
[1mStep[0m  [10/106], [94mLoss[0m : 2.13249
[1mStep[0m  [20/106], [94mLoss[0m : 2.42304
[1mStep[0m  [30/106], [94mLoss[0m : 2.45732
[1mStep[0m  [40/106], [94mLoss[0m : 2.46884
[1mStep[0m  [50/106], [94mLoss[0m : 2.35408
[1mStep[0m  [60/106], [94mLoss[0m : 2.16966
[1mStep[0m  [70/106], [94mLoss[0m : 2.30840
[1mStep[0m  [80/106], [94mLoss[0m : 1.97502
[1mStep[0m  [90/106], [94mLoss[0m : 2.33489
[1mStep[0m  [100/106], [94mLoss[0m : 2.21680

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.572, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40858
[1mStep[0m  [10/106], [94mLoss[0m : 2.23356
[1mStep[0m  [20/106], [94mLoss[0m : 2.19426
[1mStep[0m  [30/106], [94mLoss[0m : 2.24480
[1mStep[0m  [40/106], [94mLoss[0m : 2.08109
[1mStep[0m  [50/106], [94mLoss[0m : 2.06895
[1mStep[0m  [60/106], [94mLoss[0m : 2.24111
[1mStep[0m  [70/106], [94mLoss[0m : 2.24850
[1mStep[0m  [80/106], [94mLoss[0m : 2.11136
[1mStep[0m  [90/106], [94mLoss[0m : 2.41793
[1mStep[0m  [100/106], [94mLoss[0m : 2.50988

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.545, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94937
[1mStep[0m  [10/106], [94mLoss[0m : 2.03006
[1mStep[0m  [20/106], [94mLoss[0m : 2.06067
[1mStep[0m  [30/106], [94mLoss[0m : 2.19277
[1mStep[0m  [40/106], [94mLoss[0m : 2.06973
[1mStep[0m  [50/106], [94mLoss[0m : 2.21285
[1mStep[0m  [60/106], [94mLoss[0m : 1.98746
[1mStep[0m  [70/106], [94mLoss[0m : 2.12124
[1mStep[0m  [80/106], [94mLoss[0m : 2.11387
[1mStep[0m  [90/106], [94mLoss[0m : 2.23192
[1mStep[0m  [100/106], [94mLoss[0m : 2.09251

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.720, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30070
[1mStep[0m  [10/106], [94mLoss[0m : 2.04087
[1mStep[0m  [20/106], [94mLoss[0m : 2.09985
[1mStep[0m  [30/106], [94mLoss[0m : 2.05479
[1mStep[0m  [40/106], [94mLoss[0m : 2.31992
[1mStep[0m  [50/106], [94mLoss[0m : 2.01211
[1mStep[0m  [60/106], [94mLoss[0m : 1.96893
[1mStep[0m  [70/106], [94mLoss[0m : 2.22179
[1mStep[0m  [80/106], [94mLoss[0m : 2.10372
[1mStep[0m  [90/106], [94mLoss[0m : 1.86958
[1mStep[0m  [100/106], [94mLoss[0m : 2.23630

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.81057
[1mStep[0m  [10/106], [94mLoss[0m : 1.92022
[1mStep[0m  [20/106], [94mLoss[0m : 1.94122
[1mStep[0m  [30/106], [94mLoss[0m : 2.03626
[1mStep[0m  [40/106], [94mLoss[0m : 1.98810
[1mStep[0m  [50/106], [94mLoss[0m : 1.91486
[1mStep[0m  [60/106], [94mLoss[0m : 2.07296
[1mStep[0m  [70/106], [94mLoss[0m : 1.97537
[1mStep[0m  [80/106], [94mLoss[0m : 1.96326
[1mStep[0m  [90/106], [94mLoss[0m : 2.01497
[1mStep[0m  [100/106], [94mLoss[0m : 1.93155

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.067, [92mTest[0m: 2.587, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08168
[1mStep[0m  [10/106], [94mLoss[0m : 1.87647
[1mStep[0m  [20/106], [94mLoss[0m : 2.03417
[1mStep[0m  [30/106], [94mLoss[0m : 1.88829
[1mStep[0m  [40/106], [94mLoss[0m : 2.05008
[1mStep[0m  [50/106], [94mLoss[0m : 1.95501
[1mStep[0m  [60/106], [94mLoss[0m : 2.26654
[1mStep[0m  [70/106], [94mLoss[0m : 2.00793
[1mStep[0m  [80/106], [94mLoss[0m : 1.92320
[1mStep[0m  [90/106], [94mLoss[0m : 2.31491
[1mStep[0m  [100/106], [94mLoss[0m : 2.26485

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64509
[1mStep[0m  [10/106], [94mLoss[0m : 2.10482
[1mStep[0m  [20/106], [94mLoss[0m : 1.94997
[1mStep[0m  [30/106], [94mLoss[0m : 2.05767
[1mStep[0m  [40/106], [94mLoss[0m : 1.95430
[1mStep[0m  [50/106], [94mLoss[0m : 1.94406
[1mStep[0m  [60/106], [94mLoss[0m : 2.02482
[1mStep[0m  [70/106], [94mLoss[0m : 2.07702
[1mStep[0m  [80/106], [94mLoss[0m : 2.16871
[1mStep[0m  [90/106], [94mLoss[0m : 1.68759
[1mStep[0m  [100/106], [94mLoss[0m : 1.73477

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.570, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.69468
[1mStep[0m  [10/106], [94mLoss[0m : 1.66370
[1mStep[0m  [20/106], [94mLoss[0m : 1.81596
[1mStep[0m  [30/106], [94mLoss[0m : 1.99550
[1mStep[0m  [40/106], [94mLoss[0m : 2.34701
[1mStep[0m  [50/106], [94mLoss[0m : 1.79734
[1mStep[0m  [60/106], [94mLoss[0m : 1.55211
[1mStep[0m  [70/106], [94mLoss[0m : 1.79882
[1mStep[0m  [80/106], [94mLoss[0m : 1.80586
[1mStep[0m  [90/106], [94mLoss[0m : 1.95027
[1mStep[0m  [100/106], [94mLoss[0m : 1.96216

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.538, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.80098
[1mStep[0m  [10/106], [94mLoss[0m : 1.92807
[1mStep[0m  [20/106], [94mLoss[0m : 1.88805
[1mStep[0m  [30/106], [94mLoss[0m : 2.11009
[1mStep[0m  [40/106], [94mLoss[0m : 1.80985
[1mStep[0m  [50/106], [94mLoss[0m : 1.85026
[1mStep[0m  [60/106], [94mLoss[0m : 1.72055
[1mStep[0m  [70/106], [94mLoss[0m : 2.04881
[1mStep[0m  [80/106], [94mLoss[0m : 1.63975
[1mStep[0m  [90/106], [94mLoss[0m : 1.88614
[1mStep[0m  [100/106], [94mLoss[0m : 1.99090

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85831
[1mStep[0m  [10/106], [94mLoss[0m : 1.83590
[1mStep[0m  [20/106], [94mLoss[0m : 1.89914
[1mStep[0m  [30/106], [94mLoss[0m : 1.89452
[1mStep[0m  [40/106], [94mLoss[0m : 1.88550
[1mStep[0m  [50/106], [94mLoss[0m : 1.92022
[1mStep[0m  [60/106], [94mLoss[0m : 1.82121
[1mStep[0m  [70/106], [94mLoss[0m : 1.86774
[1mStep[0m  [80/106], [94mLoss[0m : 1.76919
[1mStep[0m  [90/106], [94mLoss[0m : 1.81518
[1mStep[0m  [100/106], [94mLoss[0m : 1.76023

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87624
[1mStep[0m  [10/106], [94mLoss[0m : 1.87097
[1mStep[0m  [20/106], [94mLoss[0m : 1.76655
[1mStep[0m  [30/106], [94mLoss[0m : 1.66393
[1mStep[0m  [40/106], [94mLoss[0m : 1.68002
[1mStep[0m  [50/106], [94mLoss[0m : 2.06565
[1mStep[0m  [60/106], [94mLoss[0m : 1.89286
[1mStep[0m  [70/106], [94mLoss[0m : 1.78040
[1mStep[0m  [80/106], [94mLoss[0m : 2.07631
[1mStep[0m  [90/106], [94mLoss[0m : 1.87829
[1mStep[0m  [100/106], [94mLoss[0m : 1.98743

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.81777
[1mStep[0m  [10/106], [94mLoss[0m : 1.92324
[1mStep[0m  [20/106], [94mLoss[0m : 1.61398
[1mStep[0m  [30/106], [94mLoss[0m : 1.88068
[1mStep[0m  [40/106], [94mLoss[0m : 1.71204
[1mStep[0m  [50/106], [94mLoss[0m : 1.84057
[1mStep[0m  [60/106], [94mLoss[0m : 1.96689
[1mStep[0m  [70/106], [94mLoss[0m : 1.52110
[1mStep[0m  [80/106], [94mLoss[0m : 1.63097
[1mStep[0m  [90/106], [94mLoss[0m : 1.86417
[1mStep[0m  [100/106], [94mLoss[0m : 1.67297

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90373
[1mStep[0m  [10/106], [94mLoss[0m : 1.52585
[1mStep[0m  [20/106], [94mLoss[0m : 1.74089
[1mStep[0m  [30/106], [94mLoss[0m : 1.89572
[1mStep[0m  [40/106], [94mLoss[0m : 1.54111
[1mStep[0m  [50/106], [94mLoss[0m : 1.57059
[1mStep[0m  [60/106], [94mLoss[0m : 1.82412
[1mStep[0m  [70/106], [94mLoss[0m : 2.11827
[1mStep[0m  [80/106], [94mLoss[0m : 1.63286
[1mStep[0m  [90/106], [94mLoss[0m : 1.61548
[1mStep[0m  [100/106], [94mLoss[0m : 1.58229

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.732, [92mTest[0m: 2.534, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73344
[1mStep[0m  [10/106], [94mLoss[0m : 1.72512
[1mStep[0m  [20/106], [94mLoss[0m : 1.53115
[1mStep[0m  [30/106], [94mLoss[0m : 1.63873
[1mStep[0m  [40/106], [94mLoss[0m : 1.58744
[1mStep[0m  [50/106], [94mLoss[0m : 1.65450
[1mStep[0m  [60/106], [94mLoss[0m : 1.43778
[1mStep[0m  [70/106], [94mLoss[0m : 1.63285
[1mStep[0m  [80/106], [94mLoss[0m : 1.72394
[1mStep[0m  [90/106], [94mLoss[0m : 1.65091
[1mStep[0m  [100/106], [94mLoss[0m : 1.84623

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55705
[1mStep[0m  [10/106], [94mLoss[0m : 1.45239
[1mStep[0m  [20/106], [94mLoss[0m : 1.60650
[1mStep[0m  [30/106], [94mLoss[0m : 1.66663
[1mStep[0m  [40/106], [94mLoss[0m : 1.69858
[1mStep[0m  [50/106], [94mLoss[0m : 1.70001
[1mStep[0m  [60/106], [94mLoss[0m : 1.68476
[1mStep[0m  [70/106], [94mLoss[0m : 1.70822
[1mStep[0m  [80/106], [94mLoss[0m : 1.75462
[1mStep[0m  [90/106], [94mLoss[0m : 1.87698
[1mStep[0m  [100/106], [94mLoss[0m : 1.93473

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.662, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58746
[1mStep[0m  [10/106], [94mLoss[0m : 1.47524
[1mStep[0m  [20/106], [94mLoss[0m : 1.73530
[1mStep[0m  [30/106], [94mLoss[0m : 1.53301
[1mStep[0m  [40/106], [94mLoss[0m : 1.64835
[1mStep[0m  [50/106], [94mLoss[0m : 1.75530
[1mStep[0m  [60/106], [94mLoss[0m : 1.66667
[1mStep[0m  [70/106], [94mLoss[0m : 1.69709
[1mStep[0m  [80/106], [94mLoss[0m : 1.65033
[1mStep[0m  [90/106], [94mLoss[0m : 1.74823
[1mStep[0m  [100/106], [94mLoss[0m : 1.51949

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60140
[1mStep[0m  [10/106], [94mLoss[0m : 1.58396
[1mStep[0m  [20/106], [94mLoss[0m : 1.44332
[1mStep[0m  [30/106], [94mLoss[0m : 1.69997
[1mStep[0m  [40/106], [94mLoss[0m : 1.55483
[1mStep[0m  [50/106], [94mLoss[0m : 1.71733
[1mStep[0m  [60/106], [94mLoss[0m : 1.80478
[1mStep[0m  [70/106], [94mLoss[0m : 1.62122
[1mStep[0m  [80/106], [94mLoss[0m : 1.54560
[1mStep[0m  [90/106], [94mLoss[0m : 1.73455
[1mStep[0m  [100/106], [94mLoss[0m : 1.58419

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.615, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43383
[1mStep[0m  [10/106], [94mLoss[0m : 1.63705
[1mStep[0m  [20/106], [94mLoss[0m : 1.68771
[1mStep[0m  [30/106], [94mLoss[0m : 1.51569
[1mStep[0m  [40/106], [94mLoss[0m : 1.74755
[1mStep[0m  [50/106], [94mLoss[0m : 1.72228
[1mStep[0m  [60/106], [94mLoss[0m : 1.58272
[1mStep[0m  [70/106], [94mLoss[0m : 1.74225
[1mStep[0m  [80/106], [94mLoss[0m : 1.86525
[1mStep[0m  [90/106], [94mLoss[0m : 1.53004
[1mStep[0m  [100/106], [94mLoss[0m : 1.52286

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.528, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39242
[1mStep[0m  [10/106], [94mLoss[0m : 1.48893
[1mStep[0m  [20/106], [94mLoss[0m : 1.53647
[1mStep[0m  [30/106], [94mLoss[0m : 1.51091
[1mStep[0m  [40/106], [94mLoss[0m : 1.53052
[1mStep[0m  [50/106], [94mLoss[0m : 1.57176
[1mStep[0m  [60/106], [94mLoss[0m : 1.52500
[1mStep[0m  [70/106], [94mLoss[0m : 1.83559
[1mStep[0m  [80/106], [94mLoss[0m : 1.28332
[1mStep[0m  [90/106], [94mLoss[0m : 1.72833
[1mStep[0m  [100/106], [94mLoss[0m : 1.54574

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.453, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.32237
[1mStep[0m  [10/106], [94mLoss[0m : 1.32467
[1mStep[0m  [20/106], [94mLoss[0m : 1.37800
[1mStep[0m  [30/106], [94mLoss[0m : 1.43450
[1mStep[0m  [40/106], [94mLoss[0m : 1.56682
[1mStep[0m  [50/106], [94mLoss[0m : 1.47818
[1mStep[0m  [60/106], [94mLoss[0m : 1.35389
[1mStep[0m  [70/106], [94mLoss[0m : 1.40497
[1mStep[0m  [80/106], [94mLoss[0m : 1.52655
[1mStep[0m  [90/106], [94mLoss[0m : 1.49146
[1mStep[0m  [100/106], [94mLoss[0m : 1.41915

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.497, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.46395
[1mStep[0m  [10/106], [94mLoss[0m : 1.59221
[1mStep[0m  [20/106], [94mLoss[0m : 1.45547
[1mStep[0m  [30/106], [94mLoss[0m : 1.52086
[1mStep[0m  [40/106], [94mLoss[0m : 1.39698
[1mStep[0m  [50/106], [94mLoss[0m : 1.34627
[1mStep[0m  [60/106], [94mLoss[0m : 1.49921
[1mStep[0m  [70/106], [94mLoss[0m : 1.48098
[1mStep[0m  [80/106], [94mLoss[0m : 1.54422
[1mStep[0m  [90/106], [94mLoss[0m : 1.61851
[1mStep[0m  [100/106], [94mLoss[0m : 1.63564

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.47472
[1mStep[0m  [10/106], [94mLoss[0m : 1.29151
[1mStep[0m  [20/106], [94mLoss[0m : 1.45736
[1mStep[0m  [30/106], [94mLoss[0m : 1.31366
[1mStep[0m  [40/106], [94mLoss[0m : 1.43925
[1mStep[0m  [50/106], [94mLoss[0m : 1.49719
[1mStep[0m  [60/106], [94mLoss[0m : 1.51140
[1mStep[0m  [70/106], [94mLoss[0m : 1.66939
[1mStep[0m  [80/106], [94mLoss[0m : 1.36550
[1mStep[0m  [90/106], [94mLoss[0m : 1.54671
[1mStep[0m  [100/106], [94mLoss[0m : 1.39826

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.462, [92mTest[0m: 2.450, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62205
[1mStep[0m  [10/106], [94mLoss[0m : 1.32026
[1mStep[0m  [20/106], [94mLoss[0m : 1.44912
[1mStep[0m  [30/106], [94mLoss[0m : 1.43998
[1mStep[0m  [40/106], [94mLoss[0m : 1.44619
[1mStep[0m  [50/106], [94mLoss[0m : 1.52698
[1mStep[0m  [60/106], [94mLoss[0m : 1.61318
[1mStep[0m  [70/106], [94mLoss[0m : 1.44315
[1mStep[0m  [80/106], [94mLoss[0m : 1.48747
[1mStep[0m  [90/106], [94mLoss[0m : 1.56314
[1mStep[0m  [100/106], [94mLoss[0m : 1.35912

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.448, [92mTest[0m: 2.444, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35404
[1mStep[0m  [10/106], [94mLoss[0m : 1.37154
[1mStep[0m  [20/106], [94mLoss[0m : 1.48046
[1mStep[0m  [30/106], [94mLoss[0m : 1.42022
[1mStep[0m  [40/106], [94mLoss[0m : 1.32395
[1mStep[0m  [50/106], [94mLoss[0m : 1.51679
[1mStep[0m  [60/106], [94mLoss[0m : 1.23848
[1mStep[0m  [70/106], [94mLoss[0m : 1.51783
[1mStep[0m  [80/106], [94mLoss[0m : 1.52079
[1mStep[0m  [90/106], [94mLoss[0m : 1.59461
[1mStep[0m  [100/106], [94mLoss[0m : 1.37769

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.453, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.50385
[1mStep[0m  [10/106], [94mLoss[0m : 1.43577
[1mStep[0m  [20/106], [94mLoss[0m : 1.53293
[1mStep[0m  [30/106], [94mLoss[0m : 1.64778
[1mStep[0m  [40/106], [94mLoss[0m : 1.60323
[1mStep[0m  [50/106], [94mLoss[0m : 1.46193
[1mStep[0m  [60/106], [94mLoss[0m : 1.37864
[1mStep[0m  [70/106], [94mLoss[0m : 1.54979
[1mStep[0m  [80/106], [94mLoss[0m : 1.35307
[1mStep[0m  [90/106], [94mLoss[0m : 1.35237
[1mStep[0m  [100/106], [94mLoss[0m : 1.31931

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.405, [92mTest[0m: 2.480, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.32193
[1mStep[0m  [10/106], [94mLoss[0m : 1.22298
[1mStep[0m  [20/106], [94mLoss[0m : 1.36286
[1mStep[0m  [30/106], [94mLoss[0m : 1.41285
[1mStep[0m  [40/106], [94mLoss[0m : 1.37393
[1mStep[0m  [50/106], [94mLoss[0m : 1.44238
[1mStep[0m  [60/106], [94mLoss[0m : 1.38736
[1mStep[0m  [70/106], [94mLoss[0m : 1.42399
[1mStep[0m  [80/106], [94mLoss[0m : 1.22259
[1mStep[0m  [90/106], [94mLoss[0m : 1.25473
[1mStep[0m  [100/106], [94mLoss[0m : 1.45313

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.386, [92mTest[0m: 2.430, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.19666
[1mStep[0m  [10/106], [94mLoss[0m : 1.42408
[1mStep[0m  [20/106], [94mLoss[0m : 1.50565
[1mStep[0m  [30/106], [94mLoss[0m : 1.22597
[1mStep[0m  [40/106], [94mLoss[0m : 1.37139
[1mStep[0m  [50/106], [94mLoss[0m : 1.27001
[1mStep[0m  [60/106], [94mLoss[0m : 1.37862
[1mStep[0m  [70/106], [94mLoss[0m : 1.26238
[1mStep[0m  [80/106], [94mLoss[0m : 1.38093
[1mStep[0m  [90/106], [94mLoss[0m : 1.33247
[1mStep[0m  [100/106], [94mLoss[0m : 1.14520

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.363, [92mTest[0m: 2.436, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.467
====================================

Phase 2 - Evaluation MAE:  2.466935843791602
MAE score P1      2.384341
MAE score P2      2.466936
loss              1.362537
learning_rate         0.01
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 11.31536
[1mStep[0m  [5/53], [94mLoss[0m : 8.48662
[1mStep[0m  [10/53], [94mLoss[0m : 4.36588
[1mStep[0m  [15/53], [94mLoss[0m : 3.07412
[1mStep[0m  [20/53], [94mLoss[0m : 3.75938
[1mStep[0m  [25/53], [94mLoss[0m : 3.07176
[1mStep[0m  [30/53], [94mLoss[0m : 2.81797
[1mStep[0m  [35/53], [94mLoss[0m : 2.91789
[1mStep[0m  [40/53], [94mLoss[0m : 3.03645
[1mStep[0m  [45/53], [94mLoss[0m : 2.61605
[1mStep[0m  [50/53], [94mLoss[0m : 2.80298

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.182, [92mTest[0m: 10.780, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.79030
[1mStep[0m  [5/53], [94mLoss[0m : 2.70760
[1mStep[0m  [10/53], [94mLoss[0m : 2.73950
[1mStep[0m  [15/53], [94mLoss[0m : 2.94900
[1mStep[0m  [20/53], [94mLoss[0m : 2.87151
[1mStep[0m  [25/53], [94mLoss[0m : 2.76994
[1mStep[0m  [30/53], [94mLoss[0m : 2.71387
[1mStep[0m  [35/53], [94mLoss[0m : 2.70489
[1mStep[0m  [40/53], [94mLoss[0m : 2.71165
[1mStep[0m  [45/53], [94mLoss[0m : 2.57191
[1mStep[0m  [50/53], [94mLoss[0m : 2.96262

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44930
[1mStep[0m  [5/53], [94mLoss[0m : 2.75731
[1mStep[0m  [10/53], [94mLoss[0m : 2.81549
[1mStep[0m  [15/53], [94mLoss[0m : 2.91905
[1mStep[0m  [20/53], [94mLoss[0m : 2.71695
[1mStep[0m  [25/53], [94mLoss[0m : 2.55628
[1mStep[0m  [30/53], [94mLoss[0m : 2.52890
[1mStep[0m  [35/53], [94mLoss[0m : 2.59329
[1mStep[0m  [40/53], [94mLoss[0m : 2.54627
[1mStep[0m  [45/53], [94mLoss[0m : 2.52092
[1mStep[0m  [50/53], [94mLoss[0m : 2.44917

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50175
[1mStep[0m  [5/53], [94mLoss[0m : 2.65309
[1mStep[0m  [10/53], [94mLoss[0m : 2.47398
[1mStep[0m  [15/53], [94mLoss[0m : 2.63379
[1mStep[0m  [20/53], [94mLoss[0m : 2.66610
[1mStep[0m  [25/53], [94mLoss[0m : 2.96619
[1mStep[0m  [30/53], [94mLoss[0m : 2.52729
[1mStep[0m  [35/53], [94mLoss[0m : 2.69348
[1mStep[0m  [40/53], [94mLoss[0m : 2.45588
[1mStep[0m  [45/53], [94mLoss[0m : 2.45476
[1mStep[0m  [50/53], [94mLoss[0m : 2.48812

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60795
[1mStep[0m  [5/53], [94mLoss[0m : 2.66763
[1mStep[0m  [10/53], [94mLoss[0m : 2.66912
[1mStep[0m  [15/53], [94mLoss[0m : 2.32289
[1mStep[0m  [20/53], [94mLoss[0m : 2.41622
[1mStep[0m  [25/53], [94mLoss[0m : 2.75947
[1mStep[0m  [30/53], [94mLoss[0m : 2.62726
[1mStep[0m  [35/53], [94mLoss[0m : 2.51849
[1mStep[0m  [40/53], [94mLoss[0m : 2.27358
[1mStep[0m  [45/53], [94mLoss[0m : 2.75401
[1mStep[0m  [50/53], [94mLoss[0m : 2.77865

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57915
[1mStep[0m  [5/53], [94mLoss[0m : 2.41316
[1mStep[0m  [10/53], [94mLoss[0m : 2.52525
[1mStep[0m  [15/53], [94mLoss[0m : 2.47975
[1mStep[0m  [20/53], [94mLoss[0m : 2.57493
[1mStep[0m  [25/53], [94mLoss[0m : 2.52705
[1mStep[0m  [30/53], [94mLoss[0m : 2.40315
[1mStep[0m  [35/53], [94mLoss[0m : 2.43852
[1mStep[0m  [40/53], [94mLoss[0m : 2.41745
[1mStep[0m  [45/53], [94mLoss[0m : 2.36524
[1mStep[0m  [50/53], [94mLoss[0m : 2.70584

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53207
[1mStep[0m  [5/53], [94mLoss[0m : 2.58299
[1mStep[0m  [10/53], [94mLoss[0m : 2.54620
[1mStep[0m  [15/53], [94mLoss[0m : 2.72839
[1mStep[0m  [20/53], [94mLoss[0m : 2.43203
[1mStep[0m  [25/53], [94mLoss[0m : 2.54856
[1mStep[0m  [30/53], [94mLoss[0m : 2.39998
[1mStep[0m  [35/53], [94mLoss[0m : 2.53174
[1mStep[0m  [40/53], [94mLoss[0m : 2.40049
[1mStep[0m  [45/53], [94mLoss[0m : 2.41917
[1mStep[0m  [50/53], [94mLoss[0m : 2.51845

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36283
[1mStep[0m  [5/53], [94mLoss[0m : 2.54345
[1mStep[0m  [10/53], [94mLoss[0m : 2.52441
[1mStep[0m  [15/53], [94mLoss[0m : 2.55704
[1mStep[0m  [20/53], [94mLoss[0m : 2.50709
[1mStep[0m  [25/53], [94mLoss[0m : 2.29727
[1mStep[0m  [30/53], [94mLoss[0m : 2.53082
[1mStep[0m  [35/53], [94mLoss[0m : 2.37740
[1mStep[0m  [40/53], [94mLoss[0m : 2.47206
[1mStep[0m  [45/53], [94mLoss[0m : 2.47660
[1mStep[0m  [50/53], [94mLoss[0m : 2.58466

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34186
[1mStep[0m  [5/53], [94mLoss[0m : 2.46100
[1mStep[0m  [10/53], [94mLoss[0m : 2.40336
[1mStep[0m  [15/53], [94mLoss[0m : 2.42793
[1mStep[0m  [20/53], [94mLoss[0m : 2.37793
[1mStep[0m  [25/53], [94mLoss[0m : 2.51562
[1mStep[0m  [30/53], [94mLoss[0m : 2.45899
[1mStep[0m  [35/53], [94mLoss[0m : 2.72041
[1mStep[0m  [40/53], [94mLoss[0m : 2.69000
[1mStep[0m  [45/53], [94mLoss[0m : 2.29370
[1mStep[0m  [50/53], [94mLoss[0m : 2.47060

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57493
[1mStep[0m  [5/53], [94mLoss[0m : 2.48748
[1mStep[0m  [10/53], [94mLoss[0m : 2.29886
[1mStep[0m  [15/53], [94mLoss[0m : 2.34342
[1mStep[0m  [20/53], [94mLoss[0m : 2.55780
[1mStep[0m  [25/53], [94mLoss[0m : 2.47132
[1mStep[0m  [30/53], [94mLoss[0m : 2.22123
[1mStep[0m  [35/53], [94mLoss[0m : 2.53049
[1mStep[0m  [40/53], [94mLoss[0m : 2.42801
[1mStep[0m  [45/53], [94mLoss[0m : 2.24612
[1mStep[0m  [50/53], [94mLoss[0m : 2.32924

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50088
[1mStep[0m  [5/53], [94mLoss[0m : 2.26988
[1mStep[0m  [10/53], [94mLoss[0m : 2.50004
[1mStep[0m  [15/53], [94mLoss[0m : 2.35376
[1mStep[0m  [20/53], [94mLoss[0m : 2.36745
[1mStep[0m  [25/53], [94mLoss[0m : 2.44638
[1mStep[0m  [30/53], [94mLoss[0m : 2.60821
[1mStep[0m  [35/53], [94mLoss[0m : 2.60636
[1mStep[0m  [40/53], [94mLoss[0m : 2.45643
[1mStep[0m  [45/53], [94mLoss[0m : 2.50184
[1mStep[0m  [50/53], [94mLoss[0m : 2.61893

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34171
[1mStep[0m  [5/53], [94mLoss[0m : 2.44867
[1mStep[0m  [10/53], [94mLoss[0m : 2.68950
[1mStep[0m  [15/53], [94mLoss[0m : 2.32742
[1mStep[0m  [20/53], [94mLoss[0m : 2.49595
[1mStep[0m  [25/53], [94mLoss[0m : 2.40080
[1mStep[0m  [30/53], [94mLoss[0m : 2.35245
[1mStep[0m  [35/53], [94mLoss[0m : 2.48219
[1mStep[0m  [40/53], [94mLoss[0m : 2.42233
[1mStep[0m  [45/53], [94mLoss[0m : 2.41346
[1mStep[0m  [50/53], [94mLoss[0m : 2.23897

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46240
[1mStep[0m  [5/53], [94mLoss[0m : 2.32248
[1mStep[0m  [10/53], [94mLoss[0m : 2.41598
[1mStep[0m  [15/53], [94mLoss[0m : 2.17944
[1mStep[0m  [20/53], [94mLoss[0m : 2.57669
[1mStep[0m  [25/53], [94mLoss[0m : 2.42606
[1mStep[0m  [30/53], [94mLoss[0m : 2.42310
[1mStep[0m  [35/53], [94mLoss[0m : 2.54576
[1mStep[0m  [40/53], [94mLoss[0m : 2.54396
[1mStep[0m  [45/53], [94mLoss[0m : 2.47571
[1mStep[0m  [50/53], [94mLoss[0m : 2.28000

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.73859
[1mStep[0m  [5/53], [94mLoss[0m : 2.46530
[1mStep[0m  [10/53], [94mLoss[0m : 2.45719
[1mStep[0m  [15/53], [94mLoss[0m : 2.28637
[1mStep[0m  [20/53], [94mLoss[0m : 2.53217
[1mStep[0m  [25/53], [94mLoss[0m : 2.16440
[1mStep[0m  [30/53], [94mLoss[0m : 2.31376
[1mStep[0m  [35/53], [94mLoss[0m : 2.36965
[1mStep[0m  [40/53], [94mLoss[0m : 2.26951
[1mStep[0m  [45/53], [94mLoss[0m : 2.49635
[1mStep[0m  [50/53], [94mLoss[0m : 2.33429

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52347
[1mStep[0m  [5/53], [94mLoss[0m : 2.48094
[1mStep[0m  [10/53], [94mLoss[0m : 2.52595
[1mStep[0m  [15/53], [94mLoss[0m : 2.48736
[1mStep[0m  [20/53], [94mLoss[0m : 2.66380
[1mStep[0m  [25/53], [94mLoss[0m : 2.47042
[1mStep[0m  [30/53], [94mLoss[0m : 2.61346
[1mStep[0m  [35/53], [94mLoss[0m : 2.56835
[1mStep[0m  [40/53], [94mLoss[0m : 2.28134
[1mStep[0m  [45/53], [94mLoss[0m : 2.22770
[1mStep[0m  [50/53], [94mLoss[0m : 2.41305

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35938
[1mStep[0m  [5/53], [94mLoss[0m : 2.46306
[1mStep[0m  [10/53], [94mLoss[0m : 2.34391
[1mStep[0m  [15/53], [94mLoss[0m : 2.35624
[1mStep[0m  [20/53], [94mLoss[0m : 2.37060
[1mStep[0m  [25/53], [94mLoss[0m : 2.31784
[1mStep[0m  [30/53], [94mLoss[0m : 2.53941
[1mStep[0m  [35/53], [94mLoss[0m : 2.31062
[1mStep[0m  [40/53], [94mLoss[0m : 2.23294
[1mStep[0m  [45/53], [94mLoss[0m : 2.26397
[1mStep[0m  [50/53], [94mLoss[0m : 2.32544

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51857
[1mStep[0m  [5/53], [94mLoss[0m : 2.43759
[1mStep[0m  [10/53], [94mLoss[0m : 2.18163
[1mStep[0m  [15/53], [94mLoss[0m : 2.37191
[1mStep[0m  [20/53], [94mLoss[0m : 2.27692
[1mStep[0m  [25/53], [94mLoss[0m : 2.59687
[1mStep[0m  [30/53], [94mLoss[0m : 2.22498
[1mStep[0m  [35/53], [94mLoss[0m : 2.40161
[1mStep[0m  [40/53], [94mLoss[0m : 2.56880
[1mStep[0m  [45/53], [94mLoss[0m : 2.40019
[1mStep[0m  [50/53], [94mLoss[0m : 2.20052

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37232
[1mStep[0m  [5/53], [94mLoss[0m : 2.18946
[1mStep[0m  [10/53], [94mLoss[0m : 2.52590
[1mStep[0m  [15/53], [94mLoss[0m : 2.40052
[1mStep[0m  [20/53], [94mLoss[0m : 2.37404
[1mStep[0m  [25/53], [94mLoss[0m : 2.33348
[1mStep[0m  [30/53], [94mLoss[0m : 2.34957
[1mStep[0m  [35/53], [94mLoss[0m : 2.49156
[1mStep[0m  [40/53], [94mLoss[0m : 2.28867
[1mStep[0m  [45/53], [94mLoss[0m : 2.29591
[1mStep[0m  [50/53], [94mLoss[0m : 2.46500

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33395
[1mStep[0m  [5/53], [94mLoss[0m : 2.27053
[1mStep[0m  [10/53], [94mLoss[0m : 2.45049
[1mStep[0m  [15/53], [94mLoss[0m : 2.60331
[1mStep[0m  [20/53], [94mLoss[0m : 2.41706
[1mStep[0m  [25/53], [94mLoss[0m : 2.36783
[1mStep[0m  [30/53], [94mLoss[0m : 2.43545
[1mStep[0m  [35/53], [94mLoss[0m : 2.62028
[1mStep[0m  [40/53], [94mLoss[0m : 2.63207
[1mStep[0m  [45/53], [94mLoss[0m : 2.27640
[1mStep[0m  [50/53], [94mLoss[0m : 2.45702

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.75491
[1mStep[0m  [5/53], [94mLoss[0m : 2.33996
[1mStep[0m  [10/53], [94mLoss[0m : 2.45076
[1mStep[0m  [15/53], [94mLoss[0m : 2.54645
[1mStep[0m  [20/53], [94mLoss[0m : 2.41647
[1mStep[0m  [25/53], [94mLoss[0m : 2.45723
[1mStep[0m  [30/53], [94mLoss[0m : 2.52049
[1mStep[0m  [35/53], [94mLoss[0m : 2.59337
[1mStep[0m  [40/53], [94mLoss[0m : 2.52111
[1mStep[0m  [45/53], [94mLoss[0m : 2.20529
[1mStep[0m  [50/53], [94mLoss[0m : 2.38934

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.360, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59492
[1mStep[0m  [5/53], [94mLoss[0m : 2.37270
[1mStep[0m  [10/53], [94mLoss[0m : 2.31375
[1mStep[0m  [15/53], [94mLoss[0m : 2.37355
[1mStep[0m  [20/53], [94mLoss[0m : 2.40814
[1mStep[0m  [25/53], [94mLoss[0m : 2.38069
[1mStep[0m  [30/53], [94mLoss[0m : 2.27217
[1mStep[0m  [35/53], [94mLoss[0m : 2.42887
[1mStep[0m  [40/53], [94mLoss[0m : 2.29687
[1mStep[0m  [45/53], [94mLoss[0m : 2.44861
[1mStep[0m  [50/53], [94mLoss[0m : 2.32005

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.368, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50684
[1mStep[0m  [5/53], [94mLoss[0m : 2.61434
[1mStep[0m  [10/53], [94mLoss[0m : 2.28305
[1mStep[0m  [15/53], [94mLoss[0m : 2.53225
[1mStep[0m  [20/53], [94mLoss[0m : 2.32313
[1mStep[0m  [25/53], [94mLoss[0m : 2.41929
[1mStep[0m  [30/53], [94mLoss[0m : 2.35412
[1mStep[0m  [35/53], [94mLoss[0m : 2.33930
[1mStep[0m  [40/53], [94mLoss[0m : 2.24351
[1mStep[0m  [45/53], [94mLoss[0m : 2.48650
[1mStep[0m  [50/53], [94mLoss[0m : 2.60396

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48401
[1mStep[0m  [5/53], [94mLoss[0m : 2.40919
[1mStep[0m  [10/53], [94mLoss[0m : 2.27446
[1mStep[0m  [15/53], [94mLoss[0m : 2.53928
[1mStep[0m  [20/53], [94mLoss[0m : 2.60219
[1mStep[0m  [25/53], [94mLoss[0m : 2.60416
[1mStep[0m  [30/53], [94mLoss[0m : 2.47398
[1mStep[0m  [35/53], [94mLoss[0m : 2.20524
[1mStep[0m  [40/53], [94mLoss[0m : 2.47877
[1mStep[0m  [45/53], [94mLoss[0m : 2.42189
[1mStep[0m  [50/53], [94mLoss[0m : 2.21985

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52895
[1mStep[0m  [5/53], [94mLoss[0m : 2.35805
[1mStep[0m  [10/53], [94mLoss[0m : 2.44148
[1mStep[0m  [15/53], [94mLoss[0m : 2.48048
[1mStep[0m  [20/53], [94mLoss[0m : 2.33239
[1mStep[0m  [25/53], [94mLoss[0m : 2.39563
[1mStep[0m  [30/53], [94mLoss[0m : 2.50189
[1mStep[0m  [35/53], [94mLoss[0m : 2.51648
[1mStep[0m  [40/53], [94mLoss[0m : 2.46112
[1mStep[0m  [45/53], [94mLoss[0m : 2.34831
[1mStep[0m  [50/53], [94mLoss[0m : 2.38294

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41776
[1mStep[0m  [5/53], [94mLoss[0m : 2.46822
[1mStep[0m  [10/53], [94mLoss[0m : 2.57186
[1mStep[0m  [15/53], [94mLoss[0m : 2.39218
[1mStep[0m  [20/53], [94mLoss[0m : 2.52764
[1mStep[0m  [25/53], [94mLoss[0m : 2.58836
[1mStep[0m  [30/53], [94mLoss[0m : 2.60935
[1mStep[0m  [35/53], [94mLoss[0m : 2.30523
[1mStep[0m  [40/53], [94mLoss[0m : 2.45818
[1mStep[0m  [45/53], [94mLoss[0m : 2.32904
[1mStep[0m  [50/53], [94mLoss[0m : 2.23734

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29331
[1mStep[0m  [5/53], [94mLoss[0m : 2.37297
[1mStep[0m  [10/53], [94mLoss[0m : 2.47372
[1mStep[0m  [15/53], [94mLoss[0m : 2.49793
[1mStep[0m  [20/53], [94mLoss[0m : 2.31295
[1mStep[0m  [25/53], [94mLoss[0m : 2.35362
[1mStep[0m  [30/53], [94mLoss[0m : 2.31045
[1mStep[0m  [35/53], [94mLoss[0m : 2.15225
[1mStep[0m  [40/53], [94mLoss[0m : 2.29710
[1mStep[0m  [45/53], [94mLoss[0m : 2.42673
[1mStep[0m  [50/53], [94mLoss[0m : 2.42914

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.357, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38017
[1mStep[0m  [5/53], [94mLoss[0m : 2.29764
[1mStep[0m  [10/53], [94mLoss[0m : 2.40667
[1mStep[0m  [15/53], [94mLoss[0m : 2.35399
[1mStep[0m  [20/53], [94mLoss[0m : 2.40235
[1mStep[0m  [25/53], [94mLoss[0m : 2.34940
[1mStep[0m  [30/53], [94mLoss[0m : 2.36749
[1mStep[0m  [35/53], [94mLoss[0m : 2.36507
[1mStep[0m  [40/53], [94mLoss[0m : 2.57764
[1mStep[0m  [45/53], [94mLoss[0m : 2.30781
[1mStep[0m  [50/53], [94mLoss[0m : 2.49669

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.342, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29928
[1mStep[0m  [5/53], [94mLoss[0m : 2.38114
[1mStep[0m  [10/53], [94mLoss[0m : 2.21715
[1mStep[0m  [15/53], [94mLoss[0m : 2.49516
[1mStep[0m  [20/53], [94mLoss[0m : 2.49160
[1mStep[0m  [25/53], [94mLoss[0m : 2.38931
[1mStep[0m  [30/53], [94mLoss[0m : 2.66032
[1mStep[0m  [35/53], [94mLoss[0m : 2.11327
[1mStep[0m  [40/53], [94mLoss[0m : 2.50538
[1mStep[0m  [45/53], [94mLoss[0m : 2.43199
[1mStep[0m  [50/53], [94mLoss[0m : 2.48906

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34244
[1mStep[0m  [5/53], [94mLoss[0m : 2.26384
[1mStep[0m  [10/53], [94mLoss[0m : 2.48726
[1mStep[0m  [15/53], [94mLoss[0m : 2.20568
[1mStep[0m  [20/53], [94mLoss[0m : 2.43233
[1mStep[0m  [25/53], [94mLoss[0m : 2.35343
[1mStep[0m  [30/53], [94mLoss[0m : 2.24773
[1mStep[0m  [35/53], [94mLoss[0m : 2.61007
[1mStep[0m  [40/53], [94mLoss[0m : 2.31797
[1mStep[0m  [45/53], [94mLoss[0m : 2.46137
[1mStep[0m  [50/53], [94mLoss[0m : 2.75705

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51602
[1mStep[0m  [5/53], [94mLoss[0m : 2.44289
[1mStep[0m  [10/53], [94mLoss[0m : 2.69943
[1mStep[0m  [15/53], [94mLoss[0m : 2.38161
[1mStep[0m  [20/53], [94mLoss[0m : 2.46151
[1mStep[0m  [25/53], [94mLoss[0m : 2.41322
[1mStep[0m  [30/53], [94mLoss[0m : 2.26957
[1mStep[0m  [35/53], [94mLoss[0m : 2.50479
[1mStep[0m  [40/53], [94mLoss[0m : 2.37376
[1mStep[0m  [45/53], [94mLoss[0m : 2.59325
[1mStep[0m  [50/53], [94mLoss[0m : 2.34913

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.388, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.360
====================================

Phase 1 - Evaluation MAE:  2.360066138781034
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.34530
[1mStep[0m  [5/53], [94mLoss[0m : 2.37516
[1mStep[0m  [10/53], [94mLoss[0m : 2.36474
[1mStep[0m  [15/53], [94mLoss[0m : 2.53233
[1mStep[0m  [20/53], [94mLoss[0m : 2.68692
[1mStep[0m  [25/53], [94mLoss[0m : 2.53802
[1mStep[0m  [30/53], [94mLoss[0m : 2.67761
[1mStep[0m  [35/53], [94mLoss[0m : 2.39498
[1mStep[0m  [40/53], [94mLoss[0m : 2.37501
[1mStep[0m  [45/53], [94mLoss[0m : 2.65262
[1mStep[0m  [50/53], [94mLoss[0m : 2.52723

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37676
[1mStep[0m  [5/53], [94mLoss[0m : 2.33582
[1mStep[0m  [10/53], [94mLoss[0m : 2.39993
[1mStep[0m  [15/53], [94mLoss[0m : 2.20339
[1mStep[0m  [20/53], [94mLoss[0m : 2.23079
[1mStep[0m  [25/53], [94mLoss[0m : 2.54518
[1mStep[0m  [30/53], [94mLoss[0m : 2.21536
[1mStep[0m  [35/53], [94mLoss[0m : 2.48490
[1mStep[0m  [40/53], [94mLoss[0m : 2.27862
[1mStep[0m  [45/53], [94mLoss[0m : 2.34394
[1mStep[0m  [50/53], [94mLoss[0m : 2.33711

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.720, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.05059
[1mStep[0m  [5/53], [94mLoss[0m : 2.23810
[1mStep[0m  [10/53], [94mLoss[0m : 2.39434
[1mStep[0m  [15/53], [94mLoss[0m : 2.28337
[1mStep[0m  [20/53], [94mLoss[0m : 2.26980
[1mStep[0m  [25/53], [94mLoss[0m : 2.25496
[1mStep[0m  [30/53], [94mLoss[0m : 2.34313
[1mStep[0m  [35/53], [94mLoss[0m : 2.46591
[1mStep[0m  [40/53], [94mLoss[0m : 2.39084
[1mStep[0m  [45/53], [94mLoss[0m : 2.41851
[1mStep[0m  [50/53], [94mLoss[0m : 2.12072

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37488
[1mStep[0m  [5/53], [94mLoss[0m : 2.24042
[1mStep[0m  [10/53], [94mLoss[0m : 2.21703
[1mStep[0m  [15/53], [94mLoss[0m : 2.09225
[1mStep[0m  [20/53], [94mLoss[0m : 2.19286
[1mStep[0m  [25/53], [94mLoss[0m : 2.14014
[1mStep[0m  [30/53], [94mLoss[0m : 2.29860
[1mStep[0m  [35/53], [94mLoss[0m : 2.42805
[1mStep[0m  [40/53], [94mLoss[0m : 2.27295
[1mStep[0m  [45/53], [94mLoss[0m : 2.16926
[1mStep[0m  [50/53], [94mLoss[0m : 2.36998

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.85823
[1mStep[0m  [5/53], [94mLoss[0m : 2.24075
[1mStep[0m  [10/53], [94mLoss[0m : 2.05532
[1mStep[0m  [15/53], [94mLoss[0m : 2.06005
[1mStep[0m  [20/53], [94mLoss[0m : 2.31151
[1mStep[0m  [25/53], [94mLoss[0m : 2.17441
[1mStep[0m  [30/53], [94mLoss[0m : 2.30730
[1mStep[0m  [35/53], [94mLoss[0m : 2.19392
[1mStep[0m  [40/53], [94mLoss[0m : 2.02118
[1mStep[0m  [45/53], [94mLoss[0m : 2.20343
[1mStep[0m  [50/53], [94mLoss[0m : 2.33875

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.90488
[1mStep[0m  [5/53], [94mLoss[0m : 2.00621
[1mStep[0m  [10/53], [94mLoss[0m : 2.01117
[1mStep[0m  [15/53], [94mLoss[0m : 1.94977
[1mStep[0m  [20/53], [94mLoss[0m : 1.94059
[1mStep[0m  [25/53], [94mLoss[0m : 2.01294
[1mStep[0m  [30/53], [94mLoss[0m : 2.17583
[1mStep[0m  [35/53], [94mLoss[0m : 2.05882
[1mStep[0m  [40/53], [94mLoss[0m : 2.01749
[1mStep[0m  [45/53], [94mLoss[0m : 2.32223
[1mStep[0m  [50/53], [94mLoss[0m : 2.24480

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.99161
[1mStep[0m  [5/53], [94mLoss[0m : 2.13165
[1mStep[0m  [10/53], [94mLoss[0m : 2.14488
[1mStep[0m  [15/53], [94mLoss[0m : 2.14310
[1mStep[0m  [20/53], [94mLoss[0m : 2.09039
[1mStep[0m  [25/53], [94mLoss[0m : 2.13016
[1mStep[0m  [30/53], [94mLoss[0m : 2.15502
[1mStep[0m  [35/53], [94mLoss[0m : 2.27568
[1mStep[0m  [40/53], [94mLoss[0m : 2.16151
[1mStep[0m  [45/53], [94mLoss[0m : 2.04513
[1mStep[0m  [50/53], [94mLoss[0m : 2.08535

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.078, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.01504
[1mStep[0m  [5/53], [94mLoss[0m : 2.08106
[1mStep[0m  [10/53], [94mLoss[0m : 2.12210
[1mStep[0m  [15/53], [94mLoss[0m : 2.00509
[1mStep[0m  [20/53], [94mLoss[0m : 1.95952
[1mStep[0m  [25/53], [94mLoss[0m : 2.01284
[1mStep[0m  [30/53], [94mLoss[0m : 2.12954
[1mStep[0m  [35/53], [94mLoss[0m : 2.22750
[1mStep[0m  [40/53], [94mLoss[0m : 1.99235
[1mStep[0m  [45/53], [94mLoss[0m : 2.24220
[1mStep[0m  [50/53], [94mLoss[0m : 1.91244

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.03273
[1mStep[0m  [5/53], [94mLoss[0m : 2.01514
[1mStep[0m  [10/53], [94mLoss[0m : 1.75983
[1mStep[0m  [15/53], [94mLoss[0m : 2.08589
[1mStep[0m  [20/53], [94mLoss[0m : 2.01099
[1mStep[0m  [25/53], [94mLoss[0m : 1.91425
[1mStep[0m  [30/53], [94mLoss[0m : 1.92946
[1mStep[0m  [35/53], [94mLoss[0m : 2.29791
[1mStep[0m  [40/53], [94mLoss[0m : 1.92529
[1mStep[0m  [45/53], [94mLoss[0m : 2.09046
[1mStep[0m  [50/53], [94mLoss[0m : 2.02665

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09720
[1mStep[0m  [5/53], [94mLoss[0m : 1.89406
[1mStep[0m  [10/53], [94mLoss[0m : 1.81158
[1mStep[0m  [15/53], [94mLoss[0m : 2.00912
[1mStep[0m  [20/53], [94mLoss[0m : 1.83771
[1mStep[0m  [25/53], [94mLoss[0m : 2.02863
[1mStep[0m  [30/53], [94mLoss[0m : 1.97372
[1mStep[0m  [35/53], [94mLoss[0m : 2.10361
[1mStep[0m  [40/53], [94mLoss[0m : 2.01550
[1mStep[0m  [45/53], [94mLoss[0m : 1.95227
[1mStep[0m  [50/53], [94mLoss[0m : 2.21795

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.92132
[1mStep[0m  [5/53], [94mLoss[0m : 1.94270
[1mStep[0m  [10/53], [94mLoss[0m : 2.17230
[1mStep[0m  [15/53], [94mLoss[0m : 1.99349
[1mStep[0m  [20/53], [94mLoss[0m : 2.00693
[1mStep[0m  [25/53], [94mLoss[0m : 1.87223
[1mStep[0m  [30/53], [94mLoss[0m : 1.88779
[1mStep[0m  [35/53], [94mLoss[0m : 2.03907
[1mStep[0m  [40/53], [94mLoss[0m : 1.80097
[1mStep[0m  [45/53], [94mLoss[0m : 2.00071
[1mStep[0m  [50/53], [94mLoss[0m : 2.10827

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.955, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93756
[1mStep[0m  [5/53], [94mLoss[0m : 1.83664
[1mStep[0m  [10/53], [94mLoss[0m : 1.79019
[1mStep[0m  [15/53], [94mLoss[0m : 1.81336
[1mStep[0m  [20/53], [94mLoss[0m : 1.72700
[1mStep[0m  [25/53], [94mLoss[0m : 1.96198
[1mStep[0m  [30/53], [94mLoss[0m : 2.21825
[1mStep[0m  [35/53], [94mLoss[0m : 2.03349
[1mStep[0m  [40/53], [94mLoss[0m : 2.09255
[1mStep[0m  [45/53], [94mLoss[0m : 1.95998
[1mStep[0m  [50/53], [94mLoss[0m : 2.01311

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.87906
[1mStep[0m  [5/53], [94mLoss[0m : 1.81919
[1mStep[0m  [10/53], [94mLoss[0m : 1.88801
[1mStep[0m  [15/53], [94mLoss[0m : 1.97761
[1mStep[0m  [20/53], [94mLoss[0m : 1.83783
[1mStep[0m  [25/53], [94mLoss[0m : 1.82783
[1mStep[0m  [30/53], [94mLoss[0m : 1.88647
[1mStep[0m  [35/53], [94mLoss[0m : 2.29943
[1mStep[0m  [40/53], [94mLoss[0m : 1.98748
[1mStep[0m  [45/53], [94mLoss[0m : 1.79383
[1mStep[0m  [50/53], [94mLoss[0m : 1.95997

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.74826
[1mStep[0m  [5/53], [94mLoss[0m : 1.91696
[1mStep[0m  [10/53], [94mLoss[0m : 1.91487
[1mStep[0m  [15/53], [94mLoss[0m : 1.85061
[1mStep[0m  [20/53], [94mLoss[0m : 1.89863
[1mStep[0m  [25/53], [94mLoss[0m : 2.00674
[1mStep[0m  [30/53], [94mLoss[0m : 1.77366
[1mStep[0m  [35/53], [94mLoss[0m : 2.03818
[1mStep[0m  [40/53], [94mLoss[0m : 2.01213
[1mStep[0m  [45/53], [94mLoss[0m : 1.92639
[1mStep[0m  [50/53], [94mLoss[0m : 1.80446

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.81756
[1mStep[0m  [5/53], [94mLoss[0m : 1.49506
[1mStep[0m  [10/53], [94mLoss[0m : 1.77177
[1mStep[0m  [15/53], [94mLoss[0m : 1.87301
[1mStep[0m  [20/53], [94mLoss[0m : 1.68955
[1mStep[0m  [25/53], [94mLoss[0m : 1.79302
[1mStep[0m  [30/53], [94mLoss[0m : 1.76528
[1mStep[0m  [35/53], [94mLoss[0m : 1.95473
[1mStep[0m  [40/53], [94mLoss[0m : 2.02746
[1mStep[0m  [45/53], [94mLoss[0m : 1.80354
[1mStep[0m  [50/53], [94mLoss[0m : 1.90908

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83952
[1mStep[0m  [5/53], [94mLoss[0m : 1.58106
[1mStep[0m  [10/53], [94mLoss[0m : 1.55308
[1mStep[0m  [15/53], [94mLoss[0m : 1.88428
[1mStep[0m  [20/53], [94mLoss[0m : 1.71520
[1mStep[0m  [25/53], [94mLoss[0m : 1.79757
[1mStep[0m  [30/53], [94mLoss[0m : 1.91965
[1mStep[0m  [35/53], [94mLoss[0m : 1.79467
[1mStep[0m  [40/53], [94mLoss[0m : 1.79519
[1mStep[0m  [45/53], [94mLoss[0m : 1.77703
[1mStep[0m  [50/53], [94mLoss[0m : 2.07478

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.500, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78678
[1mStep[0m  [5/53], [94mLoss[0m : 1.84137
[1mStep[0m  [10/53], [94mLoss[0m : 1.60638
[1mStep[0m  [15/53], [94mLoss[0m : 1.79790
[1mStep[0m  [20/53], [94mLoss[0m : 1.83845
[1mStep[0m  [25/53], [94mLoss[0m : 1.87855
[1mStep[0m  [30/53], [94mLoss[0m : 1.61048
[1mStep[0m  [35/53], [94mLoss[0m : 1.93009
[1mStep[0m  [40/53], [94mLoss[0m : 1.96570
[1mStep[0m  [45/53], [94mLoss[0m : 1.87287
[1mStep[0m  [50/53], [94mLoss[0m : 1.89833

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.75142
[1mStep[0m  [5/53], [94mLoss[0m : 1.80294
[1mStep[0m  [10/53], [94mLoss[0m : 1.69919
[1mStep[0m  [15/53], [94mLoss[0m : 1.74584
[1mStep[0m  [20/53], [94mLoss[0m : 1.79354
[1mStep[0m  [25/53], [94mLoss[0m : 1.73590
[1mStep[0m  [30/53], [94mLoss[0m : 1.81500
[1mStep[0m  [35/53], [94mLoss[0m : 1.78414
[1mStep[0m  [40/53], [94mLoss[0m : 1.67840
[1mStep[0m  [45/53], [94mLoss[0m : 1.74247
[1mStep[0m  [50/53], [94mLoss[0m : 1.93173

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.65040
[1mStep[0m  [5/53], [94mLoss[0m : 1.57888
[1mStep[0m  [10/53], [94mLoss[0m : 1.77722
[1mStep[0m  [15/53], [94mLoss[0m : 1.83419
[1mStep[0m  [20/53], [94mLoss[0m : 1.70435
[1mStep[0m  [25/53], [94mLoss[0m : 1.87819
[1mStep[0m  [30/53], [94mLoss[0m : 1.87016
[1mStep[0m  [35/53], [94mLoss[0m : 1.87043
[1mStep[0m  [40/53], [94mLoss[0m : 1.79837
[1mStep[0m  [45/53], [94mLoss[0m : 1.76649
[1mStep[0m  [50/53], [94mLoss[0m : 1.86877

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.794, [92mTest[0m: 2.518, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.75073
[1mStep[0m  [5/53], [94mLoss[0m : 1.72080
[1mStep[0m  [10/53], [94mLoss[0m : 1.67585
[1mStep[0m  [15/53], [94mLoss[0m : 1.69869
[1mStep[0m  [20/53], [94mLoss[0m : 1.71152
[1mStep[0m  [25/53], [94mLoss[0m : 1.70112
[1mStep[0m  [30/53], [94mLoss[0m : 1.92000
[1mStep[0m  [35/53], [94mLoss[0m : 1.82177
[1mStep[0m  [40/53], [94mLoss[0m : 1.82533
[1mStep[0m  [45/53], [94mLoss[0m : 1.71264
[1mStep[0m  [50/53], [94mLoss[0m : 1.70784

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.780, [92mTest[0m: 2.516, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80411
[1mStep[0m  [5/53], [94mLoss[0m : 1.72071
[1mStep[0m  [10/53], [94mLoss[0m : 1.71657
[1mStep[0m  [15/53], [94mLoss[0m : 1.87702
[1mStep[0m  [20/53], [94mLoss[0m : 1.68466
[1mStep[0m  [25/53], [94mLoss[0m : 1.59917
[1mStep[0m  [30/53], [94mLoss[0m : 1.66961
[1mStep[0m  [35/53], [94mLoss[0m : 1.65451
[1mStep[0m  [40/53], [94mLoss[0m : 1.68181
[1mStep[0m  [45/53], [94mLoss[0m : 1.80679
[1mStep[0m  [50/53], [94mLoss[0m : 1.84466

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.549, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66213
[1mStep[0m  [5/53], [94mLoss[0m : 1.59183
[1mStep[0m  [10/53], [94mLoss[0m : 1.73364
[1mStep[0m  [15/53], [94mLoss[0m : 1.81692
[1mStep[0m  [20/53], [94mLoss[0m : 1.67157
[1mStep[0m  [25/53], [94mLoss[0m : 1.79989
[1mStep[0m  [30/53], [94mLoss[0m : 1.65188
[1mStep[0m  [35/53], [94mLoss[0m : 1.75745
[1mStep[0m  [40/53], [94mLoss[0m : 1.82299
[1mStep[0m  [45/53], [94mLoss[0m : 1.64986
[1mStep[0m  [50/53], [94mLoss[0m : 1.85773

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84070
[1mStep[0m  [5/53], [94mLoss[0m : 1.65277
[1mStep[0m  [10/53], [94mLoss[0m : 1.62104
[1mStep[0m  [15/53], [94mLoss[0m : 1.78383
[1mStep[0m  [20/53], [94mLoss[0m : 1.63664
[1mStep[0m  [25/53], [94mLoss[0m : 1.72609
[1mStep[0m  [30/53], [94mLoss[0m : 1.66758
[1mStep[0m  [35/53], [94mLoss[0m : 1.63420
[1mStep[0m  [40/53], [94mLoss[0m : 1.83435
[1mStep[0m  [45/53], [94mLoss[0m : 1.80730
[1mStep[0m  [50/53], [94mLoss[0m : 1.74487

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66335
[1mStep[0m  [5/53], [94mLoss[0m : 1.75356
[1mStep[0m  [10/53], [94mLoss[0m : 1.54270
[1mStep[0m  [15/53], [94mLoss[0m : 1.59133
[1mStep[0m  [20/53], [94mLoss[0m : 1.59924
[1mStep[0m  [25/53], [94mLoss[0m : 1.58748
[1mStep[0m  [30/53], [94mLoss[0m : 1.75573
[1mStep[0m  [35/53], [94mLoss[0m : 1.72645
[1mStep[0m  [40/53], [94mLoss[0m : 1.66170
[1mStep[0m  [45/53], [94mLoss[0m : 1.72399
[1mStep[0m  [50/53], [94mLoss[0m : 1.91243

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.670, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.57021
[1mStep[0m  [5/53], [94mLoss[0m : 1.52298
[1mStep[0m  [10/53], [94mLoss[0m : 1.65856
[1mStep[0m  [15/53], [94mLoss[0m : 1.71016
[1mStep[0m  [20/53], [94mLoss[0m : 1.64108
[1mStep[0m  [25/53], [94mLoss[0m : 1.57204
[1mStep[0m  [30/53], [94mLoss[0m : 1.70197
[1mStep[0m  [35/53], [94mLoss[0m : 1.57863
[1mStep[0m  [40/53], [94mLoss[0m : 1.62087
[1mStep[0m  [45/53], [94mLoss[0m : 1.67513
[1mStep[0m  [50/53], [94mLoss[0m : 1.66094

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.535, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43243
[1mStep[0m  [5/53], [94mLoss[0m : 1.60291
[1mStep[0m  [10/53], [94mLoss[0m : 1.75639
[1mStep[0m  [15/53], [94mLoss[0m : 1.77640
[1mStep[0m  [20/53], [94mLoss[0m : 1.69714
[1mStep[0m  [25/53], [94mLoss[0m : 1.66051
[1mStep[0m  [30/53], [94mLoss[0m : 1.68559
[1mStep[0m  [35/53], [94mLoss[0m : 1.61091
[1mStep[0m  [40/53], [94mLoss[0m : 1.78776
[1mStep[0m  [45/53], [94mLoss[0m : 1.64767
[1mStep[0m  [50/53], [94mLoss[0m : 1.74061

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.53033
[1mStep[0m  [5/53], [94mLoss[0m : 1.73036
[1mStep[0m  [10/53], [94mLoss[0m : 1.72938
[1mStep[0m  [15/53], [94mLoss[0m : 1.88503
[1mStep[0m  [20/53], [94mLoss[0m : 1.75629
[1mStep[0m  [25/53], [94mLoss[0m : 1.62706
[1mStep[0m  [30/53], [94mLoss[0m : 1.45661
[1mStep[0m  [35/53], [94mLoss[0m : 1.77076
[1mStep[0m  [40/53], [94mLoss[0m : 1.45756
[1mStep[0m  [45/53], [94mLoss[0m : 1.55663
[1mStep[0m  [50/53], [94mLoss[0m : 1.73236

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.572, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.50644
[1mStep[0m  [5/53], [94mLoss[0m : 1.68580
[1mStep[0m  [10/53], [94mLoss[0m : 1.57289
[1mStep[0m  [15/53], [94mLoss[0m : 1.74684
[1mStep[0m  [20/53], [94mLoss[0m : 1.58289
[1mStep[0m  [25/53], [94mLoss[0m : 1.72279
[1mStep[0m  [30/53], [94mLoss[0m : 1.68467
[1mStep[0m  [35/53], [94mLoss[0m : 1.69450
[1mStep[0m  [40/53], [94mLoss[0m : 1.59231
[1mStep[0m  [45/53], [94mLoss[0m : 1.85060
[1mStep[0m  [50/53], [94mLoss[0m : 1.65993

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.563, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54662
[1mStep[0m  [5/53], [94mLoss[0m : 1.61516
[1mStep[0m  [10/53], [94mLoss[0m : 1.50233
[1mStep[0m  [15/53], [94mLoss[0m : 1.72729
[1mStep[0m  [20/53], [94mLoss[0m : 1.59741
[1mStep[0m  [25/53], [94mLoss[0m : 1.75260
[1mStep[0m  [30/53], [94mLoss[0m : 1.53838
[1mStep[0m  [35/53], [94mLoss[0m : 1.76785
[1mStep[0m  [40/53], [94mLoss[0m : 1.79864
[1mStep[0m  [45/53], [94mLoss[0m : 1.68577
[1mStep[0m  [50/53], [94mLoss[0m : 1.64662

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.547, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68302
[1mStep[0m  [5/53], [94mLoss[0m : 1.67854
[1mStep[0m  [10/53], [94mLoss[0m : 1.62173
[1mStep[0m  [15/53], [94mLoss[0m : 1.56220
[1mStep[0m  [20/53], [94mLoss[0m : 1.56167
[1mStep[0m  [25/53], [94mLoss[0m : 1.61581
[1mStep[0m  [30/53], [94mLoss[0m : 1.66636
[1mStep[0m  [35/53], [94mLoss[0m : 1.72787
[1mStep[0m  [40/53], [94mLoss[0m : 1.66827
[1mStep[0m  [45/53], [94mLoss[0m : 1.57775
[1mStep[0m  [50/53], [94mLoss[0m : 1.45593

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.544, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.585
====================================

Phase 2 - Evaluation MAE:  2.5852448298380923
MAE score P1      2.360066
MAE score P2      2.585245
loss              1.615824
learning_rate         0.01
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.34602
[1mStep[0m  [10/106], [94mLoss[0m : 6.37522
[1mStep[0m  [20/106], [94mLoss[0m : 3.18065
[1mStep[0m  [30/106], [94mLoss[0m : 3.04716
[1mStep[0m  [40/106], [94mLoss[0m : 2.72639
[1mStep[0m  [50/106], [94mLoss[0m : 2.22143
[1mStep[0m  [60/106], [94mLoss[0m : 2.83559
[1mStep[0m  [70/106], [94mLoss[0m : 2.95616
[1mStep[0m  [80/106], [94mLoss[0m : 2.38325
[1mStep[0m  [90/106], [94mLoss[0m : 2.31478
[1mStep[0m  [100/106], [94mLoss[0m : 2.57865

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.411, [92mTest[0m: 11.578, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57866
[1mStep[0m  [10/106], [94mLoss[0m : 2.72820
[1mStep[0m  [20/106], [94mLoss[0m : 2.37266
[1mStep[0m  [30/106], [94mLoss[0m : 2.56030
[1mStep[0m  [40/106], [94mLoss[0m : 2.32775
[1mStep[0m  [50/106], [94mLoss[0m : 2.38120
[1mStep[0m  [60/106], [94mLoss[0m : 2.68122
[1mStep[0m  [70/106], [94mLoss[0m : 2.34863
[1mStep[0m  [80/106], [94mLoss[0m : 2.52735
[1mStep[0m  [90/106], [94mLoss[0m : 2.42102
[1mStep[0m  [100/106], [94mLoss[0m : 2.77854

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32217
[1mStep[0m  [10/106], [94mLoss[0m : 2.28844
[1mStep[0m  [20/106], [94mLoss[0m : 2.58761
[1mStep[0m  [30/106], [94mLoss[0m : 2.68378
[1mStep[0m  [40/106], [94mLoss[0m : 2.58808
[1mStep[0m  [50/106], [94mLoss[0m : 2.56927
[1mStep[0m  [60/106], [94mLoss[0m : 2.21257
[1mStep[0m  [70/106], [94mLoss[0m : 2.58340
[1mStep[0m  [80/106], [94mLoss[0m : 2.43483
[1mStep[0m  [90/106], [94mLoss[0m : 2.47058
[1mStep[0m  [100/106], [94mLoss[0m : 2.57527

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47686
[1mStep[0m  [10/106], [94mLoss[0m : 2.25623
[1mStep[0m  [20/106], [94mLoss[0m : 2.40699
[1mStep[0m  [30/106], [94mLoss[0m : 2.56080
[1mStep[0m  [40/106], [94mLoss[0m : 2.30259
[1mStep[0m  [50/106], [94mLoss[0m : 2.48901
[1mStep[0m  [60/106], [94mLoss[0m : 2.45120
[1mStep[0m  [70/106], [94mLoss[0m : 2.45730
[1mStep[0m  [80/106], [94mLoss[0m : 2.38466
[1mStep[0m  [90/106], [94mLoss[0m : 2.38756
[1mStep[0m  [100/106], [94mLoss[0m : 2.63572

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46451
[1mStep[0m  [10/106], [94mLoss[0m : 2.54049
[1mStep[0m  [20/106], [94mLoss[0m : 2.43868
[1mStep[0m  [30/106], [94mLoss[0m : 2.83696
[1mStep[0m  [40/106], [94mLoss[0m : 2.29470
[1mStep[0m  [50/106], [94mLoss[0m : 2.55105
[1mStep[0m  [60/106], [94mLoss[0m : 2.44694
[1mStep[0m  [70/106], [94mLoss[0m : 2.56868
[1mStep[0m  [80/106], [94mLoss[0m : 2.47000
[1mStep[0m  [90/106], [94mLoss[0m : 2.55277
[1mStep[0m  [100/106], [94mLoss[0m : 2.78535

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77400
[1mStep[0m  [10/106], [94mLoss[0m : 2.57403
[1mStep[0m  [20/106], [94mLoss[0m : 2.63020
[1mStep[0m  [30/106], [94mLoss[0m : 2.33484
[1mStep[0m  [40/106], [94mLoss[0m : 2.40458
[1mStep[0m  [50/106], [94mLoss[0m : 2.65719
[1mStep[0m  [60/106], [94mLoss[0m : 2.41591
[1mStep[0m  [70/106], [94mLoss[0m : 2.49595
[1mStep[0m  [80/106], [94mLoss[0m : 2.42571
[1mStep[0m  [90/106], [94mLoss[0m : 2.43574
[1mStep[0m  [100/106], [94mLoss[0m : 2.50439

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35447
[1mStep[0m  [10/106], [94mLoss[0m : 2.62190
[1mStep[0m  [20/106], [94mLoss[0m : 2.44028
[1mStep[0m  [30/106], [94mLoss[0m : 2.45849
[1mStep[0m  [40/106], [94mLoss[0m : 2.14714
[1mStep[0m  [50/106], [94mLoss[0m : 2.38049
[1mStep[0m  [60/106], [94mLoss[0m : 2.38657
[1mStep[0m  [70/106], [94mLoss[0m : 2.59123
[1mStep[0m  [80/106], [94mLoss[0m : 2.47484
[1mStep[0m  [90/106], [94mLoss[0m : 2.31723
[1mStep[0m  [100/106], [94mLoss[0m : 2.13508

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24196
[1mStep[0m  [10/106], [94mLoss[0m : 2.37977
[1mStep[0m  [20/106], [94mLoss[0m : 2.56951
[1mStep[0m  [30/106], [94mLoss[0m : 2.29278
[1mStep[0m  [40/106], [94mLoss[0m : 2.37400
[1mStep[0m  [50/106], [94mLoss[0m : 2.26392
[1mStep[0m  [60/106], [94mLoss[0m : 2.20931
[1mStep[0m  [70/106], [94mLoss[0m : 2.10826
[1mStep[0m  [80/106], [94mLoss[0m : 2.44034
[1mStep[0m  [90/106], [94mLoss[0m : 2.56946
[1mStep[0m  [100/106], [94mLoss[0m : 2.40862

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59638
[1mStep[0m  [10/106], [94mLoss[0m : 2.51180
[1mStep[0m  [20/106], [94mLoss[0m : 2.77478
[1mStep[0m  [30/106], [94mLoss[0m : 2.69598
[1mStep[0m  [40/106], [94mLoss[0m : 2.31803
[1mStep[0m  [50/106], [94mLoss[0m : 2.50455
[1mStep[0m  [60/106], [94mLoss[0m : 2.53607
[1mStep[0m  [70/106], [94mLoss[0m : 2.50579
[1mStep[0m  [80/106], [94mLoss[0m : 2.35289
[1mStep[0m  [90/106], [94mLoss[0m : 2.53756
[1mStep[0m  [100/106], [94mLoss[0m : 2.21032

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24523
[1mStep[0m  [10/106], [94mLoss[0m : 2.29879
[1mStep[0m  [20/106], [94mLoss[0m : 2.62720
[1mStep[0m  [30/106], [94mLoss[0m : 2.87886
[1mStep[0m  [40/106], [94mLoss[0m : 2.31073
[1mStep[0m  [50/106], [94mLoss[0m : 2.55948
[1mStep[0m  [60/106], [94mLoss[0m : 2.45799
[1mStep[0m  [70/106], [94mLoss[0m : 2.63543
[1mStep[0m  [80/106], [94mLoss[0m : 2.56065
[1mStep[0m  [90/106], [94mLoss[0m : 2.53228
[1mStep[0m  [100/106], [94mLoss[0m : 2.58778

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37226
[1mStep[0m  [10/106], [94mLoss[0m : 2.66621
[1mStep[0m  [20/106], [94mLoss[0m : 2.51017
[1mStep[0m  [30/106], [94mLoss[0m : 2.69471
[1mStep[0m  [40/106], [94mLoss[0m : 2.53860
[1mStep[0m  [50/106], [94mLoss[0m : 2.14475
[1mStep[0m  [60/106], [94mLoss[0m : 2.45733
[1mStep[0m  [70/106], [94mLoss[0m : 2.53280
[1mStep[0m  [80/106], [94mLoss[0m : 2.22141
[1mStep[0m  [90/106], [94mLoss[0m : 2.64729
[1mStep[0m  [100/106], [94mLoss[0m : 2.90543

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39804
[1mStep[0m  [10/106], [94mLoss[0m : 2.18220
[1mStep[0m  [20/106], [94mLoss[0m : 2.45806
[1mStep[0m  [30/106], [94mLoss[0m : 2.41381
[1mStep[0m  [40/106], [94mLoss[0m : 2.68613
[1mStep[0m  [50/106], [94mLoss[0m : 2.58947
[1mStep[0m  [60/106], [94mLoss[0m : 2.52953
[1mStep[0m  [70/106], [94mLoss[0m : 2.18836
[1mStep[0m  [80/106], [94mLoss[0m : 2.45101
[1mStep[0m  [90/106], [94mLoss[0m : 2.45646
[1mStep[0m  [100/106], [94mLoss[0m : 2.64894

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46528
[1mStep[0m  [10/106], [94mLoss[0m : 2.56923
[1mStep[0m  [20/106], [94mLoss[0m : 2.41993
[1mStep[0m  [30/106], [94mLoss[0m : 2.46679
[1mStep[0m  [40/106], [94mLoss[0m : 2.39033
[1mStep[0m  [50/106], [94mLoss[0m : 2.23121
[1mStep[0m  [60/106], [94mLoss[0m : 2.26905
[1mStep[0m  [70/106], [94mLoss[0m : 2.28519
[1mStep[0m  [80/106], [94mLoss[0m : 2.68449
[1mStep[0m  [90/106], [94mLoss[0m : 2.62301
[1mStep[0m  [100/106], [94mLoss[0m : 2.44925

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72890
[1mStep[0m  [10/106], [94mLoss[0m : 2.08769
[1mStep[0m  [20/106], [94mLoss[0m : 2.69324
[1mStep[0m  [30/106], [94mLoss[0m : 2.51303
[1mStep[0m  [40/106], [94mLoss[0m : 2.53794
[1mStep[0m  [50/106], [94mLoss[0m : 2.31943
[1mStep[0m  [60/106], [94mLoss[0m : 2.37757
[1mStep[0m  [70/106], [94mLoss[0m : 2.34925
[1mStep[0m  [80/106], [94mLoss[0m : 2.42321
[1mStep[0m  [90/106], [94mLoss[0m : 2.50761
[1mStep[0m  [100/106], [94mLoss[0m : 2.57376

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57258
[1mStep[0m  [10/106], [94mLoss[0m : 2.32870
[1mStep[0m  [20/106], [94mLoss[0m : 2.55748
[1mStep[0m  [30/106], [94mLoss[0m : 2.88950
[1mStep[0m  [40/106], [94mLoss[0m : 2.51005
[1mStep[0m  [50/106], [94mLoss[0m : 2.46730
[1mStep[0m  [60/106], [94mLoss[0m : 2.60520
[1mStep[0m  [70/106], [94mLoss[0m : 2.43485
[1mStep[0m  [80/106], [94mLoss[0m : 2.54987
[1mStep[0m  [90/106], [94mLoss[0m : 2.64277
[1mStep[0m  [100/106], [94mLoss[0m : 2.39468

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24329
[1mStep[0m  [10/106], [94mLoss[0m : 2.47101
[1mStep[0m  [20/106], [94mLoss[0m : 2.37006
[1mStep[0m  [30/106], [94mLoss[0m : 2.62885
[1mStep[0m  [40/106], [94mLoss[0m : 2.54106
[1mStep[0m  [50/106], [94mLoss[0m : 2.53010
[1mStep[0m  [60/106], [94mLoss[0m : 2.48104
[1mStep[0m  [70/106], [94mLoss[0m : 2.43551
[1mStep[0m  [80/106], [94mLoss[0m : 2.28042
[1mStep[0m  [90/106], [94mLoss[0m : 2.54797
[1mStep[0m  [100/106], [94mLoss[0m : 2.25369

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98180
[1mStep[0m  [10/106], [94mLoss[0m : 2.87170
[1mStep[0m  [20/106], [94mLoss[0m : 2.25706
[1mStep[0m  [30/106], [94mLoss[0m : 2.69356
[1mStep[0m  [40/106], [94mLoss[0m : 2.08379
[1mStep[0m  [50/106], [94mLoss[0m : 2.72047
[1mStep[0m  [60/106], [94mLoss[0m : 2.32134
[1mStep[0m  [70/106], [94mLoss[0m : 2.44302
[1mStep[0m  [80/106], [94mLoss[0m : 2.37836
[1mStep[0m  [90/106], [94mLoss[0m : 2.37524
[1mStep[0m  [100/106], [94mLoss[0m : 2.33636

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59299
[1mStep[0m  [10/106], [94mLoss[0m : 2.23558
[1mStep[0m  [20/106], [94mLoss[0m : 2.51523
[1mStep[0m  [30/106], [94mLoss[0m : 2.42693
[1mStep[0m  [40/106], [94mLoss[0m : 2.70434
[1mStep[0m  [50/106], [94mLoss[0m : 2.46888
[1mStep[0m  [60/106], [94mLoss[0m : 2.35872
[1mStep[0m  [70/106], [94mLoss[0m : 2.28389
[1mStep[0m  [80/106], [94mLoss[0m : 2.28317
[1mStep[0m  [90/106], [94mLoss[0m : 2.26340
[1mStep[0m  [100/106], [94mLoss[0m : 2.36504

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39731
[1mStep[0m  [10/106], [94mLoss[0m : 2.39890
[1mStep[0m  [20/106], [94mLoss[0m : 2.42518
[1mStep[0m  [30/106], [94mLoss[0m : 2.46618
[1mStep[0m  [40/106], [94mLoss[0m : 2.82140
[1mStep[0m  [50/106], [94mLoss[0m : 2.37866
[1mStep[0m  [60/106], [94mLoss[0m : 2.53508
[1mStep[0m  [70/106], [94mLoss[0m : 2.81401
[1mStep[0m  [80/106], [94mLoss[0m : 2.24130
[1mStep[0m  [90/106], [94mLoss[0m : 2.48042
[1mStep[0m  [100/106], [94mLoss[0m : 2.80788

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46142
[1mStep[0m  [10/106], [94mLoss[0m : 2.74928
[1mStep[0m  [20/106], [94mLoss[0m : 2.39025
[1mStep[0m  [30/106], [94mLoss[0m : 2.30526
[1mStep[0m  [40/106], [94mLoss[0m : 2.73718
[1mStep[0m  [50/106], [94mLoss[0m : 2.19582
[1mStep[0m  [60/106], [94mLoss[0m : 2.27396
[1mStep[0m  [70/106], [94mLoss[0m : 2.64471
[1mStep[0m  [80/106], [94mLoss[0m : 2.31379
[1mStep[0m  [90/106], [94mLoss[0m : 2.45715
[1mStep[0m  [100/106], [94mLoss[0m : 2.11885

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.392, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12350
[1mStep[0m  [10/106], [94mLoss[0m : 2.35818
[1mStep[0m  [20/106], [94mLoss[0m : 2.62350
[1mStep[0m  [30/106], [94mLoss[0m : 2.46468
[1mStep[0m  [40/106], [94mLoss[0m : 2.55513
[1mStep[0m  [50/106], [94mLoss[0m : 2.22016
[1mStep[0m  [60/106], [94mLoss[0m : 2.48005
[1mStep[0m  [70/106], [94mLoss[0m : 2.50487
[1mStep[0m  [80/106], [94mLoss[0m : 2.61983
[1mStep[0m  [90/106], [94mLoss[0m : 2.55049
[1mStep[0m  [100/106], [94mLoss[0m : 2.33341

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.384, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44872
[1mStep[0m  [10/106], [94mLoss[0m : 2.22615
[1mStep[0m  [20/106], [94mLoss[0m : 2.49634
[1mStep[0m  [30/106], [94mLoss[0m : 2.61138
[1mStep[0m  [40/106], [94mLoss[0m : 2.40752
[1mStep[0m  [50/106], [94mLoss[0m : 2.54499
[1mStep[0m  [60/106], [94mLoss[0m : 2.39219
[1mStep[0m  [70/106], [94mLoss[0m : 2.39101
[1mStep[0m  [80/106], [94mLoss[0m : 2.34017
[1mStep[0m  [90/106], [94mLoss[0m : 2.43236
[1mStep[0m  [100/106], [94mLoss[0m : 2.34258

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.412, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63282
[1mStep[0m  [10/106], [94mLoss[0m : 2.41007
[1mStep[0m  [20/106], [94mLoss[0m : 2.37216
[1mStep[0m  [30/106], [94mLoss[0m : 2.50644
[1mStep[0m  [40/106], [94mLoss[0m : 2.56916
[1mStep[0m  [50/106], [94mLoss[0m : 2.30654
[1mStep[0m  [60/106], [94mLoss[0m : 2.18448
[1mStep[0m  [70/106], [94mLoss[0m : 2.26690
[1mStep[0m  [80/106], [94mLoss[0m : 2.36471
[1mStep[0m  [90/106], [94mLoss[0m : 2.32243
[1mStep[0m  [100/106], [94mLoss[0m : 2.32909

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.395, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22425
[1mStep[0m  [10/106], [94mLoss[0m : 2.17921
[1mStep[0m  [20/106], [94mLoss[0m : 2.58730
[1mStep[0m  [30/106], [94mLoss[0m : 2.70176
[1mStep[0m  [40/106], [94mLoss[0m : 2.56169
[1mStep[0m  [50/106], [94mLoss[0m : 2.61734
[1mStep[0m  [60/106], [94mLoss[0m : 2.51525
[1mStep[0m  [70/106], [94mLoss[0m : 2.54316
[1mStep[0m  [80/106], [94mLoss[0m : 2.50607
[1mStep[0m  [90/106], [94mLoss[0m : 2.42826
[1mStep[0m  [100/106], [94mLoss[0m : 2.33242

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46646
[1mStep[0m  [10/106], [94mLoss[0m : 2.47559
[1mStep[0m  [20/106], [94mLoss[0m : 2.62453
[1mStep[0m  [30/106], [94mLoss[0m : 2.35142
[1mStep[0m  [40/106], [94mLoss[0m : 2.59400
[1mStep[0m  [50/106], [94mLoss[0m : 2.39204
[1mStep[0m  [60/106], [94mLoss[0m : 2.38682
[1mStep[0m  [70/106], [94mLoss[0m : 2.27056
[1mStep[0m  [80/106], [94mLoss[0m : 2.45615
[1mStep[0m  [90/106], [94mLoss[0m : 2.63084
[1mStep[0m  [100/106], [94mLoss[0m : 2.51013

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.391, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53917
[1mStep[0m  [10/106], [94mLoss[0m : 2.49778
[1mStep[0m  [20/106], [94mLoss[0m : 2.56445
[1mStep[0m  [30/106], [94mLoss[0m : 2.45319
[1mStep[0m  [40/106], [94mLoss[0m : 2.83090
[1mStep[0m  [50/106], [94mLoss[0m : 2.34284
[1mStep[0m  [60/106], [94mLoss[0m : 2.68557
[1mStep[0m  [70/106], [94mLoss[0m : 2.15526
[1mStep[0m  [80/106], [94mLoss[0m : 2.50982
[1mStep[0m  [90/106], [94mLoss[0m : 2.84553
[1mStep[0m  [100/106], [94mLoss[0m : 2.31312

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29907
[1mStep[0m  [10/106], [94mLoss[0m : 2.67323
[1mStep[0m  [20/106], [94mLoss[0m : 2.36775
[1mStep[0m  [30/106], [94mLoss[0m : 2.23272
[1mStep[0m  [40/106], [94mLoss[0m : 2.26180
[1mStep[0m  [50/106], [94mLoss[0m : 2.60336
[1mStep[0m  [60/106], [94mLoss[0m : 2.42510
[1mStep[0m  [70/106], [94mLoss[0m : 2.33470
[1mStep[0m  [80/106], [94mLoss[0m : 2.36704
[1mStep[0m  [90/106], [94mLoss[0m : 2.36011
[1mStep[0m  [100/106], [94mLoss[0m : 2.30613

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.395, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30265
[1mStep[0m  [10/106], [94mLoss[0m : 2.53884
[1mStep[0m  [20/106], [94mLoss[0m : 2.69138
[1mStep[0m  [30/106], [94mLoss[0m : 2.23297
[1mStep[0m  [40/106], [94mLoss[0m : 2.42733
[1mStep[0m  [50/106], [94mLoss[0m : 2.51372
[1mStep[0m  [60/106], [94mLoss[0m : 2.28112
[1mStep[0m  [70/106], [94mLoss[0m : 2.41725
[1mStep[0m  [80/106], [94mLoss[0m : 2.39010
[1mStep[0m  [90/106], [94mLoss[0m : 2.64224
[1mStep[0m  [100/106], [94mLoss[0m : 2.31246

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.388, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57150
[1mStep[0m  [10/106], [94mLoss[0m : 2.40093
[1mStep[0m  [20/106], [94mLoss[0m : 2.59556
[1mStep[0m  [30/106], [94mLoss[0m : 2.31208
[1mStep[0m  [40/106], [94mLoss[0m : 2.22063
[1mStep[0m  [50/106], [94mLoss[0m : 2.49666
[1mStep[0m  [60/106], [94mLoss[0m : 2.46714
[1mStep[0m  [70/106], [94mLoss[0m : 2.53912
[1mStep[0m  [80/106], [94mLoss[0m : 2.40381
[1mStep[0m  [90/106], [94mLoss[0m : 2.71401
[1mStep[0m  [100/106], [94mLoss[0m : 2.30392

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.394, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47263
[1mStep[0m  [10/106], [94mLoss[0m : 2.35276
[1mStep[0m  [20/106], [94mLoss[0m : 2.12292
[1mStep[0m  [30/106], [94mLoss[0m : 2.33728
[1mStep[0m  [40/106], [94mLoss[0m : 2.32306
[1mStep[0m  [50/106], [94mLoss[0m : 2.48092
[1mStep[0m  [60/106], [94mLoss[0m : 2.34213
[1mStep[0m  [70/106], [94mLoss[0m : 2.41984
[1mStep[0m  [80/106], [94mLoss[0m : 2.38948
[1mStep[0m  [90/106], [94mLoss[0m : 2.42193
[1mStep[0m  [100/106], [94mLoss[0m : 2.50226

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.398
====================================

Phase 1 - Evaluation MAE:  2.3977335399051882
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.37701
[1mStep[0m  [10/106], [94mLoss[0m : 2.39734
[1mStep[0m  [20/106], [94mLoss[0m : 2.52299
[1mStep[0m  [30/106], [94mLoss[0m : 2.54452
[1mStep[0m  [40/106], [94mLoss[0m : 2.53723
[1mStep[0m  [50/106], [94mLoss[0m : 2.74307
[1mStep[0m  [60/106], [94mLoss[0m : 2.28124
[1mStep[0m  [70/106], [94mLoss[0m : 2.70548
[1mStep[0m  [80/106], [94mLoss[0m : 2.47024
[1mStep[0m  [90/106], [94mLoss[0m : 2.31547
[1mStep[0m  [100/106], [94mLoss[0m : 2.30846

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36054
[1mStep[0m  [10/106], [94mLoss[0m : 2.34922
[1mStep[0m  [20/106], [94mLoss[0m : 2.23066
[1mStep[0m  [30/106], [94mLoss[0m : 2.37565
[1mStep[0m  [40/106], [94mLoss[0m : 2.26116
[1mStep[0m  [50/106], [94mLoss[0m : 2.79321
[1mStep[0m  [60/106], [94mLoss[0m : 2.26724
[1mStep[0m  [70/106], [94mLoss[0m : 2.42118
[1mStep[0m  [80/106], [94mLoss[0m : 2.38393
[1mStep[0m  [90/106], [94mLoss[0m : 2.45583
[1mStep[0m  [100/106], [94mLoss[0m : 2.40905

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31193
[1mStep[0m  [10/106], [94mLoss[0m : 2.64147
[1mStep[0m  [20/106], [94mLoss[0m : 2.27785
[1mStep[0m  [30/106], [94mLoss[0m : 2.35344
[1mStep[0m  [40/106], [94mLoss[0m : 2.29695
[1mStep[0m  [50/106], [94mLoss[0m : 2.35269
[1mStep[0m  [60/106], [94mLoss[0m : 2.24846
[1mStep[0m  [70/106], [94mLoss[0m : 2.43510
[1mStep[0m  [80/106], [94mLoss[0m : 2.43465
[1mStep[0m  [90/106], [94mLoss[0m : 2.10891
[1mStep[0m  [100/106], [94mLoss[0m : 2.14256

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95138
[1mStep[0m  [10/106], [94mLoss[0m : 2.52660
[1mStep[0m  [20/106], [94mLoss[0m : 2.10582
[1mStep[0m  [30/106], [94mLoss[0m : 2.09394
[1mStep[0m  [40/106], [94mLoss[0m : 2.28519
[1mStep[0m  [50/106], [94mLoss[0m : 2.33106
[1mStep[0m  [60/106], [94mLoss[0m : 2.09780
[1mStep[0m  [70/106], [94mLoss[0m : 2.16272
[1mStep[0m  [80/106], [94mLoss[0m : 2.10570
[1mStep[0m  [90/106], [94mLoss[0m : 2.32665
[1mStep[0m  [100/106], [94mLoss[0m : 2.44451

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.04252
[1mStep[0m  [10/106], [94mLoss[0m : 1.93921
[1mStep[0m  [20/106], [94mLoss[0m : 2.25793
[1mStep[0m  [30/106], [94mLoss[0m : 2.43495
[1mStep[0m  [40/106], [94mLoss[0m : 1.97960
[1mStep[0m  [50/106], [94mLoss[0m : 2.00944
[1mStep[0m  [60/106], [94mLoss[0m : 2.02002
[1mStep[0m  [70/106], [94mLoss[0m : 2.15749
[1mStep[0m  [80/106], [94mLoss[0m : 2.20670
[1mStep[0m  [90/106], [94mLoss[0m : 2.07400
[1mStep[0m  [100/106], [94mLoss[0m : 2.25285

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.99997
[1mStep[0m  [10/106], [94mLoss[0m : 2.38209
[1mStep[0m  [20/106], [94mLoss[0m : 1.97797
[1mStep[0m  [30/106], [94mLoss[0m : 2.30745
[1mStep[0m  [40/106], [94mLoss[0m : 2.24941
[1mStep[0m  [50/106], [94mLoss[0m : 2.33416
[1mStep[0m  [60/106], [94mLoss[0m : 2.17814
[1mStep[0m  [70/106], [94mLoss[0m : 1.92587
[1mStep[0m  [80/106], [94mLoss[0m : 1.82520
[1mStep[0m  [90/106], [94mLoss[0m : 1.95495
[1mStep[0m  [100/106], [94mLoss[0m : 1.95201

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96581
[1mStep[0m  [10/106], [94mLoss[0m : 2.09021
[1mStep[0m  [20/106], [94mLoss[0m : 2.05386
[1mStep[0m  [30/106], [94mLoss[0m : 2.29304
[1mStep[0m  [40/106], [94mLoss[0m : 1.86246
[1mStep[0m  [50/106], [94mLoss[0m : 2.24854
[1mStep[0m  [60/106], [94mLoss[0m : 2.12575
[1mStep[0m  [70/106], [94mLoss[0m : 1.92057
[1mStep[0m  [80/106], [94mLoss[0m : 1.99126
[1mStep[0m  [90/106], [94mLoss[0m : 2.05869
[1mStep[0m  [100/106], [94mLoss[0m : 2.21049

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87452
[1mStep[0m  [10/106], [94mLoss[0m : 1.76387
[1mStep[0m  [20/106], [94mLoss[0m : 2.04875
[1mStep[0m  [30/106], [94mLoss[0m : 1.91425
[1mStep[0m  [40/106], [94mLoss[0m : 2.05726
[1mStep[0m  [50/106], [94mLoss[0m : 2.44186
[1mStep[0m  [60/106], [94mLoss[0m : 2.04010
[1mStep[0m  [70/106], [94mLoss[0m : 1.85318
[1mStep[0m  [80/106], [94mLoss[0m : 2.29152
[1mStep[0m  [90/106], [94mLoss[0m : 2.08780
[1mStep[0m  [100/106], [94mLoss[0m : 1.83780

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79206
[1mStep[0m  [10/106], [94mLoss[0m : 2.08534
[1mStep[0m  [20/106], [94mLoss[0m : 1.90764
[1mStep[0m  [30/106], [94mLoss[0m : 2.03744
[1mStep[0m  [40/106], [94mLoss[0m : 1.74100
[1mStep[0m  [50/106], [94mLoss[0m : 1.65359
[1mStep[0m  [60/106], [94mLoss[0m : 2.12235
[1mStep[0m  [70/106], [94mLoss[0m : 1.94195
[1mStep[0m  [80/106], [94mLoss[0m : 2.28612
[1mStep[0m  [90/106], [94mLoss[0m : 2.02517
[1mStep[0m  [100/106], [94mLoss[0m : 2.08182

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.954, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.03645
[1mStep[0m  [10/106], [94mLoss[0m : 1.78145
[1mStep[0m  [20/106], [94mLoss[0m : 1.81411
[1mStep[0m  [30/106], [94mLoss[0m : 1.86122
[1mStep[0m  [40/106], [94mLoss[0m : 2.09648
[1mStep[0m  [50/106], [94mLoss[0m : 2.02664
[1mStep[0m  [60/106], [94mLoss[0m : 2.11526
[1mStep[0m  [70/106], [94mLoss[0m : 2.18056
[1mStep[0m  [80/106], [94mLoss[0m : 1.86070
[1mStep[0m  [90/106], [94mLoss[0m : 1.85647
[1mStep[0m  [100/106], [94mLoss[0m : 1.92802

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13610
[1mStep[0m  [10/106], [94mLoss[0m : 1.91216
[1mStep[0m  [20/106], [94mLoss[0m : 1.61809
[1mStep[0m  [30/106], [94mLoss[0m : 1.65569
[1mStep[0m  [40/106], [94mLoss[0m : 1.75501
[1mStep[0m  [50/106], [94mLoss[0m : 1.91705
[1mStep[0m  [60/106], [94mLoss[0m : 1.70567
[1mStep[0m  [70/106], [94mLoss[0m : 1.62201
[1mStep[0m  [80/106], [94mLoss[0m : 1.71366
[1mStep[0m  [90/106], [94mLoss[0m : 1.88099
[1mStep[0m  [100/106], [94mLoss[0m : 1.86463

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.71124
[1mStep[0m  [10/106], [94mLoss[0m : 1.95609
[1mStep[0m  [20/106], [94mLoss[0m : 1.92898
[1mStep[0m  [30/106], [94mLoss[0m : 1.79279
[1mStep[0m  [40/106], [94mLoss[0m : 1.85313
[1mStep[0m  [50/106], [94mLoss[0m : 1.91381
[1mStep[0m  [60/106], [94mLoss[0m : 1.85787
[1mStep[0m  [70/106], [94mLoss[0m : 1.91756
[1mStep[0m  [80/106], [94mLoss[0m : 1.79585
[1mStep[0m  [90/106], [94mLoss[0m : 1.82551
[1mStep[0m  [100/106], [94mLoss[0m : 2.18455

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77496
[1mStep[0m  [10/106], [94mLoss[0m : 1.95978
[1mStep[0m  [20/106], [94mLoss[0m : 1.64142
[1mStep[0m  [30/106], [94mLoss[0m : 1.79512
[1mStep[0m  [40/106], [94mLoss[0m : 1.66091
[1mStep[0m  [50/106], [94mLoss[0m : 1.52555
[1mStep[0m  [60/106], [94mLoss[0m : 1.66825
[1mStep[0m  [70/106], [94mLoss[0m : 1.96961
[1mStep[0m  [80/106], [94mLoss[0m : 1.76541
[1mStep[0m  [90/106], [94mLoss[0m : 2.11557
[1mStep[0m  [100/106], [94mLoss[0m : 1.99561

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59484
[1mStep[0m  [10/106], [94mLoss[0m : 1.54501
[1mStep[0m  [20/106], [94mLoss[0m : 1.66376
[1mStep[0m  [30/106], [94mLoss[0m : 1.75428
[1mStep[0m  [40/106], [94mLoss[0m : 1.91082
[1mStep[0m  [50/106], [94mLoss[0m : 1.90576
[1mStep[0m  [60/106], [94mLoss[0m : 1.83787
[1mStep[0m  [70/106], [94mLoss[0m : 2.01116
[1mStep[0m  [80/106], [94mLoss[0m : 1.73197
[1mStep[0m  [90/106], [94mLoss[0m : 1.84355
[1mStep[0m  [100/106], [94mLoss[0m : 1.75277

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.534, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64086
[1mStep[0m  [10/106], [94mLoss[0m : 1.80520
[1mStep[0m  [20/106], [94mLoss[0m : 1.83772
[1mStep[0m  [30/106], [94mLoss[0m : 1.56343
[1mStep[0m  [40/106], [94mLoss[0m : 1.81155
[1mStep[0m  [50/106], [94mLoss[0m : 1.78090
[1mStep[0m  [60/106], [94mLoss[0m : 1.67577
[1mStep[0m  [70/106], [94mLoss[0m : 1.66824
[1mStep[0m  [80/106], [94mLoss[0m : 1.73164
[1mStep[0m  [90/106], [94mLoss[0m : 1.86739
[1mStep[0m  [100/106], [94mLoss[0m : 1.90085

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77997
[1mStep[0m  [10/106], [94mLoss[0m : 1.67711
[1mStep[0m  [20/106], [94mLoss[0m : 1.69668
[1mStep[0m  [30/106], [94mLoss[0m : 1.74402
[1mStep[0m  [40/106], [94mLoss[0m : 1.79106
[1mStep[0m  [50/106], [94mLoss[0m : 1.90414
[1mStep[0m  [60/106], [94mLoss[0m : 1.53247
[1mStep[0m  [70/106], [94mLoss[0m : 1.68321
[1mStep[0m  [80/106], [94mLoss[0m : 2.01422
[1mStep[0m  [90/106], [94mLoss[0m : 2.15197
[1mStep[0m  [100/106], [94mLoss[0m : 1.47429

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64897
[1mStep[0m  [10/106], [94mLoss[0m : 1.56923
[1mStep[0m  [20/106], [94mLoss[0m : 1.75141
[1mStep[0m  [30/106], [94mLoss[0m : 1.62014
[1mStep[0m  [40/106], [94mLoss[0m : 1.78471
[1mStep[0m  [50/106], [94mLoss[0m : 1.74234
[1mStep[0m  [60/106], [94mLoss[0m : 1.60950
[1mStep[0m  [70/106], [94mLoss[0m : 1.95427
[1mStep[0m  [80/106], [94mLoss[0m : 1.84851
[1mStep[0m  [90/106], [94mLoss[0m : 1.72077
[1mStep[0m  [100/106], [94mLoss[0m : 1.64319

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.51434
[1mStep[0m  [10/106], [94mLoss[0m : 1.58652
[1mStep[0m  [20/106], [94mLoss[0m : 1.75216
[1mStep[0m  [30/106], [94mLoss[0m : 1.51704
[1mStep[0m  [40/106], [94mLoss[0m : 1.80795
[1mStep[0m  [50/106], [94mLoss[0m : 1.52314
[1mStep[0m  [60/106], [94mLoss[0m : 1.63219
[1mStep[0m  [70/106], [94mLoss[0m : 1.48155
[1mStep[0m  [80/106], [94mLoss[0m : 1.90618
[1mStep[0m  [90/106], [94mLoss[0m : 1.80653
[1mStep[0m  [100/106], [94mLoss[0m : 1.77913

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58239
[1mStep[0m  [10/106], [94mLoss[0m : 1.59399
[1mStep[0m  [20/106], [94mLoss[0m : 1.76897
[1mStep[0m  [30/106], [94mLoss[0m : 1.69417
[1mStep[0m  [40/106], [94mLoss[0m : 1.58353
[1mStep[0m  [50/106], [94mLoss[0m : 1.57319
[1mStep[0m  [60/106], [94mLoss[0m : 1.62604
[1mStep[0m  [70/106], [94mLoss[0m : 1.71942
[1mStep[0m  [80/106], [94mLoss[0m : 1.95930
[1mStep[0m  [90/106], [94mLoss[0m : 1.53482
[1mStep[0m  [100/106], [94mLoss[0m : 1.67715

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.526, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55730
[1mStep[0m  [10/106], [94mLoss[0m : 1.49134
[1mStep[0m  [20/106], [94mLoss[0m : 1.68675
[1mStep[0m  [30/106], [94mLoss[0m : 1.49092
[1mStep[0m  [40/106], [94mLoss[0m : 1.34942
[1mStep[0m  [50/106], [94mLoss[0m : 1.43357
[1mStep[0m  [60/106], [94mLoss[0m : 1.55176
[1mStep[0m  [70/106], [94mLoss[0m : 1.64902
[1mStep[0m  [80/106], [94mLoss[0m : 1.21584
[1mStep[0m  [90/106], [94mLoss[0m : 1.56839
[1mStep[0m  [100/106], [94mLoss[0m : 1.90397

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.47289
[1mStep[0m  [10/106], [94mLoss[0m : 1.28965
[1mStep[0m  [20/106], [94mLoss[0m : 1.44845
[1mStep[0m  [30/106], [94mLoss[0m : 1.52546
[1mStep[0m  [40/106], [94mLoss[0m : 1.56267
[1mStep[0m  [50/106], [94mLoss[0m : 1.51623
[1mStep[0m  [60/106], [94mLoss[0m : 1.43332
[1mStep[0m  [70/106], [94mLoss[0m : 1.51063
[1mStep[0m  [80/106], [94mLoss[0m : 1.45668
[1mStep[0m  [90/106], [94mLoss[0m : 1.35680
[1mStep[0m  [100/106], [94mLoss[0m : 1.58370

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.535, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.47638
[1mStep[0m  [10/106], [94mLoss[0m : 1.39057
[1mStep[0m  [20/106], [94mLoss[0m : 1.46471
[1mStep[0m  [30/106], [94mLoss[0m : 1.64820
[1mStep[0m  [40/106], [94mLoss[0m : 1.53401
[1mStep[0m  [50/106], [94mLoss[0m : 1.60887
[1mStep[0m  [60/106], [94mLoss[0m : 1.46185
[1mStep[0m  [70/106], [94mLoss[0m : 1.27445
[1mStep[0m  [80/106], [94mLoss[0m : 1.64788
[1mStep[0m  [90/106], [94mLoss[0m : 1.54260
[1mStep[0m  [100/106], [94mLoss[0m : 1.43094

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44619
[1mStep[0m  [10/106], [94mLoss[0m : 1.50726
[1mStep[0m  [20/106], [94mLoss[0m : 1.40714
[1mStep[0m  [30/106], [94mLoss[0m : 1.74411
[1mStep[0m  [40/106], [94mLoss[0m : 1.62879
[1mStep[0m  [50/106], [94mLoss[0m : 1.40804
[1mStep[0m  [60/106], [94mLoss[0m : 1.47776
[1mStep[0m  [70/106], [94mLoss[0m : 1.80952
[1mStep[0m  [80/106], [94mLoss[0m : 1.56251
[1mStep[0m  [90/106], [94mLoss[0m : 1.68164
[1mStep[0m  [100/106], [94mLoss[0m : 1.57983

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.537, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.38070
[1mStep[0m  [10/106], [94mLoss[0m : 1.47828
[1mStep[0m  [20/106], [94mLoss[0m : 1.31699
[1mStep[0m  [30/106], [94mLoss[0m : 1.54725
[1mStep[0m  [40/106], [94mLoss[0m : 1.55914
[1mStep[0m  [50/106], [94mLoss[0m : 1.49894
[1mStep[0m  [60/106], [94mLoss[0m : 1.41775
[1mStep[0m  [70/106], [94mLoss[0m : 1.60022
[1mStep[0m  [80/106], [94mLoss[0m : 1.38264
[1mStep[0m  [90/106], [94mLoss[0m : 1.52180
[1mStep[0m  [100/106], [94mLoss[0m : 1.37945

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39943
[1mStep[0m  [10/106], [94mLoss[0m : 1.44869
[1mStep[0m  [20/106], [94mLoss[0m : 1.53067
[1mStep[0m  [30/106], [94mLoss[0m : 1.50450
[1mStep[0m  [40/106], [94mLoss[0m : 1.34427
[1mStep[0m  [50/106], [94mLoss[0m : 1.35324
[1mStep[0m  [60/106], [94mLoss[0m : 1.43143
[1mStep[0m  [70/106], [94mLoss[0m : 1.41140
[1mStep[0m  [80/106], [94mLoss[0m : 1.45109
[1mStep[0m  [90/106], [94mLoss[0m : 1.49138
[1mStep[0m  [100/106], [94mLoss[0m : 1.30132

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.517, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61934
[1mStep[0m  [10/106], [94mLoss[0m : 1.45757
[1mStep[0m  [20/106], [94mLoss[0m : 1.80010
[1mStep[0m  [30/106], [94mLoss[0m : 1.48116
[1mStep[0m  [40/106], [94mLoss[0m : 1.26421
[1mStep[0m  [50/106], [94mLoss[0m : 1.35432
[1mStep[0m  [60/106], [94mLoss[0m : 1.64858
[1mStep[0m  [70/106], [94mLoss[0m : 1.49685
[1mStep[0m  [80/106], [94mLoss[0m : 1.54983
[1mStep[0m  [90/106], [94mLoss[0m : 1.65609
[1mStep[0m  [100/106], [94mLoss[0m : 1.68102

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.511
====================================

Phase 2 - Evaluation MAE:  2.5106381470302366
MAE score P1       2.397734
MAE score P2       2.510638
loss               1.440864
learning_rate          0.01
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay          0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.66496
[1mStep[0m  [10/106], [94mLoss[0m : 10.78443
[1mStep[0m  [20/106], [94mLoss[0m : 10.34780
[1mStep[0m  [30/106], [94mLoss[0m : 10.66234
[1mStep[0m  [40/106], [94mLoss[0m : 10.01196
[1mStep[0m  [50/106], [94mLoss[0m : 9.04815
[1mStep[0m  [60/106], [94mLoss[0m : 8.93631
[1mStep[0m  [70/106], [94mLoss[0m : 8.27942
[1mStep[0m  [80/106], [94mLoss[0m : 7.95977
[1mStep[0m  [90/106], [94mLoss[0m : 7.32258
[1mStep[0m  [100/106], [94mLoss[0m : 7.12341

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.124, [92mTest[0m: 10.963, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.37246
[1mStep[0m  [10/106], [94mLoss[0m : 6.05966
[1mStep[0m  [20/106], [94mLoss[0m : 5.87303
[1mStep[0m  [30/106], [94mLoss[0m : 5.37796
[1mStep[0m  [40/106], [94mLoss[0m : 4.35839
[1mStep[0m  [50/106], [94mLoss[0m : 4.98030
[1mStep[0m  [60/106], [94mLoss[0m : 5.91331
[1mStep[0m  [70/106], [94mLoss[0m : 4.01704
[1mStep[0m  [80/106], [94mLoss[0m : 4.32685
[1mStep[0m  [90/106], [94mLoss[0m : 3.54607
[1mStep[0m  [100/106], [94mLoss[0m : 3.74227

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.954, [92mTest[0m: 6.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.14702
[1mStep[0m  [10/106], [94mLoss[0m : 3.01470
[1mStep[0m  [20/106], [94mLoss[0m : 2.75606
[1mStep[0m  [30/106], [94mLoss[0m : 2.67273
[1mStep[0m  [40/106], [94mLoss[0m : 2.74168
[1mStep[0m  [50/106], [94mLoss[0m : 2.92888
[1mStep[0m  [60/106], [94mLoss[0m : 2.62625
[1mStep[0m  [70/106], [94mLoss[0m : 2.71954
[1mStep[0m  [80/106], [94mLoss[0m : 3.02442
[1mStep[0m  [90/106], [94mLoss[0m : 2.44776
[1mStep[0m  [100/106], [94mLoss[0m : 2.59369

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.919, [92mTest[0m: 2.810, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71274
[1mStep[0m  [10/106], [94mLoss[0m : 2.95257
[1mStep[0m  [20/106], [94mLoss[0m : 2.52777
[1mStep[0m  [30/106], [94mLoss[0m : 2.82584
[1mStep[0m  [40/106], [94mLoss[0m : 2.62092
[1mStep[0m  [50/106], [94mLoss[0m : 2.78614
[1mStep[0m  [60/106], [94mLoss[0m : 2.59110
[1mStep[0m  [70/106], [94mLoss[0m : 2.72412
[1mStep[0m  [80/106], [94mLoss[0m : 2.33908
[1mStep[0m  [90/106], [94mLoss[0m : 2.75102
[1mStep[0m  [100/106], [94mLoss[0m : 2.73142

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54611
[1mStep[0m  [10/106], [94mLoss[0m : 2.71667
[1mStep[0m  [20/106], [94mLoss[0m : 2.78214
[1mStep[0m  [30/106], [94mLoss[0m : 2.69619
[1mStep[0m  [40/106], [94mLoss[0m : 2.52749
[1mStep[0m  [50/106], [94mLoss[0m : 2.48285
[1mStep[0m  [60/106], [94mLoss[0m : 2.82765
[1mStep[0m  [70/106], [94mLoss[0m : 3.25956
[1mStep[0m  [80/106], [94mLoss[0m : 2.72866
[1mStep[0m  [90/106], [94mLoss[0m : 2.63944
[1mStep[0m  [100/106], [94mLoss[0m : 2.60704

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65013
[1mStep[0m  [10/106], [94mLoss[0m : 2.52916
[1mStep[0m  [20/106], [94mLoss[0m : 2.61302
[1mStep[0m  [30/106], [94mLoss[0m : 2.77450
[1mStep[0m  [40/106], [94mLoss[0m : 2.70836
[1mStep[0m  [50/106], [94mLoss[0m : 2.32748
[1mStep[0m  [60/106], [94mLoss[0m : 2.69224
[1mStep[0m  [70/106], [94mLoss[0m : 3.02028
[1mStep[0m  [80/106], [94mLoss[0m : 2.95768
[1mStep[0m  [90/106], [94mLoss[0m : 2.42718
[1mStep[0m  [100/106], [94mLoss[0m : 2.83218

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45628
[1mStep[0m  [10/106], [94mLoss[0m : 2.84194
[1mStep[0m  [20/106], [94mLoss[0m : 2.80170
[1mStep[0m  [30/106], [94mLoss[0m : 2.77605
[1mStep[0m  [40/106], [94mLoss[0m : 2.51138
[1mStep[0m  [50/106], [94mLoss[0m : 2.66249
[1mStep[0m  [60/106], [94mLoss[0m : 2.65184
[1mStep[0m  [70/106], [94mLoss[0m : 2.82512
[1mStep[0m  [80/106], [94mLoss[0m : 2.43314
[1mStep[0m  [90/106], [94mLoss[0m : 2.79262
[1mStep[0m  [100/106], [94mLoss[0m : 2.68705

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23142
[1mStep[0m  [10/106], [94mLoss[0m : 2.59569
[1mStep[0m  [20/106], [94mLoss[0m : 2.19731
[1mStep[0m  [30/106], [94mLoss[0m : 2.59746
[1mStep[0m  [40/106], [94mLoss[0m : 2.76064
[1mStep[0m  [50/106], [94mLoss[0m : 2.47337
[1mStep[0m  [60/106], [94mLoss[0m : 2.67873
[1mStep[0m  [70/106], [94mLoss[0m : 2.67673
[1mStep[0m  [80/106], [94mLoss[0m : 2.47897
[1mStep[0m  [90/106], [94mLoss[0m : 2.44691
[1mStep[0m  [100/106], [94mLoss[0m : 2.59835

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78670
[1mStep[0m  [10/106], [94mLoss[0m : 2.79054
[1mStep[0m  [20/106], [94mLoss[0m : 2.41915
[1mStep[0m  [30/106], [94mLoss[0m : 2.53994
[1mStep[0m  [40/106], [94mLoss[0m : 2.64670
[1mStep[0m  [50/106], [94mLoss[0m : 2.38072
[1mStep[0m  [60/106], [94mLoss[0m : 2.25295
[1mStep[0m  [70/106], [94mLoss[0m : 2.29492
[1mStep[0m  [80/106], [94mLoss[0m : 2.50221
[1mStep[0m  [90/106], [94mLoss[0m : 2.49939
[1mStep[0m  [100/106], [94mLoss[0m : 2.58384

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71901
[1mStep[0m  [10/106], [94mLoss[0m : 2.58319
[1mStep[0m  [20/106], [94mLoss[0m : 2.65943
[1mStep[0m  [30/106], [94mLoss[0m : 2.76241
[1mStep[0m  [40/106], [94mLoss[0m : 2.27245
[1mStep[0m  [50/106], [94mLoss[0m : 2.62013
[1mStep[0m  [60/106], [94mLoss[0m : 2.33675
[1mStep[0m  [70/106], [94mLoss[0m : 2.70939
[1mStep[0m  [80/106], [94mLoss[0m : 2.59283
[1mStep[0m  [90/106], [94mLoss[0m : 2.51009
[1mStep[0m  [100/106], [94mLoss[0m : 2.59205

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48676
[1mStep[0m  [10/106], [94mLoss[0m : 2.15949
[1mStep[0m  [20/106], [94mLoss[0m : 2.85593
[1mStep[0m  [30/106], [94mLoss[0m : 2.39632
[1mStep[0m  [40/106], [94mLoss[0m : 2.60673
[1mStep[0m  [50/106], [94mLoss[0m : 2.34101
[1mStep[0m  [60/106], [94mLoss[0m : 2.35317
[1mStep[0m  [70/106], [94mLoss[0m : 2.47596
[1mStep[0m  [80/106], [94mLoss[0m : 2.71112
[1mStep[0m  [90/106], [94mLoss[0m : 2.58878
[1mStep[0m  [100/106], [94mLoss[0m : 2.79519

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37264
[1mStep[0m  [10/106], [94mLoss[0m : 2.56980
[1mStep[0m  [20/106], [94mLoss[0m : 2.66732
[1mStep[0m  [30/106], [94mLoss[0m : 2.37896
[1mStep[0m  [40/106], [94mLoss[0m : 2.42943
[1mStep[0m  [50/106], [94mLoss[0m : 2.39197
[1mStep[0m  [60/106], [94mLoss[0m : 2.53586
[1mStep[0m  [70/106], [94mLoss[0m : 2.55722
[1mStep[0m  [80/106], [94mLoss[0m : 2.60516
[1mStep[0m  [90/106], [94mLoss[0m : 2.38560
[1mStep[0m  [100/106], [94mLoss[0m : 2.54353

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74592
[1mStep[0m  [10/106], [94mLoss[0m : 2.19422
[1mStep[0m  [20/106], [94mLoss[0m : 2.29289
[1mStep[0m  [30/106], [94mLoss[0m : 2.43519
[1mStep[0m  [40/106], [94mLoss[0m : 2.63790
[1mStep[0m  [50/106], [94mLoss[0m : 2.41789
[1mStep[0m  [60/106], [94mLoss[0m : 2.75728
[1mStep[0m  [70/106], [94mLoss[0m : 2.58548
[1mStep[0m  [80/106], [94mLoss[0m : 2.69954
[1mStep[0m  [90/106], [94mLoss[0m : 2.42881
[1mStep[0m  [100/106], [94mLoss[0m : 2.29988

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32287
[1mStep[0m  [10/106], [94mLoss[0m : 2.54999
[1mStep[0m  [20/106], [94mLoss[0m : 2.09024
[1mStep[0m  [30/106], [94mLoss[0m : 2.45801
[1mStep[0m  [40/106], [94mLoss[0m : 2.51615
[1mStep[0m  [50/106], [94mLoss[0m : 2.77362
[1mStep[0m  [60/106], [94mLoss[0m : 2.56669
[1mStep[0m  [70/106], [94mLoss[0m : 2.43374
[1mStep[0m  [80/106], [94mLoss[0m : 2.47048
[1mStep[0m  [90/106], [94mLoss[0m : 2.45978
[1mStep[0m  [100/106], [94mLoss[0m : 2.70112

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33651
[1mStep[0m  [10/106], [94mLoss[0m : 2.40486
[1mStep[0m  [20/106], [94mLoss[0m : 2.39741
[1mStep[0m  [30/106], [94mLoss[0m : 2.66033
[1mStep[0m  [40/106], [94mLoss[0m : 2.43260
[1mStep[0m  [50/106], [94mLoss[0m : 2.49692
[1mStep[0m  [60/106], [94mLoss[0m : 2.55713
[1mStep[0m  [70/106], [94mLoss[0m : 2.22911
[1mStep[0m  [80/106], [94mLoss[0m : 2.62772
[1mStep[0m  [90/106], [94mLoss[0m : 2.58532
[1mStep[0m  [100/106], [94mLoss[0m : 2.29865

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71580
[1mStep[0m  [10/106], [94mLoss[0m : 2.43760
[1mStep[0m  [20/106], [94mLoss[0m : 2.60131
[1mStep[0m  [30/106], [94mLoss[0m : 2.36519
[1mStep[0m  [40/106], [94mLoss[0m : 2.59702
[1mStep[0m  [50/106], [94mLoss[0m : 2.12412
[1mStep[0m  [60/106], [94mLoss[0m : 2.65692
[1mStep[0m  [70/106], [94mLoss[0m : 2.20454
[1mStep[0m  [80/106], [94mLoss[0m : 2.54697
[1mStep[0m  [90/106], [94mLoss[0m : 2.36846
[1mStep[0m  [100/106], [94mLoss[0m : 2.77558

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45723
[1mStep[0m  [10/106], [94mLoss[0m : 2.57731
[1mStep[0m  [20/106], [94mLoss[0m : 2.37421
[1mStep[0m  [30/106], [94mLoss[0m : 2.31854
[1mStep[0m  [40/106], [94mLoss[0m : 2.41684
[1mStep[0m  [50/106], [94mLoss[0m : 2.64625
[1mStep[0m  [60/106], [94mLoss[0m : 2.28655
[1mStep[0m  [70/106], [94mLoss[0m : 2.65257
[1mStep[0m  [80/106], [94mLoss[0m : 2.46459
[1mStep[0m  [90/106], [94mLoss[0m : 2.48284
[1mStep[0m  [100/106], [94mLoss[0m : 2.57193

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57556
[1mStep[0m  [10/106], [94mLoss[0m : 2.38124
[1mStep[0m  [20/106], [94mLoss[0m : 2.49277
[1mStep[0m  [30/106], [94mLoss[0m : 2.53855
[1mStep[0m  [40/106], [94mLoss[0m : 2.66431
[1mStep[0m  [50/106], [94mLoss[0m : 2.82784
[1mStep[0m  [60/106], [94mLoss[0m : 2.45668
[1mStep[0m  [70/106], [94mLoss[0m : 2.75723
[1mStep[0m  [80/106], [94mLoss[0m : 2.37570
[1mStep[0m  [90/106], [94mLoss[0m : 2.50180
[1mStep[0m  [100/106], [94mLoss[0m : 2.61794

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49590
[1mStep[0m  [10/106], [94mLoss[0m : 2.40570
[1mStep[0m  [20/106], [94mLoss[0m : 2.35963
[1mStep[0m  [30/106], [94mLoss[0m : 2.80308
[1mStep[0m  [40/106], [94mLoss[0m : 2.70118
[1mStep[0m  [50/106], [94mLoss[0m : 2.61487
[1mStep[0m  [60/106], [94mLoss[0m : 2.68744
[1mStep[0m  [70/106], [94mLoss[0m : 2.50303
[1mStep[0m  [80/106], [94mLoss[0m : 2.51633
[1mStep[0m  [90/106], [94mLoss[0m : 2.59372
[1mStep[0m  [100/106], [94mLoss[0m : 2.58069

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31985
[1mStep[0m  [10/106], [94mLoss[0m : 2.70895
[1mStep[0m  [20/106], [94mLoss[0m : 2.48216
[1mStep[0m  [30/106], [94mLoss[0m : 2.79729
[1mStep[0m  [40/106], [94mLoss[0m : 2.16255
[1mStep[0m  [50/106], [94mLoss[0m : 2.31728
[1mStep[0m  [60/106], [94mLoss[0m : 2.51384
[1mStep[0m  [70/106], [94mLoss[0m : 2.58344
[1mStep[0m  [80/106], [94mLoss[0m : 2.88811
[1mStep[0m  [90/106], [94mLoss[0m : 2.32978
[1mStep[0m  [100/106], [94mLoss[0m : 2.49013

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.376, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35104
[1mStep[0m  [10/106], [94mLoss[0m : 2.30690
[1mStep[0m  [20/106], [94mLoss[0m : 2.41070
[1mStep[0m  [30/106], [94mLoss[0m : 2.45409
[1mStep[0m  [40/106], [94mLoss[0m : 2.40916
[1mStep[0m  [50/106], [94mLoss[0m : 2.64832
[1mStep[0m  [60/106], [94mLoss[0m : 2.48331
[1mStep[0m  [70/106], [94mLoss[0m : 2.32184
[1mStep[0m  [80/106], [94mLoss[0m : 2.31190
[1mStep[0m  [90/106], [94mLoss[0m : 2.52595
[1mStep[0m  [100/106], [94mLoss[0m : 2.61187

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.384, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20763
[1mStep[0m  [10/106], [94mLoss[0m : 2.47275
[1mStep[0m  [20/106], [94mLoss[0m : 2.55418
[1mStep[0m  [30/106], [94mLoss[0m : 2.05359
[1mStep[0m  [40/106], [94mLoss[0m : 2.58350
[1mStep[0m  [50/106], [94mLoss[0m : 2.10082
[1mStep[0m  [60/106], [94mLoss[0m : 2.31742
[1mStep[0m  [70/106], [94mLoss[0m : 2.41637
[1mStep[0m  [80/106], [94mLoss[0m : 2.40597
[1mStep[0m  [90/106], [94mLoss[0m : 2.35851
[1mStep[0m  [100/106], [94mLoss[0m : 2.31840

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.398, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29704
[1mStep[0m  [10/106], [94mLoss[0m : 2.30166
[1mStep[0m  [20/106], [94mLoss[0m : 2.17769
[1mStep[0m  [30/106], [94mLoss[0m : 2.54293
[1mStep[0m  [40/106], [94mLoss[0m : 2.44738
[1mStep[0m  [50/106], [94mLoss[0m : 2.38672
[1mStep[0m  [60/106], [94mLoss[0m : 2.76944
[1mStep[0m  [70/106], [94mLoss[0m : 2.36422
[1mStep[0m  [80/106], [94mLoss[0m : 2.57122
[1mStep[0m  [90/106], [94mLoss[0m : 2.23476
[1mStep[0m  [100/106], [94mLoss[0m : 2.25938

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.376, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53585
[1mStep[0m  [10/106], [94mLoss[0m : 2.44670
[1mStep[0m  [20/106], [94mLoss[0m : 2.19534
[1mStep[0m  [30/106], [94mLoss[0m : 1.99448
[1mStep[0m  [40/106], [94mLoss[0m : 2.16666
[1mStep[0m  [50/106], [94mLoss[0m : 2.20157
[1mStep[0m  [60/106], [94mLoss[0m : 2.51274
[1mStep[0m  [70/106], [94mLoss[0m : 2.48398
[1mStep[0m  [80/106], [94mLoss[0m : 2.46194
[1mStep[0m  [90/106], [94mLoss[0m : 2.81215
[1mStep[0m  [100/106], [94mLoss[0m : 2.18226

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.404, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49314
[1mStep[0m  [10/106], [94mLoss[0m : 2.37409
[1mStep[0m  [20/106], [94mLoss[0m : 2.29957
[1mStep[0m  [30/106], [94mLoss[0m : 2.51406
[1mStep[0m  [40/106], [94mLoss[0m : 2.60194
[1mStep[0m  [50/106], [94mLoss[0m : 2.12717
[1mStep[0m  [60/106], [94mLoss[0m : 2.67864
[1mStep[0m  [70/106], [94mLoss[0m : 2.27132
[1mStep[0m  [80/106], [94mLoss[0m : 2.86046
[1mStep[0m  [90/106], [94mLoss[0m : 2.58562
[1mStep[0m  [100/106], [94mLoss[0m : 2.42631

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39745
[1mStep[0m  [10/106], [94mLoss[0m : 2.32101
[1mStep[0m  [20/106], [94mLoss[0m : 2.75456
[1mStep[0m  [30/106], [94mLoss[0m : 2.29010
[1mStep[0m  [40/106], [94mLoss[0m : 2.25522
[1mStep[0m  [50/106], [94mLoss[0m : 2.52315
[1mStep[0m  [60/106], [94mLoss[0m : 2.39086
[1mStep[0m  [70/106], [94mLoss[0m : 2.71228
[1mStep[0m  [80/106], [94mLoss[0m : 2.42288
[1mStep[0m  [90/106], [94mLoss[0m : 2.25401
[1mStep[0m  [100/106], [94mLoss[0m : 2.25967

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.378, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.09933
[1mStep[0m  [10/106], [94mLoss[0m : 2.48914
[1mStep[0m  [20/106], [94mLoss[0m : 2.34496
[1mStep[0m  [30/106], [94mLoss[0m : 2.43990
[1mStep[0m  [40/106], [94mLoss[0m : 2.36417
[1mStep[0m  [50/106], [94mLoss[0m : 2.40658
[1mStep[0m  [60/106], [94mLoss[0m : 2.23553
[1mStep[0m  [70/106], [94mLoss[0m : 2.34440
[1mStep[0m  [80/106], [94mLoss[0m : 2.56098
[1mStep[0m  [90/106], [94mLoss[0m : 2.41499
[1mStep[0m  [100/106], [94mLoss[0m : 2.52617

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.368, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14849
[1mStep[0m  [10/106], [94mLoss[0m : 2.28953
[1mStep[0m  [20/106], [94mLoss[0m : 2.48733
[1mStep[0m  [30/106], [94mLoss[0m : 2.46116
[1mStep[0m  [40/106], [94mLoss[0m : 2.17966
[1mStep[0m  [50/106], [94mLoss[0m : 2.57949
[1mStep[0m  [60/106], [94mLoss[0m : 2.14294
[1mStep[0m  [70/106], [94mLoss[0m : 2.58575
[1mStep[0m  [80/106], [94mLoss[0m : 2.62990
[1mStep[0m  [90/106], [94mLoss[0m : 2.41568
[1mStep[0m  [100/106], [94mLoss[0m : 2.89320

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.372, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45703
[1mStep[0m  [10/106], [94mLoss[0m : 2.03801
[1mStep[0m  [20/106], [94mLoss[0m : 2.42956
[1mStep[0m  [30/106], [94mLoss[0m : 2.35435
[1mStep[0m  [40/106], [94mLoss[0m : 2.65231
[1mStep[0m  [50/106], [94mLoss[0m : 2.65034
[1mStep[0m  [60/106], [94mLoss[0m : 2.45858
[1mStep[0m  [70/106], [94mLoss[0m : 2.28630
[1mStep[0m  [80/106], [94mLoss[0m : 2.23685
[1mStep[0m  [90/106], [94mLoss[0m : 2.53698
[1mStep[0m  [100/106], [94mLoss[0m : 2.34776

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.368, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57182
[1mStep[0m  [10/106], [94mLoss[0m : 2.58952
[1mStep[0m  [20/106], [94mLoss[0m : 2.44565
[1mStep[0m  [30/106], [94mLoss[0m : 2.40668
[1mStep[0m  [40/106], [94mLoss[0m : 2.18822
[1mStep[0m  [50/106], [94mLoss[0m : 2.32210
[1mStep[0m  [60/106], [94mLoss[0m : 2.35610
[1mStep[0m  [70/106], [94mLoss[0m : 2.57552
[1mStep[0m  [80/106], [94mLoss[0m : 2.43134
[1mStep[0m  [90/106], [94mLoss[0m : 2.56617
[1mStep[0m  [100/106], [94mLoss[0m : 2.19638

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.368, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.365
====================================

Phase 1 - Evaluation MAE:  2.3653900510859938
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.12174
[1mStep[0m  [10/106], [94mLoss[0m : 2.48623
[1mStep[0m  [20/106], [94mLoss[0m : 2.73106
[1mStep[0m  [30/106], [94mLoss[0m : 2.33687
[1mStep[0m  [40/106], [94mLoss[0m : 2.42744
[1mStep[0m  [50/106], [94mLoss[0m : 2.19248
[1mStep[0m  [60/106], [94mLoss[0m : 2.54396
[1mStep[0m  [70/106], [94mLoss[0m : 2.45426
[1mStep[0m  [80/106], [94mLoss[0m : 2.59888
[1mStep[0m  [90/106], [94mLoss[0m : 2.48273
[1mStep[0m  [100/106], [94mLoss[0m : 2.59596

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46755
[1mStep[0m  [10/106], [94mLoss[0m : 2.11289
[1mStep[0m  [20/106], [94mLoss[0m : 2.38631
[1mStep[0m  [30/106], [94mLoss[0m : 2.50394
[1mStep[0m  [40/106], [94mLoss[0m : 2.26246
[1mStep[0m  [50/106], [94mLoss[0m : 2.18495
[1mStep[0m  [60/106], [94mLoss[0m : 2.58490
[1mStep[0m  [70/106], [94mLoss[0m : 2.65279
[1mStep[0m  [80/106], [94mLoss[0m : 2.58384
[1mStep[0m  [90/106], [94mLoss[0m : 2.30073
[1mStep[0m  [100/106], [94mLoss[0m : 2.67056

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23926
[1mStep[0m  [10/106], [94mLoss[0m : 2.11491
[1mStep[0m  [20/106], [94mLoss[0m : 2.35790
[1mStep[0m  [30/106], [94mLoss[0m : 2.30600
[1mStep[0m  [40/106], [94mLoss[0m : 2.33223
[1mStep[0m  [50/106], [94mLoss[0m : 2.29362
[1mStep[0m  [60/106], [94mLoss[0m : 2.38814
[1mStep[0m  [70/106], [94mLoss[0m : 2.38267
[1mStep[0m  [80/106], [94mLoss[0m : 2.13366
[1mStep[0m  [90/106], [94mLoss[0m : 2.21118
[1mStep[0m  [100/106], [94mLoss[0m : 2.40275

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39680
[1mStep[0m  [10/106], [94mLoss[0m : 2.00182
[1mStep[0m  [20/106], [94mLoss[0m : 2.10285
[1mStep[0m  [30/106], [94mLoss[0m : 2.33323
[1mStep[0m  [40/106], [94mLoss[0m : 2.39738
[1mStep[0m  [50/106], [94mLoss[0m : 2.18381
[1mStep[0m  [60/106], [94mLoss[0m : 2.48169
[1mStep[0m  [70/106], [94mLoss[0m : 1.96824
[1mStep[0m  [80/106], [94mLoss[0m : 2.09948
[1mStep[0m  [90/106], [94mLoss[0m : 2.35910
[1mStep[0m  [100/106], [94mLoss[0m : 2.41538

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16666
[1mStep[0m  [10/106], [94mLoss[0m : 2.06633
[1mStep[0m  [20/106], [94mLoss[0m : 2.22112
[1mStep[0m  [30/106], [94mLoss[0m : 2.15236
[1mStep[0m  [40/106], [94mLoss[0m : 2.08835
[1mStep[0m  [50/106], [94mLoss[0m : 2.18665
[1mStep[0m  [60/106], [94mLoss[0m : 2.12493
[1mStep[0m  [70/106], [94mLoss[0m : 2.10656
[1mStep[0m  [80/106], [94mLoss[0m : 2.35028
[1mStep[0m  [90/106], [94mLoss[0m : 2.31427
[1mStep[0m  [100/106], [94mLoss[0m : 2.20316

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.203, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42337
[1mStep[0m  [10/106], [94mLoss[0m : 2.16598
[1mStep[0m  [20/106], [94mLoss[0m : 2.21959
[1mStep[0m  [30/106], [94mLoss[0m : 1.98451
[1mStep[0m  [40/106], [94mLoss[0m : 2.05605
[1mStep[0m  [50/106], [94mLoss[0m : 2.34666
[1mStep[0m  [60/106], [94mLoss[0m : 2.22544
[1mStep[0m  [70/106], [94mLoss[0m : 2.01353
[1mStep[0m  [80/106], [94mLoss[0m : 2.37071
[1mStep[0m  [90/106], [94mLoss[0m : 2.27356
[1mStep[0m  [100/106], [94mLoss[0m : 2.09327

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79213
[1mStep[0m  [10/106], [94mLoss[0m : 1.87724
[1mStep[0m  [20/106], [94mLoss[0m : 1.84638
[1mStep[0m  [30/106], [94mLoss[0m : 2.31234
[1mStep[0m  [40/106], [94mLoss[0m : 2.23615
[1mStep[0m  [50/106], [94mLoss[0m : 1.90889
[1mStep[0m  [60/106], [94mLoss[0m : 2.03798
[1mStep[0m  [70/106], [94mLoss[0m : 1.98883
[1mStep[0m  [80/106], [94mLoss[0m : 1.88959
[1mStep[0m  [90/106], [94mLoss[0m : 2.13026
[1mStep[0m  [100/106], [94mLoss[0m : 2.25828

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.075, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.03477
[1mStep[0m  [10/106], [94mLoss[0m : 2.10623
[1mStep[0m  [20/106], [94mLoss[0m : 2.09680
[1mStep[0m  [30/106], [94mLoss[0m : 1.96448
[1mStep[0m  [40/106], [94mLoss[0m : 1.87578
[1mStep[0m  [50/106], [94mLoss[0m : 2.02122
[1mStep[0m  [60/106], [94mLoss[0m : 2.08841
[1mStep[0m  [70/106], [94mLoss[0m : 1.81366
[1mStep[0m  [80/106], [94mLoss[0m : 1.64429
[1mStep[0m  [90/106], [94mLoss[0m : 2.26554
[1mStep[0m  [100/106], [94mLoss[0m : 2.04228

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79502
[1mStep[0m  [10/106], [94mLoss[0m : 2.03701
[1mStep[0m  [20/106], [94mLoss[0m : 2.16327
[1mStep[0m  [30/106], [94mLoss[0m : 1.84966
[1mStep[0m  [40/106], [94mLoss[0m : 1.95967
[1mStep[0m  [50/106], [94mLoss[0m : 2.24476
[1mStep[0m  [60/106], [94mLoss[0m : 1.97089
[1mStep[0m  [70/106], [94mLoss[0m : 2.07798
[1mStep[0m  [80/106], [94mLoss[0m : 2.00029
[1mStep[0m  [90/106], [94mLoss[0m : 1.92695
[1mStep[0m  [100/106], [94mLoss[0m : 2.01304

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98609
[1mStep[0m  [10/106], [94mLoss[0m : 1.85382
[1mStep[0m  [20/106], [94mLoss[0m : 1.84758
[1mStep[0m  [30/106], [94mLoss[0m : 1.63387
[1mStep[0m  [40/106], [94mLoss[0m : 2.18167
[1mStep[0m  [50/106], [94mLoss[0m : 1.63255
[1mStep[0m  [60/106], [94mLoss[0m : 1.90362
[1mStep[0m  [70/106], [94mLoss[0m : 1.88463
[1mStep[0m  [80/106], [94mLoss[0m : 2.04521
[1mStep[0m  [90/106], [94mLoss[0m : 1.95075
[1mStep[0m  [100/106], [94mLoss[0m : 1.96875

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.66247
[1mStep[0m  [10/106], [94mLoss[0m : 2.14433
[1mStep[0m  [20/106], [94mLoss[0m : 1.90941
[1mStep[0m  [30/106], [94mLoss[0m : 1.88193
[1mStep[0m  [40/106], [94mLoss[0m : 1.90600
[1mStep[0m  [50/106], [94mLoss[0m : 1.98867
[1mStep[0m  [60/106], [94mLoss[0m : 2.02214
[1mStep[0m  [70/106], [94mLoss[0m : 1.83652
[1mStep[0m  [80/106], [94mLoss[0m : 1.86555
[1mStep[0m  [90/106], [94mLoss[0m : 1.89445
[1mStep[0m  [100/106], [94mLoss[0m : 2.04883

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.91067
[1mStep[0m  [10/106], [94mLoss[0m : 1.76644
[1mStep[0m  [20/106], [94mLoss[0m : 1.65900
[1mStep[0m  [30/106], [94mLoss[0m : 1.67752
[1mStep[0m  [40/106], [94mLoss[0m : 1.69958
[1mStep[0m  [50/106], [94mLoss[0m : 1.73109
[1mStep[0m  [60/106], [94mLoss[0m : 1.79537
[1mStep[0m  [70/106], [94mLoss[0m : 1.86386
[1mStep[0m  [80/106], [94mLoss[0m : 1.79918
[1mStep[0m  [90/106], [94mLoss[0m : 1.92200
[1mStep[0m  [100/106], [94mLoss[0m : 1.95838

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87438
[1mStep[0m  [10/106], [94mLoss[0m : 1.58241
[1mStep[0m  [20/106], [94mLoss[0m : 1.86368
[1mStep[0m  [30/106], [94mLoss[0m : 1.82123
[1mStep[0m  [40/106], [94mLoss[0m : 2.16572
[1mStep[0m  [50/106], [94mLoss[0m : 1.66062
[1mStep[0m  [60/106], [94mLoss[0m : 1.80188
[1mStep[0m  [70/106], [94mLoss[0m : 1.65843
[1mStep[0m  [80/106], [94mLoss[0m : 2.07977
[1mStep[0m  [90/106], [94mLoss[0m : 1.68949
[1mStep[0m  [100/106], [94mLoss[0m : 1.76363

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70723
[1mStep[0m  [10/106], [94mLoss[0m : 1.74262
[1mStep[0m  [20/106], [94mLoss[0m : 1.74308
[1mStep[0m  [30/106], [94mLoss[0m : 1.74328
[1mStep[0m  [40/106], [94mLoss[0m : 1.77056
[1mStep[0m  [50/106], [94mLoss[0m : 1.73018
[1mStep[0m  [60/106], [94mLoss[0m : 1.56238
[1mStep[0m  [70/106], [94mLoss[0m : 1.69705
[1mStep[0m  [80/106], [94mLoss[0m : 1.87683
[1mStep[0m  [90/106], [94mLoss[0m : 1.73677
[1mStep[0m  [100/106], [94mLoss[0m : 1.93073

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57854
[1mStep[0m  [10/106], [94mLoss[0m : 1.49272
[1mStep[0m  [20/106], [94mLoss[0m : 1.64096
[1mStep[0m  [30/106], [94mLoss[0m : 1.80426
[1mStep[0m  [40/106], [94mLoss[0m : 1.70695
[1mStep[0m  [50/106], [94mLoss[0m : 1.67926
[1mStep[0m  [60/106], [94mLoss[0m : 1.60800
[1mStep[0m  [70/106], [94mLoss[0m : 1.75316
[1mStep[0m  [80/106], [94mLoss[0m : 1.70044
[1mStep[0m  [90/106], [94mLoss[0m : 1.74892
[1mStep[0m  [100/106], [94mLoss[0m : 1.90534

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65293
[1mStep[0m  [10/106], [94mLoss[0m : 1.76961
[1mStep[0m  [20/106], [94mLoss[0m : 1.64588
[1mStep[0m  [30/106], [94mLoss[0m : 1.49472
[1mStep[0m  [40/106], [94mLoss[0m : 1.83113
[1mStep[0m  [50/106], [94mLoss[0m : 1.69845
[1mStep[0m  [60/106], [94mLoss[0m : 1.56506
[1mStep[0m  [70/106], [94mLoss[0m : 1.81044
[1mStep[0m  [80/106], [94mLoss[0m : 2.00163
[1mStep[0m  [90/106], [94mLoss[0m : 1.51341
[1mStep[0m  [100/106], [94mLoss[0m : 1.80121

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.38710
[1mStep[0m  [10/106], [94mLoss[0m : 1.81924
[1mStep[0m  [20/106], [94mLoss[0m : 1.48872
[1mStep[0m  [30/106], [94mLoss[0m : 1.75159
[1mStep[0m  [40/106], [94mLoss[0m : 1.41695
[1mStep[0m  [50/106], [94mLoss[0m : 1.61932
[1mStep[0m  [60/106], [94mLoss[0m : 1.63480
[1mStep[0m  [70/106], [94mLoss[0m : 1.57846
[1mStep[0m  [80/106], [94mLoss[0m : 1.69062
[1mStep[0m  [90/106], [94mLoss[0m : 1.75070
[1mStep[0m  [100/106], [94mLoss[0m : 1.61824

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.658, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52038
[1mStep[0m  [10/106], [94mLoss[0m : 1.54019
[1mStep[0m  [20/106], [94mLoss[0m : 1.57547
[1mStep[0m  [30/106], [94mLoss[0m : 1.53613
[1mStep[0m  [40/106], [94mLoss[0m : 1.66429
[1mStep[0m  [50/106], [94mLoss[0m : 1.83876
[1mStep[0m  [60/106], [94mLoss[0m : 1.53128
[1mStep[0m  [70/106], [94mLoss[0m : 1.81451
[1mStep[0m  [80/106], [94mLoss[0m : 1.67614
[1mStep[0m  [90/106], [94mLoss[0m : 1.64965
[1mStep[0m  [100/106], [94mLoss[0m : 1.54083

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58452
[1mStep[0m  [10/106], [94mLoss[0m : 1.64734
[1mStep[0m  [20/106], [94mLoss[0m : 1.48903
[1mStep[0m  [30/106], [94mLoss[0m : 1.42166
[1mStep[0m  [40/106], [94mLoss[0m : 1.72135
[1mStep[0m  [50/106], [94mLoss[0m : 1.53079
[1mStep[0m  [60/106], [94mLoss[0m : 1.42541
[1mStep[0m  [70/106], [94mLoss[0m : 1.71606
[1mStep[0m  [80/106], [94mLoss[0m : 1.41715
[1mStep[0m  [90/106], [94mLoss[0m : 1.56587
[1mStep[0m  [100/106], [94mLoss[0m : 1.59533

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.488, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.40755
[1mStep[0m  [10/106], [94mLoss[0m : 1.58996
[1mStep[0m  [20/106], [94mLoss[0m : 1.70742
[1mStep[0m  [30/106], [94mLoss[0m : 1.46391
[1mStep[0m  [40/106], [94mLoss[0m : 1.60425
[1mStep[0m  [50/106], [94mLoss[0m : 1.44622
[1mStep[0m  [60/106], [94mLoss[0m : 1.67906
[1mStep[0m  [70/106], [94mLoss[0m : 1.37534
[1mStep[0m  [80/106], [94mLoss[0m : 1.50651
[1mStep[0m  [90/106], [94mLoss[0m : 1.94885
[1mStep[0m  [100/106], [94mLoss[0m : 1.48753

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.449, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.41909
[1mStep[0m  [10/106], [94mLoss[0m : 1.26237
[1mStep[0m  [20/106], [94mLoss[0m : 1.54719
[1mStep[0m  [30/106], [94mLoss[0m : 1.56163
[1mStep[0m  [40/106], [94mLoss[0m : 1.28258
[1mStep[0m  [50/106], [94mLoss[0m : 1.60656
[1mStep[0m  [60/106], [94mLoss[0m : 1.54559
[1mStep[0m  [70/106], [94mLoss[0m : 1.60947
[1mStep[0m  [80/106], [94mLoss[0m : 1.49409
[1mStep[0m  [90/106], [94mLoss[0m : 1.44222
[1mStep[0m  [100/106], [94mLoss[0m : 1.58877

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.25939
[1mStep[0m  [10/106], [94mLoss[0m : 1.23397
[1mStep[0m  [20/106], [94mLoss[0m : 1.41304
[1mStep[0m  [30/106], [94mLoss[0m : 1.66715
[1mStep[0m  [40/106], [94mLoss[0m : 1.57055
[1mStep[0m  [50/106], [94mLoss[0m : 1.37943
[1mStep[0m  [60/106], [94mLoss[0m : 1.61261
[1mStep[0m  [70/106], [94mLoss[0m : 1.54036
[1mStep[0m  [80/106], [94mLoss[0m : 1.56921
[1mStep[0m  [90/106], [94mLoss[0m : 1.36416
[1mStep[0m  [100/106], [94mLoss[0m : 1.50838

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.437, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49491
[1mStep[0m  [10/106], [94mLoss[0m : 1.30926
[1mStep[0m  [20/106], [94mLoss[0m : 1.43834
[1mStep[0m  [30/106], [94mLoss[0m : 1.44917
[1mStep[0m  [40/106], [94mLoss[0m : 1.24534
[1mStep[0m  [50/106], [94mLoss[0m : 1.33503
[1mStep[0m  [60/106], [94mLoss[0m : 1.53068
[1mStep[0m  [70/106], [94mLoss[0m : 1.52092
[1mStep[0m  [80/106], [94mLoss[0m : 1.67593
[1mStep[0m  [90/106], [94mLoss[0m : 1.71422
[1mStep[0m  [100/106], [94mLoss[0m : 1.40791

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.474, [92mTest[0m: 2.446, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52423
[1mStep[0m  [10/106], [94mLoss[0m : 1.10475
[1mStep[0m  [20/106], [94mLoss[0m : 1.39550
[1mStep[0m  [30/106], [94mLoss[0m : 1.39759
[1mStep[0m  [40/106], [94mLoss[0m : 1.69233
[1mStep[0m  [50/106], [94mLoss[0m : 1.53027
[1mStep[0m  [60/106], [94mLoss[0m : 1.34402
[1mStep[0m  [70/106], [94mLoss[0m : 1.38892
[1mStep[0m  [80/106], [94mLoss[0m : 1.38377
[1mStep[0m  [90/106], [94mLoss[0m : 1.55480
[1mStep[0m  [100/106], [94mLoss[0m : 1.50683

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.442, [92mTest[0m: 2.469, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53326
[1mStep[0m  [10/106], [94mLoss[0m : 1.46159
[1mStep[0m  [20/106], [94mLoss[0m : 1.43156
[1mStep[0m  [30/106], [94mLoss[0m : 1.63238
[1mStep[0m  [40/106], [94mLoss[0m : 1.35070
[1mStep[0m  [50/106], [94mLoss[0m : 1.29481
[1mStep[0m  [60/106], [94mLoss[0m : 1.40167
[1mStep[0m  [70/106], [94mLoss[0m : 1.77136
[1mStep[0m  [80/106], [94mLoss[0m : 1.59081
[1mStep[0m  [90/106], [94mLoss[0m : 1.09609
[1mStep[0m  [100/106], [94mLoss[0m : 1.55088

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67028
[1mStep[0m  [10/106], [94mLoss[0m : 1.19189
[1mStep[0m  [20/106], [94mLoss[0m : 1.46221
[1mStep[0m  [30/106], [94mLoss[0m : 1.30751
[1mStep[0m  [40/106], [94mLoss[0m : 1.50729
[1mStep[0m  [50/106], [94mLoss[0m : 1.23696
[1mStep[0m  [60/106], [94mLoss[0m : 1.23297
[1mStep[0m  [70/106], [94mLoss[0m : 1.48398
[1mStep[0m  [80/106], [94mLoss[0m : 1.36037
[1mStep[0m  [90/106], [94mLoss[0m : 1.19628
[1mStep[0m  [100/106], [94mLoss[0m : 1.72846

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.405, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42912
[1mStep[0m  [10/106], [94mLoss[0m : 1.37159
[1mStep[0m  [20/106], [94mLoss[0m : 1.21504
[1mStep[0m  [30/106], [94mLoss[0m : 1.45392
[1mStep[0m  [40/106], [94mLoss[0m : 1.49372
[1mStep[0m  [50/106], [94mLoss[0m : 1.70731
[1mStep[0m  [60/106], [94mLoss[0m : 1.31547
[1mStep[0m  [70/106], [94mLoss[0m : 1.42002
[1mStep[0m  [80/106], [94mLoss[0m : 1.26675
[1mStep[0m  [90/106], [94mLoss[0m : 1.44046
[1mStep[0m  [100/106], [94mLoss[0m : 1.35035

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.386, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.465
====================================

Phase 2 - Evaluation MAE:  2.4651684738555044
MAE score P1       2.36539
MAE score P2      2.465168
loss              1.386278
learning_rate         0.01
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.28968
[1mStep[0m  [10/106], [94mLoss[0m : 4.86158
[1mStep[0m  [20/106], [94mLoss[0m : 4.14295
[1mStep[0m  [30/106], [94mLoss[0m : 2.98952
[1mStep[0m  [40/106], [94mLoss[0m : 2.68090
[1mStep[0m  [50/106], [94mLoss[0m : 2.84289
[1mStep[0m  [60/106], [94mLoss[0m : 2.83681
[1mStep[0m  [70/106], [94mLoss[0m : 2.84138
[1mStep[0m  [80/106], [94mLoss[0m : 2.68121
[1mStep[0m  [90/106], [94mLoss[0m : 2.49780
[1mStep[0m  [100/106], [94mLoss[0m : 2.42412

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.492, [92mTest[0m: 10.858, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37604
[1mStep[0m  [10/106], [94mLoss[0m : 2.28450
[1mStep[0m  [20/106], [94mLoss[0m : 2.62977
[1mStep[0m  [30/106], [94mLoss[0m : 2.68373
[1mStep[0m  [40/106], [94mLoss[0m : 2.20465
[1mStep[0m  [50/106], [94mLoss[0m : 2.64602
[1mStep[0m  [60/106], [94mLoss[0m : 2.74299
[1mStep[0m  [70/106], [94mLoss[0m : 2.21944
[1mStep[0m  [80/106], [94mLoss[0m : 2.63783
[1mStep[0m  [90/106], [94mLoss[0m : 2.61865
[1mStep[0m  [100/106], [94mLoss[0m : 2.48343

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51262
[1mStep[0m  [10/106], [94mLoss[0m : 2.55138
[1mStep[0m  [20/106], [94mLoss[0m : 2.54298
[1mStep[0m  [30/106], [94mLoss[0m : 2.71404
[1mStep[0m  [40/106], [94mLoss[0m : 2.55943
[1mStep[0m  [50/106], [94mLoss[0m : 2.18639
[1mStep[0m  [60/106], [94mLoss[0m : 2.68808
[1mStep[0m  [70/106], [94mLoss[0m : 2.35796
[1mStep[0m  [80/106], [94mLoss[0m : 2.33969
[1mStep[0m  [90/106], [94mLoss[0m : 2.32913
[1mStep[0m  [100/106], [94mLoss[0m : 2.38105

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37289
[1mStep[0m  [10/106], [94mLoss[0m : 2.57234
[1mStep[0m  [20/106], [94mLoss[0m : 2.52333
[1mStep[0m  [30/106], [94mLoss[0m : 2.33431
[1mStep[0m  [40/106], [94mLoss[0m : 2.51955
[1mStep[0m  [50/106], [94mLoss[0m : 2.93027
[1mStep[0m  [60/106], [94mLoss[0m : 2.63233
[1mStep[0m  [70/106], [94mLoss[0m : 2.66913
[1mStep[0m  [80/106], [94mLoss[0m : 2.66829
[1mStep[0m  [90/106], [94mLoss[0m : 2.51629
[1mStep[0m  [100/106], [94mLoss[0m : 2.60133

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42320
[1mStep[0m  [10/106], [94mLoss[0m : 2.11935
[1mStep[0m  [20/106], [94mLoss[0m : 2.68846
[1mStep[0m  [30/106], [94mLoss[0m : 2.43934
[1mStep[0m  [40/106], [94mLoss[0m : 2.26545
[1mStep[0m  [50/106], [94mLoss[0m : 2.72554
[1mStep[0m  [60/106], [94mLoss[0m : 2.33267
[1mStep[0m  [70/106], [94mLoss[0m : 2.58026
[1mStep[0m  [80/106], [94mLoss[0m : 2.15851
[1mStep[0m  [90/106], [94mLoss[0m : 2.36818
[1mStep[0m  [100/106], [94mLoss[0m : 2.28269

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43490
[1mStep[0m  [10/106], [94mLoss[0m : 2.22863
[1mStep[0m  [20/106], [94mLoss[0m : 2.46923
[1mStep[0m  [30/106], [94mLoss[0m : 2.23986
[1mStep[0m  [40/106], [94mLoss[0m : 2.38299
[1mStep[0m  [50/106], [94mLoss[0m : 2.33789
[1mStep[0m  [60/106], [94mLoss[0m : 2.19819
[1mStep[0m  [70/106], [94mLoss[0m : 2.50949
[1mStep[0m  [80/106], [94mLoss[0m : 2.29994
[1mStep[0m  [90/106], [94mLoss[0m : 2.33895
[1mStep[0m  [100/106], [94mLoss[0m : 2.28929

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53627
[1mStep[0m  [10/106], [94mLoss[0m : 2.31786
[1mStep[0m  [20/106], [94mLoss[0m : 2.34528
[1mStep[0m  [30/106], [94mLoss[0m : 2.24694
[1mStep[0m  [40/106], [94mLoss[0m : 2.46105
[1mStep[0m  [50/106], [94mLoss[0m : 2.17093
[1mStep[0m  [60/106], [94mLoss[0m : 2.33308
[1mStep[0m  [70/106], [94mLoss[0m : 2.51675
[1mStep[0m  [80/106], [94mLoss[0m : 2.63438
[1mStep[0m  [90/106], [94mLoss[0m : 2.34863
[1mStep[0m  [100/106], [94mLoss[0m : 2.43875

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46508
[1mStep[0m  [10/106], [94mLoss[0m : 2.41171
[1mStep[0m  [20/106], [94mLoss[0m : 2.54620
[1mStep[0m  [30/106], [94mLoss[0m : 2.53667
[1mStep[0m  [40/106], [94mLoss[0m : 2.54385
[1mStep[0m  [50/106], [94mLoss[0m : 2.42670
[1mStep[0m  [60/106], [94mLoss[0m : 2.20603
[1mStep[0m  [70/106], [94mLoss[0m : 2.55800
[1mStep[0m  [80/106], [94mLoss[0m : 2.38342
[1mStep[0m  [90/106], [94mLoss[0m : 2.18311
[1mStep[0m  [100/106], [94mLoss[0m : 2.43717

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43714
[1mStep[0m  [10/106], [94mLoss[0m : 2.45781
[1mStep[0m  [20/106], [94mLoss[0m : 2.46118
[1mStep[0m  [30/106], [94mLoss[0m : 2.51358
[1mStep[0m  [40/106], [94mLoss[0m : 2.33695
[1mStep[0m  [50/106], [94mLoss[0m : 2.45655
[1mStep[0m  [60/106], [94mLoss[0m : 2.18070
[1mStep[0m  [70/106], [94mLoss[0m : 2.46282
[1mStep[0m  [80/106], [94mLoss[0m : 2.39771
[1mStep[0m  [90/106], [94mLoss[0m : 2.50507
[1mStep[0m  [100/106], [94mLoss[0m : 2.31154

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41271
[1mStep[0m  [10/106], [94mLoss[0m : 2.36293
[1mStep[0m  [20/106], [94mLoss[0m : 2.53620
[1mStep[0m  [30/106], [94mLoss[0m : 2.42125
[1mStep[0m  [40/106], [94mLoss[0m : 1.99786
[1mStep[0m  [50/106], [94mLoss[0m : 2.37277
[1mStep[0m  [60/106], [94mLoss[0m : 2.56607
[1mStep[0m  [70/106], [94mLoss[0m : 2.48946
[1mStep[0m  [80/106], [94mLoss[0m : 2.25168
[1mStep[0m  [90/106], [94mLoss[0m : 2.72457
[1mStep[0m  [100/106], [94mLoss[0m : 2.38064

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34142
[1mStep[0m  [10/106], [94mLoss[0m : 2.29209
[1mStep[0m  [20/106], [94mLoss[0m : 2.30392
[1mStep[0m  [30/106], [94mLoss[0m : 2.63837
[1mStep[0m  [40/106], [94mLoss[0m : 2.56922
[1mStep[0m  [50/106], [94mLoss[0m : 2.24242
[1mStep[0m  [60/106], [94mLoss[0m : 2.42950
[1mStep[0m  [70/106], [94mLoss[0m : 2.34974
[1mStep[0m  [80/106], [94mLoss[0m : 2.47456
[1mStep[0m  [90/106], [94mLoss[0m : 2.48366
[1mStep[0m  [100/106], [94mLoss[0m : 2.16232

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32332
[1mStep[0m  [10/106], [94mLoss[0m : 2.09414
[1mStep[0m  [20/106], [94mLoss[0m : 2.42628
[1mStep[0m  [30/106], [94mLoss[0m : 2.56615
[1mStep[0m  [40/106], [94mLoss[0m : 2.50993
[1mStep[0m  [50/106], [94mLoss[0m : 2.61187
[1mStep[0m  [60/106], [94mLoss[0m : 2.26490
[1mStep[0m  [70/106], [94mLoss[0m : 2.42252
[1mStep[0m  [80/106], [94mLoss[0m : 2.23777
[1mStep[0m  [90/106], [94mLoss[0m : 2.53969
[1mStep[0m  [100/106], [94mLoss[0m : 2.37597

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24044
[1mStep[0m  [10/106], [94mLoss[0m : 2.17067
[1mStep[0m  [20/106], [94mLoss[0m : 2.46167
[1mStep[0m  [30/106], [94mLoss[0m : 2.63992
[1mStep[0m  [40/106], [94mLoss[0m : 2.58342
[1mStep[0m  [50/106], [94mLoss[0m : 2.45667
[1mStep[0m  [60/106], [94mLoss[0m : 2.43693
[1mStep[0m  [70/106], [94mLoss[0m : 2.19190
[1mStep[0m  [80/106], [94mLoss[0m : 2.45100
[1mStep[0m  [90/106], [94mLoss[0m : 2.33301
[1mStep[0m  [100/106], [94mLoss[0m : 2.47312

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31699
[1mStep[0m  [10/106], [94mLoss[0m : 2.16938
[1mStep[0m  [20/106], [94mLoss[0m : 2.46695
[1mStep[0m  [30/106], [94mLoss[0m : 2.23428
[1mStep[0m  [40/106], [94mLoss[0m : 2.54991
[1mStep[0m  [50/106], [94mLoss[0m : 2.30609
[1mStep[0m  [60/106], [94mLoss[0m : 2.57871
[1mStep[0m  [70/106], [94mLoss[0m : 2.43340
[1mStep[0m  [80/106], [94mLoss[0m : 2.57282
[1mStep[0m  [90/106], [94mLoss[0m : 1.98087
[1mStep[0m  [100/106], [94mLoss[0m : 2.35171

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33532
[1mStep[0m  [10/106], [94mLoss[0m : 2.41811
[1mStep[0m  [20/106], [94mLoss[0m : 2.30006
[1mStep[0m  [30/106], [94mLoss[0m : 2.18305
[1mStep[0m  [40/106], [94mLoss[0m : 2.50532
[1mStep[0m  [50/106], [94mLoss[0m : 1.94409
[1mStep[0m  [60/106], [94mLoss[0m : 2.57033
[1mStep[0m  [70/106], [94mLoss[0m : 2.38784
[1mStep[0m  [80/106], [94mLoss[0m : 2.29817
[1mStep[0m  [90/106], [94mLoss[0m : 2.35142
[1mStep[0m  [100/106], [94mLoss[0m : 2.57960

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56672
[1mStep[0m  [10/106], [94mLoss[0m : 2.33713
[1mStep[0m  [20/106], [94mLoss[0m : 1.98109
[1mStep[0m  [30/106], [94mLoss[0m : 2.35633
[1mStep[0m  [40/106], [94mLoss[0m : 2.40780
[1mStep[0m  [50/106], [94mLoss[0m : 2.30536
[1mStep[0m  [60/106], [94mLoss[0m : 2.45647
[1mStep[0m  [70/106], [94mLoss[0m : 2.26352
[1mStep[0m  [80/106], [94mLoss[0m : 2.63971
[1mStep[0m  [90/106], [94mLoss[0m : 2.30955
[1mStep[0m  [100/106], [94mLoss[0m : 2.37217

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48185
[1mStep[0m  [10/106], [94mLoss[0m : 2.31663
[1mStep[0m  [20/106], [94mLoss[0m : 2.14972
[1mStep[0m  [30/106], [94mLoss[0m : 2.39193
[1mStep[0m  [40/106], [94mLoss[0m : 2.36402
[1mStep[0m  [50/106], [94mLoss[0m : 2.64056
[1mStep[0m  [60/106], [94mLoss[0m : 2.45918
[1mStep[0m  [70/106], [94mLoss[0m : 2.16495
[1mStep[0m  [80/106], [94mLoss[0m : 2.56336
[1mStep[0m  [90/106], [94mLoss[0m : 2.56755
[1mStep[0m  [100/106], [94mLoss[0m : 2.47824

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42232
[1mStep[0m  [10/106], [94mLoss[0m : 2.14542
[1mStep[0m  [20/106], [94mLoss[0m : 2.38302
[1mStep[0m  [30/106], [94mLoss[0m : 2.24343
[1mStep[0m  [40/106], [94mLoss[0m : 2.53263
[1mStep[0m  [50/106], [94mLoss[0m : 2.30419
[1mStep[0m  [60/106], [94mLoss[0m : 2.53160
[1mStep[0m  [70/106], [94mLoss[0m : 2.45312
[1mStep[0m  [80/106], [94mLoss[0m : 2.40112
[1mStep[0m  [90/106], [94mLoss[0m : 2.27959
[1mStep[0m  [100/106], [94mLoss[0m : 2.57075

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12280
[1mStep[0m  [10/106], [94mLoss[0m : 2.15668
[1mStep[0m  [20/106], [94mLoss[0m : 2.56961
[1mStep[0m  [30/106], [94mLoss[0m : 2.18789
[1mStep[0m  [40/106], [94mLoss[0m : 2.19106
[1mStep[0m  [50/106], [94mLoss[0m : 2.12737
[1mStep[0m  [60/106], [94mLoss[0m : 2.37610
[1mStep[0m  [70/106], [94mLoss[0m : 2.06132
[1mStep[0m  [80/106], [94mLoss[0m : 2.36659
[1mStep[0m  [90/106], [94mLoss[0m : 2.48427
[1mStep[0m  [100/106], [94mLoss[0m : 2.27736

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70557
[1mStep[0m  [10/106], [94mLoss[0m : 2.31258
[1mStep[0m  [20/106], [94mLoss[0m : 2.21935
[1mStep[0m  [30/106], [94mLoss[0m : 2.29024
[1mStep[0m  [40/106], [94mLoss[0m : 2.32409
[1mStep[0m  [50/106], [94mLoss[0m : 2.48307
[1mStep[0m  [60/106], [94mLoss[0m : 2.46307
[1mStep[0m  [70/106], [94mLoss[0m : 2.55187
[1mStep[0m  [80/106], [94mLoss[0m : 2.22073
[1mStep[0m  [90/106], [94mLoss[0m : 2.30335
[1mStep[0m  [100/106], [94mLoss[0m : 2.33509

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13223
[1mStep[0m  [10/106], [94mLoss[0m : 2.15089
[1mStep[0m  [20/106], [94mLoss[0m : 2.41445
[1mStep[0m  [30/106], [94mLoss[0m : 2.32279
[1mStep[0m  [40/106], [94mLoss[0m : 2.40059
[1mStep[0m  [50/106], [94mLoss[0m : 2.29987
[1mStep[0m  [60/106], [94mLoss[0m : 2.31684
[1mStep[0m  [70/106], [94mLoss[0m : 2.34611
[1mStep[0m  [80/106], [94mLoss[0m : 2.35942
[1mStep[0m  [90/106], [94mLoss[0m : 2.43711
[1mStep[0m  [100/106], [94mLoss[0m : 2.24832

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16573
[1mStep[0m  [10/106], [94mLoss[0m : 2.47138
[1mStep[0m  [20/106], [94mLoss[0m : 2.39752
[1mStep[0m  [30/106], [94mLoss[0m : 2.34908
[1mStep[0m  [40/106], [94mLoss[0m : 2.34372
[1mStep[0m  [50/106], [94mLoss[0m : 2.22687
[1mStep[0m  [60/106], [94mLoss[0m : 2.10751
[1mStep[0m  [70/106], [94mLoss[0m : 2.24608
[1mStep[0m  [80/106], [94mLoss[0m : 2.30364
[1mStep[0m  [90/106], [94mLoss[0m : 2.07778
[1mStep[0m  [100/106], [94mLoss[0m : 2.66141

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52267
[1mStep[0m  [10/106], [94mLoss[0m : 2.25889
[1mStep[0m  [20/106], [94mLoss[0m : 2.26672
[1mStep[0m  [30/106], [94mLoss[0m : 2.17550
[1mStep[0m  [40/106], [94mLoss[0m : 2.38268
[1mStep[0m  [50/106], [94mLoss[0m : 2.24564
[1mStep[0m  [60/106], [94mLoss[0m : 2.48823
[1mStep[0m  [70/106], [94mLoss[0m : 2.45063
[1mStep[0m  [80/106], [94mLoss[0m : 2.38004
[1mStep[0m  [90/106], [94mLoss[0m : 2.32538
[1mStep[0m  [100/106], [94mLoss[0m : 2.66917

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94539
[1mStep[0m  [10/106], [94mLoss[0m : 2.25366
[1mStep[0m  [20/106], [94mLoss[0m : 2.36555
[1mStep[0m  [30/106], [94mLoss[0m : 2.65240
[1mStep[0m  [40/106], [94mLoss[0m : 2.38131
[1mStep[0m  [50/106], [94mLoss[0m : 2.17213
[1mStep[0m  [60/106], [94mLoss[0m : 2.29583
[1mStep[0m  [70/106], [94mLoss[0m : 2.10410
[1mStep[0m  [80/106], [94mLoss[0m : 2.09176
[1mStep[0m  [90/106], [94mLoss[0m : 2.22971
[1mStep[0m  [100/106], [94mLoss[0m : 2.08734

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24656
[1mStep[0m  [10/106], [94mLoss[0m : 2.53646
[1mStep[0m  [20/106], [94mLoss[0m : 2.03109
[1mStep[0m  [30/106], [94mLoss[0m : 2.17303
[1mStep[0m  [40/106], [94mLoss[0m : 2.06552
[1mStep[0m  [50/106], [94mLoss[0m : 2.45664
[1mStep[0m  [60/106], [94mLoss[0m : 2.25976
[1mStep[0m  [70/106], [94mLoss[0m : 2.05088
[1mStep[0m  [80/106], [94mLoss[0m : 2.27133
[1mStep[0m  [90/106], [94mLoss[0m : 2.25483
[1mStep[0m  [100/106], [94mLoss[0m : 2.50126

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38309
[1mStep[0m  [10/106], [94mLoss[0m : 2.15910
[1mStep[0m  [20/106], [94mLoss[0m : 2.36261
[1mStep[0m  [30/106], [94mLoss[0m : 2.22355
[1mStep[0m  [40/106], [94mLoss[0m : 2.09769
[1mStep[0m  [50/106], [94mLoss[0m : 2.22646
[1mStep[0m  [60/106], [94mLoss[0m : 2.42898
[1mStep[0m  [70/106], [94mLoss[0m : 2.34024
[1mStep[0m  [80/106], [94mLoss[0m : 2.36506
[1mStep[0m  [90/106], [94mLoss[0m : 2.50662
[1mStep[0m  [100/106], [94mLoss[0m : 2.29280

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.342, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10371
[1mStep[0m  [10/106], [94mLoss[0m : 2.29444
[1mStep[0m  [20/106], [94mLoss[0m : 2.35575
[1mStep[0m  [30/106], [94mLoss[0m : 1.92058
[1mStep[0m  [40/106], [94mLoss[0m : 2.22569
[1mStep[0m  [50/106], [94mLoss[0m : 2.14314
[1mStep[0m  [60/106], [94mLoss[0m : 2.25227
[1mStep[0m  [70/106], [94mLoss[0m : 2.60808
[1mStep[0m  [80/106], [94mLoss[0m : 2.22959
[1mStep[0m  [90/106], [94mLoss[0m : 2.47550
[1mStep[0m  [100/106], [94mLoss[0m : 2.23175

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16380
[1mStep[0m  [10/106], [94mLoss[0m : 2.19715
[1mStep[0m  [20/106], [94mLoss[0m : 2.10388
[1mStep[0m  [30/106], [94mLoss[0m : 2.06528
[1mStep[0m  [40/106], [94mLoss[0m : 2.28670
[1mStep[0m  [50/106], [94mLoss[0m : 2.37335
[1mStep[0m  [60/106], [94mLoss[0m : 2.38619
[1mStep[0m  [70/106], [94mLoss[0m : 2.52664
[1mStep[0m  [80/106], [94mLoss[0m : 2.36950
[1mStep[0m  [90/106], [94mLoss[0m : 1.98076
[1mStep[0m  [100/106], [94mLoss[0m : 2.25028

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05984
[1mStep[0m  [10/106], [94mLoss[0m : 2.37367
[1mStep[0m  [20/106], [94mLoss[0m : 1.98755
[1mStep[0m  [30/106], [94mLoss[0m : 1.95079
[1mStep[0m  [40/106], [94mLoss[0m : 2.10167
[1mStep[0m  [50/106], [94mLoss[0m : 2.40409
[1mStep[0m  [60/106], [94mLoss[0m : 2.17977
[1mStep[0m  [70/106], [94mLoss[0m : 2.05997
[1mStep[0m  [80/106], [94mLoss[0m : 2.18557
[1mStep[0m  [90/106], [94mLoss[0m : 2.58678
[1mStep[0m  [100/106], [94mLoss[0m : 2.23623

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.99303
[1mStep[0m  [10/106], [94mLoss[0m : 2.43508
[1mStep[0m  [20/106], [94mLoss[0m : 2.17810
[1mStep[0m  [30/106], [94mLoss[0m : 2.06944
[1mStep[0m  [40/106], [94mLoss[0m : 2.38207
[1mStep[0m  [50/106], [94mLoss[0m : 2.31634
[1mStep[0m  [60/106], [94mLoss[0m : 2.15374
[1mStep[0m  [70/106], [94mLoss[0m : 2.26108
[1mStep[0m  [80/106], [94mLoss[0m : 2.13862
[1mStep[0m  [90/106], [94mLoss[0m : 2.25631
[1mStep[0m  [100/106], [94mLoss[0m : 2.43798

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.315
====================================

Phase 1 - Evaluation MAE:  2.3153566122055054
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.26418
[1mStep[0m  [10/106], [94mLoss[0m : 2.49651
[1mStep[0m  [20/106], [94mLoss[0m : 2.44862
[1mStep[0m  [30/106], [94mLoss[0m : 2.62367
[1mStep[0m  [40/106], [94mLoss[0m : 2.46834
[1mStep[0m  [50/106], [94mLoss[0m : 2.78129
[1mStep[0m  [60/106], [94mLoss[0m : 2.36525
[1mStep[0m  [70/106], [94mLoss[0m : 2.18522
[1mStep[0m  [80/106], [94mLoss[0m : 2.25151
[1mStep[0m  [90/106], [94mLoss[0m : 2.50242
[1mStep[0m  [100/106], [94mLoss[0m : 2.50928

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.317, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21991
[1mStep[0m  [10/106], [94mLoss[0m : 2.25595
[1mStep[0m  [20/106], [94mLoss[0m : 2.13819
[1mStep[0m  [30/106], [94mLoss[0m : 2.38529
[1mStep[0m  [40/106], [94mLoss[0m : 2.44553
[1mStep[0m  [50/106], [94mLoss[0m : 2.25058
[1mStep[0m  [60/106], [94mLoss[0m : 2.33728
[1mStep[0m  [70/106], [94mLoss[0m : 2.31137
[1mStep[0m  [80/106], [94mLoss[0m : 2.20417
[1mStep[0m  [90/106], [94mLoss[0m : 2.53364
[1mStep[0m  [100/106], [94mLoss[0m : 2.36782

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16283
[1mStep[0m  [10/106], [94mLoss[0m : 2.20593
[1mStep[0m  [20/106], [94mLoss[0m : 2.37205
[1mStep[0m  [30/106], [94mLoss[0m : 2.03914
[1mStep[0m  [40/106], [94mLoss[0m : 2.28781
[1mStep[0m  [50/106], [94mLoss[0m : 2.05426
[1mStep[0m  [60/106], [94mLoss[0m : 2.11118
[1mStep[0m  [70/106], [94mLoss[0m : 2.48937
[1mStep[0m  [80/106], [94mLoss[0m : 2.25654
[1mStep[0m  [90/106], [94mLoss[0m : 2.60462
[1mStep[0m  [100/106], [94mLoss[0m : 2.50898

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24683
[1mStep[0m  [10/106], [94mLoss[0m : 2.06360
[1mStep[0m  [20/106], [94mLoss[0m : 2.04692
[1mStep[0m  [30/106], [94mLoss[0m : 2.15293
[1mStep[0m  [40/106], [94mLoss[0m : 1.99983
[1mStep[0m  [50/106], [94mLoss[0m : 2.40118
[1mStep[0m  [60/106], [94mLoss[0m : 2.05460
[1mStep[0m  [70/106], [94mLoss[0m : 2.26568
[1mStep[0m  [80/106], [94mLoss[0m : 2.19050
[1mStep[0m  [90/106], [94mLoss[0m : 1.99461
[1mStep[0m  [100/106], [94mLoss[0m : 2.21407

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08525
[1mStep[0m  [10/106], [94mLoss[0m : 2.06811
[1mStep[0m  [20/106], [94mLoss[0m : 1.96861
[1mStep[0m  [30/106], [94mLoss[0m : 2.09280
[1mStep[0m  [40/106], [94mLoss[0m : 2.27025
[1mStep[0m  [50/106], [94mLoss[0m : 2.44496
[1mStep[0m  [60/106], [94mLoss[0m : 2.26030
[1mStep[0m  [70/106], [94mLoss[0m : 2.07068
[1mStep[0m  [80/106], [94mLoss[0m : 2.25503
[1mStep[0m  [90/106], [94mLoss[0m : 2.08100
[1mStep[0m  [100/106], [94mLoss[0m : 2.08031

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75327
[1mStep[0m  [10/106], [94mLoss[0m : 1.69433
[1mStep[0m  [20/106], [94mLoss[0m : 2.02343
[1mStep[0m  [30/106], [94mLoss[0m : 1.90459
[1mStep[0m  [40/106], [94mLoss[0m : 2.18574
[1mStep[0m  [50/106], [94mLoss[0m : 2.17952
[1mStep[0m  [60/106], [94mLoss[0m : 1.96024
[1mStep[0m  [70/106], [94mLoss[0m : 2.01210
[1mStep[0m  [80/106], [94mLoss[0m : 2.01254
[1mStep[0m  [90/106], [94mLoss[0m : 2.03435
[1mStep[0m  [100/106], [94mLoss[0m : 2.19094

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64620
[1mStep[0m  [10/106], [94mLoss[0m : 1.91428
[1mStep[0m  [20/106], [94mLoss[0m : 1.89320
[1mStep[0m  [30/106], [94mLoss[0m : 1.61386
[1mStep[0m  [40/106], [94mLoss[0m : 2.05123
[1mStep[0m  [50/106], [94mLoss[0m : 2.34376
[1mStep[0m  [60/106], [94mLoss[0m : 1.96751
[1mStep[0m  [70/106], [94mLoss[0m : 1.88147
[1mStep[0m  [80/106], [94mLoss[0m : 2.13381
[1mStep[0m  [90/106], [94mLoss[0m : 1.89917
[1mStep[0m  [100/106], [94mLoss[0m : 2.12648

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.04079
[1mStep[0m  [10/106], [94mLoss[0m : 1.73648
[1mStep[0m  [20/106], [94mLoss[0m : 1.78440
[1mStep[0m  [30/106], [94mLoss[0m : 1.98557
[1mStep[0m  [40/106], [94mLoss[0m : 2.02463
[1mStep[0m  [50/106], [94mLoss[0m : 2.00689
[1mStep[0m  [60/106], [94mLoss[0m : 1.82986
[1mStep[0m  [70/106], [94mLoss[0m : 2.01794
[1mStep[0m  [80/106], [94mLoss[0m : 2.28161
[1mStep[0m  [90/106], [94mLoss[0m : 1.94912
[1mStep[0m  [100/106], [94mLoss[0m : 2.15027

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.923, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86007
[1mStep[0m  [10/106], [94mLoss[0m : 1.62762
[1mStep[0m  [20/106], [94mLoss[0m : 2.26075
[1mStep[0m  [30/106], [94mLoss[0m : 1.71391
[1mStep[0m  [40/106], [94mLoss[0m : 1.82618
[1mStep[0m  [50/106], [94mLoss[0m : 1.91724
[1mStep[0m  [60/106], [94mLoss[0m : 1.71865
[1mStep[0m  [70/106], [94mLoss[0m : 1.67392
[1mStep[0m  [80/106], [94mLoss[0m : 1.83996
[1mStep[0m  [90/106], [94mLoss[0m : 1.83109
[1mStep[0m  [100/106], [94mLoss[0m : 1.87187

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76113
[1mStep[0m  [10/106], [94mLoss[0m : 1.53227
[1mStep[0m  [20/106], [94mLoss[0m : 1.96051
[1mStep[0m  [30/106], [94mLoss[0m : 1.73390
[1mStep[0m  [40/106], [94mLoss[0m : 1.96028
[1mStep[0m  [50/106], [94mLoss[0m : 1.97260
[1mStep[0m  [60/106], [94mLoss[0m : 1.84286
[1mStep[0m  [70/106], [94mLoss[0m : 2.02726
[1mStep[0m  [80/106], [94mLoss[0m : 1.82356
[1mStep[0m  [90/106], [94mLoss[0m : 1.97357
[1mStep[0m  [100/106], [94mLoss[0m : 1.91244

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63717
[1mStep[0m  [10/106], [94mLoss[0m : 1.77888
[1mStep[0m  [20/106], [94mLoss[0m : 1.50023
[1mStep[0m  [30/106], [94mLoss[0m : 1.84803
[1mStep[0m  [40/106], [94mLoss[0m : 1.63257
[1mStep[0m  [50/106], [94mLoss[0m : 1.76462
[1mStep[0m  [60/106], [94mLoss[0m : 1.34054
[1mStep[0m  [70/106], [94mLoss[0m : 1.80537
[1mStep[0m  [80/106], [94mLoss[0m : 1.86971
[1mStep[0m  [90/106], [94mLoss[0m : 1.65387
[1mStep[0m  [100/106], [94mLoss[0m : 1.77025

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74077
[1mStep[0m  [10/106], [94mLoss[0m : 1.54398
[1mStep[0m  [20/106], [94mLoss[0m : 1.98755
[1mStep[0m  [30/106], [94mLoss[0m : 1.73470
[1mStep[0m  [40/106], [94mLoss[0m : 1.58087
[1mStep[0m  [50/106], [94mLoss[0m : 1.70300
[1mStep[0m  [60/106], [94mLoss[0m : 1.70336
[1mStep[0m  [70/106], [94mLoss[0m : 1.94330
[1mStep[0m  [80/106], [94mLoss[0m : 1.71820
[1mStep[0m  [90/106], [94mLoss[0m : 1.56960
[1mStep[0m  [100/106], [94mLoss[0m : 1.47013

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49395
[1mStep[0m  [10/106], [94mLoss[0m : 1.43234
[1mStep[0m  [20/106], [94mLoss[0m : 1.89457
[1mStep[0m  [30/106], [94mLoss[0m : 1.80844
[1mStep[0m  [40/106], [94mLoss[0m : 1.47946
[1mStep[0m  [50/106], [94mLoss[0m : 1.64883
[1mStep[0m  [60/106], [94mLoss[0m : 1.60238
[1mStep[0m  [70/106], [94mLoss[0m : 1.68284
[1mStep[0m  [80/106], [94mLoss[0m : 1.70244
[1mStep[0m  [90/106], [94mLoss[0m : 1.79519
[1mStep[0m  [100/106], [94mLoss[0m : 1.73918

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56325
[1mStep[0m  [10/106], [94mLoss[0m : 1.72471
[1mStep[0m  [20/106], [94mLoss[0m : 1.77324
[1mStep[0m  [30/106], [94mLoss[0m : 1.87600
[1mStep[0m  [40/106], [94mLoss[0m : 1.71040
[1mStep[0m  [50/106], [94mLoss[0m : 1.47092
[1mStep[0m  [60/106], [94mLoss[0m : 1.72406
[1mStep[0m  [70/106], [94mLoss[0m : 1.58848
[1mStep[0m  [80/106], [94mLoss[0m : 2.00782
[1mStep[0m  [90/106], [94mLoss[0m : 1.69896
[1mStep[0m  [100/106], [94mLoss[0m : 1.75556

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.669, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55181
[1mStep[0m  [10/106], [94mLoss[0m : 1.67873
[1mStep[0m  [20/106], [94mLoss[0m : 1.90711
[1mStep[0m  [30/106], [94mLoss[0m : 1.41798
[1mStep[0m  [40/106], [94mLoss[0m : 1.67994
[1mStep[0m  [50/106], [94mLoss[0m : 1.60242
[1mStep[0m  [60/106], [94mLoss[0m : 1.73022
[1mStep[0m  [70/106], [94mLoss[0m : 1.81385
[1mStep[0m  [80/106], [94mLoss[0m : 1.71582
[1mStep[0m  [90/106], [94mLoss[0m : 1.61260
[1mStep[0m  [100/106], [94mLoss[0m : 1.57463

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.41512
[1mStep[0m  [10/106], [94mLoss[0m : 1.47090
[1mStep[0m  [20/106], [94mLoss[0m : 1.73391
[1mStep[0m  [30/106], [94mLoss[0m : 1.65966
[1mStep[0m  [40/106], [94mLoss[0m : 1.45013
[1mStep[0m  [50/106], [94mLoss[0m : 1.82295
[1mStep[0m  [60/106], [94mLoss[0m : 1.61195
[1mStep[0m  [70/106], [94mLoss[0m : 1.65953
[1mStep[0m  [80/106], [94mLoss[0m : 1.44639
[1mStep[0m  [90/106], [94mLoss[0m : 1.57166
[1mStep[0m  [100/106], [94mLoss[0m : 1.39049

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63684
[1mStep[0m  [10/106], [94mLoss[0m : 1.50233
[1mStep[0m  [20/106], [94mLoss[0m : 1.64194
[1mStep[0m  [30/106], [94mLoss[0m : 1.68456
[1mStep[0m  [40/106], [94mLoss[0m : 1.48618
[1mStep[0m  [50/106], [94mLoss[0m : 1.67287
[1mStep[0m  [60/106], [94mLoss[0m : 1.83486
[1mStep[0m  [70/106], [94mLoss[0m : 1.64340
[1mStep[0m  [80/106], [94mLoss[0m : 1.59612
[1mStep[0m  [90/106], [94mLoss[0m : 1.57326
[1mStep[0m  [100/106], [94mLoss[0m : 1.59371

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.40596
[1mStep[0m  [10/106], [94mLoss[0m : 1.51758
[1mStep[0m  [20/106], [94mLoss[0m : 1.33628
[1mStep[0m  [30/106], [94mLoss[0m : 1.30582
[1mStep[0m  [40/106], [94mLoss[0m : 1.58953
[1mStep[0m  [50/106], [94mLoss[0m : 1.53098
[1mStep[0m  [60/106], [94mLoss[0m : 1.46134
[1mStep[0m  [70/106], [94mLoss[0m : 1.77206
[1mStep[0m  [80/106], [94mLoss[0m : 1.92578
[1mStep[0m  [90/106], [94mLoss[0m : 1.56299
[1mStep[0m  [100/106], [94mLoss[0m : 1.57359

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.538, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59717
[1mStep[0m  [10/106], [94mLoss[0m : 1.61659
[1mStep[0m  [20/106], [94mLoss[0m : 1.62409
[1mStep[0m  [30/106], [94mLoss[0m : 1.53181
[1mStep[0m  [40/106], [94mLoss[0m : 1.58968
[1mStep[0m  [50/106], [94mLoss[0m : 1.32128
[1mStep[0m  [60/106], [94mLoss[0m : 1.72269
[1mStep[0m  [70/106], [94mLoss[0m : 1.57931
[1mStep[0m  [80/106], [94mLoss[0m : 1.61567
[1mStep[0m  [90/106], [94mLoss[0m : 1.52600
[1mStep[0m  [100/106], [94mLoss[0m : 1.48875

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.546, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.36632
[1mStep[0m  [10/106], [94mLoss[0m : 1.71410
[1mStep[0m  [20/106], [94mLoss[0m : 1.44003
[1mStep[0m  [30/106], [94mLoss[0m : 1.48418
[1mStep[0m  [40/106], [94mLoss[0m : 1.81171
[1mStep[0m  [50/106], [94mLoss[0m : 1.42761
[1mStep[0m  [60/106], [94mLoss[0m : 1.54366
[1mStep[0m  [70/106], [94mLoss[0m : 1.41200
[1mStep[0m  [80/106], [94mLoss[0m : 1.36369
[1mStep[0m  [90/106], [94mLoss[0m : 1.54713
[1mStep[0m  [100/106], [94mLoss[0m : 1.32475

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43210
[1mStep[0m  [10/106], [94mLoss[0m : 1.44368
[1mStep[0m  [20/106], [94mLoss[0m : 1.48699
[1mStep[0m  [30/106], [94mLoss[0m : 1.36132
[1mStep[0m  [40/106], [94mLoss[0m : 1.28952
[1mStep[0m  [50/106], [94mLoss[0m : 1.63343
[1mStep[0m  [60/106], [94mLoss[0m : 1.30602
[1mStep[0m  [70/106], [94mLoss[0m : 1.45501
[1mStep[0m  [80/106], [94mLoss[0m : 1.32248
[1mStep[0m  [90/106], [94mLoss[0m : 1.55520
[1mStep[0m  [100/106], [94mLoss[0m : 1.44507

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.445, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52318
[1mStep[0m  [10/106], [94mLoss[0m : 1.49124
[1mStep[0m  [20/106], [94mLoss[0m : 1.32067
[1mStep[0m  [30/106], [94mLoss[0m : 1.66358
[1mStep[0m  [40/106], [94mLoss[0m : 1.28370
[1mStep[0m  [50/106], [94mLoss[0m : 1.42991
[1mStep[0m  [60/106], [94mLoss[0m : 1.70113
[1mStep[0m  [70/106], [94mLoss[0m : 1.40339
[1mStep[0m  [80/106], [94mLoss[0m : 1.27392
[1mStep[0m  [90/106], [94mLoss[0m : 1.54969
[1mStep[0m  [100/106], [94mLoss[0m : 1.44089

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.417, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.34321
[1mStep[0m  [10/106], [94mLoss[0m : 1.49782
[1mStep[0m  [20/106], [94mLoss[0m : 1.32056
[1mStep[0m  [30/106], [94mLoss[0m : 1.40122
[1mStep[0m  [40/106], [94mLoss[0m : 1.27856
[1mStep[0m  [50/106], [94mLoss[0m : 1.30530
[1mStep[0m  [60/106], [94mLoss[0m : 1.40351
[1mStep[0m  [70/106], [94mLoss[0m : 1.81656
[1mStep[0m  [80/106], [94mLoss[0m : 1.46002
[1mStep[0m  [90/106], [94mLoss[0m : 1.48364
[1mStep[0m  [100/106], [94mLoss[0m : 1.62653

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.396, [92mTest[0m: 2.481, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.38969
[1mStep[0m  [10/106], [94mLoss[0m : 1.39390
[1mStep[0m  [20/106], [94mLoss[0m : 1.29357
[1mStep[0m  [30/106], [94mLoss[0m : 1.58108
[1mStep[0m  [40/106], [94mLoss[0m : 1.49529
[1mStep[0m  [50/106], [94mLoss[0m : 1.34522
[1mStep[0m  [60/106], [94mLoss[0m : 1.31732
[1mStep[0m  [70/106], [94mLoss[0m : 1.40558
[1mStep[0m  [80/106], [94mLoss[0m : 1.48125
[1mStep[0m  [90/106], [94mLoss[0m : 1.36017
[1mStep[0m  [100/106], [94mLoss[0m : 1.32381

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.369, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.28400
[1mStep[0m  [10/106], [94mLoss[0m : 1.42580
[1mStep[0m  [20/106], [94mLoss[0m : 1.26118
[1mStep[0m  [30/106], [94mLoss[0m : 1.36721
[1mStep[0m  [40/106], [94mLoss[0m : 1.31946
[1mStep[0m  [50/106], [94mLoss[0m : 1.31793
[1mStep[0m  [60/106], [94mLoss[0m : 1.59694
[1mStep[0m  [70/106], [94mLoss[0m : 1.41790
[1mStep[0m  [80/106], [94mLoss[0m : 1.35853
[1mStep[0m  [90/106], [94mLoss[0m : 1.29217
[1mStep[0m  [100/106], [94mLoss[0m : 1.46084

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.344, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.460
====================================

Phase 2 - Evaluation MAE:  2.4596910161792107
MAE score P1        2.315357
MAE score P2        2.459691
loss                1.343699
learning_rate           0.01
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.68023
[1mStep[0m  [5/53], [94mLoss[0m : 11.08191
[1mStep[0m  [10/53], [94mLoss[0m : 10.53884
[1mStep[0m  [15/53], [94mLoss[0m : 10.45817
[1mStep[0m  [20/53], [94mLoss[0m : 10.53879
[1mStep[0m  [25/53], [94mLoss[0m : 10.35124
[1mStep[0m  [30/53], [94mLoss[0m : 10.79019
[1mStep[0m  [35/53], [94mLoss[0m : 10.65996
[1mStep[0m  [40/53], [94mLoss[0m : 10.24003
[1mStep[0m  [45/53], [94mLoss[0m : 9.80068
[1mStep[0m  [50/53], [94mLoss[0m : 9.87938

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.452, [92mTest[0m: 10.919, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.20689
[1mStep[0m  [5/53], [94mLoss[0m : 9.69829
[1mStep[0m  [10/53], [94mLoss[0m : 9.73876
[1mStep[0m  [15/53], [94mLoss[0m : 9.44580
[1mStep[0m  [20/53], [94mLoss[0m : 9.29745
[1mStep[0m  [25/53], [94mLoss[0m : 9.05820
[1mStep[0m  [30/53], [94mLoss[0m : 8.92407
[1mStep[0m  [35/53], [94mLoss[0m : 9.06234
[1mStep[0m  [40/53], [94mLoss[0m : 8.59944
[1mStep[0m  [45/53], [94mLoss[0m : 8.87466
[1mStep[0m  [50/53], [94mLoss[0m : 8.56674

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.264, [92mTest[0m: 9.739, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.69046
[1mStep[0m  [5/53], [94mLoss[0m : 8.44290
[1mStep[0m  [10/53], [94mLoss[0m : 8.33176
[1mStep[0m  [15/53], [94mLoss[0m : 8.39170
[1mStep[0m  [20/53], [94mLoss[0m : 7.87843
[1mStep[0m  [25/53], [94mLoss[0m : 7.82963
[1mStep[0m  [30/53], [94mLoss[0m : 7.90062
[1mStep[0m  [35/53], [94mLoss[0m : 7.51540
[1mStep[0m  [40/53], [94mLoss[0m : 7.51119
[1mStep[0m  [45/53], [94mLoss[0m : 7.35646
[1mStep[0m  [50/53], [94mLoss[0m : 7.39625

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.846, [92mTest[0m: 8.010, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.06314
[1mStep[0m  [5/53], [94mLoss[0m : 7.22418
[1mStep[0m  [10/53], [94mLoss[0m : 6.75202
[1mStep[0m  [15/53], [94mLoss[0m : 6.99417
[1mStep[0m  [20/53], [94mLoss[0m : 6.82774
[1mStep[0m  [25/53], [94mLoss[0m : 6.23026
[1mStep[0m  [30/53], [94mLoss[0m : 6.30346
[1mStep[0m  [35/53], [94mLoss[0m : 6.76973
[1mStep[0m  [40/53], [94mLoss[0m : 6.57469
[1mStep[0m  [45/53], [94mLoss[0m : 6.06127
[1mStep[0m  [50/53], [94mLoss[0m : 6.22337

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.636, [92mTest[0m: 6.505, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.71336
[1mStep[0m  [5/53], [94mLoss[0m : 6.13291
[1mStep[0m  [10/53], [94mLoss[0m : 5.81758
[1mStep[0m  [15/53], [94mLoss[0m : 5.91076
[1mStep[0m  [20/53], [94mLoss[0m : 5.55270
[1mStep[0m  [25/53], [94mLoss[0m : 5.49748
[1mStep[0m  [30/53], [94mLoss[0m : 5.48907
[1mStep[0m  [35/53], [94mLoss[0m : 5.02641
[1mStep[0m  [40/53], [94mLoss[0m : 5.01141
[1mStep[0m  [45/53], [94mLoss[0m : 4.60597
[1mStep[0m  [50/53], [94mLoss[0m : 4.81442

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.497, [92mTest[0m: 5.226, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.55491
[1mStep[0m  [5/53], [94mLoss[0m : 4.47831
[1mStep[0m  [10/53], [94mLoss[0m : 4.66004
[1mStep[0m  [15/53], [94mLoss[0m : 4.19549
[1mStep[0m  [20/53], [94mLoss[0m : 4.64264
[1mStep[0m  [25/53], [94mLoss[0m : 4.16415
[1mStep[0m  [30/53], [94mLoss[0m : 3.99569
[1mStep[0m  [35/53], [94mLoss[0m : 3.91362
[1mStep[0m  [40/53], [94mLoss[0m : 3.89708
[1mStep[0m  [45/53], [94mLoss[0m : 3.75330
[1mStep[0m  [50/53], [94mLoss[0m : 3.43722

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.246, [92mTest[0m: 4.030, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.56975
[1mStep[0m  [5/53], [94mLoss[0m : 3.29865
[1mStep[0m  [10/53], [94mLoss[0m : 3.79323
[1mStep[0m  [15/53], [94mLoss[0m : 3.55816
[1mStep[0m  [20/53], [94mLoss[0m : 3.12600
[1mStep[0m  [25/53], [94mLoss[0m : 3.19511
[1mStep[0m  [30/53], [94mLoss[0m : 3.07647
[1mStep[0m  [35/53], [94mLoss[0m : 3.13179
[1mStep[0m  [40/53], [94mLoss[0m : 2.95043
[1mStep[0m  [45/53], [94mLoss[0m : 2.73773
[1mStep[0m  [50/53], [94mLoss[0m : 2.68639

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.084, [92mTest[0m: 2.989, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58710
[1mStep[0m  [5/53], [94mLoss[0m : 2.74737
[1mStep[0m  [10/53], [94mLoss[0m : 2.52062
[1mStep[0m  [15/53], [94mLoss[0m : 2.70580
[1mStep[0m  [20/53], [94mLoss[0m : 2.45062
[1mStep[0m  [25/53], [94mLoss[0m : 2.63427
[1mStep[0m  [30/53], [94mLoss[0m : 2.65066
[1mStep[0m  [35/53], [94mLoss[0m : 2.66473
[1mStep[0m  [40/53], [94mLoss[0m : 2.61457
[1mStep[0m  [45/53], [94mLoss[0m : 2.79043
[1mStep[0m  [50/53], [94mLoss[0m : 2.69867

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58686
[1mStep[0m  [5/53], [94mLoss[0m : 2.53289
[1mStep[0m  [10/53], [94mLoss[0m : 2.88426
[1mStep[0m  [15/53], [94mLoss[0m : 2.67719
[1mStep[0m  [20/53], [94mLoss[0m : 2.75003
[1mStep[0m  [25/53], [94mLoss[0m : 2.64092
[1mStep[0m  [30/53], [94mLoss[0m : 2.62110
[1mStep[0m  [35/53], [94mLoss[0m : 2.86609
[1mStep[0m  [40/53], [94mLoss[0m : 2.41741
[1mStep[0m  [45/53], [94mLoss[0m : 2.62445
[1mStep[0m  [50/53], [94mLoss[0m : 2.51831

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.518, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.73304
[1mStep[0m  [5/53], [94mLoss[0m : 2.68847
[1mStep[0m  [10/53], [94mLoss[0m : 2.72640
[1mStep[0m  [15/53], [94mLoss[0m : 2.33409
[1mStep[0m  [20/53], [94mLoss[0m : 2.52108
[1mStep[0m  [25/53], [94mLoss[0m : 2.84402
[1mStep[0m  [30/53], [94mLoss[0m : 2.43931
[1mStep[0m  [35/53], [94mLoss[0m : 2.70288
[1mStep[0m  [40/53], [94mLoss[0m : 2.54851
[1mStep[0m  [45/53], [94mLoss[0m : 2.46001
[1mStep[0m  [50/53], [94mLoss[0m : 2.51615

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43935
[1mStep[0m  [5/53], [94mLoss[0m : 2.49440
[1mStep[0m  [10/53], [94mLoss[0m : 2.74880
[1mStep[0m  [15/53], [94mLoss[0m : 2.54443
[1mStep[0m  [20/53], [94mLoss[0m : 2.59845
[1mStep[0m  [25/53], [94mLoss[0m : 2.55141
[1mStep[0m  [30/53], [94mLoss[0m : 2.52875
[1mStep[0m  [35/53], [94mLoss[0m : 2.57354
[1mStep[0m  [40/53], [94mLoss[0m : 2.34486
[1mStep[0m  [45/53], [94mLoss[0m : 2.50106
[1mStep[0m  [50/53], [94mLoss[0m : 2.46710

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58968
[1mStep[0m  [5/53], [94mLoss[0m : 2.69653
[1mStep[0m  [10/53], [94mLoss[0m : 2.59655
[1mStep[0m  [15/53], [94mLoss[0m : 2.40158
[1mStep[0m  [20/53], [94mLoss[0m : 2.74061
[1mStep[0m  [25/53], [94mLoss[0m : 2.54988
[1mStep[0m  [30/53], [94mLoss[0m : 2.59806
[1mStep[0m  [35/53], [94mLoss[0m : 2.77368
[1mStep[0m  [40/53], [94mLoss[0m : 2.59379
[1mStep[0m  [45/53], [94mLoss[0m : 2.42364
[1mStep[0m  [50/53], [94mLoss[0m : 2.48930

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.572, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66998
[1mStep[0m  [5/53], [94mLoss[0m : 2.53465
[1mStep[0m  [10/53], [94mLoss[0m : 2.45822
[1mStep[0m  [15/53], [94mLoss[0m : 2.32265
[1mStep[0m  [20/53], [94mLoss[0m : 2.65270
[1mStep[0m  [25/53], [94mLoss[0m : 2.36518
[1mStep[0m  [30/53], [94mLoss[0m : 2.52289
[1mStep[0m  [35/53], [94mLoss[0m : 2.46297
[1mStep[0m  [40/53], [94mLoss[0m : 2.47636
[1mStep[0m  [45/53], [94mLoss[0m : 2.55657
[1mStep[0m  [50/53], [94mLoss[0m : 2.64415

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48880
[1mStep[0m  [5/53], [94mLoss[0m : 2.47824
[1mStep[0m  [10/53], [94mLoss[0m : 2.52702
[1mStep[0m  [15/53], [94mLoss[0m : 2.40401
[1mStep[0m  [20/53], [94mLoss[0m : 2.62082
[1mStep[0m  [25/53], [94mLoss[0m : 2.40890
[1mStep[0m  [30/53], [94mLoss[0m : 2.42088
[1mStep[0m  [35/53], [94mLoss[0m : 2.33066
[1mStep[0m  [40/53], [94mLoss[0m : 2.63768
[1mStep[0m  [45/53], [94mLoss[0m : 2.42597
[1mStep[0m  [50/53], [94mLoss[0m : 2.38327

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28442
[1mStep[0m  [5/53], [94mLoss[0m : 2.47214
[1mStep[0m  [10/53], [94mLoss[0m : 2.44509
[1mStep[0m  [15/53], [94mLoss[0m : 2.50585
[1mStep[0m  [20/53], [94mLoss[0m : 2.45522
[1mStep[0m  [25/53], [94mLoss[0m : 2.38912
[1mStep[0m  [30/53], [94mLoss[0m : 2.46570
[1mStep[0m  [35/53], [94mLoss[0m : 2.46575
[1mStep[0m  [40/53], [94mLoss[0m : 2.35009
[1mStep[0m  [45/53], [94mLoss[0m : 2.40605
[1mStep[0m  [50/53], [94mLoss[0m : 2.40207

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.74878
[1mStep[0m  [5/53], [94mLoss[0m : 2.44608
[1mStep[0m  [10/53], [94mLoss[0m : 2.53537
[1mStep[0m  [15/53], [94mLoss[0m : 2.35064
[1mStep[0m  [20/53], [94mLoss[0m : 2.52078
[1mStep[0m  [25/53], [94mLoss[0m : 2.49623
[1mStep[0m  [30/53], [94mLoss[0m : 2.31676
[1mStep[0m  [35/53], [94mLoss[0m : 2.45757
[1mStep[0m  [40/53], [94mLoss[0m : 2.34575
[1mStep[0m  [45/53], [94mLoss[0m : 2.38511
[1mStep[0m  [50/53], [94mLoss[0m : 2.57032

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46285
[1mStep[0m  [5/53], [94mLoss[0m : 2.38563
[1mStep[0m  [10/53], [94mLoss[0m : 2.27616
[1mStep[0m  [15/53], [94mLoss[0m : 2.29200
[1mStep[0m  [20/53], [94mLoss[0m : 2.57109
[1mStep[0m  [25/53], [94mLoss[0m : 2.48826
[1mStep[0m  [30/53], [94mLoss[0m : 2.62542
[1mStep[0m  [35/53], [94mLoss[0m : 2.32367
[1mStep[0m  [40/53], [94mLoss[0m : 2.56216
[1mStep[0m  [45/53], [94mLoss[0m : 2.43100
[1mStep[0m  [50/53], [94mLoss[0m : 2.52267

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46822
[1mStep[0m  [5/53], [94mLoss[0m : 2.37234
[1mStep[0m  [10/53], [94mLoss[0m : 2.40812
[1mStep[0m  [15/53], [94mLoss[0m : 2.27748
[1mStep[0m  [20/53], [94mLoss[0m : 2.49690
[1mStep[0m  [25/53], [94mLoss[0m : 2.52798
[1mStep[0m  [30/53], [94mLoss[0m : 2.39617
[1mStep[0m  [35/53], [94mLoss[0m : 2.46887
[1mStep[0m  [40/53], [94mLoss[0m : 2.44884
[1mStep[0m  [45/53], [94mLoss[0m : 2.55874
[1mStep[0m  [50/53], [94mLoss[0m : 2.42957

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51844
[1mStep[0m  [5/53], [94mLoss[0m : 2.38900
[1mStep[0m  [10/53], [94mLoss[0m : 2.54890
[1mStep[0m  [15/53], [94mLoss[0m : 2.44572
[1mStep[0m  [20/53], [94mLoss[0m : 2.49744
[1mStep[0m  [25/53], [94mLoss[0m : 2.46464
[1mStep[0m  [30/53], [94mLoss[0m : 2.51495
[1mStep[0m  [35/53], [94mLoss[0m : 2.55130
[1mStep[0m  [40/53], [94mLoss[0m : 2.51270
[1mStep[0m  [45/53], [94mLoss[0m : 2.43153
[1mStep[0m  [50/53], [94mLoss[0m : 2.54000

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43381
[1mStep[0m  [5/53], [94mLoss[0m : 2.46666
[1mStep[0m  [10/53], [94mLoss[0m : 2.48369
[1mStep[0m  [15/53], [94mLoss[0m : 2.51899
[1mStep[0m  [20/53], [94mLoss[0m : 2.33513
[1mStep[0m  [25/53], [94mLoss[0m : 2.46548
[1mStep[0m  [30/53], [94mLoss[0m : 2.46670
[1mStep[0m  [35/53], [94mLoss[0m : 2.66244
[1mStep[0m  [40/53], [94mLoss[0m : 2.28047
[1mStep[0m  [45/53], [94mLoss[0m : 2.49014
[1mStep[0m  [50/53], [94mLoss[0m : 2.58234

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68772
[1mStep[0m  [5/53], [94mLoss[0m : 2.46001
[1mStep[0m  [10/53], [94mLoss[0m : 2.67137
[1mStep[0m  [15/53], [94mLoss[0m : 2.65781
[1mStep[0m  [20/53], [94mLoss[0m : 2.54900
[1mStep[0m  [25/53], [94mLoss[0m : 2.70958
[1mStep[0m  [30/53], [94mLoss[0m : 2.56433
[1mStep[0m  [35/53], [94mLoss[0m : 2.63422
[1mStep[0m  [40/53], [94mLoss[0m : 2.35424
[1mStep[0m  [45/53], [94mLoss[0m : 2.40647
[1mStep[0m  [50/53], [94mLoss[0m : 2.20647

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.407, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44387
[1mStep[0m  [5/53], [94mLoss[0m : 2.34059
[1mStep[0m  [10/53], [94mLoss[0m : 2.59638
[1mStep[0m  [15/53], [94mLoss[0m : 2.41613
[1mStep[0m  [20/53], [94mLoss[0m : 2.42725
[1mStep[0m  [25/53], [94mLoss[0m : 2.56182
[1mStep[0m  [30/53], [94mLoss[0m : 2.34535
[1mStep[0m  [35/53], [94mLoss[0m : 2.52025
[1mStep[0m  [40/53], [94mLoss[0m : 2.46055
[1mStep[0m  [45/53], [94mLoss[0m : 2.28903
[1mStep[0m  [50/53], [94mLoss[0m : 2.42338

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42732
[1mStep[0m  [5/53], [94mLoss[0m : 2.49335
[1mStep[0m  [10/53], [94mLoss[0m : 2.44746
[1mStep[0m  [15/53], [94mLoss[0m : 2.42367
[1mStep[0m  [20/53], [94mLoss[0m : 2.36995
[1mStep[0m  [25/53], [94mLoss[0m : 2.30958
[1mStep[0m  [30/53], [94mLoss[0m : 2.45827
[1mStep[0m  [35/53], [94mLoss[0m : 2.41423
[1mStep[0m  [40/53], [94mLoss[0m : 2.49336
[1mStep[0m  [45/53], [94mLoss[0m : 2.43289
[1mStep[0m  [50/53], [94mLoss[0m : 2.42704

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.426, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49165
[1mStep[0m  [5/53], [94mLoss[0m : 2.32521
[1mStep[0m  [10/53], [94mLoss[0m : 2.45782
[1mStep[0m  [15/53], [94mLoss[0m : 2.44068
[1mStep[0m  [20/53], [94mLoss[0m : 2.48487
[1mStep[0m  [25/53], [94mLoss[0m : 2.34703
[1mStep[0m  [30/53], [94mLoss[0m : 2.57143
[1mStep[0m  [35/53], [94mLoss[0m : 2.41102
[1mStep[0m  [40/53], [94mLoss[0m : 2.42381
[1mStep[0m  [45/53], [94mLoss[0m : 2.58662
[1mStep[0m  [50/53], [94mLoss[0m : 2.51589

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.414, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43207
[1mStep[0m  [5/53], [94mLoss[0m : 2.42527
[1mStep[0m  [10/53], [94mLoss[0m : 2.69111
[1mStep[0m  [15/53], [94mLoss[0m : 2.26683
[1mStep[0m  [20/53], [94mLoss[0m : 2.47127
[1mStep[0m  [25/53], [94mLoss[0m : 2.47725
[1mStep[0m  [30/53], [94mLoss[0m : 2.65831
[1mStep[0m  [35/53], [94mLoss[0m : 2.39159
[1mStep[0m  [40/53], [94mLoss[0m : 2.80590
[1mStep[0m  [45/53], [94mLoss[0m : 2.34667
[1mStep[0m  [50/53], [94mLoss[0m : 2.41612

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32005
[1mStep[0m  [5/53], [94mLoss[0m : 2.38866
[1mStep[0m  [10/53], [94mLoss[0m : 2.49298
[1mStep[0m  [15/53], [94mLoss[0m : 2.37693
[1mStep[0m  [20/53], [94mLoss[0m : 2.34185
[1mStep[0m  [25/53], [94mLoss[0m : 2.59815
[1mStep[0m  [30/53], [94mLoss[0m : 2.48594
[1mStep[0m  [35/53], [94mLoss[0m : 2.61976
[1mStep[0m  [40/53], [94mLoss[0m : 2.36114
[1mStep[0m  [45/53], [94mLoss[0m : 2.72544
[1mStep[0m  [50/53], [94mLoss[0m : 2.25463

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48019
[1mStep[0m  [5/53], [94mLoss[0m : 2.53414
[1mStep[0m  [10/53], [94mLoss[0m : 2.18270
[1mStep[0m  [15/53], [94mLoss[0m : 2.41856
[1mStep[0m  [20/53], [94mLoss[0m : 2.69387
[1mStep[0m  [25/53], [94mLoss[0m : 2.36144
[1mStep[0m  [30/53], [94mLoss[0m : 2.28722
[1mStep[0m  [35/53], [94mLoss[0m : 2.63734
[1mStep[0m  [40/53], [94mLoss[0m : 2.48494
[1mStep[0m  [45/53], [94mLoss[0m : 2.48614
[1mStep[0m  [50/53], [94mLoss[0m : 2.57744

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.422, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62016
[1mStep[0m  [5/53], [94mLoss[0m : 2.52680
[1mStep[0m  [10/53], [94mLoss[0m : 2.40186
[1mStep[0m  [15/53], [94mLoss[0m : 2.34411
[1mStep[0m  [20/53], [94mLoss[0m : 2.26906
[1mStep[0m  [25/53], [94mLoss[0m : 2.36771
[1mStep[0m  [30/53], [94mLoss[0m : 2.38084
[1mStep[0m  [35/53], [94mLoss[0m : 2.48711
[1mStep[0m  [40/53], [94mLoss[0m : 2.61635
[1mStep[0m  [45/53], [94mLoss[0m : 2.56110
[1mStep[0m  [50/53], [94mLoss[0m : 2.62564

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.405, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19786
[1mStep[0m  [5/53], [94mLoss[0m : 2.49921
[1mStep[0m  [10/53], [94mLoss[0m : 2.50069
[1mStep[0m  [15/53], [94mLoss[0m : 2.41777
[1mStep[0m  [20/53], [94mLoss[0m : 2.32207
[1mStep[0m  [25/53], [94mLoss[0m : 2.44484
[1mStep[0m  [30/53], [94mLoss[0m : 2.52005
[1mStep[0m  [35/53], [94mLoss[0m : 2.39114
[1mStep[0m  [40/53], [94mLoss[0m : 2.36413
[1mStep[0m  [45/53], [94mLoss[0m : 2.36722
[1mStep[0m  [50/53], [94mLoss[0m : 2.30034

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30443
[1mStep[0m  [5/53], [94mLoss[0m : 2.28464
[1mStep[0m  [10/53], [94mLoss[0m : 2.62102
[1mStep[0m  [15/53], [94mLoss[0m : 2.56966
[1mStep[0m  [20/53], [94mLoss[0m : 2.45505
[1mStep[0m  [25/53], [94mLoss[0m : 2.49957
[1mStep[0m  [30/53], [94mLoss[0m : 2.56478
[1mStep[0m  [35/53], [94mLoss[0m : 2.61687
[1mStep[0m  [40/53], [94mLoss[0m : 2.37155
[1mStep[0m  [45/53], [94mLoss[0m : 2.47820
[1mStep[0m  [50/53], [94mLoss[0m : 2.53506

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.436, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.383
====================================

Phase 1 - Evaluation MAE:  2.3834546437630286
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.43302
[1mStep[0m  [5/53], [94mLoss[0m : 2.54860
[1mStep[0m  [10/53], [94mLoss[0m : 2.61558
[1mStep[0m  [15/53], [94mLoss[0m : 2.55563
[1mStep[0m  [20/53], [94mLoss[0m : 2.67597
[1mStep[0m  [25/53], [94mLoss[0m : 2.38781
[1mStep[0m  [30/53], [94mLoss[0m : 2.47241
[1mStep[0m  [35/53], [94mLoss[0m : 2.46940
[1mStep[0m  [40/53], [94mLoss[0m : 2.27205
[1mStep[0m  [45/53], [94mLoss[0m : 2.43171
[1mStep[0m  [50/53], [94mLoss[0m : 2.66400

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61332
[1mStep[0m  [5/53], [94mLoss[0m : 2.38005
[1mStep[0m  [10/53], [94mLoss[0m : 2.35053
[1mStep[0m  [15/53], [94mLoss[0m : 2.49767
[1mStep[0m  [20/53], [94mLoss[0m : 2.30162
[1mStep[0m  [25/53], [94mLoss[0m : 2.32045
[1mStep[0m  [30/53], [94mLoss[0m : 2.36533
[1mStep[0m  [35/53], [94mLoss[0m : 2.57472
[1mStep[0m  [40/53], [94mLoss[0m : 2.31401
[1mStep[0m  [45/53], [94mLoss[0m : 2.51417
[1mStep[0m  [50/53], [94mLoss[0m : 2.45464

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58493
[1mStep[0m  [5/53], [94mLoss[0m : 2.52110
[1mStep[0m  [10/53], [94mLoss[0m : 2.41260
[1mStep[0m  [15/53], [94mLoss[0m : 2.50097
[1mStep[0m  [20/53], [94mLoss[0m : 2.27182
[1mStep[0m  [25/53], [94mLoss[0m : 2.36682
[1mStep[0m  [30/53], [94mLoss[0m : 2.40207
[1mStep[0m  [35/53], [94mLoss[0m : 2.26340
[1mStep[0m  [40/53], [94mLoss[0m : 2.44296
[1mStep[0m  [45/53], [94mLoss[0m : 2.43083
[1mStep[0m  [50/53], [94mLoss[0m : 2.49938

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.631, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53622
[1mStep[0m  [5/53], [94mLoss[0m : 2.37634
[1mStep[0m  [10/53], [94mLoss[0m : 2.28354
[1mStep[0m  [15/53], [94mLoss[0m : 2.51684
[1mStep[0m  [20/53], [94mLoss[0m : 2.43459
[1mStep[0m  [25/53], [94mLoss[0m : 2.20724
[1mStep[0m  [30/53], [94mLoss[0m : 2.35358
[1mStep[0m  [35/53], [94mLoss[0m : 2.39599
[1mStep[0m  [40/53], [94mLoss[0m : 2.31550
[1mStep[0m  [45/53], [94mLoss[0m : 2.50692
[1mStep[0m  [50/53], [94mLoss[0m : 2.30470

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.16472
[1mStep[0m  [5/53], [94mLoss[0m : 2.19379
[1mStep[0m  [10/53], [94mLoss[0m : 2.27735
[1mStep[0m  [15/53], [94mLoss[0m : 2.37215
[1mStep[0m  [20/53], [94mLoss[0m : 2.32205
[1mStep[0m  [25/53], [94mLoss[0m : 2.51998
[1mStep[0m  [30/53], [94mLoss[0m : 2.30925
[1mStep[0m  [35/53], [94mLoss[0m : 2.20473
[1mStep[0m  [40/53], [94mLoss[0m : 2.46816
[1mStep[0m  [45/53], [94mLoss[0m : 2.45639
[1mStep[0m  [50/53], [94mLoss[0m : 2.37498

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18874
[1mStep[0m  [5/53], [94mLoss[0m : 2.30358
[1mStep[0m  [10/53], [94mLoss[0m : 2.33336
[1mStep[0m  [15/53], [94mLoss[0m : 2.37068
[1mStep[0m  [20/53], [94mLoss[0m : 2.37453
[1mStep[0m  [25/53], [94mLoss[0m : 2.16189
[1mStep[0m  [30/53], [94mLoss[0m : 2.46664
[1mStep[0m  [35/53], [94mLoss[0m : 2.23740
[1mStep[0m  [40/53], [94mLoss[0m : 2.34903
[1mStep[0m  [45/53], [94mLoss[0m : 2.24286
[1mStep[0m  [50/53], [94mLoss[0m : 2.38346

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.556, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24720
[1mStep[0m  [5/53], [94mLoss[0m : 2.21129
[1mStep[0m  [10/53], [94mLoss[0m : 2.42548
[1mStep[0m  [15/53], [94mLoss[0m : 2.28997
[1mStep[0m  [20/53], [94mLoss[0m : 2.01721
[1mStep[0m  [25/53], [94mLoss[0m : 2.15092
[1mStep[0m  [30/53], [94mLoss[0m : 2.32526
[1mStep[0m  [35/53], [94mLoss[0m : 2.21130
[1mStep[0m  [40/53], [94mLoss[0m : 2.18306
[1mStep[0m  [45/53], [94mLoss[0m : 2.37893
[1mStep[0m  [50/53], [94mLoss[0m : 2.30462

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.613, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09423
[1mStep[0m  [5/53], [94mLoss[0m : 1.99418
[1mStep[0m  [10/53], [94mLoss[0m : 2.24085
[1mStep[0m  [15/53], [94mLoss[0m : 2.37464
[1mStep[0m  [20/53], [94mLoss[0m : 2.22981
[1mStep[0m  [25/53], [94mLoss[0m : 2.04332
[1mStep[0m  [30/53], [94mLoss[0m : 2.23532
[1mStep[0m  [35/53], [94mLoss[0m : 2.19280
[1mStep[0m  [40/53], [94mLoss[0m : 2.09709
[1mStep[0m  [45/53], [94mLoss[0m : 2.42218
[1mStep[0m  [50/53], [94mLoss[0m : 2.18894

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09393
[1mStep[0m  [5/53], [94mLoss[0m : 2.49266
[1mStep[0m  [10/53], [94mLoss[0m : 2.03001
[1mStep[0m  [15/53], [94mLoss[0m : 2.40552
[1mStep[0m  [20/53], [94mLoss[0m : 2.04311
[1mStep[0m  [25/53], [94mLoss[0m : 2.30238
[1mStep[0m  [30/53], [94mLoss[0m : 2.21793
[1mStep[0m  [35/53], [94mLoss[0m : 2.15452
[1mStep[0m  [40/53], [94mLoss[0m : 2.10651
[1mStep[0m  [45/53], [94mLoss[0m : 2.08898
[1mStep[0m  [50/53], [94mLoss[0m : 1.84583

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.856, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.14722
[1mStep[0m  [5/53], [94mLoss[0m : 1.95960
[1mStep[0m  [10/53], [94mLoss[0m : 2.20444
[1mStep[0m  [15/53], [94mLoss[0m : 2.11001
[1mStep[0m  [20/53], [94mLoss[0m : 2.13455
[1mStep[0m  [25/53], [94mLoss[0m : 2.09981
[1mStep[0m  [30/53], [94mLoss[0m : 2.18595
[1mStep[0m  [35/53], [94mLoss[0m : 2.14004
[1mStep[0m  [40/53], [94mLoss[0m : 2.08510
[1mStep[0m  [45/53], [94mLoss[0m : 2.24156
[1mStep[0m  [50/53], [94mLoss[0m : 2.17999

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.710, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.85337
[1mStep[0m  [5/53], [94mLoss[0m : 1.90546
[1mStep[0m  [10/53], [94mLoss[0m : 1.89250
[1mStep[0m  [15/53], [94mLoss[0m : 2.22809
[1mStep[0m  [20/53], [94mLoss[0m : 2.10491
[1mStep[0m  [25/53], [94mLoss[0m : 2.15659
[1mStep[0m  [30/53], [94mLoss[0m : 2.05204
[1mStep[0m  [35/53], [94mLoss[0m : 2.03613
[1mStep[0m  [40/53], [94mLoss[0m : 2.13935
[1mStep[0m  [45/53], [94mLoss[0m : 2.00496
[1mStep[0m  [50/53], [94mLoss[0m : 2.26882

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.662, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17579
[1mStep[0m  [5/53], [94mLoss[0m : 2.01759
[1mStep[0m  [10/53], [94mLoss[0m : 2.06515
[1mStep[0m  [15/53], [94mLoss[0m : 2.12648
[1mStep[0m  [20/53], [94mLoss[0m : 2.03792
[1mStep[0m  [25/53], [94mLoss[0m : 2.18158
[1mStep[0m  [30/53], [94mLoss[0m : 1.99399
[1mStep[0m  [35/53], [94mLoss[0m : 2.16146
[1mStep[0m  [40/53], [94mLoss[0m : 2.08598
[1mStep[0m  [45/53], [94mLoss[0m : 2.33129
[1mStep[0m  [50/53], [94mLoss[0m : 1.97240

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.586, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.94570
[1mStep[0m  [5/53], [94mLoss[0m : 1.91794
[1mStep[0m  [10/53], [94mLoss[0m : 1.98563
[1mStep[0m  [15/53], [94mLoss[0m : 1.91261
[1mStep[0m  [20/53], [94mLoss[0m : 1.84421
[1mStep[0m  [25/53], [94mLoss[0m : 1.93946
[1mStep[0m  [30/53], [94mLoss[0m : 1.86053
[1mStep[0m  [35/53], [94mLoss[0m : 1.97576
[1mStep[0m  [40/53], [94mLoss[0m : 2.00921
[1mStep[0m  [45/53], [94mLoss[0m : 2.20412
[1mStep[0m  [50/53], [94mLoss[0m : 2.17705

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.814, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.02696
[1mStep[0m  [5/53], [94mLoss[0m : 1.85453
[1mStep[0m  [10/53], [94mLoss[0m : 1.96101
[1mStep[0m  [15/53], [94mLoss[0m : 1.99722
[1mStep[0m  [20/53], [94mLoss[0m : 1.86828
[1mStep[0m  [25/53], [94mLoss[0m : 1.77606
[1mStep[0m  [30/53], [94mLoss[0m : 1.88791
[1mStep[0m  [35/53], [94mLoss[0m : 2.10570
[1mStep[0m  [40/53], [94mLoss[0m : 1.93440
[1mStep[0m  [45/53], [94mLoss[0m : 1.98002
[1mStep[0m  [50/53], [94mLoss[0m : 1.88547

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.712, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.91368
[1mStep[0m  [5/53], [94mLoss[0m : 1.90080
[1mStep[0m  [10/53], [94mLoss[0m : 1.81941
[1mStep[0m  [15/53], [94mLoss[0m : 1.98000
[1mStep[0m  [20/53], [94mLoss[0m : 1.97023
[1mStep[0m  [25/53], [94mLoss[0m : 1.98782
[1mStep[0m  [30/53], [94mLoss[0m : 2.05471
[1mStep[0m  [35/53], [94mLoss[0m : 1.91378
[1mStep[0m  [40/53], [94mLoss[0m : 1.94530
[1mStep[0m  [45/53], [94mLoss[0m : 1.88359
[1mStep[0m  [50/53], [94mLoss[0m : 1.87051

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.552, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.91143
[1mStep[0m  [5/53], [94mLoss[0m : 2.07966
[1mStep[0m  [10/53], [94mLoss[0m : 2.02649
[1mStep[0m  [15/53], [94mLoss[0m : 1.86087
[1mStep[0m  [20/53], [94mLoss[0m : 1.81963
[1mStep[0m  [25/53], [94mLoss[0m : 2.03706
[1mStep[0m  [30/53], [94mLoss[0m : 1.97625
[1mStep[0m  [35/53], [94mLoss[0m : 1.92413
[1mStep[0m  [40/53], [94mLoss[0m : 1.75347
[1mStep[0m  [45/53], [94mLoss[0m : 1.82089
[1mStep[0m  [50/53], [94mLoss[0m : 1.85901

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.655, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71287
[1mStep[0m  [5/53], [94mLoss[0m : 1.87423
[1mStep[0m  [10/53], [94mLoss[0m : 1.92553
[1mStep[0m  [15/53], [94mLoss[0m : 2.05036
[1mStep[0m  [20/53], [94mLoss[0m : 2.07563
[1mStep[0m  [25/53], [94mLoss[0m : 1.67230
[1mStep[0m  [30/53], [94mLoss[0m : 1.97546
[1mStep[0m  [35/53], [94mLoss[0m : 1.97796
[1mStep[0m  [40/53], [94mLoss[0m : 1.94336
[1mStep[0m  [45/53], [94mLoss[0m : 1.82972
[1mStep[0m  [50/53], [94mLoss[0m : 1.90495

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.637, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71383
[1mStep[0m  [5/53], [94mLoss[0m : 1.93239
[1mStep[0m  [10/53], [94mLoss[0m : 1.81746
[1mStep[0m  [15/53], [94mLoss[0m : 1.86044
[1mStep[0m  [20/53], [94mLoss[0m : 1.88184
[1mStep[0m  [25/53], [94mLoss[0m : 1.70785
[1mStep[0m  [30/53], [94mLoss[0m : 1.80487
[1mStep[0m  [35/53], [94mLoss[0m : 1.86101
[1mStep[0m  [40/53], [94mLoss[0m : 1.89658
[1mStep[0m  [45/53], [94mLoss[0m : 1.76061
[1mStep[0m  [50/53], [94mLoss[0m : 1.89798

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.683, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78734
[1mStep[0m  [5/53], [94mLoss[0m : 1.60601
[1mStep[0m  [10/53], [94mLoss[0m : 2.01816
[1mStep[0m  [15/53], [94mLoss[0m : 1.94343
[1mStep[0m  [20/53], [94mLoss[0m : 1.69786
[1mStep[0m  [25/53], [94mLoss[0m : 1.79658
[1mStep[0m  [30/53], [94mLoss[0m : 1.93991
[1mStep[0m  [35/53], [94mLoss[0m : 1.78763
[1mStep[0m  [40/53], [94mLoss[0m : 1.85007
[1mStep[0m  [45/53], [94mLoss[0m : 1.80103
[1mStep[0m  [50/53], [94mLoss[0m : 1.76079

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.566, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.88640
[1mStep[0m  [5/53], [94mLoss[0m : 1.86260
[1mStep[0m  [10/53], [94mLoss[0m : 1.74531
[1mStep[0m  [15/53], [94mLoss[0m : 1.88723
[1mStep[0m  [20/53], [94mLoss[0m : 1.75579
[1mStep[0m  [25/53], [94mLoss[0m : 1.70068
[1mStep[0m  [30/53], [94mLoss[0m : 1.93760
[1mStep[0m  [35/53], [94mLoss[0m : 1.85287
[1mStep[0m  [40/53], [94mLoss[0m : 1.73152
[1mStep[0m  [45/53], [94mLoss[0m : 1.77799
[1mStep[0m  [50/53], [94mLoss[0m : 1.74806

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.765, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78729
[1mStep[0m  [5/53], [94mLoss[0m : 1.70216
[1mStep[0m  [10/53], [94mLoss[0m : 1.81170
[1mStep[0m  [15/53], [94mLoss[0m : 1.90865
[1mStep[0m  [20/53], [94mLoss[0m : 1.60122
[1mStep[0m  [25/53], [94mLoss[0m : 1.75680
[1mStep[0m  [30/53], [94mLoss[0m : 1.71667
[1mStep[0m  [35/53], [94mLoss[0m : 1.79111
[1mStep[0m  [40/53], [94mLoss[0m : 1.79105
[1mStep[0m  [45/53], [94mLoss[0m : 1.65791
[1mStep[0m  [50/53], [94mLoss[0m : 1.51833

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.692, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68943
[1mStep[0m  [5/53], [94mLoss[0m : 1.58398
[1mStep[0m  [10/53], [94mLoss[0m : 1.78505
[1mStep[0m  [15/53], [94mLoss[0m : 1.81347
[1mStep[0m  [20/53], [94mLoss[0m : 1.60737
[1mStep[0m  [25/53], [94mLoss[0m : 1.70397
[1mStep[0m  [30/53], [94mLoss[0m : 1.84155
[1mStep[0m  [35/53], [94mLoss[0m : 1.73384
[1mStep[0m  [40/53], [94mLoss[0m : 1.65674
[1mStep[0m  [45/53], [94mLoss[0m : 1.86891
[1mStep[0m  [50/53], [94mLoss[0m : 1.72393

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.562, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63132
[1mStep[0m  [5/53], [94mLoss[0m : 1.70797
[1mStep[0m  [10/53], [94mLoss[0m : 1.50335
[1mStep[0m  [15/53], [94mLoss[0m : 1.70350
[1mStep[0m  [20/53], [94mLoss[0m : 1.64352
[1mStep[0m  [25/53], [94mLoss[0m : 1.58767
[1mStep[0m  [30/53], [94mLoss[0m : 1.57908
[1mStep[0m  [35/53], [94mLoss[0m : 1.82773
[1mStep[0m  [40/53], [94mLoss[0m : 1.58145
[1mStep[0m  [45/53], [94mLoss[0m : 1.77830
[1mStep[0m  [50/53], [94mLoss[0m : 1.63076

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.589, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.50507
[1mStep[0m  [5/53], [94mLoss[0m : 1.43962
[1mStep[0m  [10/53], [94mLoss[0m : 1.68208
[1mStep[0m  [15/53], [94mLoss[0m : 1.52523
[1mStep[0m  [20/53], [94mLoss[0m : 1.75643
[1mStep[0m  [25/53], [94mLoss[0m : 1.62257
[1mStep[0m  [30/53], [94mLoss[0m : 1.64047
[1mStep[0m  [35/53], [94mLoss[0m : 1.77369
[1mStep[0m  [40/53], [94mLoss[0m : 1.75169
[1mStep[0m  [45/53], [94mLoss[0m : 1.72452
[1mStep[0m  [50/53], [94mLoss[0m : 1.72124

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.687, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46293
[1mStep[0m  [5/53], [94mLoss[0m : 1.70703
[1mStep[0m  [10/53], [94mLoss[0m : 1.76274
[1mStep[0m  [15/53], [94mLoss[0m : 1.58023
[1mStep[0m  [20/53], [94mLoss[0m : 1.73026
[1mStep[0m  [25/53], [94mLoss[0m : 1.68419
[1mStep[0m  [30/53], [94mLoss[0m : 1.52400
[1mStep[0m  [35/53], [94mLoss[0m : 1.61809
[1mStep[0m  [40/53], [94mLoss[0m : 1.62747
[1mStep[0m  [45/53], [94mLoss[0m : 1.67307
[1mStep[0m  [50/53], [94mLoss[0m : 1.70624

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.531, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.67431
[1mStep[0m  [5/53], [94mLoss[0m : 1.64390
[1mStep[0m  [10/53], [94mLoss[0m : 1.58417
[1mStep[0m  [15/53], [94mLoss[0m : 1.68093
[1mStep[0m  [20/53], [94mLoss[0m : 1.70734
[1mStep[0m  [25/53], [94mLoss[0m : 1.57677
[1mStep[0m  [30/53], [94mLoss[0m : 1.55092
[1mStep[0m  [35/53], [94mLoss[0m : 1.59213
[1mStep[0m  [40/53], [94mLoss[0m : 1.67658
[1mStep[0m  [45/53], [94mLoss[0m : 1.44765
[1mStep[0m  [50/53], [94mLoss[0m : 1.65613

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.621, [92mTest[0m: 2.470, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.51169
[1mStep[0m  [5/53], [94mLoss[0m : 1.55325
[1mStep[0m  [10/53], [94mLoss[0m : 1.49629
[1mStep[0m  [15/53], [94mLoss[0m : 1.56266
[1mStep[0m  [20/53], [94mLoss[0m : 1.54237
[1mStep[0m  [25/53], [94mLoss[0m : 1.68740
[1mStep[0m  [30/53], [94mLoss[0m : 1.48132
[1mStep[0m  [35/53], [94mLoss[0m : 1.52634
[1mStep[0m  [40/53], [94mLoss[0m : 1.59162
[1mStep[0m  [45/53], [94mLoss[0m : 1.62828
[1mStep[0m  [50/53], [94mLoss[0m : 1.52905

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.590, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.56112
[1mStep[0m  [5/53], [94mLoss[0m : 1.48760
[1mStep[0m  [10/53], [94mLoss[0m : 1.58171
[1mStep[0m  [15/53], [94mLoss[0m : 1.55957
[1mStep[0m  [20/53], [94mLoss[0m : 1.45936
[1mStep[0m  [25/53], [94mLoss[0m : 1.52772
[1mStep[0m  [30/53], [94mLoss[0m : 1.50799
[1mStep[0m  [35/53], [94mLoss[0m : 1.52004
[1mStep[0m  [40/53], [94mLoss[0m : 1.70293
[1mStep[0m  [45/53], [94mLoss[0m : 1.84621
[1mStep[0m  [50/53], [94mLoss[0m : 1.51009

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.576, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64147
[1mStep[0m  [5/53], [94mLoss[0m : 1.57696
[1mStep[0m  [10/53], [94mLoss[0m : 1.48078
[1mStep[0m  [15/53], [94mLoss[0m : 1.44096
[1mStep[0m  [20/53], [94mLoss[0m : 1.52075
[1mStep[0m  [25/53], [94mLoss[0m : 1.52828
[1mStep[0m  [30/53], [94mLoss[0m : 1.54058
[1mStep[0m  [35/53], [94mLoss[0m : 1.40391
[1mStep[0m  [40/53], [94mLoss[0m : 1.60955
[1mStep[0m  [45/53], [94mLoss[0m : 1.61667
[1mStep[0m  [50/53], [94mLoss[0m : 1.55201

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.582, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43409
[1mStep[0m  [5/53], [94mLoss[0m : 1.38432
[1mStep[0m  [10/53], [94mLoss[0m : 1.57480
[1mStep[0m  [15/53], [94mLoss[0m : 1.54188
[1mStep[0m  [20/53], [94mLoss[0m : 1.48829
[1mStep[0m  [25/53], [94mLoss[0m : 1.48964
[1mStep[0m  [30/53], [94mLoss[0m : 1.47283
[1mStep[0m  [35/53], [94mLoss[0m : 1.69790
[1mStep[0m  [40/53], [94mLoss[0m : 1.47681
[1mStep[0m  [45/53], [94mLoss[0m : 1.53313
[1mStep[0m  [50/53], [94mLoss[0m : 1.58125

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.706, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.728
====================================

Phase 2 - Evaluation MAE:  2.7279342137850247
MAE score P1      2.383455
MAE score P2      2.727934
loss              1.548773
learning_rate         0.01
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 11.26476
[1mStep[0m  [21/213], [94mLoss[0m : 6.43938
[1mStep[0m  [42/213], [94mLoss[0m : 3.25946
[1mStep[0m  [63/213], [94mLoss[0m : 2.82641
[1mStep[0m  [84/213], [94mLoss[0m : 2.52803
[1mStep[0m  [105/213], [94mLoss[0m : 2.57964
[1mStep[0m  [126/213], [94mLoss[0m : 2.75411
[1mStep[0m  [147/213], [94mLoss[0m : 2.50729
[1mStep[0m  [168/213], [94mLoss[0m : 2.73890
[1mStep[0m  [189/213], [94mLoss[0m : 2.27187
[1mStep[0m  [210/213], [94mLoss[0m : 2.29145

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.469, [92mTest[0m: 11.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40202
[1mStep[0m  [21/213], [94mLoss[0m : 2.42551
[1mStep[0m  [42/213], [94mLoss[0m : 3.08696
[1mStep[0m  [63/213], [94mLoss[0m : 2.44430
[1mStep[0m  [84/213], [94mLoss[0m : 2.38852
[1mStep[0m  [105/213], [94mLoss[0m : 2.67583
[1mStep[0m  [126/213], [94mLoss[0m : 2.96323
[1mStep[0m  [147/213], [94mLoss[0m : 3.01258
[1mStep[0m  [168/213], [94mLoss[0m : 2.98251
[1mStep[0m  [189/213], [94mLoss[0m : 2.33095
[1mStep[0m  [210/213], [94mLoss[0m : 2.31588

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.06103
[1mStep[0m  [21/213], [94mLoss[0m : 2.30728
[1mStep[0m  [42/213], [94mLoss[0m : 2.54054
[1mStep[0m  [63/213], [94mLoss[0m : 2.77481
[1mStep[0m  [84/213], [94mLoss[0m : 2.50860
[1mStep[0m  [105/213], [94mLoss[0m : 2.87087
[1mStep[0m  [126/213], [94mLoss[0m : 2.90253
[1mStep[0m  [147/213], [94mLoss[0m : 2.18103
[1mStep[0m  [168/213], [94mLoss[0m : 2.52535
[1mStep[0m  [189/213], [94mLoss[0m : 2.86224
[1mStep[0m  [210/213], [94mLoss[0m : 2.49697

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14376
[1mStep[0m  [21/213], [94mLoss[0m : 2.51025
[1mStep[0m  [42/213], [94mLoss[0m : 2.63529
[1mStep[0m  [63/213], [94mLoss[0m : 2.68455
[1mStep[0m  [84/213], [94mLoss[0m : 2.24665
[1mStep[0m  [105/213], [94mLoss[0m : 2.41749
[1mStep[0m  [126/213], [94mLoss[0m : 2.42099
[1mStep[0m  [147/213], [94mLoss[0m : 2.76953
[1mStep[0m  [168/213], [94mLoss[0m : 2.48426
[1mStep[0m  [189/213], [94mLoss[0m : 2.71212
[1mStep[0m  [210/213], [94mLoss[0m : 2.55669

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.60037
[1mStep[0m  [21/213], [94mLoss[0m : 2.42964
[1mStep[0m  [42/213], [94mLoss[0m : 2.75387
[1mStep[0m  [63/213], [94mLoss[0m : 2.92913
[1mStep[0m  [84/213], [94mLoss[0m : 2.17216
[1mStep[0m  [105/213], [94mLoss[0m : 2.85130
[1mStep[0m  [126/213], [94mLoss[0m : 2.84690
[1mStep[0m  [147/213], [94mLoss[0m : 2.76621
[1mStep[0m  [168/213], [94mLoss[0m : 2.44956
[1mStep[0m  [189/213], [94mLoss[0m : 2.41448
[1mStep[0m  [210/213], [94mLoss[0m : 2.96645

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42069
[1mStep[0m  [21/213], [94mLoss[0m : 2.53016
[1mStep[0m  [42/213], [94mLoss[0m : 2.52212
[1mStep[0m  [63/213], [94mLoss[0m : 2.82961
[1mStep[0m  [84/213], [94mLoss[0m : 2.58614
[1mStep[0m  [105/213], [94mLoss[0m : 2.32697
[1mStep[0m  [126/213], [94mLoss[0m : 2.64970
[1mStep[0m  [147/213], [94mLoss[0m : 2.70946
[1mStep[0m  [168/213], [94mLoss[0m : 2.43387
[1mStep[0m  [189/213], [94mLoss[0m : 2.69535
[1mStep[0m  [210/213], [94mLoss[0m : 2.41809

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.72496
[1mStep[0m  [21/213], [94mLoss[0m : 2.55586
[1mStep[0m  [42/213], [94mLoss[0m : 2.44797
[1mStep[0m  [63/213], [94mLoss[0m : 2.83263
[1mStep[0m  [84/213], [94mLoss[0m : 2.85323
[1mStep[0m  [105/213], [94mLoss[0m : 2.37104
[1mStep[0m  [126/213], [94mLoss[0m : 2.43158
[1mStep[0m  [147/213], [94mLoss[0m : 3.01730
[1mStep[0m  [168/213], [94mLoss[0m : 3.23326
[1mStep[0m  [189/213], [94mLoss[0m : 2.54130
[1mStep[0m  [210/213], [94mLoss[0m : 2.57003

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58044
[1mStep[0m  [21/213], [94mLoss[0m : 2.50456
[1mStep[0m  [42/213], [94mLoss[0m : 2.57119
[1mStep[0m  [63/213], [94mLoss[0m : 2.73123
[1mStep[0m  [84/213], [94mLoss[0m : 2.18745
[1mStep[0m  [105/213], [94mLoss[0m : 2.67551
[1mStep[0m  [126/213], [94mLoss[0m : 2.36232
[1mStep[0m  [147/213], [94mLoss[0m : 2.43963
[1mStep[0m  [168/213], [94mLoss[0m : 2.75463
[1mStep[0m  [189/213], [94mLoss[0m : 2.94512
[1mStep[0m  [210/213], [94mLoss[0m : 2.75608

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.74169
[1mStep[0m  [21/213], [94mLoss[0m : 2.49634
[1mStep[0m  [42/213], [94mLoss[0m : 2.27245
[1mStep[0m  [63/213], [94mLoss[0m : 2.68737
[1mStep[0m  [84/213], [94mLoss[0m : 2.67913
[1mStep[0m  [105/213], [94mLoss[0m : 2.96619
[1mStep[0m  [126/213], [94mLoss[0m : 2.65529
[1mStep[0m  [147/213], [94mLoss[0m : 2.43036
[1mStep[0m  [168/213], [94mLoss[0m : 2.64006
[1mStep[0m  [189/213], [94mLoss[0m : 2.42155
[1mStep[0m  [210/213], [94mLoss[0m : 2.68324

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.77735
[1mStep[0m  [21/213], [94mLoss[0m : 2.70101
[1mStep[0m  [42/213], [94mLoss[0m : 2.37673
[1mStep[0m  [63/213], [94mLoss[0m : 2.29319
[1mStep[0m  [84/213], [94mLoss[0m : 2.62616
[1mStep[0m  [105/213], [94mLoss[0m : 2.73137
[1mStep[0m  [126/213], [94mLoss[0m : 2.70686
[1mStep[0m  [147/213], [94mLoss[0m : 2.29602
[1mStep[0m  [168/213], [94mLoss[0m : 2.67840
[1mStep[0m  [189/213], [94mLoss[0m : 2.66411
[1mStep[0m  [210/213], [94mLoss[0m : 2.52891

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52593
[1mStep[0m  [21/213], [94mLoss[0m : 3.05131
[1mStep[0m  [42/213], [94mLoss[0m : 2.58961
[1mStep[0m  [63/213], [94mLoss[0m : 2.32806
[1mStep[0m  [84/213], [94mLoss[0m : 2.72660
[1mStep[0m  [105/213], [94mLoss[0m : 2.47349
[1mStep[0m  [126/213], [94mLoss[0m : 2.60735
[1mStep[0m  [147/213], [94mLoss[0m : 3.09408
[1mStep[0m  [168/213], [94mLoss[0m : 2.72089
[1mStep[0m  [189/213], [94mLoss[0m : 2.52564
[1mStep[0m  [210/213], [94mLoss[0m : 2.40899

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56240
[1mStep[0m  [21/213], [94mLoss[0m : 2.95762
[1mStep[0m  [42/213], [94mLoss[0m : 2.51901
[1mStep[0m  [63/213], [94mLoss[0m : 2.62547
[1mStep[0m  [84/213], [94mLoss[0m : 2.31487
[1mStep[0m  [105/213], [94mLoss[0m : 2.53472
[1mStep[0m  [126/213], [94mLoss[0m : 2.69284
[1mStep[0m  [147/213], [94mLoss[0m : 2.89767
[1mStep[0m  [168/213], [94mLoss[0m : 2.97006
[1mStep[0m  [189/213], [94mLoss[0m : 2.30855
[1mStep[0m  [210/213], [94mLoss[0m : 2.75560

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31842
[1mStep[0m  [21/213], [94mLoss[0m : 2.19969
[1mStep[0m  [42/213], [94mLoss[0m : 2.55247
[1mStep[0m  [63/213], [94mLoss[0m : 2.22506
[1mStep[0m  [84/213], [94mLoss[0m : 2.59817
[1mStep[0m  [105/213], [94mLoss[0m : 2.94784
[1mStep[0m  [126/213], [94mLoss[0m : 2.15676
[1mStep[0m  [147/213], [94mLoss[0m : 2.45194
[1mStep[0m  [168/213], [94mLoss[0m : 2.99247
[1mStep[0m  [189/213], [94mLoss[0m : 2.80861
[1mStep[0m  [210/213], [94mLoss[0m : 2.54643

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55539
[1mStep[0m  [21/213], [94mLoss[0m : 2.23780
[1mStep[0m  [42/213], [94mLoss[0m : 2.34810
[1mStep[0m  [63/213], [94mLoss[0m : 2.33247
[1mStep[0m  [84/213], [94mLoss[0m : 2.60441
[1mStep[0m  [105/213], [94mLoss[0m : 2.30500
[1mStep[0m  [126/213], [94mLoss[0m : 2.44852
[1mStep[0m  [147/213], [94mLoss[0m : 2.72351
[1mStep[0m  [168/213], [94mLoss[0m : 2.07750
[1mStep[0m  [189/213], [94mLoss[0m : 3.10438
[1mStep[0m  [210/213], [94mLoss[0m : 2.13648

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53414
[1mStep[0m  [21/213], [94mLoss[0m : 2.36814
[1mStep[0m  [42/213], [94mLoss[0m : 2.90942
[1mStep[0m  [63/213], [94mLoss[0m : 2.85148
[1mStep[0m  [84/213], [94mLoss[0m : 3.13883
[1mStep[0m  [105/213], [94mLoss[0m : 2.12431
[1mStep[0m  [126/213], [94mLoss[0m : 2.69625
[1mStep[0m  [147/213], [94mLoss[0m : 2.76290
[1mStep[0m  [168/213], [94mLoss[0m : 2.21123
[1mStep[0m  [189/213], [94mLoss[0m : 2.59314
[1mStep[0m  [210/213], [94mLoss[0m : 2.58548

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.81906
[1mStep[0m  [21/213], [94mLoss[0m : 2.68428
[1mStep[0m  [42/213], [94mLoss[0m : 2.62629
[1mStep[0m  [63/213], [94mLoss[0m : 2.63806
[1mStep[0m  [84/213], [94mLoss[0m : 3.32358
[1mStep[0m  [105/213], [94mLoss[0m : 2.27311
[1mStep[0m  [126/213], [94mLoss[0m : 3.03218
[1mStep[0m  [147/213], [94mLoss[0m : 2.96716
[1mStep[0m  [168/213], [94mLoss[0m : 2.80773
[1mStep[0m  [189/213], [94mLoss[0m : 2.88454
[1mStep[0m  [210/213], [94mLoss[0m : 2.86406

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.95268
[1mStep[0m  [21/213], [94mLoss[0m : 2.61667
[1mStep[0m  [42/213], [94mLoss[0m : 2.54685
[1mStep[0m  [63/213], [94mLoss[0m : 2.55519
[1mStep[0m  [84/213], [94mLoss[0m : 2.24632
[1mStep[0m  [105/213], [94mLoss[0m : 2.43788
[1mStep[0m  [126/213], [94mLoss[0m : 2.90527
[1mStep[0m  [147/213], [94mLoss[0m : 2.44334
[1mStep[0m  [168/213], [94mLoss[0m : 2.66444
[1mStep[0m  [189/213], [94mLoss[0m : 2.55311
[1mStep[0m  [210/213], [94mLoss[0m : 2.43507

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.78899
[1mStep[0m  [21/213], [94mLoss[0m : 2.61348
[1mStep[0m  [42/213], [94mLoss[0m : 2.37718
[1mStep[0m  [63/213], [94mLoss[0m : 2.72901
[1mStep[0m  [84/213], [94mLoss[0m : 2.69260
[1mStep[0m  [105/213], [94mLoss[0m : 2.26930
[1mStep[0m  [126/213], [94mLoss[0m : 2.30895
[1mStep[0m  [147/213], [94mLoss[0m : 2.30811
[1mStep[0m  [168/213], [94mLoss[0m : 2.46654
[1mStep[0m  [189/213], [94mLoss[0m : 2.27667
[1mStep[0m  [210/213], [94mLoss[0m : 2.05310

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65640
[1mStep[0m  [21/213], [94mLoss[0m : 2.80448
[1mStep[0m  [42/213], [94mLoss[0m : 2.41801
[1mStep[0m  [63/213], [94mLoss[0m : 2.53074
[1mStep[0m  [84/213], [94mLoss[0m : 2.53186
[1mStep[0m  [105/213], [94mLoss[0m : 2.69654
[1mStep[0m  [126/213], [94mLoss[0m : 2.18558
[1mStep[0m  [147/213], [94mLoss[0m : 2.01785
[1mStep[0m  [168/213], [94mLoss[0m : 2.19211
[1mStep[0m  [189/213], [94mLoss[0m : 2.31788
[1mStep[0m  [210/213], [94mLoss[0m : 2.28419

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40526
[1mStep[0m  [21/213], [94mLoss[0m : 2.36571
[1mStep[0m  [42/213], [94mLoss[0m : 2.94690
[1mStep[0m  [63/213], [94mLoss[0m : 2.53746
[1mStep[0m  [84/213], [94mLoss[0m : 2.68571
[1mStep[0m  [105/213], [94mLoss[0m : 2.70658
[1mStep[0m  [126/213], [94mLoss[0m : 2.53572
[1mStep[0m  [147/213], [94mLoss[0m : 2.32130
[1mStep[0m  [168/213], [94mLoss[0m : 2.94106
[1mStep[0m  [189/213], [94mLoss[0m : 2.73874
[1mStep[0m  [210/213], [94mLoss[0m : 2.36821

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35150
[1mStep[0m  [21/213], [94mLoss[0m : 2.60813
[1mStep[0m  [42/213], [94mLoss[0m : 2.67529
[1mStep[0m  [63/213], [94mLoss[0m : 2.42000
[1mStep[0m  [84/213], [94mLoss[0m : 2.53106
[1mStep[0m  [105/213], [94mLoss[0m : 2.59661
[1mStep[0m  [126/213], [94mLoss[0m : 2.69191
[1mStep[0m  [147/213], [94mLoss[0m : 2.61950
[1mStep[0m  [168/213], [94mLoss[0m : 2.56568
[1mStep[0m  [189/213], [94mLoss[0m : 2.62177
[1mStep[0m  [210/213], [94mLoss[0m : 2.45948

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.417, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.29885
[1mStep[0m  [21/213], [94mLoss[0m : 2.76494
[1mStep[0m  [42/213], [94mLoss[0m : 2.78918
[1mStep[0m  [63/213], [94mLoss[0m : 2.38285
[1mStep[0m  [84/213], [94mLoss[0m : 2.43044
[1mStep[0m  [105/213], [94mLoss[0m : 3.12274
[1mStep[0m  [126/213], [94mLoss[0m : 2.39236
[1mStep[0m  [147/213], [94mLoss[0m : 2.69025
[1mStep[0m  [168/213], [94mLoss[0m : 2.31383
[1mStep[0m  [189/213], [94mLoss[0m : 2.76258
[1mStep[0m  [210/213], [94mLoss[0m : 2.38346

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.413, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32732
[1mStep[0m  [21/213], [94mLoss[0m : 2.64940
[1mStep[0m  [42/213], [94mLoss[0m : 2.65536
[1mStep[0m  [63/213], [94mLoss[0m : 2.91264
[1mStep[0m  [84/213], [94mLoss[0m : 3.13994
[1mStep[0m  [105/213], [94mLoss[0m : 2.57541
[1mStep[0m  [126/213], [94mLoss[0m : 2.87591
[1mStep[0m  [147/213], [94mLoss[0m : 2.48839
[1mStep[0m  [168/213], [94mLoss[0m : 2.55782
[1mStep[0m  [189/213], [94mLoss[0m : 2.78039
[1mStep[0m  [210/213], [94mLoss[0m : 2.40388

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.420, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.57737
[1mStep[0m  [21/213], [94mLoss[0m : 2.50408
[1mStep[0m  [42/213], [94mLoss[0m : 2.68692
[1mStep[0m  [63/213], [94mLoss[0m : 2.94958
[1mStep[0m  [84/213], [94mLoss[0m : 2.79416
[1mStep[0m  [105/213], [94mLoss[0m : 2.62166
[1mStep[0m  [126/213], [94mLoss[0m : 2.61770
[1mStep[0m  [147/213], [94mLoss[0m : 2.42754
[1mStep[0m  [168/213], [94mLoss[0m : 2.73663
[1mStep[0m  [189/213], [94mLoss[0m : 2.45249
[1mStep[0m  [210/213], [94mLoss[0m : 2.29700

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.416, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.64582
[1mStep[0m  [21/213], [94mLoss[0m : 2.64062
[1mStep[0m  [42/213], [94mLoss[0m : 3.01725
[1mStep[0m  [63/213], [94mLoss[0m : 2.49071
[1mStep[0m  [84/213], [94mLoss[0m : 2.34137
[1mStep[0m  [105/213], [94mLoss[0m : 2.42491
[1mStep[0m  [126/213], [94mLoss[0m : 2.92516
[1mStep[0m  [147/213], [94mLoss[0m : 2.58294
[1mStep[0m  [168/213], [94mLoss[0m : 2.49840
[1mStep[0m  [189/213], [94mLoss[0m : 2.30266
[1mStep[0m  [210/213], [94mLoss[0m : 2.75999

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.433, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70522
[1mStep[0m  [21/213], [94mLoss[0m : 2.44023
[1mStep[0m  [42/213], [94mLoss[0m : 2.97445
[1mStep[0m  [63/213], [94mLoss[0m : 2.75795
[1mStep[0m  [84/213], [94mLoss[0m : 3.04522
[1mStep[0m  [105/213], [94mLoss[0m : 2.74005
[1mStep[0m  [126/213], [94mLoss[0m : 2.49405
[1mStep[0m  [147/213], [94mLoss[0m : 2.42039
[1mStep[0m  [168/213], [94mLoss[0m : 2.28295
[1mStep[0m  [189/213], [94mLoss[0m : 2.30577
[1mStep[0m  [210/213], [94mLoss[0m : 2.26251

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.417, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40838
[1mStep[0m  [21/213], [94mLoss[0m : 2.62086
[1mStep[0m  [42/213], [94mLoss[0m : 2.44026
[1mStep[0m  [63/213], [94mLoss[0m : 2.70378
[1mStep[0m  [84/213], [94mLoss[0m : 2.45986
[1mStep[0m  [105/213], [94mLoss[0m : 2.65191
[1mStep[0m  [126/213], [94mLoss[0m : 2.77426
[1mStep[0m  [147/213], [94mLoss[0m : 2.52582
[1mStep[0m  [168/213], [94mLoss[0m : 2.68385
[1mStep[0m  [189/213], [94mLoss[0m : 2.96250
[1mStep[0m  [210/213], [94mLoss[0m : 2.34753

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.428, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.75833
[1mStep[0m  [21/213], [94mLoss[0m : 2.75374
[1mStep[0m  [42/213], [94mLoss[0m : 2.21758
[1mStep[0m  [63/213], [94mLoss[0m : 3.11568
[1mStep[0m  [84/213], [94mLoss[0m : 2.47182
[1mStep[0m  [105/213], [94mLoss[0m : 2.52924
[1mStep[0m  [126/213], [94mLoss[0m : 2.88144
[1mStep[0m  [147/213], [94mLoss[0m : 2.18826
[1mStep[0m  [168/213], [94mLoss[0m : 2.02226
[1mStep[0m  [189/213], [94mLoss[0m : 2.56290
[1mStep[0m  [210/213], [94mLoss[0m : 2.68703

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.418, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56565
[1mStep[0m  [21/213], [94mLoss[0m : 2.70634
[1mStep[0m  [42/213], [94mLoss[0m : 2.28469
[1mStep[0m  [63/213], [94mLoss[0m : 2.18061
[1mStep[0m  [84/213], [94mLoss[0m : 2.42859
[1mStep[0m  [105/213], [94mLoss[0m : 2.29797
[1mStep[0m  [126/213], [94mLoss[0m : 2.48085
[1mStep[0m  [147/213], [94mLoss[0m : 2.72401
[1mStep[0m  [168/213], [94mLoss[0m : 2.37621
[1mStep[0m  [189/213], [94mLoss[0m : 2.61445
[1mStep[0m  [210/213], [94mLoss[0m : 3.16469

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.57715
[1mStep[0m  [21/213], [94mLoss[0m : 2.41580
[1mStep[0m  [42/213], [94mLoss[0m : 2.70189
[1mStep[0m  [63/213], [94mLoss[0m : 2.52992
[1mStep[0m  [84/213], [94mLoss[0m : 3.56312
[1mStep[0m  [105/213], [94mLoss[0m : 2.82910
[1mStep[0m  [126/213], [94mLoss[0m : 2.48135
[1mStep[0m  [147/213], [94mLoss[0m : 2.41193
[1mStep[0m  [168/213], [94mLoss[0m : 2.48429
[1mStep[0m  [189/213], [94mLoss[0m : 2.79541
[1mStep[0m  [210/213], [94mLoss[0m : 2.56595

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.424, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.434
====================================

Phase 1 - Evaluation MAE:  2.4340898597015523
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 2.93044
[1mStep[0m  [21/213], [94mLoss[0m : 2.32375
[1mStep[0m  [42/213], [94mLoss[0m : 2.36582
[1mStep[0m  [63/213], [94mLoss[0m : 2.85270
[1mStep[0m  [84/213], [94mLoss[0m : 2.94366
[1mStep[0m  [105/213], [94mLoss[0m : 2.37809
[1mStep[0m  [126/213], [94mLoss[0m : 2.63518
[1mStep[0m  [147/213], [94mLoss[0m : 2.38583
[1mStep[0m  [168/213], [94mLoss[0m : 3.04570
[1mStep[0m  [189/213], [94mLoss[0m : 3.01344
[1mStep[0m  [210/213], [94mLoss[0m : 2.14206

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42107
[1mStep[0m  [21/213], [94mLoss[0m : 2.68414
[1mStep[0m  [42/213], [94mLoss[0m : 2.57736
[1mStep[0m  [63/213], [94mLoss[0m : 1.88772
[1mStep[0m  [84/213], [94mLoss[0m : 2.65208
[1mStep[0m  [105/213], [94mLoss[0m : 2.62312
[1mStep[0m  [126/213], [94mLoss[0m : 2.50686
[1mStep[0m  [147/213], [94mLoss[0m : 2.58636
[1mStep[0m  [168/213], [94mLoss[0m : 2.52737
[1mStep[0m  [189/213], [94mLoss[0m : 2.47196
[1mStep[0m  [210/213], [94mLoss[0m : 2.60370

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.00822
[1mStep[0m  [21/213], [94mLoss[0m : 2.36189
[1mStep[0m  [42/213], [94mLoss[0m : 2.38350
[1mStep[0m  [63/213], [94mLoss[0m : 2.79243
[1mStep[0m  [84/213], [94mLoss[0m : 2.75611
[1mStep[0m  [105/213], [94mLoss[0m : 2.75967
[1mStep[0m  [126/213], [94mLoss[0m : 2.16274
[1mStep[0m  [147/213], [94mLoss[0m : 2.49044
[1mStep[0m  [168/213], [94mLoss[0m : 2.35458
[1mStep[0m  [189/213], [94mLoss[0m : 2.17484
[1mStep[0m  [210/213], [94mLoss[0m : 2.77861

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.57466
[1mStep[0m  [21/213], [94mLoss[0m : 2.29262
[1mStep[0m  [42/213], [94mLoss[0m : 2.28306
[1mStep[0m  [63/213], [94mLoss[0m : 2.24724
[1mStep[0m  [84/213], [94mLoss[0m : 2.75951
[1mStep[0m  [105/213], [94mLoss[0m : 2.18325
[1mStep[0m  [126/213], [94mLoss[0m : 2.43061
[1mStep[0m  [147/213], [94mLoss[0m : 2.53907
[1mStep[0m  [168/213], [94mLoss[0m : 2.55004
[1mStep[0m  [189/213], [94mLoss[0m : 2.18401
[1mStep[0m  [210/213], [94mLoss[0m : 2.21187

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.49666
[1mStep[0m  [21/213], [94mLoss[0m : 2.22717
[1mStep[0m  [42/213], [94mLoss[0m : 2.31110
[1mStep[0m  [63/213], [94mLoss[0m : 2.23178
[1mStep[0m  [84/213], [94mLoss[0m : 2.18952
[1mStep[0m  [105/213], [94mLoss[0m : 2.55351
[1mStep[0m  [126/213], [94mLoss[0m : 2.69155
[1mStep[0m  [147/213], [94mLoss[0m : 2.49101
[1mStep[0m  [168/213], [94mLoss[0m : 3.17699
[1mStep[0m  [189/213], [94mLoss[0m : 2.73785
[1mStep[0m  [210/213], [94mLoss[0m : 2.75873

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46214
[1mStep[0m  [21/213], [94mLoss[0m : 2.08728
[1mStep[0m  [42/213], [94mLoss[0m : 1.80230
[1mStep[0m  [63/213], [94mLoss[0m : 2.36148
[1mStep[0m  [84/213], [94mLoss[0m : 2.16747
[1mStep[0m  [105/213], [94mLoss[0m : 2.57461
[1mStep[0m  [126/213], [94mLoss[0m : 2.33067
[1mStep[0m  [147/213], [94mLoss[0m : 2.59124
[1mStep[0m  [168/213], [94mLoss[0m : 2.66912
[1mStep[0m  [189/213], [94mLoss[0m : 2.38228
[1mStep[0m  [210/213], [94mLoss[0m : 2.36880

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21959
[1mStep[0m  [21/213], [94mLoss[0m : 2.27474
[1mStep[0m  [42/213], [94mLoss[0m : 2.39390
[1mStep[0m  [63/213], [94mLoss[0m : 2.47084
[1mStep[0m  [84/213], [94mLoss[0m : 2.09334
[1mStep[0m  [105/213], [94mLoss[0m : 2.07299
[1mStep[0m  [126/213], [94mLoss[0m : 2.23348
[1mStep[0m  [147/213], [94mLoss[0m : 2.50513
[1mStep[0m  [168/213], [94mLoss[0m : 2.19149
[1mStep[0m  [189/213], [94mLoss[0m : 2.52840
[1mStep[0m  [210/213], [94mLoss[0m : 2.57870

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18506
[1mStep[0m  [21/213], [94mLoss[0m : 1.84677
[1mStep[0m  [42/213], [94mLoss[0m : 2.23463
[1mStep[0m  [63/213], [94mLoss[0m : 2.42774
[1mStep[0m  [84/213], [94mLoss[0m : 2.38427
[1mStep[0m  [105/213], [94mLoss[0m : 2.14496
[1mStep[0m  [126/213], [94mLoss[0m : 2.03404
[1mStep[0m  [147/213], [94mLoss[0m : 1.98309
[1mStep[0m  [168/213], [94mLoss[0m : 2.64196
[1mStep[0m  [189/213], [94mLoss[0m : 2.12191
[1mStep[0m  [210/213], [94mLoss[0m : 3.01260

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39633
[1mStep[0m  [21/213], [94mLoss[0m : 2.00951
[1mStep[0m  [42/213], [94mLoss[0m : 2.08770
[1mStep[0m  [63/213], [94mLoss[0m : 2.23347
[1mStep[0m  [84/213], [94mLoss[0m : 1.96557
[1mStep[0m  [105/213], [94mLoss[0m : 2.48280
[1mStep[0m  [126/213], [94mLoss[0m : 2.37010
[1mStep[0m  [147/213], [94mLoss[0m : 1.91664
[1mStep[0m  [168/213], [94mLoss[0m : 2.05152
[1mStep[0m  [189/213], [94mLoss[0m : 2.06503
[1mStep[0m  [210/213], [94mLoss[0m : 2.58737

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.01769
[1mStep[0m  [21/213], [94mLoss[0m : 2.27837
[1mStep[0m  [42/213], [94mLoss[0m : 2.24604
[1mStep[0m  [63/213], [94mLoss[0m : 2.51633
[1mStep[0m  [84/213], [94mLoss[0m : 1.99402
[1mStep[0m  [105/213], [94mLoss[0m : 2.12044
[1mStep[0m  [126/213], [94mLoss[0m : 2.01082
[1mStep[0m  [147/213], [94mLoss[0m : 2.10373
[1mStep[0m  [168/213], [94mLoss[0m : 2.14710
[1mStep[0m  [189/213], [94mLoss[0m : 2.59793
[1mStep[0m  [210/213], [94mLoss[0m : 2.51650

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.13856
[1mStep[0m  [21/213], [94mLoss[0m : 2.43703
[1mStep[0m  [42/213], [94mLoss[0m : 2.08061
[1mStep[0m  [63/213], [94mLoss[0m : 1.81725
[1mStep[0m  [84/213], [94mLoss[0m : 2.29424
[1mStep[0m  [105/213], [94mLoss[0m : 2.43641
[1mStep[0m  [126/213], [94mLoss[0m : 2.55141
[1mStep[0m  [147/213], [94mLoss[0m : 2.78321
[1mStep[0m  [168/213], [94mLoss[0m : 2.53928
[1mStep[0m  [189/213], [94mLoss[0m : 2.23455
[1mStep[0m  [210/213], [94mLoss[0m : 2.33000

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.15793
[1mStep[0m  [21/213], [94mLoss[0m : 1.87650
[1mStep[0m  [42/213], [94mLoss[0m : 1.86999
[1mStep[0m  [63/213], [94mLoss[0m : 2.20982
[1mStep[0m  [84/213], [94mLoss[0m : 2.15737
[1mStep[0m  [105/213], [94mLoss[0m : 2.04916
[1mStep[0m  [126/213], [94mLoss[0m : 2.74103
[1mStep[0m  [147/213], [94mLoss[0m : 1.77773
[1mStep[0m  [168/213], [94mLoss[0m : 2.56794
[1mStep[0m  [189/213], [94mLoss[0m : 2.12216
[1mStep[0m  [210/213], [94mLoss[0m : 2.46999

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40826
[1mStep[0m  [21/213], [94mLoss[0m : 1.69314
[1mStep[0m  [42/213], [94mLoss[0m : 2.22323
[1mStep[0m  [63/213], [94mLoss[0m : 2.00364
[1mStep[0m  [84/213], [94mLoss[0m : 2.39557
[1mStep[0m  [105/213], [94mLoss[0m : 2.48009
[1mStep[0m  [126/213], [94mLoss[0m : 2.11029
[1mStep[0m  [147/213], [94mLoss[0m : 2.05546
[1mStep[0m  [168/213], [94mLoss[0m : 2.19413
[1mStep[0m  [189/213], [94mLoss[0m : 2.09505
[1mStep[0m  [210/213], [94mLoss[0m : 2.04611

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23540
[1mStep[0m  [21/213], [94mLoss[0m : 2.31113
[1mStep[0m  [42/213], [94mLoss[0m : 1.65489
[1mStep[0m  [63/213], [94mLoss[0m : 2.39207
[1mStep[0m  [84/213], [94mLoss[0m : 2.14148
[1mStep[0m  [105/213], [94mLoss[0m : 2.06547
[1mStep[0m  [126/213], [94mLoss[0m : 2.22609
[1mStep[0m  [147/213], [94mLoss[0m : 2.54666
[1mStep[0m  [168/213], [94mLoss[0m : 2.53149
[1mStep[0m  [189/213], [94mLoss[0m : 2.31453
[1mStep[0m  [210/213], [94mLoss[0m : 1.94595

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14081
[1mStep[0m  [21/213], [94mLoss[0m : 1.76070
[1mStep[0m  [42/213], [94mLoss[0m : 2.10514
[1mStep[0m  [63/213], [94mLoss[0m : 2.14336
[1mStep[0m  [84/213], [94mLoss[0m : 2.31309
[1mStep[0m  [105/213], [94mLoss[0m : 2.07015
[1mStep[0m  [126/213], [94mLoss[0m : 2.45307
[1mStep[0m  [147/213], [94mLoss[0m : 2.21872
[1mStep[0m  [168/213], [94mLoss[0m : 2.07583
[1mStep[0m  [189/213], [94mLoss[0m : 1.99902
[1mStep[0m  [210/213], [94mLoss[0m : 1.98390

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.57921
[1mStep[0m  [21/213], [94mLoss[0m : 2.10624
[1mStep[0m  [42/213], [94mLoss[0m : 2.24110
[1mStep[0m  [63/213], [94mLoss[0m : 2.08786
[1mStep[0m  [84/213], [94mLoss[0m : 1.89764
[1mStep[0m  [105/213], [94mLoss[0m : 2.51010
[1mStep[0m  [126/213], [94mLoss[0m : 2.27399
[1mStep[0m  [147/213], [94mLoss[0m : 2.24293
[1mStep[0m  [168/213], [94mLoss[0m : 2.26732
[1mStep[0m  [189/213], [94mLoss[0m : 2.29436
[1mStep[0m  [210/213], [94mLoss[0m : 2.24510

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14655
[1mStep[0m  [21/213], [94mLoss[0m : 1.95535
[1mStep[0m  [42/213], [94mLoss[0m : 2.17182
[1mStep[0m  [63/213], [94mLoss[0m : 2.38656
[1mStep[0m  [84/213], [94mLoss[0m : 2.09709
[1mStep[0m  [105/213], [94mLoss[0m : 2.38017
[1mStep[0m  [126/213], [94mLoss[0m : 2.38501
[1mStep[0m  [147/213], [94mLoss[0m : 2.67644
[1mStep[0m  [168/213], [94mLoss[0m : 2.31160
[1mStep[0m  [189/213], [94mLoss[0m : 2.68145
[1mStep[0m  [210/213], [94mLoss[0m : 2.37980

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89320
[1mStep[0m  [21/213], [94mLoss[0m : 2.30114
[1mStep[0m  [42/213], [94mLoss[0m : 2.07772
[1mStep[0m  [63/213], [94mLoss[0m : 2.13861
[1mStep[0m  [84/213], [94mLoss[0m : 1.81052
[1mStep[0m  [105/213], [94mLoss[0m : 2.27849
[1mStep[0m  [126/213], [94mLoss[0m : 2.41599
[1mStep[0m  [147/213], [94mLoss[0m : 2.16689
[1mStep[0m  [168/213], [94mLoss[0m : 2.48729
[1mStep[0m  [189/213], [94mLoss[0m : 2.21474
[1mStep[0m  [210/213], [94mLoss[0m : 2.31105

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21300
[1mStep[0m  [21/213], [94mLoss[0m : 2.45569
[1mStep[0m  [42/213], [94mLoss[0m : 2.28475
[1mStep[0m  [63/213], [94mLoss[0m : 2.17368
[1mStep[0m  [84/213], [94mLoss[0m : 2.27806
[1mStep[0m  [105/213], [94mLoss[0m : 2.34342
[1mStep[0m  [126/213], [94mLoss[0m : 2.10957
[1mStep[0m  [147/213], [94mLoss[0m : 2.24920
[1mStep[0m  [168/213], [94mLoss[0m : 1.89356
[1mStep[0m  [189/213], [94mLoss[0m : 2.13108
[1mStep[0m  [210/213], [94mLoss[0m : 2.48177

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.177, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02999
[1mStep[0m  [21/213], [94mLoss[0m : 1.78409
[1mStep[0m  [42/213], [94mLoss[0m : 2.20130
[1mStep[0m  [63/213], [94mLoss[0m : 1.91404
[1mStep[0m  [84/213], [94mLoss[0m : 1.92259
[1mStep[0m  [105/213], [94mLoss[0m : 2.30638
[1mStep[0m  [126/213], [94mLoss[0m : 2.36870
[1mStep[0m  [147/213], [94mLoss[0m : 2.29631
[1mStep[0m  [168/213], [94mLoss[0m : 2.26056
[1mStep[0m  [189/213], [94mLoss[0m : 2.70524
[1mStep[0m  [210/213], [94mLoss[0m : 2.26802

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.449, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55132
[1mStep[0m  [21/213], [94mLoss[0m : 2.00439
[1mStep[0m  [42/213], [94mLoss[0m : 2.17904
[1mStep[0m  [63/213], [94mLoss[0m : 2.29631
[1mStep[0m  [84/213], [94mLoss[0m : 2.46423
[1mStep[0m  [105/213], [94mLoss[0m : 2.25683
[1mStep[0m  [126/213], [94mLoss[0m : 1.92163
[1mStep[0m  [147/213], [94mLoss[0m : 2.23021
[1mStep[0m  [168/213], [94mLoss[0m : 1.75890
[1mStep[0m  [189/213], [94mLoss[0m : 2.45848
[1mStep[0m  [210/213], [94mLoss[0m : 1.94673

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38734
[1mStep[0m  [21/213], [94mLoss[0m : 2.16215
[1mStep[0m  [42/213], [94mLoss[0m : 1.89828
[1mStep[0m  [63/213], [94mLoss[0m : 2.07468
[1mStep[0m  [84/213], [94mLoss[0m : 1.80712
[1mStep[0m  [105/213], [94mLoss[0m : 2.28762
[1mStep[0m  [126/213], [94mLoss[0m : 2.12275
[1mStep[0m  [147/213], [94mLoss[0m : 1.95448
[1mStep[0m  [168/213], [94mLoss[0m : 2.04714
[1mStep[0m  [189/213], [94mLoss[0m : 2.10621
[1mStep[0m  [210/213], [94mLoss[0m : 2.06594

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.439, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.93293
[1mStep[0m  [21/213], [94mLoss[0m : 1.86861
[1mStep[0m  [42/213], [94mLoss[0m : 2.08235
[1mStep[0m  [63/213], [94mLoss[0m : 1.91137
[1mStep[0m  [84/213], [94mLoss[0m : 2.02609
[1mStep[0m  [105/213], [94mLoss[0m : 1.95642
[1mStep[0m  [126/213], [94mLoss[0m : 2.36305
[1mStep[0m  [147/213], [94mLoss[0m : 2.15130
[1mStep[0m  [168/213], [94mLoss[0m : 2.18391
[1mStep[0m  [189/213], [94mLoss[0m : 2.04908
[1mStep[0m  [210/213], [94mLoss[0m : 2.54620

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02413
[1mStep[0m  [21/213], [94mLoss[0m : 1.98895
[1mStep[0m  [42/213], [94mLoss[0m : 1.93952
[1mStep[0m  [63/213], [94mLoss[0m : 2.26883
[1mStep[0m  [84/213], [94mLoss[0m : 1.73664
[1mStep[0m  [105/213], [94mLoss[0m : 1.78096
[1mStep[0m  [126/213], [94mLoss[0m : 1.90860
[1mStep[0m  [147/213], [94mLoss[0m : 2.07148
[1mStep[0m  [168/213], [94mLoss[0m : 2.06468
[1mStep[0m  [189/213], [94mLoss[0m : 2.21698
[1mStep[0m  [210/213], [94mLoss[0m : 2.24277

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.460, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.29255
[1mStep[0m  [21/213], [94mLoss[0m : 2.16379
[1mStep[0m  [42/213], [94mLoss[0m : 2.10888
[1mStep[0m  [63/213], [94mLoss[0m : 2.08747
[1mStep[0m  [84/213], [94mLoss[0m : 1.95124
[1mStep[0m  [105/213], [94mLoss[0m : 2.22153
[1mStep[0m  [126/213], [94mLoss[0m : 2.21910
[1mStep[0m  [147/213], [94mLoss[0m : 1.85913
[1mStep[0m  [168/213], [94mLoss[0m : 2.08461
[1mStep[0m  [189/213], [94mLoss[0m : 2.45723
[1mStep[0m  [210/213], [94mLoss[0m : 1.99404

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.465, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.74322
[1mStep[0m  [21/213], [94mLoss[0m : 1.92287
[1mStep[0m  [42/213], [94mLoss[0m : 1.85950
[1mStep[0m  [63/213], [94mLoss[0m : 1.96265
[1mStep[0m  [84/213], [94mLoss[0m : 2.39893
[1mStep[0m  [105/213], [94mLoss[0m : 1.99342
[1mStep[0m  [126/213], [94mLoss[0m : 1.83440
[1mStep[0m  [147/213], [94mLoss[0m : 2.44688
[1mStep[0m  [168/213], [94mLoss[0m : 2.08636
[1mStep[0m  [189/213], [94mLoss[0m : 2.36823
[1mStep[0m  [210/213], [94mLoss[0m : 2.52588

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.464, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.05299
[1mStep[0m  [21/213], [94mLoss[0m : 2.42638
[1mStep[0m  [42/213], [94mLoss[0m : 2.10354
[1mStep[0m  [63/213], [94mLoss[0m : 2.04710
[1mStep[0m  [84/213], [94mLoss[0m : 2.00191
[1mStep[0m  [105/213], [94mLoss[0m : 1.99087
[1mStep[0m  [126/213], [94mLoss[0m : 2.62676
[1mStep[0m  [147/213], [94mLoss[0m : 1.99275
[1mStep[0m  [168/213], [94mLoss[0m : 2.19710
[1mStep[0m  [189/213], [94mLoss[0m : 2.51115
[1mStep[0m  [210/213], [94mLoss[0m : 1.61294

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.454, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.93310
[1mStep[0m  [21/213], [94mLoss[0m : 2.38664
[1mStep[0m  [42/213], [94mLoss[0m : 2.23126
[1mStep[0m  [63/213], [94mLoss[0m : 2.10480
[1mStep[0m  [84/213], [94mLoss[0m : 2.00789
[1mStep[0m  [105/213], [94mLoss[0m : 1.92240
[1mStep[0m  [126/213], [94mLoss[0m : 1.99766
[1mStep[0m  [147/213], [94mLoss[0m : 2.02939
[1mStep[0m  [168/213], [94mLoss[0m : 2.29365
[1mStep[0m  [189/213], [94mLoss[0m : 1.91937
[1mStep[0m  [210/213], [94mLoss[0m : 2.10569

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.453, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.83048
[1mStep[0m  [21/213], [94mLoss[0m : 1.79657
[1mStep[0m  [42/213], [94mLoss[0m : 1.72144
[1mStep[0m  [63/213], [94mLoss[0m : 1.68307
[1mStep[0m  [84/213], [94mLoss[0m : 2.26286
[1mStep[0m  [105/213], [94mLoss[0m : 1.85156
[1mStep[0m  [126/213], [94mLoss[0m : 1.97027
[1mStep[0m  [147/213], [94mLoss[0m : 1.93556
[1mStep[0m  [168/213], [94mLoss[0m : 2.24398
[1mStep[0m  [189/213], [94mLoss[0m : 1.85146
[1mStep[0m  [210/213], [94mLoss[0m : 2.19461

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.077, [92mTest[0m: 2.469, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.98881
[1mStep[0m  [21/213], [94mLoss[0m : 2.01881
[1mStep[0m  [42/213], [94mLoss[0m : 2.02411
[1mStep[0m  [63/213], [94mLoss[0m : 2.16233
[1mStep[0m  [84/213], [94mLoss[0m : 1.86349
[1mStep[0m  [105/213], [94mLoss[0m : 2.25317
[1mStep[0m  [126/213], [94mLoss[0m : 2.07052
[1mStep[0m  [147/213], [94mLoss[0m : 2.50636
[1mStep[0m  [168/213], [94mLoss[0m : 1.86910
[1mStep[0m  [189/213], [94mLoss[0m : 1.90975
[1mStep[0m  [210/213], [94mLoss[0m : 2.42145

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.432, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.477
====================================

Phase 2 - Evaluation MAE:  2.4768086604352266
MAE score P1        2.43409
MAE score P2       2.476809
loss               2.077247
learning_rate          0.01
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 11.24075
[1mStep[0m  [5/53], [94mLoss[0m : 7.44301
[1mStep[0m  [10/53], [94mLoss[0m : 4.00297
[1mStep[0m  [15/53], [94mLoss[0m : 3.02797
[1mStep[0m  [20/53], [94mLoss[0m : 2.80430
[1mStep[0m  [25/53], [94mLoss[0m : 2.69042
[1mStep[0m  [30/53], [94mLoss[0m : 2.57832
[1mStep[0m  [35/53], [94mLoss[0m : 2.93359
[1mStep[0m  [40/53], [94mLoss[0m : 2.60039
[1mStep[0m  [45/53], [94mLoss[0m : 2.59126
[1mStep[0m  [50/53], [94mLoss[0m : 2.50544

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.735, [92mTest[0m: 10.952, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43924
[1mStep[0m  [5/53], [94mLoss[0m : 2.37678
[1mStep[0m  [10/53], [94mLoss[0m : 2.33774
[1mStep[0m  [15/53], [94mLoss[0m : 2.44661
[1mStep[0m  [20/53], [94mLoss[0m : 2.56379
[1mStep[0m  [25/53], [94mLoss[0m : 2.55403
[1mStep[0m  [30/53], [94mLoss[0m : 2.58066
[1mStep[0m  [35/53], [94mLoss[0m : 2.56888
[1mStep[0m  [40/53], [94mLoss[0m : 2.47538
[1mStep[0m  [45/53], [94mLoss[0m : 2.46609
[1mStep[0m  [50/53], [94mLoss[0m : 2.47881

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.523, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48063
[1mStep[0m  [5/53], [94mLoss[0m : 2.34389
[1mStep[0m  [10/53], [94mLoss[0m : 2.61148
[1mStep[0m  [15/53], [94mLoss[0m : 2.60502
[1mStep[0m  [20/53], [94mLoss[0m : 2.66882
[1mStep[0m  [25/53], [94mLoss[0m : 2.48021
[1mStep[0m  [30/53], [94mLoss[0m : 2.31081
[1mStep[0m  [35/53], [94mLoss[0m : 2.49126
[1mStep[0m  [40/53], [94mLoss[0m : 2.52515
[1mStep[0m  [45/53], [94mLoss[0m : 2.45802
[1mStep[0m  [50/53], [94mLoss[0m : 2.66076

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67306
[1mStep[0m  [5/53], [94mLoss[0m : 2.64433
[1mStep[0m  [10/53], [94mLoss[0m : 2.60613
[1mStep[0m  [15/53], [94mLoss[0m : 2.53240
[1mStep[0m  [20/53], [94mLoss[0m : 2.34966
[1mStep[0m  [25/53], [94mLoss[0m : 2.33335
[1mStep[0m  [30/53], [94mLoss[0m : 2.28457
[1mStep[0m  [35/53], [94mLoss[0m : 2.58233
[1mStep[0m  [40/53], [94mLoss[0m : 2.42808
[1mStep[0m  [45/53], [94mLoss[0m : 2.47926
[1mStep[0m  [50/53], [94mLoss[0m : 2.51038

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39441
[1mStep[0m  [5/53], [94mLoss[0m : 2.26956
[1mStep[0m  [10/53], [94mLoss[0m : 2.73245
[1mStep[0m  [15/53], [94mLoss[0m : 2.40925
[1mStep[0m  [20/53], [94mLoss[0m : 2.52372
[1mStep[0m  [25/53], [94mLoss[0m : 2.46378
[1mStep[0m  [30/53], [94mLoss[0m : 2.45433
[1mStep[0m  [35/53], [94mLoss[0m : 2.46845
[1mStep[0m  [40/53], [94mLoss[0m : 2.47370
[1mStep[0m  [45/53], [94mLoss[0m : 2.25264
[1mStep[0m  [50/53], [94mLoss[0m : 2.53253

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28760
[1mStep[0m  [5/53], [94mLoss[0m : 2.44319
[1mStep[0m  [10/53], [94mLoss[0m : 2.27266
[1mStep[0m  [15/53], [94mLoss[0m : 2.60472
[1mStep[0m  [20/53], [94mLoss[0m : 2.47138
[1mStep[0m  [25/53], [94mLoss[0m : 2.44598
[1mStep[0m  [30/53], [94mLoss[0m : 2.36977
[1mStep[0m  [35/53], [94mLoss[0m : 2.32614
[1mStep[0m  [40/53], [94mLoss[0m : 2.61814
[1mStep[0m  [45/53], [94mLoss[0m : 2.59916
[1mStep[0m  [50/53], [94mLoss[0m : 2.41504

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.65476
[1mStep[0m  [5/53], [94mLoss[0m : 2.57420
[1mStep[0m  [10/53], [94mLoss[0m : 2.40021
[1mStep[0m  [15/53], [94mLoss[0m : 2.62650
[1mStep[0m  [20/53], [94mLoss[0m : 2.53578
[1mStep[0m  [25/53], [94mLoss[0m : 2.49508
[1mStep[0m  [30/53], [94mLoss[0m : 2.35715
[1mStep[0m  [35/53], [94mLoss[0m : 2.32567
[1mStep[0m  [40/53], [94mLoss[0m : 2.44799
[1mStep[0m  [45/53], [94mLoss[0m : 2.54743
[1mStep[0m  [50/53], [94mLoss[0m : 2.39478

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42679
[1mStep[0m  [5/53], [94mLoss[0m : 2.36929
[1mStep[0m  [10/53], [94mLoss[0m : 2.42750
[1mStep[0m  [15/53], [94mLoss[0m : 2.65463
[1mStep[0m  [20/53], [94mLoss[0m : 2.37239
[1mStep[0m  [25/53], [94mLoss[0m : 2.56223
[1mStep[0m  [30/53], [94mLoss[0m : 2.47947
[1mStep[0m  [35/53], [94mLoss[0m : 2.40753
[1mStep[0m  [40/53], [94mLoss[0m : 2.45693
[1mStep[0m  [45/53], [94mLoss[0m : 2.36566
[1mStep[0m  [50/53], [94mLoss[0m : 2.32556

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39198
[1mStep[0m  [5/53], [94mLoss[0m : 2.30512
[1mStep[0m  [10/53], [94mLoss[0m : 2.54818
[1mStep[0m  [15/53], [94mLoss[0m : 2.47597
[1mStep[0m  [20/53], [94mLoss[0m : 2.36268
[1mStep[0m  [25/53], [94mLoss[0m : 2.47826
[1mStep[0m  [30/53], [94mLoss[0m : 2.48006
[1mStep[0m  [35/53], [94mLoss[0m : 2.54262
[1mStep[0m  [40/53], [94mLoss[0m : 2.25394
[1mStep[0m  [45/53], [94mLoss[0m : 2.32798
[1mStep[0m  [50/53], [94mLoss[0m : 2.38580

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.75365
[1mStep[0m  [5/53], [94mLoss[0m : 2.43319
[1mStep[0m  [10/53], [94mLoss[0m : 2.16051
[1mStep[0m  [15/53], [94mLoss[0m : 2.41542
[1mStep[0m  [20/53], [94mLoss[0m : 2.33173
[1mStep[0m  [25/53], [94mLoss[0m : 2.25853
[1mStep[0m  [30/53], [94mLoss[0m : 2.42063
[1mStep[0m  [35/53], [94mLoss[0m : 2.55184
[1mStep[0m  [40/53], [94mLoss[0m : 2.48100
[1mStep[0m  [45/53], [94mLoss[0m : 2.52894
[1mStep[0m  [50/53], [94mLoss[0m : 2.54891

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43863
[1mStep[0m  [5/53], [94mLoss[0m : 2.37589
[1mStep[0m  [10/53], [94mLoss[0m : 2.50632
[1mStep[0m  [15/53], [94mLoss[0m : 2.57245
[1mStep[0m  [20/53], [94mLoss[0m : 2.51560
[1mStep[0m  [25/53], [94mLoss[0m : 2.28134
[1mStep[0m  [30/53], [94mLoss[0m : 2.44689
[1mStep[0m  [35/53], [94mLoss[0m : 2.47425
[1mStep[0m  [40/53], [94mLoss[0m : 2.33088
[1mStep[0m  [45/53], [94mLoss[0m : 2.47666
[1mStep[0m  [50/53], [94mLoss[0m : 2.49693

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57416
[1mStep[0m  [5/53], [94mLoss[0m : 2.44183
[1mStep[0m  [10/53], [94mLoss[0m : 2.32963
[1mStep[0m  [15/53], [94mLoss[0m : 2.29428
[1mStep[0m  [20/53], [94mLoss[0m : 2.35378
[1mStep[0m  [25/53], [94mLoss[0m : 2.37471
[1mStep[0m  [30/53], [94mLoss[0m : 2.39497
[1mStep[0m  [35/53], [94mLoss[0m : 2.41149
[1mStep[0m  [40/53], [94mLoss[0m : 2.52345
[1mStep[0m  [45/53], [94mLoss[0m : 2.53234
[1mStep[0m  [50/53], [94mLoss[0m : 2.67485

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53635
[1mStep[0m  [5/53], [94mLoss[0m : 2.45519
[1mStep[0m  [10/53], [94mLoss[0m : 2.44084
[1mStep[0m  [15/53], [94mLoss[0m : 2.45659
[1mStep[0m  [20/53], [94mLoss[0m : 2.38535
[1mStep[0m  [25/53], [94mLoss[0m : 2.41220
[1mStep[0m  [30/53], [94mLoss[0m : 2.32670
[1mStep[0m  [35/53], [94mLoss[0m : 2.35164
[1mStep[0m  [40/53], [94mLoss[0m : 2.40124
[1mStep[0m  [45/53], [94mLoss[0m : 2.41544
[1mStep[0m  [50/53], [94mLoss[0m : 2.55021

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41083
[1mStep[0m  [5/53], [94mLoss[0m : 2.43163
[1mStep[0m  [10/53], [94mLoss[0m : 2.40593
[1mStep[0m  [15/53], [94mLoss[0m : 2.45494
[1mStep[0m  [20/53], [94mLoss[0m : 2.48492
[1mStep[0m  [25/53], [94mLoss[0m : 2.51924
[1mStep[0m  [30/53], [94mLoss[0m : 2.41793
[1mStep[0m  [35/53], [94mLoss[0m : 2.42358
[1mStep[0m  [40/53], [94mLoss[0m : 2.47904
[1mStep[0m  [45/53], [94mLoss[0m : 2.53821
[1mStep[0m  [50/53], [94mLoss[0m : 2.58309

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35367
[1mStep[0m  [5/53], [94mLoss[0m : 2.40214
[1mStep[0m  [10/53], [94mLoss[0m : 2.47503
[1mStep[0m  [15/53], [94mLoss[0m : 2.52434
[1mStep[0m  [20/53], [94mLoss[0m : 2.61432
[1mStep[0m  [25/53], [94mLoss[0m : 2.65273
[1mStep[0m  [30/53], [94mLoss[0m : 2.31596
[1mStep[0m  [35/53], [94mLoss[0m : 2.39711
[1mStep[0m  [40/53], [94mLoss[0m : 2.43749
[1mStep[0m  [45/53], [94mLoss[0m : 2.78481
[1mStep[0m  [50/53], [94mLoss[0m : 2.44267

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57478
[1mStep[0m  [5/53], [94mLoss[0m : 2.59409
[1mStep[0m  [10/53], [94mLoss[0m : 2.36527
[1mStep[0m  [15/53], [94mLoss[0m : 2.51274
[1mStep[0m  [20/53], [94mLoss[0m : 2.18841
[1mStep[0m  [25/53], [94mLoss[0m : 2.52940
[1mStep[0m  [30/53], [94mLoss[0m : 2.48555
[1mStep[0m  [35/53], [94mLoss[0m : 2.27630
[1mStep[0m  [40/53], [94mLoss[0m : 2.54580
[1mStep[0m  [45/53], [94mLoss[0m : 2.46629
[1mStep[0m  [50/53], [94mLoss[0m : 2.22374

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37877
[1mStep[0m  [5/53], [94mLoss[0m : 2.55826
[1mStep[0m  [10/53], [94mLoss[0m : 2.42270
[1mStep[0m  [15/53], [94mLoss[0m : 2.35127
[1mStep[0m  [20/53], [94mLoss[0m : 2.58869
[1mStep[0m  [25/53], [94mLoss[0m : 2.33549
[1mStep[0m  [30/53], [94mLoss[0m : 2.31646
[1mStep[0m  [35/53], [94mLoss[0m : 2.71173
[1mStep[0m  [40/53], [94mLoss[0m : 2.29056
[1mStep[0m  [45/53], [94mLoss[0m : 2.41673
[1mStep[0m  [50/53], [94mLoss[0m : 2.19574

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55054
[1mStep[0m  [5/53], [94mLoss[0m : 2.39273
[1mStep[0m  [10/53], [94mLoss[0m : 2.47766
[1mStep[0m  [15/53], [94mLoss[0m : 2.49864
[1mStep[0m  [20/53], [94mLoss[0m : 2.17063
[1mStep[0m  [25/53], [94mLoss[0m : 2.33979
[1mStep[0m  [30/53], [94mLoss[0m : 2.50199
[1mStep[0m  [35/53], [94mLoss[0m : 2.27001
[1mStep[0m  [40/53], [94mLoss[0m : 2.72977
[1mStep[0m  [45/53], [94mLoss[0m : 2.41156
[1mStep[0m  [50/53], [94mLoss[0m : 2.50527

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55213
[1mStep[0m  [5/53], [94mLoss[0m : 2.44143
[1mStep[0m  [10/53], [94mLoss[0m : 2.63810
[1mStep[0m  [15/53], [94mLoss[0m : 2.26914
[1mStep[0m  [20/53], [94mLoss[0m : 2.52352
[1mStep[0m  [25/53], [94mLoss[0m : 2.35320
[1mStep[0m  [30/53], [94mLoss[0m : 2.67508
[1mStep[0m  [35/53], [94mLoss[0m : 2.22708
[1mStep[0m  [40/53], [94mLoss[0m : 2.44401
[1mStep[0m  [45/53], [94mLoss[0m : 2.42992
[1mStep[0m  [50/53], [94mLoss[0m : 2.72485

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17519
[1mStep[0m  [5/53], [94mLoss[0m : 2.48043
[1mStep[0m  [10/53], [94mLoss[0m : 2.28219
[1mStep[0m  [15/53], [94mLoss[0m : 2.36217
[1mStep[0m  [20/53], [94mLoss[0m : 2.50503
[1mStep[0m  [25/53], [94mLoss[0m : 2.39503
[1mStep[0m  [30/53], [94mLoss[0m : 2.43477
[1mStep[0m  [35/53], [94mLoss[0m : 2.29087
[1mStep[0m  [40/53], [94mLoss[0m : 2.51796
[1mStep[0m  [45/53], [94mLoss[0m : 2.31729
[1mStep[0m  [50/53], [94mLoss[0m : 2.35777

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.417, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27931
[1mStep[0m  [5/53], [94mLoss[0m : 2.52512
[1mStep[0m  [10/53], [94mLoss[0m : 2.30791
[1mStep[0m  [15/53], [94mLoss[0m : 2.46153
[1mStep[0m  [20/53], [94mLoss[0m : 2.53523
[1mStep[0m  [25/53], [94mLoss[0m : 2.44539
[1mStep[0m  [30/53], [94mLoss[0m : 2.42855
[1mStep[0m  [35/53], [94mLoss[0m : 2.66192
[1mStep[0m  [40/53], [94mLoss[0m : 2.36817
[1mStep[0m  [45/53], [94mLoss[0m : 2.35315
[1mStep[0m  [50/53], [94mLoss[0m : 2.39604

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49241
[1mStep[0m  [5/53], [94mLoss[0m : 2.54687
[1mStep[0m  [10/53], [94mLoss[0m : 2.35294
[1mStep[0m  [15/53], [94mLoss[0m : 2.35357
[1mStep[0m  [20/53], [94mLoss[0m : 2.41382
[1mStep[0m  [25/53], [94mLoss[0m : 2.42910
[1mStep[0m  [30/53], [94mLoss[0m : 2.53601
[1mStep[0m  [35/53], [94mLoss[0m : 2.30681
[1mStep[0m  [40/53], [94mLoss[0m : 2.41397
[1mStep[0m  [45/53], [94mLoss[0m : 2.28009
[1mStep[0m  [50/53], [94mLoss[0m : 2.30341

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.425, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50192
[1mStep[0m  [5/53], [94mLoss[0m : 2.19609
[1mStep[0m  [10/53], [94mLoss[0m : 2.49107
[1mStep[0m  [15/53], [94mLoss[0m : 2.30925
[1mStep[0m  [20/53], [94mLoss[0m : 2.50173
[1mStep[0m  [25/53], [94mLoss[0m : 2.41721
[1mStep[0m  [30/53], [94mLoss[0m : 2.44801
[1mStep[0m  [35/53], [94mLoss[0m : 2.35303
[1mStep[0m  [40/53], [94mLoss[0m : 2.41001
[1mStep[0m  [45/53], [94mLoss[0m : 2.17245
[1mStep[0m  [50/53], [94mLoss[0m : 2.32360

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30719
[1mStep[0m  [5/53], [94mLoss[0m : 2.52244
[1mStep[0m  [10/53], [94mLoss[0m : 2.40076
[1mStep[0m  [15/53], [94mLoss[0m : 2.45458
[1mStep[0m  [20/53], [94mLoss[0m : 2.40914
[1mStep[0m  [25/53], [94mLoss[0m : 2.31022
[1mStep[0m  [30/53], [94mLoss[0m : 2.46784
[1mStep[0m  [35/53], [94mLoss[0m : 2.50954
[1mStep[0m  [40/53], [94mLoss[0m : 2.26597
[1mStep[0m  [45/53], [94mLoss[0m : 2.43147
[1mStep[0m  [50/53], [94mLoss[0m : 2.49904

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.397, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59589
[1mStep[0m  [5/53], [94mLoss[0m : 2.60428
[1mStep[0m  [10/53], [94mLoss[0m : 2.46338
[1mStep[0m  [15/53], [94mLoss[0m : 2.38174
[1mStep[0m  [20/53], [94mLoss[0m : 2.56218
[1mStep[0m  [25/53], [94mLoss[0m : 2.29584
[1mStep[0m  [30/53], [94mLoss[0m : 2.54237
[1mStep[0m  [35/53], [94mLoss[0m : 2.27353
[1mStep[0m  [40/53], [94mLoss[0m : 2.53850
[1mStep[0m  [45/53], [94mLoss[0m : 2.46521
[1mStep[0m  [50/53], [94mLoss[0m : 2.36156

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43155
[1mStep[0m  [5/53], [94mLoss[0m : 2.33467
[1mStep[0m  [10/53], [94mLoss[0m : 2.36838
[1mStep[0m  [15/53], [94mLoss[0m : 2.42173
[1mStep[0m  [20/53], [94mLoss[0m : 2.22417
[1mStep[0m  [25/53], [94mLoss[0m : 2.47481
[1mStep[0m  [30/53], [94mLoss[0m : 2.69763
[1mStep[0m  [35/53], [94mLoss[0m : 2.40403
[1mStep[0m  [40/53], [94mLoss[0m : 2.40589
[1mStep[0m  [45/53], [94mLoss[0m : 2.30909
[1mStep[0m  [50/53], [94mLoss[0m : 2.54407

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.410, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36274
[1mStep[0m  [5/53], [94mLoss[0m : 2.43934
[1mStep[0m  [10/53], [94mLoss[0m : 2.53946
[1mStep[0m  [15/53], [94mLoss[0m : 2.41807
[1mStep[0m  [20/53], [94mLoss[0m : 2.46097
[1mStep[0m  [25/53], [94mLoss[0m : 2.38213
[1mStep[0m  [30/53], [94mLoss[0m : 2.64119
[1mStep[0m  [35/53], [94mLoss[0m : 2.41431
[1mStep[0m  [40/53], [94mLoss[0m : 2.31114
[1mStep[0m  [45/53], [94mLoss[0m : 2.46597
[1mStep[0m  [50/53], [94mLoss[0m : 2.25658

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.412, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56884
[1mStep[0m  [5/53], [94mLoss[0m : 2.53781
[1mStep[0m  [10/53], [94mLoss[0m : 2.30980
[1mStep[0m  [15/53], [94mLoss[0m : 2.37477
[1mStep[0m  [20/53], [94mLoss[0m : 2.55872
[1mStep[0m  [25/53], [94mLoss[0m : 2.47011
[1mStep[0m  [30/53], [94mLoss[0m : 2.56034
[1mStep[0m  [35/53], [94mLoss[0m : 2.40756
[1mStep[0m  [40/53], [94mLoss[0m : 2.42027
[1mStep[0m  [45/53], [94mLoss[0m : 2.33142
[1mStep[0m  [50/53], [94mLoss[0m : 2.58954

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.410, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27465
[1mStep[0m  [5/53], [94mLoss[0m : 2.47164
[1mStep[0m  [10/53], [94mLoss[0m : 2.28044
[1mStep[0m  [15/53], [94mLoss[0m : 2.52247
[1mStep[0m  [20/53], [94mLoss[0m : 2.46822
[1mStep[0m  [25/53], [94mLoss[0m : 2.45060
[1mStep[0m  [30/53], [94mLoss[0m : 2.69367
[1mStep[0m  [35/53], [94mLoss[0m : 2.43821
[1mStep[0m  [40/53], [94mLoss[0m : 2.50097
[1mStep[0m  [45/53], [94mLoss[0m : 2.38828
[1mStep[0m  [50/53], [94mLoss[0m : 2.52045

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.406, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35426
[1mStep[0m  [5/53], [94mLoss[0m : 2.42408
[1mStep[0m  [10/53], [94mLoss[0m : 2.21852
[1mStep[0m  [15/53], [94mLoss[0m : 2.59080
[1mStep[0m  [20/53], [94mLoss[0m : 2.24471
[1mStep[0m  [25/53], [94mLoss[0m : 2.54036
[1mStep[0m  [30/53], [94mLoss[0m : 2.33486
[1mStep[0m  [35/53], [94mLoss[0m : 2.47277
[1mStep[0m  [40/53], [94mLoss[0m : 2.38529
[1mStep[0m  [45/53], [94mLoss[0m : 2.30507
[1mStep[0m  [50/53], [94mLoss[0m : 2.49898

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.404, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.409
====================================

Phase 1 - Evaluation MAE:  2.4093984273763804
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.45512
[1mStep[0m  [5/53], [94mLoss[0m : 2.62686
[1mStep[0m  [10/53], [94mLoss[0m : 2.63249
[1mStep[0m  [15/53], [94mLoss[0m : 2.64904
[1mStep[0m  [20/53], [94mLoss[0m : 2.29586
[1mStep[0m  [25/53], [94mLoss[0m : 2.45838
[1mStep[0m  [30/53], [94mLoss[0m : 2.45165
[1mStep[0m  [35/53], [94mLoss[0m : 2.42872
[1mStep[0m  [40/53], [94mLoss[0m : 2.38453
[1mStep[0m  [45/53], [94mLoss[0m : 2.34454
[1mStep[0m  [50/53], [94mLoss[0m : 2.62132

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38459
[1mStep[0m  [5/53], [94mLoss[0m : 2.36174
[1mStep[0m  [10/53], [94mLoss[0m : 2.54936
[1mStep[0m  [15/53], [94mLoss[0m : 2.28034
[1mStep[0m  [20/53], [94mLoss[0m : 2.57484
[1mStep[0m  [25/53], [94mLoss[0m : 2.43464
[1mStep[0m  [30/53], [94mLoss[0m : 2.48326
[1mStep[0m  [35/53], [94mLoss[0m : 2.60791
[1mStep[0m  [40/53], [94mLoss[0m : 2.35976
[1mStep[0m  [45/53], [94mLoss[0m : 2.34815
[1mStep[0m  [50/53], [94mLoss[0m : 2.38611

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47476
[1mStep[0m  [5/53], [94mLoss[0m : 2.51717
[1mStep[0m  [10/53], [94mLoss[0m : 2.59719
[1mStep[0m  [15/53], [94mLoss[0m : 2.35792
[1mStep[0m  [20/53], [94mLoss[0m : 2.28760
[1mStep[0m  [25/53], [94mLoss[0m : 2.37907
[1mStep[0m  [30/53], [94mLoss[0m : 2.44914
[1mStep[0m  [35/53], [94mLoss[0m : 2.26241
[1mStep[0m  [40/53], [94mLoss[0m : 2.34522
[1mStep[0m  [45/53], [94mLoss[0m : 2.31966
[1mStep[0m  [50/53], [94mLoss[0m : 2.24172

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38882
[1mStep[0m  [5/53], [94mLoss[0m : 2.17047
[1mStep[0m  [10/53], [94mLoss[0m : 2.40409
[1mStep[0m  [15/53], [94mLoss[0m : 2.21013
[1mStep[0m  [20/53], [94mLoss[0m : 2.27027
[1mStep[0m  [25/53], [94mLoss[0m : 2.28071
[1mStep[0m  [30/53], [94mLoss[0m : 2.32568
[1mStep[0m  [35/53], [94mLoss[0m : 2.27587
[1mStep[0m  [40/53], [94mLoss[0m : 2.47075
[1mStep[0m  [45/53], [94mLoss[0m : 2.23041
[1mStep[0m  [50/53], [94mLoss[0m : 2.41810

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.543, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53110
[1mStep[0m  [5/53], [94mLoss[0m : 2.30004
[1mStep[0m  [10/53], [94mLoss[0m : 2.22759
[1mStep[0m  [15/53], [94mLoss[0m : 2.23968
[1mStep[0m  [20/53], [94mLoss[0m : 2.49406
[1mStep[0m  [25/53], [94mLoss[0m : 2.12314
[1mStep[0m  [30/53], [94mLoss[0m : 2.00803
[1mStep[0m  [35/53], [94mLoss[0m : 2.29703
[1mStep[0m  [40/53], [94mLoss[0m : 2.35853
[1mStep[0m  [45/53], [94mLoss[0m : 2.43267
[1mStep[0m  [50/53], [94mLoss[0m : 2.31290

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.585, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20667
[1mStep[0m  [5/53], [94mLoss[0m : 2.36897
[1mStep[0m  [10/53], [94mLoss[0m : 2.33645
[1mStep[0m  [15/53], [94mLoss[0m : 2.38960
[1mStep[0m  [20/53], [94mLoss[0m : 2.08178
[1mStep[0m  [25/53], [94mLoss[0m : 2.30718
[1mStep[0m  [30/53], [94mLoss[0m : 2.27355
[1mStep[0m  [35/53], [94mLoss[0m : 2.14189
[1mStep[0m  [40/53], [94mLoss[0m : 2.23451
[1mStep[0m  [45/53], [94mLoss[0m : 2.13408
[1mStep[0m  [50/53], [94mLoss[0m : 2.13104

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.526, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30280
[1mStep[0m  [5/53], [94mLoss[0m : 2.31324
[1mStep[0m  [10/53], [94mLoss[0m : 2.26532
[1mStep[0m  [15/53], [94mLoss[0m : 2.25896
[1mStep[0m  [20/53], [94mLoss[0m : 2.19355
[1mStep[0m  [25/53], [94mLoss[0m : 2.17036
[1mStep[0m  [30/53], [94mLoss[0m : 2.34310
[1mStep[0m  [35/53], [94mLoss[0m : 1.99306
[1mStep[0m  [40/53], [94mLoss[0m : 2.33898
[1mStep[0m  [45/53], [94mLoss[0m : 2.16274
[1mStep[0m  [50/53], [94mLoss[0m : 2.26830

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.228, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.03845
[1mStep[0m  [5/53], [94mLoss[0m : 2.29804
[1mStep[0m  [10/53], [94mLoss[0m : 2.06061
[1mStep[0m  [15/53], [94mLoss[0m : 2.05858
[1mStep[0m  [20/53], [94mLoss[0m : 2.14064
[1mStep[0m  [25/53], [94mLoss[0m : 2.13292
[1mStep[0m  [30/53], [94mLoss[0m : 2.18601
[1mStep[0m  [35/53], [94mLoss[0m : 1.98352
[1mStep[0m  [40/53], [94mLoss[0m : 2.05630
[1mStep[0m  [45/53], [94mLoss[0m : 2.15203
[1mStep[0m  [50/53], [94mLoss[0m : 2.21175

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.186, [92mTest[0m: 2.568, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.89899
[1mStep[0m  [5/53], [94mLoss[0m : 2.23452
[1mStep[0m  [10/53], [94mLoss[0m : 1.92767
[1mStep[0m  [15/53], [94mLoss[0m : 2.11840
[1mStep[0m  [20/53], [94mLoss[0m : 2.21452
[1mStep[0m  [25/53], [94mLoss[0m : 2.05180
[1mStep[0m  [30/53], [94mLoss[0m : 2.39241
[1mStep[0m  [35/53], [94mLoss[0m : 2.13280
[1mStep[0m  [40/53], [94mLoss[0m : 2.01920
[1mStep[0m  [45/53], [94mLoss[0m : 2.10757
[1mStep[0m  [50/53], [94mLoss[0m : 2.11756

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.558, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.02199
[1mStep[0m  [5/53], [94mLoss[0m : 1.99034
[1mStep[0m  [10/53], [94mLoss[0m : 2.27117
[1mStep[0m  [15/53], [94mLoss[0m : 2.16047
[1mStep[0m  [20/53], [94mLoss[0m : 2.02172
[1mStep[0m  [25/53], [94mLoss[0m : 2.06389
[1mStep[0m  [30/53], [94mLoss[0m : 2.29953
[1mStep[0m  [35/53], [94mLoss[0m : 2.06476
[1mStep[0m  [40/53], [94mLoss[0m : 1.92578
[1mStep[0m  [45/53], [94mLoss[0m : 2.23308
[1mStep[0m  [50/53], [94mLoss[0m : 1.98149

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15791
[1mStep[0m  [5/53], [94mLoss[0m : 1.99205
[1mStep[0m  [10/53], [94mLoss[0m : 2.04480
[1mStep[0m  [15/53], [94mLoss[0m : 2.12731
[1mStep[0m  [20/53], [94mLoss[0m : 2.19581
[1mStep[0m  [25/53], [94mLoss[0m : 2.01302
[1mStep[0m  [30/53], [94mLoss[0m : 2.06977
[1mStep[0m  [35/53], [94mLoss[0m : 2.05743
[1mStep[0m  [40/53], [94mLoss[0m : 2.06331
[1mStep[0m  [45/53], [94mLoss[0m : 2.26999
[1mStep[0m  [50/53], [94mLoss[0m : 2.25391

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.531, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.96571
[1mStep[0m  [5/53], [94mLoss[0m : 2.16818
[1mStep[0m  [10/53], [94mLoss[0m : 2.03133
[1mStep[0m  [15/53], [94mLoss[0m : 1.94782
[1mStep[0m  [20/53], [94mLoss[0m : 2.25319
[1mStep[0m  [25/53], [94mLoss[0m : 2.04480
[1mStep[0m  [30/53], [94mLoss[0m : 1.94098
[1mStep[0m  [35/53], [94mLoss[0m : 2.04806
[1mStep[0m  [40/53], [94mLoss[0m : 2.02188
[1mStep[0m  [45/53], [94mLoss[0m : 2.05797
[1mStep[0m  [50/53], [94mLoss[0m : 1.98190

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.580, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.69793
[1mStep[0m  [5/53], [94mLoss[0m : 1.98662
[1mStep[0m  [10/53], [94mLoss[0m : 2.22670
[1mStep[0m  [15/53], [94mLoss[0m : 1.86159
[1mStep[0m  [20/53], [94mLoss[0m : 2.05412
[1mStep[0m  [25/53], [94mLoss[0m : 2.06551
[1mStep[0m  [30/53], [94mLoss[0m : 1.73402
[1mStep[0m  [35/53], [94mLoss[0m : 2.13937
[1mStep[0m  [40/53], [94mLoss[0m : 1.96763
[1mStep[0m  [45/53], [94mLoss[0m : 2.19754
[1mStep[0m  [50/53], [94mLoss[0m : 2.08737

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.96191
[1mStep[0m  [5/53], [94mLoss[0m : 1.88071
[1mStep[0m  [10/53], [94mLoss[0m : 1.92435
[1mStep[0m  [15/53], [94mLoss[0m : 1.82138
[1mStep[0m  [20/53], [94mLoss[0m : 1.83537
[1mStep[0m  [25/53], [94mLoss[0m : 2.01372
[1mStep[0m  [30/53], [94mLoss[0m : 1.98437
[1mStep[0m  [35/53], [94mLoss[0m : 1.98496
[1mStep[0m  [40/53], [94mLoss[0m : 1.95419
[1mStep[0m  [45/53], [94mLoss[0m : 1.86364
[1mStep[0m  [50/53], [94mLoss[0m : 1.83433

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.76867
[1mStep[0m  [5/53], [94mLoss[0m : 1.88322
[1mStep[0m  [10/53], [94mLoss[0m : 1.87270
[1mStep[0m  [15/53], [94mLoss[0m : 2.02536
[1mStep[0m  [20/53], [94mLoss[0m : 1.88917
[1mStep[0m  [25/53], [94mLoss[0m : 1.81547
[1mStep[0m  [30/53], [94mLoss[0m : 1.86769
[1mStep[0m  [35/53], [94mLoss[0m : 1.88075
[1mStep[0m  [40/53], [94mLoss[0m : 2.10141
[1mStep[0m  [45/53], [94mLoss[0m : 1.87626
[1mStep[0m  [50/53], [94mLoss[0m : 1.84792

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.91042
[1mStep[0m  [5/53], [94mLoss[0m : 1.79713
[1mStep[0m  [10/53], [94mLoss[0m : 1.80597
[1mStep[0m  [15/53], [94mLoss[0m : 1.71583
[1mStep[0m  [20/53], [94mLoss[0m : 1.83501
[1mStep[0m  [25/53], [94mLoss[0m : 1.69289
[1mStep[0m  [30/53], [94mLoss[0m : 1.85862
[1mStep[0m  [35/53], [94mLoss[0m : 1.78009
[1mStep[0m  [40/53], [94mLoss[0m : 1.90864
[1mStep[0m  [45/53], [94mLoss[0m : 1.76562
[1mStep[0m  [50/53], [94mLoss[0m : 1.81859

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84418
[1mStep[0m  [5/53], [94mLoss[0m : 1.90972
[1mStep[0m  [10/53], [94mLoss[0m : 1.74580
[1mStep[0m  [15/53], [94mLoss[0m : 1.78907
[1mStep[0m  [20/53], [94mLoss[0m : 1.88511
[1mStep[0m  [25/53], [94mLoss[0m : 1.64998
[1mStep[0m  [30/53], [94mLoss[0m : 1.90408
[1mStep[0m  [35/53], [94mLoss[0m : 1.84647
[1mStep[0m  [40/53], [94mLoss[0m : 2.02432
[1mStep[0m  [45/53], [94mLoss[0m : 1.90685
[1mStep[0m  [50/53], [94mLoss[0m : 1.99373

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.70987
[1mStep[0m  [5/53], [94mLoss[0m : 1.52403
[1mStep[0m  [10/53], [94mLoss[0m : 1.76183
[1mStep[0m  [15/53], [94mLoss[0m : 1.81860
[1mStep[0m  [20/53], [94mLoss[0m : 1.94908
[1mStep[0m  [25/53], [94mLoss[0m : 1.86392
[1mStep[0m  [30/53], [94mLoss[0m : 1.74688
[1mStep[0m  [35/53], [94mLoss[0m : 1.92392
[1mStep[0m  [40/53], [94mLoss[0m : 1.68282
[1mStep[0m  [45/53], [94mLoss[0m : 1.89808
[1mStep[0m  [50/53], [94mLoss[0m : 1.75684

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73287
[1mStep[0m  [5/53], [94mLoss[0m : 1.58824
[1mStep[0m  [10/53], [94mLoss[0m : 1.77195
[1mStep[0m  [15/53], [94mLoss[0m : 1.72544
[1mStep[0m  [20/53], [94mLoss[0m : 1.83815
[1mStep[0m  [25/53], [94mLoss[0m : 1.64529
[1mStep[0m  [30/53], [94mLoss[0m : 1.74368
[1mStep[0m  [35/53], [94mLoss[0m : 1.56285
[1mStep[0m  [40/53], [94mLoss[0m : 1.70307
[1mStep[0m  [45/53], [94mLoss[0m : 1.68210
[1mStep[0m  [50/53], [94mLoss[0m : 1.77990

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.748, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.55011
[1mStep[0m  [5/53], [94mLoss[0m : 1.69807
[1mStep[0m  [10/53], [94mLoss[0m : 1.92451
[1mStep[0m  [15/53], [94mLoss[0m : 1.82951
[1mStep[0m  [20/53], [94mLoss[0m : 1.56360
[1mStep[0m  [25/53], [94mLoss[0m : 1.52851
[1mStep[0m  [30/53], [94mLoss[0m : 1.67486
[1mStep[0m  [35/53], [94mLoss[0m : 1.85625
[1mStep[0m  [40/53], [94mLoss[0m : 1.69336
[1mStep[0m  [45/53], [94mLoss[0m : 1.63389
[1mStep[0m  [50/53], [94mLoss[0m : 1.97643

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.449, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73098
[1mStep[0m  [5/53], [94mLoss[0m : 1.73569
[1mStep[0m  [10/53], [94mLoss[0m : 1.68013
[1mStep[0m  [15/53], [94mLoss[0m : 1.80030
[1mStep[0m  [20/53], [94mLoss[0m : 1.69189
[1mStep[0m  [25/53], [94mLoss[0m : 1.75631
[1mStep[0m  [30/53], [94mLoss[0m : 1.82800
[1mStep[0m  [35/53], [94mLoss[0m : 1.61967
[1mStep[0m  [40/53], [94mLoss[0m : 1.64398
[1mStep[0m  [45/53], [94mLoss[0m : 1.75963
[1mStep[0m  [50/53], [94mLoss[0m : 1.67865

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.47792
[1mStep[0m  [5/53], [94mLoss[0m : 1.70352
[1mStep[0m  [10/53], [94mLoss[0m : 1.60320
[1mStep[0m  [15/53], [94mLoss[0m : 1.60118
[1mStep[0m  [20/53], [94mLoss[0m : 1.57252
[1mStep[0m  [25/53], [94mLoss[0m : 1.63017
[1mStep[0m  [30/53], [94mLoss[0m : 1.73758
[1mStep[0m  [35/53], [94mLoss[0m : 1.51996
[1mStep[0m  [40/53], [94mLoss[0m : 1.71887
[1mStep[0m  [45/53], [94mLoss[0m : 1.57227
[1mStep[0m  [50/53], [94mLoss[0m : 1.79561

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.502, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54252
[1mStep[0m  [5/53], [94mLoss[0m : 1.59606
[1mStep[0m  [10/53], [94mLoss[0m : 1.60150
[1mStep[0m  [15/53], [94mLoss[0m : 1.56613
[1mStep[0m  [20/53], [94mLoss[0m : 1.65590
[1mStep[0m  [25/53], [94mLoss[0m : 1.65139
[1mStep[0m  [30/53], [94mLoss[0m : 1.62760
[1mStep[0m  [35/53], [94mLoss[0m : 1.58446
[1mStep[0m  [40/53], [94mLoss[0m : 1.67606
[1mStep[0m  [45/53], [94mLoss[0m : 1.65063
[1mStep[0m  [50/53], [94mLoss[0m : 1.50888

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.40119
[1mStep[0m  [5/53], [94mLoss[0m : 1.49548
[1mStep[0m  [10/53], [94mLoss[0m : 1.62913
[1mStep[0m  [15/53], [94mLoss[0m : 1.51849
[1mStep[0m  [20/53], [94mLoss[0m : 1.63645
[1mStep[0m  [25/53], [94mLoss[0m : 1.89495
[1mStep[0m  [30/53], [94mLoss[0m : 1.66978
[1mStep[0m  [35/53], [94mLoss[0m : 1.66889
[1mStep[0m  [40/53], [94mLoss[0m : 1.67819
[1mStep[0m  [45/53], [94mLoss[0m : 1.65953
[1mStep[0m  [50/53], [94mLoss[0m : 1.56018

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.590, [92mTest[0m: 2.479, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.44045
[1mStep[0m  [5/53], [94mLoss[0m : 1.76772
[1mStep[0m  [10/53], [94mLoss[0m : 1.55124
[1mStep[0m  [15/53], [94mLoss[0m : 1.42111
[1mStep[0m  [20/53], [94mLoss[0m : 1.60473
[1mStep[0m  [25/53], [94mLoss[0m : 1.46052
[1mStep[0m  [30/53], [94mLoss[0m : 1.62535
[1mStep[0m  [35/53], [94mLoss[0m : 1.52060
[1mStep[0m  [40/53], [94mLoss[0m : 1.55751
[1mStep[0m  [45/53], [94mLoss[0m : 1.61790
[1mStep[0m  [50/53], [94mLoss[0m : 1.62402

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.558, [92mTest[0m: 2.428, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.42858
[1mStep[0m  [5/53], [94mLoss[0m : 1.72823
[1mStep[0m  [10/53], [94mLoss[0m : 1.51670
[1mStep[0m  [15/53], [94mLoss[0m : 1.26987
[1mStep[0m  [20/53], [94mLoss[0m : 1.52191
[1mStep[0m  [25/53], [94mLoss[0m : 1.61627
[1mStep[0m  [30/53], [94mLoss[0m : 1.70050
[1mStep[0m  [35/53], [94mLoss[0m : 1.59889
[1mStep[0m  [40/53], [94mLoss[0m : 1.54015
[1mStep[0m  [45/53], [94mLoss[0m : 1.65701
[1mStep[0m  [50/53], [94mLoss[0m : 1.53526

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.418, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43004
[1mStep[0m  [5/53], [94mLoss[0m : 1.46659
[1mStep[0m  [10/53], [94mLoss[0m : 1.48488
[1mStep[0m  [15/53], [94mLoss[0m : 1.55623
[1mStep[0m  [20/53], [94mLoss[0m : 1.58869
[1mStep[0m  [25/53], [94mLoss[0m : 1.35430
[1mStep[0m  [30/53], [94mLoss[0m : 1.55362
[1mStep[0m  [35/53], [94mLoss[0m : 1.60850
[1mStep[0m  [40/53], [94mLoss[0m : 1.59277
[1mStep[0m  [45/53], [94mLoss[0m : 1.39968
[1mStep[0m  [50/53], [94mLoss[0m : 1.34226

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.519, [92mTest[0m: 2.580, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.57651
[1mStep[0m  [5/53], [94mLoss[0m : 1.52131
[1mStep[0m  [10/53], [94mLoss[0m : 1.54663
[1mStep[0m  [15/53], [94mLoss[0m : 1.64353
[1mStep[0m  [20/53], [94mLoss[0m : 1.43001
[1mStep[0m  [25/53], [94mLoss[0m : 1.52641
[1mStep[0m  [30/53], [94mLoss[0m : 1.58419
[1mStep[0m  [35/53], [94mLoss[0m : 1.43664
[1mStep[0m  [40/53], [94mLoss[0m : 1.45452
[1mStep[0m  [45/53], [94mLoss[0m : 1.64191
[1mStep[0m  [50/53], [94mLoss[0m : 1.58575

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.446, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.38031
[1mStep[0m  [5/53], [94mLoss[0m : 1.60640
[1mStep[0m  [10/53], [94mLoss[0m : 1.38837
[1mStep[0m  [15/53], [94mLoss[0m : 1.38256
[1mStep[0m  [20/53], [94mLoss[0m : 1.51828
[1mStep[0m  [25/53], [94mLoss[0m : 1.55703
[1mStep[0m  [30/53], [94mLoss[0m : 1.42303
[1mStep[0m  [35/53], [94mLoss[0m : 1.46645
[1mStep[0m  [40/53], [94mLoss[0m : 1.60549
[1mStep[0m  [45/53], [94mLoss[0m : 1.48696
[1mStep[0m  [50/53], [94mLoss[0m : 1.55672

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.481, [92mTest[0m: 2.462, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.36678
[1mStep[0m  [5/53], [94mLoss[0m : 1.36606
[1mStep[0m  [10/53], [94mLoss[0m : 1.57433
[1mStep[0m  [15/53], [94mLoss[0m : 1.42017
[1mStep[0m  [20/53], [94mLoss[0m : 1.41712
[1mStep[0m  [25/53], [94mLoss[0m : 1.43921
[1mStep[0m  [30/53], [94mLoss[0m : 1.43772
[1mStep[0m  [35/53], [94mLoss[0m : 1.36322
[1mStep[0m  [40/53], [94mLoss[0m : 1.46453
[1mStep[0m  [45/53], [94mLoss[0m : 1.47149
[1mStep[0m  [50/53], [94mLoss[0m : 1.41253

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.454, [92mTest[0m: 2.547, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.587
====================================

Phase 2 - Evaluation MAE:  2.586836603971628
MAE score P1      2.409398
MAE score P2      2.586837
loss              1.453817
learning_rate         0.01
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 8, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.89835
[1mStep[0m  [5/53], [94mLoss[0m : 10.78798
[1mStep[0m  [10/53], [94mLoss[0m : 10.89876
[1mStep[0m  [15/53], [94mLoss[0m : 10.39592
[1mStep[0m  [20/53], [94mLoss[0m : 10.53128
[1mStep[0m  [25/53], [94mLoss[0m : 10.76482
[1mStep[0m  [30/53], [94mLoss[0m : 10.28476
[1mStep[0m  [35/53], [94mLoss[0m : 10.38993
[1mStep[0m  [40/53], [94mLoss[0m : 10.20080
[1mStep[0m  [45/53], [94mLoss[0m : 10.28325
[1mStep[0m  [50/53], [94mLoss[0m : 10.03808

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.476, [92mTest[0m: 10.868, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.10689
[1mStep[0m  [5/53], [94mLoss[0m : 10.00627
[1mStep[0m  [10/53], [94mLoss[0m : 9.70859
[1mStep[0m  [15/53], [94mLoss[0m : 9.74565
[1mStep[0m  [20/53], [94mLoss[0m : 9.37652
[1mStep[0m  [25/53], [94mLoss[0m : 9.55745
[1mStep[0m  [30/53], [94mLoss[0m : 9.11068
[1mStep[0m  [35/53], [94mLoss[0m : 9.23554
[1mStep[0m  [40/53], [94mLoss[0m : 8.91148
[1mStep[0m  [45/53], [94mLoss[0m : 8.51063
[1mStep[0m  [50/53], [94mLoss[0m : 8.58858

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.299, [92mTest[0m: 9.652, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.51280
[1mStep[0m  [5/53], [94mLoss[0m : 8.06567
[1mStep[0m  [10/53], [94mLoss[0m : 8.47884
[1mStep[0m  [15/53], [94mLoss[0m : 8.50722
[1mStep[0m  [20/53], [94mLoss[0m : 8.40359
[1mStep[0m  [25/53], [94mLoss[0m : 7.68735
[1mStep[0m  [30/53], [94mLoss[0m : 7.92102
[1mStep[0m  [35/53], [94mLoss[0m : 7.75964
[1mStep[0m  [40/53], [94mLoss[0m : 7.52505
[1mStep[0m  [45/53], [94mLoss[0m : 7.26172
[1mStep[0m  [50/53], [94mLoss[0m : 7.42909

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.914, [92mTest[0m: 8.035, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.20356
[1mStep[0m  [5/53], [94mLoss[0m : 7.04079
[1mStep[0m  [10/53], [94mLoss[0m : 7.29974
[1mStep[0m  [15/53], [94mLoss[0m : 6.93471
[1mStep[0m  [20/53], [94mLoss[0m : 6.72783
[1mStep[0m  [25/53], [94mLoss[0m : 6.84530
[1mStep[0m  [30/53], [94mLoss[0m : 6.70396
[1mStep[0m  [35/53], [94mLoss[0m : 6.64268
[1mStep[0m  [40/53], [94mLoss[0m : 6.58119
[1mStep[0m  [45/53], [94mLoss[0m : 6.38919
[1mStep[0m  [50/53], [94mLoss[0m : 6.61566

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.748, [92mTest[0m: 6.638, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.00508
[1mStep[0m  [5/53], [94mLoss[0m : 6.04025
[1mStep[0m  [10/53], [94mLoss[0m : 6.07315
[1mStep[0m  [15/53], [94mLoss[0m : 5.94733
[1mStep[0m  [20/53], [94mLoss[0m : 5.94214
[1mStep[0m  [25/53], [94mLoss[0m : 5.54762
[1mStep[0m  [30/53], [94mLoss[0m : 5.76008
[1mStep[0m  [35/53], [94mLoss[0m : 5.50100
[1mStep[0m  [40/53], [94mLoss[0m : 5.31571
[1mStep[0m  [45/53], [94mLoss[0m : 4.97041
[1mStep[0m  [50/53], [94mLoss[0m : 4.86650

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.661, [92mTest[0m: 5.199, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.26620
[1mStep[0m  [5/53], [94mLoss[0m : 4.99776
[1mStep[0m  [10/53], [94mLoss[0m : 5.04475
[1mStep[0m  [15/53], [94mLoss[0m : 4.34397
[1mStep[0m  [20/53], [94mLoss[0m : 4.60951
[1mStep[0m  [25/53], [94mLoss[0m : 4.54146
[1mStep[0m  [30/53], [94mLoss[0m : 4.42729
[1mStep[0m  [35/53], [94mLoss[0m : 4.20191
[1mStep[0m  [40/53], [94mLoss[0m : 3.97459
[1mStep[0m  [45/53], [94mLoss[0m : 3.93220
[1mStep[0m  [50/53], [94mLoss[0m : 3.43629

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.395, [92mTest[0m: 4.118, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.03850
[1mStep[0m  [5/53], [94mLoss[0m : 3.54948
[1mStep[0m  [10/53], [94mLoss[0m : 3.43754
[1mStep[0m  [15/53], [94mLoss[0m : 3.41148
[1mStep[0m  [20/53], [94mLoss[0m : 3.36969
[1mStep[0m  [25/53], [94mLoss[0m : 2.87582
[1mStep[0m  [30/53], [94mLoss[0m : 3.21518
[1mStep[0m  [35/53], [94mLoss[0m : 2.86741
[1mStep[0m  [40/53], [94mLoss[0m : 2.57328
[1mStep[0m  [45/53], [94mLoss[0m : 2.64433
[1mStep[0m  [50/53], [94mLoss[0m : 2.75011

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.197, [92mTest[0m: 3.066, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.98440
[1mStep[0m  [5/53], [94mLoss[0m : 2.36370
[1mStep[0m  [10/53], [94mLoss[0m : 2.88711
[1mStep[0m  [15/53], [94mLoss[0m : 2.61019
[1mStep[0m  [20/53], [94mLoss[0m : 2.42563
[1mStep[0m  [25/53], [94mLoss[0m : 2.41316
[1mStep[0m  [30/53], [94mLoss[0m : 2.69057
[1mStep[0m  [35/53], [94mLoss[0m : 2.59121
[1mStep[0m  [40/53], [94mLoss[0m : 2.69836
[1mStep[0m  [45/53], [94mLoss[0m : 2.82450
[1mStep[0m  [50/53], [94mLoss[0m : 2.63604

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.684, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71678
[1mStep[0m  [5/53], [94mLoss[0m : 2.63436
[1mStep[0m  [10/53], [94mLoss[0m : 2.49738
[1mStep[0m  [15/53], [94mLoss[0m : 2.48997
[1mStep[0m  [20/53], [94mLoss[0m : 2.73789
[1mStep[0m  [25/53], [94mLoss[0m : 2.69284
[1mStep[0m  [30/53], [94mLoss[0m : 2.73047
[1mStep[0m  [35/53], [94mLoss[0m : 2.51464
[1mStep[0m  [40/53], [94mLoss[0m : 2.66449
[1mStep[0m  [45/53], [94mLoss[0m : 2.64519
[1mStep[0m  [50/53], [94mLoss[0m : 2.66860

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62891
[1mStep[0m  [5/53], [94mLoss[0m : 2.58991
[1mStep[0m  [10/53], [94mLoss[0m : 2.33012
[1mStep[0m  [15/53], [94mLoss[0m : 2.58640
[1mStep[0m  [20/53], [94mLoss[0m : 2.52705
[1mStep[0m  [25/53], [94mLoss[0m : 2.46583
[1mStep[0m  [30/53], [94mLoss[0m : 2.51947
[1mStep[0m  [35/53], [94mLoss[0m : 2.52494
[1mStep[0m  [40/53], [94mLoss[0m : 2.72338
[1mStep[0m  [45/53], [94mLoss[0m : 2.43765
[1mStep[0m  [50/53], [94mLoss[0m : 2.46107

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.560, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46572
[1mStep[0m  [5/53], [94mLoss[0m : 2.66080
[1mStep[0m  [10/53], [94mLoss[0m : 2.62818
[1mStep[0m  [15/53], [94mLoss[0m : 2.66727
[1mStep[0m  [20/53], [94mLoss[0m : 2.59412
[1mStep[0m  [25/53], [94mLoss[0m : 2.39922
[1mStep[0m  [30/53], [94mLoss[0m : 2.45376
[1mStep[0m  [35/53], [94mLoss[0m : 2.47687
[1mStep[0m  [40/53], [94mLoss[0m : 2.55104
[1mStep[0m  [45/53], [94mLoss[0m : 2.58157
[1mStep[0m  [50/53], [94mLoss[0m : 2.45574

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41385
[1mStep[0m  [5/53], [94mLoss[0m : 2.45788
[1mStep[0m  [10/53], [94mLoss[0m : 2.53191
[1mStep[0m  [15/53], [94mLoss[0m : 2.43301
[1mStep[0m  [20/53], [94mLoss[0m : 2.46366
[1mStep[0m  [25/53], [94mLoss[0m : 2.55812
[1mStep[0m  [30/53], [94mLoss[0m : 2.60295
[1mStep[0m  [35/53], [94mLoss[0m : 2.51338
[1mStep[0m  [40/53], [94mLoss[0m : 2.42315
[1mStep[0m  [45/53], [94mLoss[0m : 2.55356
[1mStep[0m  [50/53], [94mLoss[0m : 2.62593

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.65811
[1mStep[0m  [5/53], [94mLoss[0m : 2.46869
[1mStep[0m  [10/53], [94mLoss[0m : 2.74639
[1mStep[0m  [15/53], [94mLoss[0m : 2.61499
[1mStep[0m  [20/53], [94mLoss[0m : 2.53030
[1mStep[0m  [25/53], [94mLoss[0m : 2.50876
[1mStep[0m  [30/53], [94mLoss[0m : 2.44734
[1mStep[0m  [35/53], [94mLoss[0m : 2.56113
[1mStep[0m  [40/53], [94mLoss[0m : 2.61170
[1mStep[0m  [45/53], [94mLoss[0m : 2.59158
[1mStep[0m  [50/53], [94mLoss[0m : 2.84058

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38570
[1mStep[0m  [5/53], [94mLoss[0m : 2.45742
[1mStep[0m  [10/53], [94mLoss[0m : 2.55175
[1mStep[0m  [15/53], [94mLoss[0m : 2.69470
[1mStep[0m  [20/53], [94mLoss[0m : 2.66337
[1mStep[0m  [25/53], [94mLoss[0m : 2.49907
[1mStep[0m  [30/53], [94mLoss[0m : 2.59588
[1mStep[0m  [35/53], [94mLoss[0m : 2.55738
[1mStep[0m  [40/53], [94mLoss[0m : 2.31844
[1mStep[0m  [45/53], [94mLoss[0m : 2.57519
[1mStep[0m  [50/53], [94mLoss[0m : 2.50483

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.571, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56356
[1mStep[0m  [5/53], [94mLoss[0m : 2.47533
[1mStep[0m  [10/53], [94mLoss[0m : 2.44494
[1mStep[0m  [15/53], [94mLoss[0m : 2.51450
[1mStep[0m  [20/53], [94mLoss[0m : 2.58642
[1mStep[0m  [25/53], [94mLoss[0m : 2.48259
[1mStep[0m  [30/53], [94mLoss[0m : 2.42146
[1mStep[0m  [35/53], [94mLoss[0m : 2.62614
[1mStep[0m  [40/53], [94mLoss[0m : 2.49986
[1mStep[0m  [45/53], [94mLoss[0m : 2.53368
[1mStep[0m  [50/53], [94mLoss[0m : 2.43264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56542
[1mStep[0m  [5/53], [94mLoss[0m : 2.38727
[1mStep[0m  [10/53], [94mLoss[0m : 2.24330
[1mStep[0m  [15/53], [94mLoss[0m : 2.39151
[1mStep[0m  [20/53], [94mLoss[0m : 2.51546
[1mStep[0m  [25/53], [94mLoss[0m : 2.59312
[1mStep[0m  [30/53], [94mLoss[0m : 2.56077
[1mStep[0m  [35/53], [94mLoss[0m : 2.43039
[1mStep[0m  [40/53], [94mLoss[0m : 2.54412
[1mStep[0m  [45/53], [94mLoss[0m : 2.44248
[1mStep[0m  [50/53], [94mLoss[0m : 2.63729

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.492, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47504
[1mStep[0m  [5/53], [94mLoss[0m : 2.50911
[1mStep[0m  [10/53], [94mLoss[0m : 2.47208
[1mStep[0m  [15/53], [94mLoss[0m : 2.51536
[1mStep[0m  [20/53], [94mLoss[0m : 2.42362
[1mStep[0m  [25/53], [94mLoss[0m : 2.62315
[1mStep[0m  [30/53], [94mLoss[0m : 2.42771
[1mStep[0m  [35/53], [94mLoss[0m : 2.49495
[1mStep[0m  [40/53], [94mLoss[0m : 2.43415
[1mStep[0m  [45/53], [94mLoss[0m : 2.35919
[1mStep[0m  [50/53], [94mLoss[0m : 2.74418

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30463
[1mStep[0m  [5/53], [94mLoss[0m : 2.51693
[1mStep[0m  [10/53], [94mLoss[0m : 2.59921
[1mStep[0m  [15/53], [94mLoss[0m : 2.45973
[1mStep[0m  [20/53], [94mLoss[0m : 2.52958
[1mStep[0m  [25/53], [94mLoss[0m : 2.39129
[1mStep[0m  [30/53], [94mLoss[0m : 2.78204
[1mStep[0m  [35/53], [94mLoss[0m : 2.50272
[1mStep[0m  [40/53], [94mLoss[0m : 2.33460
[1mStep[0m  [45/53], [94mLoss[0m : 2.39667
[1mStep[0m  [50/53], [94mLoss[0m : 2.59570

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53822
[1mStep[0m  [5/53], [94mLoss[0m : 2.47529
[1mStep[0m  [10/53], [94mLoss[0m : 2.34874
[1mStep[0m  [15/53], [94mLoss[0m : 2.42734
[1mStep[0m  [20/53], [94mLoss[0m : 2.67465
[1mStep[0m  [25/53], [94mLoss[0m : 2.41314
[1mStep[0m  [30/53], [94mLoss[0m : 2.43959
[1mStep[0m  [35/53], [94mLoss[0m : 2.55001
[1mStep[0m  [40/53], [94mLoss[0m : 2.63225
[1mStep[0m  [45/53], [94mLoss[0m : 2.41569
[1mStep[0m  [50/53], [94mLoss[0m : 2.56486

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32591
[1mStep[0m  [5/53], [94mLoss[0m : 2.46072
[1mStep[0m  [10/53], [94mLoss[0m : 2.29935
[1mStep[0m  [15/53], [94mLoss[0m : 2.44285
[1mStep[0m  [20/53], [94mLoss[0m : 2.23803
[1mStep[0m  [25/53], [94mLoss[0m : 2.68328
[1mStep[0m  [30/53], [94mLoss[0m : 2.48327
[1mStep[0m  [35/53], [94mLoss[0m : 2.52338
[1mStep[0m  [40/53], [94mLoss[0m : 2.44707
[1mStep[0m  [45/53], [94mLoss[0m : 2.46740
[1mStep[0m  [50/53], [94mLoss[0m : 2.43351

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34795
[1mStep[0m  [5/53], [94mLoss[0m : 2.39563
[1mStep[0m  [10/53], [94mLoss[0m : 2.20968
[1mStep[0m  [15/53], [94mLoss[0m : 2.55839
[1mStep[0m  [20/53], [94mLoss[0m : 2.46772
[1mStep[0m  [25/53], [94mLoss[0m : 2.38329
[1mStep[0m  [30/53], [94mLoss[0m : 2.52569
[1mStep[0m  [35/53], [94mLoss[0m : 2.75582
[1mStep[0m  [40/53], [94mLoss[0m : 2.70215
[1mStep[0m  [45/53], [94mLoss[0m : 2.51508
[1mStep[0m  [50/53], [94mLoss[0m : 2.66062

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37282
[1mStep[0m  [5/53], [94mLoss[0m : 2.39273
[1mStep[0m  [10/53], [94mLoss[0m : 2.52174
[1mStep[0m  [15/53], [94mLoss[0m : 2.53045
[1mStep[0m  [20/53], [94mLoss[0m : 2.50802
[1mStep[0m  [25/53], [94mLoss[0m : 2.49067
[1mStep[0m  [30/53], [94mLoss[0m : 2.32979
[1mStep[0m  [35/53], [94mLoss[0m : 2.36166
[1mStep[0m  [40/53], [94mLoss[0m : 2.62991
[1mStep[0m  [45/53], [94mLoss[0m : 2.54366
[1mStep[0m  [50/53], [94mLoss[0m : 2.31034

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32612
[1mStep[0m  [5/53], [94mLoss[0m : 2.53003
[1mStep[0m  [10/53], [94mLoss[0m : 2.57961
[1mStep[0m  [15/53], [94mLoss[0m : 2.37313
[1mStep[0m  [20/53], [94mLoss[0m : 2.35050
[1mStep[0m  [25/53], [94mLoss[0m : 2.41674
[1mStep[0m  [30/53], [94mLoss[0m : 2.45914
[1mStep[0m  [35/53], [94mLoss[0m : 2.57970
[1mStep[0m  [40/53], [94mLoss[0m : 2.50169
[1mStep[0m  [45/53], [94mLoss[0m : 2.17971
[1mStep[0m  [50/53], [94mLoss[0m : 2.32439

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.418, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41453
[1mStep[0m  [5/53], [94mLoss[0m : 2.38884
[1mStep[0m  [10/53], [94mLoss[0m : 2.45200
[1mStep[0m  [15/53], [94mLoss[0m : 2.32469
[1mStep[0m  [20/53], [94mLoss[0m : 2.51946
[1mStep[0m  [25/53], [94mLoss[0m : 2.82902
[1mStep[0m  [30/53], [94mLoss[0m : 2.31415
[1mStep[0m  [35/53], [94mLoss[0m : 2.60018
[1mStep[0m  [40/53], [94mLoss[0m : 2.35394
[1mStep[0m  [45/53], [94mLoss[0m : 2.45265
[1mStep[0m  [50/53], [94mLoss[0m : 2.39615

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.481, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39100
[1mStep[0m  [5/53], [94mLoss[0m : 2.50607
[1mStep[0m  [10/53], [94mLoss[0m : 2.32533
[1mStep[0m  [15/53], [94mLoss[0m : 2.47527
[1mStep[0m  [20/53], [94mLoss[0m : 2.40956
[1mStep[0m  [25/53], [94mLoss[0m : 2.48295
[1mStep[0m  [30/53], [94mLoss[0m : 2.71280
[1mStep[0m  [35/53], [94mLoss[0m : 2.49729
[1mStep[0m  [40/53], [94mLoss[0m : 2.81487
[1mStep[0m  [45/53], [94mLoss[0m : 2.60811
[1mStep[0m  [50/53], [94mLoss[0m : 2.53055

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52447
[1mStep[0m  [5/53], [94mLoss[0m : 2.49766
[1mStep[0m  [10/53], [94mLoss[0m : 2.54544
[1mStep[0m  [15/53], [94mLoss[0m : 2.27747
[1mStep[0m  [20/53], [94mLoss[0m : 2.43666
[1mStep[0m  [25/53], [94mLoss[0m : 2.62952
[1mStep[0m  [30/53], [94mLoss[0m : 2.53093
[1mStep[0m  [35/53], [94mLoss[0m : 2.32854
[1mStep[0m  [40/53], [94mLoss[0m : 2.59777
[1mStep[0m  [45/53], [94mLoss[0m : 2.42681
[1mStep[0m  [50/53], [94mLoss[0m : 2.51621

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51078
[1mStep[0m  [5/53], [94mLoss[0m : 2.44742
[1mStep[0m  [10/53], [94mLoss[0m : 2.58773
[1mStep[0m  [15/53], [94mLoss[0m : 2.28875
[1mStep[0m  [20/53], [94mLoss[0m : 2.55237
[1mStep[0m  [25/53], [94mLoss[0m : 2.41192
[1mStep[0m  [30/53], [94mLoss[0m : 2.44582
[1mStep[0m  [35/53], [94mLoss[0m : 2.29972
[1mStep[0m  [40/53], [94mLoss[0m : 2.65939
[1mStep[0m  [45/53], [94mLoss[0m : 2.41716
[1mStep[0m  [50/53], [94mLoss[0m : 2.27559

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42340
[1mStep[0m  [5/53], [94mLoss[0m : 2.41057
[1mStep[0m  [10/53], [94mLoss[0m : 2.37667
[1mStep[0m  [15/53], [94mLoss[0m : 2.54901
[1mStep[0m  [20/53], [94mLoss[0m : 2.34296
[1mStep[0m  [25/53], [94mLoss[0m : 2.41232
[1mStep[0m  [30/53], [94mLoss[0m : 2.60697
[1mStep[0m  [35/53], [94mLoss[0m : 2.47493
[1mStep[0m  [40/53], [94mLoss[0m : 2.34679
[1mStep[0m  [45/53], [94mLoss[0m : 2.48137
[1mStep[0m  [50/53], [94mLoss[0m : 2.44881

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37277
[1mStep[0m  [5/53], [94mLoss[0m : 2.59337
[1mStep[0m  [10/53], [94mLoss[0m : 2.35678
[1mStep[0m  [15/53], [94mLoss[0m : 2.56440
[1mStep[0m  [20/53], [94mLoss[0m : 2.35347
[1mStep[0m  [25/53], [94mLoss[0m : 2.56931
[1mStep[0m  [30/53], [94mLoss[0m : 2.29956
[1mStep[0m  [35/53], [94mLoss[0m : 2.60513
[1mStep[0m  [40/53], [94mLoss[0m : 2.45702
[1mStep[0m  [45/53], [94mLoss[0m : 2.72313
[1mStep[0m  [50/53], [94mLoss[0m : 2.32725

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.398, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43504
[1mStep[0m  [5/53], [94mLoss[0m : 2.46020
[1mStep[0m  [10/53], [94mLoss[0m : 2.46837
[1mStep[0m  [15/53], [94mLoss[0m : 2.45003
[1mStep[0m  [20/53], [94mLoss[0m : 2.31817
[1mStep[0m  [25/53], [94mLoss[0m : 2.36558
[1mStep[0m  [30/53], [94mLoss[0m : 2.38636
[1mStep[0m  [35/53], [94mLoss[0m : 2.51100
[1mStep[0m  [40/53], [94mLoss[0m : 2.41593
[1mStep[0m  [45/53], [94mLoss[0m : 2.29197
[1mStep[0m  [50/53], [94mLoss[0m : 2.26823

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.389
====================================

Phase 1 - Evaluation MAE:  2.388617992401123
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.22379
[1mStep[0m  [5/53], [94mLoss[0m : 2.66543
[1mStep[0m  [10/53], [94mLoss[0m : 2.43481
[1mStep[0m  [15/53], [94mLoss[0m : 2.47882
[1mStep[0m  [20/53], [94mLoss[0m : 2.40037
[1mStep[0m  [25/53], [94mLoss[0m : 2.51133
[1mStep[0m  [30/53], [94mLoss[0m : 2.47991
[1mStep[0m  [35/53], [94mLoss[0m : 2.68705
[1mStep[0m  [40/53], [94mLoss[0m : 2.44549
[1mStep[0m  [45/53], [94mLoss[0m : 2.52148
[1mStep[0m  [50/53], [94mLoss[0m : 2.51622

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52422
[1mStep[0m  [5/53], [94mLoss[0m : 2.31780
[1mStep[0m  [10/53], [94mLoss[0m : 2.40868
[1mStep[0m  [15/53], [94mLoss[0m : 2.35764
[1mStep[0m  [20/53], [94mLoss[0m : 2.53709
[1mStep[0m  [25/53], [94mLoss[0m : 2.41773
[1mStep[0m  [30/53], [94mLoss[0m : 2.61416
[1mStep[0m  [35/53], [94mLoss[0m : 2.64765
[1mStep[0m  [40/53], [94mLoss[0m : 2.46787
[1mStep[0m  [45/53], [94mLoss[0m : 2.39665
[1mStep[0m  [50/53], [94mLoss[0m : 2.54427

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52448
[1mStep[0m  [5/53], [94mLoss[0m : 2.56701
[1mStep[0m  [10/53], [94mLoss[0m : 2.57938
[1mStep[0m  [15/53], [94mLoss[0m : 2.58144
[1mStep[0m  [20/53], [94mLoss[0m : 2.48987
[1mStep[0m  [25/53], [94mLoss[0m : 2.54190
[1mStep[0m  [30/53], [94mLoss[0m : 2.36704
[1mStep[0m  [35/53], [94mLoss[0m : 2.34488
[1mStep[0m  [40/53], [94mLoss[0m : 2.47779
[1mStep[0m  [45/53], [94mLoss[0m : 2.66276
[1mStep[0m  [50/53], [94mLoss[0m : 2.52789

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49916
[1mStep[0m  [5/53], [94mLoss[0m : 2.60137
[1mStep[0m  [10/53], [94mLoss[0m : 2.49319
[1mStep[0m  [15/53], [94mLoss[0m : 2.18253
[1mStep[0m  [20/53], [94mLoss[0m : 2.54746
[1mStep[0m  [25/53], [94mLoss[0m : 2.40129
[1mStep[0m  [30/53], [94mLoss[0m : 2.45817
[1mStep[0m  [35/53], [94mLoss[0m : 2.24113
[1mStep[0m  [40/53], [94mLoss[0m : 2.14620
[1mStep[0m  [45/53], [94mLoss[0m : 2.24092
[1mStep[0m  [50/53], [94mLoss[0m : 2.50649

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19741
[1mStep[0m  [5/53], [94mLoss[0m : 2.39906
[1mStep[0m  [10/53], [94mLoss[0m : 2.37495
[1mStep[0m  [15/53], [94mLoss[0m : 2.40069
[1mStep[0m  [20/53], [94mLoss[0m : 2.33899
[1mStep[0m  [25/53], [94mLoss[0m : 2.12914
[1mStep[0m  [30/53], [94mLoss[0m : 2.48703
[1mStep[0m  [35/53], [94mLoss[0m : 2.31365
[1mStep[0m  [40/53], [94mLoss[0m : 2.33653
[1mStep[0m  [45/53], [94mLoss[0m : 2.32677
[1mStep[0m  [50/53], [94mLoss[0m : 2.41649

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.704, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34922
[1mStep[0m  [5/53], [94mLoss[0m : 2.44210
[1mStep[0m  [10/53], [94mLoss[0m : 2.52831
[1mStep[0m  [15/53], [94mLoss[0m : 2.27490
[1mStep[0m  [20/53], [94mLoss[0m : 2.26994
[1mStep[0m  [25/53], [94mLoss[0m : 2.41010
[1mStep[0m  [30/53], [94mLoss[0m : 2.43149
[1mStep[0m  [35/53], [94mLoss[0m : 2.54365
[1mStep[0m  [40/53], [94mLoss[0m : 2.26637
[1mStep[0m  [45/53], [94mLoss[0m : 2.21173
[1mStep[0m  [50/53], [94mLoss[0m : 2.40863

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.23714
[1mStep[0m  [5/53], [94mLoss[0m : 2.48919
[1mStep[0m  [10/53], [94mLoss[0m : 2.17865
[1mStep[0m  [15/53], [94mLoss[0m : 2.30695
[1mStep[0m  [20/53], [94mLoss[0m : 2.11430
[1mStep[0m  [25/53], [94mLoss[0m : 2.14911
[1mStep[0m  [30/53], [94mLoss[0m : 2.30458
[1mStep[0m  [35/53], [94mLoss[0m : 2.25820
[1mStep[0m  [40/53], [94mLoss[0m : 2.31606
[1mStep[0m  [45/53], [94mLoss[0m : 2.34923
[1mStep[0m  [50/53], [94mLoss[0m : 2.25586

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25654
[1mStep[0m  [5/53], [94mLoss[0m : 2.27017
[1mStep[0m  [10/53], [94mLoss[0m : 2.44637
[1mStep[0m  [15/53], [94mLoss[0m : 2.01391
[1mStep[0m  [20/53], [94mLoss[0m : 2.18498
[1mStep[0m  [25/53], [94mLoss[0m : 2.27137
[1mStep[0m  [30/53], [94mLoss[0m : 2.13484
[1mStep[0m  [35/53], [94mLoss[0m : 2.21718
[1mStep[0m  [40/53], [94mLoss[0m : 2.07476
[1mStep[0m  [45/53], [94mLoss[0m : 2.25217
[1mStep[0m  [50/53], [94mLoss[0m : 2.32294

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.559, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.14132
[1mStep[0m  [5/53], [94mLoss[0m : 2.17197
[1mStep[0m  [10/53], [94mLoss[0m : 2.09881
[1mStep[0m  [15/53], [94mLoss[0m : 2.18150
[1mStep[0m  [20/53], [94mLoss[0m : 2.17038
[1mStep[0m  [25/53], [94mLoss[0m : 2.17075
[1mStep[0m  [30/53], [94mLoss[0m : 2.26516
[1mStep[0m  [35/53], [94mLoss[0m : 1.90613
[1mStep[0m  [40/53], [94mLoss[0m : 2.10679
[1mStep[0m  [45/53], [94mLoss[0m : 2.34663
[1mStep[0m  [50/53], [94mLoss[0m : 2.39360

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.157, [92mTest[0m: 2.602, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.04303
[1mStep[0m  [5/53], [94mLoss[0m : 1.92558
[1mStep[0m  [10/53], [94mLoss[0m : 2.11706
[1mStep[0m  [15/53], [94mLoss[0m : 2.10004
[1mStep[0m  [20/53], [94mLoss[0m : 2.07970
[1mStep[0m  [25/53], [94mLoss[0m : 2.20513
[1mStep[0m  [30/53], [94mLoss[0m : 2.16084
[1mStep[0m  [35/53], [94mLoss[0m : 2.07448
[1mStep[0m  [40/53], [94mLoss[0m : 2.18190
[1mStep[0m  [45/53], [94mLoss[0m : 2.12114
[1mStep[0m  [50/53], [94mLoss[0m : 1.94335

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.657, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93458
[1mStep[0m  [5/53], [94mLoss[0m : 2.08833
[1mStep[0m  [10/53], [94mLoss[0m : 2.03985
[1mStep[0m  [15/53], [94mLoss[0m : 2.17555
[1mStep[0m  [20/53], [94mLoss[0m : 2.07428
[1mStep[0m  [25/53], [94mLoss[0m : 1.95470
[1mStep[0m  [30/53], [94mLoss[0m : 2.10687
[1mStep[0m  [35/53], [94mLoss[0m : 1.99659
[1mStep[0m  [40/53], [94mLoss[0m : 2.07506
[1mStep[0m  [45/53], [94mLoss[0m : 1.91455
[1mStep[0m  [50/53], [94mLoss[0m : 1.96889

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.632, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.05295
[1mStep[0m  [5/53], [94mLoss[0m : 1.98734
[1mStep[0m  [10/53], [94mLoss[0m : 1.84803
[1mStep[0m  [15/53], [94mLoss[0m : 2.04924
[1mStep[0m  [20/53], [94mLoss[0m : 2.07127
[1mStep[0m  [25/53], [94mLoss[0m : 1.95155
[1mStep[0m  [30/53], [94mLoss[0m : 2.15680
[1mStep[0m  [35/53], [94mLoss[0m : 2.03446
[1mStep[0m  [40/53], [94mLoss[0m : 2.32662
[1mStep[0m  [45/53], [94mLoss[0m : 1.84446
[1mStep[0m  [50/53], [94mLoss[0m : 2.10041

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.045, [92mTest[0m: 2.682, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.11257
[1mStep[0m  [5/53], [94mLoss[0m : 1.95003
[1mStep[0m  [10/53], [94mLoss[0m : 1.90436
[1mStep[0m  [15/53], [94mLoss[0m : 1.89754
[1mStep[0m  [20/53], [94mLoss[0m : 2.01597
[1mStep[0m  [25/53], [94mLoss[0m : 1.95968
[1mStep[0m  [30/53], [94mLoss[0m : 2.10697
[1mStep[0m  [35/53], [94mLoss[0m : 2.09443
[1mStep[0m  [40/53], [94mLoss[0m : 2.10676
[1mStep[0m  [45/53], [94mLoss[0m : 1.84315
[1mStep[0m  [50/53], [94mLoss[0m : 2.11502

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.599, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80152
[1mStep[0m  [5/53], [94mLoss[0m : 2.09400
[1mStep[0m  [10/53], [94mLoss[0m : 1.81048
[1mStep[0m  [15/53], [94mLoss[0m : 2.01390
[1mStep[0m  [20/53], [94mLoss[0m : 1.91322
[1mStep[0m  [25/53], [94mLoss[0m : 1.96495
[1mStep[0m  [30/53], [94mLoss[0m : 2.07498
[1mStep[0m  [35/53], [94mLoss[0m : 1.95374
[1mStep[0m  [40/53], [94mLoss[0m : 1.85549
[1mStep[0m  [45/53], [94mLoss[0m : 1.94351
[1mStep[0m  [50/53], [94mLoss[0m : 2.08838

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.647, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93901
[1mStep[0m  [5/53], [94mLoss[0m : 1.96074
[1mStep[0m  [10/53], [94mLoss[0m : 1.78740
[1mStep[0m  [15/53], [94mLoss[0m : 1.78094
[1mStep[0m  [20/53], [94mLoss[0m : 2.09151
[1mStep[0m  [25/53], [94mLoss[0m : 1.94198
[1mStep[0m  [30/53], [94mLoss[0m : 1.96795
[1mStep[0m  [35/53], [94mLoss[0m : 1.93229
[1mStep[0m  [40/53], [94mLoss[0m : 1.85936
[1mStep[0m  [45/53], [94mLoss[0m : 2.11526
[1mStep[0m  [50/53], [94mLoss[0m : 1.83371

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.84711
[1mStep[0m  [5/53], [94mLoss[0m : 1.91052
[1mStep[0m  [10/53], [94mLoss[0m : 1.80938
[1mStep[0m  [15/53], [94mLoss[0m : 1.65900
[1mStep[0m  [20/53], [94mLoss[0m : 1.87200
[1mStep[0m  [25/53], [94mLoss[0m : 1.89861
[1mStep[0m  [30/53], [94mLoss[0m : 1.89730
[1mStep[0m  [35/53], [94mLoss[0m : 1.86396
[1mStep[0m  [40/53], [94mLoss[0m : 2.01098
[1mStep[0m  [45/53], [94mLoss[0m : 2.00422
[1mStep[0m  [50/53], [94mLoss[0m : 1.84129

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.626, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.88460
[1mStep[0m  [5/53], [94mLoss[0m : 1.78567
[1mStep[0m  [10/53], [94mLoss[0m : 1.85931
[1mStep[0m  [15/53], [94mLoss[0m : 1.79404
[1mStep[0m  [20/53], [94mLoss[0m : 1.76720
[1mStep[0m  [25/53], [94mLoss[0m : 1.93488
[1mStep[0m  [30/53], [94mLoss[0m : 1.81496
[1mStep[0m  [35/53], [94mLoss[0m : 1.99628
[1mStep[0m  [40/53], [94mLoss[0m : 1.80158
[1mStep[0m  [45/53], [94mLoss[0m : 1.80524
[1mStep[0m  [50/53], [94mLoss[0m : 1.86639

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.564, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.77047
[1mStep[0m  [5/53], [94mLoss[0m : 1.75514
[1mStep[0m  [10/53], [94mLoss[0m : 1.79849
[1mStep[0m  [15/53], [94mLoss[0m : 1.98157
[1mStep[0m  [20/53], [94mLoss[0m : 1.96319
[1mStep[0m  [25/53], [94mLoss[0m : 1.65820
[1mStep[0m  [30/53], [94mLoss[0m : 1.66284
[1mStep[0m  [35/53], [94mLoss[0m : 1.87267
[1mStep[0m  [40/53], [94mLoss[0m : 1.87235
[1mStep[0m  [45/53], [94mLoss[0m : 1.82323
[1mStep[0m  [50/53], [94mLoss[0m : 2.06271

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.834, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72261
[1mStep[0m  [5/53], [94mLoss[0m : 1.84612
[1mStep[0m  [10/53], [94mLoss[0m : 1.84367
[1mStep[0m  [15/53], [94mLoss[0m : 1.91691
[1mStep[0m  [20/53], [94mLoss[0m : 1.81513
[1mStep[0m  [25/53], [94mLoss[0m : 1.87356
[1mStep[0m  [30/53], [94mLoss[0m : 1.76255
[1mStep[0m  [35/53], [94mLoss[0m : 1.73655
[1mStep[0m  [40/53], [94mLoss[0m : 1.76695
[1mStep[0m  [45/53], [94mLoss[0m : 1.81707
[1mStep[0m  [50/53], [94mLoss[0m : 1.73137

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.794, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54382
[1mStep[0m  [5/53], [94mLoss[0m : 1.94133
[1mStep[0m  [10/53], [94mLoss[0m : 1.72576
[1mStep[0m  [15/53], [94mLoss[0m : 1.69700
[1mStep[0m  [20/53], [94mLoss[0m : 1.70208
[1mStep[0m  [25/53], [94mLoss[0m : 1.71990
[1mStep[0m  [30/53], [94mLoss[0m : 1.92204
[1mStep[0m  [35/53], [94mLoss[0m : 1.78500
[1mStep[0m  [40/53], [94mLoss[0m : 1.97624
[1mStep[0m  [45/53], [94mLoss[0m : 1.76308
[1mStep[0m  [50/53], [94mLoss[0m : 1.60593

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.612, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63577
[1mStep[0m  [5/53], [94mLoss[0m : 1.74389
[1mStep[0m  [10/53], [94mLoss[0m : 1.89965
[1mStep[0m  [15/53], [94mLoss[0m : 1.63684
[1mStep[0m  [20/53], [94mLoss[0m : 1.77001
[1mStep[0m  [25/53], [94mLoss[0m : 1.69413
[1mStep[0m  [30/53], [94mLoss[0m : 1.78243
[1mStep[0m  [35/53], [94mLoss[0m : 1.75370
[1mStep[0m  [40/53], [94mLoss[0m : 1.63390
[1mStep[0m  [45/53], [94mLoss[0m : 1.78810
[1mStep[0m  [50/53], [94mLoss[0m : 1.55507

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.62042
[1mStep[0m  [5/53], [94mLoss[0m : 1.78119
[1mStep[0m  [10/53], [94mLoss[0m : 1.71778
[1mStep[0m  [15/53], [94mLoss[0m : 1.79282
[1mStep[0m  [20/53], [94mLoss[0m : 1.60886
[1mStep[0m  [25/53], [94mLoss[0m : 1.71108
[1mStep[0m  [30/53], [94mLoss[0m : 1.69599
[1mStep[0m  [35/53], [94mLoss[0m : 1.88250
[1mStep[0m  [40/53], [94mLoss[0m : 1.66320
[1mStep[0m  [45/53], [94mLoss[0m : 1.66219
[1mStep[0m  [50/53], [94mLoss[0m : 1.60442

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71342
[1mStep[0m  [5/53], [94mLoss[0m : 1.80998
[1mStep[0m  [10/53], [94mLoss[0m : 1.61405
[1mStep[0m  [15/53], [94mLoss[0m : 1.72191
[1mStep[0m  [20/53], [94mLoss[0m : 1.80133
[1mStep[0m  [25/53], [94mLoss[0m : 1.60409
[1mStep[0m  [30/53], [94mLoss[0m : 1.68677
[1mStep[0m  [35/53], [94mLoss[0m : 1.78038
[1mStep[0m  [40/53], [94mLoss[0m : 1.63249
[1mStep[0m  [45/53], [94mLoss[0m : 1.75470
[1mStep[0m  [50/53], [94mLoss[0m : 1.56323

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.587, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.48575
[1mStep[0m  [5/53], [94mLoss[0m : 1.66658
[1mStep[0m  [10/53], [94mLoss[0m : 1.58674
[1mStep[0m  [15/53], [94mLoss[0m : 1.71012
[1mStep[0m  [20/53], [94mLoss[0m : 1.62613
[1mStep[0m  [25/53], [94mLoss[0m : 1.85150
[1mStep[0m  [30/53], [94mLoss[0m : 1.76375
[1mStep[0m  [35/53], [94mLoss[0m : 1.49982
[1mStep[0m  [40/53], [94mLoss[0m : 1.59406
[1mStep[0m  [45/53], [94mLoss[0m : 1.68738
[1mStep[0m  [50/53], [94mLoss[0m : 1.76010

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.598, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66299
[1mStep[0m  [5/53], [94mLoss[0m : 1.46583
[1mStep[0m  [10/53], [94mLoss[0m : 1.60676
[1mStep[0m  [15/53], [94mLoss[0m : 1.67144
[1mStep[0m  [20/53], [94mLoss[0m : 1.73437
[1mStep[0m  [25/53], [94mLoss[0m : 1.70384
[1mStep[0m  [30/53], [94mLoss[0m : 1.46689
[1mStep[0m  [35/53], [94mLoss[0m : 1.79310
[1mStep[0m  [40/53], [94mLoss[0m : 1.72682
[1mStep[0m  [45/53], [94mLoss[0m : 1.60484
[1mStep[0m  [50/53], [94mLoss[0m : 1.69381

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.605, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.58888
[1mStep[0m  [5/53], [94mLoss[0m : 1.56530
[1mStep[0m  [10/53], [94mLoss[0m : 1.61429
[1mStep[0m  [15/53], [94mLoss[0m : 1.55015
[1mStep[0m  [20/53], [94mLoss[0m : 1.54623
[1mStep[0m  [25/53], [94mLoss[0m : 1.75344
[1mStep[0m  [30/53], [94mLoss[0m : 1.52963
[1mStep[0m  [35/53], [94mLoss[0m : 1.42735
[1mStep[0m  [40/53], [94mLoss[0m : 1.62786
[1mStep[0m  [45/53], [94mLoss[0m : 1.65458
[1mStep[0m  [50/53], [94mLoss[0m : 1.70100

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.500, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.41263
[1mStep[0m  [5/53], [94mLoss[0m : 1.54591
[1mStep[0m  [10/53], [94mLoss[0m : 1.53202
[1mStep[0m  [15/53], [94mLoss[0m : 1.49514
[1mStep[0m  [20/53], [94mLoss[0m : 1.51944
[1mStep[0m  [25/53], [94mLoss[0m : 1.67736
[1mStep[0m  [30/53], [94mLoss[0m : 1.52831
[1mStep[0m  [35/53], [94mLoss[0m : 1.56680
[1mStep[0m  [40/53], [94mLoss[0m : 1.51591
[1mStep[0m  [45/53], [94mLoss[0m : 1.59785
[1mStep[0m  [50/53], [94mLoss[0m : 1.56074

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.583, [92mTest[0m: 2.500, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.56848
[1mStep[0m  [5/53], [94mLoss[0m : 1.64981
[1mStep[0m  [10/53], [94mLoss[0m : 1.49250
[1mStep[0m  [15/53], [94mLoss[0m : 1.64444
[1mStep[0m  [20/53], [94mLoss[0m : 1.72685
[1mStep[0m  [25/53], [94mLoss[0m : 1.51915
[1mStep[0m  [30/53], [94mLoss[0m : 1.54308
[1mStep[0m  [35/53], [94mLoss[0m : 1.45731
[1mStep[0m  [40/53], [94mLoss[0m : 1.56687
[1mStep[0m  [45/53], [94mLoss[0m : 1.55258
[1mStep[0m  [50/53], [94mLoss[0m : 1.55419

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.573, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.55236
[1mStep[0m  [5/53], [94mLoss[0m : 1.55858
[1mStep[0m  [10/53], [94mLoss[0m : 1.54757
[1mStep[0m  [15/53], [94mLoss[0m : 1.35812
[1mStep[0m  [20/53], [94mLoss[0m : 1.56634
[1mStep[0m  [25/53], [94mLoss[0m : 1.49323
[1mStep[0m  [30/53], [94mLoss[0m : 1.45128
[1mStep[0m  [35/53], [94mLoss[0m : 1.40042
[1mStep[0m  [40/53], [94mLoss[0m : 1.68381
[1mStep[0m  [45/53], [94mLoss[0m : 1.47155
[1mStep[0m  [50/53], [94mLoss[0m : 1.56051

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.43972
[1mStep[0m  [5/53], [94mLoss[0m : 1.42425
[1mStep[0m  [10/53], [94mLoss[0m : 1.60316
[1mStep[0m  [15/53], [94mLoss[0m : 1.61880
[1mStep[0m  [20/53], [94mLoss[0m : 1.57992
[1mStep[0m  [25/53], [94mLoss[0m : 1.49865
[1mStep[0m  [30/53], [94mLoss[0m : 1.47362
[1mStep[0m  [35/53], [94mLoss[0m : 1.42517
[1mStep[0m  [40/53], [94mLoss[0m : 1.44896
[1mStep[0m  [45/53], [94mLoss[0m : 1.54542
[1mStep[0m  [50/53], [94mLoss[0m : 1.66567

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.475, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.443
====================================

Phase 2 - Evaluation MAE:  2.4429386212275577
MAE score P1      2.388618
MAE score P2      2.442939
loss              1.538614
learning_rate         0.01
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.40939
[1mStep[0m  [10/106], [94mLoss[0m : 6.14997
[1mStep[0m  [20/106], [94mLoss[0m : 3.11351
[1mStep[0m  [30/106], [94mLoss[0m : 2.55767
[1mStep[0m  [40/106], [94mLoss[0m : 2.48134
[1mStep[0m  [50/106], [94mLoss[0m : 3.29753
[1mStep[0m  [60/106], [94mLoss[0m : 2.78428
[1mStep[0m  [70/106], [94mLoss[0m : 2.54560
[1mStep[0m  [80/106], [94mLoss[0m : 2.99795
[1mStep[0m  [90/106], [94mLoss[0m : 2.65148
[1mStep[0m  [100/106], [94mLoss[0m : 2.82711

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.445, [92mTest[0m: 11.160, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60686
[1mStep[0m  [10/106], [94mLoss[0m : 2.39825
[1mStep[0m  [20/106], [94mLoss[0m : 2.57285
[1mStep[0m  [30/106], [94mLoss[0m : 2.88608
[1mStep[0m  [40/106], [94mLoss[0m : 2.53007
[1mStep[0m  [50/106], [94mLoss[0m : 2.68032
[1mStep[0m  [60/106], [94mLoss[0m : 2.77464
[1mStep[0m  [70/106], [94mLoss[0m : 2.53407
[1mStep[0m  [80/106], [94mLoss[0m : 2.57020
[1mStep[0m  [90/106], [94mLoss[0m : 2.71750
[1mStep[0m  [100/106], [94mLoss[0m : 2.71866

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58233
[1mStep[0m  [10/106], [94mLoss[0m : 2.36578
[1mStep[0m  [20/106], [94mLoss[0m : 2.60936
[1mStep[0m  [30/106], [94mLoss[0m : 2.50885
[1mStep[0m  [40/106], [94mLoss[0m : 2.08687
[1mStep[0m  [50/106], [94mLoss[0m : 2.55450
[1mStep[0m  [60/106], [94mLoss[0m : 2.62876
[1mStep[0m  [70/106], [94mLoss[0m : 2.48479
[1mStep[0m  [80/106], [94mLoss[0m : 2.81082
[1mStep[0m  [90/106], [94mLoss[0m : 2.82850
[1mStep[0m  [100/106], [94mLoss[0m : 2.36972

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72338
[1mStep[0m  [10/106], [94mLoss[0m : 2.53943
[1mStep[0m  [20/106], [94mLoss[0m : 2.40790
[1mStep[0m  [30/106], [94mLoss[0m : 2.39642
[1mStep[0m  [40/106], [94mLoss[0m : 2.86550
[1mStep[0m  [50/106], [94mLoss[0m : 2.56903
[1mStep[0m  [60/106], [94mLoss[0m : 2.48133
[1mStep[0m  [70/106], [94mLoss[0m : 2.64390
[1mStep[0m  [80/106], [94mLoss[0m : 2.56516
[1mStep[0m  [90/106], [94mLoss[0m : 2.65094
[1mStep[0m  [100/106], [94mLoss[0m : 2.44050

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28444
[1mStep[0m  [10/106], [94mLoss[0m : 2.52070
[1mStep[0m  [20/106], [94mLoss[0m : 2.74751
[1mStep[0m  [30/106], [94mLoss[0m : 2.69340
[1mStep[0m  [40/106], [94mLoss[0m : 2.49435
[1mStep[0m  [50/106], [94mLoss[0m : 2.64057
[1mStep[0m  [60/106], [94mLoss[0m : 2.57481
[1mStep[0m  [70/106], [94mLoss[0m : 2.47860
[1mStep[0m  [80/106], [94mLoss[0m : 2.44961
[1mStep[0m  [90/106], [94mLoss[0m : 2.47219
[1mStep[0m  [100/106], [94mLoss[0m : 2.47071

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64666
[1mStep[0m  [10/106], [94mLoss[0m : 2.56901
[1mStep[0m  [20/106], [94mLoss[0m : 2.55734
[1mStep[0m  [30/106], [94mLoss[0m : 2.38709
[1mStep[0m  [40/106], [94mLoss[0m : 2.54120
[1mStep[0m  [50/106], [94mLoss[0m : 2.42759
[1mStep[0m  [60/106], [94mLoss[0m : 2.55915
[1mStep[0m  [70/106], [94mLoss[0m : 2.54211
[1mStep[0m  [80/106], [94mLoss[0m : 2.39135
[1mStep[0m  [90/106], [94mLoss[0m : 2.80745
[1mStep[0m  [100/106], [94mLoss[0m : 2.40243

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49319
[1mStep[0m  [10/106], [94mLoss[0m : 2.60588
[1mStep[0m  [20/106], [94mLoss[0m : 2.23867
[1mStep[0m  [30/106], [94mLoss[0m : 2.75851
[1mStep[0m  [40/106], [94mLoss[0m : 2.39620
[1mStep[0m  [50/106], [94mLoss[0m : 2.76266
[1mStep[0m  [60/106], [94mLoss[0m : 2.85419
[1mStep[0m  [70/106], [94mLoss[0m : 2.73906
[1mStep[0m  [80/106], [94mLoss[0m : 2.45251
[1mStep[0m  [90/106], [94mLoss[0m : 2.42362
[1mStep[0m  [100/106], [94mLoss[0m : 2.37125

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74307
[1mStep[0m  [10/106], [94mLoss[0m : 2.45798
[1mStep[0m  [20/106], [94mLoss[0m : 2.45740
[1mStep[0m  [30/106], [94mLoss[0m : 2.48193
[1mStep[0m  [40/106], [94mLoss[0m : 2.53793
[1mStep[0m  [50/106], [94mLoss[0m : 2.38547
[1mStep[0m  [60/106], [94mLoss[0m : 2.55721
[1mStep[0m  [70/106], [94mLoss[0m : 2.71816
[1mStep[0m  [80/106], [94mLoss[0m : 2.60977
[1mStep[0m  [90/106], [94mLoss[0m : 2.41917
[1mStep[0m  [100/106], [94mLoss[0m : 2.71193

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12992
[1mStep[0m  [10/106], [94mLoss[0m : 2.70843
[1mStep[0m  [20/106], [94mLoss[0m : 2.63127
[1mStep[0m  [30/106], [94mLoss[0m : 2.43636
[1mStep[0m  [40/106], [94mLoss[0m : 2.39513
[1mStep[0m  [50/106], [94mLoss[0m : 2.42893
[1mStep[0m  [60/106], [94mLoss[0m : 2.26846
[1mStep[0m  [70/106], [94mLoss[0m : 2.31693
[1mStep[0m  [80/106], [94mLoss[0m : 2.76184
[1mStep[0m  [90/106], [94mLoss[0m : 2.48639
[1mStep[0m  [100/106], [94mLoss[0m : 2.79080

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48762
[1mStep[0m  [10/106], [94mLoss[0m : 2.62898
[1mStep[0m  [20/106], [94mLoss[0m : 2.38193
[1mStep[0m  [30/106], [94mLoss[0m : 2.48147
[1mStep[0m  [40/106], [94mLoss[0m : 2.58755
[1mStep[0m  [50/106], [94mLoss[0m : 2.35467
[1mStep[0m  [60/106], [94mLoss[0m : 2.62250
[1mStep[0m  [70/106], [94mLoss[0m : 2.37003
[1mStep[0m  [80/106], [94mLoss[0m : 2.52984
[1mStep[0m  [90/106], [94mLoss[0m : 2.61132
[1mStep[0m  [100/106], [94mLoss[0m : 2.23328

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53432
[1mStep[0m  [10/106], [94mLoss[0m : 2.29519
[1mStep[0m  [20/106], [94mLoss[0m : 2.37130
[1mStep[0m  [30/106], [94mLoss[0m : 2.67653
[1mStep[0m  [40/106], [94mLoss[0m : 2.16951
[1mStep[0m  [50/106], [94mLoss[0m : 2.53967
[1mStep[0m  [60/106], [94mLoss[0m : 2.79839
[1mStep[0m  [70/106], [94mLoss[0m : 2.56221
[1mStep[0m  [80/106], [94mLoss[0m : 2.46124
[1mStep[0m  [90/106], [94mLoss[0m : 2.26420
[1mStep[0m  [100/106], [94mLoss[0m : 2.79430

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58883
[1mStep[0m  [10/106], [94mLoss[0m : 2.49146
[1mStep[0m  [20/106], [94mLoss[0m : 2.40833
[1mStep[0m  [30/106], [94mLoss[0m : 2.45076
[1mStep[0m  [40/106], [94mLoss[0m : 2.55853
[1mStep[0m  [50/106], [94mLoss[0m : 2.45311
[1mStep[0m  [60/106], [94mLoss[0m : 2.54660
[1mStep[0m  [70/106], [94mLoss[0m : 2.73581
[1mStep[0m  [80/106], [94mLoss[0m : 2.43685
[1mStep[0m  [90/106], [94mLoss[0m : 2.86482
[1mStep[0m  [100/106], [94mLoss[0m : 2.25148

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28610
[1mStep[0m  [10/106], [94mLoss[0m : 2.66586
[1mStep[0m  [20/106], [94mLoss[0m : 2.41922
[1mStep[0m  [30/106], [94mLoss[0m : 2.48714
[1mStep[0m  [40/106], [94mLoss[0m : 2.28305
[1mStep[0m  [50/106], [94mLoss[0m : 2.33583
[1mStep[0m  [60/106], [94mLoss[0m : 2.41158
[1mStep[0m  [70/106], [94mLoss[0m : 2.45076
[1mStep[0m  [80/106], [94mLoss[0m : 2.66606
[1mStep[0m  [90/106], [94mLoss[0m : 2.22888
[1mStep[0m  [100/106], [94mLoss[0m : 2.24935

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29768
[1mStep[0m  [10/106], [94mLoss[0m : 2.53427
[1mStep[0m  [20/106], [94mLoss[0m : 2.39682
[1mStep[0m  [30/106], [94mLoss[0m : 2.70370
[1mStep[0m  [40/106], [94mLoss[0m : 2.70444
[1mStep[0m  [50/106], [94mLoss[0m : 2.43186
[1mStep[0m  [60/106], [94mLoss[0m : 2.58182
[1mStep[0m  [70/106], [94mLoss[0m : 2.50602
[1mStep[0m  [80/106], [94mLoss[0m : 2.46647
[1mStep[0m  [90/106], [94mLoss[0m : 2.10174
[1mStep[0m  [100/106], [94mLoss[0m : 2.26905

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58916
[1mStep[0m  [10/106], [94mLoss[0m : 2.35999
[1mStep[0m  [20/106], [94mLoss[0m : 2.53990
[1mStep[0m  [30/106], [94mLoss[0m : 2.53650
[1mStep[0m  [40/106], [94mLoss[0m : 2.48850
[1mStep[0m  [50/106], [94mLoss[0m : 2.43037
[1mStep[0m  [60/106], [94mLoss[0m : 2.43901
[1mStep[0m  [70/106], [94mLoss[0m : 2.13528
[1mStep[0m  [80/106], [94mLoss[0m : 2.32170
[1mStep[0m  [90/106], [94mLoss[0m : 2.30374
[1mStep[0m  [100/106], [94mLoss[0m : 2.22068

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48120
[1mStep[0m  [10/106], [94mLoss[0m : 2.58779
[1mStep[0m  [20/106], [94mLoss[0m : 2.36957
[1mStep[0m  [30/106], [94mLoss[0m : 2.37991
[1mStep[0m  [40/106], [94mLoss[0m : 2.49661
[1mStep[0m  [50/106], [94mLoss[0m : 2.21630
[1mStep[0m  [60/106], [94mLoss[0m : 2.48951
[1mStep[0m  [70/106], [94mLoss[0m : 2.38761
[1mStep[0m  [80/106], [94mLoss[0m : 2.57991
[1mStep[0m  [90/106], [94mLoss[0m : 2.46295
[1mStep[0m  [100/106], [94mLoss[0m : 2.66453

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56863
[1mStep[0m  [10/106], [94mLoss[0m : 2.41767
[1mStep[0m  [20/106], [94mLoss[0m : 2.66148
[1mStep[0m  [30/106], [94mLoss[0m : 2.56647
[1mStep[0m  [40/106], [94mLoss[0m : 2.55535
[1mStep[0m  [50/106], [94mLoss[0m : 2.47880
[1mStep[0m  [60/106], [94mLoss[0m : 2.47915
[1mStep[0m  [70/106], [94mLoss[0m : 2.42271
[1mStep[0m  [80/106], [94mLoss[0m : 2.59594
[1mStep[0m  [90/106], [94mLoss[0m : 2.61762
[1mStep[0m  [100/106], [94mLoss[0m : 2.37757

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44041
[1mStep[0m  [10/106], [94mLoss[0m : 2.74540
[1mStep[0m  [20/106], [94mLoss[0m : 2.42868
[1mStep[0m  [30/106], [94mLoss[0m : 2.57275
[1mStep[0m  [40/106], [94mLoss[0m : 2.23619
[1mStep[0m  [50/106], [94mLoss[0m : 2.54549
[1mStep[0m  [60/106], [94mLoss[0m : 2.57958
[1mStep[0m  [70/106], [94mLoss[0m : 2.36050
[1mStep[0m  [80/106], [94mLoss[0m : 2.63662
[1mStep[0m  [90/106], [94mLoss[0m : 2.45737
[1mStep[0m  [100/106], [94mLoss[0m : 2.35574

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65300
[1mStep[0m  [10/106], [94mLoss[0m : 2.51887
[1mStep[0m  [20/106], [94mLoss[0m : 2.48719
[1mStep[0m  [30/106], [94mLoss[0m : 2.32641
[1mStep[0m  [40/106], [94mLoss[0m : 2.45679
[1mStep[0m  [50/106], [94mLoss[0m : 2.40862
[1mStep[0m  [60/106], [94mLoss[0m : 2.31944
[1mStep[0m  [70/106], [94mLoss[0m : 2.43493
[1mStep[0m  [80/106], [94mLoss[0m : 2.79062
[1mStep[0m  [90/106], [94mLoss[0m : 2.15072
[1mStep[0m  [100/106], [94mLoss[0m : 2.58717

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39557
[1mStep[0m  [10/106], [94mLoss[0m : 2.69632
[1mStep[0m  [20/106], [94mLoss[0m : 2.20162
[1mStep[0m  [30/106], [94mLoss[0m : 2.71591
[1mStep[0m  [40/106], [94mLoss[0m : 2.61727
[1mStep[0m  [50/106], [94mLoss[0m : 2.42462
[1mStep[0m  [60/106], [94mLoss[0m : 2.61999
[1mStep[0m  [70/106], [94mLoss[0m : 2.83262
[1mStep[0m  [80/106], [94mLoss[0m : 2.39004
[1mStep[0m  [90/106], [94mLoss[0m : 2.55590
[1mStep[0m  [100/106], [94mLoss[0m : 2.44039

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.392, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58824
[1mStep[0m  [10/106], [94mLoss[0m : 2.46820
[1mStep[0m  [20/106], [94mLoss[0m : 2.21274
[1mStep[0m  [30/106], [94mLoss[0m : 2.48082
[1mStep[0m  [40/106], [94mLoss[0m : 2.29982
[1mStep[0m  [50/106], [94mLoss[0m : 2.38882
[1mStep[0m  [60/106], [94mLoss[0m : 2.65168
[1mStep[0m  [70/106], [94mLoss[0m : 2.62605
[1mStep[0m  [80/106], [94mLoss[0m : 2.42011
[1mStep[0m  [90/106], [94mLoss[0m : 2.29878
[1mStep[0m  [100/106], [94mLoss[0m : 2.74269

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18775
[1mStep[0m  [10/106], [94mLoss[0m : 2.39729
[1mStep[0m  [20/106], [94mLoss[0m : 2.65548
[1mStep[0m  [30/106], [94mLoss[0m : 2.55208
[1mStep[0m  [40/106], [94mLoss[0m : 2.33508
[1mStep[0m  [50/106], [94mLoss[0m : 2.47574
[1mStep[0m  [60/106], [94mLoss[0m : 2.58679
[1mStep[0m  [70/106], [94mLoss[0m : 2.37051
[1mStep[0m  [80/106], [94mLoss[0m : 2.44656
[1mStep[0m  [90/106], [94mLoss[0m : 2.49804
[1mStep[0m  [100/106], [94mLoss[0m : 2.43058

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70420
[1mStep[0m  [10/106], [94mLoss[0m : 2.68173
[1mStep[0m  [20/106], [94mLoss[0m : 2.20979
[1mStep[0m  [30/106], [94mLoss[0m : 2.41556
[1mStep[0m  [40/106], [94mLoss[0m : 2.38850
[1mStep[0m  [50/106], [94mLoss[0m : 2.20365
[1mStep[0m  [60/106], [94mLoss[0m : 2.55841
[1mStep[0m  [70/106], [94mLoss[0m : 2.65804
[1mStep[0m  [80/106], [94mLoss[0m : 2.31057
[1mStep[0m  [90/106], [94mLoss[0m : 2.45866
[1mStep[0m  [100/106], [94mLoss[0m : 2.50060

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60535
[1mStep[0m  [10/106], [94mLoss[0m : 2.59813
[1mStep[0m  [20/106], [94mLoss[0m : 2.48333
[1mStep[0m  [30/106], [94mLoss[0m : 2.48865
[1mStep[0m  [40/106], [94mLoss[0m : 2.32983
[1mStep[0m  [50/106], [94mLoss[0m : 2.59132
[1mStep[0m  [60/106], [94mLoss[0m : 2.49374
[1mStep[0m  [70/106], [94mLoss[0m : 2.77144
[1mStep[0m  [80/106], [94mLoss[0m : 2.37325
[1mStep[0m  [90/106], [94mLoss[0m : 2.11633
[1mStep[0m  [100/106], [94mLoss[0m : 2.43660

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.388, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47579
[1mStep[0m  [10/106], [94mLoss[0m : 2.10871
[1mStep[0m  [20/106], [94mLoss[0m : 2.41688
[1mStep[0m  [30/106], [94mLoss[0m : 2.68649
[1mStep[0m  [40/106], [94mLoss[0m : 2.53382
[1mStep[0m  [50/106], [94mLoss[0m : 2.19097
[1mStep[0m  [60/106], [94mLoss[0m : 2.15718
[1mStep[0m  [70/106], [94mLoss[0m : 2.56809
[1mStep[0m  [80/106], [94mLoss[0m : 2.36176
[1mStep[0m  [90/106], [94mLoss[0m : 2.02498
[1mStep[0m  [100/106], [94mLoss[0m : 2.43101

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.384, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36942
[1mStep[0m  [10/106], [94mLoss[0m : 2.13082
[1mStep[0m  [20/106], [94mLoss[0m : 2.36096
[1mStep[0m  [30/106], [94mLoss[0m : 2.40088
[1mStep[0m  [40/106], [94mLoss[0m : 2.34606
[1mStep[0m  [50/106], [94mLoss[0m : 2.36037
[1mStep[0m  [60/106], [94mLoss[0m : 2.64977
[1mStep[0m  [70/106], [94mLoss[0m : 2.57927
[1mStep[0m  [80/106], [94mLoss[0m : 2.42375
[1mStep[0m  [90/106], [94mLoss[0m : 2.65878
[1mStep[0m  [100/106], [94mLoss[0m : 2.42675

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98610
[1mStep[0m  [10/106], [94mLoss[0m : 2.41549
[1mStep[0m  [20/106], [94mLoss[0m : 2.29156
[1mStep[0m  [30/106], [94mLoss[0m : 2.27681
[1mStep[0m  [40/106], [94mLoss[0m : 2.27906
[1mStep[0m  [50/106], [94mLoss[0m : 2.74851
[1mStep[0m  [60/106], [94mLoss[0m : 2.36023
[1mStep[0m  [70/106], [94mLoss[0m : 2.62283
[1mStep[0m  [80/106], [94mLoss[0m : 2.56735
[1mStep[0m  [90/106], [94mLoss[0m : 2.24397
[1mStep[0m  [100/106], [94mLoss[0m : 2.74913

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.386, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41132
[1mStep[0m  [10/106], [94mLoss[0m : 2.42793
[1mStep[0m  [20/106], [94mLoss[0m : 2.45088
[1mStep[0m  [30/106], [94mLoss[0m : 2.41811
[1mStep[0m  [40/106], [94mLoss[0m : 2.58975
[1mStep[0m  [50/106], [94mLoss[0m : 2.26546
[1mStep[0m  [60/106], [94mLoss[0m : 2.60525
[1mStep[0m  [70/106], [94mLoss[0m : 2.17170
[1mStep[0m  [80/106], [94mLoss[0m : 2.33536
[1mStep[0m  [90/106], [94mLoss[0m : 2.82615
[1mStep[0m  [100/106], [94mLoss[0m : 2.48595

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31370
[1mStep[0m  [10/106], [94mLoss[0m : 2.73144
[1mStep[0m  [20/106], [94mLoss[0m : 2.52147
[1mStep[0m  [30/106], [94mLoss[0m : 2.55360
[1mStep[0m  [40/106], [94mLoss[0m : 2.52158
[1mStep[0m  [50/106], [94mLoss[0m : 2.33481
[1mStep[0m  [60/106], [94mLoss[0m : 2.52022
[1mStep[0m  [70/106], [94mLoss[0m : 2.30553
[1mStep[0m  [80/106], [94mLoss[0m : 2.29628
[1mStep[0m  [90/106], [94mLoss[0m : 2.52398
[1mStep[0m  [100/106], [94mLoss[0m : 2.57732

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30726
[1mStep[0m  [10/106], [94mLoss[0m : 2.11708
[1mStep[0m  [20/106], [94mLoss[0m : 2.60247
[1mStep[0m  [30/106], [94mLoss[0m : 2.17882
[1mStep[0m  [40/106], [94mLoss[0m : 2.50753
[1mStep[0m  [50/106], [94mLoss[0m : 2.28474
[1mStep[0m  [60/106], [94mLoss[0m : 2.30356
[1mStep[0m  [70/106], [94mLoss[0m : 2.26053
[1mStep[0m  [80/106], [94mLoss[0m : 2.25364
[1mStep[0m  [90/106], [94mLoss[0m : 2.51499
[1mStep[0m  [100/106], [94mLoss[0m : 2.45073

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.384, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.392
====================================

Phase 1 - Evaluation MAE:  2.3915549584154814
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.39425
[1mStep[0m  [10/106], [94mLoss[0m : 2.27169
[1mStep[0m  [20/106], [94mLoss[0m : 2.40482
[1mStep[0m  [30/106], [94mLoss[0m : 2.70032
[1mStep[0m  [40/106], [94mLoss[0m : 2.50862
[1mStep[0m  [50/106], [94mLoss[0m : 2.47530
[1mStep[0m  [60/106], [94mLoss[0m : 2.69300
[1mStep[0m  [70/106], [94mLoss[0m : 2.65799
[1mStep[0m  [80/106], [94mLoss[0m : 2.33086
[1mStep[0m  [90/106], [94mLoss[0m : 2.52740
[1mStep[0m  [100/106], [94mLoss[0m : 2.38134

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19215
[1mStep[0m  [10/106], [94mLoss[0m : 2.25352
[1mStep[0m  [20/106], [94mLoss[0m : 2.43501
[1mStep[0m  [30/106], [94mLoss[0m : 2.18374
[1mStep[0m  [40/106], [94mLoss[0m : 2.12433
[1mStep[0m  [50/106], [94mLoss[0m : 2.30808
[1mStep[0m  [60/106], [94mLoss[0m : 2.27681
[1mStep[0m  [70/106], [94mLoss[0m : 2.49655
[1mStep[0m  [80/106], [94mLoss[0m : 2.41682
[1mStep[0m  [90/106], [94mLoss[0m : 2.42959
[1mStep[0m  [100/106], [94mLoss[0m : 2.34670

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47829
[1mStep[0m  [10/106], [94mLoss[0m : 2.11522
[1mStep[0m  [20/106], [94mLoss[0m : 1.97708
[1mStep[0m  [30/106], [94mLoss[0m : 2.12015
[1mStep[0m  [40/106], [94mLoss[0m : 2.48981
[1mStep[0m  [50/106], [94mLoss[0m : 2.20310
[1mStep[0m  [60/106], [94mLoss[0m : 2.24616
[1mStep[0m  [70/106], [94mLoss[0m : 2.22747
[1mStep[0m  [80/106], [94mLoss[0m : 2.23123
[1mStep[0m  [90/106], [94mLoss[0m : 2.45132
[1mStep[0m  [100/106], [94mLoss[0m : 2.73686

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24346
[1mStep[0m  [10/106], [94mLoss[0m : 2.10651
[1mStep[0m  [20/106], [94mLoss[0m : 2.07589
[1mStep[0m  [30/106], [94mLoss[0m : 2.23756
[1mStep[0m  [40/106], [94mLoss[0m : 2.08418
[1mStep[0m  [50/106], [94mLoss[0m : 2.51185
[1mStep[0m  [60/106], [94mLoss[0m : 2.14721
[1mStep[0m  [70/106], [94mLoss[0m : 2.08582
[1mStep[0m  [80/106], [94mLoss[0m : 2.29642
[1mStep[0m  [90/106], [94mLoss[0m : 2.35602
[1mStep[0m  [100/106], [94mLoss[0m : 2.14115

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92331
[1mStep[0m  [10/106], [94mLoss[0m : 2.02873
[1mStep[0m  [20/106], [94mLoss[0m : 2.58924
[1mStep[0m  [30/106], [94mLoss[0m : 2.11320
[1mStep[0m  [40/106], [94mLoss[0m : 2.35895
[1mStep[0m  [50/106], [94mLoss[0m : 2.16449
[1mStep[0m  [60/106], [94mLoss[0m : 2.21986
[1mStep[0m  [70/106], [94mLoss[0m : 2.02411
[1mStep[0m  [80/106], [94mLoss[0m : 2.40159
[1mStep[0m  [90/106], [94mLoss[0m : 2.12314
[1mStep[0m  [100/106], [94mLoss[0m : 2.21293

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15265
[1mStep[0m  [10/106], [94mLoss[0m : 1.86009
[1mStep[0m  [20/106], [94mLoss[0m : 2.20939
[1mStep[0m  [30/106], [94mLoss[0m : 1.86676
[1mStep[0m  [40/106], [94mLoss[0m : 2.08877
[1mStep[0m  [50/106], [94mLoss[0m : 2.11625
[1mStep[0m  [60/106], [94mLoss[0m : 2.07913
[1mStep[0m  [70/106], [94mLoss[0m : 2.08395
[1mStep[0m  [80/106], [94mLoss[0m : 1.82905
[1mStep[0m  [90/106], [94mLoss[0m : 2.23304
[1mStep[0m  [100/106], [94mLoss[0m : 2.19884

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77915
[1mStep[0m  [10/106], [94mLoss[0m : 2.08815
[1mStep[0m  [20/106], [94mLoss[0m : 2.09330
[1mStep[0m  [30/106], [94mLoss[0m : 1.94162
[1mStep[0m  [40/106], [94mLoss[0m : 2.15181
[1mStep[0m  [50/106], [94mLoss[0m : 1.78370
[1mStep[0m  [60/106], [94mLoss[0m : 2.26168
[1mStep[0m  [70/106], [94mLoss[0m : 1.74098
[1mStep[0m  [80/106], [94mLoss[0m : 2.29615
[1mStep[0m  [90/106], [94mLoss[0m : 2.20625
[1mStep[0m  [100/106], [94mLoss[0m : 2.06744

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.88806
[1mStep[0m  [10/106], [94mLoss[0m : 1.93232
[1mStep[0m  [20/106], [94mLoss[0m : 2.02491
[1mStep[0m  [30/106], [94mLoss[0m : 1.79611
[1mStep[0m  [40/106], [94mLoss[0m : 1.92494
[1mStep[0m  [50/106], [94mLoss[0m : 1.62093
[1mStep[0m  [60/106], [94mLoss[0m : 1.88427
[1mStep[0m  [70/106], [94mLoss[0m : 1.88456
[1mStep[0m  [80/106], [94mLoss[0m : 1.88243
[1mStep[0m  [90/106], [94mLoss[0m : 1.87253
[1mStep[0m  [100/106], [94mLoss[0m : 2.20512

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67203
[1mStep[0m  [10/106], [94mLoss[0m : 1.71197
[1mStep[0m  [20/106], [94mLoss[0m : 2.11019
[1mStep[0m  [30/106], [94mLoss[0m : 2.31606
[1mStep[0m  [40/106], [94mLoss[0m : 1.90247
[1mStep[0m  [50/106], [94mLoss[0m : 2.01819
[1mStep[0m  [60/106], [94mLoss[0m : 2.00421
[1mStep[0m  [70/106], [94mLoss[0m : 1.89319
[1mStep[0m  [80/106], [94mLoss[0m : 2.06489
[1mStep[0m  [90/106], [94mLoss[0m : 2.22771
[1mStep[0m  [100/106], [94mLoss[0m : 1.97584

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.942, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59555
[1mStep[0m  [10/106], [94mLoss[0m : 1.68015
[1mStep[0m  [20/106], [94mLoss[0m : 1.72704
[1mStep[0m  [30/106], [94mLoss[0m : 1.73577
[1mStep[0m  [40/106], [94mLoss[0m : 1.66624
[1mStep[0m  [50/106], [94mLoss[0m : 1.71030
[1mStep[0m  [60/106], [94mLoss[0m : 1.95395
[1mStep[0m  [70/106], [94mLoss[0m : 1.71211
[1mStep[0m  [80/106], [94mLoss[0m : 1.92965
[1mStep[0m  [90/106], [94mLoss[0m : 2.00179
[1mStep[0m  [100/106], [94mLoss[0m : 1.98845

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.567, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59418
[1mStep[0m  [10/106], [94mLoss[0m : 1.63692
[1mStep[0m  [20/106], [94mLoss[0m : 1.74707
[1mStep[0m  [30/106], [94mLoss[0m : 1.95851
[1mStep[0m  [40/106], [94mLoss[0m : 1.83120
[1mStep[0m  [50/106], [94mLoss[0m : 1.87579
[1mStep[0m  [60/106], [94mLoss[0m : 1.73518
[1mStep[0m  [70/106], [94mLoss[0m : 1.83385
[1mStep[0m  [80/106], [94mLoss[0m : 2.19898
[1mStep[0m  [90/106], [94mLoss[0m : 1.85637
[1mStep[0m  [100/106], [94mLoss[0m : 1.70967

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74547
[1mStep[0m  [10/106], [94mLoss[0m : 1.69646
[1mStep[0m  [20/106], [94mLoss[0m : 1.84215
[1mStep[0m  [30/106], [94mLoss[0m : 1.54653
[1mStep[0m  [40/106], [94mLoss[0m : 1.47331
[1mStep[0m  [50/106], [94mLoss[0m : 1.88726
[1mStep[0m  [60/106], [94mLoss[0m : 1.86104
[1mStep[0m  [70/106], [94mLoss[0m : 2.25252
[1mStep[0m  [80/106], [94mLoss[0m : 1.63413
[1mStep[0m  [90/106], [94mLoss[0m : 1.93365
[1mStep[0m  [100/106], [94mLoss[0m : 1.92824

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78081
[1mStep[0m  [10/106], [94mLoss[0m : 1.78477
[1mStep[0m  [20/106], [94mLoss[0m : 1.64655
[1mStep[0m  [30/106], [94mLoss[0m : 1.76576
[1mStep[0m  [40/106], [94mLoss[0m : 1.83211
[1mStep[0m  [50/106], [94mLoss[0m : 1.81107
[1mStep[0m  [60/106], [94mLoss[0m : 1.62607
[1mStep[0m  [70/106], [94mLoss[0m : 1.98508
[1mStep[0m  [80/106], [94mLoss[0m : 1.84544
[1mStep[0m  [90/106], [94mLoss[0m : 1.65525
[1mStep[0m  [100/106], [94mLoss[0m : 1.76307

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.530, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.93090
[1mStep[0m  [10/106], [94mLoss[0m : 1.51903
[1mStep[0m  [20/106], [94mLoss[0m : 1.75562
[1mStep[0m  [30/106], [94mLoss[0m : 1.49261
[1mStep[0m  [40/106], [94mLoss[0m : 1.86948
[1mStep[0m  [50/106], [94mLoss[0m : 1.72405
[1mStep[0m  [60/106], [94mLoss[0m : 1.87178
[1mStep[0m  [70/106], [94mLoss[0m : 1.69279
[1mStep[0m  [80/106], [94mLoss[0m : 1.91764
[1mStep[0m  [90/106], [94mLoss[0m : 2.07419
[1mStep[0m  [100/106], [94mLoss[0m : 1.76326

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72115
[1mStep[0m  [10/106], [94mLoss[0m : 1.91915
[1mStep[0m  [20/106], [94mLoss[0m : 1.73447
[1mStep[0m  [30/106], [94mLoss[0m : 1.61171
[1mStep[0m  [40/106], [94mLoss[0m : 1.69469
[1mStep[0m  [50/106], [94mLoss[0m : 1.60602
[1mStep[0m  [60/106], [94mLoss[0m : 1.66774
[1mStep[0m  [70/106], [94mLoss[0m : 1.83922
[1mStep[0m  [80/106], [94mLoss[0m : 1.96216
[1mStep[0m  [90/106], [94mLoss[0m : 1.61125
[1mStep[0m  [100/106], [94mLoss[0m : 1.82349

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54477
[1mStep[0m  [10/106], [94mLoss[0m : 1.58602
[1mStep[0m  [20/106], [94mLoss[0m : 1.51810
[1mStep[0m  [30/106], [94mLoss[0m : 1.89042
[1mStep[0m  [40/106], [94mLoss[0m : 1.52349
[1mStep[0m  [50/106], [94mLoss[0m : 1.59561
[1mStep[0m  [60/106], [94mLoss[0m : 1.70412
[1mStep[0m  [70/106], [94mLoss[0m : 1.72698
[1mStep[0m  [80/106], [94mLoss[0m : 1.68283
[1mStep[0m  [90/106], [94mLoss[0m : 1.59752
[1mStep[0m  [100/106], [94mLoss[0m : 1.75998

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.562, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55219
[1mStep[0m  [10/106], [94mLoss[0m : 1.51049
[1mStep[0m  [20/106], [94mLoss[0m : 1.31434
[1mStep[0m  [30/106], [94mLoss[0m : 1.88715
[1mStep[0m  [40/106], [94mLoss[0m : 1.52500
[1mStep[0m  [50/106], [94mLoss[0m : 1.55222
[1mStep[0m  [60/106], [94mLoss[0m : 1.71166
[1mStep[0m  [70/106], [94mLoss[0m : 1.68886
[1mStep[0m  [80/106], [94mLoss[0m : 1.57909
[1mStep[0m  [90/106], [94mLoss[0m : 1.75161
[1mStep[0m  [100/106], [94mLoss[0m : 1.77541

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.545, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65462
[1mStep[0m  [10/106], [94mLoss[0m : 1.62106
[1mStep[0m  [20/106], [94mLoss[0m : 1.55681
[1mStep[0m  [30/106], [94mLoss[0m : 1.43760
[1mStep[0m  [40/106], [94mLoss[0m : 1.68457
[1mStep[0m  [50/106], [94mLoss[0m : 1.73402
[1mStep[0m  [60/106], [94mLoss[0m : 1.82125
[1mStep[0m  [70/106], [94mLoss[0m : 1.69861
[1mStep[0m  [80/106], [94mLoss[0m : 1.65384
[1mStep[0m  [90/106], [94mLoss[0m : 1.69653
[1mStep[0m  [100/106], [94mLoss[0m : 1.81219

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.37797
[1mStep[0m  [10/106], [94mLoss[0m : 1.45728
[1mStep[0m  [20/106], [94mLoss[0m : 1.71077
[1mStep[0m  [30/106], [94mLoss[0m : 1.76336
[1mStep[0m  [40/106], [94mLoss[0m : 1.73135
[1mStep[0m  [50/106], [94mLoss[0m : 1.62776
[1mStep[0m  [60/106], [94mLoss[0m : 1.72500
[1mStep[0m  [70/106], [94mLoss[0m : 1.62523
[1mStep[0m  [80/106], [94mLoss[0m : 1.57584
[1mStep[0m  [90/106], [94mLoss[0m : 1.66181
[1mStep[0m  [100/106], [94mLoss[0m : 1.44622

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83100
[1mStep[0m  [10/106], [94mLoss[0m : 1.41549
[1mStep[0m  [20/106], [94mLoss[0m : 1.27376
[1mStep[0m  [30/106], [94mLoss[0m : 1.75328
[1mStep[0m  [40/106], [94mLoss[0m : 1.57233
[1mStep[0m  [50/106], [94mLoss[0m : 1.37388
[1mStep[0m  [60/106], [94mLoss[0m : 1.62077
[1mStep[0m  [70/106], [94mLoss[0m : 1.34158
[1mStep[0m  [80/106], [94mLoss[0m : 1.63657
[1mStep[0m  [90/106], [94mLoss[0m : 1.62180
[1mStep[0m  [100/106], [94mLoss[0m : 1.65488

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.520, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35143
[1mStep[0m  [10/106], [94mLoss[0m : 1.47164
[1mStep[0m  [20/106], [94mLoss[0m : 1.55969
[1mStep[0m  [30/106], [94mLoss[0m : 1.57073
[1mStep[0m  [40/106], [94mLoss[0m : 1.59248
[1mStep[0m  [50/106], [94mLoss[0m : 1.49851
[1mStep[0m  [60/106], [94mLoss[0m : 1.46569
[1mStep[0m  [70/106], [94mLoss[0m : 1.39218
[1mStep[0m  [80/106], [94mLoss[0m : 1.51518
[1mStep[0m  [90/106], [94mLoss[0m : 1.49953
[1mStep[0m  [100/106], [94mLoss[0m : 1.55252

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.538, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35319
[1mStep[0m  [10/106], [94mLoss[0m : 1.46211
[1mStep[0m  [20/106], [94mLoss[0m : 1.35521
[1mStep[0m  [30/106], [94mLoss[0m : 1.45577
[1mStep[0m  [40/106], [94mLoss[0m : 1.88822
[1mStep[0m  [50/106], [94mLoss[0m : 1.40570
[1mStep[0m  [60/106], [94mLoss[0m : 1.49919
[1mStep[0m  [70/106], [94mLoss[0m : 1.46047
[1mStep[0m  [80/106], [94mLoss[0m : 1.61906
[1mStep[0m  [90/106], [94mLoss[0m : 1.66136
[1mStep[0m  [100/106], [94mLoss[0m : 1.61576

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.32847
[1mStep[0m  [10/106], [94mLoss[0m : 1.33259
[1mStep[0m  [20/106], [94mLoss[0m : 1.48973
[1mStep[0m  [30/106], [94mLoss[0m : 1.73409
[1mStep[0m  [40/106], [94mLoss[0m : 1.40735
[1mStep[0m  [50/106], [94mLoss[0m : 1.39881
[1mStep[0m  [60/106], [94mLoss[0m : 1.27610
[1mStep[0m  [70/106], [94mLoss[0m : 1.41039
[1mStep[0m  [80/106], [94mLoss[0m : 1.46529
[1mStep[0m  [90/106], [94mLoss[0m : 1.54941
[1mStep[0m  [100/106], [94mLoss[0m : 1.45795

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.525, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.33530
[1mStep[0m  [10/106], [94mLoss[0m : 1.41849
[1mStep[0m  [20/106], [94mLoss[0m : 1.47079
[1mStep[0m  [30/106], [94mLoss[0m : 1.40244
[1mStep[0m  [40/106], [94mLoss[0m : 1.32602
[1mStep[0m  [50/106], [94mLoss[0m : 1.44176
[1mStep[0m  [60/106], [94mLoss[0m : 1.45922
[1mStep[0m  [70/106], [94mLoss[0m : 1.40385
[1mStep[0m  [80/106], [94mLoss[0m : 1.38356
[1mStep[0m  [90/106], [94mLoss[0m : 1.38009
[1mStep[0m  [100/106], [94mLoss[0m : 1.42669

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.556, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.23015
[1mStep[0m  [10/106], [94mLoss[0m : 1.27786
[1mStep[0m  [20/106], [94mLoss[0m : 1.26383
[1mStep[0m  [30/106], [94mLoss[0m : 1.39130
[1mStep[0m  [40/106], [94mLoss[0m : 1.52557
[1mStep[0m  [50/106], [94mLoss[0m : 1.51059
[1mStep[0m  [60/106], [94mLoss[0m : 1.40730
[1mStep[0m  [70/106], [94mLoss[0m : 1.55381
[1mStep[0m  [80/106], [94mLoss[0m : 1.43541
[1mStep[0m  [90/106], [94mLoss[0m : 1.64206
[1mStep[0m  [100/106], [94mLoss[0m : 1.51869

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.433, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.496
====================================

Phase 2 - Evaluation MAE:  2.496445788527435
MAE score P1       2.391555
MAE score P2       2.496446
loss               1.433221
learning_rate          0.01
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay         0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.26092
[1mStep[0m  [10/106], [94mLoss[0m : 2.78221
[1mStep[0m  [20/106], [94mLoss[0m : 2.41595
[1mStep[0m  [30/106], [94mLoss[0m : 2.83893
[1mStep[0m  [40/106], [94mLoss[0m : 2.74903
[1mStep[0m  [50/106], [94mLoss[0m : 2.61548
[1mStep[0m  [60/106], [94mLoss[0m : 2.56211
[1mStep[0m  [70/106], [94mLoss[0m : 2.75339
[1mStep[0m  [80/106], [94mLoss[0m : 2.61576
[1mStep[0m  [90/106], [94mLoss[0m : 2.58016
[1mStep[0m  [100/106], [94mLoss[0m : 2.55653

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.952, [92mTest[0m: 11.068, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21439
[1mStep[0m  [10/106], [94mLoss[0m : 2.30624
[1mStep[0m  [20/106], [94mLoss[0m : 2.46747
[1mStep[0m  [30/106], [94mLoss[0m : 2.38212
[1mStep[0m  [40/106], [94mLoss[0m : 2.61087
[1mStep[0m  [50/106], [94mLoss[0m : 2.29875
[1mStep[0m  [60/106], [94mLoss[0m : 2.54874
[1mStep[0m  [70/106], [94mLoss[0m : 2.21968
[1mStep[0m  [80/106], [94mLoss[0m : 2.45253
[1mStep[0m  [90/106], [94mLoss[0m : 2.34534
[1mStep[0m  [100/106], [94mLoss[0m : 2.85338

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36834
[1mStep[0m  [10/106], [94mLoss[0m : 2.77518
[1mStep[0m  [20/106], [94mLoss[0m : 2.39451
[1mStep[0m  [30/106], [94mLoss[0m : 2.45239
[1mStep[0m  [40/106], [94mLoss[0m : 2.32351
[1mStep[0m  [50/106], [94mLoss[0m : 2.24995
[1mStep[0m  [60/106], [94mLoss[0m : 2.47924
[1mStep[0m  [70/106], [94mLoss[0m : 2.32058
[1mStep[0m  [80/106], [94mLoss[0m : 2.46818
[1mStep[0m  [90/106], [94mLoss[0m : 2.28788
[1mStep[0m  [100/106], [94mLoss[0m : 2.36541

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33543
[1mStep[0m  [10/106], [94mLoss[0m : 2.29405
[1mStep[0m  [20/106], [94mLoss[0m : 2.27318
[1mStep[0m  [30/106], [94mLoss[0m : 2.59306
[1mStep[0m  [40/106], [94mLoss[0m : 2.73818
[1mStep[0m  [50/106], [94mLoss[0m : 2.68225
[1mStep[0m  [60/106], [94mLoss[0m : 2.52630
[1mStep[0m  [70/106], [94mLoss[0m : 2.82012
[1mStep[0m  [80/106], [94mLoss[0m : 2.51484
[1mStep[0m  [90/106], [94mLoss[0m : 2.52973
[1mStep[0m  [100/106], [94mLoss[0m : 2.39645

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43320
[1mStep[0m  [10/106], [94mLoss[0m : 2.94588
[1mStep[0m  [20/106], [94mLoss[0m : 2.48950
[1mStep[0m  [30/106], [94mLoss[0m : 2.35256
[1mStep[0m  [40/106], [94mLoss[0m : 2.39589
[1mStep[0m  [50/106], [94mLoss[0m : 2.34978
[1mStep[0m  [60/106], [94mLoss[0m : 2.37799
[1mStep[0m  [70/106], [94mLoss[0m : 2.49363
[1mStep[0m  [80/106], [94mLoss[0m : 2.41874
[1mStep[0m  [90/106], [94mLoss[0m : 2.53019
[1mStep[0m  [100/106], [94mLoss[0m : 2.19856

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42148
[1mStep[0m  [10/106], [94mLoss[0m : 2.41052
[1mStep[0m  [20/106], [94mLoss[0m : 2.34902
[1mStep[0m  [30/106], [94mLoss[0m : 2.76496
[1mStep[0m  [40/106], [94mLoss[0m : 2.67535
[1mStep[0m  [50/106], [94mLoss[0m : 2.28125
[1mStep[0m  [60/106], [94mLoss[0m : 2.53584
[1mStep[0m  [70/106], [94mLoss[0m : 2.33204
[1mStep[0m  [80/106], [94mLoss[0m : 2.61373
[1mStep[0m  [90/106], [94mLoss[0m : 2.85996
[1mStep[0m  [100/106], [94mLoss[0m : 2.59397

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14077
[1mStep[0m  [10/106], [94mLoss[0m : 2.71271
[1mStep[0m  [20/106], [94mLoss[0m : 2.33704
[1mStep[0m  [30/106], [94mLoss[0m : 2.54749
[1mStep[0m  [40/106], [94mLoss[0m : 2.10642
[1mStep[0m  [50/106], [94mLoss[0m : 2.10196
[1mStep[0m  [60/106], [94mLoss[0m : 2.18712
[1mStep[0m  [70/106], [94mLoss[0m : 2.61321
[1mStep[0m  [80/106], [94mLoss[0m : 2.43105
[1mStep[0m  [90/106], [94mLoss[0m : 2.26837
[1mStep[0m  [100/106], [94mLoss[0m : 2.45457

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53417
[1mStep[0m  [10/106], [94mLoss[0m : 2.49302
[1mStep[0m  [20/106], [94mLoss[0m : 2.40769
[1mStep[0m  [30/106], [94mLoss[0m : 2.26920
[1mStep[0m  [40/106], [94mLoss[0m : 2.25001
[1mStep[0m  [50/106], [94mLoss[0m : 2.49653
[1mStep[0m  [60/106], [94mLoss[0m : 2.24460
[1mStep[0m  [70/106], [94mLoss[0m : 2.49043
[1mStep[0m  [80/106], [94mLoss[0m : 2.55796
[1mStep[0m  [90/106], [94mLoss[0m : 2.42805
[1mStep[0m  [100/106], [94mLoss[0m : 2.20946

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46368
[1mStep[0m  [10/106], [94mLoss[0m : 2.69049
[1mStep[0m  [20/106], [94mLoss[0m : 2.32780
[1mStep[0m  [30/106], [94mLoss[0m : 2.33890
[1mStep[0m  [40/106], [94mLoss[0m : 2.69546
[1mStep[0m  [50/106], [94mLoss[0m : 2.38391
[1mStep[0m  [60/106], [94mLoss[0m : 2.36773
[1mStep[0m  [70/106], [94mLoss[0m : 2.37016
[1mStep[0m  [80/106], [94mLoss[0m : 2.28082
[1mStep[0m  [90/106], [94mLoss[0m : 2.20173
[1mStep[0m  [100/106], [94mLoss[0m : 2.47145

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73348
[1mStep[0m  [10/106], [94mLoss[0m : 2.28171
[1mStep[0m  [20/106], [94mLoss[0m : 2.77908
[1mStep[0m  [30/106], [94mLoss[0m : 2.18561
[1mStep[0m  [40/106], [94mLoss[0m : 2.69435
[1mStep[0m  [50/106], [94mLoss[0m : 2.62335
[1mStep[0m  [60/106], [94mLoss[0m : 2.40764
[1mStep[0m  [70/106], [94mLoss[0m : 2.49216
[1mStep[0m  [80/106], [94mLoss[0m : 2.42313
[1mStep[0m  [90/106], [94mLoss[0m : 2.27336
[1mStep[0m  [100/106], [94mLoss[0m : 2.49881

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48755
[1mStep[0m  [10/106], [94mLoss[0m : 2.13894
[1mStep[0m  [20/106], [94mLoss[0m : 2.80019
[1mStep[0m  [30/106], [94mLoss[0m : 2.30979
[1mStep[0m  [40/106], [94mLoss[0m : 2.38562
[1mStep[0m  [50/106], [94mLoss[0m : 2.75776
[1mStep[0m  [60/106], [94mLoss[0m : 2.48593
[1mStep[0m  [70/106], [94mLoss[0m : 2.48969
[1mStep[0m  [80/106], [94mLoss[0m : 2.67017
[1mStep[0m  [90/106], [94mLoss[0m : 2.34181
[1mStep[0m  [100/106], [94mLoss[0m : 2.23145

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15489
[1mStep[0m  [10/106], [94mLoss[0m : 2.77343
[1mStep[0m  [20/106], [94mLoss[0m : 2.57391
[1mStep[0m  [30/106], [94mLoss[0m : 2.54996
[1mStep[0m  [40/106], [94mLoss[0m : 2.41538
[1mStep[0m  [50/106], [94mLoss[0m : 2.45714
[1mStep[0m  [60/106], [94mLoss[0m : 2.48679
[1mStep[0m  [70/106], [94mLoss[0m : 2.42727
[1mStep[0m  [80/106], [94mLoss[0m : 2.57650
[1mStep[0m  [90/106], [94mLoss[0m : 2.34478
[1mStep[0m  [100/106], [94mLoss[0m : 2.51676

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38021
[1mStep[0m  [10/106], [94mLoss[0m : 2.23387
[1mStep[0m  [20/106], [94mLoss[0m : 2.41238
[1mStep[0m  [30/106], [94mLoss[0m : 2.41515
[1mStep[0m  [40/106], [94mLoss[0m : 2.36125
[1mStep[0m  [50/106], [94mLoss[0m : 2.49480
[1mStep[0m  [60/106], [94mLoss[0m : 2.48359
[1mStep[0m  [70/106], [94mLoss[0m : 2.62236
[1mStep[0m  [80/106], [94mLoss[0m : 2.53238
[1mStep[0m  [90/106], [94mLoss[0m : 2.55711
[1mStep[0m  [100/106], [94mLoss[0m : 2.41262

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63194
[1mStep[0m  [10/106], [94mLoss[0m : 2.56110
[1mStep[0m  [20/106], [94mLoss[0m : 2.35595
[1mStep[0m  [30/106], [94mLoss[0m : 2.41094
[1mStep[0m  [40/106], [94mLoss[0m : 2.43756
[1mStep[0m  [50/106], [94mLoss[0m : 2.07345
[1mStep[0m  [60/106], [94mLoss[0m : 2.18544
[1mStep[0m  [70/106], [94mLoss[0m : 2.53671
[1mStep[0m  [80/106], [94mLoss[0m : 2.46908
[1mStep[0m  [90/106], [94mLoss[0m : 2.46547
[1mStep[0m  [100/106], [94mLoss[0m : 2.52442

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42365
[1mStep[0m  [10/106], [94mLoss[0m : 2.45437
[1mStep[0m  [20/106], [94mLoss[0m : 2.42030
[1mStep[0m  [30/106], [94mLoss[0m : 2.48857
[1mStep[0m  [40/106], [94mLoss[0m : 2.53099
[1mStep[0m  [50/106], [94mLoss[0m : 2.25361
[1mStep[0m  [60/106], [94mLoss[0m : 2.26067
[1mStep[0m  [70/106], [94mLoss[0m : 2.42948
[1mStep[0m  [80/106], [94mLoss[0m : 2.52138
[1mStep[0m  [90/106], [94mLoss[0m : 2.42084
[1mStep[0m  [100/106], [94mLoss[0m : 2.54960

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28641
[1mStep[0m  [10/106], [94mLoss[0m : 2.05290
[1mStep[0m  [20/106], [94mLoss[0m : 2.36662
[1mStep[0m  [30/106], [94mLoss[0m : 2.74435
[1mStep[0m  [40/106], [94mLoss[0m : 2.69445
[1mStep[0m  [50/106], [94mLoss[0m : 2.31537
[1mStep[0m  [60/106], [94mLoss[0m : 2.21564
[1mStep[0m  [70/106], [94mLoss[0m : 2.35397
[1mStep[0m  [80/106], [94mLoss[0m : 2.48371
[1mStep[0m  [90/106], [94mLoss[0m : 2.44108
[1mStep[0m  [100/106], [94mLoss[0m : 2.42534

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25952
[1mStep[0m  [10/106], [94mLoss[0m : 2.39367
[1mStep[0m  [20/106], [94mLoss[0m : 2.24466
[1mStep[0m  [30/106], [94mLoss[0m : 2.34856
[1mStep[0m  [40/106], [94mLoss[0m : 2.47053
[1mStep[0m  [50/106], [94mLoss[0m : 2.64463
[1mStep[0m  [60/106], [94mLoss[0m : 2.54899
[1mStep[0m  [70/106], [94mLoss[0m : 2.53090
[1mStep[0m  [80/106], [94mLoss[0m : 2.29525
[1mStep[0m  [90/106], [94mLoss[0m : 2.52650
[1mStep[0m  [100/106], [94mLoss[0m : 2.41159

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77838
[1mStep[0m  [10/106], [94mLoss[0m : 2.48650
[1mStep[0m  [20/106], [94mLoss[0m : 2.48691
[1mStep[0m  [30/106], [94mLoss[0m : 2.51091
[1mStep[0m  [40/106], [94mLoss[0m : 2.32985
[1mStep[0m  [50/106], [94mLoss[0m : 2.34580
[1mStep[0m  [60/106], [94mLoss[0m : 2.40119
[1mStep[0m  [70/106], [94mLoss[0m : 2.62766
[1mStep[0m  [80/106], [94mLoss[0m : 2.35079
[1mStep[0m  [90/106], [94mLoss[0m : 2.46020
[1mStep[0m  [100/106], [94mLoss[0m : 2.37767

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48321
[1mStep[0m  [10/106], [94mLoss[0m : 2.23755
[1mStep[0m  [20/106], [94mLoss[0m : 2.41291
[1mStep[0m  [30/106], [94mLoss[0m : 2.49715
[1mStep[0m  [40/106], [94mLoss[0m : 2.32516
[1mStep[0m  [50/106], [94mLoss[0m : 2.24737
[1mStep[0m  [60/106], [94mLoss[0m : 2.56390
[1mStep[0m  [70/106], [94mLoss[0m : 2.15646
[1mStep[0m  [80/106], [94mLoss[0m : 2.49814
[1mStep[0m  [90/106], [94mLoss[0m : 2.59644
[1mStep[0m  [100/106], [94mLoss[0m : 2.77352

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57668
[1mStep[0m  [10/106], [94mLoss[0m : 2.41636
[1mStep[0m  [20/106], [94mLoss[0m : 2.47672
[1mStep[0m  [30/106], [94mLoss[0m : 2.64216
[1mStep[0m  [40/106], [94mLoss[0m : 2.63656
[1mStep[0m  [50/106], [94mLoss[0m : 2.29490
[1mStep[0m  [60/106], [94mLoss[0m : 2.53190
[1mStep[0m  [70/106], [94mLoss[0m : 2.16806
[1mStep[0m  [80/106], [94mLoss[0m : 2.37515
[1mStep[0m  [90/106], [94mLoss[0m : 2.12776
[1mStep[0m  [100/106], [94mLoss[0m : 2.58781

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45741
[1mStep[0m  [10/106], [94mLoss[0m : 2.48252
[1mStep[0m  [20/106], [94mLoss[0m : 2.38428
[1mStep[0m  [30/106], [94mLoss[0m : 2.41367
[1mStep[0m  [40/106], [94mLoss[0m : 2.44193
[1mStep[0m  [50/106], [94mLoss[0m : 2.55197
[1mStep[0m  [60/106], [94mLoss[0m : 2.52087
[1mStep[0m  [70/106], [94mLoss[0m : 2.42913
[1mStep[0m  [80/106], [94mLoss[0m : 2.83998
[1mStep[0m  [90/106], [94mLoss[0m : 2.36849
[1mStep[0m  [100/106], [94mLoss[0m : 2.52893

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.391, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38829
[1mStep[0m  [10/106], [94mLoss[0m : 2.19430
[1mStep[0m  [20/106], [94mLoss[0m : 2.60351
[1mStep[0m  [30/106], [94mLoss[0m : 2.19370
[1mStep[0m  [40/106], [94mLoss[0m : 2.13620
[1mStep[0m  [50/106], [94mLoss[0m : 2.49427
[1mStep[0m  [60/106], [94mLoss[0m : 2.58249
[1mStep[0m  [70/106], [94mLoss[0m : 2.61556
[1mStep[0m  [80/106], [94mLoss[0m : 2.56366
[1mStep[0m  [90/106], [94mLoss[0m : 2.14010
[1mStep[0m  [100/106], [94mLoss[0m : 2.17243

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.394, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53233
[1mStep[0m  [10/106], [94mLoss[0m : 2.61845
[1mStep[0m  [20/106], [94mLoss[0m : 2.60211
[1mStep[0m  [30/106], [94mLoss[0m : 2.49097
[1mStep[0m  [40/106], [94mLoss[0m : 2.24432
[1mStep[0m  [50/106], [94mLoss[0m : 2.14382
[1mStep[0m  [60/106], [94mLoss[0m : 2.23779
[1mStep[0m  [70/106], [94mLoss[0m : 2.51618
[1mStep[0m  [80/106], [94mLoss[0m : 2.32965
[1mStep[0m  [90/106], [94mLoss[0m : 2.50107
[1mStep[0m  [100/106], [94mLoss[0m : 2.29454

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.407, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33038
[1mStep[0m  [10/106], [94mLoss[0m : 2.38023
[1mStep[0m  [20/106], [94mLoss[0m : 2.40492
[1mStep[0m  [30/106], [94mLoss[0m : 2.44833
[1mStep[0m  [40/106], [94mLoss[0m : 2.88848
[1mStep[0m  [50/106], [94mLoss[0m : 2.35716
[1mStep[0m  [60/106], [94mLoss[0m : 2.54748
[1mStep[0m  [70/106], [94mLoss[0m : 2.37633
[1mStep[0m  [80/106], [94mLoss[0m : 2.31795
[1mStep[0m  [90/106], [94mLoss[0m : 2.52092
[1mStep[0m  [100/106], [94mLoss[0m : 2.46525

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52849
[1mStep[0m  [10/106], [94mLoss[0m : 2.02247
[1mStep[0m  [20/106], [94mLoss[0m : 2.35902
[1mStep[0m  [30/106], [94mLoss[0m : 2.35056
[1mStep[0m  [40/106], [94mLoss[0m : 2.54918
[1mStep[0m  [50/106], [94mLoss[0m : 2.53787
[1mStep[0m  [60/106], [94mLoss[0m : 2.64594
[1mStep[0m  [70/106], [94mLoss[0m : 2.55858
[1mStep[0m  [80/106], [94mLoss[0m : 2.43943
[1mStep[0m  [90/106], [94mLoss[0m : 2.53214
[1mStep[0m  [100/106], [94mLoss[0m : 2.32352

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.394, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65291
[1mStep[0m  [10/106], [94mLoss[0m : 2.43546
[1mStep[0m  [20/106], [94mLoss[0m : 2.58975
[1mStep[0m  [30/106], [94mLoss[0m : 2.60253
[1mStep[0m  [40/106], [94mLoss[0m : 2.44738
[1mStep[0m  [50/106], [94mLoss[0m : 2.63129
[1mStep[0m  [60/106], [94mLoss[0m : 2.49534
[1mStep[0m  [70/106], [94mLoss[0m : 2.27345
[1mStep[0m  [80/106], [94mLoss[0m : 2.72754
[1mStep[0m  [90/106], [94mLoss[0m : 2.20114
[1mStep[0m  [100/106], [94mLoss[0m : 2.73049

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.431, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48602
[1mStep[0m  [10/106], [94mLoss[0m : 2.34078
[1mStep[0m  [20/106], [94mLoss[0m : 2.29501
[1mStep[0m  [30/106], [94mLoss[0m : 2.40473
[1mStep[0m  [40/106], [94mLoss[0m : 2.53348
[1mStep[0m  [50/106], [94mLoss[0m : 2.63933
[1mStep[0m  [60/106], [94mLoss[0m : 2.36362
[1mStep[0m  [70/106], [94mLoss[0m : 2.37230
[1mStep[0m  [80/106], [94mLoss[0m : 2.58603
[1mStep[0m  [90/106], [94mLoss[0m : 2.73637
[1mStep[0m  [100/106], [94mLoss[0m : 2.30787

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.398, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40974
[1mStep[0m  [10/106], [94mLoss[0m : 2.54744
[1mStep[0m  [20/106], [94mLoss[0m : 2.39625
[1mStep[0m  [30/106], [94mLoss[0m : 2.27811
[1mStep[0m  [40/106], [94mLoss[0m : 2.06891
[1mStep[0m  [50/106], [94mLoss[0m : 2.52184
[1mStep[0m  [60/106], [94mLoss[0m : 2.32570
[1mStep[0m  [70/106], [94mLoss[0m : 2.37414
[1mStep[0m  [80/106], [94mLoss[0m : 2.71203
[1mStep[0m  [90/106], [94mLoss[0m : 2.47964
[1mStep[0m  [100/106], [94mLoss[0m : 2.59108

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.408, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35362
[1mStep[0m  [10/106], [94mLoss[0m : 2.34143
[1mStep[0m  [20/106], [94mLoss[0m : 2.43720
[1mStep[0m  [30/106], [94mLoss[0m : 2.43445
[1mStep[0m  [40/106], [94mLoss[0m : 2.67869
[1mStep[0m  [50/106], [94mLoss[0m : 2.34794
[1mStep[0m  [60/106], [94mLoss[0m : 2.48540
[1mStep[0m  [70/106], [94mLoss[0m : 2.45601
[1mStep[0m  [80/106], [94mLoss[0m : 2.64910
[1mStep[0m  [90/106], [94mLoss[0m : 2.30448
[1mStep[0m  [100/106], [94mLoss[0m : 2.31390

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52410
[1mStep[0m  [10/106], [94mLoss[0m : 2.19226
[1mStep[0m  [20/106], [94mLoss[0m : 2.50359
[1mStep[0m  [30/106], [94mLoss[0m : 2.25638
[1mStep[0m  [40/106], [94mLoss[0m : 2.39103
[1mStep[0m  [50/106], [94mLoss[0m : 2.39322
[1mStep[0m  [60/106], [94mLoss[0m : 2.82354
[1mStep[0m  [70/106], [94mLoss[0m : 2.54895
[1mStep[0m  [80/106], [94mLoss[0m : 2.46501
[1mStep[0m  [90/106], [94mLoss[0m : 2.59590
[1mStep[0m  [100/106], [94mLoss[0m : 2.65410

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.401, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.405
====================================

Phase 1 - Evaluation MAE:  2.404568888106436
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.54938
[1mStep[0m  [10/106], [94mLoss[0m : 2.51261
[1mStep[0m  [20/106], [94mLoss[0m : 2.39160
[1mStep[0m  [30/106], [94mLoss[0m : 2.28011
[1mStep[0m  [40/106], [94mLoss[0m : 2.27915
[1mStep[0m  [50/106], [94mLoss[0m : 2.23574
[1mStep[0m  [60/106], [94mLoss[0m : 2.08946
[1mStep[0m  [70/106], [94mLoss[0m : 2.42218
[1mStep[0m  [80/106], [94mLoss[0m : 2.45007
[1mStep[0m  [90/106], [94mLoss[0m : 2.67227
[1mStep[0m  [100/106], [94mLoss[0m : 2.60691

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34127
[1mStep[0m  [10/106], [94mLoss[0m : 2.11199
[1mStep[0m  [20/106], [94mLoss[0m : 2.33411
[1mStep[0m  [30/106], [94mLoss[0m : 2.33205
[1mStep[0m  [40/106], [94mLoss[0m : 2.43386
[1mStep[0m  [50/106], [94mLoss[0m : 2.31823
[1mStep[0m  [60/106], [94mLoss[0m : 2.64699
[1mStep[0m  [70/106], [94mLoss[0m : 2.12880
[1mStep[0m  [80/106], [94mLoss[0m : 2.44414
[1mStep[0m  [90/106], [94mLoss[0m : 2.16702
[1mStep[0m  [100/106], [94mLoss[0m : 2.51748

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.510, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19818
[1mStep[0m  [10/106], [94mLoss[0m : 2.30411
[1mStep[0m  [20/106], [94mLoss[0m : 2.23459
[1mStep[0m  [30/106], [94mLoss[0m : 2.31976
[1mStep[0m  [40/106], [94mLoss[0m : 2.10051
[1mStep[0m  [50/106], [94mLoss[0m : 2.26207
[1mStep[0m  [60/106], [94mLoss[0m : 2.15955
[1mStep[0m  [70/106], [94mLoss[0m : 2.27131
[1mStep[0m  [80/106], [94mLoss[0m : 2.21231
[1mStep[0m  [90/106], [94mLoss[0m : 2.22726
[1mStep[0m  [100/106], [94mLoss[0m : 2.46549

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26378
[1mStep[0m  [10/106], [94mLoss[0m : 2.08039
[1mStep[0m  [20/106], [94mLoss[0m : 2.32121
[1mStep[0m  [30/106], [94mLoss[0m : 2.45214
[1mStep[0m  [40/106], [94mLoss[0m : 2.04827
[1mStep[0m  [50/106], [94mLoss[0m : 2.05309
[1mStep[0m  [60/106], [94mLoss[0m : 2.28877
[1mStep[0m  [70/106], [94mLoss[0m : 2.12469
[1mStep[0m  [80/106], [94mLoss[0m : 2.14174
[1mStep[0m  [90/106], [94mLoss[0m : 2.35354
[1mStep[0m  [100/106], [94mLoss[0m : 2.48521

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.212, [92mTest[0m: 2.511, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36144
[1mStep[0m  [10/106], [94mLoss[0m : 1.93196
[1mStep[0m  [20/106], [94mLoss[0m : 2.13779
[1mStep[0m  [30/106], [94mLoss[0m : 2.14292
[1mStep[0m  [40/106], [94mLoss[0m : 2.14965
[1mStep[0m  [50/106], [94mLoss[0m : 2.28800
[1mStep[0m  [60/106], [94mLoss[0m : 2.33484
[1mStep[0m  [70/106], [94mLoss[0m : 2.17987
[1mStep[0m  [80/106], [94mLoss[0m : 2.21769
[1mStep[0m  [90/106], [94mLoss[0m : 2.27056
[1mStep[0m  [100/106], [94mLoss[0m : 1.92012

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92774
[1mStep[0m  [10/106], [94mLoss[0m : 1.95265
[1mStep[0m  [20/106], [94mLoss[0m : 1.74773
[1mStep[0m  [30/106], [94mLoss[0m : 2.03345
[1mStep[0m  [40/106], [94mLoss[0m : 2.14771
[1mStep[0m  [50/106], [94mLoss[0m : 2.23320
[1mStep[0m  [60/106], [94mLoss[0m : 2.20372
[1mStep[0m  [70/106], [94mLoss[0m : 2.08818
[1mStep[0m  [80/106], [94mLoss[0m : 1.99935
[1mStep[0m  [90/106], [94mLoss[0m : 2.43807
[1mStep[0m  [100/106], [94mLoss[0m : 2.29283

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.074, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06233
[1mStep[0m  [10/106], [94mLoss[0m : 2.02403
[1mStep[0m  [20/106], [94mLoss[0m : 2.08825
[1mStep[0m  [30/106], [94mLoss[0m : 2.07103
[1mStep[0m  [40/106], [94mLoss[0m : 2.37504
[1mStep[0m  [50/106], [94mLoss[0m : 1.96560
[1mStep[0m  [60/106], [94mLoss[0m : 1.72695
[1mStep[0m  [70/106], [94mLoss[0m : 1.88175
[1mStep[0m  [80/106], [94mLoss[0m : 2.00842
[1mStep[0m  [90/106], [94mLoss[0m : 1.91036
[1mStep[0m  [100/106], [94mLoss[0m : 1.88015

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86903
[1mStep[0m  [10/106], [94mLoss[0m : 1.95236
[1mStep[0m  [20/106], [94mLoss[0m : 1.66972
[1mStep[0m  [30/106], [94mLoss[0m : 1.82050
[1mStep[0m  [40/106], [94mLoss[0m : 1.84757
[1mStep[0m  [50/106], [94mLoss[0m : 2.14167
[1mStep[0m  [60/106], [94mLoss[0m : 1.91884
[1mStep[0m  [70/106], [94mLoss[0m : 2.12402
[1mStep[0m  [80/106], [94mLoss[0m : 2.05041
[1mStep[0m  [90/106], [94mLoss[0m : 1.81589
[1mStep[0m  [100/106], [94mLoss[0m : 2.27464

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.954, [92mTest[0m: 2.522, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75301
[1mStep[0m  [10/106], [94mLoss[0m : 1.90738
[1mStep[0m  [20/106], [94mLoss[0m : 1.73701
[1mStep[0m  [30/106], [94mLoss[0m : 1.90474
[1mStep[0m  [40/106], [94mLoss[0m : 1.92490
[1mStep[0m  [50/106], [94mLoss[0m : 1.69961
[1mStep[0m  [60/106], [94mLoss[0m : 2.02175
[1mStep[0m  [70/106], [94mLoss[0m : 1.81256
[1mStep[0m  [80/106], [94mLoss[0m : 1.94122
[1mStep[0m  [90/106], [94mLoss[0m : 1.80529
[1mStep[0m  [100/106], [94mLoss[0m : 1.72302

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.915, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.82817
[1mStep[0m  [10/106], [94mLoss[0m : 1.91291
[1mStep[0m  [20/106], [94mLoss[0m : 1.71607
[1mStep[0m  [30/106], [94mLoss[0m : 1.71196
[1mStep[0m  [40/106], [94mLoss[0m : 1.97781
[1mStep[0m  [50/106], [94mLoss[0m : 2.08699
[1mStep[0m  [60/106], [94mLoss[0m : 1.59718
[1mStep[0m  [70/106], [94mLoss[0m : 1.67795
[1mStep[0m  [80/106], [94mLoss[0m : 1.85847
[1mStep[0m  [90/106], [94mLoss[0m : 2.01625
[1mStep[0m  [100/106], [94mLoss[0m : 1.84857

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.68210
[1mStep[0m  [10/106], [94mLoss[0m : 1.68643
[1mStep[0m  [20/106], [94mLoss[0m : 1.86207
[1mStep[0m  [30/106], [94mLoss[0m : 1.67053
[1mStep[0m  [40/106], [94mLoss[0m : 1.76521
[1mStep[0m  [50/106], [94mLoss[0m : 1.77220
[1mStep[0m  [60/106], [94mLoss[0m : 1.85892
[1mStep[0m  [70/106], [94mLoss[0m : 1.67204
[1mStep[0m  [80/106], [94mLoss[0m : 2.04537
[1mStep[0m  [90/106], [94mLoss[0m : 1.79088
[1mStep[0m  [100/106], [94mLoss[0m : 1.76246

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.68395
[1mStep[0m  [10/106], [94mLoss[0m : 1.59233
[1mStep[0m  [20/106], [94mLoss[0m : 1.76910
[1mStep[0m  [30/106], [94mLoss[0m : 1.70623
[1mStep[0m  [40/106], [94mLoss[0m : 1.95678
[1mStep[0m  [50/106], [94mLoss[0m : 1.74760
[1mStep[0m  [60/106], [94mLoss[0m : 1.88616
[1mStep[0m  [70/106], [94mLoss[0m : 1.80298
[1mStep[0m  [80/106], [94mLoss[0m : 1.76656
[1mStep[0m  [90/106], [94mLoss[0m : 1.97591
[1mStep[0m  [100/106], [94mLoss[0m : 1.72772

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.769, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60566
[1mStep[0m  [10/106], [94mLoss[0m : 1.71748
[1mStep[0m  [20/106], [94mLoss[0m : 1.45544
[1mStep[0m  [30/106], [94mLoss[0m : 1.74398
[1mStep[0m  [40/106], [94mLoss[0m : 1.93416
[1mStep[0m  [50/106], [94mLoss[0m : 1.77516
[1mStep[0m  [60/106], [94mLoss[0m : 1.73223
[1mStep[0m  [70/106], [94mLoss[0m : 1.69966
[1mStep[0m  [80/106], [94mLoss[0m : 1.71938
[1mStep[0m  [90/106], [94mLoss[0m : 1.70322
[1mStep[0m  [100/106], [94mLoss[0m : 1.73188

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60134
[1mStep[0m  [10/106], [94mLoss[0m : 1.56000
[1mStep[0m  [20/106], [94mLoss[0m : 1.51817
[1mStep[0m  [30/106], [94mLoss[0m : 1.68255
[1mStep[0m  [40/106], [94mLoss[0m : 1.62218
[1mStep[0m  [50/106], [94mLoss[0m : 1.50005
[1mStep[0m  [60/106], [94mLoss[0m : 1.66263
[1mStep[0m  [70/106], [94mLoss[0m : 1.86020
[1mStep[0m  [80/106], [94mLoss[0m : 1.67611
[1mStep[0m  [90/106], [94mLoss[0m : 1.78432
[1mStep[0m  [100/106], [94mLoss[0m : 1.82097

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72354
[1mStep[0m  [10/106], [94mLoss[0m : 1.82812
[1mStep[0m  [20/106], [94mLoss[0m : 1.66523
[1mStep[0m  [30/106], [94mLoss[0m : 1.55469
[1mStep[0m  [40/106], [94mLoss[0m : 1.66886
[1mStep[0m  [50/106], [94mLoss[0m : 1.65178
[1mStep[0m  [60/106], [94mLoss[0m : 1.87407
[1mStep[0m  [70/106], [94mLoss[0m : 1.57760
[1mStep[0m  [80/106], [94mLoss[0m : 1.55410
[1mStep[0m  [90/106], [94mLoss[0m : 1.63339
[1mStep[0m  [100/106], [94mLoss[0m : 1.64180

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53733
[1mStep[0m  [10/106], [94mLoss[0m : 1.37188
[1mStep[0m  [20/106], [94mLoss[0m : 1.50476
[1mStep[0m  [30/106], [94mLoss[0m : 1.83413
[1mStep[0m  [40/106], [94mLoss[0m : 1.74581
[1mStep[0m  [50/106], [94mLoss[0m : 1.56429
[1mStep[0m  [60/106], [94mLoss[0m : 1.59639
[1mStep[0m  [70/106], [94mLoss[0m : 1.59171
[1mStep[0m  [80/106], [94mLoss[0m : 1.73476
[1mStep[0m  [90/106], [94mLoss[0m : 1.75627
[1mStep[0m  [100/106], [94mLoss[0m : 1.87771

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57458
[1mStep[0m  [10/106], [94mLoss[0m : 1.58079
[1mStep[0m  [20/106], [94mLoss[0m : 1.50960
[1mStep[0m  [30/106], [94mLoss[0m : 1.75810
[1mStep[0m  [40/106], [94mLoss[0m : 1.51000
[1mStep[0m  [50/106], [94mLoss[0m : 1.59451
[1mStep[0m  [60/106], [94mLoss[0m : 1.63412
[1mStep[0m  [70/106], [94mLoss[0m : 1.49476
[1mStep[0m  [80/106], [94mLoss[0m : 1.98369
[1mStep[0m  [90/106], [94mLoss[0m : 1.34305
[1mStep[0m  [100/106], [94mLoss[0m : 1.56581

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.519, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44811
[1mStep[0m  [10/106], [94mLoss[0m : 1.41978
[1mStep[0m  [20/106], [94mLoss[0m : 1.59998
[1mStep[0m  [30/106], [94mLoss[0m : 1.65153
[1mStep[0m  [40/106], [94mLoss[0m : 1.68343
[1mStep[0m  [50/106], [94mLoss[0m : 1.35437
[1mStep[0m  [60/106], [94mLoss[0m : 1.66159
[1mStep[0m  [70/106], [94mLoss[0m : 1.47879
[1mStep[0m  [80/106], [94mLoss[0m : 1.55283
[1mStep[0m  [90/106], [94mLoss[0m : 1.65161
[1mStep[0m  [100/106], [94mLoss[0m : 1.60596

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55363
[1mStep[0m  [10/106], [94mLoss[0m : 1.42712
[1mStep[0m  [20/106], [94mLoss[0m : 1.40675
[1mStep[0m  [30/106], [94mLoss[0m : 1.55397
[1mStep[0m  [40/106], [94mLoss[0m : 1.61072
[1mStep[0m  [50/106], [94mLoss[0m : 1.40532
[1mStep[0m  [60/106], [94mLoss[0m : 1.63870
[1mStep[0m  [70/106], [94mLoss[0m : 1.52941
[1mStep[0m  [80/106], [94mLoss[0m : 1.45195
[1mStep[0m  [90/106], [94mLoss[0m : 1.62927
[1mStep[0m  [100/106], [94mLoss[0m : 1.54942

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39225
[1mStep[0m  [10/106], [94mLoss[0m : 1.27917
[1mStep[0m  [20/106], [94mLoss[0m : 1.46058
[1mStep[0m  [30/106], [94mLoss[0m : 1.47017
[1mStep[0m  [40/106], [94mLoss[0m : 1.43952
[1mStep[0m  [50/106], [94mLoss[0m : 1.44763
[1mStep[0m  [60/106], [94mLoss[0m : 1.62096
[1mStep[0m  [70/106], [94mLoss[0m : 1.50715
[1mStep[0m  [80/106], [94mLoss[0m : 1.69440
[1mStep[0m  [90/106], [94mLoss[0m : 1.56199
[1mStep[0m  [100/106], [94mLoss[0m : 1.38064

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.604, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63643
[1mStep[0m  [10/106], [94mLoss[0m : 1.50433
[1mStep[0m  [20/106], [94mLoss[0m : 1.55697
[1mStep[0m  [30/106], [94mLoss[0m : 1.69791
[1mStep[0m  [40/106], [94mLoss[0m : 1.62634
[1mStep[0m  [50/106], [94mLoss[0m : 1.33221
[1mStep[0m  [60/106], [94mLoss[0m : 1.49812
[1mStep[0m  [70/106], [94mLoss[0m : 1.24931
[1mStep[0m  [80/106], [94mLoss[0m : 1.43268
[1mStep[0m  [90/106], [94mLoss[0m : 1.65569
[1mStep[0m  [100/106], [94mLoss[0m : 1.65520

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.523, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39790
[1mStep[0m  [10/106], [94mLoss[0m : 1.33272
[1mStep[0m  [20/106], [94mLoss[0m : 1.24197
[1mStep[0m  [30/106], [94mLoss[0m : 1.44502
[1mStep[0m  [40/106], [94mLoss[0m : 1.32484
[1mStep[0m  [50/106], [94mLoss[0m : 1.60935
[1mStep[0m  [60/106], [94mLoss[0m : 1.67207
[1mStep[0m  [70/106], [94mLoss[0m : 1.28222
[1mStep[0m  [80/106], [94mLoss[0m : 1.46621
[1mStep[0m  [90/106], [94mLoss[0m : 1.47152
[1mStep[0m  [100/106], [94mLoss[0m : 1.34751

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.25068
[1mStep[0m  [10/106], [94mLoss[0m : 1.33431
[1mStep[0m  [20/106], [94mLoss[0m : 1.42900
[1mStep[0m  [30/106], [94mLoss[0m : 1.34945
[1mStep[0m  [40/106], [94mLoss[0m : 1.46538
[1mStep[0m  [50/106], [94mLoss[0m : 1.33198
[1mStep[0m  [60/106], [94mLoss[0m : 1.20068
[1mStep[0m  [70/106], [94mLoss[0m : 1.38976
[1mStep[0m  [80/106], [94mLoss[0m : 1.41010
[1mStep[0m  [90/106], [94mLoss[0m : 1.32369
[1mStep[0m  [100/106], [94mLoss[0m : 1.57404

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.395, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.30502
[1mStep[0m  [10/106], [94mLoss[0m : 1.26287
[1mStep[0m  [20/106], [94mLoss[0m : 1.51881
[1mStep[0m  [30/106], [94mLoss[0m : 1.38839
[1mStep[0m  [40/106], [94mLoss[0m : 1.26647
[1mStep[0m  [50/106], [94mLoss[0m : 1.29272
[1mStep[0m  [60/106], [94mLoss[0m : 1.61291
[1mStep[0m  [70/106], [94mLoss[0m : 1.25878
[1mStep[0m  [80/106], [94mLoss[0m : 1.31035
[1mStep[0m  [90/106], [94mLoss[0m : 1.52724
[1mStep[0m  [100/106], [94mLoss[0m : 1.65794

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.383, [92mTest[0m: 2.548, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.513
====================================

Phase 2 - Evaluation MAE:  2.513437752453786
MAE score P1      2.404569
MAE score P2      2.513438
loss              1.382643
learning_rate         0.01
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 11.70929
[1mStep[0m  [5/53], [94mLoss[0m : 5.83569
[1mStep[0m  [10/53], [94mLoss[0m : 4.33291
[1mStep[0m  [15/53], [94mLoss[0m : 3.76792
[1mStep[0m  [20/53], [94mLoss[0m : 3.21235
[1mStep[0m  [25/53], [94mLoss[0m : 3.57333
[1mStep[0m  [30/53], [94mLoss[0m : 2.62508
[1mStep[0m  [35/53], [94mLoss[0m : 2.95828
[1mStep[0m  [40/53], [94mLoss[0m : 2.74985
[1mStep[0m  [45/53], [94mLoss[0m : 2.71620
[1mStep[0m  [50/53], [94mLoss[0m : 2.57807

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.839, [92mTest[0m: 11.017, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42579
[1mStep[0m  [5/53], [94mLoss[0m : 2.45419
[1mStep[0m  [10/53], [94mLoss[0m : 2.30842
[1mStep[0m  [15/53], [94mLoss[0m : 2.74303
[1mStep[0m  [20/53], [94mLoss[0m : 2.49272
[1mStep[0m  [25/53], [94mLoss[0m : 2.58675
[1mStep[0m  [30/53], [94mLoss[0m : 2.52664
[1mStep[0m  [35/53], [94mLoss[0m : 2.43835
[1mStep[0m  [40/53], [94mLoss[0m : 2.73333
[1mStep[0m  [45/53], [94mLoss[0m : 2.55506
[1mStep[0m  [50/53], [94mLoss[0m : 2.50633

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.628, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.76951
[1mStep[0m  [5/53], [94mLoss[0m : 2.36165
[1mStep[0m  [10/53], [94mLoss[0m : 2.42278
[1mStep[0m  [15/53], [94mLoss[0m : 2.40994
[1mStep[0m  [20/53], [94mLoss[0m : 2.34869
[1mStep[0m  [25/53], [94mLoss[0m : 2.42311
[1mStep[0m  [30/53], [94mLoss[0m : 2.57395
[1mStep[0m  [35/53], [94mLoss[0m : 2.51802
[1mStep[0m  [40/53], [94mLoss[0m : 2.33070
[1mStep[0m  [45/53], [94mLoss[0m : 2.45141
[1mStep[0m  [50/53], [94mLoss[0m : 2.52202

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.596, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33450
[1mStep[0m  [5/53], [94mLoss[0m : 2.40128
[1mStep[0m  [10/53], [94mLoss[0m : 2.33677
[1mStep[0m  [15/53], [94mLoss[0m : 2.59061
[1mStep[0m  [20/53], [94mLoss[0m : 2.64944
[1mStep[0m  [25/53], [94mLoss[0m : 2.44733
[1mStep[0m  [30/53], [94mLoss[0m : 2.49214
[1mStep[0m  [35/53], [94mLoss[0m : 2.15212
[1mStep[0m  [40/53], [94mLoss[0m : 2.30363
[1mStep[0m  [45/53], [94mLoss[0m : 2.26755
[1mStep[0m  [50/53], [94mLoss[0m : 2.41737

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58217
[1mStep[0m  [5/53], [94mLoss[0m : 2.42866
[1mStep[0m  [10/53], [94mLoss[0m : 2.52567
[1mStep[0m  [15/53], [94mLoss[0m : 2.40087
[1mStep[0m  [20/53], [94mLoss[0m : 2.47358
[1mStep[0m  [25/53], [94mLoss[0m : 2.26985
[1mStep[0m  [30/53], [94mLoss[0m : 2.31940
[1mStep[0m  [35/53], [94mLoss[0m : 2.44456
[1mStep[0m  [40/53], [94mLoss[0m : 2.31504
[1mStep[0m  [45/53], [94mLoss[0m : 2.42205
[1mStep[0m  [50/53], [94mLoss[0m : 2.43513

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51554
[1mStep[0m  [5/53], [94mLoss[0m : 2.58258
[1mStep[0m  [10/53], [94mLoss[0m : 2.49339
[1mStep[0m  [15/53], [94mLoss[0m : 2.21031
[1mStep[0m  [20/53], [94mLoss[0m : 2.38305
[1mStep[0m  [25/53], [94mLoss[0m : 2.39404
[1mStep[0m  [30/53], [94mLoss[0m : 2.17499
[1mStep[0m  [35/53], [94mLoss[0m : 2.15921
[1mStep[0m  [40/53], [94mLoss[0m : 2.51099
[1mStep[0m  [45/53], [94mLoss[0m : 2.51227
[1mStep[0m  [50/53], [94mLoss[0m : 2.29996

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22124
[1mStep[0m  [5/53], [94mLoss[0m : 2.48416
[1mStep[0m  [10/53], [94mLoss[0m : 2.45531
[1mStep[0m  [15/53], [94mLoss[0m : 2.29650
[1mStep[0m  [20/53], [94mLoss[0m : 2.32539
[1mStep[0m  [25/53], [94mLoss[0m : 2.09084
[1mStep[0m  [30/53], [94mLoss[0m : 2.42052
[1mStep[0m  [35/53], [94mLoss[0m : 2.27543
[1mStep[0m  [40/53], [94mLoss[0m : 2.40092
[1mStep[0m  [45/53], [94mLoss[0m : 2.35672
[1mStep[0m  [50/53], [94mLoss[0m : 2.47657

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26197
[1mStep[0m  [5/53], [94mLoss[0m : 2.36938
[1mStep[0m  [10/53], [94mLoss[0m : 2.31957
[1mStep[0m  [15/53], [94mLoss[0m : 2.45788
[1mStep[0m  [20/53], [94mLoss[0m : 2.46628
[1mStep[0m  [25/53], [94mLoss[0m : 2.16210
[1mStep[0m  [30/53], [94mLoss[0m : 2.32333
[1mStep[0m  [35/53], [94mLoss[0m : 2.27055
[1mStep[0m  [40/53], [94mLoss[0m : 2.21689
[1mStep[0m  [45/53], [94mLoss[0m : 2.31079
[1mStep[0m  [50/53], [94mLoss[0m : 2.17667

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.01205
[1mStep[0m  [5/53], [94mLoss[0m : 2.16300
[1mStep[0m  [10/53], [94mLoss[0m : 2.27040
[1mStep[0m  [15/53], [94mLoss[0m : 2.46886
[1mStep[0m  [20/53], [94mLoss[0m : 2.36155
[1mStep[0m  [25/53], [94mLoss[0m : 2.36353
[1mStep[0m  [30/53], [94mLoss[0m : 2.32686
[1mStep[0m  [35/53], [94mLoss[0m : 2.17561
[1mStep[0m  [40/53], [94mLoss[0m : 2.41677
[1mStep[0m  [45/53], [94mLoss[0m : 2.30576
[1mStep[0m  [50/53], [94mLoss[0m : 2.27652

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56057
[1mStep[0m  [5/53], [94mLoss[0m : 2.74080
[1mStep[0m  [10/53], [94mLoss[0m : 2.49082
[1mStep[0m  [15/53], [94mLoss[0m : 2.39189
[1mStep[0m  [20/53], [94mLoss[0m : 2.29810
[1mStep[0m  [25/53], [94mLoss[0m : 2.36973
[1mStep[0m  [30/53], [94mLoss[0m : 2.54165
[1mStep[0m  [35/53], [94mLoss[0m : 2.48942
[1mStep[0m  [40/53], [94mLoss[0m : 2.37275
[1mStep[0m  [45/53], [94mLoss[0m : 2.43221
[1mStep[0m  [50/53], [94mLoss[0m : 2.38865

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18225
[1mStep[0m  [5/53], [94mLoss[0m : 2.28338
[1mStep[0m  [10/53], [94mLoss[0m : 2.51503
[1mStep[0m  [15/53], [94mLoss[0m : 2.31425
[1mStep[0m  [20/53], [94mLoss[0m : 2.10261
[1mStep[0m  [25/53], [94mLoss[0m : 2.26980
[1mStep[0m  [30/53], [94mLoss[0m : 2.26998
[1mStep[0m  [35/53], [94mLoss[0m : 2.33118
[1mStep[0m  [40/53], [94mLoss[0m : 2.40886
[1mStep[0m  [45/53], [94mLoss[0m : 2.30334
[1mStep[0m  [50/53], [94mLoss[0m : 2.49082

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36868
[1mStep[0m  [5/53], [94mLoss[0m : 2.11132
[1mStep[0m  [10/53], [94mLoss[0m : 2.34684
[1mStep[0m  [15/53], [94mLoss[0m : 2.15478
[1mStep[0m  [20/53], [94mLoss[0m : 2.47930
[1mStep[0m  [25/53], [94mLoss[0m : 2.49192
[1mStep[0m  [30/53], [94mLoss[0m : 2.42995
[1mStep[0m  [35/53], [94mLoss[0m : 2.47716
[1mStep[0m  [40/53], [94mLoss[0m : 2.29721
[1mStep[0m  [45/53], [94mLoss[0m : 2.18963
[1mStep[0m  [50/53], [94mLoss[0m : 2.42884

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36669
[1mStep[0m  [5/53], [94mLoss[0m : 2.36962
[1mStep[0m  [10/53], [94mLoss[0m : 2.44334
[1mStep[0m  [15/53], [94mLoss[0m : 2.53840
[1mStep[0m  [20/53], [94mLoss[0m : 2.41227
[1mStep[0m  [25/53], [94mLoss[0m : 2.23813
[1mStep[0m  [30/53], [94mLoss[0m : 2.30930
[1mStep[0m  [35/53], [94mLoss[0m : 2.37071
[1mStep[0m  [40/53], [94mLoss[0m : 2.27961
[1mStep[0m  [45/53], [94mLoss[0m : 2.22273
[1mStep[0m  [50/53], [94mLoss[0m : 2.42220

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51637
[1mStep[0m  [5/53], [94mLoss[0m : 2.32744
[1mStep[0m  [10/53], [94mLoss[0m : 2.34431
[1mStep[0m  [15/53], [94mLoss[0m : 2.22203
[1mStep[0m  [20/53], [94mLoss[0m : 2.53397
[1mStep[0m  [25/53], [94mLoss[0m : 2.37368
[1mStep[0m  [30/53], [94mLoss[0m : 2.27404
[1mStep[0m  [35/53], [94mLoss[0m : 2.15986
[1mStep[0m  [40/53], [94mLoss[0m : 2.38587
[1mStep[0m  [45/53], [94mLoss[0m : 2.26472
[1mStep[0m  [50/53], [94mLoss[0m : 2.15092

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39762
[1mStep[0m  [5/53], [94mLoss[0m : 2.22706
[1mStep[0m  [10/53], [94mLoss[0m : 2.40274
[1mStep[0m  [15/53], [94mLoss[0m : 2.26895
[1mStep[0m  [20/53], [94mLoss[0m : 2.49180
[1mStep[0m  [25/53], [94mLoss[0m : 2.28893
[1mStep[0m  [30/53], [94mLoss[0m : 2.37767
[1mStep[0m  [35/53], [94mLoss[0m : 2.52428
[1mStep[0m  [40/53], [94mLoss[0m : 2.23055
[1mStep[0m  [45/53], [94mLoss[0m : 2.31819
[1mStep[0m  [50/53], [94mLoss[0m : 2.35150

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25917
[1mStep[0m  [5/53], [94mLoss[0m : 2.45159
[1mStep[0m  [10/53], [94mLoss[0m : 2.56502
[1mStep[0m  [15/53], [94mLoss[0m : 2.17809
[1mStep[0m  [20/53], [94mLoss[0m : 2.42415
[1mStep[0m  [25/53], [94mLoss[0m : 2.29450
[1mStep[0m  [30/53], [94mLoss[0m : 2.32455
[1mStep[0m  [35/53], [94mLoss[0m : 2.27668
[1mStep[0m  [40/53], [94mLoss[0m : 2.19600
[1mStep[0m  [45/53], [94mLoss[0m : 2.31165
[1mStep[0m  [50/53], [94mLoss[0m : 2.23170

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47691
[1mStep[0m  [5/53], [94mLoss[0m : 2.16261
[1mStep[0m  [10/53], [94mLoss[0m : 2.37470
[1mStep[0m  [15/53], [94mLoss[0m : 2.29227
[1mStep[0m  [20/53], [94mLoss[0m : 2.24911
[1mStep[0m  [25/53], [94mLoss[0m : 2.11876
[1mStep[0m  [30/53], [94mLoss[0m : 2.39745
[1mStep[0m  [35/53], [94mLoss[0m : 2.25187
[1mStep[0m  [40/53], [94mLoss[0m : 2.07587
[1mStep[0m  [45/53], [94mLoss[0m : 2.50626
[1mStep[0m  [50/53], [94mLoss[0m : 2.40733

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46237
[1mStep[0m  [5/53], [94mLoss[0m : 2.56687
[1mStep[0m  [10/53], [94mLoss[0m : 2.27211
[1mStep[0m  [15/53], [94mLoss[0m : 2.53395
[1mStep[0m  [20/53], [94mLoss[0m : 2.32254
[1mStep[0m  [25/53], [94mLoss[0m : 2.10620
[1mStep[0m  [30/53], [94mLoss[0m : 2.20633
[1mStep[0m  [35/53], [94mLoss[0m : 2.16920
[1mStep[0m  [40/53], [94mLoss[0m : 2.16412
[1mStep[0m  [45/53], [94mLoss[0m : 2.34561
[1mStep[0m  [50/53], [94mLoss[0m : 2.44509

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38621
[1mStep[0m  [5/53], [94mLoss[0m : 2.28329
[1mStep[0m  [10/53], [94mLoss[0m : 2.21602
[1mStep[0m  [15/53], [94mLoss[0m : 2.15973
[1mStep[0m  [20/53], [94mLoss[0m : 2.38904
[1mStep[0m  [25/53], [94mLoss[0m : 2.09420
[1mStep[0m  [30/53], [94mLoss[0m : 2.16111
[1mStep[0m  [35/53], [94mLoss[0m : 2.28238
[1mStep[0m  [40/53], [94mLoss[0m : 2.33731
[1mStep[0m  [45/53], [94mLoss[0m : 2.21400
[1mStep[0m  [50/53], [94mLoss[0m : 2.35047

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40386
[1mStep[0m  [5/53], [94mLoss[0m : 2.49450
[1mStep[0m  [10/53], [94mLoss[0m : 2.21856
[1mStep[0m  [15/53], [94mLoss[0m : 2.26255
[1mStep[0m  [20/53], [94mLoss[0m : 2.34526
[1mStep[0m  [25/53], [94mLoss[0m : 2.29840
[1mStep[0m  [30/53], [94mLoss[0m : 2.24398
[1mStep[0m  [35/53], [94mLoss[0m : 2.38363
[1mStep[0m  [40/53], [94mLoss[0m : 2.18107
[1mStep[0m  [45/53], [94mLoss[0m : 2.20940
[1mStep[0m  [50/53], [94mLoss[0m : 2.44230

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18990
[1mStep[0m  [5/53], [94mLoss[0m : 2.53327
[1mStep[0m  [10/53], [94mLoss[0m : 2.16782
[1mStep[0m  [15/53], [94mLoss[0m : 2.25199
[1mStep[0m  [20/53], [94mLoss[0m : 2.27771
[1mStep[0m  [25/53], [94mLoss[0m : 2.46187
[1mStep[0m  [30/53], [94mLoss[0m : 2.06275
[1mStep[0m  [35/53], [94mLoss[0m : 2.32022
[1mStep[0m  [40/53], [94mLoss[0m : 2.36973
[1mStep[0m  [45/53], [94mLoss[0m : 2.26715
[1mStep[0m  [50/53], [94mLoss[0m : 2.23636

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.385, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49448
[1mStep[0m  [5/53], [94mLoss[0m : 2.38484
[1mStep[0m  [10/53], [94mLoss[0m : 2.24598
[1mStep[0m  [15/53], [94mLoss[0m : 2.28507
[1mStep[0m  [20/53], [94mLoss[0m : 2.41910
[1mStep[0m  [25/53], [94mLoss[0m : 2.20272
[1mStep[0m  [30/53], [94mLoss[0m : 2.32544
[1mStep[0m  [35/53], [94mLoss[0m : 2.23484
[1mStep[0m  [40/53], [94mLoss[0m : 2.38708
[1mStep[0m  [45/53], [94mLoss[0m : 2.36342
[1mStep[0m  [50/53], [94mLoss[0m : 2.30373

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19327
[1mStep[0m  [5/53], [94mLoss[0m : 2.29596
[1mStep[0m  [10/53], [94mLoss[0m : 2.30625
[1mStep[0m  [15/53], [94mLoss[0m : 2.35240
[1mStep[0m  [20/53], [94mLoss[0m : 2.42409
[1mStep[0m  [25/53], [94mLoss[0m : 2.32347
[1mStep[0m  [30/53], [94mLoss[0m : 2.36118
[1mStep[0m  [35/53], [94mLoss[0m : 2.26144
[1mStep[0m  [40/53], [94mLoss[0m : 2.18839
[1mStep[0m  [45/53], [94mLoss[0m : 2.45767
[1mStep[0m  [50/53], [94mLoss[0m : 2.20530

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42269
[1mStep[0m  [5/53], [94mLoss[0m : 2.37569
[1mStep[0m  [10/53], [94mLoss[0m : 2.38341
[1mStep[0m  [15/53], [94mLoss[0m : 2.22867
[1mStep[0m  [20/53], [94mLoss[0m : 2.22819
[1mStep[0m  [25/53], [94mLoss[0m : 2.32246
[1mStep[0m  [30/53], [94mLoss[0m : 2.11506
[1mStep[0m  [35/53], [94mLoss[0m : 2.31424
[1mStep[0m  [40/53], [94mLoss[0m : 2.32282
[1mStep[0m  [45/53], [94mLoss[0m : 2.23853
[1mStep[0m  [50/53], [94mLoss[0m : 2.35788

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08856
[1mStep[0m  [5/53], [94mLoss[0m : 2.22224
[1mStep[0m  [10/53], [94mLoss[0m : 2.56952
[1mStep[0m  [15/53], [94mLoss[0m : 2.10931
[1mStep[0m  [20/53], [94mLoss[0m : 2.54942
[1mStep[0m  [25/53], [94mLoss[0m : 2.32651
[1mStep[0m  [30/53], [94mLoss[0m : 2.52370
[1mStep[0m  [35/53], [94mLoss[0m : 2.31784
[1mStep[0m  [40/53], [94mLoss[0m : 2.47229
[1mStep[0m  [45/53], [94mLoss[0m : 2.33193
[1mStep[0m  [50/53], [94mLoss[0m : 2.31059

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17004
[1mStep[0m  [5/53], [94mLoss[0m : 2.16125
[1mStep[0m  [10/53], [94mLoss[0m : 2.18713
[1mStep[0m  [15/53], [94mLoss[0m : 2.12180
[1mStep[0m  [20/53], [94mLoss[0m : 2.44762
[1mStep[0m  [25/53], [94mLoss[0m : 2.24701
[1mStep[0m  [30/53], [94mLoss[0m : 2.42452
[1mStep[0m  [35/53], [94mLoss[0m : 2.42729
[1mStep[0m  [40/53], [94mLoss[0m : 2.30885
[1mStep[0m  [45/53], [94mLoss[0m : 2.25461
[1mStep[0m  [50/53], [94mLoss[0m : 2.35701

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.10028
[1mStep[0m  [5/53], [94mLoss[0m : 2.21015
[1mStep[0m  [10/53], [94mLoss[0m : 2.22419
[1mStep[0m  [15/53], [94mLoss[0m : 2.20092
[1mStep[0m  [20/53], [94mLoss[0m : 2.41735
[1mStep[0m  [25/53], [94mLoss[0m : 2.35485
[1mStep[0m  [30/53], [94mLoss[0m : 2.20467
[1mStep[0m  [35/53], [94mLoss[0m : 2.15780
[1mStep[0m  [40/53], [94mLoss[0m : 2.21285
[1mStep[0m  [45/53], [94mLoss[0m : 2.18711
[1mStep[0m  [50/53], [94mLoss[0m : 2.22287

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.16240
[1mStep[0m  [5/53], [94mLoss[0m : 2.24583
[1mStep[0m  [10/53], [94mLoss[0m : 2.01188
[1mStep[0m  [15/53], [94mLoss[0m : 2.46718
[1mStep[0m  [20/53], [94mLoss[0m : 2.12075
[1mStep[0m  [25/53], [94mLoss[0m : 2.14283
[1mStep[0m  [30/53], [94mLoss[0m : 2.33351
[1mStep[0m  [35/53], [94mLoss[0m : 2.22820
[1mStep[0m  [40/53], [94mLoss[0m : 2.26103
[1mStep[0m  [45/53], [94mLoss[0m : 2.50928
[1mStep[0m  [50/53], [94mLoss[0m : 2.26756

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.345, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37511
[1mStep[0m  [5/53], [94mLoss[0m : 2.26173
[1mStep[0m  [10/53], [94mLoss[0m : 2.11245
[1mStep[0m  [15/53], [94mLoss[0m : 2.27863
[1mStep[0m  [20/53], [94mLoss[0m : 2.26355
[1mStep[0m  [25/53], [94mLoss[0m : 2.44729
[1mStep[0m  [30/53], [94mLoss[0m : 2.25481
[1mStep[0m  [35/53], [94mLoss[0m : 2.48071
[1mStep[0m  [40/53], [94mLoss[0m : 2.18180
[1mStep[0m  [45/53], [94mLoss[0m : 2.31981
[1mStep[0m  [50/53], [94mLoss[0m : 2.36910

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.359, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33603
[1mStep[0m  [5/53], [94mLoss[0m : 2.26758
[1mStep[0m  [10/53], [94mLoss[0m : 2.30778
[1mStep[0m  [15/53], [94mLoss[0m : 2.15523
[1mStep[0m  [20/53], [94mLoss[0m : 2.35214
[1mStep[0m  [25/53], [94mLoss[0m : 2.30140
[1mStep[0m  [30/53], [94mLoss[0m : 2.09458
[1mStep[0m  [35/53], [94mLoss[0m : 2.33019
[1mStep[0m  [40/53], [94mLoss[0m : 2.10941
[1mStep[0m  [45/53], [94mLoss[0m : 2.30442
[1mStep[0m  [50/53], [94mLoss[0m : 2.39581

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.376, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.342
====================================

Phase 1 - Evaluation MAE:  2.341803651589614
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.14882
[1mStep[0m  [5/53], [94mLoss[0m : 2.43827
[1mStep[0m  [10/53], [94mLoss[0m : 2.53963
[1mStep[0m  [15/53], [94mLoss[0m : 2.39479
[1mStep[0m  [20/53], [94mLoss[0m : 2.57371
[1mStep[0m  [25/53], [94mLoss[0m : 2.56925
[1mStep[0m  [30/53], [94mLoss[0m : 2.42051
[1mStep[0m  [35/53], [94mLoss[0m : 2.30744
[1mStep[0m  [40/53], [94mLoss[0m : 2.49803
[1mStep[0m  [45/53], [94mLoss[0m : 2.54142
[1mStep[0m  [50/53], [94mLoss[0m : 2.46426

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34557
[1mStep[0m  [5/53], [94mLoss[0m : 2.49932
[1mStep[0m  [10/53], [94mLoss[0m : 2.34411
[1mStep[0m  [15/53], [94mLoss[0m : 2.30181
[1mStep[0m  [20/53], [94mLoss[0m : 2.14097
[1mStep[0m  [25/53], [94mLoss[0m : 2.34847
[1mStep[0m  [30/53], [94mLoss[0m : 2.36277
[1mStep[0m  [35/53], [94mLoss[0m : 2.13685
[1mStep[0m  [40/53], [94mLoss[0m : 2.18480
[1mStep[0m  [45/53], [94mLoss[0m : 2.43481
[1mStep[0m  [50/53], [94mLoss[0m : 2.51442

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18267
[1mStep[0m  [5/53], [94mLoss[0m : 2.10347
[1mStep[0m  [10/53], [94mLoss[0m : 2.03387
[1mStep[0m  [15/53], [94mLoss[0m : 2.25895
[1mStep[0m  [20/53], [94mLoss[0m : 2.30713
[1mStep[0m  [25/53], [94mLoss[0m : 2.19345
[1mStep[0m  [30/53], [94mLoss[0m : 2.14386
[1mStep[0m  [35/53], [94mLoss[0m : 2.03194
[1mStep[0m  [40/53], [94mLoss[0m : 2.09139
[1mStep[0m  [45/53], [94mLoss[0m : 2.29373
[1mStep[0m  [50/53], [94mLoss[0m : 2.21358

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.98250
[1mStep[0m  [5/53], [94mLoss[0m : 2.09220
[1mStep[0m  [10/53], [94mLoss[0m : 2.03151
[1mStep[0m  [15/53], [94mLoss[0m : 2.15705
[1mStep[0m  [20/53], [94mLoss[0m : 2.02145
[1mStep[0m  [25/53], [94mLoss[0m : 2.18938
[1mStep[0m  [30/53], [94mLoss[0m : 2.25763
[1mStep[0m  [35/53], [94mLoss[0m : 2.20750
[1mStep[0m  [40/53], [94mLoss[0m : 2.20943
[1mStep[0m  [45/53], [94mLoss[0m : 2.05716
[1mStep[0m  [50/53], [94mLoss[0m : 2.06690

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.13547
[1mStep[0m  [5/53], [94mLoss[0m : 1.90416
[1mStep[0m  [10/53], [94mLoss[0m : 1.86612
[1mStep[0m  [15/53], [94mLoss[0m : 2.13152
[1mStep[0m  [20/53], [94mLoss[0m : 1.96681
[1mStep[0m  [25/53], [94mLoss[0m : 2.03745
[1mStep[0m  [30/53], [94mLoss[0m : 2.11633
[1mStep[0m  [35/53], [94mLoss[0m : 1.99923
[1mStep[0m  [40/53], [94mLoss[0m : 1.78211
[1mStep[0m  [45/53], [94mLoss[0m : 2.12159
[1mStep[0m  [50/53], [94mLoss[0m : 2.00656

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.85755
[1mStep[0m  [5/53], [94mLoss[0m : 1.80008
[1mStep[0m  [10/53], [94mLoss[0m : 1.72342
[1mStep[0m  [15/53], [94mLoss[0m : 2.02446
[1mStep[0m  [20/53], [94mLoss[0m : 1.83730
[1mStep[0m  [25/53], [94mLoss[0m : 1.92057
[1mStep[0m  [30/53], [94mLoss[0m : 2.00891
[1mStep[0m  [35/53], [94mLoss[0m : 1.95504
[1mStep[0m  [40/53], [94mLoss[0m : 1.92368
[1mStep[0m  [45/53], [94mLoss[0m : 2.04750
[1mStep[0m  [50/53], [94mLoss[0m : 2.08605

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86033
[1mStep[0m  [5/53], [94mLoss[0m : 1.72571
[1mStep[0m  [10/53], [94mLoss[0m : 1.84748
[1mStep[0m  [15/53], [94mLoss[0m : 1.83470
[1mStep[0m  [20/53], [94mLoss[0m : 1.78339
[1mStep[0m  [25/53], [94mLoss[0m : 1.89970
[1mStep[0m  [30/53], [94mLoss[0m : 1.87463
[1mStep[0m  [35/53], [94mLoss[0m : 1.93447
[1mStep[0m  [40/53], [94mLoss[0m : 1.97443
[1mStep[0m  [45/53], [94mLoss[0m : 2.02158
[1mStep[0m  [50/53], [94mLoss[0m : 2.08835

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.866, [92mTest[0m: 2.446, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63529
[1mStep[0m  [5/53], [94mLoss[0m : 1.69010
[1mStep[0m  [10/53], [94mLoss[0m : 1.66630
[1mStep[0m  [15/53], [94mLoss[0m : 1.68831
[1mStep[0m  [20/53], [94mLoss[0m : 1.75605
[1mStep[0m  [25/53], [94mLoss[0m : 1.92189
[1mStep[0m  [30/53], [94mLoss[0m : 1.71036
[1mStep[0m  [35/53], [94mLoss[0m : 1.83851
[1mStep[0m  [40/53], [94mLoss[0m : 1.77043
[1mStep[0m  [45/53], [94mLoss[0m : 1.90796
[1mStep[0m  [50/53], [94mLoss[0m : 1.81448

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.528, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54260
[1mStep[0m  [5/53], [94mLoss[0m : 1.68236
[1mStep[0m  [10/53], [94mLoss[0m : 1.77804
[1mStep[0m  [15/53], [94mLoss[0m : 1.71155
[1mStep[0m  [20/53], [94mLoss[0m : 1.81088
[1mStep[0m  [25/53], [94mLoss[0m : 1.81937
[1mStep[0m  [30/53], [94mLoss[0m : 1.71276
[1mStep[0m  [35/53], [94mLoss[0m : 1.85400
[1mStep[0m  [40/53], [94mLoss[0m : 1.55821
[1mStep[0m  [45/53], [94mLoss[0m : 1.73248
[1mStep[0m  [50/53], [94mLoss[0m : 1.74097

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66495
[1mStep[0m  [5/53], [94mLoss[0m : 1.50716
[1mStep[0m  [10/53], [94mLoss[0m : 1.78133
[1mStep[0m  [15/53], [94mLoss[0m : 1.67415
[1mStep[0m  [20/53], [94mLoss[0m : 1.65693
[1mStep[0m  [25/53], [94mLoss[0m : 1.80624
[1mStep[0m  [30/53], [94mLoss[0m : 1.75182
[1mStep[0m  [35/53], [94mLoss[0m : 1.72135
[1mStep[0m  [40/53], [94mLoss[0m : 1.57581
[1mStep[0m  [45/53], [94mLoss[0m : 1.70914
[1mStep[0m  [50/53], [94mLoss[0m : 1.57827

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.685, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.61737
[1mStep[0m  [5/53], [94mLoss[0m : 1.72112
[1mStep[0m  [10/53], [94mLoss[0m : 1.60937
[1mStep[0m  [15/53], [94mLoss[0m : 1.74098
[1mStep[0m  [20/53], [94mLoss[0m : 1.61544
[1mStep[0m  [25/53], [94mLoss[0m : 1.64026
[1mStep[0m  [30/53], [94mLoss[0m : 1.75059
[1mStep[0m  [35/53], [94mLoss[0m : 1.71753
[1mStep[0m  [40/53], [94mLoss[0m : 1.75786
[1mStep[0m  [45/53], [94mLoss[0m : 1.64456
[1mStep[0m  [50/53], [94mLoss[0m : 1.63284

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.543, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68391
[1mStep[0m  [5/53], [94mLoss[0m : 1.71617
[1mStep[0m  [10/53], [94mLoss[0m : 1.50714
[1mStep[0m  [15/53], [94mLoss[0m : 1.58493
[1mStep[0m  [20/53], [94mLoss[0m : 1.49965
[1mStep[0m  [25/53], [94mLoss[0m : 1.66586
[1mStep[0m  [30/53], [94mLoss[0m : 1.51388
[1mStep[0m  [35/53], [94mLoss[0m : 1.68415
[1mStep[0m  [40/53], [94mLoss[0m : 1.51371
[1mStep[0m  [45/53], [94mLoss[0m : 1.64582
[1mStep[0m  [50/53], [94mLoss[0m : 1.68939

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.477, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.51522
[1mStep[0m  [5/53], [94mLoss[0m : 1.54764
[1mStep[0m  [10/53], [94mLoss[0m : 1.57285
[1mStep[0m  [15/53], [94mLoss[0m : 1.77415
[1mStep[0m  [20/53], [94mLoss[0m : 1.61367
[1mStep[0m  [25/53], [94mLoss[0m : 1.57222
[1mStep[0m  [30/53], [94mLoss[0m : 1.52139
[1mStep[0m  [35/53], [94mLoss[0m : 1.81203
[1mStep[0m  [40/53], [94mLoss[0m : 1.54019
[1mStep[0m  [45/53], [94mLoss[0m : 1.74614
[1mStep[0m  [50/53], [94mLoss[0m : 1.53728

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.590, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.35628
[1mStep[0m  [5/53], [94mLoss[0m : 1.56905
[1mStep[0m  [10/53], [94mLoss[0m : 1.49017
[1mStep[0m  [15/53], [94mLoss[0m : 1.58404
[1mStep[0m  [20/53], [94mLoss[0m : 1.52209
[1mStep[0m  [25/53], [94mLoss[0m : 1.55146
[1mStep[0m  [30/53], [94mLoss[0m : 1.28870
[1mStep[0m  [35/53], [94mLoss[0m : 1.64096
[1mStep[0m  [40/53], [94mLoss[0m : 1.39429
[1mStep[0m  [45/53], [94mLoss[0m : 1.48536
[1mStep[0m  [50/53], [94mLoss[0m : 1.71189

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.534, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.42068
[1mStep[0m  [5/53], [94mLoss[0m : 1.30495
[1mStep[0m  [10/53], [94mLoss[0m : 1.49941
[1mStep[0m  [15/53], [94mLoss[0m : 1.47883
[1mStep[0m  [20/53], [94mLoss[0m : 1.58733
[1mStep[0m  [25/53], [94mLoss[0m : 1.39296
[1mStep[0m  [30/53], [94mLoss[0m : 1.46885
[1mStep[0m  [35/53], [94mLoss[0m : 1.44472
[1mStep[0m  [40/53], [94mLoss[0m : 1.66486
[1mStep[0m  [45/53], [94mLoss[0m : 1.62709
[1mStep[0m  [50/53], [94mLoss[0m : 1.61974

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.511, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.40143
[1mStep[0m  [5/53], [94mLoss[0m : 1.42133
[1mStep[0m  [10/53], [94mLoss[0m : 1.54031
[1mStep[0m  [15/53], [94mLoss[0m : 1.49472
[1mStep[0m  [20/53], [94mLoss[0m : 1.38661
[1mStep[0m  [25/53], [94mLoss[0m : 1.39624
[1mStep[0m  [30/53], [94mLoss[0m : 1.47195
[1mStep[0m  [35/53], [94mLoss[0m : 1.49473
[1mStep[0m  [40/53], [94mLoss[0m : 1.48099
[1mStep[0m  [45/53], [94mLoss[0m : 1.50190
[1mStep[0m  [50/53], [94mLoss[0m : 1.50971

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.48429
[1mStep[0m  [5/53], [94mLoss[0m : 1.34394
[1mStep[0m  [10/53], [94mLoss[0m : 1.43735
[1mStep[0m  [15/53], [94mLoss[0m : 1.48676
[1mStep[0m  [20/53], [94mLoss[0m : 1.55534
[1mStep[0m  [25/53], [94mLoss[0m : 1.37262
[1mStep[0m  [30/53], [94mLoss[0m : 1.51206
[1mStep[0m  [35/53], [94mLoss[0m : 1.38125
[1mStep[0m  [40/53], [94mLoss[0m : 1.50518
[1mStep[0m  [45/53], [94mLoss[0m : 1.33795
[1mStep[0m  [50/53], [94mLoss[0m : 1.41918

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.431, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.38231
[1mStep[0m  [5/53], [94mLoss[0m : 1.41336
[1mStep[0m  [10/53], [94mLoss[0m : 1.63467
[1mStep[0m  [15/53], [94mLoss[0m : 1.38666
[1mStep[0m  [20/53], [94mLoss[0m : 1.30934
[1mStep[0m  [25/53], [94mLoss[0m : 1.36202
[1mStep[0m  [30/53], [94mLoss[0m : 1.22189
[1mStep[0m  [35/53], [94mLoss[0m : 1.42557
[1mStep[0m  [40/53], [94mLoss[0m : 1.54713
[1mStep[0m  [45/53], [94mLoss[0m : 1.47764
[1mStep[0m  [50/53], [94mLoss[0m : 1.46091

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.414, [92mTest[0m: 2.580, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.29079
[1mStep[0m  [5/53], [94mLoss[0m : 1.29295
[1mStep[0m  [10/53], [94mLoss[0m : 1.40359
[1mStep[0m  [15/53], [94mLoss[0m : 1.25611
[1mStep[0m  [20/53], [94mLoss[0m : 1.37579
[1mStep[0m  [25/53], [94mLoss[0m : 1.53317
[1mStep[0m  [30/53], [94mLoss[0m : 1.39660
[1mStep[0m  [35/53], [94mLoss[0m : 1.32960
[1mStep[0m  [40/53], [94mLoss[0m : 1.55650
[1mStep[0m  [45/53], [94mLoss[0m : 1.40721
[1mStep[0m  [50/53], [94mLoss[0m : 1.43118

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.412, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.29883
[1mStep[0m  [5/53], [94mLoss[0m : 1.32545
[1mStep[0m  [10/53], [94mLoss[0m : 1.18984
[1mStep[0m  [15/53], [94mLoss[0m : 1.50153
[1mStep[0m  [20/53], [94mLoss[0m : 1.33735
[1mStep[0m  [25/53], [94mLoss[0m : 1.48588
[1mStep[0m  [30/53], [94mLoss[0m : 1.26573
[1mStep[0m  [35/53], [94mLoss[0m : 1.40358
[1mStep[0m  [40/53], [94mLoss[0m : 1.43336
[1mStep[0m  [45/53], [94mLoss[0m : 1.23527
[1mStep[0m  [50/53], [94mLoss[0m : 1.55187

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.376, [92mTest[0m: 2.514, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 19 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.488
====================================

Phase 2 - Evaluation MAE:  2.487828951615554
MAE score P1      2.341804
MAE score P2      2.487829
loss              1.375861
learning_rate         0.01
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.05741
[1mStep[0m  [10/106], [94mLoss[0m : 6.41926
[1mStep[0m  [20/106], [94mLoss[0m : 3.79397
[1mStep[0m  [30/106], [94mLoss[0m : 2.65914
[1mStep[0m  [40/106], [94mLoss[0m : 2.54116
[1mStep[0m  [50/106], [94mLoss[0m : 2.49116
[1mStep[0m  [60/106], [94mLoss[0m : 2.52881
[1mStep[0m  [70/106], [94mLoss[0m : 2.85013
[1mStep[0m  [80/106], [94mLoss[0m : 2.56542
[1mStep[0m  [90/106], [94mLoss[0m : 2.74540
[1mStep[0m  [100/106], [94mLoss[0m : 2.58176

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.559, [92mTest[0m: 10.649, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64997
[1mStep[0m  [10/106], [94mLoss[0m : 2.43439
[1mStep[0m  [20/106], [94mLoss[0m : 2.56330
[1mStep[0m  [30/106], [94mLoss[0m : 2.62087
[1mStep[0m  [40/106], [94mLoss[0m : 2.65320
[1mStep[0m  [50/106], [94mLoss[0m : 2.71897
[1mStep[0m  [60/106], [94mLoss[0m : 2.70355
[1mStep[0m  [70/106], [94mLoss[0m : 2.42495
[1mStep[0m  [80/106], [94mLoss[0m : 2.99708
[1mStep[0m  [90/106], [94mLoss[0m : 2.31332
[1mStep[0m  [100/106], [94mLoss[0m : 2.59874

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.699, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71933
[1mStep[0m  [10/106], [94mLoss[0m : 2.62290
[1mStep[0m  [20/106], [94mLoss[0m : 2.37402
[1mStep[0m  [30/106], [94mLoss[0m : 2.96989
[1mStep[0m  [40/106], [94mLoss[0m : 2.85876
[1mStep[0m  [50/106], [94mLoss[0m : 2.29031
[1mStep[0m  [60/106], [94mLoss[0m : 2.49185
[1mStep[0m  [70/106], [94mLoss[0m : 2.77055
[1mStep[0m  [80/106], [94mLoss[0m : 2.43192
[1mStep[0m  [90/106], [94mLoss[0m : 2.49024
[1mStep[0m  [100/106], [94mLoss[0m : 2.60411

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.636, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46798
[1mStep[0m  [10/106], [94mLoss[0m : 2.49416
[1mStep[0m  [20/106], [94mLoss[0m : 2.67151
[1mStep[0m  [30/106], [94mLoss[0m : 2.78815
[1mStep[0m  [40/106], [94mLoss[0m : 2.46788
[1mStep[0m  [50/106], [94mLoss[0m : 2.28306
[1mStep[0m  [60/106], [94mLoss[0m : 2.40774
[1mStep[0m  [70/106], [94mLoss[0m : 2.45501
[1mStep[0m  [80/106], [94mLoss[0m : 2.96358
[1mStep[0m  [90/106], [94mLoss[0m : 2.52653
[1mStep[0m  [100/106], [94mLoss[0m : 2.40680

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.598, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.97125
[1mStep[0m  [10/106], [94mLoss[0m : 2.37499
[1mStep[0m  [20/106], [94mLoss[0m : 2.47597
[1mStep[0m  [30/106], [94mLoss[0m : 2.52537
[1mStep[0m  [40/106], [94mLoss[0m : 2.19292
[1mStep[0m  [50/106], [94mLoss[0m : 2.22272
[1mStep[0m  [60/106], [94mLoss[0m : 2.56880
[1mStep[0m  [70/106], [94mLoss[0m : 2.66069
[1mStep[0m  [80/106], [94mLoss[0m : 2.40574
[1mStep[0m  [90/106], [94mLoss[0m : 2.60414
[1mStep[0m  [100/106], [94mLoss[0m : 2.45004

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.577, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59450
[1mStep[0m  [10/106], [94mLoss[0m : 2.59699
[1mStep[0m  [20/106], [94mLoss[0m : 2.53957
[1mStep[0m  [30/106], [94mLoss[0m : 2.49212
[1mStep[0m  [40/106], [94mLoss[0m : 2.72425
[1mStep[0m  [50/106], [94mLoss[0m : 2.33916
[1mStep[0m  [60/106], [94mLoss[0m : 2.31768
[1mStep[0m  [70/106], [94mLoss[0m : 2.48941
[1mStep[0m  [80/106], [94mLoss[0m : 2.33531
[1mStep[0m  [90/106], [94mLoss[0m : 2.56987
[1mStep[0m  [100/106], [94mLoss[0m : 2.59663

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.604, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38793
[1mStep[0m  [10/106], [94mLoss[0m : 2.58683
[1mStep[0m  [20/106], [94mLoss[0m : 2.40347
[1mStep[0m  [30/106], [94mLoss[0m : 2.31646
[1mStep[0m  [40/106], [94mLoss[0m : 2.19012
[1mStep[0m  [50/106], [94mLoss[0m : 2.57536
[1mStep[0m  [60/106], [94mLoss[0m : 2.39616
[1mStep[0m  [70/106], [94mLoss[0m : 2.52353
[1mStep[0m  [80/106], [94mLoss[0m : 2.23866
[1mStep[0m  [90/106], [94mLoss[0m : 2.61260
[1mStep[0m  [100/106], [94mLoss[0m : 2.59557

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32821
[1mStep[0m  [10/106], [94mLoss[0m : 2.40257
[1mStep[0m  [20/106], [94mLoss[0m : 2.34224
[1mStep[0m  [30/106], [94mLoss[0m : 2.37853
[1mStep[0m  [40/106], [94mLoss[0m : 2.32457
[1mStep[0m  [50/106], [94mLoss[0m : 2.26358
[1mStep[0m  [60/106], [94mLoss[0m : 2.44439
[1mStep[0m  [70/106], [94mLoss[0m : 2.64535
[1mStep[0m  [80/106], [94mLoss[0m : 2.79010
[1mStep[0m  [90/106], [94mLoss[0m : 2.78951
[1mStep[0m  [100/106], [94mLoss[0m : 2.60617

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.524, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39774
[1mStep[0m  [10/106], [94mLoss[0m : 2.74698
[1mStep[0m  [20/106], [94mLoss[0m : 2.58371
[1mStep[0m  [30/106], [94mLoss[0m : 2.22883
[1mStep[0m  [40/106], [94mLoss[0m : 2.47381
[1mStep[0m  [50/106], [94mLoss[0m : 2.71343
[1mStep[0m  [60/106], [94mLoss[0m : 2.72880
[1mStep[0m  [70/106], [94mLoss[0m : 2.49259
[1mStep[0m  [80/106], [94mLoss[0m : 2.33820
[1mStep[0m  [90/106], [94mLoss[0m : 2.17366
[1mStep[0m  [100/106], [94mLoss[0m : 2.66988

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.579, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29053
[1mStep[0m  [10/106], [94mLoss[0m : 2.28255
[1mStep[0m  [20/106], [94mLoss[0m : 2.50748
[1mStep[0m  [30/106], [94mLoss[0m : 2.26739
[1mStep[0m  [40/106], [94mLoss[0m : 2.89805
[1mStep[0m  [50/106], [94mLoss[0m : 2.48112
[1mStep[0m  [60/106], [94mLoss[0m : 2.62835
[1mStep[0m  [70/106], [94mLoss[0m : 2.25712
[1mStep[0m  [80/106], [94mLoss[0m : 2.59612
[1mStep[0m  [90/106], [94mLoss[0m : 2.18353
[1mStep[0m  [100/106], [94mLoss[0m : 2.52817

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.547, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54971
[1mStep[0m  [10/106], [94mLoss[0m : 2.47652
[1mStep[0m  [20/106], [94mLoss[0m : 2.07024
[1mStep[0m  [30/106], [94mLoss[0m : 2.31991
[1mStep[0m  [40/106], [94mLoss[0m : 2.37489
[1mStep[0m  [50/106], [94mLoss[0m : 2.36915
[1mStep[0m  [60/106], [94mLoss[0m : 2.53707
[1mStep[0m  [70/106], [94mLoss[0m : 2.38470
[1mStep[0m  [80/106], [94mLoss[0m : 2.57615
[1mStep[0m  [90/106], [94mLoss[0m : 2.32735
[1mStep[0m  [100/106], [94mLoss[0m : 2.52207

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32999
[1mStep[0m  [10/106], [94mLoss[0m : 2.28906
[1mStep[0m  [20/106], [94mLoss[0m : 2.36685
[1mStep[0m  [30/106], [94mLoss[0m : 2.31620
[1mStep[0m  [40/106], [94mLoss[0m : 2.43389
[1mStep[0m  [50/106], [94mLoss[0m : 2.45239
[1mStep[0m  [60/106], [94mLoss[0m : 2.39969
[1mStep[0m  [70/106], [94mLoss[0m : 2.62241
[1mStep[0m  [80/106], [94mLoss[0m : 2.54413
[1mStep[0m  [90/106], [94mLoss[0m : 2.49393
[1mStep[0m  [100/106], [94mLoss[0m : 2.34687

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25242
[1mStep[0m  [10/106], [94mLoss[0m : 2.49209
[1mStep[0m  [20/106], [94mLoss[0m : 2.70460
[1mStep[0m  [30/106], [94mLoss[0m : 2.53191
[1mStep[0m  [40/106], [94mLoss[0m : 2.63717
[1mStep[0m  [50/106], [94mLoss[0m : 2.53269
[1mStep[0m  [60/106], [94mLoss[0m : 2.65919
[1mStep[0m  [70/106], [94mLoss[0m : 2.41386
[1mStep[0m  [80/106], [94mLoss[0m : 2.57831
[1mStep[0m  [90/106], [94mLoss[0m : 2.40820
[1mStep[0m  [100/106], [94mLoss[0m : 2.15103

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52428
[1mStep[0m  [10/106], [94mLoss[0m : 2.45487
[1mStep[0m  [20/106], [94mLoss[0m : 2.62690
[1mStep[0m  [30/106], [94mLoss[0m : 2.67319
[1mStep[0m  [40/106], [94mLoss[0m : 2.91578
[1mStep[0m  [50/106], [94mLoss[0m : 2.49477
[1mStep[0m  [60/106], [94mLoss[0m : 2.43504
[1mStep[0m  [70/106], [94mLoss[0m : 2.46050
[1mStep[0m  [80/106], [94mLoss[0m : 2.39913
[1mStep[0m  [90/106], [94mLoss[0m : 2.40139
[1mStep[0m  [100/106], [94mLoss[0m : 2.61389

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.508, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34866
[1mStep[0m  [10/106], [94mLoss[0m : 2.38018
[1mStep[0m  [20/106], [94mLoss[0m : 2.61876
[1mStep[0m  [30/106], [94mLoss[0m : 2.66527
[1mStep[0m  [40/106], [94mLoss[0m : 2.63055
[1mStep[0m  [50/106], [94mLoss[0m : 2.28871
[1mStep[0m  [60/106], [94mLoss[0m : 2.20193
[1mStep[0m  [70/106], [94mLoss[0m : 2.52473
[1mStep[0m  [80/106], [94mLoss[0m : 2.36563
[1mStep[0m  [90/106], [94mLoss[0m : 2.38788
[1mStep[0m  [100/106], [94mLoss[0m : 2.35010

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37059
[1mStep[0m  [10/106], [94mLoss[0m : 2.79423
[1mStep[0m  [20/106], [94mLoss[0m : 2.84438
[1mStep[0m  [30/106], [94mLoss[0m : 2.35517
[1mStep[0m  [40/106], [94mLoss[0m : 2.42272
[1mStep[0m  [50/106], [94mLoss[0m : 2.48051
[1mStep[0m  [60/106], [94mLoss[0m : 2.34019
[1mStep[0m  [70/106], [94mLoss[0m : 2.25220
[1mStep[0m  [80/106], [94mLoss[0m : 2.09785
[1mStep[0m  [90/106], [94mLoss[0m : 2.23919
[1mStep[0m  [100/106], [94mLoss[0m : 2.26678

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31190
[1mStep[0m  [10/106], [94mLoss[0m : 2.36067
[1mStep[0m  [20/106], [94mLoss[0m : 2.61083
[1mStep[0m  [30/106], [94mLoss[0m : 2.43420
[1mStep[0m  [40/106], [94mLoss[0m : 2.25272
[1mStep[0m  [50/106], [94mLoss[0m : 2.26885
[1mStep[0m  [60/106], [94mLoss[0m : 2.34819
[1mStep[0m  [70/106], [94mLoss[0m : 2.46944
[1mStep[0m  [80/106], [94mLoss[0m : 2.38618
[1mStep[0m  [90/106], [94mLoss[0m : 2.82504
[1mStep[0m  [100/106], [94mLoss[0m : 2.44493

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18342
[1mStep[0m  [10/106], [94mLoss[0m : 2.40205
[1mStep[0m  [20/106], [94mLoss[0m : 2.36160
[1mStep[0m  [30/106], [94mLoss[0m : 2.41100
[1mStep[0m  [40/106], [94mLoss[0m : 2.24686
[1mStep[0m  [50/106], [94mLoss[0m : 2.38190
[1mStep[0m  [60/106], [94mLoss[0m : 2.74190
[1mStep[0m  [70/106], [94mLoss[0m : 2.29019
[1mStep[0m  [80/106], [94mLoss[0m : 2.33245
[1mStep[0m  [90/106], [94mLoss[0m : 2.46156
[1mStep[0m  [100/106], [94mLoss[0m : 2.32080

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53356
[1mStep[0m  [10/106], [94mLoss[0m : 2.36424
[1mStep[0m  [20/106], [94mLoss[0m : 2.56130
[1mStep[0m  [30/106], [94mLoss[0m : 2.57425
[1mStep[0m  [40/106], [94mLoss[0m : 2.27168
[1mStep[0m  [50/106], [94mLoss[0m : 2.61109
[1mStep[0m  [60/106], [94mLoss[0m : 2.41425
[1mStep[0m  [70/106], [94mLoss[0m : 2.40681
[1mStep[0m  [80/106], [94mLoss[0m : 2.22385
[1mStep[0m  [90/106], [94mLoss[0m : 2.42418
[1mStep[0m  [100/106], [94mLoss[0m : 2.27935

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35726
[1mStep[0m  [10/106], [94mLoss[0m : 2.32735
[1mStep[0m  [20/106], [94mLoss[0m : 2.45585
[1mStep[0m  [30/106], [94mLoss[0m : 2.41451
[1mStep[0m  [40/106], [94mLoss[0m : 2.53353
[1mStep[0m  [50/106], [94mLoss[0m : 2.34984
[1mStep[0m  [60/106], [94mLoss[0m : 2.62583
[1mStep[0m  [70/106], [94mLoss[0m : 2.40327
[1mStep[0m  [80/106], [94mLoss[0m : 2.72069
[1mStep[0m  [90/106], [94mLoss[0m : 2.28717
[1mStep[0m  [100/106], [94mLoss[0m : 2.56233

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.422, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64379
[1mStep[0m  [10/106], [94mLoss[0m : 2.58086
[1mStep[0m  [20/106], [94mLoss[0m : 2.30078
[1mStep[0m  [30/106], [94mLoss[0m : 2.52657
[1mStep[0m  [40/106], [94mLoss[0m : 2.11902
[1mStep[0m  [50/106], [94mLoss[0m : 2.34067
[1mStep[0m  [60/106], [94mLoss[0m : 2.25174
[1mStep[0m  [70/106], [94mLoss[0m : 2.24570
[1mStep[0m  [80/106], [94mLoss[0m : 2.25807
[1mStep[0m  [90/106], [94mLoss[0m : 2.46491
[1mStep[0m  [100/106], [94mLoss[0m : 2.39237

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.417, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56222
[1mStep[0m  [10/106], [94mLoss[0m : 2.38991
[1mStep[0m  [20/106], [94mLoss[0m : 2.35827
[1mStep[0m  [30/106], [94mLoss[0m : 2.45404
[1mStep[0m  [40/106], [94mLoss[0m : 2.72256
[1mStep[0m  [50/106], [94mLoss[0m : 2.51230
[1mStep[0m  [60/106], [94mLoss[0m : 2.51525
[1mStep[0m  [70/106], [94mLoss[0m : 2.19721
[1mStep[0m  [80/106], [94mLoss[0m : 2.47206
[1mStep[0m  [90/106], [94mLoss[0m : 2.53416
[1mStep[0m  [100/106], [94mLoss[0m : 2.40510

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.442, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34575
[1mStep[0m  [10/106], [94mLoss[0m : 1.99438
[1mStep[0m  [20/106], [94mLoss[0m : 2.46536
[1mStep[0m  [30/106], [94mLoss[0m : 2.34258
[1mStep[0m  [40/106], [94mLoss[0m : 2.23788
[1mStep[0m  [50/106], [94mLoss[0m : 2.30573
[1mStep[0m  [60/106], [94mLoss[0m : 2.46329
[1mStep[0m  [70/106], [94mLoss[0m : 2.45214
[1mStep[0m  [80/106], [94mLoss[0m : 2.31169
[1mStep[0m  [90/106], [94mLoss[0m : 2.42626
[1mStep[0m  [100/106], [94mLoss[0m : 2.28433

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.402, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28050
[1mStep[0m  [10/106], [94mLoss[0m : 2.35193
[1mStep[0m  [20/106], [94mLoss[0m : 2.63995
[1mStep[0m  [30/106], [94mLoss[0m : 2.25211
[1mStep[0m  [40/106], [94mLoss[0m : 2.37585
[1mStep[0m  [50/106], [94mLoss[0m : 2.41959
[1mStep[0m  [60/106], [94mLoss[0m : 2.22244
[1mStep[0m  [70/106], [94mLoss[0m : 2.44582
[1mStep[0m  [80/106], [94mLoss[0m : 2.41660
[1mStep[0m  [90/106], [94mLoss[0m : 2.37834
[1mStep[0m  [100/106], [94mLoss[0m : 2.53186

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55035
[1mStep[0m  [10/106], [94mLoss[0m : 2.54464
[1mStep[0m  [20/106], [94mLoss[0m : 2.28126
[1mStep[0m  [30/106], [94mLoss[0m : 2.65127
[1mStep[0m  [40/106], [94mLoss[0m : 2.59191
[1mStep[0m  [50/106], [94mLoss[0m : 2.62153
[1mStep[0m  [60/106], [94mLoss[0m : 2.63739
[1mStep[0m  [70/106], [94mLoss[0m : 2.43211
[1mStep[0m  [80/106], [94mLoss[0m : 2.47808
[1mStep[0m  [90/106], [94mLoss[0m : 2.52513
[1mStep[0m  [100/106], [94mLoss[0m : 2.53305

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.421, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43518
[1mStep[0m  [10/106], [94mLoss[0m : 2.32301
[1mStep[0m  [20/106], [94mLoss[0m : 2.50691
[1mStep[0m  [30/106], [94mLoss[0m : 3.00442
[1mStep[0m  [40/106], [94mLoss[0m : 2.45002
[1mStep[0m  [50/106], [94mLoss[0m : 2.35216
[1mStep[0m  [60/106], [94mLoss[0m : 2.17334
[1mStep[0m  [70/106], [94mLoss[0m : 2.27759
[1mStep[0m  [80/106], [94mLoss[0m : 2.20689
[1mStep[0m  [90/106], [94mLoss[0m : 2.31668
[1mStep[0m  [100/106], [94mLoss[0m : 2.09766

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38185
[1mStep[0m  [10/106], [94mLoss[0m : 2.72388
[1mStep[0m  [20/106], [94mLoss[0m : 2.26665
[1mStep[0m  [30/106], [94mLoss[0m : 2.44501
[1mStep[0m  [40/106], [94mLoss[0m : 2.18083
[1mStep[0m  [50/106], [94mLoss[0m : 2.29977
[1mStep[0m  [60/106], [94mLoss[0m : 2.48234
[1mStep[0m  [70/106], [94mLoss[0m : 2.46037
[1mStep[0m  [80/106], [94mLoss[0m : 2.53221
[1mStep[0m  [90/106], [94mLoss[0m : 2.41287
[1mStep[0m  [100/106], [94mLoss[0m : 2.28110

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.441, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46024
[1mStep[0m  [10/106], [94mLoss[0m : 2.22869
[1mStep[0m  [20/106], [94mLoss[0m : 2.24649
[1mStep[0m  [30/106], [94mLoss[0m : 2.67546
[1mStep[0m  [40/106], [94mLoss[0m : 2.18512
[1mStep[0m  [50/106], [94mLoss[0m : 2.51884
[1mStep[0m  [60/106], [94mLoss[0m : 2.22074
[1mStep[0m  [70/106], [94mLoss[0m : 2.44096
[1mStep[0m  [80/106], [94mLoss[0m : 2.34259
[1mStep[0m  [90/106], [94mLoss[0m : 2.58037
[1mStep[0m  [100/106], [94mLoss[0m : 2.58034

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.416, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19839
[1mStep[0m  [10/106], [94mLoss[0m : 2.49470
[1mStep[0m  [20/106], [94mLoss[0m : 2.33895
[1mStep[0m  [30/106], [94mLoss[0m : 2.64340
[1mStep[0m  [40/106], [94mLoss[0m : 2.23571
[1mStep[0m  [50/106], [94mLoss[0m : 2.30727
[1mStep[0m  [60/106], [94mLoss[0m : 2.28335
[1mStep[0m  [70/106], [94mLoss[0m : 2.66726
[1mStep[0m  [80/106], [94mLoss[0m : 2.10343
[1mStep[0m  [90/106], [94mLoss[0m : 2.39034
[1mStep[0m  [100/106], [94mLoss[0m : 2.43621

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.392, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26058
[1mStep[0m  [10/106], [94mLoss[0m : 2.24555
[1mStep[0m  [20/106], [94mLoss[0m : 2.39382
[1mStep[0m  [30/106], [94mLoss[0m : 2.66127
[1mStep[0m  [40/106], [94mLoss[0m : 2.32759
[1mStep[0m  [50/106], [94mLoss[0m : 2.20960
[1mStep[0m  [60/106], [94mLoss[0m : 2.21367
[1mStep[0m  [70/106], [94mLoss[0m : 2.13313
[1mStep[0m  [80/106], [94mLoss[0m : 2.42184
[1mStep[0m  [90/106], [94mLoss[0m : 2.37085
[1mStep[0m  [100/106], [94mLoss[0m : 2.28819

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.438, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.389
====================================

Phase 1 - Evaluation MAE:  2.3888024429105363
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.22499
[1mStep[0m  [10/106], [94mLoss[0m : 2.56083
[1mStep[0m  [20/106], [94mLoss[0m : 2.26831
[1mStep[0m  [30/106], [94mLoss[0m : 2.39507
[1mStep[0m  [40/106], [94mLoss[0m : 2.79516
[1mStep[0m  [50/106], [94mLoss[0m : 2.51746
[1mStep[0m  [60/106], [94mLoss[0m : 2.42623
[1mStep[0m  [70/106], [94mLoss[0m : 2.30918
[1mStep[0m  [80/106], [94mLoss[0m : 2.59694
[1mStep[0m  [90/106], [94mLoss[0m : 2.38234
[1mStep[0m  [100/106], [94mLoss[0m : 2.64489

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17746
[1mStep[0m  [10/106], [94mLoss[0m : 2.06519
[1mStep[0m  [20/106], [94mLoss[0m : 2.47247
[1mStep[0m  [30/106], [94mLoss[0m : 2.43679
[1mStep[0m  [40/106], [94mLoss[0m : 2.41039
[1mStep[0m  [50/106], [94mLoss[0m : 2.48256
[1mStep[0m  [60/106], [94mLoss[0m : 2.30735
[1mStep[0m  [70/106], [94mLoss[0m : 2.59194
[1mStep[0m  [80/106], [94mLoss[0m : 2.59652
[1mStep[0m  [90/106], [94mLoss[0m : 2.08767
[1mStep[0m  [100/106], [94mLoss[0m : 2.09717

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.534, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27393
[1mStep[0m  [10/106], [94mLoss[0m : 2.40338
[1mStep[0m  [20/106], [94mLoss[0m : 2.51859
[1mStep[0m  [30/106], [94mLoss[0m : 2.17994
[1mStep[0m  [40/106], [94mLoss[0m : 2.20869
[1mStep[0m  [50/106], [94mLoss[0m : 2.30509
[1mStep[0m  [60/106], [94mLoss[0m : 2.37258
[1mStep[0m  [70/106], [94mLoss[0m : 2.33356
[1mStep[0m  [80/106], [94mLoss[0m : 2.48550
[1mStep[0m  [90/106], [94mLoss[0m : 2.34598
[1mStep[0m  [100/106], [94mLoss[0m : 2.56609

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.550, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29989
[1mStep[0m  [10/106], [94mLoss[0m : 2.40937
[1mStep[0m  [20/106], [94mLoss[0m : 2.14294
[1mStep[0m  [30/106], [94mLoss[0m : 2.36847
[1mStep[0m  [40/106], [94mLoss[0m : 2.32147
[1mStep[0m  [50/106], [94mLoss[0m : 2.29074
[1mStep[0m  [60/106], [94mLoss[0m : 2.35959
[1mStep[0m  [70/106], [94mLoss[0m : 2.28311
[1mStep[0m  [80/106], [94mLoss[0m : 2.37955
[1mStep[0m  [90/106], [94mLoss[0m : 2.24195
[1mStep[0m  [100/106], [94mLoss[0m : 2.19708

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22049
[1mStep[0m  [10/106], [94mLoss[0m : 2.25665
[1mStep[0m  [20/106], [94mLoss[0m : 2.23543
[1mStep[0m  [30/106], [94mLoss[0m : 2.39529
[1mStep[0m  [40/106], [94mLoss[0m : 2.43095
[1mStep[0m  [50/106], [94mLoss[0m : 2.48585
[1mStep[0m  [60/106], [94mLoss[0m : 2.10021
[1mStep[0m  [70/106], [94mLoss[0m : 2.19007
[1mStep[0m  [80/106], [94mLoss[0m : 2.46321
[1mStep[0m  [90/106], [94mLoss[0m : 2.02876
[1mStep[0m  [100/106], [94mLoss[0m : 2.30568

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15763
[1mStep[0m  [10/106], [94mLoss[0m : 2.36172
[1mStep[0m  [20/106], [94mLoss[0m : 2.25706
[1mStep[0m  [30/106], [94mLoss[0m : 2.01752
[1mStep[0m  [40/106], [94mLoss[0m : 2.31323
[1mStep[0m  [50/106], [94mLoss[0m : 2.23838
[1mStep[0m  [60/106], [94mLoss[0m : 2.16340
[1mStep[0m  [70/106], [94mLoss[0m : 2.28108
[1mStep[0m  [80/106], [94mLoss[0m : 2.08981
[1mStep[0m  [90/106], [94mLoss[0m : 2.13009
[1mStep[0m  [100/106], [94mLoss[0m : 2.23562

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.165, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96737
[1mStep[0m  [10/106], [94mLoss[0m : 2.07592
[1mStep[0m  [20/106], [94mLoss[0m : 2.13867
[1mStep[0m  [30/106], [94mLoss[0m : 2.01920
[1mStep[0m  [40/106], [94mLoss[0m : 2.06314
[1mStep[0m  [50/106], [94mLoss[0m : 2.14049
[1mStep[0m  [60/106], [94mLoss[0m : 2.16823
[1mStep[0m  [70/106], [94mLoss[0m : 1.94033
[1mStep[0m  [80/106], [94mLoss[0m : 2.08488
[1mStep[0m  [90/106], [94mLoss[0m : 1.96700
[1mStep[0m  [100/106], [94mLoss[0m : 2.03684

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98372
[1mStep[0m  [10/106], [94mLoss[0m : 1.89948
[1mStep[0m  [20/106], [94mLoss[0m : 1.74787
[1mStep[0m  [30/106], [94mLoss[0m : 2.01697
[1mStep[0m  [40/106], [94mLoss[0m : 1.98530
[1mStep[0m  [50/106], [94mLoss[0m : 2.18406
[1mStep[0m  [60/106], [94mLoss[0m : 2.10199
[1mStep[0m  [70/106], [94mLoss[0m : 2.01798
[1mStep[0m  [80/106], [94mLoss[0m : 2.11384
[1mStep[0m  [90/106], [94mLoss[0m : 1.83430
[1mStep[0m  [100/106], [94mLoss[0m : 1.84376

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.067, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78003
[1mStep[0m  [10/106], [94mLoss[0m : 1.87538
[1mStep[0m  [20/106], [94mLoss[0m : 2.14547
[1mStep[0m  [30/106], [94mLoss[0m : 1.93793
[1mStep[0m  [40/106], [94mLoss[0m : 1.81367
[1mStep[0m  [50/106], [94mLoss[0m : 2.00051
[1mStep[0m  [60/106], [94mLoss[0m : 2.07158
[1mStep[0m  [70/106], [94mLoss[0m : 1.98938
[1mStep[0m  [80/106], [94mLoss[0m : 2.11376
[1mStep[0m  [90/106], [94mLoss[0m : 2.02412
[1mStep[0m  [100/106], [94mLoss[0m : 2.33858

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10446
[1mStep[0m  [10/106], [94mLoss[0m : 1.97531
[1mStep[0m  [20/106], [94mLoss[0m : 1.73981
[1mStep[0m  [30/106], [94mLoss[0m : 1.83352
[1mStep[0m  [40/106], [94mLoss[0m : 2.00712
[1mStep[0m  [50/106], [94mLoss[0m : 2.11195
[1mStep[0m  [60/106], [94mLoss[0m : 1.95057
[1mStep[0m  [70/106], [94mLoss[0m : 1.93178
[1mStep[0m  [80/106], [94mLoss[0m : 2.00306
[1mStep[0m  [90/106], [94mLoss[0m : 1.83672
[1mStep[0m  [100/106], [94mLoss[0m : 2.12461

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.961, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26596
[1mStep[0m  [10/106], [94mLoss[0m : 1.91862
[1mStep[0m  [20/106], [94mLoss[0m : 1.94761
[1mStep[0m  [30/106], [94mLoss[0m : 1.87288
[1mStep[0m  [40/106], [94mLoss[0m : 1.93165
[1mStep[0m  [50/106], [94mLoss[0m : 1.97869
[1mStep[0m  [60/106], [94mLoss[0m : 1.88788
[1mStep[0m  [70/106], [94mLoss[0m : 2.03538
[1mStep[0m  [80/106], [94mLoss[0m : 1.93683
[1mStep[0m  [90/106], [94mLoss[0m : 1.70597
[1mStep[0m  [100/106], [94mLoss[0m : 1.95446

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87800
[1mStep[0m  [10/106], [94mLoss[0m : 1.80442
[1mStep[0m  [20/106], [94mLoss[0m : 1.66667
[1mStep[0m  [30/106], [94mLoss[0m : 1.77907
[1mStep[0m  [40/106], [94mLoss[0m : 1.86501
[1mStep[0m  [50/106], [94mLoss[0m : 1.70206
[1mStep[0m  [60/106], [94mLoss[0m : 1.70246
[1mStep[0m  [70/106], [94mLoss[0m : 2.26200
[1mStep[0m  [80/106], [94mLoss[0m : 1.64917
[1mStep[0m  [90/106], [94mLoss[0m : 1.99367
[1mStep[0m  [100/106], [94mLoss[0m : 1.94079

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77697
[1mStep[0m  [10/106], [94mLoss[0m : 1.72299
[1mStep[0m  [20/106], [94mLoss[0m : 1.65291
[1mStep[0m  [30/106], [94mLoss[0m : 1.74151
[1mStep[0m  [40/106], [94mLoss[0m : 1.91409
[1mStep[0m  [50/106], [94mLoss[0m : 1.81513
[1mStep[0m  [60/106], [94mLoss[0m : 1.98206
[1mStep[0m  [70/106], [94mLoss[0m : 1.70827
[1mStep[0m  [80/106], [94mLoss[0m : 2.21987
[1mStep[0m  [90/106], [94mLoss[0m : 1.76250
[1mStep[0m  [100/106], [94mLoss[0m : 2.13622

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.864, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56934
[1mStep[0m  [10/106], [94mLoss[0m : 1.77365
[1mStep[0m  [20/106], [94mLoss[0m : 1.73230
[1mStep[0m  [30/106], [94mLoss[0m : 1.60281
[1mStep[0m  [40/106], [94mLoss[0m : 2.02601
[1mStep[0m  [50/106], [94mLoss[0m : 1.75872
[1mStep[0m  [60/106], [94mLoss[0m : 1.67168
[1mStep[0m  [70/106], [94mLoss[0m : 1.65386
[1mStep[0m  [80/106], [94mLoss[0m : 1.81777
[1mStep[0m  [90/106], [94mLoss[0m : 1.91678
[1mStep[0m  [100/106], [94mLoss[0m : 1.81789

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75463
[1mStep[0m  [10/106], [94mLoss[0m : 1.67413
[1mStep[0m  [20/106], [94mLoss[0m : 1.77080
[1mStep[0m  [30/106], [94mLoss[0m : 1.51312
[1mStep[0m  [40/106], [94mLoss[0m : 1.95008
[1mStep[0m  [50/106], [94mLoss[0m : 1.81077
[1mStep[0m  [60/106], [94mLoss[0m : 1.80717
[1mStep[0m  [70/106], [94mLoss[0m : 1.83120
[1mStep[0m  [80/106], [94mLoss[0m : 1.90874
[1mStep[0m  [90/106], [94mLoss[0m : 1.74617
[1mStep[0m  [100/106], [94mLoss[0m : 2.04786

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.84491
[1mStep[0m  [10/106], [94mLoss[0m : 1.69288
[1mStep[0m  [20/106], [94mLoss[0m : 1.67688
[1mStep[0m  [30/106], [94mLoss[0m : 1.81933
[1mStep[0m  [40/106], [94mLoss[0m : 1.66400
[1mStep[0m  [50/106], [94mLoss[0m : 1.91679
[1mStep[0m  [60/106], [94mLoss[0m : 1.53976
[1mStep[0m  [70/106], [94mLoss[0m : 1.64235
[1mStep[0m  [80/106], [94mLoss[0m : 1.70161
[1mStep[0m  [90/106], [94mLoss[0m : 1.51153
[1mStep[0m  [100/106], [94mLoss[0m : 1.94994

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.509, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56396
[1mStep[0m  [10/106], [94mLoss[0m : 1.55383
[1mStep[0m  [20/106], [94mLoss[0m : 1.81857
[1mStep[0m  [30/106], [94mLoss[0m : 1.74276
[1mStep[0m  [40/106], [94mLoss[0m : 1.72506
[1mStep[0m  [50/106], [94mLoss[0m : 1.66630
[1mStep[0m  [60/106], [94mLoss[0m : 1.85524
[1mStep[0m  [70/106], [94mLoss[0m : 1.69542
[1mStep[0m  [80/106], [94mLoss[0m : 1.87082
[1mStep[0m  [90/106], [94mLoss[0m : 1.69584
[1mStep[0m  [100/106], [94mLoss[0m : 1.70270

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.32242
[1mStep[0m  [10/106], [94mLoss[0m : 1.68924
[1mStep[0m  [20/106], [94mLoss[0m : 1.52451
[1mStep[0m  [30/106], [94mLoss[0m : 1.72198
[1mStep[0m  [40/106], [94mLoss[0m : 1.54456
[1mStep[0m  [50/106], [94mLoss[0m : 1.58172
[1mStep[0m  [60/106], [94mLoss[0m : 1.89104
[1mStep[0m  [70/106], [94mLoss[0m : 1.50401
[1mStep[0m  [80/106], [94mLoss[0m : 2.07196
[1mStep[0m  [90/106], [94mLoss[0m : 1.74121
[1mStep[0m  [100/106], [94mLoss[0m : 1.68280

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.639, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63960
[1mStep[0m  [10/106], [94mLoss[0m : 2.12478
[1mStep[0m  [20/106], [94mLoss[0m : 1.59668
[1mStep[0m  [30/106], [94mLoss[0m : 1.68186
[1mStep[0m  [40/106], [94mLoss[0m : 1.95113
[1mStep[0m  [50/106], [94mLoss[0m : 1.83444
[1mStep[0m  [60/106], [94mLoss[0m : 1.65107
[1mStep[0m  [70/106], [94mLoss[0m : 1.78813
[1mStep[0m  [80/106], [94mLoss[0m : 1.56971
[1mStep[0m  [90/106], [94mLoss[0m : 1.88388
[1mStep[0m  [100/106], [94mLoss[0m : 1.70813

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58669
[1mStep[0m  [10/106], [94mLoss[0m : 1.71715
[1mStep[0m  [20/106], [94mLoss[0m : 1.41462
[1mStep[0m  [30/106], [94mLoss[0m : 1.45729
[1mStep[0m  [40/106], [94mLoss[0m : 1.75157
[1mStep[0m  [50/106], [94mLoss[0m : 1.62718
[1mStep[0m  [60/106], [94mLoss[0m : 1.87896
[1mStep[0m  [70/106], [94mLoss[0m : 1.54633
[1mStep[0m  [80/106], [94mLoss[0m : 1.49863
[1mStep[0m  [90/106], [94mLoss[0m : 1.88652
[1mStep[0m  [100/106], [94mLoss[0m : 1.77257

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.598, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43923
[1mStep[0m  [10/106], [94mLoss[0m : 1.71984
[1mStep[0m  [20/106], [94mLoss[0m : 1.53831
[1mStep[0m  [30/106], [94mLoss[0m : 1.56073
[1mStep[0m  [40/106], [94mLoss[0m : 1.55823
[1mStep[0m  [50/106], [94mLoss[0m : 1.61565
[1mStep[0m  [60/106], [94mLoss[0m : 1.63499
[1mStep[0m  [70/106], [94mLoss[0m : 1.52906
[1mStep[0m  [80/106], [94mLoss[0m : 1.42154
[1mStep[0m  [90/106], [94mLoss[0m : 1.69111
[1mStep[0m  [100/106], [94mLoss[0m : 1.59276

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.548, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57425
[1mStep[0m  [10/106], [94mLoss[0m : 1.47904
[1mStep[0m  [20/106], [94mLoss[0m : 1.46889
[1mStep[0m  [30/106], [94mLoss[0m : 1.52377
[1mStep[0m  [40/106], [94mLoss[0m : 1.56536
[1mStep[0m  [50/106], [94mLoss[0m : 1.79420
[1mStep[0m  [60/106], [94mLoss[0m : 1.54902
[1mStep[0m  [70/106], [94mLoss[0m : 1.82707
[1mStep[0m  [80/106], [94mLoss[0m : 1.66427
[1mStep[0m  [90/106], [94mLoss[0m : 1.68242
[1mStep[0m  [100/106], [94mLoss[0m : 1.48649

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.563, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73977
[1mStep[0m  [10/106], [94mLoss[0m : 1.55892
[1mStep[0m  [20/106], [94mLoss[0m : 1.54667
[1mStep[0m  [30/106], [94mLoss[0m : 1.55779
[1mStep[0m  [40/106], [94mLoss[0m : 1.70373
[1mStep[0m  [50/106], [94mLoss[0m : 1.47497
[1mStep[0m  [60/106], [94mLoss[0m : 1.64369
[1mStep[0m  [70/106], [94mLoss[0m : 1.58503
[1mStep[0m  [80/106], [94mLoss[0m : 1.51756
[1mStep[0m  [90/106], [94mLoss[0m : 1.37357
[1mStep[0m  [100/106], [94mLoss[0m : 1.65518

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.542, [92mTest[0m: 2.512, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.47596
[1mStep[0m  [10/106], [94mLoss[0m : 1.59188
[1mStep[0m  [20/106], [94mLoss[0m : 1.50643
[1mStep[0m  [30/106], [94mLoss[0m : 1.53590
[1mStep[0m  [40/106], [94mLoss[0m : 1.45875
[1mStep[0m  [50/106], [94mLoss[0m : 1.56110
[1mStep[0m  [60/106], [94mLoss[0m : 1.50231
[1mStep[0m  [70/106], [94mLoss[0m : 1.30301
[1mStep[0m  [80/106], [94mLoss[0m : 1.46203
[1mStep[0m  [90/106], [94mLoss[0m : 1.40251
[1mStep[0m  [100/106], [94mLoss[0m : 1.49520

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.531, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52741
[1mStep[0m  [10/106], [94mLoss[0m : 1.83898
[1mStep[0m  [20/106], [94mLoss[0m : 1.46933
[1mStep[0m  [30/106], [94mLoss[0m : 1.55511
[1mStep[0m  [40/106], [94mLoss[0m : 1.59730
[1mStep[0m  [50/106], [94mLoss[0m : 1.62571
[1mStep[0m  [60/106], [94mLoss[0m : 1.64401
[1mStep[0m  [70/106], [94mLoss[0m : 1.45596
[1mStep[0m  [80/106], [94mLoss[0m : 1.55479
[1mStep[0m  [90/106], [94mLoss[0m : 1.42923
[1mStep[0m  [100/106], [94mLoss[0m : 1.36195

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.469, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57112
[1mStep[0m  [10/106], [94mLoss[0m : 1.47918
[1mStep[0m  [20/106], [94mLoss[0m : 1.55388
[1mStep[0m  [30/106], [94mLoss[0m : 1.42449
[1mStep[0m  [40/106], [94mLoss[0m : 1.37270
[1mStep[0m  [50/106], [94mLoss[0m : 1.59564
[1mStep[0m  [60/106], [94mLoss[0m : 1.59884
[1mStep[0m  [70/106], [94mLoss[0m : 1.64400
[1mStep[0m  [80/106], [94mLoss[0m : 1.69828
[1mStep[0m  [90/106], [94mLoss[0m : 1.48125
[1mStep[0m  [100/106], [94mLoss[0m : 1.64016

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.35511
[1mStep[0m  [10/106], [94mLoss[0m : 1.64179
[1mStep[0m  [20/106], [94mLoss[0m : 1.64130
[1mStep[0m  [30/106], [94mLoss[0m : 1.27956
[1mStep[0m  [40/106], [94mLoss[0m : 1.51475
[1mStep[0m  [50/106], [94mLoss[0m : 1.44739
[1mStep[0m  [60/106], [94mLoss[0m : 1.52181
[1mStep[0m  [70/106], [94mLoss[0m : 1.46666
[1mStep[0m  [80/106], [94mLoss[0m : 1.56387
[1mStep[0m  [90/106], [94mLoss[0m : 1.52387
[1mStep[0m  [100/106], [94mLoss[0m : 1.68755

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.492, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.38651
[1mStep[0m  [10/106], [94mLoss[0m : 1.32364
[1mStep[0m  [20/106], [94mLoss[0m : 1.51699
[1mStep[0m  [30/106], [94mLoss[0m : 1.39046
[1mStep[0m  [40/106], [94mLoss[0m : 1.34327
[1mStep[0m  [50/106], [94mLoss[0m : 1.41036
[1mStep[0m  [60/106], [94mLoss[0m : 1.45958
[1mStep[0m  [70/106], [94mLoss[0m : 1.47614
[1mStep[0m  [80/106], [94mLoss[0m : 1.35342
[1mStep[0m  [90/106], [94mLoss[0m : 1.42269
[1mStep[0m  [100/106], [94mLoss[0m : 1.45835

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.462, [92mTest[0m: 2.582, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72171
[1mStep[0m  [10/106], [94mLoss[0m : 1.63062
[1mStep[0m  [20/106], [94mLoss[0m : 1.39417
[1mStep[0m  [30/106], [94mLoss[0m : 1.49692
[1mStep[0m  [40/106], [94mLoss[0m : 1.61564
[1mStep[0m  [50/106], [94mLoss[0m : 1.63615
[1mStep[0m  [60/106], [94mLoss[0m : 1.49259
[1mStep[0m  [70/106], [94mLoss[0m : 1.42698
[1mStep[0m  [80/106], [94mLoss[0m : 1.44904
[1mStep[0m  [90/106], [94mLoss[0m : 1.43868
[1mStep[0m  [100/106], [94mLoss[0m : 1.41451

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.47084
[1mStep[0m  [10/106], [94mLoss[0m : 1.53472
[1mStep[0m  [20/106], [94mLoss[0m : 1.58126
[1mStep[0m  [30/106], [94mLoss[0m : 1.35840
[1mStep[0m  [40/106], [94mLoss[0m : 1.19809
[1mStep[0m  [50/106], [94mLoss[0m : 1.56753
[1mStep[0m  [60/106], [94mLoss[0m : 1.43293
[1mStep[0m  [70/106], [94mLoss[0m : 1.28118
[1mStep[0m  [80/106], [94mLoss[0m : 1.44342
[1mStep[0m  [90/106], [94mLoss[0m : 1.18967
[1mStep[0m  [100/106], [94mLoss[0m : 1.63635

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.524
====================================

Phase 2 - Evaluation MAE:  2.524260385981146
MAE score P1      2.388802
MAE score P2       2.52426
loss              1.423114
learning_rate         0.01
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.98453
[1mStep[0m  [10/106], [94mLoss[0m : 10.37433
[1mStep[0m  [20/106], [94mLoss[0m : 10.67290
[1mStep[0m  [30/106], [94mLoss[0m : 9.74124
[1mStep[0m  [40/106], [94mLoss[0m : 10.40938
[1mStep[0m  [50/106], [94mLoss[0m : 9.80595
[1mStep[0m  [60/106], [94mLoss[0m : 9.67689
[1mStep[0m  [70/106], [94mLoss[0m : 9.69847
[1mStep[0m  [80/106], [94mLoss[0m : 9.50091
[1mStep[0m  [90/106], [94mLoss[0m : 9.32695
[1mStep[0m  [100/106], [94mLoss[0m : 9.30317

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.103, [92mTest[0m: 10.933, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.56669
[1mStep[0m  [10/106], [94mLoss[0m : 9.43240
[1mStep[0m  [20/106], [94mLoss[0m : 9.05389
[1mStep[0m  [30/106], [94mLoss[0m : 8.09509
[1mStep[0m  [40/106], [94mLoss[0m : 8.24866
[1mStep[0m  [50/106], [94mLoss[0m : 8.68713
[1mStep[0m  [60/106], [94mLoss[0m : 8.30483
[1mStep[0m  [70/106], [94mLoss[0m : 7.51039
[1mStep[0m  [80/106], [94mLoss[0m : 7.39212
[1mStep[0m  [90/106], [94mLoss[0m : 7.73852
[1mStep[0m  [100/106], [94mLoss[0m : 6.88524

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.151, [92mTest[0m: 9.086, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.53946
[1mStep[0m  [10/106], [94mLoss[0m : 5.94113
[1mStep[0m  [20/106], [94mLoss[0m : 6.02807
[1mStep[0m  [30/106], [94mLoss[0m : 6.11131
[1mStep[0m  [40/106], [94mLoss[0m : 6.06158
[1mStep[0m  [50/106], [94mLoss[0m : 5.18847
[1mStep[0m  [60/106], [94mLoss[0m : 5.29865
[1mStep[0m  [70/106], [94mLoss[0m : 5.42530
[1mStep[0m  [80/106], [94mLoss[0m : 4.39009
[1mStep[0m  [90/106], [94mLoss[0m : 4.38688
[1mStep[0m  [100/106], [94mLoss[0m : 4.25736

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.385, [92mTest[0m: 6.140, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.75482
[1mStep[0m  [10/106], [94mLoss[0m : 3.68131
[1mStep[0m  [20/106], [94mLoss[0m : 3.60826
[1mStep[0m  [30/106], [94mLoss[0m : 3.58796
[1mStep[0m  [40/106], [94mLoss[0m : 3.76761
[1mStep[0m  [50/106], [94mLoss[0m : 3.72312
[1mStep[0m  [60/106], [94mLoss[0m : 2.98912
[1mStep[0m  [70/106], [94mLoss[0m : 2.70220
[1mStep[0m  [80/106], [94mLoss[0m : 3.24152
[1mStep[0m  [90/106], [94mLoss[0m : 2.54824
[1mStep[0m  [100/106], [94mLoss[0m : 3.18761

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.347, [92mTest[0m: 3.565, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75630
[1mStep[0m  [10/106], [94mLoss[0m : 2.52767
[1mStep[0m  [20/106], [94mLoss[0m : 2.87265
[1mStep[0m  [30/106], [94mLoss[0m : 2.67106
[1mStep[0m  [40/106], [94mLoss[0m : 2.61364
[1mStep[0m  [50/106], [94mLoss[0m : 3.15232
[1mStep[0m  [60/106], [94mLoss[0m : 2.94466
[1mStep[0m  [70/106], [94mLoss[0m : 3.04723
[1mStep[0m  [80/106], [94mLoss[0m : 2.73061
[1mStep[0m  [90/106], [94mLoss[0m : 2.78419
[1mStep[0m  [100/106], [94mLoss[0m : 2.94190

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.790, [92mTest[0m: 2.608, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.82366
[1mStep[0m  [10/106], [94mLoss[0m : 2.61800
[1mStep[0m  [20/106], [94mLoss[0m : 2.98408
[1mStep[0m  [30/106], [94mLoss[0m : 2.81946
[1mStep[0m  [40/106], [94mLoss[0m : 2.93835
[1mStep[0m  [50/106], [94mLoss[0m : 3.06318
[1mStep[0m  [60/106], [94mLoss[0m : 2.84383
[1mStep[0m  [70/106], [94mLoss[0m : 2.78446
[1mStep[0m  [80/106], [94mLoss[0m : 2.78418
[1mStep[0m  [90/106], [94mLoss[0m : 2.69876
[1mStep[0m  [100/106], [94mLoss[0m : 2.86920

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.730, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71361
[1mStep[0m  [10/106], [94mLoss[0m : 2.71587
[1mStep[0m  [20/106], [94mLoss[0m : 2.55553
[1mStep[0m  [30/106], [94mLoss[0m : 2.64001
[1mStep[0m  [40/106], [94mLoss[0m : 2.92100
[1mStep[0m  [50/106], [94mLoss[0m : 2.71626
[1mStep[0m  [60/106], [94mLoss[0m : 2.70014
[1mStep[0m  [70/106], [94mLoss[0m : 2.48584
[1mStep[0m  [80/106], [94mLoss[0m : 2.69894
[1mStep[0m  [90/106], [94mLoss[0m : 2.63066
[1mStep[0m  [100/106], [94mLoss[0m : 2.68248

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51034
[1mStep[0m  [10/106], [94mLoss[0m : 2.71911
[1mStep[0m  [20/106], [94mLoss[0m : 2.73020
[1mStep[0m  [30/106], [94mLoss[0m : 2.67283
[1mStep[0m  [40/106], [94mLoss[0m : 2.45580
[1mStep[0m  [50/106], [94mLoss[0m : 2.83213
[1mStep[0m  [60/106], [94mLoss[0m : 2.40431
[1mStep[0m  [70/106], [94mLoss[0m : 2.13504
[1mStep[0m  [80/106], [94mLoss[0m : 2.75899
[1mStep[0m  [90/106], [94mLoss[0m : 2.86342
[1mStep[0m  [100/106], [94mLoss[0m : 2.77685

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75850
[1mStep[0m  [10/106], [94mLoss[0m : 2.76357
[1mStep[0m  [20/106], [94mLoss[0m : 2.63982
[1mStep[0m  [30/106], [94mLoss[0m : 2.87434
[1mStep[0m  [40/106], [94mLoss[0m : 2.51123
[1mStep[0m  [50/106], [94mLoss[0m : 2.76454
[1mStep[0m  [60/106], [94mLoss[0m : 2.60332
[1mStep[0m  [70/106], [94mLoss[0m : 2.95388
[1mStep[0m  [80/106], [94mLoss[0m : 2.65279
[1mStep[0m  [90/106], [94mLoss[0m : 2.42825
[1mStep[0m  [100/106], [94mLoss[0m : 2.65271

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53974
[1mStep[0m  [10/106], [94mLoss[0m : 2.57678
[1mStep[0m  [20/106], [94mLoss[0m : 2.78714
[1mStep[0m  [30/106], [94mLoss[0m : 2.42271
[1mStep[0m  [40/106], [94mLoss[0m : 2.64068
[1mStep[0m  [50/106], [94mLoss[0m : 2.53711
[1mStep[0m  [60/106], [94mLoss[0m : 2.43502
[1mStep[0m  [70/106], [94mLoss[0m : 2.40744
[1mStep[0m  [80/106], [94mLoss[0m : 2.34482
[1mStep[0m  [90/106], [94mLoss[0m : 2.80101
[1mStep[0m  [100/106], [94mLoss[0m : 2.79636

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55703
[1mStep[0m  [10/106], [94mLoss[0m : 2.88496
[1mStep[0m  [20/106], [94mLoss[0m : 2.45135
[1mStep[0m  [30/106], [94mLoss[0m : 2.55854
[1mStep[0m  [40/106], [94mLoss[0m : 3.01690
[1mStep[0m  [50/106], [94mLoss[0m : 2.77506
[1mStep[0m  [60/106], [94mLoss[0m : 2.54803
[1mStep[0m  [70/106], [94mLoss[0m : 2.30144
[1mStep[0m  [80/106], [94mLoss[0m : 2.77953
[1mStep[0m  [90/106], [94mLoss[0m : 2.55027
[1mStep[0m  [100/106], [94mLoss[0m : 2.58142

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55848
[1mStep[0m  [10/106], [94mLoss[0m : 2.41951
[1mStep[0m  [20/106], [94mLoss[0m : 2.81858
[1mStep[0m  [30/106], [94mLoss[0m : 2.59301
[1mStep[0m  [40/106], [94mLoss[0m : 2.60996
[1mStep[0m  [50/106], [94mLoss[0m : 2.62976
[1mStep[0m  [60/106], [94mLoss[0m : 2.58857
[1mStep[0m  [70/106], [94mLoss[0m : 2.74291
[1mStep[0m  [80/106], [94mLoss[0m : 2.73070
[1mStep[0m  [90/106], [94mLoss[0m : 2.47231
[1mStep[0m  [100/106], [94mLoss[0m : 2.68458

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56407
[1mStep[0m  [10/106], [94mLoss[0m : 2.39048
[1mStep[0m  [20/106], [94mLoss[0m : 2.92250
[1mStep[0m  [30/106], [94mLoss[0m : 2.35549
[1mStep[0m  [40/106], [94mLoss[0m : 2.45276
[1mStep[0m  [50/106], [94mLoss[0m : 2.76374
[1mStep[0m  [60/106], [94mLoss[0m : 2.50801
[1mStep[0m  [70/106], [94mLoss[0m : 2.57646
[1mStep[0m  [80/106], [94mLoss[0m : 2.22358
[1mStep[0m  [90/106], [94mLoss[0m : 2.54281
[1mStep[0m  [100/106], [94mLoss[0m : 2.73102

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74522
[1mStep[0m  [10/106], [94mLoss[0m : 2.88898
[1mStep[0m  [20/106], [94mLoss[0m : 2.41911
[1mStep[0m  [30/106], [94mLoss[0m : 2.48031
[1mStep[0m  [40/106], [94mLoss[0m : 2.69174
[1mStep[0m  [50/106], [94mLoss[0m : 2.53147
[1mStep[0m  [60/106], [94mLoss[0m : 2.61288
[1mStep[0m  [70/106], [94mLoss[0m : 2.43765
[1mStep[0m  [80/106], [94mLoss[0m : 2.43905
[1mStep[0m  [90/106], [94mLoss[0m : 2.47916
[1mStep[0m  [100/106], [94mLoss[0m : 2.39724

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84684
[1mStep[0m  [10/106], [94mLoss[0m : 2.43438
[1mStep[0m  [20/106], [94mLoss[0m : 2.33550
[1mStep[0m  [30/106], [94mLoss[0m : 2.53123
[1mStep[0m  [40/106], [94mLoss[0m : 2.44405
[1mStep[0m  [50/106], [94mLoss[0m : 2.59702
[1mStep[0m  [60/106], [94mLoss[0m : 2.41544
[1mStep[0m  [70/106], [94mLoss[0m : 2.41955
[1mStep[0m  [80/106], [94mLoss[0m : 2.76159
[1mStep[0m  [90/106], [94mLoss[0m : 2.73773
[1mStep[0m  [100/106], [94mLoss[0m : 2.38172

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39043
[1mStep[0m  [10/106], [94mLoss[0m : 2.42332
[1mStep[0m  [20/106], [94mLoss[0m : 2.87897
[1mStep[0m  [30/106], [94mLoss[0m : 2.56479
[1mStep[0m  [40/106], [94mLoss[0m : 2.66166
[1mStep[0m  [50/106], [94mLoss[0m : 2.37545
[1mStep[0m  [60/106], [94mLoss[0m : 2.68375
[1mStep[0m  [70/106], [94mLoss[0m : 2.48165
[1mStep[0m  [80/106], [94mLoss[0m : 2.46429
[1mStep[0m  [90/106], [94mLoss[0m : 2.79796
[1mStep[0m  [100/106], [94mLoss[0m : 2.89058

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21388
[1mStep[0m  [10/106], [94mLoss[0m : 2.41809
[1mStep[0m  [20/106], [94mLoss[0m : 2.51645
[1mStep[0m  [30/106], [94mLoss[0m : 2.42035
[1mStep[0m  [40/106], [94mLoss[0m : 2.47965
[1mStep[0m  [50/106], [94mLoss[0m : 2.53599
[1mStep[0m  [60/106], [94mLoss[0m : 2.71222
[1mStep[0m  [70/106], [94mLoss[0m : 2.72154
[1mStep[0m  [80/106], [94mLoss[0m : 2.48191
[1mStep[0m  [90/106], [94mLoss[0m : 2.60484
[1mStep[0m  [100/106], [94mLoss[0m : 2.45200

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53617
[1mStep[0m  [10/106], [94mLoss[0m : 2.70222
[1mStep[0m  [20/106], [94mLoss[0m : 2.66542
[1mStep[0m  [30/106], [94mLoss[0m : 2.65492
[1mStep[0m  [40/106], [94mLoss[0m : 2.68427
[1mStep[0m  [50/106], [94mLoss[0m : 2.46857
[1mStep[0m  [60/106], [94mLoss[0m : 2.68314
[1mStep[0m  [70/106], [94mLoss[0m : 2.58995
[1mStep[0m  [80/106], [94mLoss[0m : 2.25601
[1mStep[0m  [90/106], [94mLoss[0m : 2.47779
[1mStep[0m  [100/106], [94mLoss[0m : 2.30482

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29994
[1mStep[0m  [10/106], [94mLoss[0m : 2.58603
[1mStep[0m  [20/106], [94mLoss[0m : 2.80255
[1mStep[0m  [30/106], [94mLoss[0m : 2.73176
[1mStep[0m  [40/106], [94mLoss[0m : 2.24438
[1mStep[0m  [50/106], [94mLoss[0m : 2.17972
[1mStep[0m  [60/106], [94mLoss[0m : 2.57430
[1mStep[0m  [70/106], [94mLoss[0m : 2.49671
[1mStep[0m  [80/106], [94mLoss[0m : 2.87516
[1mStep[0m  [90/106], [94mLoss[0m : 2.85643
[1mStep[0m  [100/106], [94mLoss[0m : 2.45144

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76081
[1mStep[0m  [10/106], [94mLoss[0m : 2.25286
[1mStep[0m  [20/106], [94mLoss[0m : 2.53268
[1mStep[0m  [30/106], [94mLoss[0m : 2.42703
[1mStep[0m  [40/106], [94mLoss[0m : 2.59015
[1mStep[0m  [50/106], [94mLoss[0m : 2.49629
[1mStep[0m  [60/106], [94mLoss[0m : 2.45400
[1mStep[0m  [70/106], [94mLoss[0m : 2.62156
[1mStep[0m  [80/106], [94mLoss[0m : 2.52615
[1mStep[0m  [90/106], [94mLoss[0m : 2.51681
[1mStep[0m  [100/106], [94mLoss[0m : 2.47221

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.400, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44598
[1mStep[0m  [10/106], [94mLoss[0m : 2.27379
[1mStep[0m  [20/106], [94mLoss[0m : 2.42021
[1mStep[0m  [30/106], [94mLoss[0m : 2.37713
[1mStep[0m  [40/106], [94mLoss[0m : 2.56186
[1mStep[0m  [50/106], [94mLoss[0m : 2.57869
[1mStep[0m  [60/106], [94mLoss[0m : 2.48042
[1mStep[0m  [70/106], [94mLoss[0m : 2.65984
[1mStep[0m  [80/106], [94mLoss[0m : 2.51962
[1mStep[0m  [90/106], [94mLoss[0m : 2.65000
[1mStep[0m  [100/106], [94mLoss[0m : 2.26083

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62185
[1mStep[0m  [10/106], [94mLoss[0m : 2.74500
[1mStep[0m  [20/106], [94mLoss[0m : 2.52216
[1mStep[0m  [30/106], [94mLoss[0m : 2.45466
[1mStep[0m  [40/106], [94mLoss[0m : 2.67583
[1mStep[0m  [50/106], [94mLoss[0m : 2.50071
[1mStep[0m  [60/106], [94mLoss[0m : 2.14051
[1mStep[0m  [70/106], [94mLoss[0m : 2.40532
[1mStep[0m  [80/106], [94mLoss[0m : 2.68933
[1mStep[0m  [90/106], [94mLoss[0m : 2.51972
[1mStep[0m  [100/106], [94mLoss[0m : 2.62077

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.394, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74475
[1mStep[0m  [10/106], [94mLoss[0m : 2.50315
[1mStep[0m  [20/106], [94mLoss[0m : 2.42040
[1mStep[0m  [30/106], [94mLoss[0m : 2.50374
[1mStep[0m  [40/106], [94mLoss[0m : 2.20354
[1mStep[0m  [50/106], [94mLoss[0m : 2.60607
[1mStep[0m  [60/106], [94mLoss[0m : 2.38755
[1mStep[0m  [70/106], [94mLoss[0m : 2.54569
[1mStep[0m  [80/106], [94mLoss[0m : 2.62776
[1mStep[0m  [90/106], [94mLoss[0m : 2.66830
[1mStep[0m  [100/106], [94mLoss[0m : 2.48083

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61424
[1mStep[0m  [10/106], [94mLoss[0m : 2.25319
[1mStep[0m  [20/106], [94mLoss[0m : 2.58012
[1mStep[0m  [30/106], [94mLoss[0m : 2.30280
[1mStep[0m  [40/106], [94mLoss[0m : 2.54708
[1mStep[0m  [50/106], [94mLoss[0m : 2.36387
[1mStep[0m  [60/106], [94mLoss[0m : 2.48190
[1mStep[0m  [70/106], [94mLoss[0m : 2.58128
[1mStep[0m  [80/106], [94mLoss[0m : 2.42656
[1mStep[0m  [90/106], [94mLoss[0m : 2.66491
[1mStep[0m  [100/106], [94mLoss[0m : 2.57759

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26647
[1mStep[0m  [10/106], [94mLoss[0m : 2.64974
[1mStep[0m  [20/106], [94mLoss[0m : 2.62719
[1mStep[0m  [30/106], [94mLoss[0m : 2.44851
[1mStep[0m  [40/106], [94mLoss[0m : 2.45188
[1mStep[0m  [50/106], [94mLoss[0m : 2.42081
[1mStep[0m  [60/106], [94mLoss[0m : 2.27881
[1mStep[0m  [70/106], [94mLoss[0m : 2.31304
[1mStep[0m  [80/106], [94mLoss[0m : 2.51280
[1mStep[0m  [90/106], [94mLoss[0m : 2.60868
[1mStep[0m  [100/106], [94mLoss[0m : 2.32438

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53987
[1mStep[0m  [10/106], [94mLoss[0m : 2.23958
[1mStep[0m  [20/106], [94mLoss[0m : 2.35612
[1mStep[0m  [30/106], [94mLoss[0m : 2.37059
[1mStep[0m  [40/106], [94mLoss[0m : 2.58846
[1mStep[0m  [50/106], [94mLoss[0m : 2.50786
[1mStep[0m  [60/106], [94mLoss[0m : 2.50506
[1mStep[0m  [70/106], [94mLoss[0m : 2.53277
[1mStep[0m  [80/106], [94mLoss[0m : 2.68659
[1mStep[0m  [90/106], [94mLoss[0m : 2.24319
[1mStep[0m  [100/106], [94mLoss[0m : 2.43456

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.391, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49875
[1mStep[0m  [10/106], [94mLoss[0m : 2.11901
[1mStep[0m  [20/106], [94mLoss[0m : 2.69468
[1mStep[0m  [30/106], [94mLoss[0m : 2.69299
[1mStep[0m  [40/106], [94mLoss[0m : 2.63201
[1mStep[0m  [50/106], [94mLoss[0m : 2.24609
[1mStep[0m  [60/106], [94mLoss[0m : 2.79586
[1mStep[0m  [70/106], [94mLoss[0m : 2.68642
[1mStep[0m  [80/106], [94mLoss[0m : 2.58805
[1mStep[0m  [90/106], [94mLoss[0m : 2.39369
[1mStep[0m  [100/106], [94mLoss[0m : 2.37084

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.384, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27790
[1mStep[0m  [10/106], [94mLoss[0m : 2.45459
[1mStep[0m  [20/106], [94mLoss[0m : 2.51064
[1mStep[0m  [30/106], [94mLoss[0m : 2.55816
[1mStep[0m  [40/106], [94mLoss[0m : 2.42504
[1mStep[0m  [50/106], [94mLoss[0m : 2.65312
[1mStep[0m  [60/106], [94mLoss[0m : 2.33973
[1mStep[0m  [70/106], [94mLoss[0m : 2.37672
[1mStep[0m  [80/106], [94mLoss[0m : 2.38333
[1mStep[0m  [90/106], [94mLoss[0m : 2.61704
[1mStep[0m  [100/106], [94mLoss[0m : 2.14835

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.395, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33548
[1mStep[0m  [10/106], [94mLoss[0m : 2.80346
[1mStep[0m  [20/106], [94mLoss[0m : 2.22620
[1mStep[0m  [30/106], [94mLoss[0m : 3.01677
[1mStep[0m  [40/106], [94mLoss[0m : 2.72929
[1mStep[0m  [50/106], [94mLoss[0m : 2.33142
[1mStep[0m  [60/106], [94mLoss[0m : 2.29943
[1mStep[0m  [70/106], [94mLoss[0m : 2.23788
[1mStep[0m  [80/106], [94mLoss[0m : 2.53742
[1mStep[0m  [90/106], [94mLoss[0m : 2.45625
[1mStep[0m  [100/106], [94mLoss[0m : 2.83187

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.373, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31342
[1mStep[0m  [10/106], [94mLoss[0m : 2.51350
[1mStep[0m  [20/106], [94mLoss[0m : 2.38450
[1mStep[0m  [30/106], [94mLoss[0m : 2.21285
[1mStep[0m  [40/106], [94mLoss[0m : 2.57816
[1mStep[0m  [50/106], [94mLoss[0m : 2.48904
[1mStep[0m  [60/106], [94mLoss[0m : 2.22549
[1mStep[0m  [70/106], [94mLoss[0m : 2.75552
[1mStep[0m  [80/106], [94mLoss[0m : 2.75670
[1mStep[0m  [90/106], [94mLoss[0m : 2.44058
[1mStep[0m  [100/106], [94mLoss[0m : 2.33792

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.370
====================================

Phase 1 - Evaluation MAE:  2.3695439437650285
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.53022
[1mStep[0m  [10/106], [94mLoss[0m : 2.41807
[1mStep[0m  [20/106], [94mLoss[0m : 2.45521
[1mStep[0m  [30/106], [94mLoss[0m : 2.60099
[1mStep[0m  [40/106], [94mLoss[0m : 2.37409
[1mStep[0m  [50/106], [94mLoss[0m : 2.59919
[1mStep[0m  [60/106], [94mLoss[0m : 2.45530
[1mStep[0m  [70/106], [94mLoss[0m : 2.87877
[1mStep[0m  [80/106], [94mLoss[0m : 2.47362
[1mStep[0m  [90/106], [94mLoss[0m : 2.86766
[1mStep[0m  [100/106], [94mLoss[0m : 2.69326

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34125
[1mStep[0m  [10/106], [94mLoss[0m : 2.48643
[1mStep[0m  [20/106], [94mLoss[0m : 2.17210
[1mStep[0m  [30/106], [94mLoss[0m : 2.38181
[1mStep[0m  [40/106], [94mLoss[0m : 2.51811
[1mStep[0m  [50/106], [94mLoss[0m : 2.34967
[1mStep[0m  [60/106], [94mLoss[0m : 2.33731
[1mStep[0m  [70/106], [94mLoss[0m : 2.42166
[1mStep[0m  [80/106], [94mLoss[0m : 2.53968
[1mStep[0m  [90/106], [94mLoss[0m : 2.11725
[1mStep[0m  [100/106], [94mLoss[0m : 2.57656

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45017
[1mStep[0m  [10/106], [94mLoss[0m : 2.18498
[1mStep[0m  [20/106], [94mLoss[0m : 2.71164
[1mStep[0m  [30/106], [94mLoss[0m : 2.50036
[1mStep[0m  [40/106], [94mLoss[0m : 2.37160
[1mStep[0m  [50/106], [94mLoss[0m : 2.51941
[1mStep[0m  [60/106], [94mLoss[0m : 2.72609
[1mStep[0m  [70/106], [94mLoss[0m : 2.46362
[1mStep[0m  [80/106], [94mLoss[0m : 2.28288
[1mStep[0m  [90/106], [94mLoss[0m : 2.24987
[1mStep[0m  [100/106], [94mLoss[0m : 2.23926

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49836
[1mStep[0m  [10/106], [94mLoss[0m : 2.01916
[1mStep[0m  [20/106], [94mLoss[0m : 2.57308
[1mStep[0m  [30/106], [94mLoss[0m : 2.47527
[1mStep[0m  [40/106], [94mLoss[0m : 2.07484
[1mStep[0m  [50/106], [94mLoss[0m : 2.30950
[1mStep[0m  [60/106], [94mLoss[0m : 2.31222
[1mStep[0m  [70/106], [94mLoss[0m : 2.53183
[1mStep[0m  [80/106], [94mLoss[0m : 2.67936
[1mStep[0m  [90/106], [94mLoss[0m : 2.37295
[1mStep[0m  [100/106], [94mLoss[0m : 2.48487

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75612
[1mStep[0m  [10/106], [94mLoss[0m : 2.64830
[1mStep[0m  [20/106], [94mLoss[0m : 2.55031
[1mStep[0m  [30/106], [94mLoss[0m : 2.21725
[1mStep[0m  [40/106], [94mLoss[0m : 2.14804
[1mStep[0m  [50/106], [94mLoss[0m : 2.21963
[1mStep[0m  [60/106], [94mLoss[0m : 2.41385
[1mStep[0m  [70/106], [94mLoss[0m : 2.54903
[1mStep[0m  [80/106], [94mLoss[0m : 2.44505
[1mStep[0m  [90/106], [94mLoss[0m : 2.50434
[1mStep[0m  [100/106], [94mLoss[0m : 2.41884

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33530
[1mStep[0m  [10/106], [94mLoss[0m : 1.90447
[1mStep[0m  [20/106], [94mLoss[0m : 2.20991
[1mStep[0m  [30/106], [94mLoss[0m : 2.07845
[1mStep[0m  [40/106], [94mLoss[0m : 2.33842
[1mStep[0m  [50/106], [94mLoss[0m : 2.24253
[1mStep[0m  [60/106], [94mLoss[0m : 2.58758
[1mStep[0m  [70/106], [94mLoss[0m : 2.54231
[1mStep[0m  [80/106], [94mLoss[0m : 2.21126
[1mStep[0m  [90/106], [94mLoss[0m : 2.33629
[1mStep[0m  [100/106], [94mLoss[0m : 2.41508

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37341
[1mStep[0m  [10/106], [94mLoss[0m : 2.42654
[1mStep[0m  [20/106], [94mLoss[0m : 2.08781
[1mStep[0m  [30/106], [94mLoss[0m : 2.45103
[1mStep[0m  [40/106], [94mLoss[0m : 2.49550
[1mStep[0m  [50/106], [94mLoss[0m : 2.01823
[1mStep[0m  [60/106], [94mLoss[0m : 2.19357
[1mStep[0m  [70/106], [94mLoss[0m : 2.37734
[1mStep[0m  [80/106], [94mLoss[0m : 2.02901
[1mStep[0m  [90/106], [94mLoss[0m : 2.08535
[1mStep[0m  [100/106], [94mLoss[0m : 2.46881

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.212, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94094
[1mStep[0m  [10/106], [94mLoss[0m : 2.24797
[1mStep[0m  [20/106], [94mLoss[0m : 2.23321
[1mStep[0m  [30/106], [94mLoss[0m : 2.05430
[1mStep[0m  [40/106], [94mLoss[0m : 1.95264
[1mStep[0m  [50/106], [94mLoss[0m : 2.06913
[1mStep[0m  [60/106], [94mLoss[0m : 1.96516
[1mStep[0m  [70/106], [94mLoss[0m : 2.12328
[1mStep[0m  [80/106], [94mLoss[0m : 2.17059
[1mStep[0m  [90/106], [94mLoss[0m : 2.29843
[1mStep[0m  [100/106], [94mLoss[0m : 2.02674

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15571
[1mStep[0m  [10/106], [94mLoss[0m : 1.96545
[1mStep[0m  [20/106], [94mLoss[0m : 1.88211
[1mStep[0m  [30/106], [94mLoss[0m : 2.08713
[1mStep[0m  [40/106], [94mLoss[0m : 2.36517
[1mStep[0m  [50/106], [94mLoss[0m : 2.20761
[1mStep[0m  [60/106], [94mLoss[0m : 2.18361
[1mStep[0m  [70/106], [94mLoss[0m : 1.99109
[1mStep[0m  [80/106], [94mLoss[0m : 2.37456
[1mStep[0m  [90/106], [94mLoss[0m : 2.23923
[1mStep[0m  [100/106], [94mLoss[0m : 2.41290

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85319
[1mStep[0m  [10/106], [94mLoss[0m : 1.90425
[1mStep[0m  [20/106], [94mLoss[0m : 2.07852
[1mStep[0m  [30/106], [94mLoss[0m : 1.86938
[1mStep[0m  [40/106], [94mLoss[0m : 2.09524
[1mStep[0m  [50/106], [94mLoss[0m : 2.33137
[1mStep[0m  [60/106], [94mLoss[0m : 1.80379
[1mStep[0m  [70/106], [94mLoss[0m : 2.35489
[1mStep[0m  [80/106], [94mLoss[0m : 2.25374
[1mStep[0m  [90/106], [94mLoss[0m : 1.76630
[1mStep[0m  [100/106], [94mLoss[0m : 1.94940

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67519
[1mStep[0m  [10/106], [94mLoss[0m : 1.80894
[1mStep[0m  [20/106], [94mLoss[0m : 2.03747
[1mStep[0m  [30/106], [94mLoss[0m : 2.10412
[1mStep[0m  [40/106], [94mLoss[0m : 2.12690
[1mStep[0m  [50/106], [94mLoss[0m : 2.29165
[1mStep[0m  [60/106], [94mLoss[0m : 1.98724
[1mStep[0m  [70/106], [94mLoss[0m : 2.29359
[1mStep[0m  [80/106], [94mLoss[0m : 2.00443
[1mStep[0m  [90/106], [94mLoss[0m : 1.86033
[1mStep[0m  [100/106], [94mLoss[0m : 2.07215

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07609
[1mStep[0m  [10/106], [94mLoss[0m : 1.79949
[1mStep[0m  [20/106], [94mLoss[0m : 1.88370
[1mStep[0m  [30/106], [94mLoss[0m : 2.13821
[1mStep[0m  [40/106], [94mLoss[0m : 2.01050
[1mStep[0m  [50/106], [94mLoss[0m : 2.07351
[1mStep[0m  [60/106], [94mLoss[0m : 2.24315
[1mStep[0m  [70/106], [94mLoss[0m : 1.94405
[1mStep[0m  [80/106], [94mLoss[0m : 1.97913
[1mStep[0m  [90/106], [94mLoss[0m : 1.78603
[1mStep[0m  [100/106], [94mLoss[0m : 1.83365

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10637
[1mStep[0m  [10/106], [94mLoss[0m : 2.01907
[1mStep[0m  [20/106], [94mLoss[0m : 1.83385
[1mStep[0m  [30/106], [94mLoss[0m : 2.09683
[1mStep[0m  [40/106], [94mLoss[0m : 2.06068
[1mStep[0m  [50/106], [94mLoss[0m : 1.60170
[1mStep[0m  [60/106], [94mLoss[0m : 2.29400
[1mStep[0m  [70/106], [94mLoss[0m : 1.93847
[1mStep[0m  [80/106], [94mLoss[0m : 1.72140
[1mStep[0m  [90/106], [94mLoss[0m : 2.02481
[1mStep[0m  [100/106], [94mLoss[0m : 1.76800

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89389
[1mStep[0m  [10/106], [94mLoss[0m : 2.10825
[1mStep[0m  [20/106], [94mLoss[0m : 1.89453
[1mStep[0m  [30/106], [94mLoss[0m : 2.31632
[1mStep[0m  [40/106], [94mLoss[0m : 1.73719
[1mStep[0m  [50/106], [94mLoss[0m : 1.97827
[1mStep[0m  [60/106], [94mLoss[0m : 2.12957
[1mStep[0m  [70/106], [94mLoss[0m : 1.79802
[1mStep[0m  [80/106], [94mLoss[0m : 1.88475
[1mStep[0m  [90/106], [94mLoss[0m : 1.82309
[1mStep[0m  [100/106], [94mLoss[0m : 1.58974

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.915, [92mTest[0m: 2.546, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75711
[1mStep[0m  [10/106], [94mLoss[0m : 1.67650
[1mStep[0m  [20/106], [94mLoss[0m : 1.76837
[1mStep[0m  [30/106], [94mLoss[0m : 1.85712
[1mStep[0m  [40/106], [94mLoss[0m : 1.95588
[1mStep[0m  [50/106], [94mLoss[0m : 1.81240
[1mStep[0m  [60/106], [94mLoss[0m : 1.85918
[1mStep[0m  [70/106], [94mLoss[0m : 1.86641
[1mStep[0m  [80/106], [94mLoss[0m : 1.85270
[1mStep[0m  [90/106], [94mLoss[0m : 1.74109
[1mStep[0m  [100/106], [94mLoss[0m : 1.79667

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.495, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79017
[1mStep[0m  [10/106], [94mLoss[0m : 1.80800
[1mStep[0m  [20/106], [94mLoss[0m : 1.73220
[1mStep[0m  [30/106], [94mLoss[0m : 1.72057
[1mStep[0m  [40/106], [94mLoss[0m : 1.79467
[1mStep[0m  [50/106], [94mLoss[0m : 2.00246
[1mStep[0m  [60/106], [94mLoss[0m : 1.77493
[1mStep[0m  [70/106], [94mLoss[0m : 1.67845
[1mStep[0m  [80/106], [94mLoss[0m : 1.97380
[1mStep[0m  [90/106], [94mLoss[0m : 1.76816
[1mStep[0m  [100/106], [94mLoss[0m : 1.93507

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89429
[1mStep[0m  [10/106], [94mLoss[0m : 1.99479
[1mStep[0m  [20/106], [94mLoss[0m : 1.79097
[1mStep[0m  [30/106], [94mLoss[0m : 1.91107
[1mStep[0m  [40/106], [94mLoss[0m : 1.85664
[1mStep[0m  [50/106], [94mLoss[0m : 1.67953
[1mStep[0m  [60/106], [94mLoss[0m : 1.87153
[1mStep[0m  [70/106], [94mLoss[0m : 1.85228
[1mStep[0m  [80/106], [94mLoss[0m : 1.84565
[1mStep[0m  [90/106], [94mLoss[0m : 1.73382
[1mStep[0m  [100/106], [94mLoss[0m : 1.99570

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61338
[1mStep[0m  [10/106], [94mLoss[0m : 1.63501
[1mStep[0m  [20/106], [94mLoss[0m : 1.72273
[1mStep[0m  [30/106], [94mLoss[0m : 1.71717
[1mStep[0m  [40/106], [94mLoss[0m : 1.74277
[1mStep[0m  [50/106], [94mLoss[0m : 1.97882
[1mStep[0m  [60/106], [94mLoss[0m : 2.09201
[1mStep[0m  [70/106], [94mLoss[0m : 1.66753
[1mStep[0m  [80/106], [94mLoss[0m : 1.63342
[1mStep[0m  [90/106], [94mLoss[0m : 1.75297
[1mStep[0m  [100/106], [94mLoss[0m : 1.77303

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64626
[1mStep[0m  [10/106], [94mLoss[0m : 1.65643
[1mStep[0m  [20/106], [94mLoss[0m : 1.57124
[1mStep[0m  [30/106], [94mLoss[0m : 1.64486
[1mStep[0m  [40/106], [94mLoss[0m : 1.59495
[1mStep[0m  [50/106], [94mLoss[0m : 1.81186
[1mStep[0m  [60/106], [94mLoss[0m : 1.81351
[1mStep[0m  [70/106], [94mLoss[0m : 1.92001
[1mStep[0m  [80/106], [94mLoss[0m : 1.80114
[1mStep[0m  [90/106], [94mLoss[0m : 1.67605
[1mStep[0m  [100/106], [94mLoss[0m : 1.74352

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74134
[1mStep[0m  [10/106], [94mLoss[0m : 1.74444
[1mStep[0m  [20/106], [94mLoss[0m : 1.55852
[1mStep[0m  [30/106], [94mLoss[0m : 1.48897
[1mStep[0m  [40/106], [94mLoss[0m : 1.64251
[1mStep[0m  [50/106], [94mLoss[0m : 1.79934
[1mStep[0m  [60/106], [94mLoss[0m : 1.66458
[1mStep[0m  [70/106], [94mLoss[0m : 1.72874
[1mStep[0m  [80/106], [94mLoss[0m : 1.83337
[1mStep[0m  [90/106], [94mLoss[0m : 1.77407
[1mStep[0m  [100/106], [94mLoss[0m : 1.66590

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83708
[1mStep[0m  [10/106], [94mLoss[0m : 1.55899
[1mStep[0m  [20/106], [94mLoss[0m : 1.70485
[1mStep[0m  [30/106], [94mLoss[0m : 1.52140
[1mStep[0m  [40/106], [94mLoss[0m : 1.74978
[1mStep[0m  [50/106], [94mLoss[0m : 1.58191
[1mStep[0m  [60/106], [94mLoss[0m : 1.53155
[1mStep[0m  [70/106], [94mLoss[0m : 1.89421
[1mStep[0m  [80/106], [94mLoss[0m : 1.67535
[1mStep[0m  [90/106], [94mLoss[0m : 1.68828
[1mStep[0m  [100/106], [94mLoss[0m : 1.65175

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60628
[1mStep[0m  [10/106], [94mLoss[0m : 1.53394
[1mStep[0m  [20/106], [94mLoss[0m : 1.65470
[1mStep[0m  [30/106], [94mLoss[0m : 1.68413
[1mStep[0m  [40/106], [94mLoss[0m : 1.41123
[1mStep[0m  [50/106], [94mLoss[0m : 1.82552
[1mStep[0m  [60/106], [94mLoss[0m : 1.62743
[1mStep[0m  [70/106], [94mLoss[0m : 1.48170
[1mStep[0m  [80/106], [94mLoss[0m : 1.81337
[1mStep[0m  [90/106], [94mLoss[0m : 1.91015
[1mStep[0m  [100/106], [94mLoss[0m : 1.65856

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.662, [92mTest[0m: 2.559, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44699
[1mStep[0m  [10/106], [94mLoss[0m : 1.55668
[1mStep[0m  [20/106], [94mLoss[0m : 1.37705
[1mStep[0m  [30/106], [94mLoss[0m : 1.62959
[1mStep[0m  [40/106], [94mLoss[0m : 1.72776
[1mStep[0m  [50/106], [94mLoss[0m : 1.69114
[1mStep[0m  [60/106], [94mLoss[0m : 1.72201
[1mStep[0m  [70/106], [94mLoss[0m : 1.66625
[1mStep[0m  [80/106], [94mLoss[0m : 1.58663
[1mStep[0m  [90/106], [94mLoss[0m : 1.57241
[1mStep[0m  [100/106], [94mLoss[0m : 1.51093

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.628, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49357
[1mStep[0m  [10/106], [94mLoss[0m : 1.61957
[1mStep[0m  [20/106], [94mLoss[0m : 1.57001
[1mStep[0m  [30/106], [94mLoss[0m : 1.46640
[1mStep[0m  [40/106], [94mLoss[0m : 1.59508
[1mStep[0m  [50/106], [94mLoss[0m : 1.53417
[1mStep[0m  [60/106], [94mLoss[0m : 1.53343
[1mStep[0m  [70/106], [94mLoss[0m : 1.66712
[1mStep[0m  [80/106], [94mLoss[0m : 1.36520
[1mStep[0m  [90/106], [94mLoss[0m : 1.61006
[1mStep[0m  [100/106], [94mLoss[0m : 1.56139

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.606, [92mTest[0m: 2.593, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61058
[1mStep[0m  [10/106], [94mLoss[0m : 1.68790
[1mStep[0m  [20/106], [94mLoss[0m : 1.35547
[1mStep[0m  [30/106], [94mLoss[0m : 1.72563
[1mStep[0m  [40/106], [94mLoss[0m : 1.55272
[1mStep[0m  [50/106], [94mLoss[0m : 1.61844
[1mStep[0m  [60/106], [94mLoss[0m : 1.44724
[1mStep[0m  [70/106], [94mLoss[0m : 1.69740
[1mStep[0m  [80/106], [94mLoss[0m : 1.54195
[1mStep[0m  [90/106], [94mLoss[0m : 1.52326
[1mStep[0m  [100/106], [94mLoss[0m : 1.56522

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.36370
[1mStep[0m  [10/106], [94mLoss[0m : 1.47901
[1mStep[0m  [20/106], [94mLoss[0m : 1.56110
[1mStep[0m  [30/106], [94mLoss[0m : 1.35169
[1mStep[0m  [40/106], [94mLoss[0m : 1.55594
[1mStep[0m  [50/106], [94mLoss[0m : 1.59676
[1mStep[0m  [60/106], [94mLoss[0m : 1.57668
[1mStep[0m  [70/106], [94mLoss[0m : 1.45425
[1mStep[0m  [80/106], [94mLoss[0m : 1.72441
[1mStep[0m  [90/106], [94mLoss[0m : 1.34219
[1mStep[0m  [100/106], [94mLoss[0m : 1.55155

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.554, [92mTest[0m: 2.516, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55393
[1mStep[0m  [10/106], [94mLoss[0m : 1.50921
[1mStep[0m  [20/106], [94mLoss[0m : 1.29905
[1mStep[0m  [30/106], [94mLoss[0m : 1.64197
[1mStep[0m  [40/106], [94mLoss[0m : 1.46314
[1mStep[0m  [50/106], [94mLoss[0m : 1.61020
[1mStep[0m  [60/106], [94mLoss[0m : 1.61724
[1mStep[0m  [70/106], [94mLoss[0m : 1.56040
[1mStep[0m  [80/106], [94mLoss[0m : 1.53266
[1mStep[0m  [90/106], [94mLoss[0m : 1.42880
[1mStep[0m  [100/106], [94mLoss[0m : 1.50926

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43698
[1mStep[0m  [10/106], [94mLoss[0m : 1.58564
[1mStep[0m  [20/106], [94mLoss[0m : 1.53234
[1mStep[0m  [30/106], [94mLoss[0m : 1.41871
[1mStep[0m  [40/106], [94mLoss[0m : 1.60150
[1mStep[0m  [50/106], [94mLoss[0m : 1.46006
[1mStep[0m  [60/106], [94mLoss[0m : 1.63252
[1mStep[0m  [70/106], [94mLoss[0m : 1.75325
[1mStep[0m  [80/106], [94mLoss[0m : 1.60302
[1mStep[0m  [90/106], [94mLoss[0m : 1.43996
[1mStep[0m  [100/106], [94mLoss[0m : 1.64559

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.36510
[1mStep[0m  [10/106], [94mLoss[0m : 1.58207
[1mStep[0m  [20/106], [94mLoss[0m : 1.68609
[1mStep[0m  [30/106], [94mLoss[0m : 1.63374
[1mStep[0m  [40/106], [94mLoss[0m : 1.52729
[1mStep[0m  [50/106], [94mLoss[0m : 1.40681
[1mStep[0m  [60/106], [94mLoss[0m : 1.42206
[1mStep[0m  [70/106], [94mLoss[0m : 1.68005
[1mStep[0m  [80/106], [94mLoss[0m : 1.47579
[1mStep[0m  [90/106], [94mLoss[0m : 1.50879
[1mStep[0m  [100/106], [94mLoss[0m : 1.54298

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.499, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57313
[1mStep[0m  [10/106], [94mLoss[0m : 1.54084
[1mStep[0m  [20/106], [94mLoss[0m : 1.38197
[1mStep[0m  [30/106], [94mLoss[0m : 1.58916
[1mStep[0m  [40/106], [94mLoss[0m : 1.55548
[1mStep[0m  [50/106], [94mLoss[0m : 1.81899
[1mStep[0m  [60/106], [94mLoss[0m : 1.37380
[1mStep[0m  [70/106], [94mLoss[0m : 1.31497
[1mStep[0m  [80/106], [94mLoss[0m : 1.59577
[1mStep[0m  [90/106], [94mLoss[0m : 1.53771
[1mStep[0m  [100/106], [94mLoss[0m : 1.61924

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.493, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.489
====================================

Phase 2 - Evaluation MAE:  2.488746800512638
MAE score P1       2.369544
MAE score P2       2.488747
loss               1.492909
learning_rate          0.01
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay         0.0001
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.01, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.89521
[1mStep[0m  [10/106], [94mLoss[0m : 6.55319
[1mStep[0m  [20/106], [94mLoss[0m : 3.32893
[1mStep[0m  [30/106], [94mLoss[0m : 2.69564
[1mStep[0m  [40/106], [94mLoss[0m : 2.74778
[1mStep[0m  [50/106], [94mLoss[0m : 2.72844
[1mStep[0m  [60/106], [94mLoss[0m : 2.51560
[1mStep[0m  [70/106], [94mLoss[0m : 3.10281
[1mStep[0m  [80/106], [94mLoss[0m : 2.30848
[1mStep[0m  [90/106], [94mLoss[0m : 2.52669
[1mStep[0m  [100/106], [94mLoss[0m : 2.55013

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.450, [92mTest[0m: 11.304, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69863
[1mStep[0m  [10/106], [94mLoss[0m : 2.70270
[1mStep[0m  [20/106], [94mLoss[0m : 2.74527
[1mStep[0m  [30/106], [94mLoss[0m : 2.50679
[1mStep[0m  [40/106], [94mLoss[0m : 2.63742
[1mStep[0m  [50/106], [94mLoss[0m : 2.40263
[1mStep[0m  [60/106], [94mLoss[0m : 2.40943
[1mStep[0m  [70/106], [94mLoss[0m : 2.44024
[1mStep[0m  [80/106], [94mLoss[0m : 2.67487
[1mStep[0m  [90/106], [94mLoss[0m : 2.46706
[1mStep[0m  [100/106], [94mLoss[0m : 2.45410

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51136
[1mStep[0m  [10/106], [94mLoss[0m : 2.34781
[1mStep[0m  [20/106], [94mLoss[0m : 2.56378
[1mStep[0m  [30/106], [94mLoss[0m : 2.38639
[1mStep[0m  [40/106], [94mLoss[0m : 2.69879
[1mStep[0m  [50/106], [94mLoss[0m : 2.73819
[1mStep[0m  [60/106], [94mLoss[0m : 2.52252
[1mStep[0m  [70/106], [94mLoss[0m : 2.60292
[1mStep[0m  [80/106], [94mLoss[0m : 2.53098
[1mStep[0m  [90/106], [94mLoss[0m : 2.57558
[1mStep[0m  [100/106], [94mLoss[0m : 2.29879

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47213
[1mStep[0m  [10/106], [94mLoss[0m : 2.61509
[1mStep[0m  [20/106], [94mLoss[0m : 2.53063
[1mStep[0m  [30/106], [94mLoss[0m : 2.62057
[1mStep[0m  [40/106], [94mLoss[0m : 2.43519
[1mStep[0m  [50/106], [94mLoss[0m : 2.61811
[1mStep[0m  [60/106], [94mLoss[0m : 2.56127
[1mStep[0m  [70/106], [94mLoss[0m : 2.60225
[1mStep[0m  [80/106], [94mLoss[0m : 2.43383
[1mStep[0m  [90/106], [94mLoss[0m : 2.81873
[1mStep[0m  [100/106], [94mLoss[0m : 2.48386

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13477
[1mStep[0m  [10/106], [94mLoss[0m : 2.25267
[1mStep[0m  [20/106], [94mLoss[0m : 2.49990
[1mStep[0m  [30/106], [94mLoss[0m : 2.39138
[1mStep[0m  [40/106], [94mLoss[0m : 2.69724
[1mStep[0m  [50/106], [94mLoss[0m : 2.44682
[1mStep[0m  [60/106], [94mLoss[0m : 2.63116
[1mStep[0m  [70/106], [94mLoss[0m : 2.40587
[1mStep[0m  [80/106], [94mLoss[0m : 2.45541
[1mStep[0m  [90/106], [94mLoss[0m : 2.42275
[1mStep[0m  [100/106], [94mLoss[0m : 2.89353

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.413, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75827
[1mStep[0m  [10/106], [94mLoss[0m : 2.29747
[1mStep[0m  [20/106], [94mLoss[0m : 2.39959
[1mStep[0m  [30/106], [94mLoss[0m : 3.03694
[1mStep[0m  [40/106], [94mLoss[0m : 2.69981
[1mStep[0m  [50/106], [94mLoss[0m : 2.27442
[1mStep[0m  [60/106], [94mLoss[0m : 2.58034
[1mStep[0m  [70/106], [94mLoss[0m : 2.23765
[1mStep[0m  [80/106], [94mLoss[0m : 2.33718
[1mStep[0m  [90/106], [94mLoss[0m : 2.40107
[1mStep[0m  [100/106], [94mLoss[0m : 2.46703

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15949
[1mStep[0m  [10/106], [94mLoss[0m : 2.38271
[1mStep[0m  [20/106], [94mLoss[0m : 2.11356
[1mStep[0m  [30/106], [94mLoss[0m : 2.47650
[1mStep[0m  [40/106], [94mLoss[0m : 2.80399
[1mStep[0m  [50/106], [94mLoss[0m : 2.58012
[1mStep[0m  [60/106], [94mLoss[0m : 2.59861
[1mStep[0m  [70/106], [94mLoss[0m : 2.65482
[1mStep[0m  [80/106], [94mLoss[0m : 2.58207
[1mStep[0m  [90/106], [94mLoss[0m : 2.49592
[1mStep[0m  [100/106], [94mLoss[0m : 2.64383

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37477
[1mStep[0m  [10/106], [94mLoss[0m : 2.54642
[1mStep[0m  [20/106], [94mLoss[0m : 2.51793
[1mStep[0m  [30/106], [94mLoss[0m : 2.33189
[1mStep[0m  [40/106], [94mLoss[0m : 2.46486
[1mStep[0m  [50/106], [94mLoss[0m : 2.46769
[1mStep[0m  [60/106], [94mLoss[0m : 2.11565
[1mStep[0m  [70/106], [94mLoss[0m : 2.61313
[1mStep[0m  [80/106], [94mLoss[0m : 2.27610
[1mStep[0m  [90/106], [94mLoss[0m : 2.52307
[1mStep[0m  [100/106], [94mLoss[0m : 2.46732

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38977
[1mStep[0m  [10/106], [94mLoss[0m : 2.41023
[1mStep[0m  [20/106], [94mLoss[0m : 2.45215
[1mStep[0m  [30/106], [94mLoss[0m : 2.64185
[1mStep[0m  [40/106], [94mLoss[0m : 2.27518
[1mStep[0m  [50/106], [94mLoss[0m : 2.71189
[1mStep[0m  [60/106], [94mLoss[0m : 2.43384
[1mStep[0m  [70/106], [94mLoss[0m : 2.63995
[1mStep[0m  [80/106], [94mLoss[0m : 2.23584
[1mStep[0m  [90/106], [94mLoss[0m : 2.18180
[1mStep[0m  [100/106], [94mLoss[0m : 2.51696

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46860
[1mStep[0m  [10/106], [94mLoss[0m : 2.37390
[1mStep[0m  [20/106], [94mLoss[0m : 2.39158
[1mStep[0m  [30/106], [94mLoss[0m : 2.51806
[1mStep[0m  [40/106], [94mLoss[0m : 2.39198
[1mStep[0m  [50/106], [94mLoss[0m : 2.17110
[1mStep[0m  [60/106], [94mLoss[0m : 2.52210
[1mStep[0m  [70/106], [94mLoss[0m : 2.70326
[1mStep[0m  [80/106], [94mLoss[0m : 2.45470
[1mStep[0m  [90/106], [94mLoss[0m : 2.51948
[1mStep[0m  [100/106], [94mLoss[0m : 2.64846

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54623
[1mStep[0m  [10/106], [94mLoss[0m : 2.26223
[1mStep[0m  [20/106], [94mLoss[0m : 2.61405
[1mStep[0m  [30/106], [94mLoss[0m : 2.76943
[1mStep[0m  [40/106], [94mLoss[0m : 2.62277
[1mStep[0m  [50/106], [94mLoss[0m : 2.46092
[1mStep[0m  [60/106], [94mLoss[0m : 2.42935
[1mStep[0m  [70/106], [94mLoss[0m : 2.32502
[1mStep[0m  [80/106], [94mLoss[0m : 2.75255
[1mStep[0m  [90/106], [94mLoss[0m : 2.21841
[1mStep[0m  [100/106], [94mLoss[0m : 2.82447

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71781
[1mStep[0m  [10/106], [94mLoss[0m : 2.32180
[1mStep[0m  [20/106], [94mLoss[0m : 2.28652
[1mStep[0m  [30/106], [94mLoss[0m : 2.61844
[1mStep[0m  [40/106], [94mLoss[0m : 2.59573
[1mStep[0m  [50/106], [94mLoss[0m : 2.71605
[1mStep[0m  [60/106], [94mLoss[0m : 2.54648
[1mStep[0m  [70/106], [94mLoss[0m : 2.48998
[1mStep[0m  [80/106], [94mLoss[0m : 2.29150
[1mStep[0m  [90/106], [94mLoss[0m : 2.34129
[1mStep[0m  [100/106], [94mLoss[0m : 2.63259

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71112
[1mStep[0m  [10/106], [94mLoss[0m : 2.40988
[1mStep[0m  [20/106], [94mLoss[0m : 2.35480
[1mStep[0m  [30/106], [94mLoss[0m : 2.42735
[1mStep[0m  [40/106], [94mLoss[0m : 2.54462
[1mStep[0m  [50/106], [94mLoss[0m : 2.48531
[1mStep[0m  [60/106], [94mLoss[0m : 2.66197
[1mStep[0m  [70/106], [94mLoss[0m : 2.59910
[1mStep[0m  [80/106], [94mLoss[0m : 2.77713
[1mStep[0m  [90/106], [94mLoss[0m : 2.44136
[1mStep[0m  [100/106], [94mLoss[0m : 2.41465

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65275
[1mStep[0m  [10/106], [94mLoss[0m : 2.24152
[1mStep[0m  [20/106], [94mLoss[0m : 2.28196
[1mStep[0m  [30/106], [94mLoss[0m : 2.38395
[1mStep[0m  [40/106], [94mLoss[0m : 2.31875
[1mStep[0m  [50/106], [94mLoss[0m : 2.55126
[1mStep[0m  [60/106], [94mLoss[0m : 2.38307
[1mStep[0m  [70/106], [94mLoss[0m : 2.71764
[1mStep[0m  [80/106], [94mLoss[0m : 2.50944
[1mStep[0m  [90/106], [94mLoss[0m : 3.12745
[1mStep[0m  [100/106], [94mLoss[0m : 2.31969

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53355
[1mStep[0m  [10/106], [94mLoss[0m : 2.49137
[1mStep[0m  [20/106], [94mLoss[0m : 2.64195
[1mStep[0m  [30/106], [94mLoss[0m : 2.54872
[1mStep[0m  [40/106], [94mLoss[0m : 2.75480
[1mStep[0m  [50/106], [94mLoss[0m : 2.41711
[1mStep[0m  [60/106], [94mLoss[0m : 2.46844
[1mStep[0m  [70/106], [94mLoss[0m : 2.48239
[1mStep[0m  [80/106], [94mLoss[0m : 2.72240
[1mStep[0m  [90/106], [94mLoss[0m : 2.65016
[1mStep[0m  [100/106], [94mLoss[0m : 2.50261

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84423
[1mStep[0m  [10/106], [94mLoss[0m : 2.73048
[1mStep[0m  [20/106], [94mLoss[0m : 2.07544
[1mStep[0m  [30/106], [94mLoss[0m : 2.76614
[1mStep[0m  [40/106], [94mLoss[0m : 2.34154
[1mStep[0m  [50/106], [94mLoss[0m : 2.65888
[1mStep[0m  [60/106], [94mLoss[0m : 2.67770
[1mStep[0m  [70/106], [94mLoss[0m : 2.39044
[1mStep[0m  [80/106], [94mLoss[0m : 2.25712
[1mStep[0m  [90/106], [94mLoss[0m : 2.13027
[1mStep[0m  [100/106], [94mLoss[0m : 2.68776

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48349
[1mStep[0m  [10/106], [94mLoss[0m : 2.41724
[1mStep[0m  [20/106], [94mLoss[0m : 2.53168
[1mStep[0m  [30/106], [94mLoss[0m : 2.54200
[1mStep[0m  [40/106], [94mLoss[0m : 2.39288
[1mStep[0m  [50/106], [94mLoss[0m : 2.40828
[1mStep[0m  [60/106], [94mLoss[0m : 2.20058
[1mStep[0m  [70/106], [94mLoss[0m : 2.33294
[1mStep[0m  [80/106], [94mLoss[0m : 2.49524
[1mStep[0m  [90/106], [94mLoss[0m : 2.51529
[1mStep[0m  [100/106], [94mLoss[0m : 2.78086

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76309
[1mStep[0m  [10/106], [94mLoss[0m : 2.70900
[1mStep[0m  [20/106], [94mLoss[0m : 2.43176
[1mStep[0m  [30/106], [94mLoss[0m : 2.36539
[1mStep[0m  [40/106], [94mLoss[0m : 2.55879
[1mStep[0m  [50/106], [94mLoss[0m : 2.24592
[1mStep[0m  [60/106], [94mLoss[0m : 2.46552
[1mStep[0m  [70/106], [94mLoss[0m : 2.43540
[1mStep[0m  [80/106], [94mLoss[0m : 2.48810
[1mStep[0m  [90/106], [94mLoss[0m : 2.08428
[1mStep[0m  [100/106], [94mLoss[0m : 2.31349

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52267
[1mStep[0m  [10/106], [94mLoss[0m : 2.40497
[1mStep[0m  [20/106], [94mLoss[0m : 2.72126
[1mStep[0m  [30/106], [94mLoss[0m : 2.58495
[1mStep[0m  [40/106], [94mLoss[0m : 3.02922
[1mStep[0m  [50/106], [94mLoss[0m : 2.46881
[1mStep[0m  [60/106], [94mLoss[0m : 2.35596
[1mStep[0m  [70/106], [94mLoss[0m : 2.38231
[1mStep[0m  [80/106], [94mLoss[0m : 2.24150
[1mStep[0m  [90/106], [94mLoss[0m : 2.58081
[1mStep[0m  [100/106], [94mLoss[0m : 2.39929

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25195
[1mStep[0m  [10/106], [94mLoss[0m : 2.37074
[1mStep[0m  [20/106], [94mLoss[0m : 2.52611
[1mStep[0m  [30/106], [94mLoss[0m : 2.30962
[1mStep[0m  [40/106], [94mLoss[0m : 2.30822
[1mStep[0m  [50/106], [94mLoss[0m : 2.80010
[1mStep[0m  [60/106], [94mLoss[0m : 2.47972
[1mStep[0m  [70/106], [94mLoss[0m : 2.32741
[1mStep[0m  [80/106], [94mLoss[0m : 2.43447
[1mStep[0m  [90/106], [94mLoss[0m : 2.36924
[1mStep[0m  [100/106], [94mLoss[0m : 2.54612

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.385, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36706
[1mStep[0m  [10/106], [94mLoss[0m : 2.47288
[1mStep[0m  [20/106], [94mLoss[0m : 2.58057
[1mStep[0m  [30/106], [94mLoss[0m : 2.23342
[1mStep[0m  [40/106], [94mLoss[0m : 2.46996
[1mStep[0m  [50/106], [94mLoss[0m : 2.57664
[1mStep[0m  [60/106], [94mLoss[0m : 2.59020
[1mStep[0m  [70/106], [94mLoss[0m : 2.76747
[1mStep[0m  [80/106], [94mLoss[0m : 2.47943
[1mStep[0m  [90/106], [94mLoss[0m : 2.64237
[1mStep[0m  [100/106], [94mLoss[0m : 2.48172

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.392, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.03447
[1mStep[0m  [10/106], [94mLoss[0m : 2.46070
[1mStep[0m  [20/106], [94mLoss[0m : 2.87052
[1mStep[0m  [30/106], [94mLoss[0m : 2.41762
[1mStep[0m  [40/106], [94mLoss[0m : 2.39992
[1mStep[0m  [50/106], [94mLoss[0m : 2.68852
[1mStep[0m  [60/106], [94mLoss[0m : 2.36549
[1mStep[0m  [70/106], [94mLoss[0m : 2.64697
[1mStep[0m  [80/106], [94mLoss[0m : 2.64794
[1mStep[0m  [90/106], [94mLoss[0m : 2.86341
[1mStep[0m  [100/106], [94mLoss[0m : 2.30369

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.390, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57672
[1mStep[0m  [10/106], [94mLoss[0m : 2.19302
[1mStep[0m  [20/106], [94mLoss[0m : 2.33821
[1mStep[0m  [30/106], [94mLoss[0m : 2.90373
[1mStep[0m  [40/106], [94mLoss[0m : 2.48490
[1mStep[0m  [50/106], [94mLoss[0m : 2.49805
[1mStep[0m  [60/106], [94mLoss[0m : 2.59969
[1mStep[0m  [70/106], [94mLoss[0m : 2.41142
[1mStep[0m  [80/106], [94mLoss[0m : 2.19286
[1mStep[0m  [90/106], [94mLoss[0m : 2.54252
[1mStep[0m  [100/106], [94mLoss[0m : 2.57617

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.395, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31665
[1mStep[0m  [10/106], [94mLoss[0m : 2.43835
[1mStep[0m  [20/106], [94mLoss[0m : 2.43289
[1mStep[0m  [30/106], [94mLoss[0m : 2.45826
[1mStep[0m  [40/106], [94mLoss[0m : 2.17856
[1mStep[0m  [50/106], [94mLoss[0m : 2.26354
[1mStep[0m  [60/106], [94mLoss[0m : 2.43889
[1mStep[0m  [70/106], [94mLoss[0m : 1.99999
[1mStep[0m  [80/106], [94mLoss[0m : 2.50688
[1mStep[0m  [90/106], [94mLoss[0m : 2.46914
[1mStep[0m  [100/106], [94mLoss[0m : 2.33944

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.391, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53820
[1mStep[0m  [10/106], [94mLoss[0m : 2.70645
[1mStep[0m  [20/106], [94mLoss[0m : 2.32478
[1mStep[0m  [30/106], [94mLoss[0m : 2.47892
[1mStep[0m  [40/106], [94mLoss[0m : 2.35581
[1mStep[0m  [50/106], [94mLoss[0m : 2.49148
[1mStep[0m  [60/106], [94mLoss[0m : 2.47419
[1mStep[0m  [70/106], [94mLoss[0m : 2.24170
[1mStep[0m  [80/106], [94mLoss[0m : 2.43699
[1mStep[0m  [90/106], [94mLoss[0m : 2.68985
[1mStep[0m  [100/106], [94mLoss[0m : 2.55344

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.394, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31975
[1mStep[0m  [10/106], [94mLoss[0m : 2.51277
[1mStep[0m  [20/106], [94mLoss[0m : 2.26625
[1mStep[0m  [30/106], [94mLoss[0m : 2.28940
[1mStep[0m  [40/106], [94mLoss[0m : 2.38459
[1mStep[0m  [50/106], [94mLoss[0m : 2.55363
[1mStep[0m  [60/106], [94mLoss[0m : 2.11091
[1mStep[0m  [70/106], [94mLoss[0m : 2.69303
[1mStep[0m  [80/106], [94mLoss[0m : 2.51838
[1mStep[0m  [90/106], [94mLoss[0m : 2.35421
[1mStep[0m  [100/106], [94mLoss[0m : 2.51369

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.391, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31593
[1mStep[0m  [10/106], [94mLoss[0m : 2.49919
[1mStep[0m  [20/106], [94mLoss[0m : 2.38701
[1mStep[0m  [30/106], [94mLoss[0m : 2.34285
[1mStep[0m  [40/106], [94mLoss[0m : 2.13695
[1mStep[0m  [50/106], [94mLoss[0m : 2.35991
[1mStep[0m  [60/106], [94mLoss[0m : 2.51380
[1mStep[0m  [70/106], [94mLoss[0m : 2.57242
[1mStep[0m  [80/106], [94mLoss[0m : 2.40588
[1mStep[0m  [90/106], [94mLoss[0m : 2.57891
[1mStep[0m  [100/106], [94mLoss[0m : 2.42221

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42878
[1mStep[0m  [10/106], [94mLoss[0m : 2.75669
[1mStep[0m  [20/106], [94mLoss[0m : 2.61516
[1mStep[0m  [30/106], [94mLoss[0m : 2.40057
[1mStep[0m  [40/106], [94mLoss[0m : 2.40342
[1mStep[0m  [50/106], [94mLoss[0m : 2.44370
[1mStep[0m  [60/106], [94mLoss[0m : 2.22429
[1mStep[0m  [70/106], [94mLoss[0m : 2.24576
[1mStep[0m  [80/106], [94mLoss[0m : 2.34633
[1mStep[0m  [90/106], [94mLoss[0m : 2.40546
[1mStep[0m  [100/106], [94mLoss[0m : 2.54299

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.388, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12703
[1mStep[0m  [10/106], [94mLoss[0m : 2.39531
[1mStep[0m  [20/106], [94mLoss[0m : 2.84367
[1mStep[0m  [30/106], [94mLoss[0m : 2.36564
[1mStep[0m  [40/106], [94mLoss[0m : 2.92291
[1mStep[0m  [50/106], [94mLoss[0m : 2.45164
[1mStep[0m  [60/106], [94mLoss[0m : 2.31545
[1mStep[0m  [70/106], [94mLoss[0m : 2.66178
[1mStep[0m  [80/106], [94mLoss[0m : 2.63363
[1mStep[0m  [90/106], [94mLoss[0m : 2.64607
[1mStep[0m  [100/106], [94mLoss[0m : 2.51672

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.393, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41533
[1mStep[0m  [10/106], [94mLoss[0m : 2.60252
[1mStep[0m  [20/106], [94mLoss[0m : 2.55031
[1mStep[0m  [30/106], [94mLoss[0m : 2.27879
[1mStep[0m  [40/106], [94mLoss[0m : 2.33809
[1mStep[0m  [50/106], [94mLoss[0m : 2.41934
[1mStep[0m  [60/106], [94mLoss[0m : 2.47947
[1mStep[0m  [70/106], [94mLoss[0m : 2.42701
[1mStep[0m  [80/106], [94mLoss[0m : 2.58956
[1mStep[0m  [90/106], [94mLoss[0m : 2.46687
[1mStep[0m  [100/106], [94mLoss[0m : 2.55838

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.388, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.381
====================================

Phase 1 - Evaluation MAE:  2.381477310972394
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.48151
[1mStep[0m  [10/106], [94mLoss[0m : 2.17207
[1mStep[0m  [20/106], [94mLoss[0m : 2.43813
[1mStep[0m  [30/106], [94mLoss[0m : 2.32696
[1mStep[0m  [40/106], [94mLoss[0m : 2.32364
[1mStep[0m  [50/106], [94mLoss[0m : 2.32840
[1mStep[0m  [60/106], [94mLoss[0m : 2.66512
[1mStep[0m  [70/106], [94mLoss[0m : 2.21756
[1mStep[0m  [80/106], [94mLoss[0m : 2.65745
[1mStep[0m  [90/106], [94mLoss[0m : 2.45028
[1mStep[0m  [100/106], [94mLoss[0m : 2.59643

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33216
[1mStep[0m  [10/106], [94mLoss[0m : 2.09277
[1mStep[0m  [20/106], [94mLoss[0m : 2.24453
[1mStep[0m  [30/106], [94mLoss[0m : 2.49838
[1mStep[0m  [40/106], [94mLoss[0m : 2.42275
[1mStep[0m  [50/106], [94mLoss[0m : 2.67181
[1mStep[0m  [60/106], [94mLoss[0m : 2.38583
[1mStep[0m  [70/106], [94mLoss[0m : 2.24233
[1mStep[0m  [80/106], [94mLoss[0m : 2.32860
[1mStep[0m  [90/106], [94mLoss[0m : 2.54924
[1mStep[0m  [100/106], [94mLoss[0m : 2.57819

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38978
[1mStep[0m  [10/106], [94mLoss[0m : 2.29582
[1mStep[0m  [20/106], [94mLoss[0m : 2.19202
[1mStep[0m  [30/106], [94mLoss[0m : 2.04414
[1mStep[0m  [40/106], [94mLoss[0m : 2.33997
[1mStep[0m  [50/106], [94mLoss[0m : 2.40686
[1mStep[0m  [60/106], [94mLoss[0m : 2.37173
[1mStep[0m  [70/106], [94mLoss[0m : 2.25351
[1mStep[0m  [80/106], [94mLoss[0m : 2.55161
[1mStep[0m  [90/106], [94mLoss[0m : 2.23298
[1mStep[0m  [100/106], [94mLoss[0m : 2.06881

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44475
[1mStep[0m  [10/106], [94mLoss[0m : 2.74818
[1mStep[0m  [20/106], [94mLoss[0m : 2.39230
[1mStep[0m  [30/106], [94mLoss[0m : 2.38708
[1mStep[0m  [40/106], [94mLoss[0m : 2.09726
[1mStep[0m  [50/106], [94mLoss[0m : 2.14746
[1mStep[0m  [60/106], [94mLoss[0m : 2.17319
[1mStep[0m  [70/106], [94mLoss[0m : 2.00394
[1mStep[0m  [80/106], [94mLoss[0m : 2.30711
[1mStep[0m  [90/106], [94mLoss[0m : 2.40777
[1mStep[0m  [100/106], [94mLoss[0m : 2.26517

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19948
[1mStep[0m  [10/106], [94mLoss[0m : 2.39397
[1mStep[0m  [20/106], [94mLoss[0m : 2.00283
[1mStep[0m  [30/106], [94mLoss[0m : 2.06347
[1mStep[0m  [40/106], [94mLoss[0m : 2.11317
[1mStep[0m  [50/106], [94mLoss[0m : 1.97171
[1mStep[0m  [60/106], [94mLoss[0m : 2.51372
[1mStep[0m  [70/106], [94mLoss[0m : 2.07406
[1mStep[0m  [80/106], [94mLoss[0m : 2.40636
[1mStep[0m  [90/106], [94mLoss[0m : 2.28797
[1mStep[0m  [100/106], [94mLoss[0m : 2.34416

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17519
[1mStep[0m  [10/106], [94mLoss[0m : 2.17020
[1mStep[0m  [20/106], [94mLoss[0m : 2.43986
[1mStep[0m  [30/106], [94mLoss[0m : 1.94402
[1mStep[0m  [40/106], [94mLoss[0m : 2.17809
[1mStep[0m  [50/106], [94mLoss[0m : 2.03365
[1mStep[0m  [60/106], [94mLoss[0m : 2.44492
[1mStep[0m  [70/106], [94mLoss[0m : 1.83934
[1mStep[0m  [80/106], [94mLoss[0m : 2.30111
[1mStep[0m  [90/106], [94mLoss[0m : 2.38207
[1mStep[0m  [100/106], [94mLoss[0m : 1.83797

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95241
[1mStep[0m  [10/106], [94mLoss[0m : 2.24279
[1mStep[0m  [20/106], [94mLoss[0m : 1.89987
[1mStep[0m  [30/106], [94mLoss[0m : 1.91757
[1mStep[0m  [40/106], [94mLoss[0m : 2.20666
[1mStep[0m  [50/106], [94mLoss[0m : 2.13519
[1mStep[0m  [60/106], [94mLoss[0m : 2.40743
[1mStep[0m  [70/106], [94mLoss[0m : 2.02298
[1mStep[0m  [80/106], [94mLoss[0m : 1.63457
[1mStep[0m  [90/106], [94mLoss[0m : 2.06883
[1mStep[0m  [100/106], [94mLoss[0m : 2.29806

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10507
[1mStep[0m  [10/106], [94mLoss[0m : 2.08197
[1mStep[0m  [20/106], [94mLoss[0m : 2.12651
[1mStep[0m  [30/106], [94mLoss[0m : 1.78742
[1mStep[0m  [40/106], [94mLoss[0m : 1.75405
[1mStep[0m  [50/106], [94mLoss[0m : 2.42732
[1mStep[0m  [60/106], [94mLoss[0m : 2.04592
[1mStep[0m  [70/106], [94mLoss[0m : 1.97427
[1mStep[0m  [80/106], [94mLoss[0m : 2.05094
[1mStep[0m  [90/106], [94mLoss[0m : 2.36808
[1mStep[0m  [100/106], [94mLoss[0m : 2.14535

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.71389
[1mStep[0m  [10/106], [94mLoss[0m : 1.79369
[1mStep[0m  [20/106], [94mLoss[0m : 2.42550
[1mStep[0m  [30/106], [94mLoss[0m : 1.86757
[1mStep[0m  [40/106], [94mLoss[0m : 2.05649
[1mStep[0m  [50/106], [94mLoss[0m : 1.70946
[1mStep[0m  [60/106], [94mLoss[0m : 2.14970
[1mStep[0m  [70/106], [94mLoss[0m : 2.20270
[1mStep[0m  [80/106], [94mLoss[0m : 1.98701
[1mStep[0m  [90/106], [94mLoss[0m : 1.95153
[1mStep[0m  [100/106], [94mLoss[0m : 2.01604

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89941
[1mStep[0m  [10/106], [94mLoss[0m : 1.77732
[1mStep[0m  [20/106], [94mLoss[0m : 1.80850
[1mStep[0m  [30/106], [94mLoss[0m : 1.84056
[1mStep[0m  [40/106], [94mLoss[0m : 2.16979
[1mStep[0m  [50/106], [94mLoss[0m : 2.08676
[1mStep[0m  [60/106], [94mLoss[0m : 1.95781
[1mStep[0m  [70/106], [94mLoss[0m : 1.76584
[1mStep[0m  [80/106], [94mLoss[0m : 1.94372
[1mStep[0m  [90/106], [94mLoss[0m : 2.15527
[1mStep[0m  [100/106], [94mLoss[0m : 1.59903

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90082
[1mStep[0m  [10/106], [94mLoss[0m : 1.77939
[1mStep[0m  [20/106], [94mLoss[0m : 2.14406
[1mStep[0m  [30/106], [94mLoss[0m : 1.79728
[1mStep[0m  [40/106], [94mLoss[0m : 1.75971
[1mStep[0m  [50/106], [94mLoss[0m : 1.89792
[1mStep[0m  [60/106], [94mLoss[0m : 1.80658
[1mStep[0m  [70/106], [94mLoss[0m : 1.68732
[1mStep[0m  [80/106], [94mLoss[0m : 1.93786
[1mStep[0m  [90/106], [94mLoss[0m : 1.78336
[1mStep[0m  [100/106], [94mLoss[0m : 1.71979

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.68622
[1mStep[0m  [10/106], [94mLoss[0m : 1.92939
[1mStep[0m  [20/106], [94mLoss[0m : 2.41097
[1mStep[0m  [30/106], [94mLoss[0m : 1.80504
[1mStep[0m  [40/106], [94mLoss[0m : 1.82501
[1mStep[0m  [50/106], [94mLoss[0m : 1.80914
[1mStep[0m  [60/106], [94mLoss[0m : 1.93408
[1mStep[0m  [70/106], [94mLoss[0m : 1.87961
[1mStep[0m  [80/106], [94mLoss[0m : 1.78134
[1mStep[0m  [90/106], [94mLoss[0m : 1.97292
[1mStep[0m  [100/106], [94mLoss[0m : 2.06569

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.538, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74166
[1mStep[0m  [10/106], [94mLoss[0m : 1.64279
[1mStep[0m  [20/106], [94mLoss[0m : 1.67263
[1mStep[0m  [30/106], [94mLoss[0m : 1.64138
[1mStep[0m  [40/106], [94mLoss[0m : 2.05094
[1mStep[0m  [50/106], [94mLoss[0m : 1.89482
[1mStep[0m  [60/106], [94mLoss[0m : 1.78843
[1mStep[0m  [70/106], [94mLoss[0m : 1.80596
[1mStep[0m  [80/106], [94mLoss[0m : 1.88773
[1mStep[0m  [90/106], [94mLoss[0m : 1.83741
[1mStep[0m  [100/106], [94mLoss[0m : 1.99405

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72864
[1mStep[0m  [10/106], [94mLoss[0m : 1.60374
[1mStep[0m  [20/106], [94mLoss[0m : 1.71113
[1mStep[0m  [30/106], [94mLoss[0m : 1.80942
[1mStep[0m  [40/106], [94mLoss[0m : 1.90999
[1mStep[0m  [50/106], [94mLoss[0m : 1.74024
[1mStep[0m  [60/106], [94mLoss[0m : 1.87738
[1mStep[0m  [70/106], [94mLoss[0m : 1.74470
[1mStep[0m  [80/106], [94mLoss[0m : 1.72361
[1mStep[0m  [90/106], [94mLoss[0m : 2.02335
[1mStep[0m  [100/106], [94mLoss[0m : 2.08100

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62697
[1mStep[0m  [10/106], [94mLoss[0m : 1.78044
[1mStep[0m  [20/106], [94mLoss[0m : 1.75980
[1mStep[0m  [30/106], [94mLoss[0m : 2.10286
[1mStep[0m  [40/106], [94mLoss[0m : 1.65512
[1mStep[0m  [50/106], [94mLoss[0m : 1.66884
[1mStep[0m  [60/106], [94mLoss[0m : 1.52819
[1mStep[0m  [70/106], [94mLoss[0m : 1.79867
[1mStep[0m  [80/106], [94mLoss[0m : 1.66626
[1mStep[0m  [90/106], [94mLoss[0m : 1.80502
[1mStep[0m  [100/106], [94mLoss[0m : 1.89088

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74025
[1mStep[0m  [10/106], [94mLoss[0m : 1.49371
[1mStep[0m  [20/106], [94mLoss[0m : 1.58597
[1mStep[0m  [30/106], [94mLoss[0m : 1.51523
[1mStep[0m  [40/106], [94mLoss[0m : 1.69226
[1mStep[0m  [50/106], [94mLoss[0m : 1.84360
[1mStep[0m  [60/106], [94mLoss[0m : 1.86271
[1mStep[0m  [70/106], [94mLoss[0m : 1.81883
[1mStep[0m  [80/106], [94mLoss[0m : 1.79646
[1mStep[0m  [90/106], [94mLoss[0m : 1.57462
[1mStep[0m  [100/106], [94mLoss[0m : 1.62290

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49122
[1mStep[0m  [10/106], [94mLoss[0m : 1.56664
[1mStep[0m  [20/106], [94mLoss[0m : 1.49336
[1mStep[0m  [30/106], [94mLoss[0m : 1.74513
[1mStep[0m  [40/106], [94mLoss[0m : 1.94177
[1mStep[0m  [50/106], [94mLoss[0m : 1.61401
[1mStep[0m  [60/106], [94mLoss[0m : 1.54846
[1mStep[0m  [70/106], [94mLoss[0m : 1.80591
[1mStep[0m  [80/106], [94mLoss[0m : 1.67829
[1mStep[0m  [90/106], [94mLoss[0m : 1.90555
[1mStep[0m  [100/106], [94mLoss[0m : 1.67303

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.49930
[1mStep[0m  [10/106], [94mLoss[0m : 1.70194
[1mStep[0m  [20/106], [94mLoss[0m : 1.73459
[1mStep[0m  [30/106], [94mLoss[0m : 1.54721
[1mStep[0m  [40/106], [94mLoss[0m : 1.77847
[1mStep[0m  [50/106], [94mLoss[0m : 1.65018
[1mStep[0m  [60/106], [94mLoss[0m : 1.50335
[1mStep[0m  [70/106], [94mLoss[0m : 1.74808
[1mStep[0m  [80/106], [94mLoss[0m : 1.77461
[1mStep[0m  [90/106], [94mLoss[0m : 1.62004
[1mStep[0m  [100/106], [94mLoss[0m : 1.57464

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.50319
[1mStep[0m  [10/106], [94mLoss[0m : 1.58571
[1mStep[0m  [20/106], [94mLoss[0m : 1.75187
[1mStep[0m  [30/106], [94mLoss[0m : 1.42504
[1mStep[0m  [40/106], [94mLoss[0m : 1.48142
[1mStep[0m  [50/106], [94mLoss[0m : 1.46323
[1mStep[0m  [60/106], [94mLoss[0m : 1.63640
[1mStep[0m  [70/106], [94mLoss[0m : 1.68660
[1mStep[0m  [80/106], [94mLoss[0m : 1.56043
[1mStep[0m  [90/106], [94mLoss[0m : 1.57602
[1mStep[0m  [100/106], [94mLoss[0m : 1.74954

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61429
[1mStep[0m  [10/106], [94mLoss[0m : 1.33628
[1mStep[0m  [20/106], [94mLoss[0m : 1.56759
[1mStep[0m  [30/106], [94mLoss[0m : 1.52812
[1mStep[0m  [40/106], [94mLoss[0m : 1.55854
[1mStep[0m  [50/106], [94mLoss[0m : 1.56629
[1mStep[0m  [60/106], [94mLoss[0m : 1.64467
[1mStep[0m  [70/106], [94mLoss[0m : 1.65410
[1mStep[0m  [80/106], [94mLoss[0m : 1.56617
[1mStep[0m  [90/106], [94mLoss[0m : 1.46926
[1mStep[0m  [100/106], [94mLoss[0m : 1.59302

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57207
[1mStep[0m  [10/106], [94mLoss[0m : 1.39577
[1mStep[0m  [20/106], [94mLoss[0m : 1.70851
[1mStep[0m  [30/106], [94mLoss[0m : 1.45927
[1mStep[0m  [40/106], [94mLoss[0m : 1.58554
[1mStep[0m  [50/106], [94mLoss[0m : 1.59573
[1mStep[0m  [60/106], [94mLoss[0m : 1.96818
[1mStep[0m  [70/106], [94mLoss[0m : 1.72334
[1mStep[0m  [80/106], [94mLoss[0m : 1.36947
[1mStep[0m  [90/106], [94mLoss[0m : 1.57665
[1mStep[0m  [100/106], [94mLoss[0m : 1.41923

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67235
[1mStep[0m  [10/106], [94mLoss[0m : 1.32146
[1mStep[0m  [20/106], [94mLoss[0m : 1.44592
[1mStep[0m  [30/106], [94mLoss[0m : 1.59804
[1mStep[0m  [40/106], [94mLoss[0m : 1.63064
[1mStep[0m  [50/106], [94mLoss[0m : 1.68112
[1mStep[0m  [60/106], [94mLoss[0m : 1.49591
[1mStep[0m  [70/106], [94mLoss[0m : 1.38445
[1mStep[0m  [80/106], [94mLoss[0m : 1.40474
[1mStep[0m  [90/106], [94mLoss[0m : 1.59990
[1mStep[0m  [100/106], [94mLoss[0m : 1.56428

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.445, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.34071
[1mStep[0m  [10/106], [94mLoss[0m : 1.46839
[1mStep[0m  [20/106], [94mLoss[0m : 1.53910
[1mStep[0m  [30/106], [94mLoss[0m : 1.42603
[1mStep[0m  [40/106], [94mLoss[0m : 1.59515
[1mStep[0m  [50/106], [94mLoss[0m : 1.37840
[1mStep[0m  [60/106], [94mLoss[0m : 1.44569
[1mStep[0m  [70/106], [94mLoss[0m : 1.50591
[1mStep[0m  [80/106], [94mLoss[0m : 1.58068
[1mStep[0m  [90/106], [94mLoss[0m : 1.62523
[1mStep[0m  [100/106], [94mLoss[0m : 1.54822

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.28904
[1mStep[0m  [10/106], [94mLoss[0m : 1.47168
[1mStep[0m  [20/106], [94mLoss[0m : 1.48925
[1mStep[0m  [30/106], [94mLoss[0m : 1.44769
[1mStep[0m  [40/106], [94mLoss[0m : 1.53674
[1mStep[0m  [50/106], [94mLoss[0m : 1.59807
[1mStep[0m  [60/106], [94mLoss[0m : 1.76536
[1mStep[0m  [70/106], [94mLoss[0m : 1.43921
[1mStep[0m  [80/106], [94mLoss[0m : 1.67333
[1mStep[0m  [90/106], [94mLoss[0m : 1.18373
[1mStep[0m  [100/106], [94mLoss[0m : 1.69808

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.534, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39158
[1mStep[0m  [10/106], [94mLoss[0m : 1.30822
[1mStep[0m  [20/106], [94mLoss[0m : 1.28947
[1mStep[0m  [30/106], [94mLoss[0m : 1.40140
[1mStep[0m  [40/106], [94mLoss[0m : 1.42253
[1mStep[0m  [50/106], [94mLoss[0m : 1.31467
[1mStep[0m  [60/106], [94mLoss[0m : 1.57770
[1mStep[0m  [70/106], [94mLoss[0m : 1.46492
[1mStep[0m  [80/106], [94mLoss[0m : 1.43023
[1mStep[0m  [90/106], [94mLoss[0m : 1.59621
[1mStep[0m  [100/106], [94mLoss[0m : 1.34349

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.475, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43165
[1mStep[0m  [10/106], [94mLoss[0m : 1.39735
[1mStep[0m  [20/106], [94mLoss[0m : 1.41140
[1mStep[0m  [30/106], [94mLoss[0m : 1.35896
[1mStep[0m  [40/106], [94mLoss[0m : 1.51916
[1mStep[0m  [50/106], [94mLoss[0m : 1.37777
[1mStep[0m  [60/106], [94mLoss[0m : 1.43727
[1mStep[0m  [70/106], [94mLoss[0m : 1.56119
[1mStep[0m  [80/106], [94mLoss[0m : 1.69832
[1mStep[0m  [90/106], [94mLoss[0m : 1.42958
[1mStep[0m  [100/106], [94mLoss[0m : 1.39029

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.444, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.40414
[1mStep[0m  [10/106], [94mLoss[0m : 1.31044
[1mStep[0m  [20/106], [94mLoss[0m : 1.35685
[1mStep[0m  [30/106], [94mLoss[0m : 1.51026
[1mStep[0m  [40/106], [94mLoss[0m : 1.82187
[1mStep[0m  [50/106], [94mLoss[0m : 1.27440
[1mStep[0m  [60/106], [94mLoss[0m : 1.56734
[1mStep[0m  [70/106], [94mLoss[0m : 1.39572
[1mStep[0m  [80/106], [94mLoss[0m : 1.40331
[1mStep[0m  [90/106], [94mLoss[0m : 1.38604
[1mStep[0m  [100/106], [94mLoss[0m : 1.49980

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.419, [92mTest[0m: 2.462, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.476
====================================

Phase 2 - Evaluation MAE:  2.475704903872508
MAE score P1      2.381477
MAE score P2      2.475705
loss              1.419256
learning_rate         0.01
batch_size             128
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.44971
[1mStep[0m  [10/106], [94mLoss[0m : 10.45823
[1mStep[0m  [20/106], [94mLoss[0m : 10.38307
[1mStep[0m  [30/106], [94mLoss[0m : 10.12239
[1mStep[0m  [40/106], [94mLoss[0m : 8.73996
[1mStep[0m  [50/106], [94mLoss[0m : 9.05505
[1mStep[0m  [60/106], [94mLoss[0m : 8.23033
[1mStep[0m  [70/106], [94mLoss[0m : 8.43526
[1mStep[0m  [80/106], [94mLoss[0m : 7.36406
[1mStep[0m  [90/106], [94mLoss[0m : 7.30042
[1mStep[0m  [100/106], [94mLoss[0m : 6.83211

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.838, [92mTest[0m: 10.926, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.40497
[1mStep[0m  [10/106], [94mLoss[0m : 6.38419
[1mStep[0m  [20/106], [94mLoss[0m : 6.15436
[1mStep[0m  [30/106], [94mLoss[0m : 5.29151
[1mStep[0m  [40/106], [94mLoss[0m : 5.10853
[1mStep[0m  [50/106], [94mLoss[0m : 4.71123
[1mStep[0m  [60/106], [94mLoss[0m : 3.88956
[1mStep[0m  [70/106], [94mLoss[0m : 3.75621
[1mStep[0m  [80/106], [94mLoss[0m : 3.29273
[1mStep[0m  [90/106], [94mLoss[0m : 3.20134
[1mStep[0m  [100/106], [94mLoss[0m : 3.04601

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.602, [92mTest[0m: 6.033, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.88750
[1mStep[0m  [10/106], [94mLoss[0m : 2.84460
[1mStep[0m  [20/106], [94mLoss[0m : 2.68833
[1mStep[0m  [30/106], [94mLoss[0m : 2.93796
[1mStep[0m  [40/106], [94mLoss[0m : 2.43677
[1mStep[0m  [50/106], [94mLoss[0m : 2.58092
[1mStep[0m  [60/106], [94mLoss[0m : 2.61546
[1mStep[0m  [70/106], [94mLoss[0m : 2.77536
[1mStep[0m  [80/106], [94mLoss[0m : 2.71719
[1mStep[0m  [90/106], [94mLoss[0m : 2.58766
[1mStep[0m  [100/106], [94mLoss[0m : 2.59528

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.630, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61501
[1mStep[0m  [10/106], [94mLoss[0m : 2.58023
[1mStep[0m  [20/106], [94mLoss[0m : 2.71374
[1mStep[0m  [30/106], [94mLoss[0m : 2.89136
[1mStep[0m  [40/106], [94mLoss[0m : 2.60771
[1mStep[0m  [50/106], [94mLoss[0m : 2.58973
[1mStep[0m  [60/106], [94mLoss[0m : 2.55951
